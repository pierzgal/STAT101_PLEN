# Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych

Ten rozdział wprowadza studentów w podstawy nauki o danych i statystyki – obszary wiedzy kluczowe dla badaczy nauk społecznych.

## Czym są Statystyka i Nauka o Danych?

::: callout-important
Statystyka i data science to sztuka i nauka wydobywania wiedzy z danych – pomagają zrozumieć świat poprzez metodyczną analizę zebranych informacji.
:::

1.  Nauka o danych i statystyka stanowią kluczowe narzędzia badawcze, które umożliwiają lepsze zrozumienie zjawisk społecznych, niezależnie od specjalizacji: politologii, ekonomii, socjologii czy innych nauk społecznych. Pozwalają analizować trendy, zachowania społeczne i efekty różnych polityk, dostarczając solidnych podstaw do formułowania wniosków opartych na danych empirycznych.

2.  Statystyka zapewnia matematyczne fundamenty analizy danych – uczy projektowania badań, syntetyzowania zebranych informacji i weryfikowania hipotez badawczych. Nauka o danych rozszerza te możliwości, integrując statystykę z umiejętnościami programistycznymi i wiedzą dziedzinową, co pozwala efektywnie pracować nawet ze złożonymi zbiorami danych.

3.  Wspólnie te dziedziny znacząco zwiększają możliwości badawcze. Umożliwiają gromadzenie i analizę dużych zbiorów danych, tworzenie czytelnych wizualizacji złożonych informacji, odkrywanie niewidocznych na pierwszy rzut oka prawidłowości w zachowaniach społecznych oraz ewaluację skuteczności różnych rozwiązań. Te umiejętności mają szerokie zastosowanie – od analizy procesów wyborczych i zjawisk ekonomicznych po badanie nierówności społecznych.

4.  W epoce cyfrowej, charakteryzującej się gwałtownym przyrostem dostępnych danych, kompetencje w zakresie ich analizy stają się niezbędnym elementem warsztatu współczesnych badaczy i specjalistów nauk społecznych.

::: callout-note
W naukach społecznych nauka o danych stanowi zestaw metod do rozwiązywania złożonych problemów badawczych – łączy podejście statystyczne, narzędzia informatyczne i wiedzę specjalistyczną, by skuteczniej analizować procesy społeczne.
:::

## Związek Między Statystyką a Nauką o Danych

Statystyka i data science to ściśle powiązane dziedziny o znaczącym obszarze wspólnym. Zamiast traktować je jako całkowicie odrębne dyscypliny, warto postrzegać je jako komplementarne podejścia w spektrum metod analizy danych:

Nauka o danych może być postrzegana jako współczesne rozwinięcie tradycyjnej statystyki, które ewoluowało w odpowiedzi na nowe możliwości technologiczne i potrzebę analizy coraz bardziej złożonych danych społecznych.


## Podstawowe Koncepcje w Nauce o Danych i Statystyce

### Dane i Populacje (Data and Populations) – kluczowe pojęcia

1.  **Dane**: Informacje zebrane w procesie badawczym – mogą to być odpowiedzi z kwestionariuszy, wyniki eksperymentów, wskaźniki ekonomiczne, treści z mediów społecznościowych lub inne mierzalne obserwacje.

2.  **Populacja**: Pełny zbiór jednostek (osób, instytucji, wydarzeń), których dotyczy badanie – cała grupa, o której badacz chce formułować wnioski.

    -   Przykład: W badaniu preferencji wyborczych populację stanowią wszyscy uprawnieni do głosowania obywatele danego kraju.

3. **Próba**: Podzbiór populacji wybrany do badania. **Reprezentatywna** próba odzwierciedla kluczowe cechy populacji docelowej w odpowiednich proporcjach. Efektywne pobieranie próbek uwzględnia różnorodność jednostek w zakresie istotnych zmiennych (demograficznych, behawioralnych, itp.). W prostej próbie losowej (SRS), każda jednostka w populacji ma równe i niezależne prawdopodobieństwo wyboru. Reprezentatywność próby zależy zarówno od losowości metody pobierania, jak i od wystarczającej wielkości próby, aby zminimalizować błąd.

    -   Przykład: Zamiast badać wszystkich uprawnionych wyborców, analizuje się 1500 losowo wybranych osób z uwzględnieniem odpowiedniego rozkładu wieku, płci, wykształcenia i regionu zamieszkania.

    Prawidłowo dobrana próba umożliwia wnioskowanie o całej populacji przy znaczącej optymalizacji zasobów badawczych.

4. **Pobieranie próbek** (*sampling*, próbkowanie) to procedura wybierania jednostek z populacji do badania. Nieobciążona metoda pobierania próbek daje każdej jednostce w populacji równą szansę na wybór, zapewniając reprezentatywne wyniki.

::: {.panel-tabset group="language"}
## SRS
**Prosty Dobór Losowy**: Każda jednostka ma równe prawdopodobieństwo wyboru. Cała populacja jest losowo próbkowana bez żadnego z góry ustalonego wzorca.

![Prosty Dobór Losowy. Źródło: https://app.passyourmath.com/courses/theory/116/1750/26640/en](stat_imgs/Simple_Random_Sampling2.svg)

## Dobór warstwowy
**Dobór Warstwowy**: Populacja jest podzielona na odrębne podgrupy (warstwy) przed losowym pobraniem próbek z każdej warstwy proporcjonalnie.

![Dobór Warstwowy. Źródło: https://app.passyourmath.com/courses/theory/116/1750/26640/en](stat_imgs/Stratified2.svg)

## Dobór grupowy
**Dobór Grupowy**: Populacja jest podzielona na skupiska (klastry), a całe skupiska są losowo wybierane do analizy zamiast pojedynczych jednostek.

![Dobór Grupowy. Źródło: https://app.passyourmath.com/courses/theory/116/1750/26640/en](stat_imgs/Cluster2.svg)
:::

::: callout-note
## Proces Generowania Danych (PGD) i Superpopulacja: Rozszerzenie Tradycyjnych Koncepcji

W tradycyjnej statystyce pracujemy z dwoma kluczowymi pojęciami:

-   **Populacja**: Cała grupa jednostek, którą chcemy badać.
-   **Próba**: Podzbiór populacji, który faktycznie obserwujemy i analizujemy.

Współczesne badania często wymagają myślenia wykraczającego poza ten dychotomiczny podział. Tu wkraczają koncepcje **Procesu Generowania Danych (PGD)** i **superpopulacji**, które pogłębiają nasze rozumienie danych i ich struktury.

### Kluczowe definicje

**Populacja**: Zbiór wszystkich jednostek, które są bezpośrednim przedmiotem naszego badania (np. wszyscy dorośli mieszkańcy Polski).

**Próba**: Podzbiór populacji, który faktycznie badamy (np. 1000 losowo wybranych dorosłych Polaków).

**Superpopulacja**: Teoretyczny koncept obejmujący nie tylko obserwowalne jednostki z aktualnej populacji, ale wszystkie potencjalne jednostki, które mogłyby wystąpić w podobnych warunkach (np. wszyscy możliwi dorośli Polacy - obecni, przyszli i hipotetyczni).

**Proces Generowania Danych (PGD)**: Mechanizmy i czynniki, które generują wartości obserwowanych zmiennych w systemie.

### Proces Generowania Danych (PGD; Data Generating Process, DGP)

**Definicja formalna**:
PGD to zbiór mechanizmów statystycznych i przyczynowych odpowiedzialnych za wytwarzanie obserwowanych wartości zmiennych w systemie, opisany najczęściej za pomocą funkcji matematycznych i rozkładów prawdopodobieństwa.

**Intuicyjne wyjaśnienie**:
PGD można postrzegać jako "mechanizm" lub "czarną skrzynkę", która przekształca przyczyny w skutki. To fundamentalny proces, który produkuje dane obserwowane w rzeczywistym świecie - zarówno w naszej próbie, jak i w całej populacji, a także potencjalnie poza nią.

### Przykłady ilustrujące różnice

#### Przykład 1: Badanie opinii wyborców

- **Populacja**: Wszyscy zarejestrowani wyborcy w Polsce w 2023 roku (ok. 30 milionów osób)
- **Próba**: 1000 losowo wybranych wyborców ankietowanych w sondażu
- **Superpopulacja**: Wszyscy możliwi wyborcy (obecni, przyszli i hipotetyczni) oraz wszystkie możliwe scenariusze głosowania
- **PGD**: Złożony mechanizm kształtujący opinie i decyzje wyborcze, w tym:
  - Czynniki demograficzne (wiek, wykształcenie, miejsce zamieszkania)
  - Warunki ekonomiczne (dochód, status zawodowy)
  - Wpływ mediów i debaty publicznej
  - Doświadczenia osobiste
  - Historyczne uwarunkowania polityczne

#### Przykład 2: Badanie efektów leku przeciwcukrzycowego

- **Populacja**: Wszyscy pacjenci z cukrzycą typu 2 w danym kraju (np. 2 miliony osób)
- **Próba**: 500 pacjentów uczestniczących w badaniu klinicznym
- **Superpopulacja**: Wszyscy możliwi pacjenci z cukrzycą typu 2 (obecni i przyszli) z różnymi profilami genetycznymi i środowiskowymi
- **PGD**: Biologiczny mechanizm obejmujący:
  - Interakcje leku z receptorami w organizmie
  - Indywidualne uwarunkowania genetyczne
  - Czynniki środowiskowe (dieta, aktywność fizyczna)
  - Interakcje z innymi lekami
  - Mechanizmy metaboliczne organizmu

#### Przykład 3: Gdy próba równa się populacji

Badanie wszystkich 50 stanów USA:

- **Tradycyjne podejście**: Nie ma rozróżnienia między próbą a populacją (badamy wszystkie stany)
- **Podejście superpopulacyjne**:
  - **Populacja/Próba**: 50 istniejących stanów USA
  - **Superpopulacja**: Teoretyczny zbiór wszystkich możliwych jednostek terytorialnych typu "stan" w różnych warunkach historycznych, politycznych i społecznych
  - **PGD**: Fundamentalne mechanizmy geograficzne, historyczne, polityczne i społeczno-ekonomiczne kształtujące charakterystyki stanów

#### Przykład 4: Jakość pizzy w Nowym Jorku

- **Populacja**: Wszystkie obecnie działające pizzerie w Nowym Jorku (np. 2000 lokali)
- **Próba**: 50 losowo wybranych pizzerii z różnych dzielnic
- **Superpopulacja**: Wszystkie możliwe pizzerie, które mogłyby istnieć w Nowym Jorku:
  - Obecnie działające
  - Przyszłe (jeszcze nieotwarte)
  - Historyczne (już zamknięte)
  - Hipotetyczne (w alternatywnych warunkach ekonomicznych czy kulturowych)
- **PGD**: Czynniki wpływające na jakość pizzy:
  - Składniki i ich jakość
  - Umiejętności i doświadczenie szefów kuchni
  - Sprzęt i infrastruktura kuchenna
  - Metody przygotowania i przepisy
  - Czynniki środowiskowe (np. jakość lokalnej wody)
  - Wpływy kulturowe i tradycje kulinarne
  - Uwarunkowania ekonomiczne (koszty operacyjne, czynsze)

PGD jest jak "przepis na jakość pizzy", który determinuje rezultaty dla wszystkich potencjalnych pizzerii w superpopulacji, nie tylko dla obecnie istniejących lokali.

### Praktyczne znaczenie tych różnic

1. **Wnioskowanie statystyczne**:
   - W tradycyjnym podejściu: Z próby wnioskujemy o populacji
   - W podejściu superpopulacyjnym: Z modelu wnioskujemy o ogólnych prawidłowościach wykraczających poza obserwowaną populację

2. **Generalizacja wyników**:
   - Tradycyjne podejście: Wyniki możemy generalizować tylko na obecną populację
   - Podejście superpopulacyjne: Wyniki można generalizować na przyszłe lub hipotetyczne przypadki

3. **Interpretacja niepewności**:
   - W podejściu tradycyjnym: Niepewność wynika głównie z losowości próby
   - W podejściu superpopulacyjnym: Niepewność wynika zarówno z losowości próby, jak i z losowości procesów w ramach PGD

4. **Modelowanie predykcyjne**:
   - Tradycyjne podejście: Ograniczone do obecnej populacji
   - Podejście superpopulacyjne: Pozwala modelować nowe, nieobserwowane jeszcze przypadki

Zrozumienie różnic między tymi koncepcjami jest kluczowe dla prawidłowej interpretacji wyników badań i właściwego wnioskowania statystycznego, szczególnie w badaniach obejmujących złożone systemy społeczne, ekonomiczne czy biologiczne.

:::

```{mermaid}
graph TD
    A[Data Generating Process DGP]
    B(Population)
    C[Sample]
    A -->|Generates| B
    B -->|Sampled from| C
    C -.->|Inference| B
    C -.->|Inference| A
    B -.->|Inference| A
    
    classDef dgp fill:#f9f,stroke:#333,stroke-width:4px;
    classDef pop fill:#bbf,stroke:#333,stroke-width:2px;
    classDef sam fill:#bfb,stroke:#333,stroke-width:2px;
    
    class A dgp;
    class B pop;
    class C sam;
```



::: {.callout-important}
## Parametr populacji ($\theta$)

Wartość liczbowa charakteryzująca określoną cechę całej populacji. Przykładami parametrów populacji są:

- $\mu$ (średnia populacji)
- $\sigma^2$ (wariancja populacji)
- $\sigma$ (odchylenie standardowe populacji)
- $p$ (proporcja w populacji)

Parametry populacji zazwyczaj nie są znane i stanowią przedmiot estymacji na podstawie próby.

## Rozkład statystyki ($\hat{\theta}$) z próby

**Statystyka z próby** (a sample statistic) $\hat{\theta}$ to wartość wyliczona na podstawie obserwacji z próby, która służy do estymacji parametru populacji $\theta$. Rozkład statystyki opisuje, jak zmieniają się wartości tej statystyki przy wielokrotnym pobieraniu prób o tej samej liczebności $n$ z tej samej populacji.

Rozkład statystyki odnosi się do rozkładu prawdopodobieństwa, który opisuje wszystkie możliwe wartości, jakie może przyjąć dana statystyka, gdy jest obliczana z różnych prób losowych pobieranych z tej samej populacji, wraz z prawdopodobieństwem uzyskania każdej wartości.

Gdy pobieramy próbę z populacji i obliczamy statystykę (np. średnią z próby), otrzymujemy jedną konkretną wartość. Jednakże gdybyśmy powtórzyli ten proces wielokrotnie – pobierając różne próby losowe o tej samej wielkości z tej samej populacji – za każdym razem otrzymalibyśmy inne wartości tej statystyki. Rozkład statystyki opisuje właśnie ten wzorzec zmienności.

Kluczowe cechy rozkładu statystyki:

1. Pokazuje, zróżnicowanie statystyki w różnych próbach
2. Pomaga zrozumieć błąd losowy i niepewność związaną z próbkowaniem
3. Pozwala formułować stwierdzenia probabilistyczne o tym, jak blisko nasza estymacja jest prawdziwego parametru populacji
4. Stanowi podstawę wnioskowania statystycznego, w tym przedziałów ufności i testów hipotez

Przykład: Jeśli interesuje nas średnia populacji $\mu$, to jej estymatorem jest średnia z próby $\bar{x}$. Rozkład statystyki $\bar{x}$ pokazuje, jakie wartości może przyjmować średnia z próby i z jakim prawdopodobieństwem. W określonych warunkach, ten rozkład zbliża się do "rozkładu normalnego" wraz ze wzrostem wielkości próby (zgodnie z Centralnym Twierdzeniem Granicznym), nawet jeśli oryginalny rozkład populacji nie jest "normalny".

Możemy przedstawić to formalnie następująco:
$$\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i$$

gdzie $x_1, x_2, ..., x_n$ to obserwacje w próbie losowej.

## Wartość oczekiwana statystyki z próby

Wartość oczekiwana statystyki z próby $E(\hat{\theta})$ to średnia wartość, jaką przyjmuje dana statystyka przy wielokrotnym pobieraniu prób z populacji.

Istnieją ważne twierdzenia w statystyce, które gwarantują nam, że dla dobrze skonstruowanych estymatorów wartość oczekiwana statystyki jest równa parametrowi, który chcemy estymować. Mówiąc nieformalnie - "dobre" estymatory są skonstruowane tak, żeby "średnio" trafiały w cel, czyli:

$$E(\hat{\theta}) = \theta$$

Na przykład, dla średniej z próby możemy udowodnić, że:
$$E(\bar{x}) = E\left(\frac{1}{n}\sum_{i=1}^{n}x_i\right) = \frac{1}{n}\sum_{i=1}^{n}E(x_i) = \frac{1}{n} \cdot n \cdot \mu = \mu$$

Taki estymator nazywamy nieobciążonym. Co ważne, nie wszystkie estymatory są nieobciążone - niektóre mają systematyczne odchylenie, np. estymator wariancji $s^2$ musi zawierać korektę $\frac{1}{n-1}$ zamiast $\frac{1}{n}$, aby być nieobciążonym.

:::

![The process of using a sample from a population to obtain a point estimate of a population parameter. In this case, a sample of 10 individuals yielded 6 who own an iPhone, resulting in an estimated population proportion of 60% iPhone owners. The actual population proportion is 53.8%. Retrieved from: https://datasciencebook.ca/inference.html](stat_imgs/population_vs_sample.png)

![Various random samples of equal size and their statistics. Retrieved from: https://allmodelsarewrong.github.io/mse.html](stat_imgs/sampling-estimators.svg)


::: {.callout-note}
# Rozkład normalny - krzywa dzwonowa

Rozkład normalny (znany również jako rozkład Gaussa lub krzywa dzwonowa) to jeden z najważniejszych rozkładów prawdopodobieństwa w statystyce.

Rozkład normalny ma charakterystyczny kształt przypominający dzwon lub kopułę - stąd popularna nazwa "krzywa dzwonowa". Jest on symetryczny wokół średniej, co oznacza, że wartości powyżej i poniżej średniej występują z takim samym prawdopodobieństwem.

Co ciekawe, rozkład normalny pojawia się w naturze zaskakująco często. Wzrost ludzi, błędy pomiarowe, wyniki testów, a nawet odchylenia w procesach produkcyjnych często podążają za tym rozkładem.

Najważniejsze jest to, że zgodnie z Centralnym Twierdzeniem Granicznym, średnie z prób mają tendencję do podążania za rozkładem normalnym, nawet jeśli oryginalne dane nie mają rozkładu normalnego. To właśnie dlatego rozkład normalny jest tak fundamentalny dla statystyki i wnioskowania statystycznego!

Rozkład normalny jest w pełni opisany dwoma parametrami: średnią $\mu$ (która określa środek rozkładu) i odchyleniem standardowym $\sigma$ (które określa, jak szeroki jest "dzwon").
:::


## Dane i Populacje

Dane stanowią podstawę analizy statystycznej. Aby lepiej zrozumieć ich rolę, warto poznać kluczowe pojęcia.

### Rodzaje Danych

-   **Dane pierwotne** (*Primary data*): Zebrane bezpośrednio w określonym celu badawczym, np. przeprowadzenie własnej ankiety
-   **Dane wtórne** (*Secondary data*): Uzyskane z istniejących źródeł, np. baz danych czy publikacji innych badaczy

::: callout-note
## Populacja i Próba - fundamentalne rozróżnienie

-   **Populacja**: Pełny zbiór wszystkich elementów/jednostek, o których chcemy wnioskować (np. wszyscy dorośli obywatele Polski)
-   **Próba**: Podzbiór populacji, który badamy w praktyce (np. 1000 losowo wybranych dorosłych obywateli Polski)

W praktyce badawczej niemal zawsze analizujemy próbę, a następnie wnioskujemy o populacji.
:::

### Zmienne i Stałe

**Zmienne** to cechy, które mogą przyjmować różne wartości w zbiorze danych. Stanowią one obiekt naszych badań i analiz.

#### Klasyfikacja Zmiennych

1.  **Zmienne Ilościowe** (*Quantitative*):
    -   **Ciągłe** (*Continuous*): Mogą przyjmować dowolną wartość w określonym przedziale, np. wzrost, waga, temperatura
    -   **Dyskretne** (*Discrete*): Przyjmują tylko określone wartości (zwykle liczby całkowite), np. liczba dzieci, liczba błędów
2.  **Zmienne Jakościowe** (*Qualitative*):
    -   **Nominalne** (*Nominal*): Kategorie bez naturalnej kolejności, np. grupa krwi, płeć, województwo
    -   **Porządkowe** (*Ordinal*): Kategorie z naturalną kolejnością, np. wykształcenie (podstawowe, średnie, wyższe), skala Likerta (1-5)

**Stałe** to wartości, które pozostają niezmienne w trakcie analizy i często służą jako punkty odniesienia.

## Parametry Populacji i Związane Pojęcia - Kluczowe Rozróżnienia

W statystyce istnieje kilka podobnie brzmiących pojęć, które często są mylone. Poniżej przedstawiam ich klarowne rozróżnienie:

### Parametr Populacji i Estymand

**Parametr populacji** to wartość liczbowa opisująca cechę całej populacji. Kluczowe cechy:

1.  Dotyczy *całej* populacji, nie tylko próby
2.  Jest zwykle oznaczany greckimi literami (μ, σ, π, ρ)
3.  W większości przypadków pozostaje **nieznany** (nie możemy zbadać całej populacji)
4.  Jest determinowany przez rzeczywisty Proces Generujący Dane (DGP)

**Estymand** (*Estimand*) to konkretny parametr populacji lub funkcja parametrów, którą chcemy oszacować. Jest to *cel naszej estymacji*.

Przykłady parametrów populacji:

-   Średnia populacji (μ): Prawdziwa średnia wartość cechy w populacji
-   Wariancja populacji (σ²): Prawdziwa miara zmienności w populacji
-   Proporcja populacji (p): Prawdziwa proporcja jednostek w populacji posiadających daną cechę

::: callout-important
## Ważne rozróżnienie!

Estymand (parametr populacji) to wartość w populacji, którą chcemy poznać, ale która pozostaje dla nas nieznana. Jest to nasz cel badawczy.
:::

### Estymator (Statystyka)

**Estymator** to funkcja matematyczna (wzór, procedura), która na podstawie danych z próby dostarcza oszacowania parametru populacji. **Estymator jest zmienną losową**, ponieważ jego wartość zależy od konkretnej próby.

**Statystyka** to każda miara obliczona na podstawie danych z próby. Gdy statystyka służy do oszacowania parametru populacji, nazywamy ją estymatorem.

Przykłady estymatorów (statystyk):

-   Średnia z próby: $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ (estymator średniej populacji μ)
-   Wariancja z próby: $s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2$ (estymator wariancji populacji σ²)
-   Proporcja z próby: $\hat{p} = \frac{x}{n}$ (estymator proporcji populacji p)

::: callout-note
## Estymator jako procedura

Estymator należy rozumieć jako **przepis** na obliczenie wartości na podstawie próby. Ten sam estymator zastosowany do różnych prób da różne wyniki.

Przykład: Estymator średniej $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ to procedura "zsumuj wszystkie wartości i podziel przez ich liczbę".
:::

### Oszacowanie (Estymata)

**Oszacowanie** (estymata, ang. *estimate*) to konkretna wartość liczbowa otrzymana po zastosowaniu estymatora do określonej próby. Jest to pojedyncza liczba, będąca realizacją zmiennej losowej, jaką jest estymator.

::: callout-tip
## Przykład rozróżnienia tych pojęć

-   **Estymand**: Średnia wysokość wszystkich dorosłych Polaków (μ) - nieznana wartość
-   **Estymator**: Wzór na średnią z próby $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ - procedura obliczeniowa
-   **Oszacowanie** (estymata): 173.5 cm - konkretna wartość otrzymana z próby

Różne próby dadzą różne oszacowania tego samego estymandy przy użyciu tego samego estymatora.
:::

### Właściwości Estymatorów

Dobry estymator powinien posiadać korzystne właściwości statystyczne:

1.  **Nieobciążoność** (*Unbiasedness*): Estymator jest nieobciążony, jeśli jego wartość oczekiwana (średnia z wielu prób) jest równa estymandzie. Formalnie: E(θ̂) = θ

2.  **Efektywność** (*Efficiency*): Estymator jest efektywny, jeśli ma najmniejszą możliwą wariancję spośród wszystkich nieobciążonych estymatorów

3.  **Zgodność** (*Consistency*): Estymator jest zgodny, jeśli wraz ze wzrostem wielkości próby jego wartość zbliża się do prawdziwej wartości parametru

4.  **Dostateczność** (*Sufficiency*): Estymator jest dostateczny, jeśli wykorzystuje wszystkie dostępne informacje z próby odnośnie szacowanego parametru

## Modele Statystyczne i Wnioskowanie

### Modele Statystyczne

Model statystyczny to matematyczna reprezentacja rzeczywistości, która opisuje relacje między zmiennymi i strukturę danych. Pozwala na opisanie procesu generującego dane (DGP) i wnioskowanie o parametrach.

::: callout-note
# Komponenty Modelu Statystycznego

Model statystyczny to matematyczne ramy służące do reprezentowania zależności w danych i formułowania prognoz. Kompletny model statystyczny składa się z:

1. **Forma funkcyjna**: Struktura matematyczna określająca relację między zmiennymi (np. liniowa, kwadratowa, wykładnicza, logarytmiczna)

2. **Zmienne**: 
   - Zmienna zależna: To, co staramy się przewidzieć lub wyjaśnić
   - Zmienne niezależne/objaśniające: Czynniki, które mogą wpływać na zmienną zależną

3. **Parametry**: Nieznane wartości, które szacujemy na podstawie danych, kwantyfikujące relację między zmiennymi (np. współczynniki regresji, średnie, wariancje)

4. **Składnik losowy**: Człon błędu lub element stochastyczny, który uwzględnia niepewność i zmienność niewyjaśnioną przez model

5. **Założenia dotyczące rozkładu prawdopodobieństwa**: Specyfikacje dotyczące tego, jak jest rozłożony składnik losowy (np. rozkład normalny, Poissona, dwumianowy)
:::

Przykład modelu regresji liniowej: $y = \beta_0 + \beta_1x + \epsilon$, gdzie $\epsilon \sim N(0, \sigma^2)$

W tym modelu:

- $\beta_0$ i $\beta_1$ to parametry (estymandy), które chcemy oszacować
- $\epsilon$ to składnik losowy reprezentujący niewyjaśnioną zmienność
- Zakładamy normalność rozkładu błędów losowych

### Wnioskowanie Przyczynowe vs. Predykcyjne

W analizie statystycznej możemy mieć dwa główne cele:

1.  **Wnioskowanie przyczynowe**: Ustalenie, czy zmienna X *powoduje* zmianę w zmiennej Y
    -   Wymaga dodatkowych założeń lub specjalnych projektów badawczych
    -   Umożliwia przewidywanie efektów interwencji
2.  **Wnioskowanie predykcyjne**: Przewidywanie wartości Y na podstawie X
    -   Nie musi zakładać związku przyczynowego
    -   Koncentruje się na dokładności przewidywań

::: callout-warning
## Korelacja ≠ Przyczynowość

W statystyce, związek pozorny (spurious relationship) lub korelacja pozorna (spurious correlation) to relacja matematyczna, w której dwa lub więcej zdarzeń lub zmiennych są ze sobą powiązane, ale nie pozostają w relacji przyczynowo-skutkowej. Wynika to albo z przypadkowego zbiegu okoliczności, albo z obecności pewnego trzeciego, niewidocznego czynnika (określanego jako "wspólna zmienna odpowiedzi" (common response variable), "czynnik zakłócający" (confounding factor) lub "zmienna ukryta" (lurking variable)).

Jednym z najczęstszych błędów w statystyce jest interpretowanie korelacji jako dowodu na przyczynowość. Dwie zmienne mogą być silnie skorelowane z powodu:

1.  Zmiennej zakłócającej (confounder), która wpływa na obie zmienne
2.  Odwróconej przyczynowości (Y wpływa na X, a nie odwrotnie)
3.  Przypadku (korelacja pozorna)
:::

### Problemy Wnioskowania Przyczynowego

Fundamentalnym problemem wnioskowania przyczynowego jest niemożność obserwowania **kontrfaktów** (alternatywnych/kontrfaktycznych scenariuszy). Dla danej jednostki możemy zaobserwować tylko jeden potencjalny wynik.

![Fundamentalny problem wnioskowania przyczynowego: Możemy traktować wnioskowanie przyczynowe jako problem PREDYKCJI. Jak przewidzieć kontrfakt, skoro nigdy go nie obserwujemy?](stat_imgs/meme_horse.svg){fig-align="center"}

Przykład:

- Obserwujemy osobę, która skończyła studia i zarabia 8000 zł
- Nie możemy zaobserwować, ile ta sama osoba zarabiałaby, gdyby nie skończyła studiów

Metody przyczynowe próbują rozwiązać ten problem przez: 1. Randomizowane eksperymenty 2. Metoda zmiennych instrumentalnych 3. Matching 4. Analizę regresji nieciągłej 5. Modele regresji difference-in-differences

Wnioskowanie przyczynowe jest utrudnione przez różne problemy, takie jak:

![Błąd wspólnej przyczyny (confounding bias) i pozorna korelacja: picie alkoholu wieczorem jest wspólną przyczyną spania w butach i budzenia się z bólem głowy](stat_imgs/IMG_4337.jpg){fig-align="center"}

![Odwrócona przyczynowość](stat_imgs/ff13-23.png){fig-align="center"}

## Wnioskowanie Statystyczne

Wnioskowanie statystyczne to proces formułowania wniosków o populacji na podstawie danych z próby. Obejmuje dwa główne obszary:

### 1. Estymacja

Estymacja to proces szacowania nieznanych parametrów populacji na podstawie danych z próby. Wyróżniamy:

-   **Estymację punktową**: Podajemy pojedynczą wartość (oszacowanie) jako najlepsze przybliżenie parametru
-   **Estymację przedziałową**: Konstruujemy przedział ufności, który wskazuje zakres możliwych wartości parametru zgodnych z naszymi danymi

Przykład przedziału ufności: "95% przedział ufności dla średniego wzrostu dorosłych Polaków wynosi (173 cm, 175 cm)".

**Poprawna interpretacja przedziału ufności**: Gdybyśmy wielokrotnie pobierali próby z tej samej populacji i dla każdej z nich konstruowali 95% przedział ufności według tej samej metody, to około 95% tak skonstruowanych przedziałów zawierałoby prawdziwą wartość parametru populacji.

**Niepoprawna interpretacja**: "Jest 95% szans, że prawdziwa średnia znajduje się w przedziale (173 cm, 175 cm)" – jest to błędne, ponieważ parametr populacji jest wartością stałą (choć nieznaną), a nie zmienną losową.

### 2. Testowanie Hipotez

Testowanie hipotez to formalna procedura weryfikacji przypuszczeń dotyczących parametrów populacji. Najlepiej zrozumieć tę koncepcję na konkretnym przykładzie:

::: callout-tip
## Przykład: Test dwumianowy dla monety

Wyobraźmy sobie, że chcemy sprawdzić, czy moneta jest uczciwa.

1.  **Pytanie badawcze**: Czy moneta jest uczciwa (prawdopodobieństwo wypadnięcia orła = 0.5)?

2.  **Formułujemy hipotezy**:

    -   **Hipoteza zerowa (H₀)**: p = 0.5 (moneta jest uczciwa)
    -   **Hipoteza alternatywna (H₁)**: p ≠ 0.5 (moneta nie jest uczciwa)

3.  **Zbieramy dane**: Rzucamy monetą 100 razy i otrzymujemy 65 orłów.

4.  **Analizujemy**: Czy 65 orłów na 100 rzutów jest dowodem przeciwko hipotezie, że moneta jest uczciwa?

5.  **Rozumowanie**:

    -   Jeśli moneta byłaby uczciwa (p = 0.5), to liczba orłów w 100 rzutach powinna podlegać rozkładowi dwumianowemu B(100, 0.5)
    -   Dla tego rozkładu oczekujemy średnio 50 orłów, z odchyleniem standardowym √(100 × 0.5 × 0.5) = 5
    -   Otrzymanie 65 orłów oznacza odchylenie o 3 odchylenia standardowe od oczekiwanej wartości
    -   Prawdopodobieństwo uzyskania 65 lub więcej orłów przy uczciwej monecie jest bardzo małe (p \< 0.01)

6.  **Wniosek**: Ponieważ zaobserwowany wynik jest bardzo mało prawdopodobny przy założeniu, że moneta jest uczciwa, odrzucamy hipotezę zerową i wnioskujemy, że moneta najprawdopodobniej nie jest uczciwa.
:::


Ogólna procedura testowania hipotez:

1. Formułujemy hipotezę zerową (H₀) i alternatywną (H₁)
2. Wybieramy poziom istotności α (najczęściej 0.05) - reguła decyzyjna, która pomaga ocenić co to znaczy małe prawdopodobieństwo zaobserwowanego wyniku przy założeniu, że testowana hipoteza jest prawdziwa
3. Zbieramy dane i obliczamy odpowiednią statystykę testową
4. Obliczamy p-wartość (prawdopodobieństwo uzyskania naszych danych lub bardziej ekstremalnych, przy założeniu prawdziwości H₀)
5. Podejmujemy decyzję: jeśli p < α, odrzucamy H₀ na rzecz H₁


::: callout-note
## Intuicja za testowaniem hipotez

Testowanie hipotez przypomina procedurę sądową:

- H₀ odpowiada zasadzie "niewinny, dopóki nie udowodni się winy" (zakładamy, że parametr ma określoną wartość)
- Dane stanowią "dowody" przeciwko H₀
- P-wartość określa, jak silne są te dowody
- Jeśli dowody są wystarczająco mocne (p \< α), "skazujemy" H₀ (odrzucamy ją)
- Jeśli dowody nie są wystarczająco mocne, nie odrzucamy H₀ (ale nie udowadniamy jej prawdziwości)

:::

::: callout-important
## Częste błędy interpretacji p-wartości i testów

1.  P-wartość **NIE** jest prawdopodobieństwem, że hipoteza zerowa jest prawdziwa
2.  P-wartość **NIE** jest prawdopodobieństwem popełnienia błędu przy odrzuceniu H₀
3.  Brak odrzucenia H₀ **NIE** oznacza jej udowodnienia (brak dowodów przeciwko oskarżonemu nie dowodzi jego niewinności)
4.  Bardzo mała p-wartość **NIE** oznacza dużego efektu praktycznego (istotność statystyczna ≠ istotność praktyczna)
5.  P-wartość zależy od wielkości próby - przy bardzo dużych próbach nawet małe, nieistotne praktycznie różnice mogą być statystycznie istotne

**Definicja p-wartości**: Prawdopodobieństwo zaobserwowania wyniku co najmniej tak ekstremalnego jak uzyskany, przy założeniu prawdziwości hipotezy zerowej.
:::

::: callout-tip
## Typy błędów w testowaniu hipotez

-   **Błąd I rodzaju (α)**: Odrzucenie prawdziwej hipotezy zerowej ("skazanie niewinnego")
    -   Prawdopodobieństwo tego błędu kontrolujemy poprzez poziom istotności α
-   **Błąd II rodzaju (β)**: Nieodrzucenie fałszywej hipotezy zerowej ("uniewinnienie winnego")
    -   Prawdopodobieństwo uniknięcia tego błędu (1-β) nazywamy mocą testu
    -   Moc testu zwiększa się wraz z wielkością próby i wielkością efektu
:::

## Mocne Podstawy Dobrego Badania

Aby przeprowadzić rzetelne badanie statystyczne, należy zadbać o:

1.  **Reprezentatywność próby**: Próba powinna dobrze odzwierciedlać badaną populację
2.  **Odpowiedni rozmiar próby**: Większe próby dają dokładniejsze oszacowania i większą moc statystyczną
3.  **Kontrolę zmiennych zakłócających**: Zarówno w projektowaniu badania, jak i analizie danych
4.  **Właściwe metody statystyczne**: Dopasowane do typu danych i pytań badawczych
5.  **Przejrzystą interpretację**: Uwzględniającą ograniczenia badania i alternatywne wyjaśnienia

::: callout-tip
## Podsumowanie kluczowych pojęć często mylonych przez studentów:

| Pojęcie | Definicja | Przykład |
|----|----|----|
| **Parametr populacji (Estymand)** | Wartość charakteryzująca populację, zwykle nieznana | μ (średnia populacji) |
| **Estymator (Statystyka)** | Funkcja/procedura szacowania parametru na podstawie próby | $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ |
| **Oszacowanie (Estymata)** | Konkretna wartość uzyskana po zastosowaniu estymatora do próby | $\bar{x} = 173.5$ cm |
| **Błąd standardowy** | Miara zmienności estymatora między próbami | $SE(\bar{x}) = \frac{s}{\sqrt{n}}$ |
| **Przedział ufności** | Zakres wartości, który z określonym prawdopodobieństwem zawiera parametr | (173 cm, 175 cm) |
| **P-wartość** | Prawdopodobieństwo zaobserwowania danych przy założeniu H₀ | p = 0.03 |
:::

## Główne Komponenty Procesu Badawczego w Nauce o Danych

### Zbieranie Danych

-   **Metody eksperymentalne**: Kontrolowane badania z manipulacją zmiennych
-   **Badania obserwacyjne**: Gromadzenie danych bez ingerencji badacza
-   **Ankiety i wywiady**: Zbieranie danych bezpośrednio od respondentów
-   **Dane administracyjne**: Wykorzystanie istniejących rejestrów i baz danych
-   **Zbieranie danych cyfrowych**: Dane z internetu, mediów społecznościowych, czujników IoT

### Przetwarzanie i Przygotowanie Danych

-   **Czyszczenie danych**: Identyfikacja i korekta błędów, niespójności, duplikatów
-   **Obsługa brakujących wartości**: Imputacja, usuwanie obserwacji, analiza braków
-   **Transformacja danych**: Normalizacja, standaryzacja, przekształcenia rozkładów
-   **Inżynieria cech**: Tworzenie nowych zmiennych na podstawie istniejących
-   **Redukcja wymiarowości**: Uproszczenie danych z zachowaniem istotnych informacji

### Analiza Danych i Wnioskowanie

-   **Eksploracyjna analiza danych (EDA)**: Badanie struktury danych, wykrywanie wzorców
-   **Modelowanie statystyczne**: Budowa modeli opisujących relacje między zmiennymi
-   **Wnioskowanie statystyczne**: Testowanie hipotez, przedziały ufności, wnioskowanie przyczynowe
-   **Uczenie maszynowe**: Wykorzystanie algorytmów do automatycznego uczenia się z danych
-   **Interpretacja wyników**: Nadawanie znaczenia odkrytym zależnościom

### Komunikacja i Wdrożenie Wyników

-   **Wizualizacja danych**: Przekazywanie wyników w formie graficznej
-   **Raportowanie**: Przygotowanie raportów, artykułów, prezentacji
-   **Podejmowanie decyzji**: Wykorzystanie wyników analizy do praktycznych działań
-   **Wdrażanie modeli**: Implementacja rozwiązań w realnych systemach
-   **Ewaluacja i monitoring**: Ocena skuteczności wdrożonych rozwiązań

::: callout-note
## Od teorii do praktyki

W praktyce proces badawczy jest iteracyjny. Wyniki analizy danych często prowadzą do nowych pytań, dodatkowego zbierania danych lub modyfikacji modeli. Dobry badacz musi być gotowy na wielokrotne przechodzenie między tymi etapami.
:::

## Narzędzia do Nauki o Danych w Naukach Społecznych

W tym kursie będziemy głównie używać R do naszej analizy danych, ponieważ jest on szeroko stosowany w badaniach nauk społecznych.

### R w Analizie Danych Nauk Społecznych

R oferuje potężne możliwości dla badań w naukach społecznych, od manipulacji danymi po zaawansowane modelowanie statystyczne.

```{r}
#| code-fold: true
#| code-summary: "Kliknij, aby pokazać/ukryć kod R"

library(tidyverse)

# Set seed for reproducibility
set.seed(42)

# Generate example data with a Simpson's Paradox
n <- 1000
data <- tibble(
  age_group = sample(c("Young", "Middle", "Old"), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),
  education_years = case_when(
    age_group == "Young" ~ rnorm(n, mean = 10, sd = 1),
    age_group == "Middle" ~ rnorm(n, mean = 13, sd = 1),
    age_group == "Old" ~ rnorm(n, mean = 16, sd = 1)
  ),
  income = case_when(
    age_group == "Young" ~ 70000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),
    age_group == "Middle" ~ 50000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),
    age_group == "Old" ~ 30000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000)
  )
)

# Basic data summary
summary(data)

# Correlation analysis
cor(data %>% select(education_years, income))

# Overall trend (Simpson's Paradox)
overall_plot <- ggplot(data, aes(x = education_years, y = income)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Overall Relationship between Education and Income",
       subtitle = "Simpson's Paradox: Appears negative",
       x = "Years of Education", y = "Income") +
  theme_minimal()

# Trend by age group (Resolving Simpson's Paradox)
grouped_plot <- ggplot(data, aes(x = education_years, y = income, color = age_group)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Relationship between Education and Income by Age Group",
       subtitle = "Resolving Simpson's Paradox: Positive relationship within groups",
       x = "Years of Education", y = "Income") +
  theme_minimal()

# Statistical analysis
model_overall <- lm(income ~ education_years, data = data)
model_by_age <- lm(income ~ education_years + age_group, data = data)

# Print results
print(overall_plot)
print(grouped_plot)
print(summary(model_overall))
print(summary(model_by_age))

# Calculate and print correlations
overall_cor <- cor(data$education_years, data$income)
group_cors <- data %>%
  group_by(age_group) %>%
  summarize(correlation = cor(education_years, income))

print("Overall correlation:")
print(overall_cor)
print("Correlations by age group:")
print(group_cors)
```

Ten przykład demonstruje podstawowe operacje na danych, statystyki opisowe i wizualizację danych przy użyciu R.

## Wnioskowanie przyczynowe a badania obserwacyjne

W naukach społecznych i nie tylko, zrozumienie relacji między zmiennymi jest kluczowe. Dwa główne podejścia to wnioskowanie przyczynowe i badania obserwacyjne, każde z własnymi mocnymi stronami i ograniczeniami.

::: panel-tabset
### Wnioskowanie przyczynowe

-   Dąży do ustalenia związków przyczynowo-skutkowych
-   Często obejmuje plany eksperymentalne lub zaawansowane techniki statystyczne
-   Stara się odpowiedzieć na pytania "Co by było, gdyby?" i określić wpływ interwencji
-   Przykłady: Randomizowane badania kontrolowane, projekty quasi-eksperymentalne, zmienne instrumentalne

### Badania obserwacyjne

-   Badają relacje między zmiennymi bez bezpośredniej interwencji
-   Opierają się na danych zebranych w naturalnych warunkach lub z istniejących zbiorów danych
-   Mogą identyfikować korelacje i wzorce, ale mają trudności z ustaleniem przyczynowości
-   Przykłady: Badania kohortowe, badania kliniczno-kontrolne, przekrojowe badania ankietowe

### Kluczowe rozróżnienie: Korelacja vs. Przyczynowość
:::

::: callout-important
## Pamiętaj: Korelacja nie implikuje przyczynowości

Fundamentalna zasada w badaniach głosi, że korelacja między dwiema zmiennymi niekoniecznie implikuje związek przyczynowy. Ta koncepcja jest kluczowa przy interpretacji wyników badań obserwacyjnych.

-   **Korelacja**: Mierzy siłę i kierunek związku między zmiennymi
-   **Przyczynowość**: Wskazuje, że zmiany w jednej zmiennej bezpośrednio powodują zmiany w drugiej

Chociaż silne korelacje mogą sugerować potencjalne związki przyczynowe, do ustalenia przyczynowości wymagane są dodatkowe dowody i rygorystyczne metody.
:::

::: panel-tabset
### Wyzwania w ustalaniu przyczynowości

-   Zmienne zakłócające: Niezmierzone czynniki wpływające zarówno na domniemaną przyczynę, jak i skutek
-   Odwrotna przyczynowość: Domniemany skutek może w rzeczywistości powodować domniemaną przyczynę
-   Błąd selekcji: Nielosowy dobór uczestników do grup badawczych

### Metody wzmacniania twierdzeń przyczynowych

1.  Randomizowane badania kontrolowane (gdy są etyczne i wykonalne)
2.  Naturalne eksperymenty lub projekty quasi-eksperymentalne
3.  Dopasowanie według propensity score
4.  Analiza różnicy w różnicach
5.  Podejścia oparte na zmiennych instrumentalnych
6.  Skierowane grafy acykliczne (DAG) do wizualizacji relacji przyczynowych

### Znaczenie w naukach społecznych

Zrozumienie różnicy między wnioskowaniem przyczynowym a badaniami obserwacyjnymi jest kluczowe w naukach społecznych, gdzie względy etyczne często ograniczają manipulacje eksperymentalne. Badacze muszą starannie projektować badania i interpretować wyniki, aby uniknąć wprowadzających w błąd wniosków dotyczących przyczynowości.
:::

## Modele w Nauce: Od Deterministycznych do Stochastycznych

Modele są niezbędnymi narzędziami w badaniach naukowych, pomagając naukowcom reprezentować, rozumieć i przewidywać złożone zjawiska. Ta sekcja omawia główne typy modeli stosowanych w nauce, wraz z przykładami ich zastosowań. Należy pamiętać, że te kategorie często się nakładają, a wiele modeli naukowych łączy w sobie różne aspekty.

### Modele Matematyczne

Modele matematyczne wykorzystują równania i koncepcje matematyczne do opisywania i analizowania systemów lub zjawisk. Można je podzielić na kilka podkategorii, choć należy pamiętać, że niektóre złożone modele mogą zawierać elementy z wielu kategorii:

#### a. Modele Deterministyczne

Modele deterministyczne dostarczają precyzyjnych przewidywań na podstawie zestawu zmiennych, bez uwzględniania losowości na poziomie makroskopowym.

**Przykład:** Prawa ruchu Newtona, które mogą precyzyjnie przewidzieć ruch obiektów pod wpływem znanych sił w mechanice klasycznej.

#### b. Modele Stochastyczne

Modele stochastyczne uwzględniają losowość i prawdopodobieństwo. Jednak kluczowe jest rozróżnienie dwóch fundamentalnie różnych typów modeli stochastycznych:

##### i. Klasyczne Modele Stochastyczne

Te modele zajmują się losowością wynikającą z niepełnej informacji lub złożonych interakcji w systemach klasycznych. Podstawowy system jest deterministyczny, ale praktyczne ograniczenia w pomiarach lub obliczeniach prowadzą do użycia opisów probabilistycznych.

**Przykład:** Modele regresji w statystyce, gdzie losowość reprezentuje niewyjaśnioną zmienność lub błąd pomiaru:

$$y = β_0 + β_1x + ε$$

Gdzie:

-   $y$ to zmienna zależna (np. wielkość popytu na dobro)
-   $x$ to zmienna niezależna (np. cena lub dochód konsumenta)
-   $β_0$ i $β_1$ to parametry
-   $ε$ to składnik błędu, reprezentujący niewyjaśnioną zmienność

##### ii. Kwantowe Modele Stochastyczne

Te modele zajmują się fundamentalną, nieredukowalną losowością nieodłącznie związaną z systemami mechaniki kwantowej. Ta losowość nie wynika z braku informacji, ale jest podstawową cechą rzeczywistości kwantowej.

**Przykład:** Model Standardowy w fizyce cząstek elementarnych, który opisuje interakcje cząstek za pomocą kwantowej teorii pola. Na przykład, rozpad cząstki jest z natury probabilistyczny:

$$P(t) = e^{-t/τ}$$

Gdzie:

-   $P(t)$ to prawdopodobieństwo, że cząstka nie rozpadła się po czasie t
-   $τ$ to średni czas życia cząstki

#### c. Modele Symulacji Komputerowych

Symulacje komputerowe wykorzystują algorytmy i metody obliczeniowe oparte na modelach matematycznych do symulowania złożonych systemów i przewidywania ich zachowania w czasie. Mogą być deterministyczne lub stochastyczne.

**Przykład:** Modele klimatyczne symulujące system klimatyczny Ziemi, uwzględniające czynniki takie jak skład atmosfery, prądy oceaniczne i promieniowanie słoneczne do prognozowania przyszłych scenariuszy klimatycznych.

### Modele Koncepcyjne

Modele koncepcyjne to abstrakcyjne reprezentacje systemów lub procesów, często wykorzystujące diagramy lub schematy blokowe do ilustrowania relacji między komponentami.

**Przykład:** Model obiegu wody w naukach o Ziemi, który ilustruje ciągły ruch wody w obrębie Ziemi i atmosfery poprzez procesy takie jak parowanie, opady i spływ powierzchniowy.

### Modele Fizyczne

Modele fizyczne to namacalne reprezentacje obiektów lub systemów, często w formie pomniejszonej lub uproszczonej wersji rzeczywistego obiektu.

**Przykład:** Modele tunelu aerodynamicznego w badaniach aerodynamiki, używane do badania efektów przepływu powietrza wokół obiektów stałych i optymalizacji projektów samolotów, pojazdów lub budynków.

### Modele Teoretyczne

Modele teoretyczne to abstrakcyjne ramy oparte na fundamentalnych zasadach i hipotezach, często używane do wyjaśniania obserwowanych zjawisk lub przewidywania nowych. Te modele często wykorzystują równania matematyczne i mogą być deterministyczne lub stochastyczne.

**Przykład:** Teoria ewolucji poprzez dobór naturalny, która dostarcza ram do zrozumienia różnorodności i adaptacji form życia w czasie.

### Podsumowanie

Te różne formy modeli odgrywają kluczową rolę w badaniach naukowych, każda oferując unikalne zalety dla zrozumienia i przewidywania zjawisk naturalnych. Naukowcy często używają wielu typów modeli jednocześnie, aby uzyskać kompleksowy wgląd w złożone systemy i procesy.

Ważne jest, aby zdawać sobie sprawę, że te kategorie nie są wzajemnie wykluczające i często się nakładają:

1.  Modele matematyczne stanowią podstawę dla wielu innych typów modeli, w tym symulacji komputerowych i niektórych modeli teoretycznych.
2.  Modele symulacji komputerowych są zasadniczo modelami matematycznymi implementowanymi za pomocą metod obliczeniowych i mogą być deterministyczne lub stochastyczne.
3.  Modele teoretyczne często wykorzystują sformułowania matematyczne i mogą być implementowane jako symulacje komputerowe.
4.  Modele fizyczne mogą być projektowane na podstawie modeli matematycznych i mogą być używane do walidacji symulacji komputerowych.

Wybór typu modelu często zależy od konkretnego pytania badawczego, natury badanego systemu, dostępnych danych oraz zasobów obliczeniowych. W miarę postępu nauki granice między tymi typami modeli coraz bardziej się zacierają, prowadząc do coraz bardziej wyrafinowanych i interdyscyplinarnych podejść do modelowania złożonych zjawisk.

Kluczowe jest rozróżnienie różnych typów modeli stochastycznych. Klasyczne modele stochastyczne, takie jak te używane w analizie regresji, zajmują się losowością wynikającą z niepełnej informacji lub złożonych interakcji w systemach, które są zasadniczo deterministyczne. Z drugiej strony, kwantowe modele stochastyczne, jak te w fizyce cząstek, zajmują się fundamentalną, nieredukowalną losowością nieodłącznie związaną z systemami mechaniki kwantowej. To rozróżnienie odzwierciedla głębokie różnice między klasycznymi a kwantowymi paradygmatami w fizyce i podkreśla różnorodne sposoby, w jakie prawdopodobieństwo jest wykorzystywane w modelowaniu naukowym.

## Zrozumienie Pozornych Korelacji, Zmiennych Zakłócających i Kolizyjnych (\*)

W tej sekcji zbadamy trzy ważne pojęcia w analizie statystycznej: pozorne korelacje, zmienne zakłócające i zmienne kolizyjne. Zrozumienie tych pojęć jest kluczowe dla uniknięcia błędnej interpretacji danych i wyciągania nieprawidłowych wniosków z analiz statystycznych.

Zacznijmy od załadowania niezbędnych bibliotek:

```{r}
#| label: setup
#| message: false

library(tidyverse)
library(dagitty)
library(ggdag)
set.seed(123) # dla powtarzalności
```

### Pozorne Korelacje

Pozorne korelacje to związki między zmiennymi, które wydają się przyczynowe, ale w rzeczywistości są przypadkowe lub spowodowane przez niewidoczny trzeci czynnik.

#### Przykład: Sprzedaż lodów a przypadki utonięć

Stwórzmy zbiór danych, który pokazuje pozorną korelację między sprzedażą lodów a przypadkami utonięć:

```{r}
#| label: spurious-data

n <- 100
dane_pozorne <- tibble(
  temperatura = rnorm(n, mean = 25, sd = 5),
  sprzedaz_lodow = 100 + 5 * temperatura + rnorm(n, sd = 10),
  przypadki_utoniec = 1 + 0.5 * temperatura + rnorm(n, sd = 2)
)

ggplot(dane_pozorne, aes(x = sprzedaz_lodow, y = przypadki_utoniec)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Pozorna Korelacja: Sprzedaż Lodów vs Przypadki Utonięć",
       x = "Sprzedaż Lodów", y = "Przypadki Utonięć")
```

Ten wykres pokazuje pozytywną korelację między sprzedażą lodów a przypadkami utonięć. Jednak ta relacja jest pozorna. Prawdziwą przyczyną obu zjawisk jest temperatura:

```{r}
#| label: spurious-explanation

ggplot(dane_pozorne, aes(x = temperatura)) +
  geom_point(aes(y = sprzedaz_lodow), color = "blue") +
  geom_point(aes(y = przypadki_utoniec * 10), color = "red") +
  geom_smooth(aes(y = sprzedaz_lodow), method = "lm", se = FALSE, color = "blue") +
  geom_smooth(aes(y = przypadki_utoniec * 10), method = "lm", se = FALSE, color = "red") +
  scale_y_continuous(
    name = "Sprzedaż Lodów",
    sec.axis = sec_axis(~./10, name = "Przypadki Utonięć")
  ) +
  labs(title = "Temperatura jako Wspólna Przyczyna",
       x = "Temperatura")
```

### Zmienne Zakłócające

Zmienna zakłócająca to zmienna, która wpływa zarówno na zmienną zależną, jak i niezależną, powodując pozorny związek.

#### Przykład: Edukacja, Dochód i Wiek

Stwórzmy zbiór danych, w którym wiek zakłóca relację między edukacją a dochodem:

```{r}
#| label: confounder-data

library(tidyverse)
library(viridis)

n <- 1000
confounder_data <- tibble(
  age = runif(n, 25, 65),
  education = round(10 + 0.1 * age + rnorm(n, sd = 2)),
  income = 20000 + 1000 * education + 500 * age + rnorm(n, sd = 5000)
)

# Without controlling for age
model_naive <- lm(income ~ education, data = confounder_data)
# Controlling for age
model_adjusted <- lm(income ~ education + age, data = confounder_data)

# Create age groups for visualization
confounder_data <- confounder_data %>%
  mutate(age_group = cut(age, breaks = 3, labels = c("Young", "Middle", "Old")))

# Visualize
ggplot(confounder_data, aes(x = education, y = income)) +
  geom_point(aes(color = age), alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red", linewidth = 1.2) +
  geom_smooth(aes(group = age_group, color = as.numeric(age_group)), 
              method = "lm", se = FALSE, linewidth = 1) +
  scale_color_viridis_c(name = "Age", 
                        breaks = c(30, 45, 60), 
                        labels = c("Young", "Middle", "Old")) +
  labs(title = "Education vs Income, Confounded by Age",
       x = "Years of Education", y = "Income") +
  theme_minimal()
```

Porównajmy współczynniki:

```{r}
#| label: confounder-models

summary(model_naive)$coefficients["education", "Estimate"]
summary(model_adjusted)$coefficients["education", "Estimate"]
```

Efekt edukacji na dochód jest przeszacowany, gdy nie kontrolujemy wieku.

### Zmienne Kolizyjne

Zmienna kolizyjna to zmienna, na którą wpływają zarówno zmienna niezależna, jak i zmienna zależna. Kontrolowanie zmiennej kolizyjnej może wprowadzić pozorną korelację.

#### Przykład: Satysfakcja z pracy, Wynagrodzenie i Równowaga między pracą a życiem prywatnym

Stwórzmy zbiór danych, w którym równowaga między pracą a życiem prywatnym jest zmienną kolizyjną między satysfakcją z pracy a wynagrodzeniem:

```{r}
#| label: collider-data

n <- 1000
dane_kolizyjne <- tibble(
  satysfakcja_z_pracy = rnorm(n),
  wynagrodzenie = rnorm(n),
  rownowaga_praca_zycie = -0.5 * satysfakcja_z_pracy - 0.5 * wynagrodzenie + rnorm(n, sd = 0.5)
)

# Bez kontrolowania równowagi praca-życie
model_poprawny <- lm(wynagrodzenie ~ satysfakcja_z_pracy, data = dane_kolizyjne)

# Błędne kontrolowanie równowagi praca-życie
model_kolizyjny <- lm(wynagrodzenie ~ satysfakcja_z_pracy + rownowaga_praca_zycie, data = dane_kolizyjne)

# Wizualizacja
ggplot(dane_kolizyjne, aes(x = satysfakcja_z_pracy, y = wynagrodzenie, color = rownowaga_praca_zycie)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  scale_color_viridis_c() +
  labs(title = "Satysfakcja z Pracy vs Wynagrodzenie, Równowaga Praca-Życie jako Zmienna Kolizyjna",
       x = "Satysfakcja z Pracy", y = "Wynagrodzenie")
```

Porównajmy współczynniki:

```{r}
#| label: collider-models

summary(model_poprawny)$coefficients["satysfakcja_z_pracy", "Estimate"]
summary(model_kolizyjny)$coefficients["satysfakcja_z_pracy", "Estimate"]
```

Kontrolowanie zmiennej kolizyjnej (równowaga praca-życie) wprowadza pozorną korelację między satysfakcją z pracy a wynagrodzeniem.

### Podsumowanie

Zrozumienie pozornych korelacji, zmiennych zakłócających i kolizyjnych jest kluczowe dla prawidłowej analizy statystycznej i wnioskowania przyczynowego. Zawsze rozważ podstawową strukturę przyczynową swoich danych i bądź ostrożny w kwestii tego, które zmienne kontrolujesz w swoich analizach.

### Dalsza Lektura

-   Pearl, J. (2009). Causality: Models, Reasoning, and Inference. Cambridge University Press.
-   Hernán, M. A., & Robins, J. M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.

## Etyczne Aspekty w Analizie Danych Nauk Społecznych

Etyka odgrywa kluczową rolę w badaniach nauk społecznych:

1.  **Prywatność i Zgoda**: Zapewnienie prywatności uczestników i świadomej zgody
2.  **Ochrona Danych**: Bezpieczne przechowywanie i zarządzanie wrażliwymi danymi osobowymi
3.  **Błędy i Reprezentacja**: Adresowanie błędów próbkowania i zapewnienie różnorodnej reprezentacji
4.  **Przejrzystość**: Jasne komunikowanie metod badawczych i ograniczeń
5.  **Wpływ Społeczny**: Rozważanie potencjalnych społecznych implikacji wyników badań

::: callout-warning
Naukowcy społeczni muszą starannie rozważyć etyczne implikacje swoich praktyk zbierania, analizy i rozpowszechniania danych.
:::

### Kluczowe Wnioski

1.  Nauka o danych w naukach społecznych bazuje na tradycyjnych metodach statystycznych, włączając nowe technologie do analizy złożonych zjawisk społecznych.
2.  Zrozumienie koncepcji takich jak populacja, próba i procesy generowania danych jest kluczowe dla prawidłowych badań w naukach społecznych.
3.  Proces nauki o danych w badaniach społecznych obejmuje wiele etapów, od etycznego zbierania danych po komunikację wniosków.
4.  R jest potężnym narzędziem do analizy danych w naukach społecznych, oferującym szeroki zakres możliwości.
5.  Aspekty etyczne powinny być na pierwszym planie każdego projektu związanego z danymi w naukach społecznych.

## Appendix A: Losowość Klasyczna a Kwantowa: Zrozumienie Fundamentalnych Różnic

Aby zrozumieć, jak losowość w mechanice kwantowej różni się od losowości reprezentowanej przez składnik błędu w modelach regresji, musimy przeanalizować ich pochodzenie, naturę i implikacje.

### Pochodzenie Losowości

#### Losowość Klasyczna (Modele Regresji)

-   **Źródło**: Niekompletna informacja lub złożone interakcje w systemie, który w zasadzie jest deterministyczny.
-   **Natura**: Niepewność epistemiczna (wynikająca z braku wiedzy).
-   **Przykład**: W modelu regresji, $y = β_0 + β_1x + ε$, składnik błędu ε reprezentuje niewyjaśnioną zmienność.

#### Losowość Kwantowa

-   **Źródło**: Fundamentalna właściwość systemów kwantowych.
-   **Natura**: Niepewność ontyczna (nieodłączna cecha systemu, nie wynika z braku wiedzy).
-   **Przykład**: Dokładny moment rozpadu atomu radioaktywnego nie może być przewidziany, można określić jedynie jego prawdopodobieństwo.

### Implikacje Filozoficzne

#### Losowość Klasyczna

-   **Determinizm**: Podstawowa rzeczywistość jest deterministyczna; losowość odzwierciedla naszą niewiedzę.
-   **Ukryte Zmienne**: W zasadzie, gdybyśmy mieli pełną informację, moglibyśmy dokładnie przewidzieć wyniki.

#### Losowość Kwantowa

-   **Indeterminizm**: Losowość jest fundamentalną cechą rzeczywistości, nie tylko naszego jej opisu.
-   **Brak Ukrytych Zmiennych**: Nawet przy pełnej informacji o systemie kwantowym, niektóre wyniki pozostają nieprzewidywalne (co sugeruje twierdzenie Bella).

### Ujęcie Matematyczne

#### Losowość Klasyczna

-   **Teoria Prawdopodobieństwa**: Oparta na klasycznej teorii prawdopodobieństwa.
-   **Rozkład**: Często zakłada się znane rozkłady (np. rozkład normalny w wielu modelach regresji).
-   **Centralne Twierdzenie Graniczne**: Stosuje się do dużych prób zmiennych losowych.

#### Losowość Kwantowa

-   **Prawdopodobieństwo Kwantowe**: Oparte na matematycznych podstawach mechaniki kwantowej.
-   **Funkcja Falowa**: Opisuje stan kwantowy i jego ewolucję.
-   **Reguła Borna**: Określa prawdopodobieństwa wyników pomiarów na podstawie funkcji falowej.

### Przewidywalność i Kontrola

#### Losowość Klasyczna

-   **Redukowalna**: W zasadzie można ją zmniejszyć, zbierając więcej danych lub poprawiając dokładność pomiarów.
-   **Kontrolowalna**: Błędy systematyczne można zidentyfikować i skorygować.

#### Losowość Kwantowa

-   **Nieredukowalna**: Nie można jej wyeliminować nawet przy idealnych pomiarach.
-   **Fundamentalnie Niekontrolowalna**: Sam akt pomiaru wpływa na system (problem pomiaru).

### Praktyczne Implikacje

#### Losowość Klasyczna

-   **Redukcja Błędów**: Koncentracja na udoskonalaniu technik pomiarowych i zbierania danych.
-   **Udoskonalanie Modelu**: Dążenie do wyjaśnienia większej wariancji i zmniejszenia składnika błędu.

#### Losowość Kwantowa

-   **Nieodłączne Ograniczenie**: Akceptacja fundamentalnych granic przewidywalności.
-   **Przewidywania Probabilistyczne**: Skupienie na dokładnych rozkładach prawdopodobieństwa zamiast na dokładnych wynikach.

### Przykłady Pomagające Zrozumieć Różnicę

#### Przykład Losowości Klasycznej

Wyobraź sobie rzut monetą. Fizyka klasyczna mówi, że wynik jest zdeterminowany przez warunki początkowe (przyłożona siła, opór powietrza itp.). "Losowość" wynika z naszej niezdolności do precyzyjnego zmierzenia i uwzględnienia wszystkich tych czynników.

#### Przykład Losowości Kwantowej

W eksperymencie z podwójną szczeliną pojedyncze cząstki wykazują wzory interferencyjne, jakby przechodziły przez obie szczeliny jednocześnie. Dokładna ścieżka każdej pojedynczej cząstki jest fundamentalnie nieokreślona do momentu pomiaru, a tej nieokreśloności nie można rozwiązać przez bardziej precyzyjne pomiary.

### Podsumowanie

Chociaż oba rodzaje losowości prowadzą do probabilistycznych przewidywań, ich fundamentalne natury są zupełnie różne:

-   Losowość klasyczna w modelach regresji jest odzwierciedleniem naszej niepełnej wiedzy lub ograniczeń pomiarowych w systemie, który w zasadzie jest deterministyczny.
-   Losowość kwantowa jest fundamentalną właściwością systemów kwantowych, reprezentującą nieodłączną nieokreśloność w naturze, która utrzymuje się nawet przy doskonałej wiedzy i pomiarze.

Zrozumienie tych różnic jest kluczowe dla prawidłowej interpretacji i stosowania modeli statystycznych w różnych kontekstach naukowych, od nauk społecznych wykorzystujących analizę regresji po eksperymenty z fizyki kwantowej.

## Appendix B: Duże Modele Językowe - Zrozumienie Ich Stochastycznej Natury

Duże Modele Językowe (LLM), takie jak GPT-3, BERT i Claude, zrewolucjonizowały przetwarzanie języka naturalnego, ale mogą popełniać zagadkowe błędy, szczególnie w zadaniach matematycznych. Ten dodatek wyjaśnia funkcjonowanie LLM, ich stochastyczną naturę i porównuje je z klasycznymi modelami statystycznymi.

### Podstawy LLM i Ich Stochastyczna Natura

LLM są trenowane na ogromnych zbiorach danych tekstowych, aby przewidywać rozkład prawdopodobieństwa następnego tokenu w sekwencji. Wykorzystują architektury transformerowe do przetwarzania i generowania tekstu. Kluczowe aspekty ich stochastycznej natury obejmują:

1.  Probabilistyczny wybór tokenów: LLM wybierają każde słowo na podstawie obliczonych prawdopodobieństw, a nie stałych reguł.
2.  Losowość kontrolowana temperaturą: Parametr "temperatury" dostosowuje losowość wyborów, równoważąc kreatywność i spójność.
3.  Niedeterministyczne wyniki: Te same dane wejściowe mogą prowadzić do różnych wyników w oddzielnych uruchomieniach.
4.  Kontekstowa niejednoznaczność: LLM interpretują kontekst probabilistycznie, co czasami prowadzi do nieporozumień.

### Porównanie z Klasycznymi Modelami Statystycznymi

Aby lepiej zrozumieć LLM, porównajmy je z regresją Najmniejszych Kwadratów (OLS):

| Aspekt | Regresja OLS | Duże Modele Językowe |
|----|----|----|
| Podstawowa funkcja | Przewiduje ciągłe wyniki na podstawie zmiennych wejściowych | Przewiduje rozkład prawdopodobieństwa następnego tokenu na podstawie poprzednich tokenów |
| Wejście-Wyjście | Zmienne ciągłe, relacje liniowe | Dyskretne tokeny, relacje nieliniowe |
| Typ predykcji | Predykcje punktowe z przedziałami ufności | Rozkłady prawdopodobieństwa dla możliwych tokenów |
| Złożoność modelu | Niewiele parametrów | Miliardy parametrów |
| Interpretowalność | Jasne interpretacje współczynników | Largely nieprzejrzyste działanie wewnętrzne |
| Obsługa szumu | Zakłada losowy szum w zmiennej wynikowej | Radzi sobie ze zmiennością języka naturalnego |
| Ekstrapolacja | Mniej wiarygodna poza zakresem treningu | Mniej wiarygodna dla nieznanych tematów |

Oba modele dążą do nauczenia się mapowania wejścia-wyjścia na podstawie wzorców w danych treningowych.

### Implikacje dla Zadań Matematycznych

Stochastyczna natura LLM wpływa na operacje matematyczne:

1.  Zmienne wyniki dla powtarzanych obliczeń: Każda próba może dać inny wynik ze względu na probabilistyczny wybór tokenów.
2.  Pewność nie gwarantuje poprawności: Wysoka pewność modelu może wystąpić nawet dla niepoprawnych odpowiedzi.
3.  Aproksymacja zamiast dokładnych obliczeń: LLM dopasowują wzorce zamiast wykonywać precyzyjne obliczenia.

Ograniczenia w zadaniach matematycznych wynikają z:

-   Niedopasowania celu treningu: LLM są trenowane do przewidywania języka, nie dokładności matematycznej.
-   Braku jawnego rozumowania matematycznego: Nie mają wbudowanych reguł czy operacji matematycznych.
-   Braku pamięci roboczej: LLM nie mogą niezawodnie przechowywać i manipulować wynikami pośrednimi.
-   Ograniczonego okna kontekstowego: Mogą tracić istotne informacje w długich problemach.
-   Ograniczeń danych treningowych: Niedoreprezentowanie pewnych koncepcji matematycznych może prowadzić do słabych wyników.
-   Braku kontroli spójności: LLM nie weryfikują logicznej spójności swoich wyników.

### Najlepsze Praktyki i Wnioski

Przy korzystaniu z LLM do zadań matematycznych:

1.  Skup się na wyjaśnieniach koncepcyjnych, nie na dokładnych obliczeniach: LLM doskonale wyjaśniają koncepcje, ale mogą zawodzić w dokładnych obliczeniach.
2.  Weryfikuj wyniki dedykowanym oprogramowaniem: Zawsze sprawdzaj obliczenia LLM odpowiednimi narzędziami matematycznymi.
3.  Rozbijaj złożone problemy: Podział zadań na mniejsze kroki może poprawić wydajność LLM.
4.  Bądź świadomy efektów przeformułowania: Różne sformułowania tego samego problemu mogą dawać różne wyniki.
5.  Używaj jako narzędzi wspomagających, nie zamienników dla ekspertyzy: LLM powinny uzupełniać, a nie zastępować wiedzę matematyczną.

Zrozumienie probabilistycznej natury LLM pomaga wykorzystać ich mocne strony w zadaniach językowych, jednocześnie uznając ich ograniczenia w dziedzinach wymagających deterministycznej precyzji, takich jak matematyka.

## Appendix C: Modele Deterministyczne a Modele Stochastyczne (\*)

### Modele Deterministyczne

Modele deterministyczne to te, w których wynik jest w pełni określony przez wartości parametrów i warunki początkowe. Modele te są często używane w fizyce i inżynierii.

### Przykład: Ruch Jednostajnie Przyspieszony

Klasycznym przykładem modelu deterministycznego jest ruch jednostajnie przyspieszony, opisany równaniem:

$$x(t) = x_0 + v_0t + \frac{1}{2}at^2$$

Gdzie:

-   $x(t)$ to położenie w czasie $t$
-   $x_0$ to położenie początkowe
-   $v_0$ to prędkość początkowa
-   $a$ to przyspieszenie
-   $t$ to czas

Zasymulujmy to w R:

```{r}
# Ruch jednostajnie przyspieszony
symuluj_ruch_przyspieszony <- function(x0, v0, a, t) {
  x0 + v0 * t + 0.5 * a * t^2
}

# Generowanie danych
t <- seq(0, 10, by = 0.1)
x <- symuluj_ruch_przyspieszony(x0 = 0, v0 = 2, a = 1, t = t)

# Wykres
plot(t, x, type = "l", xlab = "Czas", ylab = "Położenie", 
     main = "Ruch Jednostajnie Przyspieszony")
```

Ten kod wygeneruje wykres ruchu jednostajnie przyspieszonego, który jest intuicyjnym przykładem z dynamiki Newtona. W tym przypadku obiekt zaczyna ruch z początkową prędkością i przyspiesza jednostajnie, co prowadzi do parabolicznej trajektorii na wykresie położenia w funkcji czasu.

### Modele Stochastyczne w Naukach Społecznych

Modele stochastyczne uwzględniają losowość i są często używane w naukach społecznych, gdzie istnieje nieodłączna niepewność w badanych systemach.

### Przykład: Regresja Metodą Najmniejszych Kwadratów (OLS)

OLS to podstawowy model stochastyczny w naukach społecznych. Jest reprezentowany jako:

$$Y = \beta_0 + \beta_1X + \epsilon$$

Gdzie:

-   $Y$ to zmienna zależna
-   $X$ to zmienna niezależna
-   $\beta_0$ i $\beta_1$ to parametry
-   $\epsilon$ to składnik błędu (komponent stochastyczny)

Zademonstrujmy OLS w R:

```{r}
# Generowanie przykładowych danych
set.seed(123)
X <- rnorm(100)
Y <- 2 + 3*X + rnorm(100, sd = 0.5)

# Dopasowanie modelu OLS
model <- lm(Y ~ X)

# Podsumowanie modelu
summary(model)

# Wykres
plot(X, Y, main = "Regresja OLS")
abline(model, col = "red")
```

To dopasuje model OLS do symulowanych danych i wykreśli wyniki.

![Retrieved from: https://scientistcafe.com/ids/vbtradeoff](stat_imgs/ModelError.png)

### Zaawansowane Modele Stochastyczne: Duże Modele Językowe

Duże Modele Językowe (LLM), takie jak GPT-3, to złożone modele stochastyczne używane w przetwarzaniu języka naturalnego. Chociaż nie możemy zaimplementować pełnego LLM w tym tutorialu, możemy omówić jego zasady.

LLM opierają się na architekturze transformatora i wykorzystują mechanizmy samouwagi. Są trenowane na ogromnych ilościach danych tekstowych i uczą się przewidywać następny token w sekwencji.

Rdzeń LLM można postrzegać jako warunkowy rozkład prawdopodobieństwa:

$$P(x_t | x_{<t}, \theta)$$

Gdzie:

-   $x_t$ to aktualny token
-   $x_{<t}$ reprezentuje wszystkie poprzednie tokeny
-   $\theta$ to parametry modelu

::: callout-note
Tokeny w Dużych Modelach Językowych (LLM) to podstawowe jednostki tekstu, które model przetwarza. Można je postrzegać jako części słów lub znaki interpunkcyjne. Oto kluczowe informacje o tokenach:

Definicja: Tokeny to najmniejsze jednostki tekstu, które LLM przetwarza. Mogą to być całe słowa, części słów, a nawet pojedyncze znaki lub znaki interpunkcyjne. Tokenizacja: Proces dzielenia tekstu na tokeny nazywa się tokenizacją. LLM używają specyficznych algorytmów do wykonania tego zadania. Przykłady:

Słowo "kot" może być pojedynczym tokenem. Dłuższe słowo jak "zrozumienie" może być podzielone na wiele tokenów, np. "zrozum" i "ienie". Znaki interpunkcyjne jak "." czy "?" są często oddzielnymi tokenami. Powszechne przedrostki lub przyrostki mogą być własnymi tokenami.

Słownictwo: LLM mają ustalone słownictwo tokenów, które rozpoznają. To słownictwo zazwyczaj obejmuje od dziesiątek tysięcy do setek tysięcy tokenów. Znaczenie: Sposób tokenizacji tekstu może wpływać na to, jak model rozumie i generuje język. Jest to szczególnie ważne przy obsłudze różnych języków, rzadkich słów lub specjalistycznego słownictwa. Kontekst: W równaniu dla LLM: $$P(x_t | x_{<t}, \theta)$$ Gdzie:

$x_t$ reprezentuje bieżący token $x_{<t}$ reprezentuje wszystkie poprzednie tokeny w sekwencji $\theta$ reprezentuje parametry modelu
:::

W przeciwieństwie do modeli deterministycznych, LLM produkują różne wyniki nawet dla tego samego wejścia ze względu na ich stochastyczną naturę.

### Podsumowanie

Każdy rodzaj modelu ma swoje miejsce w nauce, w zależności od badanego systemu i poziomu niepewności.

Pamiętaj, że wybór między modelami deterministycznymi a stochastycznymi często zależy od natury badanego systemu i pytań, na które próbujesz odpowiedzieć. Modele deterministyczne są świetne dla systemów o dobrze zrozumiałej mechanice, podczas gdy modele stochastyczne sprawdzają się przy radzeniu sobie z nieodłączną losowością lub złożonymi, nie w pełni zrozumiałymi systemami.

## Appendix D: Wprowadzenie do R, RStudio i tidyverse

R to potężny język programowania i środowisko do obliczeń statystycznych i grafiki. Jest szeroko stosowany w środowisku akademickim, szczególnie w naukach społecznych, do analizy danych i wizualizacji.

#### Kluczowe cechy R:

-   Otwarty kod źródłowy i darmowy
-   Rozbudowany ekosystem pakietów
-   Silne wsparcie społeczności
-   Doskonały do analizy statystycznej i wizualizacji danych

### Pierwsze kroki z RStudio

RStudio to zintegrowane środowisko programistyczne (IDE) dla R, które ułatwia pracę z R.

#### Instalacja R i RStudio

1.  Pobierz i zainstaluj R ze strony [CRAN](https://cran.r-project.org/)
2.  Pobierz i zainstaluj RStudio ze [strony RStudio](https://www.rstudio.com/products/rstudio/download/)

#### Interfejs RStudio

RStudio ma cztery główne panele:

1.  **Edytor źródłowy**: Gdzie piszesz i edytujesz skrypty R
2.  **Konsola**: Gdzie możesz wpisywać polecenia R i widzieć wyniki
3.  **Środowisko/Historia**: Pokazuje wszystkie obiekty w twoim obszarze roboczym i historię poleceń
4.  **Pliki/Wykresy/Pakiety/Pomoc**: Wielofunkcyjny panel do zarządzania plikami, przeglądania wykresów, zarządzania pakietami i dostępu do pomocy

#### Podstawowe funkcje RStudio

-   Tworzenie nowego skryptu R: Plik \> Nowy plik \> Skrypt R
-   Uruchamianie kodu: Zaznacz kod i naciśnij Ctrl+Enter (Cmd+Enter na Macu)
-   Instalowanie pakietów: Narzędzia \> Instaluj pakiety
-   Uzyskiwanie pomocy: Wpisz `?nazwa_funkcji` w konsoli

### Podstawy R

#### Typy danych w R

```{r}
# Numeryczny
x <- 10.5
class(x)

# Całkowity
y <- 1L
class(y)

# Znakowy
imie <- "Alicja"
class(imie)

# Logiczny
jest_studentem <- TRUE
class(jest_studentem)
```

#### Struktury danych

##### Wektory

```{r}
# Tworzenie wektora
liczby <- c(1, 2, 3, 4, 5)
owoce <- c("jabłko", "banan", "wiśnia")

# Operacje na wektorach
liczby + 2
liczby * 2
mean(liczby)
length(owoce)
```

##### Macierze

```{r}
# Tworzenie macierzy
m <- matrix(1:6, nrow = 2, ncol = 3)
print(m)

# Operacje na macierzach
t(m)  # transpozycja
m * 2  # mnożenie skalarne
```

##### Ramki danych

```{r}
# Tworzenie ramki danych
df <- data.frame(
  imie = c("Alicja", "Bartek", "Celina"),
  wiek = c(25, 30, 35),
  student = c(TRUE, FALSE, TRUE)
)
print(df)

# Dostęp do elementów ramki danych
df$imie
df[1, 2]
df[df$wiek > 25, ]
```

#### Funkcje

```{r}
# Definiowanie funkcji
powitaj <- function(imie) {
  paste("Cześć,", imie, "!")
}

# Użycie funkcji
powitaj("Alicja")

# Funkcja z wieloma argumentami
oblicz_bmi <- function(waga, wzrost) {
  bmi <- waga / (wzrost^2)
  return(bmi)
}

oblicz_bmi(70, 1.75)
```

#### Struktury kontrolne

```{r}
# Instrukcja if-else
x <- 10
if (x > 5) {
  print("x jest większe niż 5")
} else {
  print("x nie jest większe niż 5")
}

# Pętla for
for (i in 1:5) {
  print(paste("Iteracja", i))
}

# Pętla while
licznik <- 1
while (licznik <= 5) {
  print(paste("Licznik:", licznik))
  licznik <- licznik + 1
}
```

### Wprowadzenie do tidyverse

Tidyverse to kolekcja pakietów R zaprojektowanych do nauki o danych. Te pakiety mają wspólną filozofię i są zaprojektowane do bezproblemowej współpracy.

#### Kluczowe pakiety tidyverse

-   ggplot2: do wizualizacji danych
-   dplyr: do manipulacji danymi
-   tidyr: do porządkowania danych
-   readr: do odczytu danych prostokątnych
-   purrr: do programowania funkcyjnego
-   tibble: nowoczesne ujęcie ramek danych

#### Rozpoczęcie pracy z tidyverse

```{r}
# Instalacja tidyverse (uruchom raz)
# install.packages("tidyverse")

# Wczytanie tidyverse
library(tidyverse)
```

#### Import danych z readr

```{r}
#| eval: false
# Odczyt plików CSV
dane <- read_csv("dane_spoleczne.csv")

# Odczyt innych formatów plików
read_tsv("dane.tsv")  # Wartości oddzielone tabulatorem
read_delim("dane.txt", delim = "|")  # Niestandardowy separator
```

#### Manipulacja danymi z dplyr

```{r}
# Użyjmy wbudowanego zbioru danych mtcars
data("mtcars")

# Wybieranie kolumn
mtcars %>% 
  select(mpg, cyl, hp)

# Filtrowanie wierszy
mtcars %>% 
  filter(cyl == 4)

# Sortowanie danych
mtcars %>% 
  arrange(desc(mpg))

# Tworzenie nowych zmiennych
mtcars %>% 
  mutate(kpl = mpg * 0.425)

# Podsumowywanie danych
mtcars %>% 
  group_by(cyl) %>% 
  summarize(srednie_mpg = mean(mpg),
            liczba = n())
```

#### Wizualizacja danych z ggplot2

```{r}
#| label: wykres-rozrzutu
#| fig-cap: "Waga samochodu vs. Zużycie paliwa"
# Wykres rozrzutu
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  labs(title = "Waga samochodu vs. Zużycie paliwa",
       x = "Waga (1000 funtów)",
       y = "Mile na galon")
```

```{r}
#| label: wykres-slupkowy
#| fig-cap: "Liczba samochodów według liczby cylindrów"
# Wykres słupkowy
mtcars %>% 
  count(cyl) %>% 
  ggplot(aes(x = factor(cyl), y = n)) +
  geom_bar(stat = "identity") +
  labs(title = "Liczba samochodów według liczby cylindrów",
       x = "Liczba cylindrów",
       y = "Liczba")
```

```{r}
#| label: wykres-pudelkowy
#| fig-cap: "Zużycie paliwa według liczby cylindrów"
# Wykres pudełkowy
ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +
  geom_boxplot() +
  labs(title = "Zużycie paliwa według liczby cylindrów",
       x = "Liczba cylindrów",
       y = "Mile na galon")
```

### Dodatkowe zasoby

-   [R for Data Science](https://r4ds.had.co.nz/)
-   [Dokumentacja tidyverse](https://www.tidyverse.org/)
-   [Ściągawki RStudio](https://www.rstudio.com/resources/cheatsheets/)
-   [Przewodnik Quarto](https://quarto.org/docs/guide/)
-   [R Cookbook](http://www.cookbook-r.com/)

Pamiętaj, aby eksperymentować z kodem, modyfikować przykłady i nie wahaj się korzystać z wbudowanego systemu pomocy R (dostępnego przez wpisanie `?nazwa_funkcji` w konsoli), gdy napotkasz nieznane funkcje lub koncepcje.
