# Introduction to Data Science and Statistics

## What are Statistics and Data Science?

> **Important**
>
> Statistics and data science are both the art and science of learning from data – they help us understand the world through methodical analysis of collected information.

Statistics and data science provide essential tools for social science researchers, regardless of specialization. Whether you study political science, economics, sociology, or another social science field, these tools enable you to:

- Analyze social trends and behaviors
- Measure the effects of various policies 
- Form conclusions based on empirical evidence rather than intuition

Statistics provides the mathematical foundations for data analysis, including research design, information synthesis, and hypothesis testing. Data science extends these capabilities by combining statistics with programming skills and domain knowledge, enabling work with complex datasets.

In today's digital era, with rapidly expanding available data, these analytical competencies have become essential for contemporary researchers and social science specialists.

> **Note**
>
> In social sciences, data science constitutes a set of methods for solving complex research problems – combining statistical approaches, computational tools, and specialized knowledge to more effectively analyze social processes.

## The Relationship Between Statistics and Data Science

Rather than viewing statistics and data science as separate disciplines, it's more helpful to see them as complementary approaches within the spectrum of data analysis methods. Data science can be understood as a contemporary extension of traditional statistics that has evolved in response to:

- New technological possibilities
- The need to analyze increasingly complex social data
- The availability of computational tools for handling large datasets

## Basic Concepts in Data Science and Statistics

### Data and Populations

1. **Data**: Information collected during research – this includes survey responses, experimental results, economic indicators, social media content, or any other measurable observations.

2. **Population**: The complete set of units (individuals, institutions, events) that the research concerns – the entire group about which the researcher wants to draw conclusions.
   - *Example*: In a study of voting preferences, the population consists of all citizens eligible to vote in a given country.

3. **Sample**: A subset of the population selected for study. A **representative** sample reflects the key characteristics of the target population in appropriate proportions.
   - *Example*: Instead of studying all eligible voters, researchers might analyze 1,500 randomly selected individuals, accounting for the appropriate distribution of age, gender, education, and region of residence.

   A properly selected sample enables inference about the entire population while significantly reducing research costs and time.

4. **Sampling**: The procedure of selecting individuals from a population for investigation. An unbiased sampling method gives every individual in the population an equal chance of selection, ensuring representative results.

5. **Statistical inference**: The process of drawing conclusions about a population based on data from a sample. It involves:
   - Calculating estimates for population parameters
   - Assessing the reliability of these estimates
   - Testing hypotheses about population characteristics

![The process of using a sample to estimate a population parameter. In this example, a sample of 10 individuals found 6 who own an iPhone, yielding an estimated population proportion of 60%. The actual population proportion is 53.8%.](stat_imgs/population_vs_sample.png)



### Statistical Inference: How Can a Small Sample Represent a Large Population?

When pollsters survey just 1,000 voters to predict an election with 30,000,000 voters (only 0.003% of the population), it might seem puzzling. How can such a tiny fraction tell us about the whole?

It's similar to tasting soup. When you cook a large pot of soup and stir it thoroughly, you don't need to eat the entire pot to know how it tastes. A single spoonful is enough—as long as the soup is well-stirred.

::: callout-note
**The Soup Analogy: A Taste of Statistics**

![](stat_imgs/soup-svgrepo-com.svg){width="30%"}

-   When you taste a spoonful of soup and decide it isn't salty enough, that's exploratory/descriptive analysis.
-   If you generalize and conclude that your entire pot of soup needs salt, that's an inference.
-   For your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population).
-   If the soup is not well stirred (heterogeneous population), it doesn't matter how large a spoon you have (sample size), it will still not accurately represent the whole. If the soup is well stirred (homogeneous population), even a small spoon will suffice to test the soup.
:::



#### Why Random Sampling Works

Random sampling works because of three key principles:

1. **Equal opportunity**: Every person in the population has the same chance of being selected, which prevents systematic bias.

2. **Representative diversity**: When everyone has an equal chance, you naturally tend to get people from all different groups in roughly the same proportions as they exist in the population.
   
   *Example*: If 51% of voters are women, then about 51% of your random sample will likely be women (give or take some random variation).

3. **Law of large numbers**: As your sample size (n) grows, random fluctuations become less influential and your sample statistics get closer to the true population values.
   
   *Example*: If you flip a fair coin 10 times, you might get 7 heads (70%). But if you flip it 1,000 times, you're much more likely to get close to 500 heads (50%).

Without proper random sampling, you could easily end up with a biased sample. For instance, if you only surveyed people at a shopping mall during weekday mornings, you'd miss most working people and students, giving you a skewed picture of the population.


::: {.callout-important}
## Randomness as a Fundamental Law of Nature

Randomness can be viewed as one of the fundamental laws of nature that shapes reality at multiple levels:

- **In quantum mechanics**: Heisenberg's uncertainty principle and the probabilistic nature of quantum phenomena indicate that randomness is built into the fundamental structure of reality, rather than merely resulting from imperfections in our measurements or knowledge

- **In genetics**: Random mutations and genetic recombinations constitute the basic mechanism of evolution and biological diversity

- **In chaos theory**: Deterministic systems can exhibit unpredictable behaviors due to sensitivity to initial conditions (the so-called "butterfly effect")

- **In statistical research**: Random sampling is the foundation for inferring properties of a population - without this natural property, we would not be able to formulate reliable generalizations from limited data sets

This natural randomness becomes the foundation of empirical science methodology, particularly evident in two key aspects:

**1. Randomization in Experiments**

Randomization is the process of randomly assigning research units to experimental groups. It is a key element of experimental methodology that:
- Minimizes the influence of confounding variables
- Balances unknown factors between groups
- Reduces the risk of systematic error
- Enables the use of statistical tests for analyzing results

Example in R:
```{r}
# Randomization in an experiment
set.seed(42) # for reproducibility
participants <- 1:30  # 30 study participants
groups <- c(rep("Control", 15), rep("Experimental", 15))
random_assignment <- sample(groups)  # random assignment to groups
head(data.frame(ID = participants[1:6], Group = random_assignment[1:6]))
```

**2. Simple Random Sampling**

Simple random sampling is the basic method of taking a sample in which each element of the population has an equal chance of being included in the sample. This property of randomness:
- Ensures sample representativeness
- Enables inference about population parameters
- Allows for calculating random errors and constructing confidence intervals

Example in R:
```{r}
# Drawing a simple random sample from a population
population <- 1:1000  # population of 1000 elements
sample_data <- sample(population, 50)  # random sample of 50 elements
summary(sample_data)  # basic statistics of the drawn sample
```

Recognizing randomness as a law of nature changes our perception of reality - from a deterministic view, where everything is theoretically predictable with sufficient knowledge, to a stochastic one, where uncertainty and variability are inherent features of the world. This allows us to design studies that utilize this natural property to reliably understand the reality around us.
:::


::: {.panel-tabset group="language"}
## Simple Random Sampling

**Simple Random Sampling**: Every individual has an equal probability of selection. The entire population is randomly sampled without any predetermined pattern.

![Simple Random Sampling. Retrieved from: https://app.passyourmath.com/courses/theory/116/1750/26640/en](stat_imgs/Simple_Random_Sampling2.svg)

## Stratified sampling
**Stratified Sampling**: The population is divided into distinct subgroups (strata) before random samples are drawn from each stratum proportionally.

![Stratified Sampling. Retrieved from: https://app.passyourmath.com/courses/theory/116/1750/26640/en](stat_imgs/Stratified2.svg)

## Cluster sampling
**Cluster Sampling**: The population is divided into clusters, and entire clusters are randomly selected for analysis rather than individuals.

![Cluster Sampling. Retrieved from: https://app.passyourmath.com/courses/theory/116/1750/26640/en](stat_imgs/Cluster2.svg)
:::


---

## **Population Parameters and Statistical Estimation**

### **Population Parameters**
A **population parameter** ($\theta$) is a numerical value that describes a specific characteristic of an entire population. These parameters are typically unknown and are estimated using sample data.

#### **Common Population Parameters**
- **$\mu$ (Population Mean)**: The average value of the population.  
- **$\sigma^2$ (Population Variance)**: The average squared deviation from the mean.  
- **$\sigma$ (Population Standard Deviation)**: The square root of the variance, measuring the spread of the population.  
- **$p$ (Population Proportion)**: The fraction of the population that exhibits a specific characteristic.

Since studying an entire population is often impractical, we rely on samples to estimate these parameters.

---

### **Statistical Estimation Concepts**

#### **Estimand**
An **estimand** is the specific population parameter (or function of parameters) that we aim to estimate. It represents the unknown value in the population that we want to determine.  

> **Key Distinction**  
> The estimand is the unknown population parameter (e.g., $\mu$, $\sigma^2$, $p$), while the estimator is the method used to calculate the estimate.

---

#### **Estimator (Statistic)**
An **estimator** is a mathematical function or procedure used to estimate a population parameter based on sample data. It is a random variable because its value depends on the specific sample.  

A **statistic** is any measure calculated from sample data. When used to estimate a population parameter, it is called an estimator.

##### **Examples of Estimators**
- **Sample Mean**: $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ (estimator of $\mu$).  
- **Sample Variance**: $s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2$ (estimator of $\sigma^2$).  
- **Sample Proportion**: $\hat{p} = \frac{x}{n}$ (estimator of $p$).

> **Note**  
> An estimator is a **procedure** for calculating a value based on a sample. Applying the same estimator to different samples yields different estimates.

---

#### **Estimate**
An **estimate** is a specific numerical value obtained by applying an estimator to a sample. It is a realization of the random variable (the estimator).

##### **Example**
- **Estimand**: Average height of all adults in a country ($\mu$).  
- **Estimator**: Sample mean formula ($\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$).  
- **Estimate**: 173.5 cm (specific value from a sample).

---

### **Properties of Good Estimators**
A good estimator should have the following properties:  
1. **Unbiasedness**: The expected value of the estimator equals the population parameter ($E(\hat{\theta}) = \theta$).  
2. **Efficiency**: The estimator has the smallest possible variance among all unbiased estimators.  
3. **Consistency**: As the sample size increases, the estimator converges to the true parameter value.  
4. **Sufficiency**: The estimator uses all available information in the sample about the parameter.

---

### **Expected Value of an Estimator**
The **expected value** of a random variable is the long-run average value it would take if the experiment were repeated infinitely. For an estimator, the expected value represents the average value of the statistic across repeated samples from the population.

For a well-constructed estimator, the expected value equals the population parameter:  
$$
E(\hat{\theta}) = \theta
$$  
Such an estimator is called **unbiased**.

##### **Example: Unbiased vs. Biased Estimators**
- The sample mean ($\bar{x}$) is an unbiased estimator of the population mean ($\mu$).  
- The sample variance ($s^2$) requires a correction factor ($\frac{1}{n-1}$ instead of $\frac{1}{n}$) to be unbiased.

---

### **Distribution of Sample Statistics**
A **sample statistic** ($\hat{\theta}$) is a value calculated from sample data and used to estimate a population parameter ($\theta$).  

The **distribution of a statistic** describes how its values vary when repeatedly drawing samples of the same size ($n$) from the same population. This concept is central to statistical inference.

#### **Key Points About the Distribution of a Statistic**
1. It shows how the statistic varies across different samples.  
2. It helps quantify sampling error and uncertainty.  
3. It enables probabilistic statements about the accuracy of estimates.  
4. It forms the basis for statistical inference.

#### **Example: Sample Mean**
The sample mean ($\bar{x}$) is an estimator of the population mean ($\mu$). It is calculated as:  
$$
\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i
$$  
where $x_1, x_2, ..., x_n$ are observations in a random sample.

---

### **Types of Data and Variables**

Data is the foundation of statistical analysis. Understanding its types and characteristics is essential.

#### **Sources of Data**
1. **Primary Data**: Collected directly for a specific research purpose (e.g., surveys, experiments).  
2. **Secondary Data**: Obtained from existing sources (e.g., databases, government records).

#### **Variables and Constants**
- **Variables**: Characteristics that can take on different values in a dataset.  
- **Constants**: Values that remain unchanged throughout the analysis.

#### **Classification of Variables**
1. **Quantitative Variables** (represent quantities or measurements):  
   - **Continuous**: Can take any value within a range (e.g., height, temperature).  
   - **Discrete**: Take specific, often integer values (e.g., number of children, errors).  

2. **Qualitative Variables** (represent categories or qualities):  
   - **Nominal**: Categories with no inherent order (e.g., blood type, gender).  
   - **Ordinal**: Categories with a natural order (e.g., education level, satisfaction ratings).




---

## Statistical Models and Inference

### Statistical Models

A **statistical model** is a mathematical framework that represents the relationships between variables and the structure of data. It helps describe the **data-generating process (DGP)** and enables us to make inferences about unknown parameters.

> **Components of a Statistical Model**
>
> A complete statistical model consists of the following elements:
>
> 1. **Functional form**: The mathematical structure that defines the relationship between variables (e.g., linear, quadratic, exponential).
> 2. **Variables**:
>    - **Dependent variable(s)**: The outcome we aim to predict or explain.
>    - **Independent/explanatory variables**: The factors that may influence the dependent variable.
> 3. **Parameters**: Unknown quantities that we estimate from the data (e.g., regression coefficients like $\beta_0$ and $\beta_1$).
> 4. **Random component**: The error term ($\epsilon$) that accounts for unexplained variability in the data.
> 5. **Probability distribution assumptions**: Assumptions about the distribution of the random component (e.g., normality, homoscedasticity).

**Example of a Linear Regression Model**:
$$
y = \beta_0 + \beta_1x + \epsilon, \quad \text{where} \quad \epsilon \sim N(0, \sigma^2)
$$

In this model:
- $\beta_0$ (intercept) and $\beta_1$ (slope) are parameters to be estimated.
- $\epsilon$ represents the random error term, capturing variability not explained by the model.
- The error term is assumed to follow a normal distribution with a mean of 0 and variance $\sigma^2$.

---

### Causal vs. Predictive Inference

In statistical modelling, there are two primary goals:

1. **Causal Inference**:
   - **Goal**: Determine whether a change in variable **X** *causes* a change in variable **Y**.
   - **Requirements**: Strong assumptions or specialized research designs (e.g., randomized controlled trials, instrumental variables).
   - **Application**: Used to predict the effects of interventions or policy changes.
   - **Example**: Does increasing the minimum wage (X) cause a reduction in employment (Y)?

2. **Predictive Inference**:
   - **Goal**: Predict the values of **Y** based on **X**.
   - **Requirements**: No need to assume a causal relationship between variables.
   - **Focus**: Maximizing prediction accuracy, often using machine learning techniques.
   - **Example**: Predicting house prices (Y) based on features like size, location, and number of bedrooms (X).

> **Warning: Correlation ≠ Causation**
>
> A **spurious relationship** (or spurious correlation) occurs when two variables are statistically associated but not causally related. This can happen due to:
>
> 1. **Confounding**: A third variable influences both X and Y.
>    - *Example*: Ice cream sales (X) and drowning incidents (Y) are correlated because both increase in summer (confounder: temperature).
> 2. **Reverse Causality**: Y affects X, not the other way around.
>    - *Example*: Higher crime rates (Y) lead to more police presence (X), not vice versa.
> 3. **Chance**: Random correlations that occur by coincidence.
>    - *Example*: A correlation between the number of pirates and global temperature (purely coincidental).

---



### Challenges of Causal Inference

The fundamental problem of causal inference is the impossibility of observing **counterfactuals** (alternative scenarios). For a given unit, we can observe only one potential outcome.

![The fundamental problem of causal inference: We can think of causal inference as a PREDICTION problem. How can we predict the counterfactual given that we never observe it?](stat_imgs/meme_horse.svg)

Example:
- We observe a person who completed college and earns $8,000 per month
- We cannot observe how much the same person would earn if they had not completed college

Causal methods attempt to solve this problem through:

1. Randomized experiments
2. Instrumental variables
3. Matching methods
4. Regression discontinuity analysis
5. Difference-in-differences

Common problems in causal inference:

![Confounding bias: drinking the night before is a common cause of sleeping with shoes on and waking up with a headache](stat_imgs/IMG_4337.jpg)

![Reverse causality](stat_imgs/ff13-23.png)

## Statistical Inference

Statistical inference is the process of drawing conclusions about a population based on sample data. It encompasses two main areas:

### 1. Estimation

Estimation is the process of using sample data to estimate unknown population parameters. We distinguish:

- **Point estimation**: Providing a single value as the best approximation of the parameter
- **Interval estimation**: Constructing a confidence interval that indicates the range of possible parameter values consistent with our data

Example of a confidence interval: "The 95% confidence interval for the average height of adults is (173 cm, 175 cm)."

**Correct interpretation of confidence interval**: If we were to repeatedly take samples from the same population and construct a 95% confidence interval for each sample using the same method, about 95% of these intervals would contain the true population parameter value.

**Incorrect interpretation**: "There is a 95% chance that the true mean is in the interval (173 cm, 175 cm)" – this is incorrect because the population parameter is a fixed (though unknown) value, not a random variable.

### 2. Hypothesis Testing

Hypothesis testing is a formal procedure for verifying claims about population parameters.

> **Example: Binomial test for a coin**
>
> Imagine we want to check if a coin is fair.
>
> 1. **Research question**: Is the coin fair (probability of heads = 0.5)?
>
> 2. **Formulate hypotheses**:
>    - **Null hypothesis (H₀)**: p = 0.5 (the coin is fair)
>    - **Alternative hypothesis (H₁)**: p ≠ 0.5 (the coin is not fair)
>
> 3. **Collect data**: We flip the coin 100 times and get 65 heads.
>
> 4. **Analyze**: Is 65 heads out of 100 flips evidence against the hypothesis that the coin is fair?
>
> 5. **Reasoning**:
>    - If the coin were fair (p = 0.5), the number of heads in 100 flips should follow a binomial distribution B(100, 0.5)
>    - For this distribution, we expect an average of 50 heads, with a standard deviation of √(100 × 0.5 × 0.5) = 5
>    - Getting 65 heads means a deviation of 3 standard deviations from the expected value
>    - The probability of getting 65 or more heads with a fair coin is very small (p < 0.01)
>
> 6. **Conclusion**: Since the observed result is very unlikely under the assumption that the coin is fair, we reject the null hypothesis and conclude that the coin is most likely not fair.

General hypothesis testing procedure:

1. Formulate the null hypothesis (H₀) and alternative hypothesis (H₁)
2. Choose a significance level α (usually 0.05)
3. Collect data and calculate the appropriate test statistic
4. Calculate the p-value (the probability of obtaining our data or more extreme results, assuming H₀ is true)
5. Make a decision: if p < α, reject H₀ in favor of H₁

> **Note: Intuition behind hypothesis testing**
>
> Hypothesis testing resembles a court procedure:

> - H₀ corresponds to the principle of "innocent until proven guilty"
> - Data constitutes "evidence" against H₀
> - P-value determines how strong this evidence is
> - If the evidence is strong enough (p < α), we "convict" H₀ (reject it)
> - If the evidence is not strong enough, we do not reject H₀ (but we do not prove its truth)

> **Important: Common errors in interpreting p-values and tests**
>
> 1. P-value is **NOT** the probability that the null hypothesis is true
> 2. P-value is **NOT** the probability of making an error when rejecting H₀
> 3. Failing to reject H₀ does **NOT** mean proving it
> 4. A very small p-value does **NOT** indicate a large practical effect (statistical significance ≠ practical significance)
> 5. P-value depends on sample size - with very large samples, even small, practically insignificant differences can be statistically significant
>
> **Definition of p-value**: The probability of observing a result at least as extreme as the one obtained, assuming the null hypothesis is true.

### Types of Errors in Hypothesis Testing

- **Type I error (α)**: Rejecting a true null hypothesis ("convicting an innocent")
  - The probability of this error is controlled by the significance level α
- **Type II error (β)**: Failing to reject a false null hypothesis ("acquitting the guilty")
  - The probability of avoiding this error (1-β) is called the power of the test
  - Test power increases with sample size and effect size

## Foundations for Good Statistical Research

To conduct reliable statistical research, ensure:

1. **Representativeness of the sample**: The sample should well reflect the studied population
2. **Adequate sample size**: Larger samples provide more accurate estimates and greater statistical power
3. **Control of confounding variables**: Both in research design and data analysis
4. **Appropriate statistical methods**: Matched to the type of data and research questions
5. **Clear interpretation**: Taking into account study limitations and alternative explanations

### Summary of Key Concepts

| Concept | Definition | Example |
|----------------------|--------------------------|------------------------|
| **Population parameter (Estimand)** | Value characterizing the population, usually unknown | μ (population mean) |
| **Estimator (Statistic)** | Function/procedure for estimating a parameter based on a sample | $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ |
| **Estimate** | Specific value obtained after applying an estimator to a sample | $\bar{x} = 173.5$ cm |
| **Standard error** | Measure of estimator variability between samples | $SE(\bar{x}) = \frac{s}{\sqrt{n}}$ |
| **Confidence interval** | Range of values that, with specified probability, contains the parameter | (173 cm, 175 cm) |
| **P-value** | Probability of observing the data assuming H₀ is true | p = 0.03 |


---

::: {.callout-important}
## Understanding Statistical Error

When using a sample to learn about a population, we inevitably encounter statistical error:

**Statistical error** is the difference between our sample estimate and the true population value.

Statistical errors can compromise the validity and reliability of research findings. Understanding these errors is crucial for designing robust studies, analyzing data accurately, and drawing appropriate conclusions. Below is a detailed classification of statistical errors:

---

### Sampling Errors

**Random Sampling Errors**

- These arise from the natural variability inherent in random sampling.
- *Example*: If you randomly select 30 students from a university to measure average height, the average will vary slightly with each sample.
- These errors decrease as the sample size increases and can be estimated using metrics like the *margin of error*.
- While unavoidable, they can be managed through proper sampling techniques.

**Non-Random (Systematic) Sampling Errors (Bias)**

- **Selection bias**: Occurs when the sample does not represent the target population.
  - *Example*: A phone survey that only reaches individuals with landlines, excluding those who rely solely on cell phones.
- **Undercoverage**: Certain groups are systematically excluded from the sample.
  - *Example*: A campus survey conducted only during daytime classes misses evening students.
- **Self-selection bias**: Results are skewed because participants choose whether to participate.
  - *Example*: Only individuals with strong opinions respond to a customer satisfaction survey.
- **Non-response bias**: Occurs when specific groups are less likely to respond.
  - *Example*: Busy individuals may not complete a questionnaire, leading to overrepresentation of those with more free time.

---

### Measurement Errors

**Random Measurement Errors**

- Unpredictable fluctuations in measurements due to "noise."
  - *Example*: A shaky hand measuring blood pressure yields slightly different readings each time.
- These errors tend to cancel out with repeated measurements.

**Systematic Measurement Errors**

- **Calibration errors**: Consistent inaccuracies in measurement tools.
  - *Example*: A scale that always adds 2 pounds to every measurement.
- **Observer bias**: The researcher’s expectations influence data collection.
  - *Example*: A researcher unconsciously records data in a way that supports their hypothesis.
- **Social desirability bias**: Participants respond in a way they perceive as socially acceptable.
  - *Example*: Underreporting junk food consumption in a nutrition study.

---

### Inference Errors

**Hypothesis Testing Errors**

- **Type I error (False Positive)**: Concluding an effect exists when it does not.
  - *Example*: Claiming a new drug is effective when its benefits are due to random chance.
- **Type II error (False Negative)**: Failing to detect an effect that actually exists.
  - *Example*: Concluding a new drug is ineffective when it actually helps patients.
- **Type III error**: Solving the wrong problem or answering the wrong question entirely.
  - *Explanation*: This occurs when researchers focus on a question or hypothesis that is irrelevant or misaligned with the actual problem they need to address. It’s not about statistical analysis being wrong, but about asking the wrong question in the first place.
  - *Example*: A company studies whether a new marketing strategy increases website traffic (the wrong question) when the real issue is whether it increases sales (the right question).
  - *Another Example*: A researcher investigates whether a teaching method improves test scores (the wrong question) when the real goal is to improve long-term student understanding (the right question).

**Statistical Power Issues**

- **Underpowered studies**: Insufficient sample size to detect true effects.
  - *Example*: Testing a weight loss program with only 5 participants.

---

### Model Specification Errors

**Variable Selection Errors**

- **Omitted variable bias**: Excluding important variables that influence the outcome.
  - *Example*: Studying exercise and weight loss without accounting for diet.
- **Confounding**: An external factor influences both the predictor and outcome.
  - *Example*: People who exercise may also eat healthier, affecting weight loss results.

**Assumption Errors**

- Using statistical tests that do not align with the data’s characteristics.
  - *Example*: Assuming a normal distribution when the data is skewed.

---

### Data Processing and Analysis Errors

**Data Handling Errors**

- **Data entry errors**: Mistakes during data recording.
  - *Example*: Typing "1050" instead of "150" for a participant’s weight.
- **Missing data problems**: Failing to account for incomplete data.
  - *Example*: Calculating average test scores while ignoring absent students.

**Analysis Execution Errors**

- **Software bugs or user errors**: Mistakes in using statistical software.
  - *Example*: Selecting the wrong statistical test in analysis software.

---

### Interpretation and Reporting Errors

**Causal Inference Errors**

- **Correlation-causation fallacy**: Assuming causation from correlation.
  - *Example*: Concluding that chocolate consumption causes Nobel Prizes because the two are correlated.

**Reporting and Communication Errors**

- **p-hacking**: Conducting multiple analyses until a significant result is found.
  - *Example*: Analyzing data in 20 ways and reporting only the significant outcome.
- **Cherry-picking**: Selectively reporting results that support a hypothesis.
  - *Example*: A diet pill company only publishing studies where their product worked.

---

### Remedy Strategies

To minimize statistical errors:

- **Proper research design**: Plan studies carefully before data collection.
- **Adequate sample size**: Ensure sufficient participants based on power calculations.
- **Pre-registration**: Declare analysis plans before examining the data.
- **Transparency**: Share data and methods openly.
- **Replication**: Encourage independent replication of studies.

Understanding and addressing these errors ensures research findings are trustworthy, preventing poor decisions in fields like medicine, public policy, education, and business.
:::

::: {.callout-note}
## Optional: Proof that the sample mean is an unbiased estimator

For the sample mean $\bar{x}$, we can prove it's an unbiased estimator of the population mean $\mu$ as follows:

$E(\bar{x}) = E\left(\frac{1}{n}\sum_{i=1}^{n}x_i\right)$

Using the linearity property of expected values (the expected value of a sum equals the sum of expected values):

$E(\bar{x}) = \frac{1}{n}\sum_{i=1}^{n}E(x_i)$

Since each $x_i$ is a random observation from the population with mean $\mu$:

$E(x_i) = \mu \text{ for all } i$

Therefore:

$E(\bar{x}) = \frac{1}{n}\sum_{i=1}^{n}\mu = \frac{1}{n} \cdot n \cdot \mu = \mu$

This proves that the sample mean is an unbiased estimator of the population mean.
:::

::: {.callout-note}
## Optional: Understanding why the sample variance needs n-1 in the denominator

The sample variance is commonly defined as:

$s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2$

Why do we use $n-1$ in the denominator rather than $n$?

Let's consider what happens if we used $n$ instead:

$\tilde{s}^2 = \frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2$

To check if this is unbiased, we need to find $E(\tilde{s}^2)$ and see if it equals $\sigma^2$.

Without going through all the mathematical steps, the key insight is:
1. When we calculate $(x_i - \bar{x})^2$, we're using $\bar{x}$ (the sample mean) instead of $\mu$ (the population mean)
2. The sample mean $\bar{x}$ is itself estimated from the same data
3. This introduces a dependency that makes our variance estimator systematically underestimate the true variance
4. Specifically, $E(\tilde{s}^2) = \frac{n-1}{n}\sigma^2$, which means it's biased by a factor of $\frac{n-1}{n}$
5. To correct this bias, we divide by $n-1$ instead of $n$, giving us:
   $E(s^2) = E\left(\frac{n}{n-1}\tilde{s}^2\right) = \frac{n}{n-1}E(\tilde{s}^2) = \frac{n}{n-1}\frac{n-1}{n}\sigma^2 = \sigma^2$

Intuitively, we're using $n-1$ because we've already "used up" one degree of freedom when estimating the mean. The denominator $n-1$ is called the "degrees of freedom" - the number of independent pieces of information available for estimating the variance after estimating the mean.
:::


::: {.callout-note}
## Data Storage Methods in Data Science

Data scientists use various methods to store data. Here are the most common ones:

---

### **CSV Files**
These are simple text files where data is stored in rows, with values separated by commas.

- **Pros**:
  - Easy to open and edit, even in a basic text editor.
  - Works with most data analysis tools.
  - Simple to share with others.

- **Cons**:
  - Not suitable for complex data (e.g., images or videos).
  - Can be slow with very large datasets.
  - Sometimes issues with special characters (e.g., accents).

---

### **SQL Databases**
These are advanced systems for storing data in tables that can be linked together.

- **Pros**:
  - Ideal for managing complex data (e.g., orders and customers).
  - Fast searching and analysis.
  - Secure and reliable.

- **Cons**:
  - Requires more expertise to manage.
  - Harder to adapt to changes in data structure.
  - Can be costly to maintain.
  
---

### **Other Formats**

#### **Parquet**
- **Pros**:
  - Fast and efficient, especially for large datasets.
  - Saves space through compression.

- **Cons**:
  - Harder to directly view or edit by humans.

#### **JSON**
- **Pros**:
  - Flexible, great for storing nested or complex data.
  - Human-readable.

- **Cons**:
  - Takes up more space than CSV.
  - Slower to process.

#### **NoSQL**
- **Pros**:
  - Highly flexible, no fixed structure required.
  - Scalable, great for huge datasets.

- **Cons**:
  - Less suitable for complex relationships between data.
:::



::: {.callout-note}
## The Normal Distribution - The Bell Curve

> **Note**
>
> The normal distribution (also known as the Gaussian distribution or bell curve) is one of the most important probability distributions in statistics. It has a symmetric bell shape and is fully described by two parameters:
> 
> - The mean $\mu$ (centers the distribution)
> - The standard deviation $\sigma$ (determines the width of the "bell")
>
> The normal distribution appears frequently in nature and human systems (heights, measurement errors, test scores). 

### Mathematical Representation

The probability density function (PDF) of the normal distribution is:

$$
f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
$$

Where:

- $x$ is a random variable
- $\mu$ is the mean (location parameter)
- $\sigma$ is the standard deviation (scale parameter)
- $e$ is Euler's number (approximately 2.71828)
- $\pi$ is the mathematical constant pi (approximately 3.14159)

### Practical Significance

The normal distribution is especially important because of:

1. The **68-95-99.7 Rule**: Approximately 68% of values fall within 1 standard deviation of the mean, 95% within 2 standard deviations, and 99.7% within 3 standard deviations.

2. The **Central Limit Theorem**: Sample means tend to follow a normal distribution regardless of the original distribution's shape (given a sufficiently large sample size).

3. **Statistical Inference**: The normal distribution forms the foundation for many statistical tests and confidence intervals.
:::


::: {.callout-note}

```{r}
#| echo: false
#| message: false
library(ggplot2)
library(dplyr)
set.seed(123)  # Reproducibility
```

## Data Generating Process, Superpopulation, Population, and Sample

To understand statistical inference, we must distinguish four key concepts: the **Data Generating Process (DGP)**, **superpopulation**, **population**, and **sample**. We’ll use an example of worker wages to illustrate these ideas.

---

### Data Generating Process (DGP)

The DGP is the "true" mechanism that produces the data we observe. It includes:  
- Systematic components (e.g., relationships between variables)  
- Random components (e.g., measurement error, unobserved factors)  

**Example**: Suppose a worker’s hourly wage ($Y$) depends on their years of education ($X$) and unobserved factors ($\epsilon$):  
$$ Y = 20 + 2.5X + \epsilon \quad \text{where } \epsilon \sim N(0, 5^2) $$  

We can simulate this DGP in R:  
```{r}
# Simulate DGP for education (X) and wages (Y)
n_superpopulation <- 100000  # Superpopulation size
dgp_data <- tibble(
  education = rpois(n_superpopulation, lambda = 10),  # Years of education (~Poisson)
  epsilon = rnorm(n_superpopulation, mean = 0, sd = 5),
  wage = 20 + 2.5 * education + epsilon
)

# Plot the relationship
ggplot(dgp_data, aes(x = education, y = wage)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "DGP: True Relationship Between Education and Wages")
```

---

### Superpopulation

The superpopulation is a hypothetical, often infinite, set of potential observations from which our *observed* population is drawn. It is governed by the DGP.  

**Example**:  
- **Superpopulation**: All potential workers (past, present, future) whose wages follow the DGP above.  
- **Observed population**: A finite subset (e.g., all workers in 2024).  

---

### Population

The population is the finite set of units we want to study. Its parameters (e.g., mean wage) are fixed but unknown.  

**Example**:  
```{r}
# Generate our "population" (a finite subset of the superpopulation)
population <- dgp_data %>% slice_sample(n = 5000)  # 5,000 workers

# True population mean wage (unknown in practice)
pop_mean_wage <- mean(population$wage)
pop_mean_wage  # Display value
```

---

### Sample

A sample is a subset of the population, used to estimate population parameters.  

**Example**:  
```{r}
# Take a random sample from the population
sample_size <- 200
sample_data <- population %>% 
  slice_sample(n = sample_size)

# Estimate mean wage from the sample
sample_mean_wage <- mean(sample_data$wage)
sample_mean_wage  # Compare to pop_mean_wage
```

---

### Conceptual Diagram

```{mermaid}
graph TD
  A[DGP<br>True wage model] --> B[Superpopulation<br>All potential workers]
  B --> C[Population<br>All workers in 2024]
  C --> D[Sample<br>200 surveyed workers]
```

---

### Why This Matters  
- **Statistical inference** uses the sample to estimate population parameters (e.g., mean wage).  
- The **superpopulation** framework allows us to generalize findings beyond the observed population.  
- The **DGP** reminds us that our models are approximations of reality.  

**Discussion Question**: If the DGP includes `education` and `epsilon`, what might `epsilon` represent in the context of wages?  


### Examples from Electoral Studies

**Example 1: Voter Turnout**

- **DGP**: $Turnout_i = \beta_0 + \beta_1 Age_i + \beta_2 Education_i + \beta_3 Income_i + \varepsilon_i$
- **Superpopulation**: All possible decisions to participate in elections that voters with different socio-demographic characteristics could make under different conditions
- **Population**: All eligible voters in the 2020 presidential election
- **Sample**: 1500 respondents of an exit poll after the election

**Example 2: Support for Political Parties**

- **DGP**: $Support_{ij} = \beta_0 + \beta_1 Ideology_i + \beta_2 EconomicSituation_i + \beta_3 Age_i + \varepsilon_i$
- **Superpopulation**: All possible electoral preferences of people with different characteristics in different socio-economic conditions
- **Population**: All voters in the United States in 2023
- **Sample**: Pre-election survey respondents (n=1000)

### Simulation and Estimation of a Demand Function as DGP in R

The following code illustrates how to simulate a demand function with multiple predictors as a DGP and estimate its parameters using OLS regression:

```{r}
#| warning: false
#| message: false

# Loading required packages
library(tidyverse)

# Setting random seed for reproducible results
set.seed(123)

# 1. Defining the "true" DGP: Product demand function
# Model: Q = beta0 + beta1*P + beta2*I + beta3*P_sub + beta4*P_comp + beta5*A + epsilon
# Where:
# Q = quantity demanded
# P = product price
# I = consumer income
# P_sub = price of substitute good
# P_comp = price of complementary good
# A = advertising expenditure

# True parameter values (consistent with economic theory)
beta0_true <- 100      # Constant
beta1_true <- -2.5     # Own price effect (negative - according to the law of demand)
beta2_true <- 0.8      # Income effect (positive - normal good)
beta3_true <- 1.2      # Substitute price effect (positive)
beta4_true <- -0.7     # Complementary good price effect (negative)
beta5_true <- 0.5      # Advertising effect (positive)
sigma_true <- 5        # Standard deviation of random error

# 2. Simulating superpopulation (5000 potential markets/periods)
n_super <- 5000

# Generating predictors
price <- runif(n_super, min = 5, max = 15)             # Product price ($)
income <- rnorm(n_super, mean = 3000, sd = 500)        # Average income ($)
price_substitute <- runif(n_super, min = 4, max = 16)  # Substitute price ($)
price_complement <- runif(n_super, min = 2, max = 8)   # Complement price ($)
advertising <- runif(n_super, min = 0, max = 100)      # Advertising expenditure ($1000s)

# Adding correlation between variables (e.g., price and substitute price)
price_substitute <- price_substitute + rnorm(n_super, mean = 0.2 * price, sd = 1)

# Generating demand according to DGP
epsilon <- rnorm(n_super, mean = 0, sd = sigma_true)  # Random component
demand <- beta0_true + 
         beta1_true * price + 
         beta2_true * (income/1000) +  # scaling income for better interpretation
         beta3_true * price_substitute + 
         beta4_true * price_complement + 
         beta5_true * (advertising/10) +   # scaling advertising for better interpretation
         epsilon

# Creating superpopulation data frame
superpopulation <- tibble(
  id = 1:n_super,
  price = price,
  income = income,
  price_substitute = price_substitute,
  price_complement = price_complement,
  advertising = advertising,
  demand = demand
)

# Show first few observations
head(superpopulation)

# 3. Drawing a sample from superpopulation (e.g., 200 observations)
n_sample <- 200
sample_indices <- sample(1:n_super, n_sample)
sample_data <- superpopulation[sample_indices, ]

# 4. Estimating OLS model based on the sample
ols_model <- lm(demand ~ price + I(income/1000) + price_substitute + 
                price_complement + I(advertising/10), data = sample_data)

# 5. Displaying model summary
summary(ols_model)

# 6. Comparing true parameters with estimated ones
true_parameters <- c(beta0_true, beta1_true, beta2_true, 
                     beta3_true, beta4_true, beta5_true)
estimated_parameters <- coef(ols_model)

comparison <- tibble(
  parameter = c("Intercept", "Price", "Income (thousands)", "Substitute price", 
               "Complement price", "Advertising (10 thousands)"),
  true_value = true_parameters,
  estimated_value = estimated_parameters,
  difference = estimated_value - true_value,
  percent_error = abs(difference / true_value) * 100
)

# Display comparison
print(comparison)

# 7. Visualizing comparison of true and estimated parameters
ggplot(comparison, aes(x = parameter, y = true_value)) +
  geom_point(color = "blue", size = 3) +
  geom_point(aes(y = estimated_value), color = "red", size = 3) +
  geom_segment(aes(xend = parameter, y = true_value, 
                   yend = estimated_value), color = "gray") +
  labs(title = "Comparison of True DGP with Estimated Model",
       subtitle = "Blue points: true values, Red points: estimated values",
       x = "Parameter", y = "Value") +
  theme_minimal() +
  coord_flip()

# 8. Checking model's predictive ability on new data
# We draw new data from superpopulation (not used in estimation)
new_indices <- sample(setdiff(1:n_super, sample_indices), 100)
new_data <- superpopulation[new_indices, ]

# Prediction on new data
new_data$predicted_demand <- predict(ols_model, newdata = new_data)

# Calculating mean squared error (MSE) of prediction
mse <- mean((new_data$demand - new_data$predicted_demand)^2)
rmse <- sqrt(mse)
cat("Root Mean Squared Error (RMSE):", round(rmse, 2), "\n")

# 9. Visualizing relationship between price and demand
# (ceteris paribus effect - controlling for other variables)
ceteris_paribus <- tibble(
  price = seq(5, 15, length.out = 100),
  income = mean(sample_data$income),
  price_substitute = mean(sample_data$price_substitute),
  price_complement = mean(sample_data$price_complement),
  advertising = mean(sample_data$advertising)
)

# Calculating predicted demand according to true DGP
ceteris_paribus$true_demand <- beta0_true + 
                              beta1_true * ceteris_paribus$price + 
                              beta2_true * (ceteris_paribus$income/1000) + 
                              beta3_true * ceteris_paribus$price_substitute + 
                              beta4_true * ceteris_paribus$price_complement + 
                              beta5_true * (ceteris_paribus$advertising/10)

# Calculating predicted demand according to estimated model
ceteris_paribus$estimated_demand <- predict(ols_model, newdata = ceteris_paribus)

# Visualization
ggplot(ceteris_paribus, aes(x = price)) +
  geom_line(aes(y = true_demand, color = "True DGP"), size = 1.2) +
  geom_line(aes(y = estimated_demand, color = "Estimated model"), size = 1.2) +
  scale_color_manual(values = c("True DGP" = "blue", "Estimated model" = "red")) +
  labs(title = "Demand Curve: True DGP vs. Estimated Model",
       subtitle = "Ceteris paribus effect (with other variables held constant)",
       x = "Product price ($)",
       y = "Demand (quantity)",
       color = "Model") +
  theme_minimal()
```

This code demonstrates:

1. Defining a complex DGP for a demand function with multiple predictors consistent with economic theory
2. Simulating a superpopulation according to this DGP
3. Drawing a sample from the superpopulation
4. Estimating model parameters using OLS regression
5. Comparing estimated parameters with their true values
6. Visualizing the comparison of true and estimated parameters
7. Checking the model's predictive ability on new data
8. Visualizing the relationship between price and demand with ceteris paribus effect

This example shows how we can simulate complex economic relationships and then use econometric methods to discover these relationships based on a sample of data. It is an excellent illustration of how economic theory, DGP, and statistical methods are interconnected.

In reality, we never know the true DGP - that's exactly what we're trying to discover through statistical analysis. Simulations of this type, however, allow us to conceptually understand how statistical inference connects with the concept of DGP and superpopulation.


```{mermaid}
graph TD
    DGP[Data Generating Process] -->|Generates| SP[Superpopulation]
    SP -->|Single finite realization| A[Population]
    A -->|Random Selection| B[Sample]
    B -->|Statistical Inference| C[Estimates & Conclusions]
    C -->|Generalize back to| A
    C -.->|Infer parameters of| SP
    C -.->|Ultimate goal: estimate parameters of| DGP
    
    style DGP fill:#1E90FF,stroke:#000,stroke-width:4px,color:#FFF
    style SP fill:#9932CC,stroke:#000,stroke-width:4px,color:#FFF
    style A fill:#DC143C,stroke:#000,stroke-width:4px,color:#FFF
    style B fill:#228B22,stroke:#000,stroke-width:2px,color:#FFF
    style C fill:#8B4513,stroke:#000,stroke-width:2px,color:#FFF
    
    classDef note fill:#F0F0F0,stroke:#000,stroke-width:1px;
    H[["Superpopulation:
    All possible outcomes
    generated by the DGP
    (infinite, hypothetical)"]]
    D[["DGP:
    True mechanism with
    unknown parameters
    that generates data"]]
    E[["Population:
    Single, specific, finite
    realization from the
    superpopulation"]]
    F[["Sample:
    Observed subset
    of the population"]]
    G[["Inference:
    Drawing conclusions about
    population, superpopulation,
    and ultimately the DGP"]]
    
    class D,E,F,G,H note
    
    H --> SP
    D --> DGP
    E --> A
    F --> B
    G --> C
```


### Supplementary Examples

#### Example 1: Voter Opinion Survey

-   **Population**: All registered voters in Poland in 2023 (approx. 30 million people).
-   **Sample**: 1000 randomly selected voters surveyed in a poll.
-   **Superpopulation**: All potential voters (current, future, and hypothetical) and all possible voting scenarios.
-   **DGP** (Data Generating Process): The complex mechanism shaping voter opinions and decisions, including:
    -   Demographic factors (age, education, place of residence).
    -   Economic conditions (income, employment status).
    -   Media influence and public debate.
    -   Personal experiences.
    -   Historical political context.

#### Example 2: Study of a Diabetes Drug’s Effectiveness

-   **Population**: All patients with type 2 diabetes in a given country (e.g., 2 million people).
-   **Sample**: 500 patients participating in a clinical trial.
-   **Superpopulation**: All potential patients with type 2 diabetes (current and future) with varying genetic and environmental profiles.
-   **DGP**: The biological mechanism involving:
    -   Drug interactions with receptors in the body.
    -   Individual genetic predispositions.
    -   Environmental factors (diet, physical activity).
    -   Interactions with other medications.
    -   Metabolic mechanisms of the body.

#### Example 3: When the Sample Equals the Population

Study of all 50 U.S. states:

-   **Traditional Approach**: No distinction between sample and population (all states are studied).
-   **Superpopulation Approach**:
    -   **Population/Sample**: The 50 existing U.S. states.
    -   **Superpopulation**: The theoretical set of all possible territorial units resembling "states" under different historical, political, and social conditions.
    -   **DGP**: Fundamental geographic, historical, political, and socio-economic mechanisms shaping state characteristics.

#### Example 4: Pizza Quality in New York City

-   **Population**: All currently operating pizzerias in New York City (e.g., 2000 establishments).
-   **Sample**: 50 randomly selected pizzerias from different neighborhoods.
-   **Superpopulation**: All possible pizzerias that could exist in New York City:
    -   Currently operating.
    -   Future (not yet opened).
    -   Historical (already closed).
    -   Hypothetical (under alternative economic or cultural conditions).
-   **DGP**: Factors influencing pizza quality:
    -   Ingredients and their quality.
    -   Skills and experience of chefs.
    -   Kitchen equipment and infrastructure.
    -   Preparation methods and recipes.
    -   Environmental factors (e.g., local water quality).
    -   Cultural influences and culinary traditions.
    -   Economic conditions (operational costs, rent).

The DGP is like a "recipe for pizza quality" that determines outcomes for all potential pizzerias in the superpopulation, not just the currently existing ones.

:::


---

::: {.callout-note}
### Sample Size Considerations

Determining the appropriate sample size is critical for reliable statistical analysis. The required sample size depends on three key factors:

1. **Type of Estimate**  
   - What are you estimating? (e.g., proportion, mean, regression parameter, etc.)
   
2. **Desired Accuracy**  
   - Higher accuracy requires a larger sample size.  
   - Example: Estimating within ±1% requires approximately 9 times more observations than estimating within ±3%.

3. **Population Variability**  
   - For **proportions**: Maximum variability occurs at 50%, while variability decreases as proportions approach 0% or 100%.  
   - For **means**: Variability depends on the variance of the measurements.  
   - For **regression models**: Consider the variance of predictors and outcome variables.

---

### Small Population Considerations

When working with small populations (e.g., fewer than 1,000 individuals), special considerations apply:

- **Sampling Methods**:  
  - Simple random sampling traditionally assumes sampling **with replacement**.  
  - In practice, sampling **without replacement** is more common.  
  - The difference between these methods becomes significant in small populations.

- **Finite Population Correction (FPC)**:  
  - Essential when sampling more than 5-10% of the population.  
  - Adjusts standard error calculations to account for the reduced variability in small populations.

#### Practical Recommendations for Small Populations:
- For populations under 200, consider conducting a **complete census**.  
- Use statistical formulas designed for sampling without replacement.  
- Standard formulas (based on sampling with replacement) overestimate error in small populations.  
- Clearly document the sampling methodology in research reports.

#### Key Differences Between Sampling Methods:
- **Sampling with replacement**: Each unit has a constant probability of selection (e.g., 1/N) in each draw.  
- **Sampling without replacement**: Selection probability changes in subsequent draws (e.g., 1/N, 1/(N-1), 1/(N-2), etc.).  
- In large populations, the difference is negligible; in small populations, it is significant.  
- Sampling without replacement generally results in lower estimator variance.

---

### Statistical Estimation and Sample Size

#### Proportions (Percentages)
When estimating a proportion (e.g., percentage of voters supporting a candidate), the required sample size depends on how close the proportion is to 50%.

**Why Proportions Near 50% Require Larger Samples**:  
Proportions follow a binomial distribution, where variance is maximized at 50% and minimized at 0% or 100%. This means greater uncertainty and larger sample sizes are needed for proportions near 50%.

**Example**:  
| True Population Proportion | Sample Size | Margin of Error |
|----------------------------|-------------|-----------------|
| 50%                        | 100         | ±10%            |
| 50%                        | 400         | ±5%             |
| 50%                        | 1,000       | ±3%             |
| 10%                        | 100         | ±6%             |
| 10%                        | 400         | ±3%             |
| 10%                        | 1,000       | ±2%             |

**Key Observations**:  
1. **Larger sample sizes reduce margin of error**.  
2. **Proportions near 50% have larger margins of error** than those near 0% or 100%.

**Population Size Impact**:  
For proportions near 50%, a sample size of ~1,000 provides a margin of error of ±3%, regardless of whether the population is 30,000 or 30 million. This explains why national polls can be accurate with relatively small samples.

---

#### Averages (Means)
When estimating an average (e.g., household income or height):

- Sample size depends on the **variance** of the measurements.  
- Populations with greater variability require larger samples.  
- Unlike proportions, there is no "maximum uncertainty" at a specific value.  
- Sample size is directly proportional to the variance of the variable.

**Example**:  
Estimating average income typically requires a larger sample than estimating average height because income varies more widely.

---

#### Model Parameters and Complex Estimation
Statistical modeling (e.g., regression) involves estimating relationships between variables. Sample size requirements become more nuanced in these contexts.

**Key Factors for Regression Models**:  
1. **Number of Parameters**:  
   - Each predictor variable and interaction term consumes degrees of freedom.  
   - Rule of thumb: 10-20 observations per variable (minimum).  
   - For detecting subtle effects: 50-100+ observations per variable may be needed.

2. **Effect Size**:  
   - Smaller effects require larger samples to detect reliably.  
   - Conduct power analysis based on expected effect sizes.

3. **Model Complexity**:  
   - Linear vs. non-linear relationships.  
   - Interaction effects, nested/hierarchical data structures.  
   - Non-normal distributions or heteroscedasticity.

4. **Explained Variability**:  
   - Higher \(R^2\) values may reduce sample size requirements.  
   - Greater residual variance increases sample size needs.

---

### Sample Size Planning in Practice

When planning a study, consider the following:

1. **Statistical Power**:  
   - Aim for at least 80% power to detect effects.  
   - Conduct formal power analyses to determine sample size.

2. **Precision**:  
   - Define the desired width of confidence intervals.  
   - Greater precision requires larger samples.

3. **Resource Constraints**:  
   - Balance statistical ideals with budget, time, and participant availability.  
   - Consider sequential or adaptive sampling designs.

4. **Ethical Considerations**:  
   - Collect enough data to answer research questions reliably.  
   - Avoid unnecessary burden on participants or resource waste.

---

### Why 50% Represents Maximum Uncertainty for Proportions

**Simulation Example**:  
Imagine a bag of 100 coins (gold or silver). You estimate the percentage of gold coins by drawing 20 coins repeatedly.

**Scenario A: 50% Gold (Maximum Uncertainty)**  
- Results vary widely (35% to 70%), with an average error of 10 percentage points.

**Scenario B: 10% Gold (Less Uncertainty)**  
- Results stay closer to the true value (0% to 15%), with an average error of 4 percentage points.

**Conclusion**:  
Proportions near 50% have higher natural variability, requiring larger samples for accurate estimation.

---

### Opinion Poll Example: Popular vs. Unpopular Candidates

In a poll of 1,000 people:  
- **Major candidate polling at 40%**: Accuracy of ±3% (support between 37% and 43%).  
- **Minor candidate polling at 3%**: Accuracy of ±1% (support between 2% and 4%).

**Key Insight**:  
- **Absolute accuracy** varies with the proportion.  
- **Relative accuracy** (as a percentage of the estimate) is higher for minor candidates.

---

### Summary: Key Takeaways

1. **Sample Size Depends On**:  
   - Type of estimate (proportion, mean, regression parameter).  
   - Desired accuracy and population variability.  
   - For proportions, values near 50% require larger samples.

2. **Small Populations Require Special Considerations**:  
   - Use finite population correction.  
   - Consider complete censuses for very small populations.

3. **Statistical Power and Precision**:  
   - Aim for sufficient power and precision while balancing resource constraints.

4. **Ethical and Practical Balance**:  
   - Ensure reliable results without overburdening participants or resources.

By understanding these principles, you can design studies that yield accurate and meaningful insights.
:::


---

# Appendices: Additional Topics in Statistics and Data Science (*)

## Appendix A: R for Social Science Data Analysis

R offers powerful capabilities for social science research, from data manipulation to advanced statistical modeling.

```{r}
#| code-fold: true
#| code-summary: "Click to show/hide R code"

library(tidyverse)

# Set seed for reproducibility
set.seed(42)

# Generate example data with a Simpson's Paradox
n <- 1000
data <- tibble(
  age_group = sample(c("Young", "Middle", "Old"), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),
  education_years = case_when(
    age_group == "Young" ~ rnorm(n, mean = 10, sd = 1),
    age_group == "Middle" ~ rnorm(n, mean = 13, sd = 1),
    age_group == "Old" ~ rnorm(n, mean = 16, sd = 1)
  ),
  income = case_when(
    age_group == "Young" ~ 70000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),
    age_group == "Middle" ~ 50000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),
    age_group == "Old" ~ 30000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000)
  )
)

# Basic data summary
summary(data)

# Correlation analysis
cor(data %>% select(education_years, income))

# Overall trend (Simpson's Paradox)
overall_plot <- ggplot(data, aes(x = education_years, y = income)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Overall Relationship between Education and Income",
       subtitle = "Simpson's Paradox: Appears negative",
       x = "Years of Education", y = "Income") +
  theme_minimal()

# Trend by age group (Resolving Simpson's Paradox)
grouped_plot <- ggplot(data, aes(x = education_years, y = income, color = age_group)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Relationship between Education and Income by Age Group",
       subtitle = "Resolving Simpson's Paradox: Positive relationship within groups",
       x = "Years of Education", y = "Income") +
  theme_minimal()

# Print results
print(overall_plot)
print(grouped_plot)
```

This example demonstrates Simpson's Paradox, where the overall relationship between education and income appears negative, but when grouped by age, the relationship within each group is positive. This illustrates how critical it is to consider confounding variables in your analysis.

## Appendix B: Causal Inference vs. Observational Studies

Understanding the relationship between variables is crucial in social sciences. Two key approaches are causal inference and observational studies, each with distinct strengths and limitations.

### Causal Inference

- Aims to establish cause-and-effect relationships
- Often involves experimental designs or advanced statistical techniques
- Seeks to answer "What if?" questions and determine the impact of interventions
- Examples: Randomized controlled trials, quasi-experimental designs, instrumental variables

### Observational Studies

- Examine relationships between variables without direct intervention
- Rely on data collected from natural settings or existing datasets
- Can identify correlations and patterns but struggle to establish causation
- Examples: Cohort studies, case-control studies, cross-sectional surveys

> **Important: Correlation Does Not Imply Causation**
>
> - **Correlation**: Measures the strength and direction of a relationship between variables
> - **Causation**: Indicates that changes in one variable directly cause changes in another
>
> While strong correlations can suggest potential causal links, additional evidence and rigorous methods are required to establish causality.

### Challenges in Establishing Causality

- **Confounding variables**: Unmeasured factors that affect both the presumed cause and effect
- **Reverse causality**: The presumed effect might actually be causing the presumed cause
- **Selection bias**: Non-random selection of subjects into study groups

### Methods to Strengthen Causal Claims

1. Randomized controlled trials (when ethical and feasible)
2. Natural experiments or quasi-experimental designs
3. Propensity score matching
4. Difference-in-differences analysis
5. Instrumental variable approaches
6. Directed acyclic graphs (DAGs) for visualizing causal relationships

Understanding these distinctions is crucial in social sciences, where ethical considerations often limit experimental manipulation.

## Appendix C: Understanding Spurious Correlations, Confounders, and Colliders

These concepts are essential for avoiding misinterpretations in statistical analysis. Let's explore them with R examples.

```{r}
#| message: false
#| code-fold: true
#| code-summary: "Load required libraries"

library(tidyverse)
library(viridis)
set.seed(123) # for reproducibility
```

### Spurious Correlations

Spurious correlations are relationships between variables that appear to be causal but are actually coincidental or caused by an unseen third factor.

**Example: Ice Cream Sales and Drowning Incidents**

```{r}
#| code-fold: true
#| code-summary: "R code for spurious correlation example"

# Create dataset
n <- 100
spurious_data <- tibble(
  temperature = rnorm(n, mean = 25, sd = 5),
  ice_cream_sales = 100 + 5 * temperature + rnorm(n, sd = 10),
  drowning_incidents = 1 + 0.5 * temperature + rnorm(n, sd = 2)
)

# Plot the apparent correlation
p1 <- ggplot(spurious_data, aes(x = ice_cream_sales, y = drowning_incidents)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "#0072B2") +
  labs(title = "Spurious Correlation: Ice Cream Sales vs. Drowning",
       x = "Ice Cream Sales", 
       y = "Drowning Incidents") +
  theme_minimal()

# Show the common cause
p2 <- ggplot(spurious_data, aes(x = temperature)) +
  geom_point(aes(y = ice_cream_sales), color = "#D55E00", alpha = 0.7) +
  geom_point(aes(y = drowning_incidents * 10), color = "#0072B2", alpha = 0.7) +
  geom_smooth(aes(y = ice_cream_sales), method = "lm", 
              se = FALSE, color = "#D55E00") +
  geom_smooth(aes(y = drowning_incidents * 10), method = "lm", 
              se = FALSE, color = "#0072B2") +
  scale_y_continuous(
    name = "Ice Cream Sales",
    sec.axis = sec_axis(~./10, name = "Drowning Incidents")
  ) +
  labs(title = "Temperature as the Common Cause",
       x = "Temperature (°C)") +
  theme_minimal() +
  theme(
    axis.title.y.left = element_text(color = "#D55E00"),
    axis.title.y.right = element_text(color = "#0072B2")
  )

# Calculate correlation
cor_value <- cor(spurious_data$ice_cream_sales, spurious_data$drowning_incidents)

# Display plots
print(p1)
print(p2)
cat("Correlation between ice cream sales and drowning incidents:", round(cor_value, 3))
```

In this example, temperature is the common cause (confounder) that influences both ice cream sales and drowning incidents. When we plot them against each other, they appear correlated (r ≈ 0.5), but this is spurious. The relationship disappears when we control for temperature.

### Confounders

A confounder is a variable that influences both the dependent variable and independent variable, causing a spurious association.

**Example: Education, Income, and Age**

```{r}
#| code-fold: true
#| code-summary: "R code for confounder example"

# Create dataset
n <- 1000
confounder_data <- tibble(
  age = runif(n, 25, 65),
  education = round(10 + 0.1 * age + rnorm(n, sd = 2)),
  income = 20000 + 1000 * education + 500 * age + rnorm(n, sd = 5000)
)

# Create age groups for visualization
confounder_data <- confounder_data %>%
  mutate(age_group = cut(age, breaks = 3, 
                         labels = c("Young", "Middle", "Older")))

# Models with and without controlling for the confounder
model_naive <- lm(income ~ education, data = confounder_data)
model_adjusted <- lm(income ~ education + age, data = confounder_data)

# Visualization
ggplot(confounder_data, aes(x = education, y = income)) +
  geom_point(aes(color = age), alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "black", linewidth = 1.2) +
  geom_smooth(aes(group = age_group, color = as.numeric(age_group)), 
              method = "lm", se = FALSE, linewidth = 1) +
  scale_color_viridis_c(name = "Age", 
                      breaks = c(30, 45, 60), 
                      labels = c("Young", "Middle", "Older")) +
  labs(title = "Education vs Income, Confounded by Age",
       subtitle = paste("Without controlling for age: effect =", 
                        round(coef(model_naive)["education"], 1),
                        "| With age control: effect =", 
                        round(coef(model_adjusted)["education"], 1)),
       x = "Years of Education", 
       y = "Income") +
  theme_minimal()
```

In this example, age is a confounder in the relationship between education and income. Without controlling for age, we overestimate the effect of education on income (the black line). When we examine the relationship within specific age groups (colored lines), we see a more accurate representation of the true effect.

### Colliders

A collider is a variable that is influenced by both the independent variable and the dependent variable. Controlling for a collider can introduce a spurious correlation.

**Example: Job Satisfaction, Salary, and Work-Life Balance**

```{r}
#| code-fold: true
#| code-summary: "R code for collider example"

# Create dataset
n <- 1000
collider_data <- tibble(
  job_satisfaction = rnorm(n),
  salary = rnorm(n),
  # Both job satisfaction and salary negatively affect work-life balance
  work_life_balance = -0.5 * job_satisfaction - 0.5 * salary + rnorm(n, sd = 0.5)
)

# Without controlling for work-life balance
model_correct <- lm(salary ~ job_satisfaction, data = collider_data)

# Incorrectly controlling for the collider
model_collider <- lm(salary ~ job_satisfaction + work_life_balance, data = collider_data)

# Visualization
p <- ggplot(collider_data, aes(x = job_satisfaction, y = salary, 
                          color = work_life_balance)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  scale_color_viridis_c(name = "Work-Life\nBalance") +
  labs(title = "Job Satisfaction vs Salary, with Work-Life Balance as Collider",
       subtitle = paste("Without controlling: correlation =", 
                        round(coef(model_correct)["job_satisfaction"], 3),
                        "| With control: correlation =", 
                        round(coef(model_collider)["job_satisfaction"], 3)),
       x = "Job Satisfaction", 
       y = "Salary") +
  theme_minimal()

print(p)
```

In this example, there's no inherent relationship between job satisfaction and salary (the black line shows near-zero correlation). However, both variables negatively impact work-life balance. If we control for work-life balance (the collider), we introduce a positive correlation between job satisfaction and salary that doesn't actually exist.

### Simpson's Paradox

Simpson's paradox occurs when a trend appears in different groups of data but disappears or reverses when these groups are combined.

**Example: Treatment Effectiveness Across Age Groups**

```{r}
#| code-fold: true
#| code-summary: "R code for Simpson's paradox example"

# Create example dataset
set.seed(123)
n <- 1000

simpson_data <- tibble(
  age_group = sample(c("Young", "Older"), n, replace = TRUE, 
                     prob = c(0.7, 0.3)),
  treatment = sample(c("Treatment A", "Treatment B"), n, replace = TRUE,
                    prob = c(0.5, 0.5))
) %>%
  mutate(
    # Different recovery rates based on age and treatment
    recovery_prob = case_when(
      age_group == "Young" & treatment == "Treatment A" ~ 0.70,
      age_group == "Young" & treatment == "Treatment B" ~ 0.80,
      age_group == "Older" & treatment == "Treatment A" ~ 0.50,
      age_group == "Older" & treatment == "Treatment B" ~ 0.40,
      TRUE ~ 0
    ),
    # More older people get Treatment A
    treatment = if_else(
      age_group == "Older" & runif(n) < 0.7, 
      "Treatment A", 
      treatment
    ),
    # Generate recovery outcomes
    recovered = rbinom(n, 1, recovery_prob)
  )

# Aggregate data
overall_rates <- simpson_data %>%
  group_by(treatment) %>%
  summarize(
    total_patients = n(),
    recovered_patients = sum(recovered),
    recovery_rate = mean(recovered)
  )

by_age_rates <- simpson_data %>%
  group_by(treatment, age_group) %>%
  summarize(
    total_patients = n(),
    recovered_patients = sum(recovered),
    recovery_rate = mean(recovered)
  )

# Create visualization
overall_plot <- ggplot(overall_rates, 
                      aes(x = treatment, y = recovery_rate, fill = treatment)) +
  geom_col() +
  geom_text(aes(label = paste0(round(recovery_rate*100, 1), "%")), 
            vjust = -0.5) +
  scale_y_continuous(labels = scales::percent, limits = c(0, 0.8)) +
  labs(title = "Overall Recovery Rates",
       subtitle = "Simpson's Paradox: Treatment B appears worse overall",
       x = "", y = "Recovery Rate") +
  theme_minimal() +
  theme(legend.position = "none")

by_age_plot <- ggplot(by_age_rates, 
                     aes(x = treatment, y = recovery_rate, fill = treatment)) +
  geom_col() +
  geom_text(aes(label = paste0(round(recovery_rate*100, 1), "%")), 
            vjust = -0.5) +
  scale_y_continuous(labels = scales::percent, limits = c(0, 0.8)) +
  labs(title = "Recovery Rates by Age Group",
       subtitle = "Within each age group, Treatment B is actually better",
       x = "", y = "Recovery Rate") +
  facet_wrap(~age_group) +
  theme_minimal() +
  theme(legend.position = "none")

# Display tables and plots
knitr::kable(overall_rates, caption = "Overall Recovery Rates by Treatment")
knitr::kable(by_age_rates, caption = "Recovery Rates by Treatment and Age Group")
print(overall_plot)
print(by_age_plot)
```

Simpson's paradox is occurring here because:

1. **Within each age group**: Treatment B has a higher recovery rate than Treatment A
2. **Overall**: Treatment A appears to have a higher recovery rate than Treatment B

This paradox happens because:
- Treatment A is given more frequently to older patients
- Older patients have lower recovery rates regardless of treatment
- This skews the overall average to make Treatment A look better, even though Treatment B is better for both young and older patients

### Directed Acyclic Graphs (DAGs)

DAGs are powerful tools for visualizing causal relationships and identifying potential biases in statistical analyses.

```{r}
#| code-fold: true
#| code-summary: "R code for DAG examples"

# Try to load dagitty and ggdag if available
if (requireNamespace("dagitty", quietly = TRUE) && 
    requireNamespace("ggdag", quietly = TRUE)) {
  
  library(dagitty)
  library(ggdag)
  
  # Example 1: Confounder
  confounder_dag <- dagitty('dag {
    X -> Y
    Z -> X
    Z -> Y
  }')
  
  # Example 2: Collider
  collider_dag <- dagitty('dag {
    X -> Z
    Y -> Z
    X -- Y [unobserved]
  }')
  
  # Example 3: Simpson's Paradox
  simpson_dag <- dagitty('dag {
    Treatment -> Recovery
    Age -> Treatment
    Age -> Recovery
  }')
  
  # Plot the DAGs
  p1 <- ggdag(confounder_dag) + 
    theme_dag() + 
    labs(title = "Confounder (Z)")
  
  p2 <- ggdag(collider_dag) + 
    theme_dag() + 
    labs(title = "Collider (Z)")
  
  p3 <- ggdag(simpson_dag) + 
    theme_dag() + 
    labs(title = "Simpson's Paradox Structure")
  
  print(p1)
  print(p2)
  print(p3)
  
} else {
  cat("DAG visualization packages not installed. Install dagitty and ggdag packages for these examples.")
}
```

DAGs help us visualize different causal structures:

1. **Confounder**: A variable (Z) that affects both the exposure (X) and outcome (Y)
2. **Collider**: A variable (Z) that is affected by both the exposure (X) and another variable (Y)
3. **Simpson's Paradox**: Often involves a confounder that influences both the treatment/exposure and the outcome

Understanding these structures helps us decide which variables to control for in our analyses and which to leave out.

## Appendix D: Models in Science: From Deterministic to Stochastic

Models are essential tools in scientific research, helping scientists to represent, understand, and predict complex phenomena.

### Types of Models

#### Mathematical Models

Mathematical models use equations to describe and analyze systems. They can be divided into:

##### Deterministic Models

Deterministic models provide precise predictions based on a set of variables, without incorporating randomness.

**Example:** Newton's laws of motion, which can precisely predict the motion of objects under known forces:

$$x(t) = x_0 + v_0t + \frac{1}{2}at^2$$

Where $x(t)$ is the position at time $t$, $x_0$ is the initial position, $v_0$ is the initial velocity, and $a$ is the acceleration.

##### Stochastic Models

Stochastic models incorporate randomness and probability. They come in two fundamentally different types:

**Classical Stochastic Models**: Deal with randomness arising from incomplete information or complex interactions in classical systems. The underlying system is deterministic, but practical limitations lead to probabilistic descriptions.

**Example:** Regression models in statistics, where the randomness represents unexplained variation:

$$y = \beta_0 + \beta_1x + \varepsilon$$

Where $y$ is the dependent variable, $x$ is the independent variable, $\beta_0$ and $\beta_1$ are parameters, and $\varepsilon$ is the error term.

**Quantum Stochastic Models**: Deal with the fundamental, irreducible randomness inherent in quantum mechanical systems.

**Example:** The decay of a radioactive particle follows a probability distribution:

$$P(t) = e^{-t/\tau}$$

Where $P(t)$ is the probability that the particle has not decayed after time $t$, and $\tau$ is the mean lifetime of the particle.

#### Other Model Types

- **Computer Simulation Models**: Use algorithms to simulate complex systems
- **Conceptual Models**: Abstract representations using diagrams or flowcharts
- **Physical Models**: Tangible representations like scale models
- **Theoretical Models**: Abstract frameworks based on fundamental principles

### Model Error and Bias-Variance Tradeoff

All models involve some degree of error. Understanding the balance between bias and variance is crucial:

- **Bias**: Systematic error from simplifying assumptions
- **Variance**: Error from sensitivity to small fluctuations in the training data

![Bias-Variance Tradeoff in Models](stat_imgs/ModelError.png)

The ideal model balances complexity to minimize both bias and variance, leading to the best predictive performance.

## Appendix E: Classical vs Quantum Randomness

To understand how randomness differs across scientific disciplines, we need to examine the origins and implications of different types of uncertainty.

### Origin of Randomness

#### Classical Randomness (e.g., Regression Models)

- **Source**: Incomplete information or complex interactions in an otherwise deterministic system
- **Nature**: Epistemic uncertainty (due to lack of knowledge)
- **Example**: In a regression model, the error term represents unexplained variation

#### Quantum Randomness

- **Source**: Fundamental property of quantum systems
- **Nature**: Ontic uncertainty (inherent to the system, not due to lack of knowledge)
- **Example**: The exact time of decay of a radioactive atom cannot be predicted

### Philosophical Implications

#### Classical Randomness

- **Determinism**: Underlying reality is deterministic; randomness reflects our ignorance
- **Hidden Variables**: In principle, with complete information, we could predict outcomes precisely

#### Quantum Randomness

- **Indeterminism**: Randomness is a fundamental feature of reality
- **No Hidden Variables**: Even with complete information, some outcomes remain unpredictable (as suggested by Bell's theorem)

### Practical Implications

#### Classical Randomness

- **Reducible**: Can be reduced by gathering more data or improving measurement precision
- **Controllable**: Systematic errors can be identified and corrected

#### Quantum Randomness

- **Irreducible**: Cannot be eliminated even with perfect measurements
- **Fundamentally Uncontrollable**: The act of measurement itself affects the system

Understanding these differences is crucial for correctly interpreting statistical models in different scientific contexts.

## Appendix F: Ethical Considerations in Social Science Data Analysis

Ethics play a vital role in social science research. Key considerations include:

### 1. Privacy and Consent

- Ensure participants understand how their data will be used
- Obtain informed consent before collecting data
- Protect personally identifiable information
- Consider cultural differences in privacy expectations

### 2. Data Protection

- Securely store sensitive data
- Implement appropriate access controls
- Follow relevant regulations (e.g., GDPR, HIPAA)
- Have a data management plan that includes secure disposal

### 3. Bias and Representation

- Address sampling bias that could exclude marginalized groups
- Ensure diverse representation in research
- Consider how variable definitions might reflect social biases
- Be transparent about limitations in population coverage

### 4. Transparency and Reproducibility

- Clearly document research methods
- Share code and data when possible
- Pre-register studies when appropriate
- Acknowledge limitations and potential biases

### 5. Social Impact

- Consider the potential societal implications of research findings
- Avoid reinforcing harmful stereotypes
- Think about how results might be misinterpreted or misused
- Engage with communities being studied

Ethical considerations should be integrated throughout the research process, from study design to data collection, analysis, and reporting of results.

## Appendix G: Introduction to RStudio and the tidyverse

R is a powerful programming language for statistical computing and graphics. RStudio provides an integrated development environment that makes working with R easier.

### Getting Started with RStudio

RStudio has four main panes:

1. **Source Editor**: Where you write and edit your R scripts
2. **Console**: Where you run R commands and see output
3. **Environment/History**: Shows your workspace objects and command history
4. **Files/Plots/Packages/Help**: For file management, viewing plots, managing packages, and accessing help

### The tidyverse Ecosystem

The tidyverse is a collection of R packages designed for data science with a consistent design philosophy.

Key packages include:

- **ggplot2**: Data visualization
- **dplyr**: Data manipulation
- **tidyr**: Data tidying
- **readr**: Data import
- **purrr**: Functional programming
- **tibble**: Modern data frames

### Basic tidyverse Workflow

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Click to show/hide R code"

# Load tidyverse
library(tidyverse)

# Read data
data <- read_csv("my_data.csv")

# Clean and transform
cleaned_data <- data %>%
  filter(!is.na(important_variable)) %>%
  select(var1, var2, var3) %>%
  mutate(new_var = var1 / var2)

# Group and summarize
summary_stats <- cleaned_data %>%
  group_by(category) %>%
  summarize(
    mean_val = mean(var3),
    count = n()
  )

# Visualize
ggplot(cleaned_data, aes(x = var1, y = var2, color = category)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Relationship between Variables",
       x = "Variable 1",
       y = "Variable 2") +
  theme_minimal()
```

This workflow demonstrates the power of the tidyverse's pipe operator (`%>%`), which allows you to chain operations together in a readable way.

### Resources for Learning R

- [R for Data Science](https://r4ds.had.co.nz/)
- [tidyverse documentation](https://www.tidyverse.org/)
- [RStudio Cheat Sheets](https://www.rstudio.com/resources/cheatsheets/)
- [Quarto Guide](https://quarto.org/docs/guide/)

The best way to learn R is through practice. Start with small, manageable projects and gradually build your skills.

