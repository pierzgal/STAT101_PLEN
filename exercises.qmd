---
title: "Exercises (Test)"
author: "Michał Pierzgalski"
date: today
editor: 
  markdown: 
    wrap: 72
---


## Exercise 1. Identifying Measurement Scales

For each of the following variables, determine the most appropriate scale of measurement (**Nominal, Ordinal, Interval, or Ratio**). Also evaluate whether the variable is discrete or continuous.

1.  Gender: ***nominal level of measurement***, and ***discrete***;
2.  Customer satisfaction: Poor, Fair, Good, Excellent
3.  Height (questionnaire): "I am: very short, short, average, tall, very tall"
4.  Height (inches)
5.  Reaction time (milliseconds)
6.  Postal codes: e.g., 61548, 61761, 62461, 47424, 65233
7.  Age (years)
8.  Nationality
9.  Street addresses
10. Military ranks
11. Left-Right political scale placement
12. Family size: 1 child, 2 children, 3 children, ...
13. IQ score
14. Shirt size (S, M, L, ...)
15. Movie ratings (1 star, 2 stars, 3 stars)
16. Temperature (Celsius)
17. Temperature (Kelvin)
18. Blood types: A, B, AB, O
19. Income categories: low, medium, high
20. Voter turnout
21. Political party affiliation
22. Electoral district magnitude

Remember to justify your choices for each variable.

**For instance**: In Stevens' typology of measurement scales, street addresses are nominal data. This is because:

They serve purely as labels/identifiers. They have no inherent ordering (123 Main St isn't "more than" 23 Oak St). You can't perform meaningful mathematical operations on them.The only valid operation is testing for equality/inequality (is this the same address or different?)

---

## Exercise 2. Center and dispersion of data

```{r setup, echo=FALSE}
# Create the data (hidden from students)
company_x <- c(2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35)
company_y <- c(3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8)
```

### Data

We have salary data (in thousands of euros) from two small European companies:

| Index | Company X | Company Y |
|-------|-----------|-----------|
| 1     | 2         | 3         |
| 2     | 2         | 3         |
| 3     | 2         | 4         |
| 4     | 3         | 4         |
| 5     | 3         | 4         |
| 6     | 3         | 4         |
| 7     | 3         | 4         |
| 8     | 3         | 4         |
| 9     | 3         | 5         |
| 10    | 4         | 5         |
| 11    | 4         | 5         |
| 12    | 4         | 5         |
| 13    | 4         | 5         |
| 14    | 4         | 5         |
| 15    | 5         | 6         |
| 16    | 5         | 6         |
| 17    | 5         | 6         |
| 18    | 5         | 7         |
| 19    | 20        | 7         |
| 20    | 35        | 8         |

This table presents the data for both Company X and Company Y side by side, with an index column for easy reference.

### Tasks

**A. Measures of Central Tendency**

1. Calculate the **mean** salary for Company X and Company Y.
2. Calculate the **median** salary for both companies.
3. Find the **mode** for each company's salary distribution.
4. Compare the mean and median for Company X. What does the difference between these values tell you about the data?

**B. Quartiles and Percentiles**

5. Calculate the first quartile (Q1), second quartile (Q2), and third quartile (Q3) for both companies.
6. Verify that Q2 equals the median you calculated in question 2.

**C. Measures of Dispersion**

7. Calculate the **range** for both companies.
8. Calculate the **interquartile range (IQR)** for both companies.
9. Calculate the **variance** and **standard deviation** for both companies.
10. Which company has more variability in salaries? Support your answer with the appropriate statistics.

**D. Outlier Detection**

11. Use the IQR method to identify potential outliers in both companies. (Recall: outliers are values below Q1 - 1.5×IQR or above Q3 + 1.5×IQR)
12. How do the outliers in Company X affect the mean? Calculate the mean without the outliers and compare.

**E. Boxplots**

13. Draw a boxplot for Company X and Company Y (you may draw them side by side for comparison).
14. Label the following on each boxplot: minimum, Q1, median, Q3, maximum, and any outliers.

**F. Interpretation and Comparison**

15. If you were reporting the "average" salary for Company X to potential employees, would you use the mean or the median? Justify your choice.
16. Which company appears to have a more equitable salary distribution? Support your answer with statistical evidence.

---

## Solutions

### A. Measures of Central Tendency

**Question 1: Calculate the mean salary for Company X and Company Y**

The mean is calculated using the formula: $\bar{x} = \frac{\sum x_i}{n}$

**Company X:**

Step 1: Add all salary values

$$\sum x_i = 2 + 2 + 2 + 3 + 3 + 3 + 3 + 3 + 3 + 4 + 4 + 4 + 4 + 4 + 5 + 5 + 5 + 5 + 20 + 35$$

Let's organize this by grouping repeated values:

- $2 \times 3 = 6$
- $3 \times 6 = 18$
- $4 \times 5 = 20$
- $5 \times 4 = 20$
- $20 \times 1 = 20$
- $35 \times 1 = 35$

Total: $6 + 18 + 20 + 20 + 20 + 35 = 119$

Step 2: Divide by the number of observations (n = 20)

$$\bar{x} = \frac{119}{20} = 5.95 \text{ thousand euros}$$

**Company Y:**

Step 1: Add all salary values

$$\sum y_i = 3 + 3 + 4 + 4 + 4 + 4 + 4 + 4 + 5 + 5 + 5 + 5 + 5 + 5 + 6 + 6 + 6 + 7 + 7 + 8$$

Grouping repeated values:

- $3 \times 2 = 6$
- $4 \times 6 = 24$
- $5 \times 6 = 30$
- $6 \times 3 = 18$
- $7 \times 2 = 14$
- $8 \times 1 = 8$

Total: $6 + 24 + 30 + 18 + 14 + 8 = 100$

Step 2: Divide by n = 20

$$\bar{y} = \frac{100}{20} = 5.00 \text{ thousand euros}$$

```{r verify-mean, echo=TRUE}
# Verification with R
mean_x <- mean(company_x)
mean_y <- mean(company_y)
cat("Company X mean:", mean_x, "thousand euros\n")
cat("Company Y mean:", mean_y, "thousand euros\n")
```

---

**Question 2: Calculate the median salary for both companies**

The median is the middle value when data is arranged in order. For an even number of observations (n = 20), the median is the average of the 10th and 11th values.

**Company X:**

Step 1: Arrange data in ascending order

| Position | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 |
|----------|---|---|---|---|---|---|---|---|---|----|----|----|----|----|----|----|----|----|----|----|
| Salary   | 2 | 2 | 2 | 3 | 3 | 3 | 3 | 3 | 3 | 4  | 4  | 4  | 4  | 4  | 5  | 5  | 5  | 5  | 20 | 35 |

Step 2: Identify the middle values (positions 10 and 11)

Position 10 = 4, Position 11 = 4

Step 3: Calculate the median

$$\text{Median}_X = \frac{4 + 4}{2} = \frac{8}{2} = 4 \text{ thousand euros}$$

**Company Y:**

Step 1: Arrange data in ascending order

| Position | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 |
|----------|---|---|---|---|---|---|---|---|---|----|----|----|----|----|----|----|----|----|----|----|
| Salary   | 3 | 3 | 4 | 4 | 4 | 4 | 4 | 4 | 5 | 5  | 5  | 5  | 5  | 5  | 6  | 6  | 6  | 7  | 7  | 8  |

Step 2: Identify the middle values (positions 10 and 11)

Position 10 = 5, Position 11 = 5

Step 3: Calculate the median

$$\text{Median}_Y = \frac{5 + 5}{2} = \frac{10}{2} = 5 \text{ thousand euros}$$

```{r verify-median, echo=TRUE}
# Verification with R
median_x <- median(company_x)
median_y <- median(company_y)
cat("Company X median:", median_x, "thousand euros\n")
cat("Company Y median:", median_y, "thousand euros\n")
```

---

**Question 3: Find the mode for each company's salary distribution**

The mode is the value that appears most frequently in the dataset.

**Company X:**

Step 1: Create a frequency table

| Salary | 2 | 3 | 4 | 5 | 20 | 35 |
|--------|---|---|---|---|----|-----|
| Frequency | 3 | 6 | 5 | 4 | 1  | 1   |

Step 2: Identify the highest frequency

The highest frequency is 6, which corresponds to a salary of 3.

**Mode for Company X = 3 thousand euros**

**Company Y:**

Step 1: Create a frequency table

| Salary | 3 | 4 | 5 | 6 | 7 | 8 |
|--------|---|---|---|---|---|---|
| Frequency | 2 | 6 | 6 | 3 | 2 | 1 |

Step 2: Identify the highest frequency

The highest frequency is 6, which appears for both 4 and 5.

**Mode for Company Y = 4 and 5 thousand euros (bimodal distribution)**

```{r verify-mode, echo=TRUE}
# Verification with R
get_mode <- function(x) {
  freq_table <- table(x)
  max_freq <- max(freq_table)
  modes <- as.numeric(names(freq_table)[freq_table == max_freq])
  modes
}
cat("Company X mode:", get_mode(company_x), "thousand euros\n")
cat("Company Y mode(s):", get_mode(company_y), "thousand euros\n")
```

---

**Question 4: Compare the mean and median for Company X**

**Comparison:**

- Mean = 5.95 thousand euros
- Median = 4.00 thousand euros
- Difference = 5.95 - 4.00 = 1.95 thousand euros

**Interpretation:**

The mean is substantially higher than the median (by almost 2 thousand euros or 49%). This tells us that the data is **positively skewed** (skewed to the right). 

The presence of two very high salaries (20 and 35 thousand euros) pulls the mean upward, while the median remains unaffected by these extreme values. The median better represents the "typical" salary in Company X, as 50% of employees earn 4 thousand euros or less.

This pattern is common in salary data where a few high earners (executives, owners) earn substantially more than the majority of employees.

---

### B. Quartiles and Percentiles

**Question 5: Calculate Q1, Q2, and Q3 for both companies**

We will use the **split-and-median method** (Type 2) for calculating quartiles. For an even number of observations, this method excludes the median values when finding Q1 and Q3.

**Split-and-median method:**

1. Find the median (Q2) of the entire dataset
2. For Q1: Find the median of the lower half, **excluding** the values used in the overall median
3. For Q3: Find the median of the upper half, **excluding** the values used in the overall median

---

**Company X (n = 20, sorted data):**

Data: 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35

**Step 1: Find Q2 (Median)**

The median uses positions 10 and 11: both are 4

$$Q2 = \frac{4 + 4}{2} = 4 \text{ thousand euros}$$

**Step 2: Find Q1**

Lower half (positions 1-9, excluding positions 10-11): 2, 2, 2, 3, 3, 3, 3, 3, 3

With 9 values, the median is the 5th value.

$$Q1 = 3 \text{ thousand euros}$$

**Step 3: Find Q3**

Upper half (positions 12-20, excluding positions 10-11): 4, 4, 4, 5, 5, 5, 5, 20, 35

With 9 values, the median is the 5th value.

$$Q3 = 5 \text{ thousand euros}$$

---

**Company Y (n = 20, sorted data):**

Data: 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8

**Step 1: Find Q2 (Median)**

The median uses positions 10 and 11: both are 5

$$Q2 = \frac{5 + 5}{2} = 5 \text{ thousand euros}$$

**Step 2: Find Q1**

Lower half (positions 1-9, excluding positions 10-11): 3, 3, 4, 4, 4, 4, 4, 4, 5

With 9 values, the median is the 5th value.

$$Q1 = 4 \text{ thousand euros}$$

**Step 3: Find Q3**

Upper half (positions 12-20, excluding positions 10-11): 5, 5, 5, 6, 6, 6, 7, 7, 8

With 9 values, the median is the 5th value.

$$Q3 = 6 \text{ thousand euros}$$

---

**Summary of Quartiles:**

| Company | Q1 | Q2 (Median) | Q3 |
|---------|-------|-------------|-------|
| X       | 3     | 4           | 5     |
| Y       | 4     | 5           | 6     |

```{r verify-quartiles, echo=TRUE}
# Verification with R (using split-and-median method - type 2)
q_x <- quantile(company_x, probs = c(0.25, 0.50, 0.75), type = 2)
q_y <- quantile(company_y, probs = c(0.25, 0.50, 0.75), type = 2)

cat("Company X quartiles (split-and-median method):\n")
print(q_x)

cat("\nCompany Y quartiles (split-and-median method):\n")
print(q_y)
```

---

**Question 6: Verify that Q2 equals the median**

**Verification:**

**Company X:**

- Q2 = 4 thousand euros
- Median = 4 thousand euros
- ✓ **Confirmed: Q2 = Median**

**Company Y:**

- Q2 = 5 thousand euros  
- Median = 5 thousand euros
- ✓ **Confirmed: Q2 = Median**

This is always true by definition: the second quartile (Q2) is the value that divides the dataset in half, which is exactly what the median does.

---

### C. Measures of Dispersion

**Question 7: Calculate the range for both companies**

The range measures the spread between the smallest and largest values.

Formula: $\text{Range} = \text{Maximum} - \text{Minimum}$

**Company X:**

$$\text{Range}_X = 35 - 2 = 33 \text{ thousand euros}$$

**Company Y:**

$$\text{Range}_Y = 8 - 3 = 5 \text{ thousand euros}$$

**Interpretation:** Company X has a range 6.6 times larger than Company Y, indicating much greater spread in salaries.

```{r verify-range, echo=TRUE}
# Verification with R
range_x <- max(company_x) - min(company_x)
range_y <- max(company_y) - min(company_y)
cat("Company X range:", range_x, "thousand euros\n")
cat("Company Y range:", range_y, "thousand euros\n")
```

---

**Question 8: Calculate the interquartile range (IQR)**

The IQR measures the spread of the middle 50% of the data.

Formula: $\text{IQR} = Q3 - Q1$

**Company X:**

$$\text{IQR}_X = 5 - 3 = 2 \text{ thousand euros}$$

**Company Y:**

$$\text{IQR}_Y = 6 - 4 = 2 \text{ thousand euros}$$

**Interpretation:** Both companies have the same IQR of 2 thousand euros, meaning the middle 50% of employees in each company have the same salary spread. However, this doesn't tell the whole story about overall variability—Company X's range and standard deviation are much larger due to extreme outliers.

```{r verify-iqr, echo=TRUE}
# Verification with R (using type 2 to match our manual calculations)
iqr_x <- IQR(company_x, type = 2)
iqr_y <- IQR(company_y, type = 2)
cat("Company X IQR:", iqr_x, "thousand euros\n")
cat("Company Y IQR:", iqr_y, "thousand euros\n")
```

---

**Question 9: Calculate the variance and standard deviation**

**Company X:**

Step 1: Calculate deviations from the mean ($x_i - \bar{x}$) where $\bar{x} = 5.95$

| Salary ($x_i$) | Frequency | Deviation ($x_i - 5.95$) | $(x_i - 5.95)^2$ | Frequency × $(x_i - 5.95)^2$ |
|----------------|-----------|-------------------------|------------------|------------------------------|
| 2              | 3         | -3.95                   | 15.6025          | 46.8075                      |
| 3              | 6         | -2.95                   | 8.7025           | 52.215                       |
| 4              | 5         | -1.95                   | 3.8025           | 19.0125                      |
| 5              | 4         | -0.95                   | 0.9025           | 3.61                         |
| 20             | 1         | 14.05                   | 197.4025         | 197.4025                     |
| 35             | 1         | 29.05                   | 843.9025         | 843.9025                     |

Step 2: Sum of squared deviations

$$\sum (x_i - \bar{x})^2 \times f_i = 46.8075 + 52.215 + 19.0125 + 3.61 + 197.4025 + 843.9025 = 1162.95$$

Step 3: Calculate sample variance

$$s^2 = \frac{\sum (x_i - \bar{x})^2}{n-1} = \frac{1162.95}{19} = 61.21 \text{ (thousand euros)}^2$$

Step 4: Calculate standard deviation

$$s = \sqrt{61.21} = 7.82 \text{ thousand euros}$$

---

**Company Y:**

Step 1: Calculate deviations from the mean ($y_i - \bar{y}$) where $\bar{y} = 5.00$

| Salary ($y_i$) | Frequency | Deviation ($y_i - 5.00$) | $(y_i - 5.00)^2$ | Frequency × $(y_i - 5.00)^2$ |
|----------------|-----------|-------------------------|------------------|------------------------------|
| 3              | 2         | -2.00                   | 4.00             | 8.00                         |
| 4              | 6         | -1.00                   | 1.00             | 6.00                         |
| 5              | 6         | 0.00                    | 0.00             | 0.00                         |
| 6              | 3         | 1.00                    | 1.00             | 3.00                         |
| 7              | 2         | 2.00                    | 4.00             | 8.00                         |
| 8              | 1         | 3.00                    | 9.00             | 9.00                         |

Step 2: Sum of squared deviations

$$\sum (y_i - \bar{y})^2 \times f_i = 8.00 + 6.00 + 0.00 + 3.00 + 8.00 + 9.00 = 34.00$$

Step 3: Calculate sample variance

$$s^2 = \frac{34.00}{19} = 1.79 \text{ (thousand euros)}^2$$

Step 4: Calculate standard deviation

$$s = \sqrt{1.79} = 1.34 \text{ thousand euros}$$

**Summary:**

| Company | Variance | Standard Deviation |
|---------|----------|-------------------|
| X       | 61.21    | 7.82              |
| Y       | 1.79     | 1.34              |

```{r verify-variance-sd, echo=TRUE}
# Verification with R
var_x <- var(company_x)
sd_x <- sd(company_x)
var_y <- var(company_y)
sd_y <- sd(company_y)
cat("Company X - Variance:", round(var_x, 2), ", SD:", round(sd_x, 2), "\n")
cat("Company Y - Variance:", round(var_y, 2), ", SD:", round(sd_y, 2), "\n")
```

---

**Question 10: Which company has more variability?**

**Company X has significantly more variability** in salaries than Company Y. All dispersion measures support this conclusion:

| Measure | Company X | Company Y | Ratio (X/Y) |
|---------|-----------|-----------|-------------|
| Range   | 33        | 5         | 6.6×        |
| IQR     | 2         | 2         | 1.0×        |
| Variance| 61.21     | 1.79      | 34.2×       |
| Std Dev | 7.82      | 1.34      | 5.8×        |

**Interpretation:**

Company X's standard deviation (7.82) is almost 6 times larger than Company Y's (1.34), meaning that salaries in Company X deviate much more from the average. While both companies have the same IQR (2 thousand euros), this only tells us about the middle 50% of the data. Company X's high overall variability is primarily driven by the presence of two extreme outliers (20 and 35 thousand euros), which dramatically increase the variance and standard deviation.

---

### D. Outlier Detection

**Question 11: Identify potential outliers using the IQR method**

The IQR method defines outliers as values that fall outside the "fences":

- **Lower fence** = $Q1 - 1.5 \times \text{IQR}$
- **Upper fence** = $Q3 + 1.5 \times \text{IQR}$

**Company X:**

Step 1: Calculate the fences

Given: $Q1 = 3$, $Q3 = 5$, $\text{IQR} = 2$

$$\text{Lower fence} = 3 - 1.5 \times 2 = 3 - 3 = 0$$

$$\text{Upper fence} = 5 + 1.5 \times 2 = 5 + 3 = 8$$

Step 2: Identify outliers

Any value < 0 or > 8 is an outlier.

Looking at the data: 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, **20**, **35**

**Outliers for Company X: 20 and 35 thousand euros**

---

**Company Y:**

Step 1: Calculate the fences

Given: $Q1 = 4$, $Q3 = 6$, $\text{IQR} = 2$

$$\text{Lower fence} = 4 - 1.5 \times 2 = 4 - 3 = 1$$

$$\text{Upper fence} = 6 + 1.5 \times 2 = 6 + 3 = 9$$

Step 2: Identify outliers

Any value < 1 or > 9 is an outlier.

Looking at the data: 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8

All values fall within the range [1, 9].

**No outliers for Company Y!**

```{r verify-outliers, echo=TRUE}
# Verification with R using our quartiles from the split-and-median method
identify_outliers <- function(x, q1, q3) {
  iqr <- q3 - q1
  lower <- q1 - 1.5 * iqr
  upper <- q3 + 1.5 * iqr
  outliers <- x[x < lower | x > upper]
  list(outliers = outliers, lower = lower, upper = upper)
}

out_x <- identify_outliers(company_x, 3, 5)
out_y <- identify_outliers(company_y, 4, 6)

cat("Company X:\n")
cat("  Fences: [", out_x$lower, ",", out_x$upper, "]\n")
cat("  Outliers:", out_x$outliers, "\n\n")

cat("Company Y:\n")
cat("  Fences: [", out_y$lower, ",", out_y$upper, "]\n")
cat("  Outliers:", if(length(out_y$outliers) == 0) "None" else out_y$outliers, "\n")
```

---

**Question 12: How do outliers affect the mean in Company X?**

Let's recalculate the mean after removing the two outliers (20 and 35).

**Original data (n = 20):**

Mean = 5.95 thousand euros (calculated in Question 1)

**Data without outliers (n = 18):**

Remaining data: 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5

Step 1: Sum the remaining values

$$\sum x_i = (2 \times 3) + (3 \times 6) + (4 \times 5) + (5 \times 4)$$

$$= 6 + 18 + 20 + 20 = 64$$

Step 2: Calculate the new mean

$$\bar{x}_{\text{no outliers}} = \frac{64}{18} = 3.56 \text{ thousand euros}$$

**Comparison:**

- Mean with outliers: 5.95 thousand euros
- Mean without outliers: 3.56 thousand euros
- Difference: $5.95 - 3.56 = 2.39$ thousand euros
- Percentage change: $\frac{5.95 - 3.56}{5.95} \times 100 = 40.2\%$

**Interpretation:**

The two outliers increase the mean by 2.39 thousand euros, representing a 40% inflation of the average salary. This demonstrates how sensitive the mean is to extreme values. The mean without outliers (3.56) is actually below the median (4.00), which better represents the typical employee's salary.

```{r verify-mean-no-outliers, echo=TRUE}
# Verification with R
company_x_no_outliers <- company_x[company_x <= 8]
mean_no_outliers <- mean(company_x_no_outliers)
cat("Mean without outliers:", round(mean_no_outliers, 2), "\n")
cat("Difference from original:", round(mean_x - mean_no_outliers, 2), "\n")
cat("Percentage change:", round((mean_x - mean_no_outliers)/mean_x * 100, 1), "%\n")
```

---

### E. Boxplots

**Question 13 & 14: Draw and label boxplots**

**Manual boxplot construction:**

To draw a boxplot, we need the five-number summary plus any outliers:

**Company X:**

1. Minimum (non-outlier): 2
2. Q1: 3
3. Median (Q2): 4
4. Q3: 5
5. Maximum (non-outlier): 5
6. Outliers: 20, 35

**Company Y:**

1. Minimum: 3
2. Q1: 4
3. Median (Q2): 5
4. Q3: 6
5. Maximum: 8
6. Outliers: None

**Boxplot structure:**

- Draw a box from Q1 to Q3 (the box contains the middle 50% of data)
- Draw a line inside the box at the median
- Draw "whiskers" (lines) from the box edges to the minimum and maximum non-outlier values
- Plot outliers as individual points beyond the whiskers

```{r boxplots, fig.width=10, fig.height=6, echo=TRUE}
# Create custom boxplots using our split-and-median quartiles

# Function to calculate boxplot statistics with outlier detection
calc_boxplot_stats <- function(data, q1, q3) {
  iqr <- q3 - q1
  med <- median(data)
  
  # Calculate fences for outlier detection
  lower_fence <- q1 - 1.5 * iqr
  upper_fence <- q3 + 1.5 * iqr
  
  # Identify outliers
  outliers <- data[data < lower_fence | data > upper_fence]
  
  # Calculate whisker endpoints (most extreme non-outlier values)
  non_outliers <- data[data >= lower_fence & data <= upper_fence]
  lower_whisker <- min(non_outliers)
  upper_whisker <- max(non_outliers)
  
  return(list(
    stats = c(lower_whisker, q1, med, q3, upper_whisker),
    outliers = outliers
  ))
}

# Calculate statistics for both companies
stats_x <- calc_boxplot_stats(company_x, 3, 5)
stats_y <- calc_boxplot_stats(company_y, 4, 6)

# Prepare data for bxp()
boxplot_data <- list(
  stats = matrix(c(stats_x$stats, stats_y$stats), nrow = 5, ncol = 2),
  n = c(length(company_x), length(company_y)),
  conf = matrix(c(NA, NA, NA, NA), nrow = 2, ncol = 2),
  out = c(stats_x$outliers, stats_y$outliers),
  group = c(rep(1, length(stats_x$outliers)), rep(2, length(stats_y$outliers))),
  names = c("Company X", "Company Y")
)

# Create the boxplot
par(mar = c(5, 5, 4, 2))
bxp(boxplot_data,
    main = "Salary Distribution Comparison",
    ylab = "Salary (thousands of euros)",
    xlab = "Company",
    col = c("lightblue", "lightgreen"),
    border = c("darkblue", "darkgreen"),
    outcol = "red",
    outpch = 19,
    outcex = 1.5,
    boxwex = 0.5,
    ylim = c(0, 22))

# Add horizontal grid lines
grid(nx = NA, ny = NULL, col = "gray", lty = "dotted")

# Add annotations for Company X
text(0.7, 4, "Median = 4", pos = 2, cex = 0.85, col = "darkblue")
text(0.7, 3, "Q1 = 3", pos = 2, cex = 0.85, col = "darkblue")
text(0.7, 5, "Q3 = 5", pos = 2, cex = 0.85, col = "darkblue")
text(0.7, 2, "Min = 2", pos = 2, cex = 0.85, col = "darkblue")

# Add annotations for Company Y
text(2.3, 5, "Median = 5", pos = 4, cex = 0.85, col = "darkgreen")
text(2.3, 4, "Q1 = 4", pos = 4, cex = 0.85, col = "darkgreen")
text(2.3, 6, "Q3 = 6", pos = 4, cex = 0.85, col = "darkgreen")
text(2.3, 3, "Min = 3", pos = 4, cex = 0.85, col = "darkgreen")
text(2.3, 8, "Max = 8", pos = 4, cex = 0.85, col = "darkgreen")

# Label outliers for Company X
text(0.9, 20, "Outlier: 20", pos = 4, cex = 0.8, col = "red")
text(0.9, 35, "Outlier: 35", pos = 4, cex = 0.8, col = "red")
```

**Note:** R's `quantile()` function uses Type 7 by default (linear interpolation), but our manual calculations follow a simpler split-and-median approach that corresponds to `type = 2` in R. To ensure the boxplot matches our hand calculations exactly, we use the `bxp()` function with manually specified quartiles calculated using `type = 2`. The box edges now represent Q1 and Q3 from our manual calculations, the thick line shows the median, and the whiskers extend to the most extreme non-outlier values.

**Visual interpretation:**

- **Company X**: The box is relatively small (from 3 to 5), indicating that the middle 50% of employees earn within a 2-thousand-euro range. However, the two outliers (shown as red points at 20 and 35) are far above the rest of the data, showing extreme salary inequality. The whisker extends from 2 to 5, representing the non-outlier range.

- **Company Y**: The box spans from 4 to 6, and there are no outliers. The distribution is more uniform with all values relatively close together. The whiskers extend from 3 to 8, showing the full range of the data since there are no outliers to exclude.

- **Comparison**: Company X has a compressed main distribution with extreme outliers, while Company Y has a slightly more spread-out but consistent distribution with no extreme values.

---

### F. Interpretation and Comparison

**Question 15: Which "average" to report for Company X?**

**Answer: Use the MEDIAN (4 thousand euros) rather than the mean (5.95 thousand euros).**

**Justification:**

When reporting to potential employees, the median provides a more honest and representative picture of what they can expect to earn because:

1. **Representativeness**: The median (4 thousand euros) represents the typical employee better. Half of all employees earn at or below this amount. In contrast, only 2 out of 20 employees (10%) earn more than the mean of 5.95.

2. **Resistance to outliers**: The median is not affected by the two extremely high salaries (20 and 35 thousand euros), which likely belong to executives or owners. These outliers pull the mean upward by 2.39 thousand euros (67%).

3. **Accurate expectations**: A potential employee is much more likely to earn around 4 thousand euros than 5.95. Using the mean would create unrealistic expectations and could be considered misleading.

4. **Ethical reporting**: The median provides a more ethical representation of "typical" compensation, avoiding the impression that outlier salaries are achievable for most employees.

**Supporting evidence:**

- 18 out of 20 employees (90%) earn ≤ 5 thousand euros
- The mode is 3 thousand euros (most common salary)
- The median (4) and mode (3) are close together, confirming they represent typical salaries
- The mean is inflated by just 2 extreme values

---

**Question 16: Which company has a more equitable salary distribution?**

**Answer: Company Y has a significantly more equitable salary distribution.**

**Statistical Evidence:**

**1. Measures of dispersion:**

| Statistic | Company X | Company Y | Interpretation |
|-----------|-----------|-----------|----------------|
| Standard Deviation | 7.82 | 1.34 | X has 5.8× more variation |
| IQR | 2.00 | 2.00 | Same variation in middle 50% |
| Range | 33 | 5 | X has 6.6× wider spread |

**2. Mean vs. Median comparison:**

- **Company X**: Mean (5.95) >> Median (4.00), difference of 1.95 (49% higher)
  - This large gap indicates severe positive skewness due to outliers
  
- **Company Y**: Mean (5.00) = Median (5.00), difference of 0.00 (perfectly symmetric!)
  - This **perfect symmetry** indicates an ideally equitable distribution

**3. Coefficient of Variation (CV):**

The CV measures relative variability (standard deviation relative to the mean):

$$CV_X = \frac{s_X}{\bar{x}_X} = \frac{7.82}{5.95} = 1.31 \text{ or } 131\%$$

$$CV_Y = \frac{s_Y}{\bar{y}_Y} = \frac{1.34}{5.00} = 0.27 \text{ or } 27\%$$

Company Y's CV is 4.9 times smaller, indicating much more consistent salaries relative to the average.

**4. Outlier analysis:**

- **Company X**: Two extreme outliers (20, 35) that are 5× and 8.75× the median
- **Company Y**: No outliers—all salaries fall within the expected range [1, 9]

**5. Salary distribution:**

- **Company X**: Most employees (90%) earn 2-5 thousand euros, while 2 individuals earn 20 and 35
- **Company Y**: All employees earn 3-8 thousand euros, with a bimodal distribution centered at 4 and 5

**Conclusion:**

Company Y demonstrates greater salary equity through:

- Lower absolute and relative variability
- Perfectly symmetric distribution (mean = median = 5.00)
- No outliers
- Bimodal distribution with balanced clustering around center values
- Salaries clustered within a narrow, reasonable range
- All employees earn within a 2.67× range (8÷3), compared to Company X's 17.5× range (35÷2)

Company X shows high inequality, with most employees earning 2-5 thousand euros while two individuals earn vastly more, suggesting a hierarchical structure with significant pay disparities between regular employees and leadership.

```{r final-comparison, echo=TRUE}
# Coefficient of Variation comparison
cv_x <- sd(company_x) / mean(company_x)
cv_y <- sd(company_y) / mean(company_y)

cat("Coefficient of Variation:\n")
cat("  Company X:", round(cv_x, 3), "(", round(cv_x * 100, 1), "%)\n")
cat("  Company Y:", round(cv_y, 3), "(", round(cv_y * 100, 1), "%)\n\n")

cat("Summary of Equity Indicators:\n")
cat("  Mean-Median difference (X):", round(mean(company_x) - median(company_x), 2), "\n")
cat("  Mean-Median difference (Y):", round(mean(company_y) - median(company_y), 2), "\n\n")

cat("Five-Number Summary:\n")
cat("Company X: Min = 2, Q1 = 3, Median = 4, Q3 = 5, Max(non-outlier) = 5\n")
cat("           Outliers: 20, 35\n")
cat("Company Y: Min = 3, Q1 = 4, Median = 5, Q3 = 6, Max = 8\n")
cat("           Outliers: None\n")
```

---

**End of Solutions**

---

## Exercise 3a. Comparing Electoral District Size Variation Between Countries
```{r setup_2, include=FALSE}
library(tidyverse)
library(knitr)
options(digits = 4)
```

### Learning Objectives

In this exercise, you will practice calculating and interpreting measures of central tendency and dispersion to compare electoral systems.

### Background

Electoral district magnitude (the number of representatives elected per district) varies significantly across democracies. Some countries use small districts (single-member districts), while others use large districts. Understanding this variation is crucial for analyzing how electoral systems affect representation and party systems.

### Data

We have electoral district size (district magnitude) data from two countries:
```{r data}
x <- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)  # Country X (High var.)
y <- c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13)  # Country Y (Low var.)
kable(data.frame(
  "Country_X" = x,
  "Country_Y" = y
))
```

### Questions

**Part A: Measures of Central Tendency**

Calculate the following for both countries (show your work):

1. Mean
2. Median
3. Mode

**Part B: Measures of Dispersion**

Calculate the following for both countries (show your work):

4. Range
5. Variance
6. Standard deviation
7. Interquartile range (IQR)

**Part C: Interpretation**

8. Which country has more uniform district sizes? Support your answer with specific statistics.
9. What might be the substantive implications of these differences for representation and party competition?

**Part D: Verification (Optional)**

10. Verify your calculations using R functions.

---

### Solutions

#### Part A: Measures of Central Tendency

**Question 1: Mean**

*Country X:*

$$\bar{x} = \frac{\sum_{i=1}^{n} x_i}{n} = \frac{1 + 3 + 5 + 7 + 9 + 11 + 13 + 15 + 17 + 19}{10}$$

$$\bar{x} = \frac{100}{10} = 10$$

*Country Y:*

$$\bar{y} = \frac{8 + 9 + 9 + 10 + 10 + 11 + 11 + 12 + 12 + 13}{10}$$

$$\bar{y} = \frac{105}{10} = 10.5$$

**Question 2: Median**

*Country X:*

The data are already sorted: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19

With $n = 10$ (even), the median is the average of the 5th and 6th values:

$$\text{Median}_X = \frac{9 + 11}{2} = \frac{20}{2} = 10$$

*Country Y:*

The data are already sorted: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13

$$\text{Median}_Y = \frac{10 + 11}{2} = \frac{21}{2} = 10.5$$

**Question 3: Mode**

*Country X:*

Each value appears exactly once, so there is **no mode** (or all values are modes).

*Country Y:*

The values 9, 10, 11, and 12 each appear twice. This distribution is **multimodal** (specifically, it has four modes: 9, 10, 11, and 12).

---

#### Part B: Measures of Dispersion

**Question 4: Range**

*Country X:*

$$\text{Range}_X = \max(x) - \min(x) = 19 - 1 = 18$$

*Country Y:*

$$\text{Range}_Y = \max(y) - \min(y) = 13 - 8 = 5$$

**Question 5: Variance**

*Country X:*

We use the sample variance formula: $s^2 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})^2}{n-1}$

First, calculate deviations from the mean ($\bar{x} = 10$):

| $x_i$ | $x_i - \bar{x}$ | $(x_i - \bar{x})^2$ |
|-------|-----------------|---------------------|
| 1     | -9              | 81                  |
| 3     | -7              | 49                  |
| 5     | -5              | 25                  |
| 7     | -3              | 9                   |
| 9     | -1              | 1                   |
| 11    | 1               | 1                   |
| 13    | 3               | 9                   |
| 15    | 5               | 25                  |
| 17    | 7               | 49                  |
| 19    | 9               | 81                  |
| **Sum** |               | **330**             |

$$s_X^2 = \frac{330}{10-1} = \frac{330}{9} = 36.67$$

*Country Y:*

First, calculate deviations from the mean ($\bar{y} = 10.5$):

| $y_i$ | $y_i - \bar{y}$ | $(y_i - \bar{y})^2$ |
|-------|-----------------|---------------------|
| 8     | -2.5            | 6.25                |
| 9     | -1.5            | 2.25                |
| 9     | -1.5            | 2.25                |
| 10    | -0.5            | 0.25                |
| 10    | -0.5            | 0.25                |
| 11    | 0.5             | 0.25                |
| 11    | 0.5             | 0.25                |
| 12    | 1.5             | 2.25                |
| 12    | 1.5             | 2.25                |
| 13    | 2.5             | 6.25                |
| **Sum** |               | **22.5**            |

$$s_Y^2 = \frac{22.5}{10-1} = \frac{22.5}{9} = 2.50$$

**Question 6: Standard Deviation**

*Country X:*

$$s_X = \sqrt{s_X^2} = \sqrt{36.67} = 6.055$$

*Country Y:*

$$s_Y = \sqrt{s_Y^2} = \sqrt{2.50} = 1.581$$

**Question 7: Interquartile Range (IQR)**

*Country X:*

For $n = 10$, we find quartiles using the median of each half:

Lower half: 1, 3, 5, 7, 9 → $Q_1 = 5$

Upper half: 11, 13, 15, 17, 19 → $Q_3 = 15$

$$\text{IQR}_X = Q_3 - Q_1 = 15 - 5 = 10$$

*Country Y:*

Lower half: 8, 9, 9, 10, 10 → $Q_1 = 9$

Upper half: 11, 11, 12, 12, 13 → $Q_3 = 12$

$$\text{IQR}_Y = Q_3 - Q_1 = 12 - 9 = 3$$

---

#### Part C: Interpretation

**Question 8:**

Country Y has much more uniform district sizes. Evidence:

- **Standard deviation:** Country Y ($s = 1.58$) has much less variation than Country X ($s = 6.06$)
- **Variance:** Country Y ($s^2 = 2.50$) is roughly 15 times smaller than Country X ($s^2 = 36.67$)
- **Range:** Country Y's range (5) is much smaller than Country X's range (18)
- **IQR:** Country Y's IQR (3) is much smaller than Country X's IQR (10)

All dispersion measures consistently show that Country Y has more uniform district sizes.

**Question 9:**

Possible implications:

Country X's high variation suggests a **mixed electoral system** with both small and large districts. This could create:

- Unequal opportunities for representation across districts
- Different party competition dynamics (two-party competition in small districts vs. multiparty in large districts)
- Potential concerns about equality of representation

Country Y's uniform district sizes suggest a more **consistent electoral system**, which may promote:

- More equal representation across regions
- Similar party competition dynamics nationwide
- Greater predictability in electoral outcomes

---

#### Part D: Verification in R

```{r}
# Country X
cat("Country X:\n")
cat("Mean:", mean(x), "\n")
cat("Median:", median(x), "\n")
cat("Variance:", var(x), "\n")
cat("SD:", sd(x), "\n")
cat("Range:", max(x) - min(x), "\n")
cat("IQR:", IQR(x, type = 2), "\n")
cat("Q1:", quantile(x, 0.25, type = 2), "\n")
cat("Q3:", quantile(x, 0.75, type = 2), "\n\n")

# Country Y
cat("Country Y:\n")
cat("Mean:", mean(y), "\n")
cat("Median:", median(y), "\n")
cat("Variance:", var(y), "\n")
cat("SD:", sd(y), "\n")
cat("Range:", max(y) - min(y), "\n")
cat("IQR:", IQR(y, type = 2), "\n")
cat("Q1:", quantile(y, 0.25, type = 2), "\n")
cat("Q3:", quantile(y, 0.75, type = 2), "\n")
```

**Note:** R's `quantile()` function uses Type 7 by default, which employs linear interpolation between order statistics. However, our manual calculations follow a simpler "split-and-median" approach that corresponds to `type = 2` in R. To verify our hand calculations, we explicitly specify `type = 2`. Note that base R's `boxplot()` function uses Type 7, so we use custom code to ensure our boxplot matches the manual calculations.

---


## Exercise 3b. Comparing Electoral District Size Variation Between Countries

```{r, include=FALSE}
library(knitr)
options(digits = 4)
```

### Learning Objectives

In this exercise, you will practice calculating and interpreting measures of central tendency and dispersion to compare electoral systems.

### Background

Electoral district magnitude (the number of representatives elected per district) varies significantly across democracies. Some countries use small districts (single-member districts), while others use large districts. Understanding this variation is crucial for analyzing how electoral systems affect representation and party systems.

### Data

We have electoral district size (district magnitude) data from two countries:

```{r}
x <- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)  # Country X (High var.)
y <- c(8, 9, 9, 10, 10, 11, 11, 11, 12, 16)  # Country Y (Low var.)
kable(data.frame(
  "Country_X" = x,
  "Country_Y" = y
))
```

### Questions

**Part A: Measures of Central Tendency**

Calculate the following for both countries (show your work):

1.  Mean
2.  Median
3.  Mode

**Part B: Measures of Dispersion**

Calculate the following for both countries (show your work):

4.  Range
5.  Variance
6.  Standard deviation
7.  Interquartile range (IQR) using the Tukey method

**Part C: Visualization**

8.  Create side-by-side boxplots to compare the distributions of district sizes.

**Part D: Interpretation**

9.  Which country has more uniform district sizes? Support your answer with specific statistics.
10. What might be the substantive implications of these differences for representation and party competition? How does the outlier in Country Y affect your interpretation?

**Part E: Verification (Optional)**

11. Verify your calculations using R functions.

------------------------------------------------------------------------

### Solutions

#### Part A: Measures of Central Tendency

**Question 1: Mean**

*Country X:*

$$\bar{x} = \frac{\sum_{i=1}^{n} x_i}{n} = \frac{1 + 3 + 5 + 7 + 9 + 11 + 13 + 15 + 17 + 19}{10}$$

$$\bar{x} = \frac{100}{10} = 10$$

*Country Y:*

$$\bar{y} = \frac{8 + 9 + 9 + 10 + 10 + 11 + 11 + 11 + 12 + 16}{10}$$

$$\bar{y} = \frac{107}{10} = 10.7$$

**Question 2: Median**

*Country X:*

The data are already sorted: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19

With $n = 10$ (even), the median is the average of the 5th and 6th values:

$$\text{Median}_X = \frac{9 + 11}{2} = \frac{20}{2} = 10$$

*Country Y:*

The data are already sorted: 8, 9, 9, 10, 10, 11, 11, 11, 12, 16

$$\text{Median}_Y = \frac{10 + 11}{2} = \frac{21}{2} = 10.5$$

**Question 3: Mode**

*Country X:*

Each value appears exactly once, so there is **no mode** (or all values are modes).

*Country Y:*

The value 11 appears three times, more than any other value. The mode is **11**.

------------------------------------------------------------------------

#### Part B: Measures of Dispersion

**Question 4: Range**

*Country X:*

$$\text{Range}_X = \max(x) - \min(x) = 19 - 1 = 18$$

*Country Y:*

$$\text{Range}_Y = \max(y) - \min(y) = 16 - 8 = 8$$

**Question 5: Variance**

*Country X:*

We use the sample variance formula: $s^2 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})^2}{n-1}$

First, calculate deviations from the mean ($\bar{x} = 10$):

| $x_i$   | $x_i - \bar{x}$ | $(x_i - \bar{x})^2$ |
|---------|-----------------|---------------------|
| 1       | -9              | 81                  |
| 3       | -7              | 49                  |
| 5       | -5              | 25                  |
| 7       | -3              | 9                   |
| 9       | -1              | 1                   |
| 11      | 1               | 1                   |
| 13      | 3               | 9                   |
| 15      | 5               | 25                  |
| 17      | 7               | 49                  |
| 19      | 9               | 81                  |
| **Sum** |                 | **330**             |

$$s_X^2 = \frac{330}{10-1} = \frac{330}{9} = 36.67$$

*Country Y:*

First, calculate deviations from the mean ($\bar{y} = 10.7$):

| $y_i$   | $y_i - \bar{y}$ | $(y_i - \bar{y})^2$ |
|---------|-----------------|---------------------|
| 8       | -2.7            | 7.29                |
| 9       | -1.7            | 2.89                |
| 9       | -1.7            | 2.89                |
| 10      | -0.7            | 0.49                |
| 10      | -0.7            | 0.49                |
| 11      | 0.3             | 0.09                |
| 11      | 0.3             | 0.09                |
| 11      | 0.3             | 0.09                |
| 12      | 1.3             | 1.69                |
| 16      | 5.3             | 28.09               |
| **Sum** |                 | **44.1**            |

$$s_Y^2 = \frac{44.1}{10-1} = \frac{44.1}{9} = 4.90$$

**Question 6: Standard Deviation**

*Country X:*

$$s_X = \sqrt{s_X^2} = \sqrt{36.67} = 6.055$$

*Country Y:*

$$s_Y = \sqrt{s_Y^2} = \sqrt{4.90} = 2.214$$

**Question 7: Interquartile Range (IQR) - Tukey Method**

The Tukey method excludes the median when calculating quartiles.

*Country X:*

Data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19

Median is at position 5.5 (between 9 and 11)

Lower half (positions 1-5): 1, 3, 5, 7, 9 → $Q_1 = 5$ (median of lower half)

Upper half (positions 6-10): 11, 13, 15, 17, 19 → $Q_3 = 15$ (median of upper half)

$$\text{IQR}_X = Q_3 - Q_1 = 15 - 5 = 10$$

*Country Y:*

Data: 8, 9, 9, 10, 10, 11, 11, 11, 12, 16

Median is at position 5.5 (between 10 and 11)

Lower half (positions 1-5): 8, 9, 9, 10, 10 → $Q_1 = 9$ (median of lower half)

Upper half (positions 6-10): 11, 11, 11, 12, 16 → $Q_3 = 11$ (median of upper half)

$$\text{IQR}_Y = Q_3 - Q_1 = 11 - 9 = 2$$

------------------------------------------------------------------------

#### Part C: Visualization

**Question 8: Boxplots**

```{r, fig.width=8, fig.height=5}
# Create side-by-side boxplots using manually calculated quartiles (type 2)
# to match our hand calculations

# Function to calculate boxplot statistics with outlier detection
calc_boxplot_stats <- function(data) {
  q1 <- quantile(data, 0.25, type = 2)
  q3 <- quantile(data, 0.75, type = 2)
  iqr <- q3 - q1
  med <- median(data)
  
  # Calculate fences for outlier detection
  lower_fence <- q1 - 1.5 * iqr
  upper_fence <- q3 + 1.5 * iqr
  
  # Identify outliers
  outliers <- data[data < lower_fence | data > upper_fence]
  
  # Calculate whisker endpoints (most extreme non-outlier values)
  non_outliers <- data[data >= lower_fence & data <= upper_fence]
  lower_whisker <- min(non_outliers)
  upper_whisker <- max(non_outliers)
  
  return(list(
    stats = c(lower_whisker, q1, med, q3, upper_whisker),
    outliers = outliers
  ))
}

# Calculate statistics for both countries
stats_x <- calc_boxplot_stats(x)
stats_y <- calc_boxplot_stats(y)

# Prepare data for bxp()
boxplot_data <- list(
  stats = matrix(c(stats_x$stats, stats_y$stats), nrow = 5, ncol = 2),
  n = c(length(x), length(y)),
  conf = matrix(c(NA, NA, NA, NA), nrow = 2, ncol = 2),
  out = c(stats_x$outliers, stats_y$outliers),
  group = c(rep(1, length(stats_x$outliers)), rep(2, length(stats_y$outliers))),
  names = c("Country X", "Country Y")
)

# Create the boxplot with custom statistics
bxp(boxplot_data,
    main = "Comparison of Electoral District Sizes",
    ylab = "District Magnitude (Number of Representatives)",
    xlab = "Country",
    col = c("#66C2A5", "#FC8D62"),
    border = "black",
    las = 1,
    boxwex = 0.6)

# Add a grid for easier reading
grid(nx = NA, ny = NULL, col = "lightgray", lty = "dotted")

# Verify boxplot statistics match our manual calculations
cat("\nBoxplot statistics verification:\n")
cat("Country X - Q1:", quantile(x, 0.25, type = 2), "Q3:", quantile(x, 0.75, type = 2), "\n")
cat("Country Y - Q1:", quantile(y, 0.25, type = 2), "Q3:", quantile(y, 0.75, type = 2), "\n")
cat("Country Y outlier(s):", stats_y$outliers, "\n")
```

The boxplots reveal important features of each distribution:

**Country X:** The boxplot shows a symmetric distribution with the median at the center of the box. The whiskers extend equally in both directions, indicating that the data are evenly spread across the range.

**Country Y:** The boxplot shows a more compact distribution with a notable outlier at 16. Most districts cluster between 8 and 12, but the presence of one very large district (16 representatives) creates asymmetry in the distribution. The outlier is identified using the 1.5×IQR rule.

**Note:** Base R's `boxplot()` function automatically uses the Tukey method (quantile Type 7) for calculating quartiles, which matches exactly with our manual calculations. The box edges represent Q1 and Q3, the thick line shows the median, and the whiskers extend to the most extreme data points within 1.5×IQR from the box edges.

------------------------------------------------------------------------

#### Part D: Interpretation

**Question 9:**

The interpretation is more nuanced than it initially appears:

**Standard metrics suggest Country Y has more uniform sizes:**

-   **Standard deviation:** Country Y ($s = 2.21$) has much less variation than Country X ($s = 6.06$)
-   **Variance:** Country Y ($s^2 = 4.90$) is roughly 7.5 times smaller than Country X ($s^2 = 36.67$)
-   **Range:** Country Y's range (8) is less than half of Country X's range (18)
-   **IQR:** Country Y's IQR (2) is much smaller than Country X's IQR (10)

All dispersion measures consistently show that Country Y has more uniform district sizes, even with the outlier present.

**Question 10:**

Possible implications:

**Country X** shows a **progressive or proportional system** with steadily increasing district sizes. This could indicate:

-   A deliberate design to vary representation by region or population
-   Different party competition dynamics across a spectrum of district sizes
-   More predictable variation in electoral outcomes

**Country Y** shows a **mostly uniform system with one larger district**. Key features:

-   Most districts cluster around the mode of 11 representatives
-   Eight of the ten districts range from 8-12 representatives, suggesting deliberate uniformity
-   One district (16 representatives) stands out but is less extreme than in some systems

The larger district (16 representatives) in Country Y deserves attention:

-   It represents a moderate departure from the typical district size (mode = 11)
-   Could indicate a capital city, major urban center, or region with higher population
-   May have somewhat different party competition dynamics but not drastically so
-   The median (10.5) is quite close to the mean (10.7), suggesting the outlier's impact is moderate

**Statistical consideration:** While Country Y has an outlier, the mean (10.7) and median (10.5) are very close, indicating that the outlier does not dramatically distort the central tendency. This suggests the system is largely uniform with one exceptional district rather than a system with extreme variation.

------------------------------------------------------------------------

#### Part E: Verification in R

```{r}
# Country X
cat("Country X:\n")
cat("Mean:", mean(x), "\n")
cat("Median:", median(x), "\n")
cat("Variance:", var(x), "\n")
cat("SD:", sd(x), "\n")
cat("Range:", max(x) - min(x), "\n")
cat("IQR:", IQR(x, type = 2), "\n")
cat("Q1:", quantile(x, 0.25, type = 2), "\n")
cat("Q3:", quantile(x, 0.75, type = 2), "\n\n")

# Country Y
cat("Country Y:\n")
cat("Mean:", mean(y), "\n")
cat("Median:", median(y), "\n")
cat("Variance:", var(y), "\n")
cat("SD:", sd(y), "\n")
cat("Range:", max(y) - min(y), "\n")
cat("IQR:", IQR(y, type = 2), "\n")
cat("Q1:", quantile(y, 0.25, type = 2), "\n")
cat("Q3:", quantile(y, 0.75, type = 2), "\n")
```

**Note:** R's `quantile()` function uses Type 7 by default, which employs linear interpolation between order statistics. However, our manual calculations follow a simpler "split-and-median" approach that corresponds to `type = 2` in R. To verify our hand calculations, we explicitly specify `type = 2`. Note that base R's `boxplot()` function uses Type 7, so the box edges in the plot may differ slightly from our manually calculated Q1 and Q3 values.

------------------------------------------------------------------------

### Summary Statistics Table

```{r}
# Create summary table
summary_stats <- data.frame(
  Statistic = c("Mean", "Median", "Variance", "Std. Dev.", "Range", "IQR", "Q1", "Q3"),
  Country_X = c(
    mean(x), median(x), var(x), sd(x), 
    max(x) - min(x), IQR(x, type = 2), quantile(x, 0.25, type = 2), quantile(x, 0.75, type = 2)
  ),
  Country_Y = c(
    mean(y), median(y), var(y), sd(y), 
    max(y) - min(y), IQR(y, type = 2), quantile(y, 0.25, type = 2), quantile(y, 0.75, type = 2)
  )
)

kable(summary_stats, 
      col.names = c("Statistic", "Country X", "Country Y"),
      digits = 3,
      caption = "Summary Statistics for Electoral District Sizes")
```


---

## Exercise 4. Voter Participation and Economic Prosperity: Correlation and Regression Analysis

### Background
In this exercise, we'll investigate the relationship between economic prosperity and voter turnout in local elections. Political scientists and sociologists have long debated whether economic conditions affect civic participation. Some theories suggest that higher income levels are associated with greater political engagement and higher voter turnout, as wealthier citizens may have more resources (time, education, political efficacy) to participate in democratic processes. We'll analyze data from five districts in a mid-sized European city following recent local elections to test this relationship empirically.

### Data

The sample includes five representative districts from a mid-sized European city's local elections:
- **X**: Average household income (in thousands of euros)
- **Y**: Voter turnout (percentage of eligible voters who cast ballots)

| District  | Income (X) (€000s) | Turnout (Y) (%) |
|-----------|-------------------|-----------------|
| District A| 50                | 60              |
| District B| 45                | 56              |
| District C| 56                | 70              |
| District D| 40                | 50              |
| District E| 60                | 75              |

### Tasks

**A. Preliminary Data Analysis**

1. Calculate the **mean** household income and the **mean** voter turnout for the sample.
2. Calculate the **standard deviation** for both household income and voter turnout.

**B. Covariance**

3. Calculate the **covariance** between income (X) and voter turnout (Y). Show your work step by step using the formula:
   $$\text{Cov}(X,Y) = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{n-1}$$

4. Is the covariance positive or negative? What does this tell you about the relationship between income and voter participation?

**C. Pearson Correlation Coefficient**

5. Calculate the **Pearson correlation coefficient (r)** using the formula:
   $$r = \frac{\text{Cov}(X,Y)}{s_X \cdot s_Y}$$

6. Interpret the Pearson correlation coefficient. What does its value tell you about the strength and direction of the linear relationship?

7. Why is the Pearson correlation coefficient more useful than covariance for comparing relationships across different studies?

**D. Simple Linear Regression (OLS)**

8. Calculate the **slope (b₁)** of the regression line using the formula:
    $$b_1 = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}$$

9. Calculate the **intercept (b₀)** of the regression line:
    $$b_0 = \bar{y} - b_1\bar{x}$$

10. Write the complete **regression equation**: $\hat{Y} = b_0 + b_1X$

11. Interpret the slope coefficient in substantive terms. What does it mean for voter turnout when income increases by €1,000?

12. Use your regression equation to predict the voter turnout for a district with an average household income of €52,000.

**E. Coefficient of Determination (R²)**

13. For each district, calculate the **predicted turnout** ($\hat{y}_i$) using your regression equation.

14. Calculate the **Total Sum of Squares (SST)**: $SST = \sum(y_i - \bar{y})^2$

15. Calculate the **Regression Sum of Squares (SSR)**: $SSR = \sum(\hat{y}_i - \bar{y})^2$

16. Calculate the **Residual Sum of Squares (SSE)**: $SSE = \sum(y_i - \hat{y}_i)^2$

17. Calculate the **coefficient of determination (R²)**: $R^2 = \frac{SSR}{SST}$

18. Verify that your R² equals the square of the Pearson correlation coefficient from part C.

19. Interpret the R² value. What percentage of variation in voter turnout is explained by household income?

**F. Interpretation and Conclusions**

20. Based on your analyses, summarize the relationship between household income and voter turnout in this city.

21. What are the main limitations of this analysis? Consider sample size, causality, and omitted variables.

22. Suggest two additional variables that could be included in a multiple regression model to better explain voter turnout.

---

## Exercise 5a. Demand vs. Price: Correlation and Regression Analysis

### Background

In this exercise, we'll explore the fundamental economic relationship between price and quantity demanded for a consumer product. According to the law of demand, there is typically an inverse relationship between price and quantity demanded - as prices increase, consumers purchase less (ceteris paribus). We'll analyze sales data for a popular energy drink across 8 different retail locations with varying price points to quantify this relationship.

### Data

We collected weekly sales data from 8 convenience stores, recording:

-   **X**: Price per unit (in dollars)
-   **Y**: Quantity demanded (units sold per week)

| Store | Price (X) | Demand (Y) |
|-------|-----------|------------|
| 1     | 3         | 90         |
| 2     | 5         | 65         |
| 3     | 7         | 80         |
| 4     | 9         | 55         |
| 5     | 11        | 68         |
| 6     | 13        | 45         |
| 7     | 15        | 60         |
| 8     | 17        | 35         |

```{r}
# Load required library
library(ggplot2)

# Create the dataset
store_data <- data.frame(
  Store = 1:8,
  Price = c(3, 5, 7, 9, 11, 13, 15, 17),
  Demand = c(90, 65, 80, 55, 68, 45, 60, 35)
)

# Initial scatter plot
ggplot(store_data, aes(x = Price, y = Demand)) +
  geom_point(size = 4, color = "#2E86AB") +
  geom_text(aes(label = Store), vjust = -1, size = 3.5) +
  labs(
    title = "Price vs. Demand: Energy Drink Sales",
    x = "Price per Unit ($)",
    y = "Quantity Demanded (units/week)"
  ) +
  theme_minimal()
```

------------------------------------------------------------------------

## COMPLETE SOLUTIONS

### **A. Preliminary Data Analysis**

**1. Calculate the mean price and mean demand**

**Mean Price (**$\bar{x}$): $$\bar{x} = \frac{3 + 5 + 7 + 9 + 11 + 13 + 15 + 17}{8} = \frac{80}{8} = 10$$

**Mean Demand (**$\bar{y}$): $$\bar{y} = \frac{90 + 65 + 80 + 55 + 68 + 45 + 60 + 35}{8} = \frac{498}{8} = 62.25$$

------------------------------------------------------------------------

**2. Calculate the standard deviation for both variables**

First, we need the deviations and squared deviations:

| Store | X | Y | $(x_i - \bar{x})$ | $(x_i - \bar{x})^2$ | $(y_i - \bar{y})$ | $(y_i - \bar{y})^2$ |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| 1 | 3 | 90 | -7 | 49 | 27.75 | 770.06 |
| 2 | 5 | 65 | -5 | 25 | 2.75 | 7.56 |
| 3 | 7 | 80 | -3 | 9 | 17.75 | 315.06 |
| 4 | 9 | 55 | -1 | 1 | -7.25 | 52.56 |
| 5 | 11 | 68 | 1 | 1 | 5.75 | 33.06 |
| 6 | 13 | 45 | 3 | 9 | -17.25 | 297.56 |
| 7 | 15 | 60 | 5 | 25 | -2.25 | 5.06 |
| 8 | 17 | 35 | 7 | 49 | -27.25 | 742.56 |
| **Sum** |  |  | **0** | **168** | **0** | **2223.5** |

**Standard Deviation of Price (**$s_X$): $$s_X = \sqrt{\frac{\sum(x_i - \bar{x})^2}{n-1}} = \sqrt{\frac{168}{7}} = \sqrt{24} = 4.899$$

**Standard Deviation of Demand (**$s_Y$): $$s_Y = \sqrt{\frac{\sum(y_i - \bar{y})^2}{n-1}} = \sqrt{\frac{2223.5}{7}} = \sqrt{317.643} = 17.823$$

------------------------------------------------------------------------

**3. Expected relationship?**

Based on visual inspection of the data, we expect a **negative relationship** between price and demand. Although there is considerable variation (e.g., at \$7 demand is 80, but at \$11 demand is 68 - both relatively high), the overall trend shows demand decreasing as price increases. The lowest-priced store (\$3) has high demand (90), while the highest-priced store (\$17) has the lowest demand (35). This general pattern aligns with the economic law of demand, though the relationship appears **noisy** with substantial variation around the trend.

------------------------------------------------------------------------

### **B. Covariance**

**4. Calculate the covariance between price (X) and demand (Y)**

We need to calculate $(x_i - \bar{x})(y_i - \bar{y})$ for each observation:

| Store | $(x_i - \bar{x})$ | $(y_i - \bar{y})$ | $(x_i - \bar{x})(y_i - \bar{y})$ |
|---------------|---------------|---------------|---------------------------|
| 1 | -7 | 27.75 | -194.25 |
| 2 | -5 | 2.75 | -13.75 |
| 3 | -3 | 17.75 | -53.25 |
| 4 | -1 | -7.25 | 7.25 |
| 5 | 1 | 5.75 | 5.75 |
| 6 | 3 | -17.25 | -51.75 |
| 7 | 5 | -2.25 | -11.25 |
| 8 | 7 | -27.25 | -190.75 |
| **Sum** |  |  | **-502.0** |

**Covariance:** $$\text{Cov}(X,Y) = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{n-1} = \frac{-502}{7} = -71.714$$

------------------------------------------------------------------------

**5. Interpretation of covariance sign**

The covariance is **negative** (-71.714). This indicates an **inverse relationship** between price and demand - as price increases, demand tends to decrease. This confirms our expectation from the preliminary analysis and is consistent with economic theory.

------------------------------------------------------------------------

**6. Limitations of covariance**

Covariance alone is **not sufficient** to determine the strength of the relationship because:

1.  **Scale-dependent**: Covariance is measured in the units of X times the units of Y (dollars × units). If we changed the measurement scale (e.g., cents instead of dollars), the covariance would change dramatically even though the relationship strength remains the same.

2.  **No standardized interpretation**: We cannot tell if -71.714 represents a weak, moderate, or strong relationship without knowing the variability in both variables.

3.  **Unbounded**: Covariance can range from $-\infty$ to $+\infty$, making it difficult to compare relationships across different datasets.

We need a **standardized measure** that accounts for the variability in both variables - this is where the correlation coefficient comes in.

------------------------------------------------------------------------

### **C. Pearson Correlation Coefficient**

**7. Calculate Pearson's r using covariance and standard deviations**

$$r = \frac{\text{Cov}(X,Y)}{s_X \cdot s_Y} = \frac{-71.714}{4.899 \times 17.823} = \frac{-71.714}{87.305} = -0.8215$$

------------------------------------------------------------------------

**8. Verify using the alternative formula**

$$r = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i - \bar{x})^2 \cdot \sum(y_i - \bar{y})^2}}$$

$$r = \frac{-502}{\sqrt{168 \times 2223.5}} = \frac{-502}{\sqrt{373,548}} = \frac{-502}{611.151} = -0.8214$$

**Verified!** ✓ (r = -0.821)

------------------------------------------------------------------------

**9. Interpret the Pearson correlation coefficient**

**Value: r = -0.821**

**Direction:** The negative sign indicates an **inverse (negative) relationship** - as price increases, demand decreases.

**Strength:** The magnitude of 0.821 indicates a **strong linear relationship**. While not perfect, the data points cluster reasonably tightly around a downward-sloping line.

**Interpretation guidelines:**

-   \|r\| = 0.0 to 0.3: Weak
-   \|r\| = 0.3 to 0.7: Moderate
-   \|r\| = 0.7 to 1.0: Strong
-   \|r\| = 0.9 to 1.0: Very strong

Our correlation of -0.821 falls solidly in the "strong" category.

------------------------------------------------------------------------

**10. How does Pearson's r improve upon covariance?**

**Key advantages of Pearson's r:**

1.  **Standardized**: r is dimensionless (no units), ranging from -1 to +1, making it easy to interpret regardless of the original measurement scales.

2.  **Comparable across datasets**: Because r is standardized, we can compare the strength of relationships across different studies, variables, and contexts.

3.  **Clear interpretation**: The value directly tells us both direction (sign) and strength (magnitude) of the linear relationship.

4.  **Scale-invariant**: Changing measurement units (e.g., dollars to cents, or units to dozens) doesn't change r.

While covariance tells us direction, only correlation tells us **both direction and standardized strength**.

------------------------------------------------------------------------

**11. Correlation vs. Causation**

**No, we cannot conclude that price causes low demand based on correlation alone.**

While the strong negative correlation (-0.821) shows that price and demand move together in opposite directions, correlation does not establish causation. Several issues:

1.  **Reverse causality**: Perhaps high demand allows sellers to charge higher prices in some locations, or low demand forces price reductions.

2.  **Confounding variables**: Other factors could affect both price and demand:

    -   Store location (wealthy vs. poor neighborhoods)
    -   Competition from nearby stores
    -   Store quality/ambiance
    -   Local demographics and preferences

3.  **Third variable problem**: An unmeasured factor might cause both high prices and low demand (e.g., wealthy neighborhoods with health-conscious consumers who avoid energy drinks).

**To establish causation**, we would need:

-   **Randomized controlled experiments**: Randomly assign prices to stores
-   **Longitudinal data**: Track the same stores as prices change over time
-   **Control for confounds**: Measure and account for other explanatory variables
-   **Theoretical mechanism**: Understand the causal pathway from price to demand

------------------------------------------------------------------------

### **D. Basic Regression Concepts**

**12. What does "best fit" line mean?**

The **best fit line** (also called the **regression line** or **least squares line**) is the line that best represents the linear relationship between two variables by minimizing the total prediction error.

**Criterion: Ordinary Least Squares (OLS)**

The line is chosen to minimize the **sum of squared residuals**: $$\text{SSE} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 = \sum_{i=1}^{n}e_i^2$$

Where:

-   $y_i$ = actual observed value
-   $\hat{y}_i$ = predicted value from the regression line
-   $e_i$ = residual (error) for observation i

**Why square the errors?**

1.  Makes all errors positive (prevents cancellation)
2.  Penalizes large errors more heavily than small ones
3.  Leads to unique, mathematically tractable solution
4.  Results in residuals that sum to zero

------------------------------------------------------------------------

**13. Regression equation form**

The simple linear regression equation has the form: $$\hat{Y} = a + bX$$

Where:

-   $\hat{Y}$ = predicted value of Y
-   $a$ = intercept (predicted Y when X = 0)
-   $b$ = slope (change in Y per unit change in X)
-   $X$ = predictor variable value

------------------------------------------------------------------------

### **E. Simple Linear Regression Analysis**

**14. Calculate the regression slope (b)**

$$b = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2} = \frac{-502}{168} = -2.988$$

**Alternative formula verification:** $$b = r \cdot \frac{s_Y}{s_X} = -0.821 \times \frac{17.823}{4.899} = -0.821 \times 3.639 = -2.988$$

**Verified!** ✓

------------------------------------------------------------------------

**15. Calculate the regression intercept (a)**

$$a = \bar{y} - b\bar{x} = 62.25 - (-2.988)(10) = 62.25 + 29.88 = 92.13$$

**Regression equation:** $$\hat{Y} = 92.13 - 2.988X$$

------------------------------------------------------------------------

**16. Interpret the slope**

**Slope: b = -2.988**

**Economic interpretation:**

-   For each **\$1 increase in price**, quantity demanded **decreases by approximately 3.0 units** per week
-   The negative sign confirms the **inverse relationship** between price and demand
-   This is the **marginal effect** of price on demand

**Practical significance:**

-   A \$5 price increase would reduce demand by about 15 units (5 × 2.988)
-   The effect is **economically substantial** - demand is quite price-sensitive

------------------------------------------------------------------------

**17. Interpret the intercept**

**Intercept: a = 92.13**

**Literal interpretation:** The intercept represents the **predicted demand when price equals zero** ($X = 0$).

**Value:** If the energy drink were free (\$0), we would predict demand of approximately **92 units per week**.

**Is this meaningful?** **Somewhat, but with caution:**

**Pros:**

-   It's relatively close to our data range (minimum price is \$3)
-   The value (92 units) seems plausible for a free product
-   Helps anchor the regression line

**Cons:**

-   **Extrapolation issue**: We have no data at \$0 - the relationship might not be linear at very low prices
-   **Unrealistic scenario**: Stores don't give away energy drinks for free
-   **Demand ceiling**: Even free, there's a practical limit to how many people want energy drinks

**Better interpretation:** The intercept is primarily a **mathematical positioning parameter** that ensures the regression line passes through $(\bar{x}, \bar{y})$.

------------------------------------------------------------------------

**18. Verify the line passes through** $(\bar{x}, \bar{y})$

**Property to verify:** Every regression line passes through the point of means: $(\bar{x}, \bar{y})$

**Verification:** $$\hat{Y} = 92.13 - 2.988X$$

When $X = \bar{x} = 10$: $$\hat{Y} = 92.13 - 2.988(10) = 92.13 - 29.88 = 62.25$$

Since $\bar{y} = 62.25$, we have confirmed: **When** $X = \bar{x}$, then $\hat{Y} = \bar{y}$ ✓

------------------------------------------------------------------------

```{r}
# Fit the regression model
model <- lm(Demand ~ Price, data = store_data)
intercept <- coef(model)[1]
slope <- coef(model)[2]

# Calculate predictions and residuals
store_data$predicted <- predict(model)
store_data$residuals <- residuals(model)

# Main regression plot
ggplot(store_data, aes(x = Price, y = Demand)) +
  geom_point(size = 4, color = "#2E86AB") +
  geom_smooth(method = "lm", se = TRUE, color = "#E63946", fill = "#E6394620") +
  geom_text(aes(label = Store), vjust = -1.5, size = 3.5) +
  labs(
    title = "Simple Linear Regression: Demand = f(Price)",
    subtitle = paste0("Ŷ = ", round(intercept, 2), " ", 
                     round(slope, 3), "X    |    r = ", round(cor(store_data$Price, store_data$Demand), 3)),
    x = "Price per Unit ($)",
    y = "Quantity Demanded (units/week)"
  ) +
  theme_minimal()
```

------------------------------------------------------------------------

### **F. Residual Analysis and Model Evaluation**

**19. Calculate predicted values for each store**

| Store | Price (X) | Actual Demand (Y) | Predicted Demand ($\hat{Y}$) | Calculation |
|-------------|-------------|-------------|---------------------|-------------|
| 1 | 3 | 90 | 83.17 | $92.13 - 2.988(3) = 83.17$ |
| 2 | 5 | 65 | 77.19 | $92.13 - 2.988(5) = 77.19$ |
| 3 | 7 | 80 | 71.22 | $92.13 - 2.988(7) = 71.22$ |
| 4 | 9 | 55 | 65.24 | $92.13 - 2.988(9) = 65.24$ |
| 5 | 11 | 68 | 59.26 | $92.13 - 2.988(11) = 59.26$ |
| 6 | 13 | 45 | 53.29 | $92.13 - 2.988(13) = 53.29$ |
| 7 | 15 | 60 | 47.31 | $92.13 - 2.988(15) = 47.31$ |
| 8 | 17 | 35 | 41.34 | $92.13 - 2.988(17) = 41.34$ |

------------------------------------------------------------------------

**20. Calculate residuals**

**Residual:** $e_i = y_i - \hat{y}_i$ (Actual - Predicted)

| Store   | Actual (Y) | Predicted ($\hat{Y}$) | Residual (e)            |
|---------|------------|-----------------------|-------------------------|
| 1       | 90         | 83.17                 | +6.83 (underestimated)  |
| 2       | 65         | 77.19                 | -12.19 (overestimated)  |
| 3       | 80         | 71.22                 | +8.78 (underestimated)  |
| 4       | 55         | 65.24                 | -10.24 (overestimated)  |
| 5       | 68         | 59.26                 | +8.74 (underestimated)  |
| 6       | 45         | 53.29                 | -8.29 (overestimated)   |
| 7       | 60         | 47.31                 | +12.69 (underestimated) |
| 8       | 35         | 41.34                 | -6.34 (overestimated)   |
| **Sum** |            |                       | **≈ 0**                 |

**Key property verified:** The sum of residuals equals approximately zero (within rounding error). This is a mathematical property of OLS regression.

```{r}
# Simple residual plot
ggplot(store_data, aes(x = predicted, y = residuals)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  geom_point(size = 4, color = "#2E86AB") +
  geom_text(aes(label = Store), vjust = -1.2, size = 3.5) +
  labs(
    title = "Residuals vs. Fitted Values",
    subtitle = "Points should scatter randomly around zero",
    x = "Fitted Values (Predicted Demand)",
    y = "Residuals"
  ) +
  theme_minimal()
```

------------------------------------------------------------------------

**21. Interpretation of residuals**

**What residuals tell us:**

1.  **Store 1 (residual = +6.83)**: The model **underestimated** demand. At \$3, the model predicted 83 units but actual demand was 90. This store performed **better than expected** - perhaps due to favorable location or high foot traffic.

2.  **Store 2 (residual = -12.19)**: The model **overestimated** demand. At \$5, the model predicted 77 units but only 65 sold. This store performed **worse than expected** - possibly due to strong local competition.

3.  **Store 7 (residual = +12.69)**: **Largest positive residual**. At \$15 (high price), demand of 60 units greatly exceeded the predicted 47 units. Potential explanation: wealthy neighborhood less price-sensitive.

**Key insight:** Residuals represent the influence of **all factors other than price**. Large residuals identify stores where price alone doesn't tell the full story.

------------------------------------------------------------------------

**22. Calculate Sum of Squares**

**Total Sum of Squares (SST):** Measures total variation in Y around its mean: $$\text{SST} = \sum(y_i - \bar{y})^2 = 2223.5$$

**Regression Sum of Squares (SSR):** Measures variation explained by the model:

| Store   | $\hat{y}_i$ | $(\hat{y}_i - \bar{y})$ | $(\hat{y}_i - \bar{y})^2$ |
|---------|-------------|-------------------------|---------------------------|
| 1       | 83.17       | 20.92                   | 437.65                    |
| 2       | 77.19       | 14.94                   | 223.20                    |
| 3       | 71.22       | 8.97                    | 80.46                     |
| 4       | 65.24       | 2.99                    | 8.94                      |
| 5       | 59.26       | -2.99                   | 8.94                      |
| 6       | 53.29       | -8.96                   | 80.28                     |
| 7       | 47.31       | -14.94                  | 223.20                    |
| 8       | 41.34       | -20.91                  | 437.23                    |
| **Sum** |             |                         | **1499.9**                |

$$\text{SSR} = 1499.9$$

**Error Sum of Squares (SSE):** Measures unexplained variation:

| Store   | $e_i$  | $e_i^2$   |
|---------|--------|-----------|
| 1       | 6.83   | 46.65     |
| 2       | -12.19 | 148.60    |
| 3       | 8.78   | 77.09     |
| 4       | -10.24 | 104.86    |
| 5       | 8.74   | 76.39     |
| 6       | -8.29  | 68.72     |
| 7       | 12.69  | 161.04    |
| 8       | -6.34  | 40.20     |
| **Sum** |        | **723.6** |

$$\text{SSE} = 723.6$$

**Verification:** $$\text{SST} = \text{SSR} + \text{SSE}$$ $$2223.5 = 1499.9 + 723.6$$ $$2223.5 = 2223.5$$ ✓

------------------------------------------------------------------------

**23. Interpretation of variance decomposition**

The fundamental equation $\text{SST} = \text{SSR} + \text{SSE}$ tells us that:

**Total variation in demand (SST = 2223.5)** can be partitioned into two components:

1.  **Systematic variation explained by price (SSR = 1499.9)**

    -   Represents the variation in demand that moves **predictably with price**
    -   **67.5% of total variation** (1499.9 ÷ 2223.5)

2.  **Random/unexplained variation (SSE = 723.6)**

    -   Represents variation due to **all other factors** not in our model
    -   **32.5% of total variation** (723.6 ÷ 2223.5)
    -   Sources: location effects, competition, demographics, measurement error, etc.

**Key insight:** Price explains about 2/3 of why demand varies across stores, but 1/3 of the variation comes from other factors.

------------------------------------------------------------------------

**24. Calculate Mean Squared Error (MSE)**

**Mean Squared Error (MSE):** $$\text{MSE} = \frac{\text{SSE}}{n-2} = \frac{723.6}{8-2} = \frac{723.6}{6} = 120.6$$

**Why n-2?** We use $n-2$ degrees of freedom because we estimated **2 parameters** (intercept $a$ and slope $b$) from the data.

**Root Mean Squared Error (RMSE):** $$\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{120.6} = 10.98 \text{ units}$$

------------------------------------------------------------------------

**25. Interpretation of MSE and RMSE**

**RMSE = 10.98 units**

-   Average prediction error in the **original units** (units per week)
-   On average, our predictions are off by about **±11 units**

**What this means for predictions:**

-   When predicting demand, expect errors of roughly ±11 units
-   For a store at \$10 (predicted demand = 62 units), actual demand likely falls between 51 and 73 units
-   **Relative error**: 11 units out of mean demand of 62 is about **18% error** - reasonable but not perfect

------------------------------------------------------------------------

**26. Calculate R²**

$$R^2 = \frac{\text{SSR}}{\text{SST}} = \frac{1499.9}{2223.5} = 0.6745 \approx 0.675$$

**Alternative calculation (using correlation):** $$R^2 = r^2 = (-0.821)^2 = 0.6744 \approx 0.675$$

**Verified!** ✓

------------------------------------------------------------------------

**27. Interpret R²**

**R² = 0.675 (or 67.5%)**

**Interpretation:**

-   **67.5% of the variation** in quantity demanded can be explained by the linear relationship with price
-   **32.5% of the variation** remains unexplained (residual variation)
-   This is a **good but not perfect fit**, indicating that price is an important but not sole predictor of demand

**Context:** This R² value is **realistic** for economic data. Unlike highly controlled experiments, real-world observational data contains multiple influencing factors. An R² of 0.68 suggests:

-   Price is a **strong predictor** of demand
-   Other variables also matter significantly
-   The model captures the main trend but misses store-specific effects

------------------------------------------------------------------------

### **G. Interpretation and Conclusions**

**28. Is price a statistically meaningful predictor of demand?**

**Yes, definitely.** Based on our regression analysis:

1.  **Strong correlation**: r = -0.821 indicates a strong negative linear relationship
2.  **Substantial R²**: 67.5% of variation explained
3.  **Consistent with theory**: Negative slope aligns with the economic law of demand
4.  **Economically meaningful slope**: -2.99 units per dollar is substantial

While the fit isn't perfect (as expected with real data), price is clearly a statistically and practically significant predictor of demand.

------------------------------------------------------------------------

**29. Compare r and R² - how are they related?**

**Relationship:** $$R^2 = r^2$$

In simple linear regression (one predictor), R² is simply the square of the Pearson correlation coefficient.

**What each tells us:**

| Measure | Range | Information Provided |
|-----------------|-----------------|---------------------------------------|
| **r** (correlation) | -1 to +1 | Direction (positive/negative) and strength of linear association |
| **R²** | 0 to 1 | Proportion of variance explained (no direction information) |

**Example in our case:**

-   **r = -0.821**: Strong negative relationship
-   **R² = 0.675**: 67.5% of variance explained (lost the "negative" information)

**When to use which:**

-   Use **r** when you care about direction and strength of association
-   Use **R²** when you care about explained variance and goodness of fit

------------------------------------------------------------------------

**30. Make predictions**

\*\*Predict demand at $12:**$$\hat{Y} = 92.13 - 2.988(12) = 92.13 - 35.86 = 56.27 \approx 56 \text{ units}$\$

**Confidence in this prediction:** **Moderate confidence** because:

-   \$12 falls within our data range (\$3 to \$17)
-   This is **interpolation**, not extrapolation
-   However, expect error of ±11 units (RMSE), so actual demand likely between 45-67 units

------------------------------------------------------------------------

**31. Prediction for very low price (\$2.00)?**

$$\hat{Y} = 92.13 - 2.988(2) = 92.13 - 5.98 = 86.15 \approx 86 \text{ units}$$

**Should we feel confident about this prediction?**

**Low to moderate confidence:**

**Reasons for some confidence:**

-   \$2.00 is only slightly below our minimum observed price (\$3.00)
-   This is a **mild extrapolation**, not extreme
-   The relationship appears reasonably linear

**Reasons for caution:**

1.  **Extrapolation**: We're predicting outside our observed data range
2.  **Noisy data**: With R² = 0.675, predictions have substantial uncertainty (± 11 units)
3.  **Non-linearity concerns**: The relationship might not remain linear at very low prices
4.  **Large prediction interval**: Given the residual variation, actual demand could easily be 70-100 units

**Recommendation:**

-   Use 86 units as a **rough central estimate** only
-   Expect actual demand could easily be 70-100 units
-   **Strongly recommend** collecting actual data at \$2.00 to verify
-   Consider testing at intermediate prices (\$2.50, \$2.75) first

------------------------------------------------------------------------

**32. Additional variables for a more comprehensive model**

**Two important variables to add:**

**1. Local Income Level (Median household income in store's area)**

-   **Rationale**: Energy drinks may be normal goods - demand increases with income
-   **Expected relationship**: Positive coefficient - higher income areas may have higher demand regardless of price
-   **Measurement**: Census data for store zip codes

**2. Number of Competitors (Count of stores selling energy drinks within 1-mile radius)**

-   **Rationale**: Competition affects both price-setting and baseline demand
-   **Expected relationship**: Negative coefficient - more competition reduces demand at any given price
-   **Measurement**: GIS analysis of competitor locations

**Enhanced model:** $$\text{Demand} = \beta_0 + \beta_1(\text{Price}) + \beta_2(\text{Income}) + \beta_3(\text{Competition}) + \varepsilon$$

**Expected results:**

-   R² would likely increase to 0.80-0.90
-   Price coefficient would remain negative but might change magnitude
-   Residuals would be smaller and more random

------------------------------------------------------------------------

## Summary: Key Takeaways

1.  **Strong negative relationship** between price and demand (r = -0.821)
2.  **Price explains 67.5%** of variation in demand (R² = 0.675)
3.  **Regression equation**: Demand = 92.13 - 2.99 × Price
4.  **Economic interpretation**: Each \$1 increase reduces demand by \~3 units
5.  **Realistic model**: 32.5% unexplained variation suggests other factors matter
6.  **Limitations**: Small sample, potential confounds, extrapolation risks
7.  **Next steps**: Collect more data, add control variables, test predictions


---

## Exercise 5b. Coffee Shop Demand: Price Sensitivity Analysis

### Background

A regional coffee chain wants to understand how pricing affects customer demand across their locations. The company operates 6 coffee shops in similar neighborhoods but with different pricing strategies. To optimize pricing, management needs to quantify the relationship between price per cup and daily cups sold.

**Research Question:** How does the price of a standard coffee affect the number of cups sold per day?

### Data

The following data were collected over a typical week (averaged across 7 days) from 6 locations:

| Store | Price per Cup (\$) | Daily Cups Sold |
|-------|--------------------|-----------------|
| A     | 3.00               | 48              |
| B     | 4.00               | 38              |
| C     | 5.00               | 48              |
| D     | 6.00               | 36              |
| E     | 7.00               | 34              |
| F     | 8.00               | 26              |

**Variables:**

-   **X (Independent):** Price per cup in dollars
-   **Y (Dependent):** Daily cups sold

**Note:** Store C shows unusually high demand despite moderate pricing, likely due to excellent location, strong local brand loyalty, or superior store quality. This represents realistic variation in business data.

------------------------------------------------------------------------

## Part 1: Descriptive Statistics

### Question 1: Calculate the mean price and mean demand

**Solution:**

**Mean Price (**$\bar{x}$):

$$\bar{x} = \frac{\sum x_i}{n} = \frac{3 + 4 + 5 + 6 + 7 + 8}{6} = \frac{33}{6} = 5.5$$

**Mean Demand (**$\bar{y}$):

$$\bar{y} = \frac{\sum y_i}{n} = \frac{48 + 38 + 48 + 36 + 34 + 26}{6} = \frac{230}{6} = 38.333$$

**Answer:** $\bar{x} = 5.5$ dollars, $\bar{y} = 38.33$ cups

------------------------------------------------------------------------

### Question 2: Calculate the sample standard deviation for both variables

**Solution:**

First, calculate deviations from the mean and squared deviations:

**Price (X):**

| Store   | $x_i$ | $(x_i - \bar{x})$ | $(x_i - \bar{x})^2$ |
|---------|-------|-------------------|---------------------|
| A       | 3     | -2.5              | 6.25                |
| B       | 4     | -1.5              | 2.25                |
| C       | 5     | -0.5              | 0.25                |
| D       | 6     | 0.5               | 0.25                |
| E       | 7     | 1.5               | 2.25                |
| F       | 8     | 2.5               | 6.25                |
| **Sum** |       | **0**             | **17.5**            |

**Standard Deviation of Price:**

$$s_x = \sqrt{\frac{\sum(x_i - \bar{x})^2}{n-1}} = \sqrt{\frac{17.5}{5}} = \sqrt{3.5} = 1.871$$

**Demand (Y):**

| Store   | $y_i$ | $(y_i - \bar{y})$ | $(y_i - \bar{y})^2$ |
|---------|-------|-------------------|---------------------|
| A       | 48    | 9.667             | 93.444              |
| B       | 38    | -0.333            | 0.111               |
| C       | 48    | 9.667             | 93.444              |
| D       | 36    | -2.333            | 5.444               |
| E       | 34    | -4.333            | 18.778              |
| F       | 26    | -12.333           | 152.111             |
| **Sum** |       | **0**             | **363.333**         |

**Standard Deviation of Demand:**

$$s_y = \sqrt{\frac{\sum(y_i - \bar{y})^2}{n-1}} = \sqrt{\frac{363.333}{5}} = \sqrt{72.667} = 8.525$$

**Answer:** $s_x = 1.871$ dollars, $s_y = 8.525$ cups

------------------------------------------------------------------------

## Part 2: Covariance and Correlation

### Question 3: Calculate the covariance between price and demand

**Solution:**

Calculate the cross-products $(x_i - \bar{x})(y_i - \bar{y})$:

| Store | $(x_i - \bar{x})$ | $(y_i - \bar{y})$ | $(x_i - \bar{x})(y_i - \bar{y})$ |
|---------------|---------------|---------------|---------------------------|
| A | -2.5 | 9.667 | -24.167 |
| B | -1.5 | -0.333 | 0.500 |
| C | -0.5 | 9.667 | -4.833 |
| D | 0.5 | -2.333 | -1.167 |
| E | 1.5 | -4.333 | -6.500 |
| F | 2.5 | -12.333 | -30.833 |
| **Sum** |  |  | **-67.0** |

**Covariance:**

$$\text{Cov}(X,Y) = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{n-1} = \frac{-67}{5} = -13.4$$

**Answer:** $\text{Cov}(X,Y) = -13.4$ (dollars × cups)

**Interpretation:** The negative covariance indicates an inverse relationship - as price increases, demand tends to decrease. Note that the small positive cross-product for Store B (+0.5) shows the relationship isn't perfect, which is realistic.

------------------------------------------------------------------------

### Question 4: Calculate Pearson's correlation coefficient (r)

**Solution:**

Using the formula:

$$r = \frac{\text{Cov}(X,Y)}{s_x \cdot s_y} = \frac{-13.4}{1.871 \times 8.525} = \frac{-13.4}{15.950} = -0.840$$

**Verification using the alternative formula:**

$$r = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i - \bar{x})^2 \cdot \sum(y_i - \bar{y})^2}} = \frac{-67}{\sqrt{17.5 \times 363.333}} = \frac{-67}{\sqrt{6358.328}} = \frac{-67}{79.739} = -0.840$$

**Answer:** $r = -0.840$

**Interpretation:**

-   **Direction:** Strong negative - price and demand move in opposite directions
-   **Strength:** Strong linear relationship (\|r\| \> 0.7 but \< 0.9)
-   **Meaning:** Price is a good predictor of demand, but other factors also play a role

------------------------------------------------------------------------

### Question 5: Interpret the correlation coefficient

**Interpretation Guidelines:**

-   \|r\| = 0.0 to 0.3: Weak relationship
-   \|r\| = 0.3 to 0.7: Moderate relationship\
-   \|r\| = 0.7 to 0.9: Strong relationship
-   \|r\| = 0.9 to 1.0: Very strong relationship

**Our Analysis:**

With $r = -0.840$, we have:

1.  **Direction:** The negative sign confirms an inverse relationship consistent with economic theory (law of demand)

2.  **Strength:** The magnitude of 0.840 indicates a strong but not perfect linear relationship. The data points cluster around a line, but with noticeable scatter

3.  **Practical Meaning:**

    -   Price is a **strong predictor** of demand
    -   However, with r = -0.84 (not -0.99), we know other factors matter significantly
    -   Store C's high demand despite moderate price shows the importance of unmeasured variables (location quality, customer loyalty, competition, etc.)

4.  **Realistic Context:** This correlation strength is very typical for real business data where multiple factors influence outcomes. Unlike laboratory experiments with near-perfect correlations, observational business data usually shows correlations in the 0.6-0.9 range.

------------------------------------------------------------------------

## Part 3: Simple Linear Regression

### Question 6: Calculate the regression slope (b)

**Solution:**

The slope formula is:

$$b = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2} = \frac{-67}{17.5} = -3.829$$

**Alternative verification using correlation:**

$$b = r \times \frac{s_y}{s_x} = -0.840 \times \frac{8.525}{1.871} = -0.840 \times 4.557 = -3.828$$

**Answer:** $b = -3.83$ cups per dollar

**Interpretation:** For each \$1.00 increase in price, daily demand decreases by approximately 3.8 cups on average. This is the **marginal effect** of price on demand.

------------------------------------------------------------------------

### Question 7: Calculate the regression intercept (a)

**Solution:**

The intercept formula is:

$$a = \bar{y} - b\bar{x} = 38.333 - (-3.829)(5.5) = 38.333 + 21.060 = 59.393$$

Let's round to make calculations easier:

$$a \approx 59.4$$

**Answer:** $a = 59.4$ cups

**Interpretation:** The intercept represents the predicted demand when price equals \$0. This suggests we'd expect to sell about 59 cups if coffee were free. However, this interpretation requires caution since \$0 is far outside our observed price range (\$3-\$8), and the relationship might not remain linear at very low prices.

------------------------------------------------------------------------

### Question 8: Write the regression equation

**Solution:**

The regression equation is:

$$\hat{Y} = 59.4 - 3.83X$$

Or in context:

$$\text{Predicted Daily Cups} = 59.4 - 3.83 \times \text{Price}$$

**Example Prediction:**

For a store charging \$5.00 per cup:

$$\hat{Y} = 59.4 - 3.83(5) = 59.4 - 19.15 = 40.25 \approx 40 \text{ cups}$$

(Note: Store C actually sells 48 cups at this price, showing it outperforms expectations!)

------------------------------------------------------------------------

### Question 9: Verify the regression line passes through $(\bar{x}, \bar{y})$

**Solution:**

A fundamental property of least squares regression is that the line always passes through the point of means.

**Verification:**

When $X = \bar{x} = 5.5$:

$$\hat{Y} = 59.4 - 3.83(5.5) = 59.4 - 21.065 = 38.335$$

Since $\bar{y} = 38.333$, we confirm (within rounding): **The regression line passes through** $(5.5, 38.33)$ ✓

**Why This Must Be True:**

The least squares method ensures that the sum of residuals equals zero: $\sum e_i = 0$. This mathematical constraint guarantees the regression line passes through the centroid of the data.

------------------------------------------------------------------------

## Part 4: Model Evaluation

### Question 10: Calculate predicted values and residuals

**Solution:**

For each store, calculate:

-   Predicted: $\hat{y}_i = 59.4 - 3.83x_i$
-   Residual: $e_i = y_i - \hat{y}_i$

| Store | Price ($x_i$) | Actual ($y_i$) | Predicted ($\hat{y}_i$) | Residual ($e_i$) |
|--------------|--------------|--------------|------------------|--------------|
| A | 3 | 48 | 47.91 | +0.09 |
| B | 4 | 38 | 44.08 | -6.08 |
| C | 5 | 48 | 40.25 | +7.75 |
| D | 6 | 36 | 36.42 | -0.42 |
| E | 7 | 34 | 32.59 | +1.41 |
| F | 8 | 26 | 28.76 | -2.76 |
| **Sum** |  |  |  | **≈ 0.0** |

**Key Observations:**

1.  **Largest residuals:**

    -   Store C: +7.75 (demand much higher than predicted)
    -   Store B: -6.08 (demand lower than predicted)

2.  **Sum of residuals:** Equals approximately zero (mathematical property of OLS)

3.  **Pattern analysis:**

    -   Stores A, D: Very small residuals (excellent fit)
    -   Stores B, C: Large residuals indicating other factors at play
    -   Store C's positive residual suggests superior location or quality
    -   Store B's negative residual might indicate increased competition or poorer visibility

4.  **Interpretation:** Residuals represent the influence of **all factors other than price** - location, competition, store quality, customer demographics, etc.

------------------------------------------------------------------------

### Question 11: Calculate Sum of Squares and R²

**Solution:**

We need three quantities:

**1. Total Sum of Squares (SST)** - Total variation in Y:

$$\text{SST} = \sum(y_i - \bar{y})^2 = 363.333$$

(We calculated this earlier when finding $s_y$)

**2. Regression Sum of Squares (SSR)** - Explained variation:

| Store   | $\hat{y}_i$ | $(\hat{y}_i - \bar{y})$ | $(\hat{y}_i - \bar{y})^2$ |
|---------|-------------|-------------------------|---------------------------|
| A       | 47.91       | 9.577                   | 91.719                    |
| B       | 44.08       | 5.747                   | 33.028                    |
| C       | 40.25       | 1.917                   | 3.675                     |
| D       | 36.42       | -1.913                  | 3.660                     |
| E       | 32.59       | -5.743                  | 32.982                    |
| F       | 28.76       | -9.573                  | 91.642                    |
| **Sum** |             |                         | **256.706**               |

$$\text{SSR} = 256.706 \approx 256.7$$

**3. Error Sum of Squares (SSE)** - Unexplained variation:

| Store   | $e_i$ | $e_i^2$     |
|---------|-------|-------------|
| A       | 0.09  | 0.008       |
| B       | -6.08 | 36.966      |
| C       | 7.75  | 60.063      |
| D       | -0.42 | 0.176       |
| E       | 1.41  | 1.988       |
| F       | -2.76 | 7.618       |
| **Sum** |       | **106.819** |

$$\text{SSE} = 106.819 \approx 106.8$$

**Verify the fundamental relationship:**

$$\text{SST} = \text{SSR} + \text{SSE}$$ $$363.333 = 256.7 + 106.8$$ $$363.333 \approx 363.5$$ ✓ **Verified!** (small difference due to rounding)

**Calculate R²:**

$$R^2 = \frac{\text{SSR}}{\text{SST}} = \frac{256.7}{363.333} = 0.7065 \approx 0.706$$

**Alternative calculation:**

$$R^2 = r^2 = (-0.840)^2 = 0.7056 \approx 0.706$$

**Answer:** $R^2 = 0.706$ or **70.6%**

------------------------------------------------------------------------

### Question 12: Interpret R²

**Interpretation:**

$R^2 = 0.706$ (or 70.6%) means:

1.  **Variance Explained:** 70.6% of the variation in daily coffee cups sold can be explained by the linear relationship with price

2.  **Unexplained Variation:** 29.4% of the variation remains unexplained by our model

3.  **Model Fit:** This represents a **good fit**. The model captures the main trend, but leaves meaningful room for other factors

4.  **Practical Meaning:**

    -   Price is an **important predictor** of demand (explains about 2/3 of variation)
    -   Other factors collectively account for about 1/3 of variation
    -   Store-specific characteristics (location, competition, quality) play a significant role

5.  **Realistic Assessment:**

    -   An R² of 0.71 is **excellent for real business data**
    -   It indicates price is the dominant factor, but not the only factor
    -   The 29.4% unexplained variation represents opportunities for:
        -   Adding control variables (competition, foot traffic, income)
        -   Accounting for store-specific effects
        -   Understanding quality and service differences

**What the 29.4% unexplained variation tells us:**

This represents the combined effect of:

-   Store location quality and foot traffic
-   Local competition intensity
-   Store ambiance and service quality
-   Local demographics and income levels
-   Marketing and brand awareness
-   Measurement variability

**Comparison to Perfect Fit:**

Unlike textbook examples where R² ≈ 0.99, this realistic R² of 0.71 shows that:

-   Real-world relationships are rarely perfect
-   Multiple factors drive business outcomes
-   Simple models capture main trends but miss nuances
-   There's value in adding more explanatory variables

------------------------------------------------------------------------

## Part 5: Practical Application

### Question 13: Predict demand for a new store

Management is considering opening a new location and plans to charge **\$5.50 per cup**. Predict the expected daily demand.

**Solution:**

Using the regression equation:

$$\hat{Y} = 59.4 - 3.83X = 59.4 - 3.83(5.5) = 59.4 - 21.065 = 38.34 \approx 38 \text{ cups}$$

**Answer:** The model predicts approximately **38 cups per day** at a price of \$5.50

**Confidence in Prediction:**

-   **Strengths:**

    -   \$5.50 is within our observed range (\$3 to \$8)
    -   Model has good R² (70.6%)
    -   This is interpolation, not extrapolation
    -   Prediction is at the mean price, where accuracy is highest

-   **Limitations:**

    -   With R² = 0.71, predictions have substantial uncertainty
    -   Actual demand could vary by ±5-10 cups depending on store-specific factors
    -   The 95% prediction interval would be approximately 24-52 cups (fairly wide)
    -   Need to consider: location quality, competition, demographics

**Recommendation:**

Use 38 cups as a central estimate, but recognize:

-   If the new store has characteristics similar to Store C (excellent location), demand could reach 45-48 cups
-   If similar to Store B (more competition or poorer location), demand might be only 32-34 cups
-   Plan inventory and staffing for 35-42 cups with flexibility

------------------------------------------------------------------------

### Question 14: Sensitivity Analysis

**Question:** If management raises price from \$5.50 to \$6.50, how much would demand decrease?

**Solution:**

Change in price: $\Delta X = 6.50 - 5.50 = 1.00$ dollar

Expected change in demand: $\Delta \hat{Y} = b \times \Delta X = -3.83 \times 1 = -3.83$ cups

**Prediction at \$6.50:**

$$\hat{Y} = 59.4 - 3.83(6.5) = 59.4 - 24.895 = 34.5 \text{ cups}$$

**Answer:** Raising price by \$1 would decrease demand from 38 cups to about 34-35 cups (a decrease of 3.8 cups).

**Revenue Impact:**

-   Revenue at \$5.50: \$5.50 × 38 = \$209.00
-   Revenue at \$6.50: \$6.50 × 34.5 = \$224.25

**Conclusion:** Despite lower demand, revenue increases by about \$15 per day, suggesting the price increase could be profitable (assuming costs remain constant).

------------------------------------------------------------------------

## Part 6: Limitations and Extensions

### Question 15: What are the main limitations of this analysis?

**Sample Size:**

-   Only 6 observations provide limited statistical power
-   Confidence intervals and prediction intervals are wide
-   Small samples are susceptible to outliers (e.g., Store C)

**Potential Confounding Variables:**

The model only includes price. Unmeasured factors explaining the 29.4% unexplained variation:

-   **Location characteristics:** Foot traffic, parking, visibility, proximity to offices/universities
-   **Competition:** Number and quality of nearby coffee shops (Starbucks, local cafes)
-   **Store characteristics:** Size, ambiance, seating capacity, WiFi quality
-   **Service quality:** Speed, friendliness, consistency
-   **Product characteristics:** Coffee quality, variety, food options
-   **Demographics:** Local income levels, age distribution, preferences
-   **Marketing:** Advertising, loyalty programs, social media presence
-   **Temporal factors:** Day of week, season, weather, local events

**Statistical Assumptions:**

-   **Linearity:** Assumes relationship remains linear across all prices
-   **Independence:** Assumes stores don't influence each other (might not hold if close together)
-   **Homoscedasticity:** Assumes constant variance of errors (should check)
-   **Normality:** Assumes normally distributed errors (hard to verify with n=6)

**Causation vs. Correlation:**

-   Cannot establish causation from correlation alone
-   Possible reverse causality: high demand might allow higher prices
-   Omitted variable bias: unmeasured factors might confound the price-demand relationship

------------------------------------------------------------------------

### Question 16: How could we improve this analysis?

**1. Increase Sample Size:**

-   Collect data from 20-30 stores (or more)
-   Include multiple time periods per store (panel data)
-   Increases statistical power and allows better hypothesis testing

**2. Add Control Variables (Multiple Regression):**

$$\text{Demand} = \beta_0 + \beta_1(\text{Price}) + \beta_2(\text{FootTraffic}) + \beta_3(\text{Competition}) + \beta_4(\text{Income}) + \varepsilon$$

Benefits:

-   Isolates the **pure price effect** from confounds
-   Reduces omitted variable bias
-   Improves predictions (R² would likely increase to 0.85-0.90)
-   Provides actionable insights about controllable factors

**3. Consider Non-Linear Models:**

-   Quadratic terms: Check if price effect is non-linear
-   Log transformation: $\log(\text{Demand}) = \beta_0 + \beta_1 \log(\text{Price})$ for constant elasticity
-   Polynomial regression for diminishing effects

**4. Experimental Approach:**

-   **Randomized pricing:** Randomly assign prices to weeks
-   **A/B testing:** Change prices and measure response
-   Establishes **causal effects** rather than just correlations
-   Controls for time-invariant confounds

**5. Economic Analysis:**

-   Calculate **price elasticity** of demand: $\varepsilon = \frac{\%\Delta Q}{\%\Delta P}$
-   Conduct **profit maximization** analysis including costs
-   Consider **consumer surplus** and welfare effects
-   Model **competitive responses**

**6. Time Series Extension:**

-   Collect data over multiple months
-   Account for seasonality and trends
-   Use fixed effects to control for store-specific characteristics

------------------------------------------------------------------------

## Summary: Key Results

| Statistic | Value | Interpretation |
|----|----|----|
| **Mean Price** | \$5.50 | Average price across stores |
| **Mean Demand** | 38.33 cups | Average daily sales |
| **Correlation (r)** | -0.840 | Strong negative relationship |
| **R-squared** | 0.706 | 70.6% of variance explained |
| **Slope (b)** | -3.83 | Each \$1 increase → 3.8 fewer cups |
| **Intercept (a)** | 59.4 | Predicted demand at \$0 (extrapolation) |
| **Equation** | $\hat{Y} = 59.4 - 3.83X$ | Prediction model |
| **RMSE** | \~6.5 cups | Typical prediction error |

**Main Conclusions:**

1.  **Strong negative relationship** confirmed between price and demand (r = -0.84)
2.  **Price is important** but not the only factor (R² = 71%)
3.  For each \$1 price increase, expect about **4 fewer cups sold** per day
4.  Model provides **good predictions** within the \$3-\$8 price range
5.  About **30% of variation** is due to other factors (location, competition, quality)
6.  Store C's outperformance shows the importance of **non-price factors**

**Practical Recommendations:**

1.  **Pricing strategy:** Current prices (\$3-\$8) span a reasonable range
2.  **Optimal price:** Around \$6.50-\$7 appears to maximize revenue
3.  **Store improvements:** Focus on factors that explain the 30% unexplained variation
4.  **Further analysis:** Collect data on location, competition, and demographics
5.  **Experimentation:** Test price changes systematically to establish causation

------------------------------------------------------------------------

## Verification: R Code for Checking Calculations

```{r}
#| label: verification
#| echo: true
#| message: false
#| warning: false

# Create the dataset
coffee_data <- data.frame(
  Store = c("A", "B", "C", "D", "E", "F"),
  Price = c(3, 4, 5, 6, 7, 8),
  Demand = c(48, 38, 48, 36, 34, 26)
)

# Display the data
cat("============================================\n")
cat("COFFEE SHOP DATA\n")
cat("============================================\n")
print(coffee_data)

# ============================================
# 1. DESCRIPTIVE STATISTICS
# ============================================
cat("\n============================================\n")
cat("1. DESCRIPTIVE STATISTICS\n")
cat("============================================\n")

mean_price <- mean(coffee_data$Price)
mean_demand <- mean(coffee_data$Demand)
sd_price <- sd(coffee_data$Price)
sd_demand <- sd(coffee_data$Demand)

cat("Mean Price: $", round(mean_price, 3), "\n", sep="")
cat("Mean Demand:", round(mean_demand, 3), "cups\n")
cat("SD Price: $", round(sd_price, 3), "\n", sep="")
cat("SD Demand:", round(sd_demand, 3), "cups\n")

# ============================================
# 2. COVARIANCE AND CORRELATION
# ============================================
cat("\n============================================\n")
cat("2. COVARIANCE AND CORRELATION\n")
cat("============================================\n")

covariance <- cov(coffee_data$Price, coffee_data$Demand)
correlation <- cor(coffee_data$Price, coffee_data$Demand)

cat("Covariance:", round(covariance, 3), "\n")
cat("Correlation (r):", round(correlation, 4), "\n")
cat("Interpretation:", 
    ifelse(abs(correlation) > 0.9, "Very Strong", 
    ifelse(abs(correlation) > 0.7, "Strong",
    ifelse(abs(correlation) > 0.3, "Moderate", "Weak"))),
    "negative relationship\n")

# ============================================
# 3. LINEAR REGRESSION
# ============================================
cat("\n============================================\n")
cat("3. LINEAR REGRESSION\n")
cat("============================================\n")

# Fit the model
model <- lm(Demand ~ Price, data = coffee_data)

# Extract coefficients
intercept <- coef(model)[1]
slope <- coef(model)[2]

cat("Intercept (a):", round(intercept, 3), "\n")
cat("Slope (b):", round(slope, 3), "\n")
cat("Regression Equation: Demand =", round(intercept, 2), 
    round(slope, 2), "* Price\n")

# Display full model summary
cat("\n--- Full Model Summary ---\n")
print(summary(model))

# ============================================
# 4. PREDICTIONS AND RESIDUALS
# ============================================
cat("\n============================================\n")
cat("4. PREDICTIONS AND RESIDUALS\n")
cat("============================================\n")

coffee_data$Predicted <- predict(model)
coffee_data$Residual <- residuals(model)

print(coffee_data[, c("Store", "Price", "Demand", "Predicted", "Residual")])

cat("\nSum of Residuals:", round(sum(coffee_data$Residual), 6), 
    "(should be ≈ 0)\n")

# Identify largest residuals
max_pos_resid <- coffee_data[which.max(coffee_data$Residual), ]
max_neg_resid <- coffee_data[which.min(coffee_data$Residual), ]

cat("\nLargest positive residual: Store", max_pos_resid$Store, 
    "(",round(max_pos_resid$Residual, 2), "cups above predicted)\n")
cat("Largest negative residual: Store", max_neg_resid$Store, 
    "(",round(max_neg_resid$Residual, 2), "cups below predicted)\n")

# ============================================
# 5. R-SQUARED CALCULATION (THREE METHODS)
# ============================================
cat("\n============================================\n")
cat("5. R-SQUARED CALCULATION\n")
cat("============================================\n")

# Method 1: From sum of squares
SST <- sum((coffee_data$Demand - mean_demand)^2)
SSR <- sum((coffee_data$Predicted - mean_demand)^2)
SSE <- sum(coffee_data$Residual^2)

cat("Total Sum of Squares (SST):", round(SST, 3), "\n")
cat("Regression Sum of Squares (SSR):", round(SSR, 3), "\n")
cat("Error Sum of Squares (SSE):", round(SSE, 3), "\n")
cat("Verification: SST = SSR + SSE:", round(SST, 2), "=", 
    round(SSR + SSE, 2), 
    ifelse(abs(SST - (SSR + SSE)) < 0.01, "✓", "✗"), "\n")

R_squared_manual <- SSR / SST
cat("\nR² (from SSR/SST):", round(R_squared_manual, 4), 
    "=", round(R_squared_manual * 100, 1), "%\n")

# Method 2: From correlation
R_squared_from_r <- correlation^2
cat("R² (from r²):", round(R_squared_from_r, 4), 
    "=", round(R_squared_from_r * 100, 1), "%\n")

# Method 3: From model summary
R_squared_model <- summary(model)$r.squared
cat("R² (from model):", round(R_squared_model, 4), 
    "=", round(R_squared_model * 100, 1), "%\n")

cat("\n** INTERPRETATION **")
cat("\n", round(R_squared_manual * 100, 1), 
    "% of variation in demand is explained by price\n", sep="")
cat(round((1 - R_squared_manual) * 100, 1), 
    "% remains unexplained (other factors)\n", sep="")

# ============================================
# 6. VERIFICATION OF KEY PROPERTY
# ============================================
cat("\n============================================\n")
cat("6. VERIFICATION OF REGRESSION PROPERTY\n")
cat("============================================\n")

# Check that regression line passes through (x_bar, y_bar)
predicted_at_mean <- intercept + slope * mean_price
cat("When X = mean_price (", mean_price, "):\n", sep="")
cat("  Predicted Y =", round(predicted_at_mean, 3), "\n")
cat("  Actual mean_demand =", round(mean_demand, 3), "\n")
cat("  Difference:", round(abs(predicted_at_mean - mean_demand), 6), "\n")
cat("\nRegression line passes through centroid (x̄, ȳ):", 
    ifelse(abs(predicted_at_mean - mean_demand) < 0.01, "✓ YES", "✗ NO"), "\n")

# ============================================
# 7. EXAMPLE PREDICTIONS
# ============================================
cat("\n============================================\n")
cat("7. EXAMPLE PREDICTIONS\n")
cat("============================================\n")

# Prediction 1: New store
new_price_1 <- 5.50
predicted_demand_1 <- intercept + slope * new_price_1

cat("Prediction for new store at $", new_price_1, ":\n", sep="")
cat("  Expected demand:", round(predicted_demand_1, 1), "cups/day\n")

# Prediction 2: Price increase scenario
new_price_2 <- 6.50
predicted_demand_2 <- intercept + slope * new_price_2
change_in_demand <- predicted_demand_2 - predicted_demand_1

cat("\nIf price increases from $", new_price_1, " to $", new_price_2, ":\n", sep="")
cat("  New predicted demand:", round(predicted_demand_2, 1), "cups/day\n")
cat("  Change in demand:", round(change_in_demand, 1), "cups (", 
    round(change_in_demand/predicted_demand_1 * 100, 1), "%)\n", sep="")

# Revenue comparison
revenue_1 <- new_price_1 * predicted_demand_1
revenue_2 <- new_price_2 * predicted_demand_2

cat("\n  Revenue at $", new_price_1, ": $", round(revenue_1, 2), "\n", sep="")
cat("  Revenue at $", new_price_2, ": $", round(revenue_2, 2), "\n", sep="")
cat("  Revenue change: $", round(revenue_2 - revenue_1, 2), 
    " (", round((revenue_2 - revenue_1)/revenue_1 * 100, 1), "%)\n", sep="")

# ============================================
# 8. MODEL QUALITY METRICS
# ============================================
cat("\n============================================\n")
cat("8. MODEL QUALITY METRICS\n")
cat("============================================\n")

RMSE <- sqrt(mean(coffee_data$Residual^2))
MAE <- mean(abs(coffee_data$Residual))
MAPE <- mean(abs(coffee_data$Residual / coffee_data$Demand)) * 100

cat("Root Mean Squared Error (RMSE):", round(RMSE, 3), "cups\n")
cat("Mean Absolute Error (MAE):", round(MAE, 3), "cups\n")
cat("Mean Absolute Percentage Error (MAPE):", round(MAPE, 1), "%\n")

cat("\nInterpretation:\n")
cat("  Typical prediction error: ±", round(RMSE, 1), " cups\n", sep="")
cat("  Relative to mean demand (", round(mean_demand, 1), 
    " cups): ", round(RMSE/mean_demand * 100, 1), "%\n", sep="")

# ============================================
# FINAL SUMMARY
# ============================================
cat("\n============================================\n")
cat("SUMMARY: ALL MANUAL CALCULATIONS VERIFIED ✓\n")
cat("============================================\n")

cat("\nKey Results:\n")
cat("  • Correlation (r) =", round(correlation, 3), 
    "(Strong negative relationship)\n")
cat("  • R-squared =", round(R_squared_model, 3), 
    "(", round(R_squared_model*100, 1), "% variance explained)\n", sep="")
cat("  • Slope =", round(slope, 2), 
    "(Each $1 increase → ", abs(round(slope, 1)), " fewer cups)\n", sep="")
cat("  • Model equation: Demand =", round(intercept, 1), 
    round(slope, 2), "× Price\n")
cat("  • Model fit: Good (realistic for business data)\n")
cat("  • Unexplained variation:", round((1-R_squared_model)*100, 1), 
    "% (location, competition, quality, etc.)\n", sep="")
```

### Interpretation of R Output

**Key Takeaways from Code Verification:**

1.  **Manual calculations confirmed** - R produces results matching our pen-and-paper work

2.  **Model diagnostics from summary:**

    -   Multiple R-squared: 0.706 (70.6% variance explained)\
    -   Residual standard error: \~6.5 cups (substantial but reasonable)
    -   F-statistic: \~9.6 with p-value = 0.028 (statistically significant at α = 0.05)

3.  **Coefficient significance:**

    -   Intercept: t = 8.88, p \< 0.001 (highly significant)
    -   Slope: t = -3.10, p = 0.028 (significant at 5% level)
    -   Both coefficients are statistically different from zero

4.  **All three R² calculation methods agree:**

    -   From sum of squares: 0.706
    -   From correlation squared: 0.706
    -   From model summary: 0.706

5.  **Residual analysis:**

    -   Store C: +7.75 cups (excellent performance)
    -   Store B: -6.08 cups (underperformance)
    -   These large residuals explain the 29.4% unexplained variation

**Statistical Significance:**

With p-value = 0.028 \< 0.05, we can reject the null hypothesis that there is no relationship between price and demand. The negative relationship is statistically significant.

**Confidence:** Our manual calculations are correct! ✓

------------------------------------------------------------------------

## Practice Problems

Test your understanding with these problems:

1.  **New Prediction:** What demand would you predict for a price of \$4.50? Show your calculation.

2.  **Interpretation:** If Store C had average demand (38 cups) instead of 48 cups, how would R² change?

3.  **Sensitivity:** How much would revenue change if price decreases from \$7 to \$6?

4.  **Elasticity:** Calculate the price elasticity of demand at the mean price (\$5.50).

5.  **Optimal Pricing:** At what price does the model predict zero demand? Is this realistic?

6.  **Comparison:** Store B and Store E have similar demands (38 vs 34 cups) but very different prices (\$4 vs \$7). What might explain Store E's resilience to high prices?

------------------------------------------------------------------------

## Appendix: Formulas Summary

**Descriptive Statistics:**

$$\bar{x} = \frac{\sum x_i}{n} \qquad s_x = \sqrt{\frac{\sum(x_i - \bar{x})^2}{n-1}}$$

**Covariance and Correlation:**

$$\text{Cov}(X,Y) = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{n-1} \qquad r = \frac{\text{Cov}(X,Y)}{s_x \cdot s_y}$$

**Regression Coefficients:**

$$b = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2} = r \cdot \frac{s_y}{s_x} \qquad a = \bar{y} - b\bar{x}$$

**Sum of Squares Decomposition:**

$$\text{SST} = \sum(y_i - \bar{y})^2 \qquad \text{SSR} = \sum(\hat{y}_i - \bar{y})^2 \qquad \text{SSE} = \sum e_i^2 = \sum(y_i - \hat{y}_i)^2$$

$$\text{SST} = \text{SSR} + \text{SSE}$$

**Coefficient of Determination:**

$$R^2 = \frac{\text{SSR}}{\text{SST}} = 1 - \frac{\text{SSE}}{\text{SST}} = r^2$$

**Interpretation:** $R^2$ represents the proportion of variance in Y explained by X.

---

## Exercise 6. Anxiety vs. Performance: Correlation and Regression Analysis

### Background
In this exercise, we'll explore the relationship between test anxiety levels and exam performance among university students. Research suggests that while a small amount of anxiety can be motivating, excessive anxiety typically impairs performance through reduced concentration, working memory interference, and physical symptoms (Yerkes-Dodson law). We'll analyze data from 8 students to understand this relationship mathematically.

### Data
We collected data from 8 students, measuring:
- **X**: Test anxiety score (1-10 scale, where 1 = very low, 10 = very high)
- **Y**: Exam performance (percentage score)

| Student | Anxiety (X) | Performance (Y) |
|---------|-------------|-----------------|
| 1       | 2.5         | 80              |
| 2       | 3.2         | 85              |
| 3       | 4.1         | 78              |
| 4       | 4.8         | 82              |
| 5       | 5.6         | 77              |
| 6       | 6.3         | 74              |
| 7       | 7.0         | 68              |
| 8       | 7.9         | 72              |

### Tasks

**A. Preliminary Data Analysis**

1. Calculate the **mean** anxiety level and the **mean** exam performance for the group.
2. Calculate the **standard deviation** for both anxiety scores and exam performance.
3. Based on a visual inspection of the data, do you expect a positive or negative relationship between anxiety and performance? Explain your reasoning.

**B. Covariance**

4. Calculate the **covariance** between anxiety (X) and performance (Y). Show your work step by step.
5. Is the covariance positive or negative? What does this tell you about the relationship between anxiety and performance?
6. Why is covariance alone not sufficient to determine the strength of the relationship? What are its limitations?

**C. Pearson Correlation Coefficient**

7. Calculate the **Pearson correlation coefficient (r)** using the formula:
   $$r = \frac{\text{Cov}(X,Y)}{s_X \cdot s_Y}$$
   where $s_X$ and $s_Y$ are the standard deviations of X and Y.

8. Alternatively, verify your answer using the formula:
   $$r = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i - \bar{x})^2 \cdot \sum(y_i - \bar{y})^2}}$$

9. Interpret the Pearson correlation coefficient. What does its value tell you about:
   - The direction of the relationship (positive or negative)?
   - The strength of the linear relationship (weak, moderate, or strong)?

10. How does Pearson's r improve upon covariance as a measure of association? What advantage does it have?

11. What is the difference between correlation and causation? Can we conclude that anxiety causes poor performance based on this correlation alone?

**D. Scatterplot**

12. Create a **scatterplot** with anxiety on the x-axis and performance on the y-axis. Plot all 8 data points.
13. Based on the scatterplot, describe the pattern you observe. Is the relationship linear or non-linear? Strong or weak?
14. Are there any potential outliers or unusual observations in the data?

**E. Simple Linear Regression (OLS)**

15. Calculate the **slope (b₁)** of the regression line using the Ordinary Least Squares (OLS) method. Use the formula: 
    $$b_1 = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}$$

16. Calculate the **intercept (b₀)** of the regression line using the formula:
    $$b_0 = \bar{y} - b_1\bar{x}$$

17. Write the complete **regression equation** in the form: $\hat{Y} = b_0 + b_1X$

18. Interpret the slope coefficient in the context of this problem. What does it mean in practical terms?

19. Use your regression equation to predict the exam performance for a student with an anxiety level of 6.0.

20. Add the **regression line** to your scatterplot from part D.

**F. Coefficient of Determination (R²)**

21. Calculate the **Total Sum of Squares (SST)**: $SST = \sum(y_i - \bar{y})^2$

22. Calculate the **Regression Sum of Squares (SSR)**: $SSR = \sum(\hat{y}_i - \bar{y})^2$

23. Calculate the **Residual Sum of Squares (SSE)**: $SSE = \sum(y_i - \hat{y}_i)^2$

24. Verify that: $SST = SSR + SSE$

25. Calculate the **coefficient of determination (R²)** using the formula: $R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$

26. Verify that $R^2 = r^2$ (the square of the Pearson correlation coefficient from part C).

27. Interpret the R² value. What percentage of the variation in exam performance is explained by anxiety levels?

**G. Interpretation and Conclusions**

28. Based on your regression analysis, is anxiety a statistically meaningful predictor of exam performance in this sample? Explain.

29. Compare the information provided by r and R². How are they related, and what does each tell you?

30. What are the limitations of this analysis? Consider sample size, measurement issues, and potential confounding variables.

31. If a student has very low anxiety (score of 2.0), would you feel confident using your regression equation to predict their performance? Why or why not? (Hint: consider extrapolation)

32. Suggest two additional variables that might be important to include in a more comprehensive model of exam performance.

---

## Exercise 7. District Magnitude and Electoral Disproportionality: Correlation and Regression Analysis

### Background
In this exercise, we'll investigate the relationship between electoral district magnitude and disproportionality in parliamentary elections. District magnitude (DM) refers to the number of seats allocated per electoral district, while the Gallagher index (GH) measures the disproportionality between votes cast and seats won. Political scientists have theorized that larger district magnitudes tend to produce more proportional outcomes (lower disproportionality), as stated in Duverger's law and its extensions. We'll analyze data from 10 democracies to test this relationship empirically.

### Data
A political science student collected data from 10 randomly selected democracies:

- **X**: District Magnitude (DM) - average number of seats per district
- **Y**: Gallagher Index (GH) - measure of electoral disproportionality (higher values = more disproportional)

| Democracy | District Magnitude (X) | Gallagher Index (Y) |
|-----------|------------------------|---------------------|
| 1         | 2                      | 18.2                |
| 2         | 3                      | 16.7                |
| 3         | 4                      | 15.8                |
| 4         | 5                      | 15.3                |
| 5         | 6                      | 15.0                |
| 6         | 7                      | 14.8                |
| 7         | 8                      | 14.7                |
| 8         | 9                      | 14.6                |
| 9         | 10                     | 14.55               |
| 10        | 11                     | 14.52               |

### Tasks

**A. Preliminary Data Analysis**

1. Calculate the **mean** district magnitude and the **mean** Gallagher index for the sample.
2. Calculate the **standard deviation** for both district magnitude and the Gallagher index.
3. Based on a visual inspection of the data, do you expect a positive or negative relationship between district magnitude and disproportionality? Does this align with political science theory?

**B. Covariance**

4. Calculate the **covariance** between district magnitude (X) and the Gallagher index (Y). Show your work step by step using the formula:

   $$\text{Cov}(X,Y) = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{n-1}$$
   

5. Is the covariance positive or negative? What does this tell you about the relationship between district magnitude and disproportionality?

6. What are the units of your covariance? Why does this make it difficult to interpret the strength of the relationship?

**C. Pearson Correlation Coefficient**

7. Calculate the **Pearson correlation coefficient (r)** using the formula:
   $$r = \frac{\text{Cov}(X,Y)}{s_X \cdot s_Y}$$

8. Interpret the Pearson correlation coefficient:
   - Is the relationship positive or negative?
   - How strong is the linear relationship (weak, moderate, or strong)?
   - What does this tell you about the district magnitude-disproportionality relationship?

9. Calculate **r²** and interpret what it means in this context.

10. Why is the Pearson correlation coefficient more useful than covariance for describing the strength of a linear relationship?

**D. Simple Linear Regression (OLS)**

11. Calculate the **slope (b₁)** of the regression line using the Ordinary Least Squares (OLS) method:
    $$b_1 = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}$$

12. Calculate the **intercept (b₀)** of the regression line:
    $$b_0 = \bar{y} - b_1\bar{x}$$

13. Write the complete **regression equation**: $\hat{Y} = b_0 + b_1X$

14. Interpret the slope coefficient in substantive political science terms. What does it mean for electoral systems?

15. Use your regression equation to predict the Gallagher index for a democracy with a district magnitude of 7.5.

16. What is the relationship between the slope (b₁) and the Pearson correlation coefficient (r)? Show this mathematically.

**E. Coefficient of Determination (R²)**

17. For each observation, calculate the **predicted value** ($\hat{y}_i$) using your regression equation.

18. For each observation, calculate the **residual** ($e_i = y_i - \hat{y}_i$).

19. Calculate the **Total Sum of Squares (SST)**: $SST = \sum(y_i - \bar{y})^2$

20. Calculate the **Regression Sum of Squares (SSR)**: $SSR = \sum(\hat{y}_i - \bar{y})^2$

21. Calculate the **Residual Sum of Squares (SSE)**: $SSE = \sum(y_i - \hat{y}_i)^2$

22. Verify that: $SST = SSR + SSE$

23. Calculate the **coefficient of determination (R²)**: $R^2 = \frac{SSR}{SST}$

24. Verify that your R² equals the square of the Pearson correlation coefficient from part C.

25. Interpret the R² value in the context of electoral systems. What percentage of variation in disproportionality is explained by district magnitude?

26. What is the difference between r and R² in terms of interpretation? When would you report one versus the other?

**F. Interpretation and Conclusions**

27. Based on all your analyses (covariance, Pearson correlation, regression), summarize the relationship between district magnitude and electoral disproportionality.

28. Do your findings support Duverger's hypothesis? Explain with reference to your statistical results.

29. Compare the information provided by the following measures:
    - Covariance
    - Pearson correlation (r)
    - Regression slope (b₁)
    - R²

30. What are the limitations of this analysis? Consider:
    - Sample size
    - External validity (generalizability)
    - Omitted variables (what other factors might affect disproportionality?)

31. Would you feel confident using your regression model to predict disproportionality for a democracy with DM = 20? Why or why not? What statistical concept is relevant here?

32. Suggest three additional variables that could be included in a multiple regression model to better explain electoral disproportionality.

33. Based on your regression results, what policy recommendation would you make to a country that wants to reduce electoral disproportionality?

---

## Exercise 8: Sum and Product Rules in Counting

Consider the following counting problems and determine whether to use the **sum rule** (addition) or the **product rule** (multiplication).

**a.** A restaurant offers 4 types of soup, 6 main courses, and 3 desserts. If a complete meal consists of one soup, one main course, and one dessert, how many different complete meals are possible?

**b.** A student can take either a bus or a train to get to campus. There are 3 different bus lines and 2 different train lines. How many different ways can the student travel to campus?

**c.** To create a password, you must choose 2 letters followed by 3 digits. How many different passwords are possible (assuming letters and digits can be repeated)?

**d.** A committee needs either 1 representative from the Math department (5 candidates) OR 1 representative from the Physics department (4 candidates), but not both. How many ways can the representative be chosen?

**Hints:**

- **Product Rule (Multiply)**: Use when tasks are performed in sequence AND all tasks must be completed. Think: "this AND that AND that..."

- **Sum Rule (Add)**: Use when there are mutually exclusive alternatives. Think: "this OR that (but not both)..."

- For multiple choices in sequence where each choice is independent, multiply the number of options at each step.

- For alphabet: 26 letters; for digits: 10 digits (0-9).

**Answers:**

**a.** Product rule: $4 \times 6 \times 3 = 72$ different complete meals.

*Explanation*: You must choose soup AND main course AND dessert (all three selections in sequence).

**b.** Sum rule: $3 + 2 = 5$ different ways.

*Explanation*: You choose bus OR train (mutually exclusive alternatives).

**c.** Product rule: $26 \times 26 \times 10 \times 10 \times 10 = 676{,}000$ different passwords.

*Explanation*: You must choose 1st letter AND 2nd letter AND 1st digit AND 2nd digit AND 3rd digit (five sequential choices).

**d.** Sum rule: $5 + 4 = 9$ different ways.

*Explanation*: You choose from Math OR from Physics (mutually exclusive alternatives - cannot choose both).

---

## Exercise 9: Sampling with and without Replacement

An urn contains 3 green balls and 2 red balls. Two balls are drawn from the urn. Calculate the probability for each of the following scenarios:

a. The first ball is red and the second ball is green (order matters); drawing **without replacement**.

b. The first ball is red and the second ball is green (order matters); drawing **with replacement**.

c. The balls are of different colors (order doesn't matter); drawing **without replacement**.

d. The balls are of different colors (order doesn't matter); drawing **with replacement**.

**Hints:**

- When drawing *without replacement*, the total number of balls and the composition change after the first draw.

- When drawing *with replacement*, each draw is independent with the same probabilities.

- When order doesn't matter, consider all possible ways to achieve the outcome (e.g., red-then-green OR green-then-red).

- Use the multiplication rule for sequential events: $P(A \text{ and } B) = P(A) \times P(B|A)$

- This problem can be solved using **probability trees** or by constructing the **sample space** visualized as a 2D grid, then applying the classical (naive) definition of probability: $P(A) = \frac{\text{favorable outcomes}}{\text{total outcomes}}$

**Solutions:**

**a.** Without replacement, red then green:

$$P(\text{Red}_1 \cap \text{Green}_2) = \frac{2}{5} \times \frac{3}{4} = \frac{6}{20} = \frac{3}{10}$$

*Explanation*: Initially 2 red out of 5 balls; after removing one red, 3 green out of 4 remaining balls.

**b.** With replacement, red then green:

$$P(\text{Red}_1 \cap \text{Green}_2) = \frac{2}{5} \times \frac{3}{5} = \frac{6}{25}$$

*Explanation*: Each draw is independent with the same probabilities (2 red and 3 green out of 5 balls each time).

**c.** Without replacement, different colors (order doesn't matter):

$$P(\text{Different}) = P(\text{Red}_1 \cap \text{Green}_2) + P(\text{Green}_1 \cap \text{Red}_2) = \frac{2}{5} \times \frac{3}{4} + \frac{3}{5} \times \frac{2}{4} = \frac{6}{20} + \frac{6}{20} = \frac{12}{20} = \frac{3}{5}$$

*Explanation*: Add probabilities for both orderings (red-green OR green-red).

**d.** With replacement, different colors (order doesn't matter):

$$P(\text{Different}) = P(\text{Red}_1 \cap \text{Green}_2) + P(\text{Green}_1 \cap \text{Red}_2) = \frac{2}{5} \times \frac{3}{5} + \frac{3}{5} \times \frac{2}{5} = \frac{6}{25} + \frac{6}{25} = \frac{12}{25}$$

*Explanation*: Add probabilities for both orderings, with each draw independent.

**Alternative approach using sample space:**

- **With replacement**: Create a 5×5 grid where rows represent the first draw (R1, R2, G1, G2, G3) and columns represent the second draw (R1, R2, G1, G2, G3). Total outcomes: 25. Count favorable outcomes in the grid.

- **Without replacement**: Create a grid excluding diagonal elements (can't draw the same ball twice). Total outcomes: 20. Count favorable outcomes in the grid.

---


## Exercise 10: Drawing Balls from an Urn

An urn contains 4 red balls and 3 black balls (balls of the same color are indistinguishable). Two balls are drawn from the urn **without replacement**.

What is the probability that:

a. Both balls are red.

b. The first ball is red and the second ball is black.

c. The balls are of different colors.

**Hints:**

- When drawing without replacement, the total number of balls and the composition change after the first draw.

- Use the multiplication rule: $P(A \cap B) = P(A) \times P(B|A)$

- When order doesn't matter (part c), consider all possible orderings.

- This problem can be solved using **probability trees** or by listing the **sample space** as ordered pairs. For the sample space approach, label the balls (R1, R2, R3, R4, B1, B2, B3) to ensure equally likely outcomes, even though balls of the same color are physically indistinguishable.

**Solutions:**

**a.** Both balls are red:

$$P(\text{Red}_1 \cap \text{Red}_2) = \frac{4}{7} \times \frac{3}{6} = \frac{12}{42} = \frac{2}{7}$$

*Explanation*: Initially 4 red out of 7 balls; after removing one red ball, 3 red out of 6 remaining balls.

**b.** First red, second black:

$$P(\text{Red}_1 \cap \text{Black}_2) = \frac{4}{7} \times \frac{3}{6} = \frac{12}{42} = \frac{2}{7}$$

*Explanation*: Initially 4 red out of 7 balls; after removing one red ball, 3 black out of 6 remaining balls.

**c.** Different colors (order doesn't matter):

$$P(\text{Different}) = P(\text{Red}_1 \cap \text{Black}_2) + P(\text{Black}_1 \cap \text{Red}_2) = \frac{4}{7} \times \frac{3}{6} + \frac{3}{7} \times \frac{4}{6} = \frac{12}{42} + \frac{12}{42} = \frac{24}{42} = \frac{4}{7}$$

*Explanation*: Add probabilities for both orderings (red-black OR black-red).

**Alternative approach using sample space:**

Label the balls as R1, R2, R3, R4, B1, B2, B3 to create a sample space with equally likely outcomes. Create a 7×6 grid (or list all ordered pairs) where the first coordinate represents the first draw and the second coordinate represents the second draw. Since we draw without replacement, exclude cases where the same ball is drawn twice. Total outcomes: $7 \times 6 = 42$. Count favorable outcomes for each scenario (e.g., for part a, count all pairs where both coordinates are red balls).

---

## Exercise 11: Complement Rule

A bag contains 5 red balls and 3 blue balls. Two balls are drawn from the bag **without replacement**, one after another.

Calculate the probability that at least one ball is red.

**Hints:**

- "At least one" means one or more (one red OR two red).

- The **complement rule** is often easier: $P(A) = 1 - P(A^c)$, where $A^c$ is the complement of event $A$.

- The complement of "at least one red" is "no red balls" (i.e., "both blue").

- Calculate $P(\text{both blue})$ first, then use the complement rule.

**Solution:**

Let $A$ = "at least one ball is red"

The complement is $A^c$ = "both balls are blue"

$$P(\text{both blue}) = \frac{3}{8} \times \frac{2}{7} = \frac{6}{56} = \frac{3}{28}$$

Using the complement rule:

$$P(\text{at least one red}) = 1 - P(\text{both blue}) = 1 - \frac{3}{28} = \frac{25}{28}$$

**Alternative direct approach:**

$$P(\text{at least one red}) = P(\text{Red}_1 \cap \text{Blue}_2) + P(\text{Blue}_1 \cap \text{Red}_2) + P(\text{Red}_1 \cap \text{Red}_2)$$

$$= \frac{5}{8} \times \frac{3}{7} + \frac{3}{8} \times \frac{5}{7} + \frac{5}{8} \times \frac{4}{7} = \frac{15}{56} + \frac{15}{56} + \frac{20}{56} = \frac{50}{56} = \frac{25}{28}$$

The complement approach is simpler!

---

## Exercise 12: Addition Rule for Probability

Consider a standard deck of 52 playing cards. A card is selected at random.

What is the probability that the card is a diamond or a ten?

**Hints:**

- A standard deck has 4 suits (hearts, diamonds, clubs, spades), each with 13 cards.

- Use the **addition rule**: $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

- Be careful not to count cards twice! The ten of diamonds is both a diamond AND a ten.

- Identify: How many diamonds? How many tens? How many cards are both?

**Solution:**

Let $A$ = "card is a diamond" and $B$ = "card is a ten"

- Number of diamonds: 13
- Number of tens: 4 (one in each suit)
- Number of cards that are both diamond and ten: 1 (ten of diamonds)

Using the addition rule:

$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

$$= \frac{13}{52} + \frac{4}{52} - \frac{1}{52} = \frac{16}{52} = \frac{4}{13}$$

*Explanation*: We add the probabilities of drawing a diamond and drawing a ten, but we must subtract the probability of drawing the ten of diamonds (which was counted twice).

---

## Exercise 13: Conditional Probability and Multiplication Rule

A box contains blocks of different shapes and colors:

- Triangular blocks: 3 red and 2 green
- Oval blocks: 2 red and 3 green  
- Quadrilateral blocks: 3 green (no red)

One block is randomly selected from the box.

Calculate the following probabilities:

a. $P(\text{triangle})$

b. $P(\text{green})$

c. $P(\text{triangle} \cap \text{green})$

d. $P(\text{triangle} \mid \text{green})$

e. $P(\text{green} \mid \text{triangle})$

f. Verify the multiplication rule: $P(A \cap B) = P(A) \times P(B \mid A) = P(B) \times P(A \mid B)$

**Hints:**

- First, count the total number of blocks in the box.

- **Conditional probability formula**: $P(A \mid B) = \frac{P(A \cap B)}{P(B)}$

- $P(A \mid B)$ reads as "probability of $A$ given $B$" - it represents the probability of $A$ occurring when we know that $B$ has occurred.

- **Multiplication rule**: $P(A \cap B) = P(A) \times P(B \mid A) = P(B) \times P(A \mid B)$

- For conditional probability, restrict your sample space to only the outcomes where the condition is satisfied.

**Solutions:**

Total number of blocks: $5 + 5 + 3 = 13$

**a.** Probability of selecting a triangle:

$$P(\text{triangle}) = \frac{5}{13}$$

*Explanation*: There are 5 triangular blocks out of 13 total blocks.

**b.** Probability of selecting a green block:

$$P(\text{green}) = \frac{2 + 3 + 3}{13} = \frac{8}{13}$$

*Explanation*: There are 8 green blocks (2 triangular + 3 oval + 3 quadrilateral) out of 13 total blocks.

**c.** Probability of selecting a block that is both triangle and green:

$$P(\text{triangle} \cap \text{green}) = \frac{2}{13}$$

*Explanation*: There are 2 blocks that are both triangular and green out of 13 total blocks.

**d.** Probability of triangle given that the block is green:

$$P(\text{triangle} \mid \text{green}) = \frac{P(\text{triangle} \cap \text{green})}{P(\text{green})} = \frac{2/13}{8/13} = \frac{2}{8} = \frac{1}{4}$$

*Alternative reasoning*: If we know the block is green, we restrict our sample space to the 8 green blocks. Of these 8 green blocks, 2 are triangular. Therefore, $P(\text{triangle} \mid \text{green}) = \frac{2}{8} = \frac{1}{4}$.

**e.** Probability of green given that the block is a triangle:

$$P(\text{green} \mid \text{triangle}) = \frac{P(\text{triangle} \cap \text{green})}{P(\text{triangle})} = \frac{2/13}{5/13} = \frac{2}{5}$$

*Alternative reasoning*: If we know the block is triangular, we restrict our sample space to the 5 triangular blocks. Of these 5 triangular blocks, 2 are green. Therefore, $P(\text{green} \mid \text{triangle}) = \frac{2}{5}$.

**f.** Verification of the multiplication rule:

$$P(\text{triangle} \cap \text{green}) = P(\text{triangle}) \times P(\text{green} \mid \text{triangle}) = \frac{5}{13} \times \frac{2}{5} = \frac{2}{13}$$ ✓

$$P(\text{triangle} \cap \text{green}) = P(\text{green}) \times P(\text{triangle} \mid \text{green}) = \frac{8}{13} \times \frac{1}{4} = \frac{2}{13}$$ ✓

Both formulations give the same result!

---

## Exercise 14: Bayes' Theorem and the Law of Total Probability

Last week, Alice went to her doctor for a routine medical check-up. This morning, her doctor called to tell her that the result of one of the tests is positive, indicating that she may have a disease, which we'll denote by $D$. The prevalence of this disease in the population for people of Alice's age is 1 in 1000.

Thinking there might be an error, Alice asked about the accuracy of the test. The doctor told her that the test is 95% accurate in both directions:

- When someone has disease $D$, the test correctly detects it 95% of the time (sensitivity or true positive rate).
- When someone does not have disease $D$, the test is correctly negative 95% of the time (specificity or true negative rate).

What is the probability that Alice actually has the disease given that she tested positive? Calculate $P(D \mid T^+)$.

**Hints:**

- Let $D$ = "Alice has the disease" and $T^+$ = "Test is positive"
- Given information:
  - $P(D) = \frac{1}{1000} = 0.001$ (prevalence)
  - $P(T^+ \mid D) = 0.95$ (sensitivity)
  - $P(T^- \mid D^c) = 0.95$ (specificity), which means $P(T^+ \mid D^c) = 0.05$ (false positive rate)
- Use **Bayes' theorem**: $P(D \mid T^+) = \frac{P(T^+ \mid D) \times P(D)}{P(T^+)}$
- Use the **law of total probability** to find $P(T^+)$:
  $$P(T^+) = P(T^+ \mid D) \times P(D) + P(T^+ \mid D^c) \times P(D^c)$$

**Solution:**

**Step 1:** Identify the known probabilities

- $P(D) = 0.001$ (prevalence in population)
- $P(D^c) = 1 - 0.001 = 0.999$ (probability of not having disease)
- $P(T^+ \mid D) = 0.95$ (sensitivity)
- $P(T^+ \mid D^c) = 0.05$ (false positive rate)

**Step 2:** Calculate $P(T^+)$ using the law of total probability

The law of total probability partitions the event "positive test" into two mutually exclusive scenarios: testing positive when having the disease OR testing positive when not having the disease.

$$P(T^+) = P(T^+ \mid D) \times P(D) + P(T^+ \mid D^c) \times P(D^c)$$

$$P(T^+) = 0.95 \times 0.001 + 0.05 \times 0.999$$

$$P(T^+) = 0.00095 + 0.04995 = 0.0509$$

**Step 3:** Apply Bayes' theorem

$$P(D \mid T^+) = \frac{P(T^+ \mid D) \times P(D)}{P(T^+)} = \frac{0.95 \times 0.001}{0.0509} = \frac{0.00095}{0.0509} \approx 0.0187$$

$$P(D \mid T^+) \approx 1.87\% \approx \frac{1}{54}$$

**Interpretation:**

Despite the test being 95% accurate, the probability that Alice actually has the disease given a positive test result is only about **1.9%** or roughly **1 in 54**!

This counterintuitive result occurs because the disease is very rare (only 1 in 1000 people have it). Even though the test is quite accurate, the large number of healthy people (999 out of 1000) means that the 5% false positive rate generates many more false positives than there are true positives.

**Visualization using a population of 100,000 people:**

- People with disease $D$: $100{,}000 \times 0.001 = 100$
  - True positives: $100 \times 0.95 = 95$
  - False negatives: $100 \times 0.05 = 5$

- People without disease $D$: $100{,}000 \times 0.999 = 99{,}900$
  - False positives: $99{,}900 \times 0.05 = 4{,}995$
  - True negatives: $99{,}900 \times 0.95 = 94{,}905$

Total positive tests: $95 + 4{,}995 = 5{,}090$

$$P(D \mid T^+) = \frac{95}{5{,}090} \approx 0.0187 \approx 1.87\%$$

---

## Exercise 15: COVID-19 Testing and Base Rate

Suppose you think you may have contracted COVID-19, an infectious disease caused by the SARS-CoV-2 virus. You decide to take a diagnostic test, and you want to use its result to determine if you are infected or not.

**Test characteristics:**

- **Sensitivity** (true positive rate): $P(T = 1 \mid D = 1) = 87.5\%$ – the probability of a positive test given that you actually have the disease.
- **Specificity** (true negative rate): $P(T = 0 \mid D = 0) = 97.5\%$ – the probability of a negative test given that you do not have the disease.
- **False positive rate**: $P(T = 1 \mid D = 0) = 1 - 0.975 = 2.5\%$
- **False negative rate**: $P(T = 0 \mid D = 1) = 1 - 0.875 = 12.5\%$

where $D = 1$ means "has disease" and $T = 1$ means "test is positive".

**Task:**

Calculate $P(D = 1 \mid T = 1)$ (the probability that you have COVID-19 given a positive test) for two scenarios:

a. **High prevalence**: $P(D = 1) = 0.1$ (10%), which was the prevalence in New York City in Spring 2020.

b. **Low prevalence**: $P(D = 1) = 0.01$ (1%), representing a lower community transmission rate.

**Hints:**

- Use Bayes' theorem: $P(D = 1 \mid T = 1) = \frac{P(T = 1 \mid D = 1) \times P(D = 1)}{P(T = 1)}$
- Use the law of total probability to find $P(T = 1)$

**Solutions:**

**a. High prevalence (10%):**

Given:
- $P(D = 1) = 0.1$, so $P(D = 0) = 0.9$
- $P(T = 1 \mid D = 1) = 0.875$
- $P(T = 1 \mid D = 0) = 0.025$

Calculate $P(T = 1)$:
$$P(T = 1) = P(T = 1 \mid D = 1) \times P(D = 1) + P(T = 1 \mid D = 0) \times P(D = 0)$$
$$P(T = 1) = 0.875 \times 0.1 + 0.025 \times 0.9 = 0.0875 + 0.0225 = 0.11$$

Apply Bayes' theorem:
$$P(D = 1 \mid T = 1) = \frac{0.875 \times 0.1}{0.11} = \frac{0.0875}{0.11} \approx 0.795 = 79.5\%$$

**b. Low prevalence (1%):**

Given:
- $P(D = 1) = 0.01$, so $P(D = 0) = 0.99$
- $P(T = 1 \mid D = 1) = 0.875$
- $P(T = 1 \mid D = 0) = 0.025$

Calculate $P(T = 1)$:
$$P(T = 1) = 0.875 \times 0.01 + 0.025 \times 0.99 = 0.00875 + 0.02475 = 0.0335$$

Apply Bayes' theorem:
$$P(D = 1 \mid T = 1) = \frac{0.875 \times 0.01}{0.0335} = \frac{0.00875}{0.0335} \approx 0.261 = 26.1\%$$

**Key insight:**

The **base rate** (prevalence) dramatically affects the interpretation of test results:

- At 10% prevalence: A positive test means you have about an **80% chance** of being infected.
- At 1% prevalence: A positive test means you have only about a **26% chance** of being infected.

This demonstrates why the same test can have very different interpretations depending on the disease prevalence in your community!

---

## Exercise 16: Independence of Events

A fair coin is flipped 3 times. Let $H_1$ = "tails on the first flip" and $A$ = "exactly two tails in total".

Are events $H_1$ and $A$ independent?

**Hints:**

- Two events $A$ and $B$ are **independent** if and only if: $P(A \cap B) = P(A) \times P(B)$

- Alternatively, events are independent if: $P(A \mid B) = P(A)$ (knowing $B$ doesn't change the probability of $A$)

- List the sample space for three coin flips and identify which outcomes belong to each event.

- For a fair coin, each outcome (sequence of 3 flips) has probability $(1/2)^3 = 1/8$.

**Solution:**

**Step 1:** List the sample space

$$\Omega = \{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\}$$

Total outcomes: 8, each with probability $1/8$.

**Step 2:** Identify outcomes for each event

- $H_1$ = "tails on first flip" = $\{THH, THT, TTH, TTT\}$
  $$P(H_1) = \frac{4}{8} = \frac{1}{2}$$

- $A$ = "exactly two tails" = $\{HTT, THT, TTH\}$
  $$P(A) = \frac{3}{8}$$

- $H_1 \cap A$ = "tails on first flip AND exactly two tails" = $\{THT, TTH\}$
  $$P(H_1 \cap A) = \frac{2}{8} = \frac{1}{4}$$

**Step 3:** Check independence condition

If independent, we need: $P(H_1 \cap A) = P(H_1) \times P(A)$

$$P(H_1) \times P(A) = \frac{1}{2} \times \frac{3}{8} = \frac{3}{16}$$

$$P(H_1 \cap A) = \frac{2}{8} = \frac{4}{16}$$

Since $\frac{4}{16} \neq \frac{3}{16}$, the events are **NOT independent**.

**Alternative approach using conditional probability:**

$$P(A \mid H_1) = \frac{P(H_1 \cap A)}{P(H_1)} = \frac{2/8}{1/2} = \frac{2}{8} \times 2 = \frac{4}{8} = \frac{1}{2}$$

Since $P(A \mid H_1) = \frac{1}{2} \neq \frac{3}{8} = P(A)$, the events are **NOT independent**.

**Intuitive explanation:**

Knowing that the first flip is tails changes the probability of getting exactly two tails total. If the first flip is tails, we need exactly one more tail in the remaining two flips (probability = $\frac{1}{2}$). Without this information, the probability of exactly two tails in three flips is $\frac{3}{8}$. Since these probabilities differ, the events are dependent.

---

## Exercise 17: Hypothesis Testing as Probabilistic Proof by Contradiction

A man claims to have extrasensory perception (ESP). As a test, a fair coin is flipped 10 times and the man is asked to predict the outcome in advance. He gets 7 out of 10 correct.

What is the probability that he would have done at least this well (at least 7 correct) if he had no ESP?

**Remark:** It is reasonable to assume that the probability of randomly guessing the outcome is $\frac{1}{2}$ (no ESP).

**The Logic of Hypothesis Testing: Probabilistic Proof by Contradiction**

Hypothesis testing works like a court trial or proof by contradiction:

1. **Presumption of "innocence"**: We start by assuming the man has no ESP (null hypothesis $H_0$: $p = 0.5$), just like a court presumes innocence.

2. **Examine the evidence**: We observe the data (7 out of 10 correct predictions).

3. **Calculate probability of evidence under presumption**: We ask: "If he truly has no ESP, how likely is it to see this evidence (or stronger evidence) by pure chance?"

4. **Make a decision**: 
   - If this probability is **very small** (e.g., < 5%), the evidence is unusual under our presumption, so we have reason to doubt it. We might conclude he has ESP.
   - If this probability is **not small**, the evidence could easily occur by chance, so we have no strong reason to doubt our presumption. We cannot conclude he has ESP.

**Hints:**

- $H_0$: $p = 0.5$ (no ESP, random guessing)
- $H_1$: $p > 0.5$ (has ESP, better than random)
- Number of correct predictions: $X \sim \text{Binomial}(n=10, p=0.5)$
- Calculate: p-value = $P(X \geq 7 \mid p = 0.5)$

**Solution:**

**Step 1:** State the presumption (null hypothesis)

$H_0$: The man has no ESP; he is randomly guessing with probability $p = 0.5$.

**Step 2:** Calculate the p-value

The p-value answers: "If he has no ESP, what is the probability of getting 7 or more correct by pure luck?"

$$\text{p-value} = P(X \geq 7 \mid p = 0.5) = P(X = 7) + P(X = 8) + P(X = 9) + P(X = 10)$$

Using the binomial formula with $n = 10$ and $p = 0.5$:

$$P(X = 7) = \binom{10}{7} (0.5)^{10} = 120 \times \frac{1}{1024} = \frac{120}{1024}$$

$$P(X = 8) = \binom{10}{8} (0.5)^{10} = 45 \times \frac{1}{1024} = \frac{45}{1024}$$

$$P(X = 9) = \binom{10}{9} (0.5)^{10} = 10 \times \frac{1}{1024} = \frac{10}{1024}$$

$$P(X = 10) = \binom{10}{10} (0.5)^{10} = 1 \times \frac{1}{1024} = \frac{1}{1024}$$

$$\text{p-value} = \frac{120 + 45 + 10 + 1}{1024} = \frac{176}{1024} \approx 0.172 = 17.2\%$$

**Step 3:** Interpret the result

The p-value of 17.2% means: **If the man has no ESP and is purely guessing, there is about a 17% chance he would get 7 or more correct out of 10 by random luck alone.**

This is **not a particularly unusual outcome**. About 1 in 6 people randomly guessing would do this well or better.

**Conclusion (using the "proof by contradiction" logic):**

- We started by presuming "no ESP" (like presuming innocence).
- We found that the evidence (7 out of 10 correct) has a **17.2% chance** of occurring by pure chance under this presumption.
- This probability is **not sufficiently small** (it's much larger than the typical 5% threshold).
- Therefore, we **cannot reject** the presumption of "no ESP."
- **Verdict**: The evidence is not strong enough to conclude he has ESP. His performance could easily be explained by luck.

**Analogy to a court trial:**

- Just as a court doesn't prove innocence (it either finds guilt beyond reasonable doubt or fails to do so), we don't prove he has no ESP.
- We simply say: "The evidence is not strong enough to conclude he has ESP beyond reasonable doubt."
- If the p-value were very small (e.g., 0.01 or 1%), we would say: "It's highly unlikely this result occurred by chance, so we have strong evidence for ESP."


---


## Exercise 18: Hypothesis Testing with Binomial Distribution

A candidate in an election believes that approximately 50% ($p = 0.5$) of residents in a certain city support her.

A researcher wants to test whether the candidate correctly estimates her support. Ten people were asked if they support the candidate; 7 people answered yes.

Using the sample data, calculate the so-called **p-value** and decide whether there is sufficiently strong evidence to reject the candidate's hypothesis that $p = 0.5$. Assume the significance level (critical probability) is $\alpha = 0.05$.

Note: We will conduct a **one-sided test** assuming the candidate may **underestimate** her support (i.e., the true support might be higher than 50%).

**Hints:**

- **Null hypothesis** $H_0$: $p = 0.5$ (candidate's support is 50%)
- **Alternative hypothesis** $H_1$: $p > 0.5$ (candidate's support is greater than 50%, meaning she underestimates it)
- The number of supporters follows a **binomial distribution**: $X \sim \text{Binomial}(n=10, p=0.5)$
- **p-value** = probability of observing a result as extreme or more extreme than what we observed, assuming $H_0$ is true
- For a one-sided test (right-tailed): p-value = $P(X \geq 7 \mid p = 0.5)$
- **Binomial probability formula**: $P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}$
- **Decision rule**: If p-value $< \alpha$, reject $H_0$; otherwise, fail to reject $H_0$

**Solution:**

**Step 1:** State the hypotheses

- $H_0$: $p = 0.5$ (the candidate's estimate is correct)
- $H_1$: $p > 0.5$ (the candidate underestimates her support)

**Step 2:** Calculate the p-value

The p-value is the probability of getting 7 or more supporters out of 10 people, assuming the true support is 50%:

$$\text{p-value} = P(X \geq 7 \mid p = 0.5) = P(X = 7) + P(X = 8) + P(X = 9) + P(X = 10)$$

For each term, use the binomial formula with $n = 10$ and $p = 0.5$:

$$P(X = 7) = \binom{10}{7} (0.5)^{7} (0.5)^{3} = 120 \times (0.5)^{10} = \frac{120}{1024}$$

$$P(X = 8) = \binom{10}{8} (0.5)^{8} (0.5)^{2} = 45 \times (0.5)^{10} = \frac{45}{1024}$$

$$P(X = 9) = \binom{10}{9} (0.5)^{9} (0.5)^{1} = 10 \times (0.5)^{10} = \frac{10}{1024}$$

$$P(X = 10) = \binom{10}{10} (0.5)^{10} (0.5)^{0} = 1 \times (0.5)^{10} = \frac{1}{1024}$$

Sum these probabilities:

$$\text{p-value} = \frac{120 + 45 + 10 + 1}{1024} = \frac{176}{1024} \approx 0.172$$

**Step 3:** Make a decision

Compare the p-value with the significance level $\alpha = 0.05$:

$$\text{p-value} = 0.172 > 0.05 = \alpha$$

Since the p-value is greater than the significance level, we **fail to reject** the null hypothesis $H_0$.

**Conclusion:**

There is **not sufficient evidence** at the 5% significance level to conclude that the candidate underestimates her support. While 7 out of 10 supporters seems high, this result could reasonably occur by chance even if the true support is only 50%. The observed data is consistent with the candidate's estimate of 50% support.

**Interpretation of p-value:**

The p-value of 0.172 means that if the true support is indeed 50%, there is about a 17.2% chance of observing 7 or more supporters in a random sample of 10 people. This is not an unusual occurrence, so we don't have strong evidence against the candidate's claim.

---

## Exercise 19: Hypothesis Testing for Proportions (One-Sided Test)

A politician believes that support for her country's EU membership is about 98% ($p = 0.98$). A researcher suspects this might be an overestimation and conducts a survey. In a sample of 15 people ($n = 15$), the researcher observes that 13 people support membership.

Let $X$ = the number of people in the sample who support EU membership. We observed $X = 13$ "successes" in 15 Bernoulli trials.

**Task:**

Test whether there is sufficient evidence to conclude that the politician overestimates support for EU membership. Use a significance level of $\alpha = 0.05$.

**Hints:**

- $H_0$: $p = 0.98$ (politician's claim is correct)
- $H_1$: $p < 0.98$ (researcher suspects overestimation; true support is lower)
- This is a **left-tailed (one-sided) test**
- $X \sim \text{Binomial}(n=15, p=0.98)$ under $H_0$
- p-value = $P(X \leq 13 \mid p = 0.98)$
- Use the **complement rule** to simplify: $P(X \leq 13) = 1 - P(X > 13) = 1 - [P(X = 14) + P(X = 15)]$

**Solution:**

**Step 1:** State the hypotheses

- $H_0$: $p = 0.98$ (support is 98% as the politician claims)
- $H_1$: $p < 0.98$ (support is less than 98%; politician overestimates)

**Step 2:** Calculate the p-value

The p-value is the probability of observing 13 or fewer supporters (out of 15) if the true support is 98%:

$$\text{p-value} = P(X \leq 13 \mid p = 0.98)$$

Using the **complement rule** (calculating fewer terms):

$$P(X \leq 13) = 1 - P(X > 13) = 1 - [P(X = 14) + P(X = 15)]$$

Calculate $P(X = 14)$:
$$P(X = 14) = \binom{15}{14} (0.98)^{14} (0.02)^{1} = 15 \times (0.98)^{14} \times 0.02$$
$$= 15 \times 0.7536 \times 0.02 = 0.2261$$

Calculate $P(X = 15)$:
$$P(X = 15) = \binom{15}{15} (0.98)^{15} (0.02)^{0} = 1 \times (0.98)^{15}$$
$$= 0.7386$$

Therefore:
$$P(X > 13) = P(X = 14) + P(X = 15) = 0.2261 + 0.7386 = 0.9647$$

$$\text{p-value} = P(X \leq 13) = 1 - 0.9647 = 0.0353 = 3.53\%$$

**Step 3:** Make a decision

Compare with significance level $\alpha = 0.05$:
$$\text{p-value} = 0.0353 < 0.05 = \alpha$$

Since the p-value is less than the significance level, we **reject** the null hypothesis $H_0$.

**Conclusion:**

There **is sufficient evidence** at the 5% significance level to conclude that the politician overestimates support for EU membership. The true support appears to be lower than 98%.

**Interpretation:**

Observing only 13 out of 15 people (86.7%) in support would occur only about 3.5% of the time by chance if the true support were really 98%. This is sufficiently unusual to conclude that the politician's estimate of 98% is likely too high.

**Key insight:**

Even though 13 out of 15 (86.7%) might seem close to 98%, when the claimed proportion is very high (like 98%), even small deviations become statistically significant. With such a high claimed rate, we would expect to see 14 or 15 supporters in nearly every sample of 15 people (96.5% of the time). Seeing only 13 is unusual enough to cast doubt on the 98% claim.

---


::: {.callout-note icon=false}
## Understanding Bessel's Correction and Degrees of Freedom

### Why Do We Use $n-1$ Instead of $n$?

When calculating **sample variance**, we use $n-1$ in the denominator rather than $n$. This adjustment is called **Bessel's correction**. To understand why this is necessary, we need to explore several key ideas.

### The Sample Mean as a "Constant Model"

The sample mean $\bar{x}$ has a special property: **it minimizes the sum of squared deviations from the data**.

**Claim:** Among all possible values $c$, the value $\bar{x}$ makes $\sum_{i=1}^{n}(x_i - c)^2$ as small as possible.

**Proof:**

We want to find $c$ that minimizes:
$$f(c) = \sum_{i=1}^{n}(x_i - c)^2$$

Taking the derivative with respect to $c$:
$$\frac{df}{dc} = -2\sum_{i=1}^{n}(x_i - c)$$

Setting equal to zero:
$$\sum_{i=1}^{n}(x_i - c) = 0 \implies \sum_{i=1}^{n}x_i = nc \implies c = \frac{1}{n}\sum_{i=1}^{n}x_i = \bar{x}$$

The second derivative $\frac{d^2f}{dc^2} = 2n > 0$ confirms this is a minimum. Therefore, $\bar{x}$ is the **least-squares fit** to the data—it's the simplest possible model (a constant) that best fits our observations in the least-squares sense.

### Two Important Properties of the Mean

It's crucial to distinguish between two related but different properties:

**Property 1: Sum of deviations equals zero**
$$\sum_{i=1}^{n}(x_i - \bar{x}) = 0$$

This follows directly from the definition of $\bar{x}$. It means the positive deviations exactly cancel the negative deviations.

**Property 2: Minimizes sum of squared deviations**
$$\bar{x} = \arg\min_{c} \sum_{i=1}^{n}(x_i - c)^2$$

This means $\bar{x}$ makes the data appear as "tight" as possible (in the least-squares sense). Property 1 is actually the **first-order condition** we get when finding the minimum in Property 2.

### Why This Creates Bias

Because $\bar{x}$ minimizes $\sum(x_i - \bar{x})^2$ for our specific sample, the sum of squared deviations from $\bar{x}$ will be **systematically smaller** than the sum of squared deviations from the true population mean $\mu$ (which we don't know).

**Mathematically:**
$$\sum_{i=1}^{n}(x_i - \bar{x})^2 \leq \sum_{i=1}^{n}(x_i - \mu)^2$$

with strict inequality when $\bar{x} \neq \mu$ (which is typical).

**Concrete Example:**

Suppose the true population is $\{46, 48, 50, 52, 54\}$ with mean $\mu = 50$.

Our sample is $\{46, 48, 52\}$ with mean $\bar{x} = 48.67$.

- Sum of squared deviations from $\bar{x}$: 
  $$(46-48.67)^2 + (48-48.67)^2 + (52-48.67)^2 = 18.67$$

- Sum of squared deviations from $\mu = 50$: 
  $$(46-50)^2 + (48-50)^2 + (52-50)^2 = 24$$

Notice $18.67 < 24$. Our sample appears "tighter" around $\bar{x}$ than it would around the true mean $\mu$.

If we naively used:
$$\frac{1}{n}\sum_{i=1}^{n}(x_i - \bar{x})^2 = \frac{18.67}{3} = 6.22$$

we would **underestimate** the true population variance.

### Population vs. Sample Variance: Why the Difference?

**Population Variance (uses $N$):**
$$\sigma^2 = \frac{1}{N}\sum_{i=1}^{N}(x_i - \mu)^2$$

- We know the **true mean** $\mu$
- We're computing a **descriptive parameter**, not estimating
- No bias because $\mu$ is the actual population mean
- We have all $N$ values

**Sample Variance (uses $n-1$):**
$$s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2$$

- We **don't know** $\mu$, so we estimate it with $\bar{x}$
- We're using an **estimator** to infer about the population
- Using $\bar{x}$ (calculated from the same data) creates downward bias
- Using $n-1$ instead of $n$ corrects this bias

### Mathematical Justification

It can be proven that:

**If we use $n$:**
$$E\left[\frac{1}{n}\sum_{i=1}^{n}(x_i - \bar{x})^2\right] = \frac{n-1}{n}\sigma^2$$

This is **biased**—it systematically underestimates $\sigma^2$ by a factor of $\frac{n-1}{n}$.

**If we use $n-1$:**
$$E\left[\frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2\right] = \sigma^2$$

This is **unbiased**—on average, it gives the correct population variance.

### Degrees of Freedom: The Intuition

The term "degrees of freedom" captures the idea that we've "used up" one piece of information from our data.

**Before calculating $\bar{x}$:** All $n$ observations are "free" and independent.

**After calculating $\bar{x}$:** Once we know $n-1$ of the deviations $(x_i - \bar{x})$, the $n$-th deviation is **completely determined** by the constraint:
$$\sum_{i=1}^{n}(x_i - \bar{x}) = 0$$

**Example:** If $n=3$ and we know $(x_1 - \bar{x}) = -2$ and $(x_2 - \bar{x}) = 1$, then we automatically know $(x_3 - \bar{x}) = 1$ (to make the sum equal zero).

So we have only $(n-1)$ "degrees of freedom" for estimating variance. We've spent one degree of freedom estimating $\mu$ with $\bar{x}$, leaving us with $(n-1)$ independent pieces of information.

### Summary

1. **$\bar{x}$ minimizes $\sum(x_i - \bar{x})^2$**: It's the least-squares constant model for the data
2. **This creates bias**: Deviations from $\bar{x}$ are artificially small compared to deviations from $\mu$
3. **$n-1$ corrects the bias**: Makes $s^2$ an unbiased estimator of $\sigma^2$
4. **Population variance uses $N$**: Because we know the true $\mu$, no estimation, no bias
5. **Degrees of freedom $(n-1)$**: We've used one piece of information to estimate the mean

**Important note about standard deviation:** While Bessel's correction makes the **variance** $s^2$ unbiased, the **standard deviation** $s = \sqrt{s^2}$ remains slightly biased (tends to underestimate $\sigma$) due to the non-linearity of the square root function. However, this bias is small for reasonably large samples.
:::

---
