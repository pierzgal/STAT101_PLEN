# Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych

Ten rozdział wprowadza studentów w podstawy nauki o danych i statystyki – obszary wiedzy kluczowe dla badaczy nauk społecznych.

## Czym są Statystyka i Nauka o Danych?

::: callout-important
Statystyka i data science to sztuka i nauka wydobywania wiedzy z danych – pomagają zrozumieć świat poprzez metodyczną analizę zebranych informacji.
:::

1.  Nauka o danych i statystyka stanowią kluczowe narzędzia badawcze, które umożliwiają lepsze zrozumienie zjawisk społecznych, niezależnie od specjalizacji: politologii, ekonomii, socjologii czy innych nauk społecznych. Pozwalają analizować trendy, zachowania społeczne i efekty różnych polityk, dostarczając solidnych podstaw do formułowania wniosków opartych na danych empirycznych.

2.  Statystyka zapewnia matematyczne fundamenty analizy danych – uczy projektowania badań, syntetyzowania zebranych informacji i weryfikowania hipotez badawczych. Nauka o danych rozszerza te możliwości, integrując statystykę z umiejętnościami programistycznymi i wiedzą dziedzinową, co pozwala efektywnie pracować nawet ze złożonymi zbiorami danych.

3.  Wspólnie te dziedziny znacząco zwiększają możliwości badawcze. Umożliwiają gromadzenie i analizę dużych zbiorów danych, tworzenie czytelnych wizualizacji złożonych informacji, odkrywanie niewidocznych na pierwszy rzut oka prawidłowości w zachowaniach społecznych oraz ewaluację skuteczności różnych rozwiązań. Te umiejętności mają szerokie zastosowanie – od analizy procesów wyborczych i zjawisk ekonomicznych po badanie nierówności społecznych.

4.  W epoce cyfrowej, charakteryzującej się gwałtownym przyrostem dostępnych danych, kompetencje w zakresie ich analizy stają się niezbędnym elementem warsztatu współczesnych badaczy i specjalistów nauk społecznych.

::: callout-note
W naukach społecznych nauka o danych stanowi zestaw metod do rozwiązywania złożonych problemów badawczych – łączy podejście statystyczne, narzędzia informatyczne i wiedzę specjalistyczną, by skuteczniej analizować procesy społeczne.
:::

## Związek Między Statystyką a Nauką o Danych

Statystyka i data science to ściśle powiązane dziedziny o znaczącym obszarze wspólnym. Zamiast traktować je jako całkowicie odrębne dyscypliny, warto postrzegać je jako komplementarne podejścia w spektrum metod analizy danych:

Nauka o danych może być postrzegana jako współczesne rozwinięcie tradycyjnej statystyki, które ewoluowało w odpowiedzi na nowe możliwości technologiczne i potrzebę analizy coraz bardziej złożonych danych społecznych.

## Podstawowe Koncepcje w Nauce o Danych i Statystyce

### Dane i Populacje (Data and Populations) – kluczowe pojęcia

1.  **Dane**: Informacje zebrane w procesie badawczym – mogą to być odpowiedzi z kwestionariuszy, wyniki eksperymentów, wskaźniki ekonomiczne, treści z mediów społecznościowych lub inne mierzalne obserwacje.

2.  **Populacja**: Pełny zbiór jednostek (osób, instytucji, wydarzeń), których dotyczy badanie – cała grupa, o której badacz chce formułować wnioski.

    -   Przykład: W badaniu preferencji wyborczych populację stanowią wszyscy uprawnieni do głosowania obywatele danego kraju.

3.  **Próba**: Podzbiór populacji wybrany do badania. **Reprezentatywna** próba odzwierciedla kluczowe cechy populacji docelowej w odpowiednich proporcjach. Efektywne pobieranie próbek uwzględnia różnorodność jednostek w zakresie istotnych zmiennych (demograficznych, behawioralnych, itp.). W prostej próbie losowej (SRS), każda jednostka w populacji ma równe i niezależne prawdopodobieństwo wyboru. Reprezentatywność próby zależy zarówno od losowości metody pobierania, jak i od wystarczającej wielkości próby, aby zminimalizować błąd.

    -   Przykład: Zamiast badać wszystkich uprawnionych wyborców, analizuje się 1500 losowo wybranych osób z uwzględnieniem odpowiedniego rozkładu wieku, płci, wykształcenia i regionu zamieszkania.

    Prawidłowo dobrana próba umożliwia wnioskowanie o całej populacji przy znaczącej optymalizacji zasobów badawczych.

4.  **Pobieranie próbek** (*sampling*, próbkowanie) to procedura wybierania jednostek z populacji do badania. Nieobciążona metoda pobierania próbek daje każdej jednostce w populacji równą szansę na wybór, zapewniając reprezentatywne wyniki.

5.  **Wnioskowanie statystyczne** to proces wyciągania wniosków o populacji na podstawie danych z próby. Polega na wykorzystaniu statystyk z próby do oszacowania parametrów populacji, oceny wiarygodności tych oszacowań oraz testowania hipotez dotyczących charakterystyk populacji.

Kluczowe elementy wnioskowania statystycznego:

-   Estymacja: Obliczanie estymatorów punktowych (konkretnych wartości) i przedziałowych (przedziałów ufności) dla parametrów (charakterystyk) populacji, takich jak średnie, proporcje czy wariancje.
-   Testowanie hipotez: Ocena twierdzeń dotyczących parametrów populacji poprzez określenie prawdopodobieństwa uzyskania zaobserwowanych wyników próby, jeśli twierdzenie jest prawdziwe.

![The process of using a sample from a population to obtain a point estimate of a population parameter. In this case, a sample of 10 individuals yielded 6 who own an iPhone, resulting in an estimated population proportion of 60% iPhone owners. The actual population proportion is 53.8%. Retrieved from: https://datasciencebook.ca/inference.html](stat_imgs/population_vs_sample.png)

::: callout-important
## Wnioskowanie Statystyczne: Jak Mała Próba Może Reprezentować Dużą Populację?

Kiedy ankieterzy badają tylko 1 000 wyborców, aby przewidzieć wynik wyborów z 30 000 000 wyborców, wydaje się to zagadkowe. Jak tak mały ułamek (zaledwie 0,003%) może nam powiedzieć coś o całości?

Pomyśl o tym jak o próbowaniu zupy. Kiedy gotujesz duży garnek zupy i dokładnie ją mieszasz, nie musisz zjeść całego garnka, aby wiedzieć, jak smakuje. Wystarczy jedna łyżka – pod warunkiem, że zupa jest dobrze wymieszana.

### Dlaczego Losowe Pobieranie Próby Działa (ang. Random Sampling)

Losowe pobieranie próby działa, ponieważ:

1.  **Daje każdemu równą szansę** na wybór, unikając stronniczości
2.  **Wykorzystuje moc prawdopodobieństwa**, aby zapewnić, że próba przypomina populację
3.  **Przy wystarczającej liczbie osób** naturalne wahania zwykle się równoważą

Kluczowa jest losowość. Jeśli wybieramy osoby naprawdę losowo, prawdopodobnie otrzymamy mieszankę, która odzwierciedla całą populację. Bez losowości moglibyśmy przypadkowo skupić się na jednej "części zupy", która nie reprezentuje całości.

### Zrozumienie Błędu Statystycznego

Gdy używamy próby do poznania populacji, nieuchronnie napotykamy błąd statystyczny:

**Błąd statystyczny** to różnica między naszym oszacowaniem z próby a prawdziwą wartością w populacji. Możemy go podzielić na dwa główne rodzaje:

1.  **Błąd losowy (błąd próbkowania)**: Naturalna zmienność, która występuje przypadkowo, gdy wybieramy losową próbę. Dzieje się tak po prostu dlatego, że badamy część, a nie całą populację.

    **Błąd standardowy** jest miarą błędu próbkowania. Pokazuje, jak bardzo nasze statystyki z próby (jak średnie czy proporcje) różniłyby się, gdybyśmy wielokrotnie pobierali różne próby z tej samej populacji.

    -   Mniejsze błędy standardowe wskazują na bardziej precyzyjne oszacowania
    -   Błąd standardowy zmniejsza się wraz ze wzrostem wielkości próby (w przybliżeniu proporcjonalnie do 1/√n)
    -   Na błąd standardowy wpływa zmienność w populacji

2.  **Błąd systematyczny (błąd niepróbkowy)**: Występuje, gdy nasze metody zbierania danych systematycznie zniekształcają wyniki w jednym kierunku. W przeciwieństwie do błędu próbkowania, błędu systematycznego NIE można zmniejszyć przez zwiększenie wielkości próby.

    Źródła błędu systematycznego obejmują:

    -   **Błąd selekcji**: Gdy metoda wyboru próby faworyzuje pewne grupy (np. ankiety telefoniczne mogą pomijać osoby bez telefonów)
    -   **Błąd odpowiedzi**: Gdy respondenci udzielają niedokładnych odpowiedzi (np. zawyżanie dochodów w ankietach)
    -   **Błąd braku odpowiedzi**: Gdy osoby, które odmawiają udziału, różnią się od tych, które uczestniczą
    -   **Błąd pomiaru**: Wynikający z niedokładnych narzędzi lub metod pomiaru

Na przykład, jeśli obliczymy średni wzrost z próby, błąd standardowy mówi nam, jak bardzo ta średnia mogłaby się różnić, gdybyśmy pobrali wiele różnych losowych prób. Jednak jeśli nasz przyrząd pomiarowy jest wadliwy i zawsze dodaje 2 cm do każdego pomiaru, mamy do czynienia z błędem systematycznym, którego zwiększenie wielkości próby nie naprawi.

Dobry projekt badania musi uwzględniać oba rodzaje błędów:

-   Błąd próbkowania można zmniejszyć przez zwiększenie wielkości próby
-   Błąd systematyczny można zmniejszyć tylko przez ulepszenie metod zbierania danych

### Jak Duża Powinna Być Nasza Próba?

Niezbędna wielkość próby zależy od trzech głównych czynników:

1.  **Jakiego rodzaju szacunek przeprowadzasz?** (proporcja czy średnia)

2.  **Jak dokładny musi być twój szacunek?**

    -   Większa dokładność = większa próba
    -   Przykład: Szacowanie z dokładnością do ±1 punktu procentowego wymaga około 9 razy więcej osób niż szacowanie z dokładnością do ±3 punktów procentowych
    -   Przykład: Jeśli potrzebujesz być bardziej pewny swoich wyników (dla kluczowych decyzji), będziesz potrzebować większej próby

3.  **Jak zróżnicowana jest twoja populacja?**

    -   Dla proporcji: Największa zmienność występuje przy wartości 50%, najmniejsza przy 0% lub 100%
    -   Dla średnich: Sprawdź, jak bardzo pomiary różnią się od siebie (wariancja) w badanej populacji

**A co z małymi populacjami?**

Przy pracy z małymi populacjami (np. 1 000 osób lub mniej):

-   Występują istotne różnice w metodologii w porównaniu do dużych populacji
-   Tradycyjnie rozumiana prosta próba losowa zakłada losowanie ze zwracaniem (z jednakowym prawdopodobieństwem wyboru każdej jednostki)
-   W praktyce badawczej stosuje się zwykle losowanie bez zwracania, gdzie prawdopodobieństwa wyboru zmieniają się po każdym losowaniu
-   Im mniejsza populacja, tym większa różnica między tymi podejściami
-   **Współczynnik korekty dla populacji skończonej** koryguje właśnie tę rozbieżność i staje się niezbędny przy badaniu więcej niż 5-10% populacji

**Wyjaśnienie różnicy:**

-   Losowanie ze zwracaniem: każda jednostka ma stałe prawdopodobieństwo wyboru w każdym losowaniu (np. 1/N)
-   Losowanie bez zwracania: prawdopodobieństwo wyboru zmienia się w kolejnych losowaniach (z 1/N na 1/(N-1), 1/(N-2) itd.)
-   W dużych populacjach różnica jest minimalna, w małych - fundamentalna
-   Schematy losowania bez zwracania mają często mniejszą wariancję estymatora niż przy losowaniu ze zwracaniem

**Podejścia do małych populacji:**

-   Badanie pełne eliminuje problemy związane z losowaniem
-   Stratyfikacja może poprawić reprezentatywność, ale nie rozwiązuje problemu zmieniających się prawdopodobieństw
-   Metody nieparametryczne, które uwzględniają specyfikę małych populacji
-   Techniki "bootstrap" z odpowiednimi modyfikacjami dla małych populacji

**Praktyczne zalecenia:**

-   Dla populacji poniżej 200 jednostek rozważ badanie pełne
-   Stosuj odpowiednie wzory statystyczne uwzględniające losowanie bez zwracania
-   Standardowe wzory na błędy standardowe oparte na losowaniu ze zwracaniem będą zawyżać błąd w małych populacjach
-   Wyraźnie dokumentuj mechanizm losowania w raportach badawczych

Te rozróżnienia są istotne w badaniach małych społeczności, organizacji, czy badaniach pilotażowych, gdzie wpływ schematu losowania na wnioskowanie statystyczne jest znacznie większy niż w dużych badaniach reprezentatywnych.

### Proporcje a Średnie

#### Proporcje (procenty)

Przy szacowaniu proporcji (takiej jak odsetek wyborców popierających kandydata):

-   Proporcja reprezentuje część populacji o określonej cesze

-   W zasadzie liczymy odpowiedzi "tak" vs "nie"

-   Obliczanie wielkości próby koncentruje się na oczekiwanej proporcji (p):

    -   Proporcja bliska 50% ma maksymalną niepewność i wymaga większych prób
    -   Proporcja bliska 10% lub 90% ma mniejszą niepewność i wymaga mniejszych prób

-   Dla typowego sondażu wyborczego z proporcjami bliskimi 50%, około 1 000 losowo wybranych osób zapewnia dokładność około ±3 punktów procentowych

-   Co ciekawe, ta wielkość próby pozostaje taka sama, niezależnie od tego, czy twoja populacja liczy 30 000 czy 30 milionów

#### Średnie

Przy szacowaniu średniej (takiej jak typowy dochód gospodarstwa domowego lub wzrost):

-   Średnia podsumowuje ciągłe pomiary, które mogą przyjmować wiele różnych wartości

-   Wielkość próby zależy głównie od tego, jak bardzo te pomiary różnią się między osobami

-   Kluczowe względy dla wielkości próby:

    -   Populacje o bardziej zróżnicowanych pomiarach (wysoka odchylenie standardowe) wymagają większych prób
    -   Bardziej precyzyjne szacunki wymagają większych prób
    -   W przeciwieństwie do proporcji, nie ma "maksymalnej niepewności" przy określonej wartości

-   Na przykład, szacowanie średniego dochodu może wymagać większej próby niż szacowanie średniego wzrostu, ponieważ dochody zazwyczaj różnią się bardziej niż wzrost

#### Parametry modeli regresyjnych

Przy szacowaniu parametrów modeli regresyjnych (takich jak współczynniki w regresji liniowej):

-   **Czym jest regresja?** Modele regresji opisują, jak jedna zmienna (zmienna zależna) wiąże się z jedną lub wieloma innymi zmiennymi (zmiennymi niezależnymi). Na przykład, jak sprzedaż zależy od wydatków na reklamę, lub jak plony zależą od opadów deszczu i temperatury.

-   Parametry te pokazują związki między zmiennymi, np. jak wzrost wpływa na wagę

-   Wielkość próby dla modeli regresyjnych jest bardziej złożona niż dla prostych średnich czy proporcji

-   Kluczowe czynniki wpływające na wymaganą wielkość próby:

    -   Liczba zmiennych w modelu (więcej zmiennych = większa próba)
    -   Siła badanych efektów (słabsze efekty wymagają większych prób do wykrycia)
    -   Zmienność danych (większa zmienność = większa próba)
    -   Złożoność modelu (bardziej złożone modele wymagają większych prób)

**Proste reguły dla modeli regresyjnych:**

-   Podstawowa zasada: co najmniej 10-20 obserwacji na każdą zmienną w modelu
-   Dla wykrywania słabych efektów: może być potrzebne nawet 100+ obserwacji na zmienną
-   Modele wielopoziomowe lub hierarchiczne: wymagają znacznie większych prób

**Jak parametry regresyjne mają się do proporcji lub średnich?**

-   Parametry regresyjne są bardziej złożone – opisują relacje między zmiennymi, a nie tylko pojedyncze wartości
-   Podczas gdy proporcje i średnie skupiają się na jednej zmiennej, regresja analizuje współzależności
-   Błędy standardowe parametrów regresji zależą od wszystkich czynników wpływających na zmienność średnich, plus dodatkowo od korelacji między zmiennymi

**Jak ocenić, że próba była zbyt mała?**

-   Duże błędy standardowe w stosunku do wartości parametrów (szerokie przedziały ufności)
-   Niestabilność wyników – małe zmiany w danych powodują duże zmiany w parametrach
-   Nieistotne statystycznie wyniki, mimo że teoria sugeruje silne efekty
-   Problemy z dopasowaniem modelu (np. przeuczenie, niestabilne predykcje)
-   Brak mocy statystycznej do wykrycia efektów o oczekiwanej wielkości

### Przykład Maksymalnej Niepewności: Dlaczego 50% to "Najgorszy Przypadek"

**Demonstracja maksymalnej niepewności na monetach:**

Wyobraź sobie, że masz 100 monet w worku, które są albo złote, albo srebrne. Chcesz oszacować, jaki procent jest złoty, losowo wybierając 10 monet.

**Scenariusz A: 50% Złotych (Maksymalna Niepewność)**

-   Worek zawiera 50 złotych i 50 srebrnych monet (50% złotych)
-   Jeśli wyciągniesz 10 monet, możesz zobaczyć: 3 złote, 6 złotych, 4 złote, itd.
-   Wyniki mogą się znacznie różnić od prawdziwego odsetka
-   Twoja próba może łatwo odbiegać o 20% lub więcej od prawdziwych 50%

**Scenariusz B: 10% Złotych (Mniejsza Niepewność)**

-   Worek zawiera 10 złotych i 90 srebrnych monet (10% złotych)
-   Jeśli wyciągniesz 10 monet, zazwyczaj zobaczysz: 0, 1 lub 2 złote monety
-   Jest mało prawdopodobne, że wyciągniesz 5 złotych monet (co byłoby bardzo dalekie od prawdziwych 10%)
-   Twój szacunek jest naturalnie bardziej stabilny i bliższy prawdziwemu odsetkowi

Dlatego potrzebujemy większych prób, gdy proporcja jest bliska 50% - istnieje więcej naturalnej zmienności w tym, co możemy zaobserwować przez przypadek.

**Przykład Sondażu: Popularni vs. Niepopularni Kandydaci**

W sondażu wyborczym z 1 000 osób:

**Dla głównego kandydata z poparciem 40%:**

-   Nasza dokładność wyniosłaby około ±3 punktów procentowych
-   Możemy oszacować, że jego poparcie wynosi między 37% a 43%
-   Jest to wystarczająco precyzyjne, aby dokonać użytecznych prognoz

**Dla kandydata z mniejszym poparciem na poziomie 3%:**

-   Nasza dokładność wyniosłaby około ±1 punkt procentowy (mniejsza niż dla głównego kandydata)
-   Możemy oszacować, że jego poparcie wynosi między 2% a 4%
-   Jest to również wystarczająco precyzyjne dla praktycznych celów

**Ważne wyjaśnienie dotyczące dokładności:**

1.  **Dokładność bezwzględna (w punktach procentowych):**

    -   Dla kandydata z małym poparciem (3%): ±1 punkt procentowy (mniejszy błąd bezwzględny)
    -   Dla głównego kandydata (40%): ±3 punkty procentowe (większy błąd bezwzględny)
    -   Dzieje się tak, ponieważ proporcje dalekie od 50% (jak 3%) naturalnie mają mniejszą zmienność

2.  **Dokładność względna (jako proporcja szacunku):**

    -   Dla kandydata z małym poparciem (3%): ±1 punkt procentowy to 33% ich poziomu poparcia
    -   Dla głównego kandydata (40%): ±3 punkty procentowe to tylko 7,5% ich poziomu poparcia
    -   Więc choć błąd bezwzględny jest mniejszy, błąd względny jest znacznie większy dla kandydatów z małym poparciem

Na przykład, jeśli sondaż pokazuje 3% poparcia dla kandydata z małym poparciem z dokładnością ±1 punkt procentowy, ich rzeczywiste poparcie może być o 33% wyższe lub niższe niż zmierzone (między 2% a 4%). Dla głównego kandydata z poparciem 40% z dokładnością ±3 punkty procentowe, ich rzeczywiste poparcie będzie się różnić tylko o 7,5% od zmierzonej wartości.

To wyjaśnia, dlaczego sondaże mogą wiarygodnie wykrywać obecność kandydatów z małym poparciem i przybliżać ich poparcie, ale mogą mieć trudności z precyzyjnym mierzeniem małych zmian w ich poziomach poparcia. Ta sama wielkość próby daje różne dokładności bezwzględne w zależności od tego, jak blisko proporcja jest 50%.

#### Dlaczego Są Różne?

Chociaż proporcję można obliczyć jako średnią z 0 i 1 (nie/tak), zachowują się one inaczej w praktyce:

**Dla proporcji (takich jak sondaż wyborczy):**

-   Wyobraź sobie rzucanie monetami: Gdy wyniki są bliskie 50-50 (jak w zaciętych wyborach), istnieje większa niepewność co do prawdziwego odsetka
-   Przykład 1: Jeśli 500 z 1 000 osób (50%) popiera Kandydata A, dokładność wynosi około ±3 punktów procentowych
-   Przykład 2: Jeśli 900 z 1 000 (90%) popiera Kandydata A, dokładność wynosi tylko około ±2 punktów procentowych
-   Przykład 3: W ankiecie klasowej, gdzie 15 z 20 uczniów (75%) woli pizzę od hamburgerów, dokładność wynosi około ±19 punktów procentowych - znacznie większa niż przy 1 000 osobach!
-   Im bliżej 50%, tym więcej osób musimy zbadać, aby być pewnym naszego wyniku

**Dla ogólnych średnich (takich jak dochód gospodarstwa domowego):**

-   Wyobraź sobie pomiar różnych rzeczy:

    -   Wzrost: Większość dorosłych ma między 150 cm a 195 cm (niewielka zmienność)
    -   Dochody: Ludzie mogą zarabiać od 0 zł do milionów (ogromna zmienność)
    -   Wyniki testów: Większość uczniów zdobywa między 60-90 punktów na teście (umiarkowana zmienność)

-   Przykład 1: Aby oszacować średni wzrost z dokładnością do ±2,5 cm, możemy potrzebować tylko 100 osób

-   Przykład 2: Aby oszacować średni dochód z dokładnością do ±1 000 zł, możemy potrzebować 10 000 osób

-   Przykład 3: Aby oszacować średnie wyniki testów z dokładnością do ±2 punktów, możemy potrzebować 500 osób

-   Im większa zmienność w populacji, tym większej próby potrzeba dla tej samej precyzji

**Porównanie w świecie rzeczywistym:**

Jeśli chcemy tej samej dokładności (±3 punktów procentowych):

-   Dla pytania tak/nie, gdzie spodziewamy się podziału bliskiego 50-50: około 1 000 osób
-   Do pomiaru średniego zużycia energii w gospodarstwach domowych: prawdopodobnie 2 000-3 000 gospodarstw
-   Do oszacowania średniego czasu dojazdu: prawdopodobnie 500-600 dojeżdżających

Siła próbkowania polega na tym, że przy właściwej randomizacji możemy dowiedzieć się o bardzo dużych populacjach, badając stosunkowo małe próby!
:::

![The process of using a sample from a population to obtain a point estimate of a population parameter. In this case, a sample of 10 individuals yielded 6 who own an iPhone, resulting in an estimated population proportion of 60% iPhone owners. The actual population proportion is 53.8%. Retrieved from: https://datasciencebook.ca/inference.html](stat_imgs/population_vs_sample.png)

::: {.panel-tabset group="language"}
## SRS

**Prosty Dobór Losowy**: Każda jednostka ma równe prawdopodobieństwo wyboru. Cała populacja jest losowo próbkowana bez żadnego z góry ustalonego wzorca.

![Prosty Dobór Losowy. Źródło: https://app.passyourmath.com/courses/theory/116/1750/26640/en](stat_imgs/Simple_Random_Sampling2.svg)

## Dobór warstwowy

**Dobór Warstwowy**: Populacja jest podzielona na odrębne podgrupy (warstwy) przed losowym pobraniem próbek z każdej warstwy proporcjonalnie.

![Dobór Warstwowy. Źródło: https://app.passyourmath.com/courses/theory/116/1750/26640/en](stat_imgs/Stratified2.svg)

## Dobór grupowy

**Dobór Grupowy**: Populacja jest podzielona na skupiska (klastry), a całe skupiska są losowo wybierane do analizy zamiast pojedynczych jednostek.

![Dobór Grupowy. Źródło: https://app.passyourmath.com/courses/theory/116/1750/26640/en](stat_imgs/Cluster2.svg)
:::

::: callout-note
## Data Generating Process i Superpopulacja w kontekście badań statystycznych

W statystyce i ekonometrii często posługujemy się pojęciami populacji i próby. Jednak dla pełnego zrozumienia statystycznych modeli warto wprowadzić dwa dodatkowe koncepty: **data generating process (DGP)** oraz **superpopulację**. Pojęcia te pomogą lepiej zrozumieć, w jaki sposób powstają dane, które analizujemy.

### Podstawowe pojęcia

**Populacja**: Zbiór wszystkich jednostek (osób, firm, krajów itp.), które są przedmiotem naszego badania. Przykładowo, wszystkie gospodarstwa domowe w Polsce.

**Próba**: Podzbiór populacji, który badamy, aby wyciągnąć wnioski o całej populacji. Przykładowo, 1000 losowo wybranych gospodarstw domowych w Polsce.

### Data Generating Process (DGP)

**Data Generating Process** (proces generujący dane) to mechanizm lub model stochastyczny (losowy), który określa, w jaki sposób powstają wartości zmiennych, które obserwujemy.

Kluczowe cechy DGP:

-   Jest to abstrakcyjny, matematyczny model opisujący, jak zmienne są ze sobą powiązane
-   Zawiera zarówno element deterministyczny (przewidywalny), jak i losowy (nieprzewidywalny)
-   Zwykle wyrażany za pomocą równań i rozkładów prawdopodobieństwa

#### Przykład DGP:

Załóżmy, że badamy dochody gospodarstw domowych. Model DGP (równanie matematyczne) mógłby wyglądać tak:

$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \varepsilon_i$

Gdzie:

-   $Y_i$ to dochód gospodarstwa domowego $i$
-   $X_{1i}$ to liczba osób pracujących w gospodarstwie $i$
-   $X_{2i}$ to poziom wykształcenia głównego żywiciela gospodarstwa $i$
-   $\varepsilon_i$ to czynnik losowy (błąd)
-   $\beta_0, \beta_1, \beta_2$ to parametry modelu

W tym przykładzie DGP określa, jak dochód zależy od liczby pracujących osób i wykształcenia, plus element losowy.

### Superpopulacja

**Superpopulacja** to hipotetyczna, nieskończona populacja, z której nasza rzeczywista populacja może być traktowana jako próba.

Kluczowe cechy superpopulacji:

-   Jest konceptualnym rozszerzeniem pojęcia populacji
-   Reprezentuje wszystkie możliwe realizacje procesu generującego dane (DGP)
-   Pozwala na wnioskowanie nie tylko o aktualnej populacji, ale o ogólnych prawidłowościach

#### Przykłady superpopulacji:

1.  Badamy wyniki matur w Polsce w 2024 roku (populacja). Superpopulacja to hipotetyczne wyniki matur wszystkich możliwych uczniów, którzy mogliby zdawać maturę w podobnych warunkach, w dowolnym roku.

2.  Analizujemy PKB krajów europejskich w 2023 roku (populacja). Superpopulacja to wszystkie możliwe realizacje PKB tych krajów w różnych warunkach gospodarczych.

### Relacje między pojęciami

```         
DGP → Superpopulacja → Populacja → Próba
```

-   **DGP** definiuje mechanizm generowania danych
-   **Superpopulacja** to wszystkie możliwe realizacje DGP
-   **Populacja** to konkretna, skończona realizacja z superpopulacji
-   **Próba** to podzbiór populacji, który faktycznie obserwujemy i analizujemy

### Praktyczne znaczenie

#### Dlaczego te pojęcia są ważne?

1.  **Modelowanie ekonometryczne** - DGP jest fundamentem modelowania ekonometrycznego, pozwala zrozumieć, jak zmienne ekonomiczne wpływają na siebie nawzajem.

2.  **Wnioskowanie statystyczne** - Koncepcja superpopulacji pozwala na formułowanie wniosków ogólnych, wykraczających poza konkretny moment czasowy czy grupę.

3.  **Prognozowanie** - Zrozumienie DGP umożliwia lepsze prognozowanie przyszłych wartości zmiennych ekonomicznych.

4.  **Weryfikacja hipotez** - Pozwala lepiej określić, czy zaobserwowane zależności są dziełem przypadku, czy odzwierciedlają rzeczywiste prawidłowości.

### Przykład zastosowania w ekonomii

Załóżmy, że ekonomista bada wpływ stóp procentowych na inflację:

-   **DGP**: Równanie opisujące, jak stopa procentowa i inne czynniki wpływają na inflację
-   **Superpopulacja**: Wszystkie możliwe wartości inflacji przy różnych poziomach stóp procentowych i innych czynników
-   **Populacja**: Faktyczne wartości inflacji we wszystkich krajach w danym okresie
-   **Próba**: Dane o inflacji z 30 wybranych krajów, które badamy

### Przykłady ze studiów wyborczych

**Przykład 1: Frekwencja wyborcza**

-   **DGP**: $Frekwencja_i = \beta_0 + \beta_1 Wiek_i + \beta_2 Wykształcenie_i + \beta_3 Dochód_i + \varepsilon_i$
-   **Superpopulacja**: Wszystkie możliwe decyzje o udziale w wyborach, które mogliby podjąć wyborcy o różnych cechach społeczno-demograficznych w różnych warunkach
-   **Populacja**: Wszyscy uprawnieni do głosowania w wyborach prezydenckich 2020
-   **Próba**: 1500 respondentów ankiety exit poll po wyborach

**Przykład 2: Poparcie dla partii politycznych**

-   **DGP**: $Poparcie_{ij} = \beta_0 + \beta_1 Ideologia_i + \beta_2 SytuacjaEkonomiczna_i + \beta_3 Wiek_i + \varepsilon_i$
-   **Superpopulacja**: Wszystkie możliwe preferencje wyborcze osób o różnych cechach w różnych warunkach społeczno-ekonomicznych
-   **Populacja**: Wszyscy wyborcy w Polsce w 2023 roku
-   **Próba**: Respondenci sondażu przedwyborczego (n=1000)

### Symulacja i estymacja funkcji popytu jako DGP w R

Poniższy kod ilustruje, jak można zasymulować funkcję popytu z wieloma predyktorami jako DGP i estymować jej parametry za pomocą regresji OLS:

```{r}
#| warning: false
#| message: false

# Załadowanie potrzebnych pakietów
library(tidyverse)

# Ustawienie ziarna losowości dla powtarzalnych wyników
set.seed(123)

# 1. Zdefiniowanie "prawdziwego" DGP: Funkcja popytu na produkt
# Model: Q = beta0 + beta1*P + beta2*I + beta3*P_sub + beta4*P_comp + beta5*A + epsilon
# Gdzie:
# Q = ilość popytu na produkt
# P = cena produktu
# I = dochód konsumentów
# P_sub = cena dobra substytucyjnego
# P_comp = cena dobra komplementarnego
# A = wydatki na reklamę

# Prawdziwe wartości parametrów (zgodne z teorią ekonomiczną)
beta0_true <- 100      # Stała
beta1_true <- -2.5     # Efekt własnej ceny (ujemny - zgodnie z prawem popytu)
beta2_true <- 0.8      # Efekt dochodu (dodatni - dobro normalne)
beta3_true <- 1.2      # Efekt ceny substytutu (dodatni)
beta4_true <- -0.7     # Efekt ceny dobra komplementarnego (ujemny)
beta5_true <- 0.5      # Efekt reklamy (dodatni)
sigma_true <- 5        # Odchylenie standardowe błędu losowego

# 2. Symulacja superpopulacji (5000 potencjalnych rynków/okresów)
n_super <- 5000

# Generowanie predyktorów
cena <- runif(n_super, min = 5, max = 15)               # Cena produktu (zł)
dochod <- rnorm(n_super, mean = 3000, sd = 500)         # Średni dochód (zł)
cena_substytutu <- runif(n_super, min = 4, max = 16)    # Cena substytutu (zł)
cena_komplementu <- runif(n_super, min = 2, max = 8)    # Cena komplementu (zł)
reklama <- runif(n_super, min = 0, max = 100)           # Wydatki na reklamę (tys. zł)

# Dodanie korelacji między zmiennymi (np. cena i cena substytutu)
cena_substytutu <- cena_substytutu + rnorm(n_super, mean = 0.2 * cena, sd = 1)

# Generowanie popytu zgodnie z DGP
epsilon <- rnorm(n_super, mean = 0, sd = sigma_true)  # Składnik losowy
popyt <- beta0_true + 
         beta1_true * cena + 
         beta2_true * (dochod/1000) +  # skalowanie dochodu dla lepszej interpretacji
         beta3_true * cena_substytutu + 
         beta4_true * cena_komplementu + 
         beta5_true * (reklama/10) +    # skalowanie reklamy dla lepszej interpretacji
         epsilon

# Tworzenie ramki danych superpopulacji
superpopulacja <- tibble(
  id = 1:n_super,
  cena = cena,
  dochod = dochod,
  cena_substytutu = cena_substytutu,
  cena_komplementu = cena_komplementu,
  reklama = reklama,
  popyt = popyt
)

# Pokaż kilka pierwszych obserwacji
head(superpopulacja)

# 3. Pobieranie próby z superpopulacji (np. 200 obserwacji)
n_sample <- 200
indeksy_proby <- sample(1:n_super, n_sample)
proba <- superpopulacja[indeksy_proby, ]

# 4. Estymacja modelu OLS na podstawie próby
model_ols <- lm(popyt ~ cena + I(dochod/1000) + cena_substytutu + 
                cena_komplementu + I(reklama/10), data = proba)

# 5. Wyświetlenie podsumowania modelu
summary(model_ols)

# 6. Porównanie prawdziwych parametrów z estymowanymi
parametry_prawdziwe <- c(beta0_true, beta1_true, beta2_true, 
                         beta3_true, beta4_true, beta5_true)
parametry_estymowane <- coef(model_ols)

porownanie <- tibble(
  parametr = c("Stała", "Cena", "Dochód (tys.)", "Cena substytutu", 
               "Cena komplementu", "Reklama (10 tys.)"),
  prawdziwa_wartość = parametry_prawdziwe,
  estymowana_wartość = parametry_estymowane,
  różnica = estymowana_wartość - prawdziwa_wartość,
  błąd_procentowy = abs(różnica / prawdziwa_wartość) * 100
)

# Wyświetlenie porównania
print(porownanie)

# 7. Wizualizacja porównania prawdziwych i estymowanych parametrów
ggplot(porownanie, aes(x = parametr, y = prawdziwa_wartość)) +
  geom_point(color = "blue", size = 3) +
  geom_point(aes(y = estymowana_wartość), color = "red", size = 3) +
  geom_segment(aes(xend = parametr, y = prawdziwa_wartość, 
                   yend = estymowana_wartość), color = "gray") +
  labs(title = "Porównanie prawdziwego DGP z estymowanym modelem",
       subtitle = "Niebieskie punkty: prawdziwe wartości, czerwone punkty: estymowane wartości",
       x = "Parametr", y = "Wartość") +
  theme_minimal() +
  coord_flip()

# 8. Sprawdzenie zdolności predykcyjnej modelu na nowych danych
# Pobieramy nowe dane z superpopulacji (nie używane w estymacji)
nowe_indeksy <- sample(setdiff(1:n_super, indeksy_proby), 100)
nowe_dane <- superpopulacja[nowe_indeksy, ]

# Predykcja na nowych danych
nowe_dane$przewidywany_popyt <- predict(model_ols, newdata = nowe_dane)

# Obliczenie błędu średniokwadratowego (MSE) predykcji
mse <- mean((nowe_dane$popyt - nowe_dane$przewidywany_popyt)^2)
rmse <- sqrt(mse)
cat("Pierwiastek błędu średniokwadratowego (RMSE):", round(rmse, 2), "\n")

# 9. Wizualizacja relacji między ceną a popytem
# (efekt ceteris paribus - przy kontroli innych zmiennych)
ceteris_paribus <- tibble(
  cena = seq(5, 15, length.out = 100),
  dochod = mean(proba$dochod),
  cena_substytutu = mean(proba$cena_substytutu),
  cena_komplementu = mean(proba$cena_komplementu),
  reklama = mean(proba$reklama)
)

# Obliczenie przewidywanego popytu według prawdziwego DGP
ceteris_paribus$popyt_prawdziwy <- beta0_true + 
                                 beta1_true * ceteris_paribus$cena + 
                                 beta2_true * (ceteris_paribus$dochod/1000) + 
                                 beta3_true * ceteris_paribus$cena_substytutu + 
                                 beta4_true * ceteris_paribus$cena_komplementu + 
                                 beta5_true * (ceteris_paribus$reklama/10)

# Obliczenie przewidywanego popytu według estymowanego modelu
ceteris_paribus$popyt_estymowany <- predict(model_ols, newdata = ceteris_paribus)

# Wizualizacja
ggplot(ceteris_paribus, aes(x = cena)) +
  geom_line(aes(y = popyt_prawdziwy, color = "Prawdziwy DGP"), size = 1.2) +
  geom_line(aes(y = popyt_estymowany, color = "Estymowany model"), size = 1.2) +
  scale_color_manual(values = c("Prawdziwy DGP" = "blue", "Estymowany model" = "red")) +
  labs(title = "Krzywa popytu: Prawdziwy DGP vs. Estymowany model",
       subtitle = "Efekt ceteris paribus (przy stałych wartościach innych zmiennych)",
       x = "Cena produktu (zł)",
       y = "Popyt (ilość)",
       color = "Model") +
  theme_minimal()
```

Ten kod pokazuje:

1.  Definiowanie złożonego DGP dla funkcji popytu z wieloma predyktorami zgodnej z teorią ekonomiczną
2.  Symulowanie superpopulacji zgodnie z tym DGP
3.  Pobieranie próby z superpopulacji
4.  Estymację parametrów modelu za pomocą regresji OLS
5.  Porównanie estymowanych parametrów z ich prawdziwymi wartościami
6.  Wizualizację porównania prawdziwych i estymowanych parametrów
7.  Sprawdzenie zdolności predykcyjnej modelu na nowych danych
8.  Wizualizację relacji między ceną a popytem z efektem ceteris paribus

Przykład ten pokazuje, jak można symulować złożone zależności ekonomiczne, a następnie używać metod ekonometrycznych do odkrywania tych zależności na podstawie próby danych. Jest to doskonała ilustracja, jak teoria ekonomiczna, DGP i metody statystyczne są ze sobą powiązane.

W rzeczywistości nigdy nie znamy prawdziwego DGP - to właśnie próbujemy odkryć za pomocą analizy statystycznej. Symulacje tego typu pozwalają jednak zrozumieć konceptualnie, jak wnioskowanie statystyczne łączy się z pojęciem DGP i superpopulacji.

### Przykłady pomocnicze

#### Przykład 1: Badanie opinii wyborców

-   **Populacja**: Wszyscy zarejestrowani wyborcy w Polsce w 2023 roku (ok. 30 milionów osób)
-   **Próba**: 1000 losowo wybranych wyborców ankietowanych w sondażu
-   **Superpopulacja**: Wszyscy możliwi wyborcy (obecni, przyszli i hipotetyczni) oraz wszystkie możliwe scenariusze głosowania
-   **DGP** (Data Generating Process): Złożony mechanizm kształtujący opinie i decyzje wyborcze, w tym:
    -   Czynniki demograficzne (wiek, wykształcenie, miejsce zamieszkania)
    -   Warunki ekonomiczne (dochód, status zawodowy)
    -   Wpływ mediów i debaty publicznej
    -   Doświadczenia osobiste
    -   Historyczne uwarunkowania polityczne

#### Przykład 2: Badanie efektów leku przeciwcukrzycowego

-   **Populacja**: Wszyscy pacjenci z cukrzycą typu 2 w danym kraju (np. 2 miliony osób)
-   **Próba**: 500 pacjentów uczestniczących w badaniu klinicznym
-   **Superpopulacja**: Wszyscy możliwi pacjenci z cukrzycą typu 2 (obecni i przyszli) z różnymi profilami genetycznymi i środowiskowymi
-   **DGP**: Biologiczny mechanizm obejmujący:
    -   Interakcje leku z receptorami w organizmie
    -   Indywidualne uwarunkowania genetyczne
    -   Czynniki środowiskowe (dieta, aktywność fizyczna)
    -   Interakcje z innymi lekami
    -   Mechanizmy metaboliczne organizmu

#### Przykład 3: Gdy próba równa się populacji

Badanie wszystkich 50 stanów USA:

-   **Tradycyjne podejście**: Nie ma rozróżnienia między próbą a populacją (badamy wszystkie stany)
-   **Podejście superpopulacyjne**:
    -   **Populacja/Próba**: 50 istniejących stanów USA
    -   **Superpopulacja**: Teoretyczny zbiór wszystkich możliwych jednostek terytorialnych typu "stan" w różnych warunkach historycznych, politycznych i społecznych
    -   **DGP**: Fundamentalne mechanizmy geograficzne, historyczne, polityczne i społeczno-ekonomiczne kształtujące charakterystyki stanów

#### Przykład 4: Jakość pizzy w Nowym Jorku

-   **Populacja**: Wszystkie obecnie działające pizzerie w Nowym Jorku (np. 2000 lokali)
-   **Próba**: 50 losowo wybranych pizzerii z różnych dzielnic
-   **Superpopulacja**: Wszystkie możliwe pizzerie, które mogłyby istnieć w Nowym Jorku:
    -   Obecnie działające
    -   Przyszłe (jeszcze nieotwarte)
    -   Historyczne (już zamknięte)
    -   Hipotetyczne (w alternatywnych warunkach ekonomicznych czy kulturowych)
-   **DGP**: Czynniki wpływające na jakość pizzy:
    -   Składniki i ich jakość
    -   Umiejętności i doświadczenie szefów kuchni
    -   Sprzęt i infrastruktura kuchenna
    -   Metody przygotowania i przepisy
    -   Czynniki środowiskowe (np. jakość lokalnej wody)
    -   Wpływy kulturowe i tradycje kulinarne
    -   Uwarunkowania ekonomiczne (koszty operacyjne, czynsze)

DGP jest jak "przepis na jakość pizzy", który determinuje rezultaty dla wszystkich potencjalnych pizzerii w superpopulacji, nie tylko dla obecnie istniejących lokali.

```{mermaid}
graph TD
    A[Data Generating Process DGP]
    S(Superpopulacja)
    B(Populacja)
    C[Próba]
    A -->|Generuje| S
    S -->|Jedna realizacja| B
    B -->|Pobieranie próby| C
    C -.->|Wnioskowanie| B
    C -.->|Wnioskowanie| S
    C -.->|Wnioskowanie| A
    B -.->|Wnioskowanie| S
    B -.->|Wnioskowanie| A
    S -.->|Wnioskowanie| A
    
    classDef dgp fill:#f9f,stroke:#333,stroke-width:4px;
    classDef superpop fill:#fce,stroke:#333,stroke-width:3px;
    classDef pop fill:#bbf,stroke:#333,stroke-width:2px;
    classDef sam fill:#bfb,stroke:#333,stroke-width:2px;
    
    class A dgp;
    class S superpop;
    class B pop;
    class C sam;
```
:::

::: callout-important
## Parametr populacji ($\theta$)

Wartość liczbowa charakteryzująca określoną cechę całej populacji. Przykładami parametrów populacji są:

-   $\mu$ (średnia populacji)
-   $\sigma^2$ (wariancja populacji)
-   $\sigma$ (odchylenie standardowe populacji)
-   $p$ (proporcja w populacji)

Parametry populacji zazwyczaj nie są znane i stanowią przedmiot estymacji na podstawie próby.

## Rozkład statystyki ($\hat{\theta}$) z próby

**Statystyka z próby** (a sample statistic) $\hat{\theta}$ to wartość wyliczona na podstawie obserwacji z próby, która służy do estymacji parametru populacji $\theta$. Rozkład statystyki opisuje, jak zmieniają się wartości tej statystyki przy wielokrotnym pobieraniu prób o tej samej liczebności $n$ z tej samej populacji.

Rozkład statystyki odnosi się do rozkładu prawdopodobieństwa, który opisuje wszystkie możliwe wartości, jakie może przyjąć dana statystyka, gdy jest obliczana z różnych prób losowych pobieranych z tej samej populacji, wraz z prawdopodobieństwem uzyskania każdej wartości.

Gdy pobieramy próbę z populacji i obliczamy statystykę (np. średnią z próby), otrzymujemy jedną konkretną wartość. Jednakże gdybyśmy powtórzyli ten proces wielokrotnie – pobierając różne próby losowe o tej samej wielkości z tej samej populacji – za każdym razem otrzymalibyśmy inne wartości tej statystyki. Rozkład statystyki opisuje właśnie ten wzorzec zmienności.

Kluczowe cechy rozkładu statystyki:

1.  Pokazuje, zróżnicowanie statystyki w różnych próbach
2.  Pomaga zrozumieć błąd losowy i niepewność związaną z próbkowaniem
3.  Pozwala formułować stwierdzenia probabilistyczne o tym, jak blisko nasza estymacja jest prawdziwego parametru populacji
4.  Stanowi podstawę wnioskowania statystycznego, w tym przedziałów ufności i testów hipotez

Przykład: Jeśli interesuje nas średnia populacji $\mu$, to jej estymatorem jest średnia z próby $\bar{x}$. Rozkład statystyki $\bar{x}$ pokazuje, jakie wartości może przyjmować średnia z próby i z jakim prawdopodobieństwem. W określonych warunkach, ten rozkład zbliża się do "rozkładu normalnego" wraz ze wzrostem wielkości próby (zgodnie z Centralnym Twierdzeniem Granicznym), nawet jeśli oryginalny rozkład populacji nie jest "normalny".

Możemy przedstawić to formalnie następująco: $$\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i$$

gdzie $x_1, x_2, ..., x_n$ to obserwacje w próbie losowej.

## Wartość oczekiwana statystyki z próby

Wartość oczekiwana statystyki z próby $E(\hat{\theta})$ to średnia wartość, jaką przyjmuje dana statystyka przy wielokrotnym pobieraniu prób z populacji.

Istnieją ważne twierdzenia w statystyce, które gwarantują nam, że dla dobrze skonstruowanych estymatorów wartość oczekiwana statystyki jest równa parametrowi, który chcemy estymować. Mówiąc nieformalnie - "dobre" estymatory są skonstruowane tak, żeby "średnio" trafiały w cel, czyli:

$$E(\hat{\theta}) = \theta$$

Na przykład, dla średniej z próby możemy udowodnić, że: $$E(\bar{x}) = E\left(\frac{1}{n}\sum_{i=1}^{n}x_i\right) = \frac{1}{n}\sum_{i=1}^{n}E(x_i) = \frac{1}{n} \cdot n \cdot \mu = \mu$$

Taki estymator nazywamy nieobciążonym. Co ważne, nie wszystkie estymatory są nieobciążone - niektóre mają systematyczne odchylenie, np. estymator wariancji $s^2$ musi zawierać korektę $\frac{1}{n-1}$ zamiast $\frac{1}{n}$, aby być nieobciążonym.
:::

![Various random samples of equal size and their statistics. Retrieved from: https://allmodelsarewrong.github.io/mse.html](stat_imgs/sampling-estimators.svg)

::: callout-note
## Rozkład Normalny - Krzywa Dzwonowa

### Kluczowe Właściwości

-   Rozkład normalny (znany również jako rozkład Gaussa lub krzywa dzwonowa) jest jednym z najważniejszych rozkładów prawdopodobieństwa w statystyce.
-   Ma charakterystyczny kształt dzwonu lub kopuły, który jest symetryczny wokół średniej, co oznacza, że wartości powyżej i poniżej średniej występują z jednakowym prawdopodobieństwem.
-   Jest w pełni opisany przez tylko dwa parametry:
    -   Średnią $\mu$ (wyznacza centrum rozkładu)
    -   Odchylenie standardowe $\sigma$ (określa szerokość "dzwonu")

### Reprezentacja Matematyczna

Funkcja "gęstości" prawdopodobieństwa (PDF) rozkładu normalnego to:

$$
f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
$$

Gdzie:

-   $x$ to zmienna losowa
-   $\mu$ to średnia (parametr położenia)
-   $\sigma$ to odchylenie standardowe (parametr skali)
-   $e$ to liczba Eulera (w przybliżeniu 2,71828)
-   $\pi$ to stała matematyczna pi (w przybliżeniu 3,14159)

### Znaczenie Praktyczne

Rozkład normalny pojawia się często w naturze i systemach społecznych:

-   Rozkład wzrostu i wagi ludzi
-   Błędy pomiarowe w eksperymentach naukowych
-   Wyniki testów i osiągnięcia akademickie
-   Odchylenia w procesach produkcyjnych

#### Teoria Statystyczna

-   **Reguła 68-95-99,7**: Około 68% wartości mieści się w granicach 1 odchylenia standardowego od średniej, 95% w granicach 2 odchyleń standardowych, a 99,7% w granicach 3 odchyleń standardowych.
-   **Centralne Twierdzenie Graniczne**: Średnie próbkowe mają tendencję do podążania za rozkładem normalnym niezależnie od kształtu oryginalnego rozkładu (przy odpowiednio dużej wielkości próby).
-   **Wnioskowanie Statystyczne**: Stanowi podstawę dla wielu testów statystycznych i przedziałów ufności.
:::

## Dane i Populacje

Dane stanowią podstawę analizy statystycznej. Aby lepiej zrozumieć ich rolę, warto poznać kluczowe pojęcia.

### Rodzaje Danych

-   **Dane pierwotne** (*Primary data*): Zebrane bezpośrednio w określonym celu badawczym, np. przeprowadzenie własnej ankiety
-   **Dane wtórne** (*Secondary data*): Uzyskane z istniejących źródeł, np. baz danych czy publikacji innych badaczy

::: callout-note
## Populacja i Próba - fundamentalne rozróżnienie

-   **Populacja**: Pełny zbiór wszystkich elementów/jednostek, o których chcemy wnioskować (np. wszyscy dorośli obywatele Polski)
-   **Próba**: Podzbiór populacji, który badamy w praktyce (np. 1000 losowo wybranych dorosłych obywateli Polski)

W praktyce badawczej niemal zawsze analizujemy próbę, a następnie wnioskujemy o populacji.
:::

### Zmienne i Stałe

**Zmienne** to cechy, które mogą przyjmować różne wartości w zbiorze danych. Stanowią one obiekt naszych badań i analiz.

#### Klasyfikacja Zmiennych

1.  **Zmienne Ilościowe** (*Quantitative*):
    -   **Ciągłe** (*Continuous*): Mogą przyjmować dowolną wartość w określonym przedziale, np. wzrost, waga, temperatura
    -   **Dyskretne** (*Discrete*): Przyjmują tylko określone wartości (zwykle liczby całkowite), np. liczba dzieci, liczba błędów
2.  **Zmienne Jakościowe** (*Qualitative*):
    -   **Nominalne** (*Nominal*): Kategorie bez naturalnej kolejności, np. grupa krwi, płeć, województwo
    -   **Porządkowe** (*Ordinal*): Kategorie z naturalną kolejnością, np. wykształcenie (podstawowe, średnie, wyższe), skala Likerta (1-5)

**Stałe** to wartości, które pozostają niezmienne w trakcie analizy i często służą jako punkty odniesienia.

## Parametry Populacji i Związane Pojęcia - Kluczowe Rozróżnienia

W statystyce istnieje kilka podobnie brzmiących pojęć, które często są mylone. Poniżej przedstawiam ich klarowne rozróżnienie:

### Parametr Populacji i Estymand

**Parametr populacji** to wartość liczbowa opisująca cechę całej populacji. Kluczowe cechy:

1.  Dotyczy *całej* populacji, nie tylko próby
2.  Jest zwykle oznaczany greckimi literami (μ, σ, π, ρ)
3.  W większości przypadków pozostaje **nieznany** (nie możemy zbadać całej populacji)
4.  Jest determinowany przez rzeczywisty Proces Generujący Dane (DGP)

**Estymand** (*Estimand*) to konkretny parametr populacji lub funkcja parametrów, którą chcemy oszacować. Jest to *cel naszej estymacji*.

Przykłady parametrów populacji:

-   Średnia populacji (μ): Prawdziwa średnia wartość cechy w populacji
-   Wariancja populacji (σ²): Prawdziwa miara zmienności w populacji
-   Proporcja populacji (p): Prawdziwa proporcja jednostek w populacji posiadających daną cechę

::: callout-important
## Ważne rozróżnienie!

Estymand (parametr populacji) to wartość w populacji, którą chcemy poznać, ale która pozostaje dla nas nieznana. Jest to nasz cel badawczy.
:::

### Estymator (Statystyka)

**Estymator** to funkcja matematyczna (wzór, procedura), która na podstawie danych z próby dostarcza oszacowania parametru populacji. **Estymator jest zmienną losową**, ponieważ jego wartość zależy od konkretnej próby.

**Statystyka** to każda miara obliczona na podstawie danych z próby. Gdy statystyka służy do oszacowania parametru populacji, nazywamy ją estymatorem.

Przykłady estymatorów (statystyk):

-   Średnia z próby: $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ (estymator średniej populacji μ)
-   Wariancja z próby: $s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2$ (estymator wariancji populacji σ²)
-   Proporcja z próby: $\hat{p} = \frac{x}{n}$ (estymator proporcji populacji p)

::: callout-note
## Estymator jako procedura

Estymator należy rozumieć jako **przepis** na obliczenie wartości na podstawie próby. Ten sam estymator zastosowany do różnych prób da różne wyniki.

Przykład: Estymator średniej $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ to procedura "zsumuj wszystkie wartości i podziel przez ich liczbę".
:::

### Oszacowanie (Estymata)

**Oszacowanie** (estymata, ang. *estimate*) to konkretna wartość liczbowa otrzymana po zastosowaniu estymatora do określonej próby. Jest to pojedyncza liczba, będąca realizacją zmiennej losowej, jaką jest estymator.

::: callout-tip
## Przykład rozróżnienia tych pojęć

-   **Estymand**: Średnia wysokość wszystkich dorosłych Polaków (μ) - nieznana wartość
-   **Estymator**: Wzór na średnią z próby $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ - procedura obliczeniowa
-   **Oszacowanie** (estymata): 173.5 cm - konkretna wartość otrzymana z próby

Różne próby dadzą różne oszacowania tego samego estymandy przy użyciu tego samego estymatora.
:::

### Właściwości Estymatorów

Dobry estymator powinien posiadać korzystne właściwości statystyczne:

1.  **Nieobciążoność** (*Unbiasedness*): Estymator jest nieobciążony, jeśli jego wartość oczekiwana (średnia z wielu prób) jest równa estymandzie. Formalnie: E(θ̂) = θ

2.  **Efektywność** (*Efficiency*): Estymator jest efektywny, jeśli ma najmniejszą możliwą wariancję spośród wszystkich nieobciążonych estymatorów

3.  **Zgodność** (*Consistency*): Estymator jest zgodny, jeśli wraz ze wzrostem wielkości próby jego wartość zbliża się do prawdziwej wartości parametru

4.  **Dostateczność** (*Sufficiency*): Estymator jest dostateczny, jeśli wykorzystuje wszystkie dostępne informacje z próby odnośnie szacowanego parametru

## Modele Statystyczne i Wnioskowanie

### Modele Statystyczne

Model statystyczny to matematyczna reprezentacja rzeczywistości, która opisuje relacje między zmiennymi i strukturę danych. Pozwala na opisanie procesu generującego dane (DGP) i wnioskowanie o parametrach.

::: callout-note
# Komponenty Modelu Statystycznego

Model statystyczny to matematyczne ramy służące do reprezentowania zależności w danych i formułowania prognoz. Kompletny model statystyczny składa się z:

1.  **Forma funkcyjna**: Struktura matematyczna określająca relację między zmiennymi (np. liniowa, kwadratowa, wykładnicza, logarytmiczna)

2.  **Zmienne**:

    -   Zmienna zależna: To, co staramy się przewidzieć lub wyjaśnić
    -   Zmienne niezależne/objaśniające: Czynniki, które mogą wpływać na zmienną zależną

3.  **Parametry**: Nieznane wartości, które szacujemy na podstawie danych, kwantyfikujące relację między zmiennymi (np. współczynniki regresji, średnie, wariancje)

4.  **Składnik losowy**: Człon błędu lub element stochastyczny, który uwzględnia niepewność i zmienność niewyjaśnioną przez model

5.  **Założenia dotyczące rozkładu prawdopodobieństwa**: Specyfikacje dotyczące tego, jak jest rozłożony składnik losowy (np. rozkład normalny, Poissona, dwumianowy)
:::

Przykład modelu regresji liniowej: $y = \beta_0 + \beta_1x + \epsilon$, gdzie $\epsilon \sim N(0, \sigma^2)$

W tym modelu:

-   $\beta_0$ i $\beta_1$ to parametry (estymandy), które chcemy oszacować
-   $\epsilon$ to składnik losowy reprezentujący niewyjaśnioną zmienność
-   Zakładamy normalność rozkładu błędów losowych

### Wnioskowanie Przyczynowe vs. Predykcyjne

W analizie statystycznej możemy mieć dwa główne cele:

1.  **Wnioskowanie przyczynowe**: Ustalenie, czy zmienna X *powoduje* zmianę w zmiennej Y
    -   Wymaga dodatkowych założeń lub specjalnych projektów badawczych
    -   Umożliwia przewidywanie efektów interwencji
2.  **Wnioskowanie predykcyjne**: Przewidywanie wartości Y na podstawie X
    -   Nie musi zakładać związku przyczynowego
    -   Koncentruje się na dokładności przewidywań

::: callout-warning
## Korelacja ≠ Przyczynowość

W statystyce, związek pozorny (spurious relationship) lub korelacja pozorna (spurious correlation) to relacja matematyczna, w której dwa lub więcej zdarzeń lub zmiennych są ze sobą powiązane, ale nie pozostają w relacji przyczynowo-skutkowej. Wynika to albo z przypadkowego zbiegu okoliczności, albo z obecności pewnego trzeciego, niewidocznego czynnika (określanego jako "wspólna zmienna odpowiedzi" (common response variable), "czynnik zakłócający" (confounding factor) lub "zmienna ukryta" (lurking variable)).

Jednym z najczęstszych błędów w statystyce jest interpretowanie korelacji jako dowodu na przyczynowość. Dwie zmienne mogą być silnie skorelowane z powodu:

1.  Zmiennej zakłócającej (confounder), która wpływa na obie zmienne
2.  Odwróconej przyczynowości (Y wpływa na X, a nie odwrotnie)
3.  Przypadku (korelacja pozorna)
:::

### Problemy Wnioskowania Przyczynowego

Fundamentalnym problemem wnioskowania przyczynowego jest niemożność obserwowania **kontrfaktów** (alternatywnych/kontrfaktycznych scenariuszy). Dla danej jednostki możemy zaobserwować tylko jeden potencjalny wynik.

![Fundamentalny problem wnioskowania przyczynowego: Możemy traktować wnioskowanie przyczynowe jako problem PREDYKCJI. Jak przewidzieć kontrfakt, skoro nigdy go nie obserwujemy?](stat_imgs/meme_horse.svg){fig-align="center"}

Przykład:

-   Obserwujemy osobę, która skończyła studia i zarabia 8000 zł
-   Nie możemy zaobserwować, ile ta sama osoba zarabiałaby, gdyby nie skończyła studiów

Metody przyczynowe próbują rozwiązać ten problem przez: 1. Randomizowane eksperymenty 2. Metoda zmiennych instrumentalnych 3. Matching 4. Analizę regresji nieciągłej 5. Modele regresji difference-in-differences

Wnioskowanie przyczynowe jest utrudnione przez różne problemy, takie jak:

![Błąd wspólnej przyczyny (confounding bias) i pozorna korelacja: picie alkoholu wieczorem jest wspólną przyczyną spania w butach i budzenia się z bólem głowy](stat_imgs/IMG_4337.jpg){fig-align="center"}

![Odwrócona przyczynowość](stat_imgs/ff13-23.png){fig-align="center"}

## Wnioskowanie Statystyczne

Wnioskowanie statystyczne to proces formułowania wniosków o populacji na podstawie danych z próby. Obejmuje dwa główne obszary:

### 1. Estymacja

Estymacja to proces szacowania nieznanych parametrów populacji na podstawie danych z próby. Wyróżniamy:

-   **Estymację punktową**: Podajemy pojedynczą wartość (oszacowanie) jako najlepsze przybliżenie parametru
-   **Estymację przedziałową**: Konstruujemy przedział ufności, który wskazuje zakres możliwych wartości parametru zgodnych z naszymi danymi

Przykład przedziału ufności: "95% przedział ufności dla średniego wzrostu dorosłych Polaków wynosi (173 cm, 175 cm)".

**Poprawna interpretacja przedziału ufności**: Gdybyśmy wielokrotnie pobierali próby z tej samej populacji i dla każdej z nich konstruowali 95% przedział ufności według tej samej metody, to około 95% tak skonstruowanych przedziałów zawierałoby prawdziwą wartość parametru populacji.

**Niepoprawna interpretacja**: "Jest 95% szans, że prawdziwa średnia znajduje się w przedziale (173 cm, 175 cm)" – jest to błędne, ponieważ parametr populacji jest wartością stałą (choć nieznaną), a nie zmienną losową.

### 2. Testowanie Hipotez

Testowanie hipotez to formalna procedura weryfikacji przypuszczeń dotyczących parametrów populacji. Najlepiej zrozumieć tę koncepcję na konkretnym przykładzie:

::: callout-tip
## Przykład: Test dwumianowy dla monety

Wyobraźmy sobie, że chcemy sprawdzić, czy moneta jest uczciwa.

1.  **Pytanie badawcze**: Czy moneta jest uczciwa (prawdopodobieństwo wypadnięcia orła = 0.5)?

2.  **Formułujemy hipotezy**:

    -   **Hipoteza zerowa (H₀)**: p = 0.5 (moneta jest uczciwa)
    -   **Hipoteza alternatywna (H₁)**: p ≠ 0.5 (moneta nie jest uczciwa)

3.  **Zbieramy dane**: Rzucamy monetą 100 razy i otrzymujemy 65 orłów.

4.  **Analizujemy**: Czy 65 orłów na 100 rzutów jest dowodem przeciwko hipotezie, że moneta jest uczciwa?

5.  **Rozumowanie**:

    -   Jeśli moneta byłaby uczciwa (p = 0.5), to liczba orłów w 100 rzutach powinna podlegać rozkładowi dwumianowemu B(100, 0.5)
    -   Dla tego rozkładu oczekujemy średnio 50 orłów, z odchyleniem standardowym √(100 × 0.5 × 0.5) = 5
    -   Otrzymanie 65 orłów oznacza odchylenie o 3 odchylenia standardowe od oczekiwanej wartości
    -   Prawdopodobieństwo uzyskania 65 lub więcej orłów przy uczciwej monecie jest bardzo małe (p \< 0.01)

6.  **Wniosek**: Ponieważ zaobserwowany wynik jest bardzo mało prawdopodobny przy założeniu, że moneta jest uczciwa, odrzucamy hipotezę zerową i wnioskujemy, że moneta najprawdopodobniej nie jest uczciwa.
:::

Ogólna procedura testowania hipotez:

1.  Formułujemy hipotezę zerową (H₀) i alternatywną (H₁)
2.  Wybieramy poziom istotności α (najczęściej 0.05) - reguła decyzyjna, która pomaga ocenić co to znaczy małe prawdopodobieństwo zaobserwowanego wyniku przy założeniu, że testowana hipoteza jest prawdziwa
3.  Zbieramy dane i obliczamy odpowiednią statystykę testową
4.  Obliczamy p-wartość (prawdopodobieństwo uzyskania naszych danych lub bardziej ekstremalnych, przy założeniu prawdziwości H₀)
5.  Podejmujemy decyzję: jeśli p \< α, odrzucamy H₀ na rzecz H₁

::: callout-note
## Intuicja za testowaniem hipotez

Testowanie hipotez przypomina procedurę sądową:

-   H₀ odpowiada zasadzie "niewinny, dopóki nie udowodni się winy" (zakładamy, że parametr ma określoną wartość)
-   Dane stanowią "dowody" przeciwko H₀
-   P-wartość określa, jak silne są te dowody
-   Jeśli dowody są wystarczająco mocne (p \< α), "skazujemy" H₀ (odrzucamy ją)
-   Jeśli dowody nie są wystarczająco mocne, nie odrzucamy H₀ (ale nie udowadniamy jej prawdziwości)
:::

::: callout-important
## Częste błędy interpretacji p-wartości i testów

1.  P-wartość **NIE** jest prawdopodobieństwem, że hipoteza zerowa jest prawdziwa
2.  P-wartość **NIE** jest prawdopodobieństwem popełnienia błędu przy odrzuceniu H₀
3.  Brak odrzucenia H₀ **NIE** oznacza jej udowodnienia (brak dowodów przeciwko oskarżonemu nie dowodzi jego niewinności)
4.  Bardzo mała p-wartość **NIE** oznacza dużego efektu praktycznego (istotność statystyczna ≠ istotność praktyczna)
5.  P-wartość zależy od wielkości próby - przy bardzo dużych próbach nawet małe, nieistotne praktycznie różnice mogą być statystycznie istotne

**Definicja p-wartości**: Prawdopodobieństwo zaobserwowania wyniku co najmniej tak ekstremalnego jak uzyskany, przy założeniu prawdziwości hipotezy zerowej.
:::

::: callout-tip
## Typy błędów w testowaniu hipotez

-   **Błąd I rodzaju (α)**: Odrzucenie prawdziwej hipotezy zerowej ("skazanie niewinnego")
    -   Prawdopodobieństwo tego błędu kontrolujemy poprzez poziom istotności α
-   **Błąd II rodzaju (β)**: Nieodrzucenie fałszywej hipotezy zerowej ("uniewinnienie winnego")
    -   Prawdopodobieństwo uniknięcia tego błędu (1-β) nazywamy mocą testu
    -   Moc testu zwiększa się wraz z wielkością próby i wielkością efektu
:::

## Mocne Podstawy Dobrego Badania

Aby przeprowadzić rzetelne badanie statystyczne, należy zadbać o:

1.  **Reprezentatywność próby**: Próba powinna dobrze odzwierciedlać badaną populację
2.  **Odpowiedni rozmiar próby**: Większe próby dają dokładniejsze oszacowania i większą moc statystyczną
3.  **Kontrolę zmiennych zakłócających**: Zarówno w projektowaniu badania, jak i analizie danych
4.  **Właściwe metody statystyczne**: Dopasowane do typu danych i pytań badawczych
5.  **Przejrzystą interpretację**: Uwzględniającą ograniczenia badania i alternatywne wyjaśnienia

::: callout-tip
## Podsumowanie kluczowych pojęć często mylonych przez studentów:

| Pojęcie | Definicja | Przykład |
|------------------------|------------------------|------------------------|
| **Parametr populacji (Estymand)** | Wartość charakteryzująca populację, zwykle nieznana | μ (średnia populacji) |
| **Estymator (Statystyka)** | Funkcja/procedura szacowania parametru na podstawie próby | $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ |
| **Oszacowanie (Estymata)** | Konkretna wartość uzyskana po zastosowaniu estymatora do próby | $\bar{x} = 173.5$ cm |
| **Błąd standardowy** | Miara zmienności estymatora między próbami | $SE(\bar{x}) = \frac{s}{\sqrt{n}}$ |
| **Przedział ufności** | Zakres wartości, który z określonym prawdopodobieństwem zawiera parametr | (173 cm, 175 cm) |
| **P-wartość** | Prawdopodobieństwo zaobserwowania danych przy założeniu H₀ | p = 0.03 |
:::

## Narzędzia do Nauki o Danych w Naukach Społecznych(\*)

W tym kursie będziemy głównie używać R do naszej analizy danych, ponieważ jest on szeroko stosowany w badaniach nauk społecznych.

### R w Analizie Danych Nauk Społecznych

R oferuje potężne możliwości dla badań w naukach społecznych, od manipulacji danymi po zaawansowane modelowanie statystyczne.

```{r}
#| code-fold: true
#| code-summary: "Kliknij, aby pokazać/ukryć kod R"

library(tidyverse)

# Set seed for reproducibility
set.seed(42)

# Generate example data with a Simpson's Paradox
n <- 1000
data <- tibble(
  age_group = sample(c("Young", "Middle", "Old"), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),
  education_years = case_when(
    age_group == "Young" ~ rnorm(n, mean = 10, sd = 1),
    age_group == "Middle" ~ rnorm(n, mean = 13, sd = 1),
    age_group == "Old" ~ rnorm(n, mean = 16, sd = 1)
  ),
  income = case_when(
    age_group == "Young" ~ 70000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),
    age_group == "Middle" ~ 50000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),
    age_group == "Old" ~ 30000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000)
  )
)

# Basic data summary
summary(data)

# Correlation analysis
cor(data %>% select(education_years, income))

# Overall trend (Simpson's Paradox)
overall_plot <- ggplot(data, aes(x = education_years, y = income)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Overall Relationship between Education and Income",
       subtitle = "Simpson's Paradox: Appears negative",
       x = "Years of Education", y = "Income") +
  theme_minimal()

# Trend by age group (Resolving Simpson's Paradox)
grouped_plot <- ggplot(data, aes(x = education_years, y = income, color = age_group)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Relationship between Education and Income by Age Group",
       subtitle = "Resolving Simpson's Paradox: Positive relationship within groups",
       x = "Years of Education", y = "Income") +
  theme_minimal()

# Statistical analysis
model_overall <- lm(income ~ education_years, data = data)
model_by_age <- lm(income ~ education_years + age_group, data = data)

# Print results
print(overall_plot)
print(grouped_plot)
print(summary(model_overall))
print(summary(model_by_age))

# Calculate and print correlations
overall_cor <- cor(data$education_years, data$income)
group_cors <- data %>%
  group_by(age_group) %>%
  summarize(correlation = cor(education_years, income))

print("Overall correlation:")
print(overall_cor)
print("Correlations by age group:")
print(group_cors)
```

Ten przykład demonstruje podstawowe operacje na danych, statystyki opisowe i wizualizację danych przy użyciu R.

## Wnioskowanie przyczynowe a badania obserwacyjne

W naukach społecznych i nie tylko, zrozumienie relacji między zmiennymi jest kluczowe. Dwa główne podejścia to wnioskowanie przyczynowe i badania obserwacyjne, każde z własnymi mocnymi stronami i ograniczeniami.

::: panel-tabset
### Wnioskowanie przyczynowe

-   Dąży do ustalenia związków przyczynowo-skutkowych
-   Często obejmuje plany eksperymentalne lub zaawansowane techniki statystyczne
-   Stara się odpowiedzieć na pytania "Co by było, gdyby?" i określić wpływ interwencji
-   Przykłady: Randomizowane badania kontrolowane, projekty quasi-eksperymentalne, zmienne instrumentalne

### Badania obserwacyjne

-   Badają relacje między zmiennymi bez bezpośredniej interwencji
-   Opierają się na danych zebranych w naturalnych warunkach lub z istniejących zbiorów danych
-   Mogą identyfikować korelacje i wzorce, ale mają trudności z ustaleniem przyczynowości
-   Przykłady: Badania kohortowe, badania kliniczno-kontrolne, przekrojowe badania ankietowe

### Kluczowe rozróżnienie: Korelacja vs. Przyczynowość

Fundamentalna zasada w badaniach głosi, że korelacja między dwiema zmiennymi niekoniecznie implikuje związek przyczynowy. Ta koncepcja jest kluczowa przy interpretacji wyników badań obserwacyjnych.
:::

::: callout-important
## Pamiętaj: Korelacja nie implikuje przyczynowości

-   **Korelacja**: Mierzy siłę i kierunek związku między zmiennymi
-   **Przyczynowość**: Wskazuje, że zmiany w jednej zmiennej bezpośrednio powodują zmiany w drugiej

Chociaż silne korelacje mogą sugerować potencjalne związki przyczynowe, do ustalenia przyczynowości wymagane są dodatkowe dowody i rygorystyczne metody.
:::

::: panel-tabset
### Wyzwania w ustalaniu przyczynowości

-   Zmienne zakłócające: Niezmierzone czynniki wpływające zarówno na domniemaną przyczynę, jak i skutek
-   Odwrotna przyczynowość: Domniemany skutek może w rzeczywistości powodować domniemaną przyczynę
-   Błąd selekcji: Nielosowy dobór uczestników do grup badawczych

### Metody wzmacniania twierdzeń przyczynowych

1.  Randomizowane badania kontrolowane (gdy są etyczne i wykonalne)
2.  Naturalne eksperymenty lub projekty quasi-eksperymentalne
3.  Dopasowanie według propensity score
4.  Analiza różnicy w różnicach
5.  Podejścia oparte na zmiennych instrumentalnych
6.  Skierowane grafy acykliczne (DAG) do wizualizacji relacji przyczynowych

### Znaczenie w naukach społecznych

Zrozumienie różnicy między wnioskowaniem przyczynowym a badaniami obserwacyjnymi jest kluczowe w naukach społecznych, gdzie względy etyczne często ograniczają manipulacje eksperymentalne. Badacze muszą starannie projektować badania i interpretować wyniki, aby uniknąć wprowadzających w błąd wniosków dotyczących przyczynowości.
:::

## Modele w Nauce: Od Deterministycznych do Stochastycznych

Modele są niezbędnymi narzędziami w badaniach naukowych, pomagając naukowcom reprezentować, rozumieć i przewidywać złożone zjawiska. Ta sekcja omawia główne typy modeli stosowanych w nauce, wraz z przykładami ich zastosowań. Należy pamiętać, że te kategorie często się nakładają, a wiele modeli naukowych łączy w sobie różne aspekty.

### Modele Matematyczne

Modele matematyczne wykorzystują równania i koncepcje matematyczne do opisywania i analizowania systemów lub zjawisk. Można je podzielić na kilka podkategorii, choć należy pamiętać, że niektóre złożone modele mogą zawierać elementy z wielu kategorii:

#### a. Modele Deterministyczne

Modele deterministyczne dostarczają precyzyjnych przewidywań na podstawie zestawu zmiennych, bez uwzględniania losowości na poziomie makroskopowym.

**Przykład:** Prawa ruchu Newtona, które mogą precyzyjnie przewidzieć ruch obiektów pod wpływem znanych sił w mechanice klasycznej.

#### b. Modele Stochastyczne

Modele stochastyczne uwzględniają losowość i prawdopodobieństwo. Jednak kluczowe jest rozróżnienie dwóch fundamentalnie różnych typów modeli stochastycznych:

##### i. Klasyczne Modele Stochastyczne

Te modele zajmują się losowością wynikającą z niepełnej informacji lub złożonych interakcji w systemach klasycznych. Podstawowy system jest deterministyczny, ale praktyczne ograniczenia w pomiarach lub obliczeniach prowadzą do użycia opisów probabilistycznych.

**Przykład:** Modele regresji w statystyce, gdzie losowość reprezentuje niewyjaśnioną zmienność lub błąd pomiaru:

$$y = β_0 + β_1x + ε$$

Gdzie:

-   $y$ to zmienna zależna (np. wielkość popytu na dobro)
-   $x$ to zmienna niezależna (np. cena lub dochód konsumenta)
-   $β_0$ i $β_1$ to parametry
-   $ε$ to składnik błędu, reprezentujący niewyjaśnioną zmienność

##### ii. Kwantowe Modele Stochastyczne

Te modele zajmują się fundamentalną, nieredukowalną losowością nieodłącznie związaną z systemami mechaniki kwantowej. Ta losowość nie wynika z braku informacji, ale jest podstawową cechą rzeczywistości kwantowej.

**Przykład:** Model Standardowy w fizyce cząstek elementarnych, który opisuje interakcje cząstek za pomocą kwantowej teorii pola. Na przykład, rozpad cząstki jest z natury probabilistyczny:

$$P(t) = e^{-t/τ}$$

Gdzie:

-   $P(t)$ to prawdopodobieństwo, że cząstka nie rozpadła się po czasie t
-   $τ$ to średni czas życia cząstki

#### c. Modele Symulacji Komputerowych

Symulacje komputerowe wykorzystują algorytmy i metody obliczeniowe oparte na modelach matematycznych do symulowania złożonych systemów i przewidywania ich zachowania w czasie. Mogą być deterministyczne lub stochastyczne.

**Przykład:** Modele klimatyczne symulujące system klimatyczny Ziemi, uwzględniające czynniki takie jak skład atmosfery, prądy oceaniczne i promieniowanie słoneczne do prognozowania przyszłych scenariuszy klimatycznych.

### Modele Koncepcyjne

Modele koncepcyjne to abstrakcyjne reprezentacje systemów lub procesów, często wykorzystujące diagramy lub schematy blokowe do ilustrowania relacji między komponentami.

**Przykład:** Model obiegu wody w naukach o Ziemi, który ilustruje ciągły ruch wody w obrębie Ziemi i atmosfery poprzez procesy takie jak parowanie, opady i spływ powierzchniowy.

### Modele Fizyczne

Modele fizyczne to namacalne reprezentacje obiektów lub systemów, często w formie pomniejszonej lub uproszczonej wersji rzeczywistego obiektu.

**Przykład:** Modele tunelu aerodynamicznego w badaniach aerodynamiki, używane do badania efektów przepływu powietrza wokół obiektów stałych i optymalizacji projektów samolotów, pojazdów lub budynków.

### Modele Teoretyczne

Modele teoretyczne to abstrakcyjne ramy oparte na fundamentalnych zasadach i hipotezach, często używane do wyjaśniania obserwowanych zjawisk lub przewidywania nowych. Te modele często wykorzystują równania matematyczne i mogą być deterministyczne lub stochastyczne.

**Przykład:** Teoria ewolucji poprzez dobór naturalny, która dostarcza ram do zrozumienia różnorodności i adaptacji form życia w czasie.

### Podsumowanie

Te różne formy modeli odgrywają kluczową rolę w badaniach naukowych, każda oferując unikalne zalety dla zrozumienia i przewidywania zjawisk naturalnych. Naukowcy często używają wielu typów modeli jednocześnie, aby uzyskać kompleksowy wgląd w złożone systemy i procesy.

Ważne jest, aby zdawać sobie sprawę, że te kategorie nie są wzajemnie wykluczające i często się nakładają:

1.  Modele matematyczne stanowią podstawę dla wielu innych typów modeli, w tym symulacji komputerowych i niektórych modeli teoretycznych.
2.  Modele symulacji komputerowych są zasadniczo modelami matematycznymi implementowanymi za pomocą metod obliczeniowych i mogą być deterministyczne lub stochastyczne.
3.  Modele teoretyczne często wykorzystują sformułowania matematyczne i mogą być implementowane jako symulacje komputerowe.
4.  Modele fizyczne mogą być projektowane na podstawie modeli matematycznych i mogą być używane do walidacji symulacji komputerowych.

Wybór typu modelu często zależy od konkretnego pytania badawczego, natury badanego systemu, dostępnych danych oraz zasobów obliczeniowych. W miarę postępu nauki granice między tymi typami modeli coraz bardziej się zacierają, prowadząc do coraz bardziej wyrafinowanych i interdyscyplinarnych podejść do modelowania złożonych zjawisk.

Kluczowe jest rozróżnienie różnych typów modeli stochastycznych. Klasyczne modele stochastyczne, takie jak te używane w analizie regresji, zajmują się losowością wynikającą z niepełnej informacji lub złożonych interakcji w systemach, które są zasadniczo deterministyczne. Z drugiej strony, kwantowe modele stochastyczne, jak te w fizyce cząstek, zajmują się fundamentalną, nieredukowalną losowością nieodłącznie związaną z systemami mechaniki kwantowej. To rozróżnienie odzwierciedla głębokie różnice między klasycznymi a kwantowymi paradygmatami w fizyce i podkreśla różnorodne sposoby, w jakie prawdopodobieństwo jest wykorzystywane w modelowaniu naukowym.

## Zrozumienie Pozornych Korelacji, Zmiennych Zakłócających i Kolizyjnych (\*)

W tej sekcji zbadamy trzy ważne pojęcia w analizie statystycznej: pozorne korelacje, zmienne zakłócające i zmienne kolizyjne. Zrozumienie tych pojęć jest kluczowe dla uniknięcia błędnej interpretacji danych i wyciągania nieprawidłowych wniosków z analiz statystycznych.

Zacznijmy od załadowania niezbędnych bibliotek:

```{r}
#| label: setup
#| message: false

library(tidyverse)
library(dagitty)
library(ggdag)
set.seed(123) # dla powtarzalności
```

### Pozorne Korelacje

Pozorne korelacje to związki między zmiennymi, które wydają się przyczynowe, ale w rzeczywistości są przypadkowe lub spowodowane przez niewidoczny trzeci czynnik.

#### Przykład: Sprzedaż lodów a przypadki utonięć

Stwórzmy zbiór danych, który pokazuje pozorną korelację między sprzedażą lodów a przypadkami utonięć:

```{r}
#| label: spurious-data

n <- 100
dane_pozorne <- tibble(
  temperatura = rnorm(n, mean = 25, sd = 5),
  sprzedaz_lodow = 100 + 5 * temperatura + rnorm(n, sd = 10),
  przypadki_utoniec = 1 + 0.5 * temperatura + rnorm(n, sd = 2)
)

ggplot(dane_pozorne, aes(x = sprzedaz_lodow, y = przypadki_utoniec)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Pozorna Korelacja: Sprzedaż Lodów vs Przypadki Utonięć",
       x = "Sprzedaż Lodów", y = "Przypadki Utonięć")
```

Ten wykres pokazuje pozytywną korelację między sprzedażą lodów a przypadkami utonięć. Jednak ta relacja jest pozorna. Prawdziwą przyczyną obu zjawisk jest temperatura:

```{r}
#| label: spurious-explanation

ggplot(dane_pozorne, aes(x = temperatura)) +
  geom_point(aes(y = sprzedaz_lodow), color = "blue") +
  geom_point(aes(y = przypadki_utoniec * 10), color = "red") +
  geom_smooth(aes(y = sprzedaz_lodow), method = "lm", se = FALSE, color = "blue") +
  geom_smooth(aes(y = przypadki_utoniec * 10), method = "lm", se = FALSE, color = "red") +
  scale_y_continuous(
    name = "Sprzedaż Lodów",
    sec.axis = sec_axis(~./10, name = "Przypadki Utonięć")
  ) +
  labs(title = "Temperatura jako Wspólna Przyczyna",
       x = "Temperatura")
```

### Zmienne Zakłócające

Zmienna zakłócająca to zmienna, która wpływa zarówno na zmienną zależną, jak i niezależną, powodując pozorny związek.

#### Przykład: Edukacja, Dochód i Wiek

Stwórzmy zbiór danych, w którym wiek zakłóca relację między edukacją a dochodem:

```{r}
#| label: confounder-data

library(tidyverse)
library(viridis)

n <- 1000
confounder_data <- tibble(
  age = runif(n, 25, 65),
  education = round(10 + 0.1 * age + rnorm(n, sd = 2)),
  income = 20000 + 1000 * education + 500 * age + rnorm(n, sd = 5000)
)

# Without controlling for age
model_naive <- lm(income ~ education, data = confounder_data)
# Controlling for age
model_adjusted <- lm(income ~ education + age, data = confounder_data)

# Create age groups for visualization
confounder_data <- confounder_data %>%
  mutate(age_group = cut(age, breaks = 3, labels = c("Young", "Middle", "Old")))

# Visualize
ggplot(confounder_data, aes(x = education, y = income)) +
  geom_point(aes(color = age), alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red", linewidth = 1.2) +
  geom_smooth(aes(group = age_group, color = as.numeric(age_group)), 
              method = "lm", se = FALSE, linewidth = 1) +
  scale_color_viridis_c(name = "Age", 
                        breaks = c(30, 45, 60), 
                        labels = c("Young", "Middle", "Old")) +
  labs(title = "Education vs Income, Confounded by Age",
       x = "Years of Education", y = "Income") +
  theme_minimal()
```

Porównajmy współczynniki:

```{r}
#| label: confounder-models

summary(model_naive)$coefficients["education", "Estimate"]
summary(model_adjusted)$coefficients["education", "Estimate"]
```

Efekt edukacji na dochód jest przeszacowany, gdy nie kontrolujemy wieku.

### Zmienne Kolizyjne

Zmienna kolizyjna to zmienna, na którą wpływają zarówno zmienna niezależna, jak i zmienna zależna. Kontrolowanie zmiennej kolizyjnej może wprowadzić pozorną korelację.

#### Przykład: Satysfakcja z pracy, Wynagrodzenie i Równowaga między pracą a życiem prywatnym

Stwórzmy zbiór danych, w którym równowaga między pracą a życiem prywatnym jest zmienną kolizyjną między satysfakcją z pracy a wynagrodzeniem:

```{r}
#| label: collider-data

n <- 1000
dane_kolizyjne <- tibble(
  satysfakcja_z_pracy = rnorm(n),
  wynagrodzenie = rnorm(n),
  rownowaga_praca_zycie = -0.5 * satysfakcja_z_pracy - 0.5 * wynagrodzenie + rnorm(n, sd = 0.5)
)

# Bez kontrolowania równowagi praca-życie
model_poprawny <- lm(wynagrodzenie ~ satysfakcja_z_pracy, data = dane_kolizyjne)

# Błędne kontrolowanie równowagi praca-życie
model_kolizyjny <- lm(wynagrodzenie ~ satysfakcja_z_pracy + rownowaga_praca_zycie, data = dane_kolizyjne)

# Wizualizacja
ggplot(dane_kolizyjne, aes(x = satysfakcja_z_pracy, y = wynagrodzenie, color = rownowaga_praca_zycie)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  scale_color_viridis_c() +
  labs(title = "Satysfakcja z Pracy vs Wynagrodzenie, Równowaga Praca-Życie jako Zmienna Kolizyjna",
       x = "Satysfakcja z Pracy", y = "Wynagrodzenie")
```

Porównajmy współczynniki:

```{r}
#| label: collider-models

summary(model_poprawny)$coefficients["satysfakcja_z_pracy", "Estimate"]
summary(model_kolizyjny)$coefficients["satysfakcja_z_pracy", "Estimate"]
```

Kontrolowanie zmiennej kolizyjnej (równowaga praca-życie) wprowadza pozorną korelację między satysfakcją z pracy a wynagrodzeniem.

Zrozumienie pozornych korelacji, zmiennych zakłócających i kolizyjnych jest kluczowe dla prawidłowej analizy statystycznej i wnioskowania przyczynowego. Zawsze rozważ podstawową strukturę przyczynową swoich danych i bądź ostrożny w kwestii tego, które zmienne kontrolujesz w swoich analizach.

## Etyczne Aspekty w Analizie Danych Nauk Społecznych

Etyka odgrywa kluczową rolę w badaniach nauk społecznych:

1.  **Prywatność i Zgoda**: Zapewnienie prywatności uczestników i świadomej zgody
2.  **Ochrona Danych**: Bezpieczne przechowywanie i zarządzanie wrażliwymi danymi osobowymi
3.  **Błędy i Reprezentacja**: Adresowanie błędów próbkowania i zapewnienie różnorodnej reprezentacji
4.  **Przejrzystość**: Jasne komunikowanie metod badawczych i ograniczeń
5.  **Wpływ Społeczny**: Rozważanie potencjalnych społecznych implikacji wyników badań

## Appendix A: Losowość Klasyczna a Kwantowa: Zrozumienie Fundamentalnych Różnic

Aby zrozumieć, jak losowość w mechanice kwantowej różni się od losowości reprezentowanej przez składnik błędu w modelach regresji, musimy przeanalizować ich pochodzenie, naturę i implikacje.

### Pochodzenie Losowości

#### Losowość Klasyczna (Modele Regresji)

-   **Źródło**: Niekompletna informacja lub złożone interakcje w systemie, który w zasadzie jest deterministyczny.
-   **Natura**: Niepewność epistemiczna (wynikająca z braku wiedzy).
-   **Przykład**: W modelu regresji, $y = β_0 + β_1x + ε$, składnik błędu ε reprezentuje niewyjaśnioną zmienność.

#### Losowość Kwantowa

-   **Źródło**: Fundamentalna właściwość systemów kwantowych.
-   **Natura**: Niepewność ontyczna (nieodłączna cecha systemu, nie wynika z braku wiedzy).
-   **Przykład**: Dokładny moment rozpadu atomu radioaktywnego nie może być przewidziany, można określić jedynie jego prawdopodobieństwo.

### Implikacje Filozoficzne

#### Losowość Klasyczna

-   **Determinizm**: Podstawowa rzeczywistość jest deterministyczna; losowość odzwierciedla naszą niewiedzę.
-   **Ukryte Zmienne**: W zasadzie, gdybyśmy mieli pełną informację, moglibyśmy dokładnie przewidzieć wyniki.

#### Losowość Kwantowa

-   **Indeterminizm**: Losowość jest fundamentalną cechą rzeczywistości, nie tylko naszego jej opisu.
-   **Brak Ukrytych Zmiennych**: Nawet przy pełnej informacji o systemie kwantowym, niektóre wyniki pozostają nieprzewidywalne (co sugeruje twierdzenie Bella).

### Ujęcie Matematyczne

#### Losowość Klasyczna

-   **Teoria Prawdopodobieństwa**: Oparta na klasycznej teorii prawdopodobieństwa.
-   **Rozkład**: Często zakłada się znane rozkłady (np. rozkład normalny w wielu modelach regresji).
-   **Centralne Twierdzenie Graniczne**: Stosuje się do dużych prób zmiennych losowych.

#### Losowość Kwantowa

-   **Prawdopodobieństwo Kwantowe**: Oparte na matematycznych podstawach mechaniki kwantowej.
-   **Funkcja Falowa**: Opisuje stan kwantowy i jego ewolucję.
-   **Reguła Borna**: Określa prawdopodobieństwa wyników pomiarów na podstawie funkcji falowej.

### Przewidywalność i Kontrola

#### Losowość Klasyczna

-   **Redukowalna**: W zasadzie można ją zmniejszyć, zbierając więcej danych lub poprawiając dokładność pomiarów.
-   **Kontrolowalna**: Błędy systematyczne można zidentyfikować i skorygować.

#### Losowość Kwantowa

-   **Nieredukowalna**: Nie można jej wyeliminować nawet przy idealnych pomiarach.
-   **Fundamentalnie Niekontrolowalna**: Sam akt pomiaru wpływa na system (problem pomiaru).

### Praktyczne Implikacje

#### Losowość Klasyczna

-   **Redukcja Błędów**: Koncentracja na udoskonalaniu technik pomiarowych i zbierania danych.
-   **Udoskonalanie Modelu**: Dążenie do wyjaśnienia większej wariancji i zmniejszenia składnika błędu.

#### Losowość Kwantowa

-   **Nieodłączne Ograniczenie**: Akceptacja fundamentalnych granic przewidywalności.
-   **Przewidywania Probabilistyczne**: Skupienie na dokładnych rozkładach prawdopodobieństwa zamiast na dokładnych wynikach.

### Przykłady Pomagające Zrozumieć Różnicę

#### Przykład Losowości Klasycznej

Wyobraź sobie rzut monetą. Fizyka klasyczna mówi, że wynik jest zdeterminowany przez warunki początkowe (przyłożona siła, opór powietrza itp.). "Losowość" wynika z naszej niezdolności do precyzyjnego zmierzenia i uwzględnienia wszystkich tych czynników.

#### Przykład Losowości Kwantowej

W eksperymencie z podwójną szczeliną pojedyncze cząstki wykazują wzory interferencyjne, jakby przechodziły przez obie szczeliny jednocześnie. Dokładna ścieżka każdej pojedynczej cząstki jest fundamentalnie nieokreślona do momentu pomiaru, a tej nieokreśloności nie można rozwiązać przez bardziej precyzyjne pomiary.

### Podsumowanie

Chociaż oba rodzaje losowości prowadzą do probabilistycznych przewidywań, ich fundamentalne natury są zupełnie różne:

-   Losowość klasyczna w modelach regresji jest odzwierciedleniem naszej niepełnej wiedzy lub ograniczeń pomiarowych w systemie, który w zasadzie jest deterministyczny.
-   Losowość kwantowa jest fundamentalną właściwością systemów kwantowych, reprezentującą nieodłączną nieokreśloność w naturze, która utrzymuje się nawet przy doskonałej wiedzy i pomiarze.

Zrozumienie tych różnic jest kluczowe dla prawidłowej interpretacji i stosowania modeli statystycznych w różnych kontekstach naukowych, od nauk społecznych wykorzystujących analizę regresji po eksperymenty z fizyki kwantowej.

## Appendix B: Duże Modele Językowe - Zrozumienie Ich Stochastycznej Natury

Duże Modele Językowe (LLM), takie jak GPT-3, BERT i Claude, zrewolucjonizowały przetwarzanie języka naturalnego, ale mogą popełniać zagadkowe błędy, szczególnie w zadaniach matematycznych. Ten dodatek wyjaśnia funkcjonowanie LLM, ich stochastyczną naturę i porównuje je z klasycznymi modelami statystycznymi.

### Podstawy LLM i Ich Stochastyczna Natura

LLM są trenowane na ogromnych zbiorach danych tekstowych, aby przewidywać rozkład prawdopodobieństwa następnego tokenu w sekwencji. Wykorzystują architektury transformerowe do przetwarzania i generowania tekstu. Kluczowe aspekty ich stochastycznej natury obejmują:

1.  Probabilistyczny wybór tokenów: LLM wybierają każde słowo na podstawie obliczonych prawdopodobieństw, a nie stałych reguł.
2.  Losowość kontrolowana temperaturą: Parametr "temperatury" dostosowuje losowość wyborów, równoważąc kreatywność i spójność.
3.  Niedeterministyczne wyniki: Te same dane wejściowe mogą prowadzić do różnych wyników w oddzielnych uruchomieniach.
4.  Kontekstowa niejednoznaczność: LLM interpretują kontekst probabilistycznie, co czasami prowadzi do nieporozumień.

### Porównanie z Klasycznymi Modelami Statystycznymi

Aby lepiej zrozumieć LLM, porównajmy je z regresją Najmniejszych Kwadratów (OLS):

| Aspekt | Regresja OLS | Duże Modele Językowe |
|------------------------|------------------------|------------------------|
| Podstawowa funkcja | Przewiduje ciągłe wyniki na podstawie zmiennych wejściowych | Przewiduje rozkład prawdopodobieństwa następnego tokenu na podstawie poprzednich tokenów |
| Wejście-Wyjście | Zmienne ciągłe, relacje liniowe | Dyskretne tokeny, relacje nieliniowe |
| Typ predykcji | Predykcje punktowe z przedziałami ufności | Rozkłady prawdopodobieństwa dla możliwych tokenów |
| Złożoność modelu | Niewiele parametrów | Miliardy parametrów |
| Interpretowalność | Jasne interpretacje współczynników | Largely nieprzejrzyste działanie wewnętrzne |
| Obsługa szumu | Zakłada losowy szum w zmiennej wynikowej | Radzi sobie ze zmiennością języka naturalnego |
| Ekstrapolacja | Mniej wiarygodna poza zakresem treningu | Mniej wiarygodna dla nieznanych tematów |

Oba modele dążą do nauczenia się mapowania wejścia-wyjścia na podstawie wzorców w danych treningowych.

### Implikacje dla Zadań Matematycznych

Stochastyczna natura LLM wpływa na operacje matematyczne:

1.  Zmienne wyniki dla powtarzanych obliczeń: Każda próba może dać inny wynik ze względu na probabilistyczny wybór tokenów.
2.  Pewność nie gwarantuje poprawności: Wysoka pewność modelu może wystąpić nawet dla niepoprawnych odpowiedzi.
3.  Aproksymacja zamiast dokładnych obliczeń: LLM dopasowują wzorce zamiast wykonywać precyzyjne obliczenia.

Ograniczenia w zadaniach matematycznych wynikają z:

-   Niedopasowania celu treningu: LLM są trenowane do przewidywania języka, nie dokładności matematycznej.
-   Braku jawnego rozumowania matematycznego: Nie mają wbudowanych reguł czy operacji matematycznych.
-   Braku pamięci roboczej: LLM nie mogą niezawodnie przechowywać i manipulować wynikami pośrednimi.
-   Ograniczonego okna kontekstowego: Mogą tracić istotne informacje w długich problemach.
-   Ograniczeń danych treningowych: Niedoreprezentowanie pewnych koncepcji matematycznych może prowadzić do słabych wyników.
-   Braku kontroli spójności: LLM nie weryfikują logicznej spójności swoich wyników.

### Najlepsze Praktyki i Wnioski

Przy korzystaniu z LLM do zadań matematycznych:

1.  Skup się na wyjaśnieniach koncepcyjnych, nie na dokładnych obliczeniach: LLM doskonale wyjaśniają koncepcje, ale mogą zawodzić w dokładnych obliczeniach.
2.  Weryfikuj wyniki dedykowanym oprogramowaniem: Zawsze sprawdzaj obliczenia LLM odpowiednimi narzędziami matematycznymi.
3.  Rozbijaj złożone problemy: Podział zadań na mniejsze kroki może poprawić wydajność LLM.
4.  Bądź świadomy efektów przeformułowania: Różne sformułowania tego samego problemu mogą dawać różne wyniki.
5.  Używaj jako narzędzi wspomagających, nie zamienników dla ekspertyzy: LLM powinny uzupełniać, a nie zastępować wiedzę matematyczną.

Zrozumienie probabilistycznej natury LLM pomaga wykorzystać ich mocne strony w zadaniach językowych, jednocześnie uznając ich ograniczenia w dziedzinach wymagających deterministycznej precyzji, takich jak matematyka.

## Appendix C: Modele Deterministyczne a Modele Stochastyczne (\*)

### Modele Deterministyczne

Modele deterministyczne to te, w których wynik jest w pełni określony przez wartości parametrów i warunki początkowe. Modele te są często używane w fizyce i inżynierii.

### Przykład: Ruch Jednostajnie Przyspieszony

Klasycznym przykładem modelu deterministycznego jest ruch jednostajnie przyspieszony, opisany równaniem:

$$x(t) = x_0 + v_0t + \frac{1}{2}at^2$$

Gdzie:

-   $x(t)$ to położenie w czasie $t$
-   $x_0$ to położenie początkowe
-   $v_0$ to prędkość początkowa
-   $a$ to przyspieszenie
-   $t$ to czas

Zasymulujmy to w R:

```{r}
# Ruch jednostajnie przyspieszony
symuluj_ruch_przyspieszony <- function(x0, v0, a, t) {
  x0 + v0 * t + 0.5 * a * t^2
}

# Generowanie danych
t <- seq(0, 10, by = 0.1)
x <- symuluj_ruch_przyspieszony(x0 = 0, v0 = 2, a = 1, t = t)

# Wykres
plot(t, x, type = "l", xlab = "Czas", ylab = "Położenie", 
     main = "Ruch Jednostajnie Przyspieszony")
```

Ten kod wygeneruje wykres ruchu jednostajnie przyspieszonego, który jest intuicyjnym przykładem z dynamiki Newtona. W tym przypadku obiekt zaczyna ruch z początkową prędkością i przyspiesza jednostajnie, co prowadzi do parabolicznej trajektorii na wykresie położenia w funkcji czasu.

### Modele Stochastyczne w Naukach Społecznych

Modele stochastyczne uwzględniają losowość i są często używane w naukach społecznych, gdzie istnieje nieodłączna niepewność w badanych systemach.

### Przykład: Regresja Metodą Najmniejszych Kwadratów (OLS)

OLS to podstawowy model stochastyczny w naukach społecznych. Jest reprezentowany jako:

$$Y = \beta_0 + \beta_1X + \epsilon$$

Gdzie:

-   $Y$ to zmienna zależna
-   $X$ to zmienna niezależna
-   $\beta_0$ i $\beta_1$ to parametry
-   $\epsilon$ to składnik błędu (komponent stochastyczny)

Zademonstrujmy OLS w R:

```{r}
# Generowanie przykładowych danych
set.seed(123)
X <- rnorm(100)
Y <- 2 + 3*X + rnorm(100, sd = 0.5)

# Dopasowanie modelu OLS
model <- lm(Y ~ X)

# Podsumowanie modelu
summary(model)

# Wykres
plot(X, Y, main = "Regresja OLS")
abline(model, col = "red")
```

To dopasuje model OLS do symulowanych danych i wykreśli wyniki.

![Retrieved from: https://scientistcafe.com/ids/vbtradeoff](stat_imgs/ModelError.png)

### Zaawansowane Modele Stochastyczne: Duże Modele Językowe

Duże Modele Językowe (LLM), takie jak GPT-3, to złożone modele stochastyczne używane w przetwarzaniu języka naturalnego. Chociaż nie możemy zaimplementować pełnego LLM w tym tutorialu, możemy omówić jego zasady.

LLM opierają się na architekturze transformatora i wykorzystują mechanizmy samouwagi. Są trenowane na ogromnych ilościach danych tekstowych i uczą się przewidywać następny token w sekwencji.

Rdzeń LLM można postrzegać jako warunkowy rozkład prawdopodobieństwa:

$$P(x_t | x_{<t}, \theta)$$

Gdzie:

-   $x_t$ to aktualny token
-   $x_{<t}$ reprezentuje wszystkie poprzednie tokeny
-   $\theta$ to parametry modelu

::: callout-note
Tokeny w Dużych Modelach Językowych (LLM) to podstawowe jednostki tekstu, które model przetwarza. Można je postrzegać jako części słów lub znaki interpunkcyjne. Oto kluczowe informacje o tokenach:

Definicja: Tokeny to najmniejsze jednostki tekstu, które LLM przetwarza. Mogą to być całe słowa, części słów, a nawet pojedyncze znaki lub znaki interpunkcyjne. Tokenizacja: Proces dzielenia tekstu na tokeny nazywa się tokenizacją. LLM używają specyficznych algorytmów do wykonania tego zadania. Przykłady:

Słowo "kot" może być pojedynczym tokenem. Dłuższe słowo jak "zrozumienie" może być podzielone na wiele tokenów, np. "zrozum" i "ienie". Znaki interpunkcyjne jak "." czy "?" są często oddzielnymi tokenami. Powszechne przedrostki lub przyrostki mogą być własnymi tokenami.

Słownictwo: LLM mają ustalone słownictwo tokenów, które rozpoznają. To słownictwo zazwyczaj obejmuje od dziesiątek tysięcy do setek tysięcy tokenów. Znaczenie: Sposób tokenizacji tekstu może wpływać na to, jak model rozumie i generuje język. Jest to szczególnie ważne przy obsłudze różnych języków, rzadkich słów lub specjalistycznego słownictwa. Kontekst: W równaniu dla LLM: $$P(x_t | x_{<t}, \theta)$$ Gdzie:

$x_t$ reprezentuje bieżący token $x_{<t}$ reprezentuje wszystkie poprzednie tokeny w sekwencji $\theta$ reprezentuje parametry modelu
:::

W przeciwieństwie do modeli deterministycznych, LLM produkują różne wyniki nawet dla tego samego wejścia ze względu na ich stochastyczną naturę.

### Podsumowanie

Każdy rodzaj modelu ma swoje miejsce w nauce, w zależności od badanego systemu i poziomu niepewności.

Pamiętaj, że wybór między modelami deterministycznymi a stochastycznymi często zależy od natury badanego systemu i pytań, na które próbujesz odpowiedzieć. Modele deterministyczne są świetne dla systemów o dobrze zrozumiałej mechanice, podczas gdy modele stochastyczne sprawdzają się przy radzeniu sobie z nieodłączną losowością lub złożonymi, nie w pełni zrozumiałymi systemami.

## Appendix D: Wprowadzenie do R, RStudio i tidyverse

R to potężny język programowania i środowisko do obliczeń statystycznych i grafiki. Jest szeroko stosowany w środowisku akademickim, szczególnie w naukach społecznych, do analizy danych i wizualizacji.

#### Kluczowe cechy R:

-   Otwarty kod źródłowy i darmowy
-   Rozbudowany ekosystem pakietów
-   Silne wsparcie społeczności
-   Doskonały do analizy statystycznej i wizualizacji danych

### Pierwsze kroki z RStudio

RStudio to zintegrowane środowisko programistyczne (IDE) dla R, które ułatwia pracę z R.

#### Instalacja R i RStudio

1.  Pobierz i zainstaluj R ze strony [CRAN](https://cran.r-project.org/)
2.  Pobierz i zainstaluj RStudio ze [strony RStudio](https://www.rstudio.com/products/rstudio/download/)

#### Interfejs RStudio

RStudio ma cztery główne panele:

1.  **Edytor źródłowy**: Gdzie piszesz i edytujesz skrypty R
2.  **Konsola**: Gdzie możesz wpisywać polecenia R i widzieć wyniki
3.  **Środowisko/Historia**: Pokazuje wszystkie obiekty w twoim obszarze roboczym i historię poleceń
4.  **Pliki/Wykresy/Pakiety/Pomoc**: Wielofunkcyjny panel do zarządzania plikami, przeglądania wykresów, zarządzania pakietami i dostępu do pomocy

#### Podstawowe funkcje RStudio

-   Tworzenie nowego skryptu R: Plik \> Nowy plik \> Skrypt R
-   Uruchamianie kodu: Zaznacz kod i naciśnij Ctrl+Enter (Cmd+Enter na Macu)
-   Instalowanie pakietów: Narzędzia \> Instaluj pakiety
-   Uzyskiwanie pomocy: Wpisz `?nazwa_funkcji` w konsoli

### Podstawy R

#### Typy danych w R

```{r}
# Numeryczny
x <- 10.5
class(x)

# Całkowity
y <- 1L
class(y)

# Znakowy
imie <- "Alicja"
class(imie)

# Logiczny
jest_studentem <- TRUE
class(jest_studentem)
```

#### Struktury danych

##### Wektory

```{r}
# Tworzenie wektora
liczby <- c(1, 2, 3, 4, 5)
owoce <- c("jabłko", "banan", "wiśnia")

# Operacje na wektorach
liczby + 2
liczby * 2
mean(liczby)
length(owoce)
```

##### Macierze

```{r}
# Tworzenie macierzy
m <- matrix(1:6, nrow = 2, ncol = 3)
print(m)

# Operacje na macierzach
t(m)  # transpozycja
m * 2  # mnożenie skalarne
```

##### Ramki danych

```{r}
# Tworzenie ramki danych
df <- data.frame(
  imie = c("Alicja", "Bartek", "Celina"),
  wiek = c(25, 30, 35),
  student = c(TRUE, FALSE, TRUE)
)
print(df)

# Dostęp do elementów ramki danych
df$imie
df[1, 2]
df[df$wiek > 25, ]
```

#### Funkcje

```{r}
# Definiowanie funkcji
powitaj <- function(imie) {
  paste("Cześć,", imie, "!")
}

# Użycie funkcji
powitaj("Alicja")

# Funkcja z wieloma argumentami
oblicz_bmi <- function(waga, wzrost) {
  bmi <- waga / (wzrost^2)
  return(bmi)
}

oblicz_bmi(70, 1.75)
```

#### Struktury kontrolne

```{r}
# Instrukcja if-else
x <- 10
if (x > 5) {
  print("x jest większe niż 5")
} else {
  print("x nie jest większe niż 5")
}

# Pętla for
for (i in 1:5) {
  print(paste("Iteracja", i))
}

# Pętla while
licznik <- 1
while (licznik <= 5) {
  print(paste("Licznik:", licznik))
  licznik <- licznik + 1
}
```

### Wprowadzenie do tidyverse

Tidyverse to kolekcja pakietów R zaprojektowanych do nauki o danych. Te pakiety mają wspólną filozofię i są zaprojektowane do bezproblemowej współpracy.

#### Kluczowe pakiety tidyverse

-   ggplot2: do wizualizacji danych
-   dplyr: do manipulacji danymi
-   tidyr: do porządkowania danych
-   readr: do odczytu danych prostokątnych
-   purrr: do programowania funkcyjnego
-   tibble: nowoczesne ujęcie ramek danych

#### Rozpoczęcie pracy z tidyverse

```{r}
# Instalacja tidyverse (uruchom raz)
# install.packages("tidyverse")

# Wczytanie tidyverse
library(tidyverse)
```

#### Import danych z readr

```{r}
#| eval: false
# Odczyt plików CSV
dane <- read_csv("dane_spoleczne.csv")

# Odczyt innych formatów plików
read_tsv("dane.tsv")  # Wartości oddzielone tabulatorem
read_delim("dane.txt", delim = "|")  # Niestandardowy separator
```

#### Manipulacja danymi z dplyr

```{r}
# Użyjmy wbudowanego zbioru danych mtcars
data("mtcars")

# Wybieranie kolumn
mtcars %>% 
  select(mpg, cyl, hp)

# Filtrowanie wierszy
mtcars %>% 
  filter(cyl == 4)

# Sortowanie danych
mtcars %>% 
  arrange(desc(mpg))

# Tworzenie nowych zmiennych
mtcars %>% 
  mutate(kpl = mpg * 0.425)

# Podsumowywanie danych
mtcars %>% 
  group_by(cyl) %>% 
  summarize(srednie_mpg = mean(mpg),
            liczba = n())
```

#### Wizualizacja danych z ggplot2

```{r}
#| label: wykres-rozrzutu
#| fig-cap: "Waga samochodu vs. Zużycie paliwa"
# Wykres rozrzutu
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  labs(title = "Waga samochodu vs. Zużycie paliwa",
       x = "Waga (1000 funtów)",
       y = "Mile na galon")
```

```{r}
#| label: wykres-slupkowy
#| fig-cap: "Liczba samochodów według liczby cylindrów"
# Wykres słupkowy
mtcars %>% 
  count(cyl) %>% 
  ggplot(aes(x = factor(cyl), y = n)) +
  geom_bar(stat = "identity") +
  labs(title = "Liczba samochodów według liczby cylindrów",
       x = "Liczba cylindrów",
       y = "Liczba")
```

```{r}
#| label: wykres-pudelkowy
#| fig-cap: "Zużycie paliwa według liczby cylindrów"
# Wykres pudełkowy
ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +
  geom_boxplot() +
  labs(title = "Zużycie paliwa według liczby cylindrów",
       x = "Liczba cylindrów",
       y = "Mile na galon")
```

### Dodatkowe materiały

-   [R for Data Science](https://r4ds.had.co.nz/)
-   [Dokumentacja tidyverse](https://www.tidyverse.org/)
-   [Ściągawki RStudio](https://www.rstudio.com/resources/cheatsheets/)
-   [Przewodnik Quarto](https://quarto.org/docs/guide/)
-   [R Cookbook](http://www.cookbook-r.com/)

Pamiętaj, aby eksperymentować z kodem, modyfikować przykłady i nie wahaj się korzystać z wbudowanego systemu pomocy R (dostępnego przez wpisanie `?nazwa_funkcji` w konsoli), gdy napotkasz nieznane funkcje lub koncepcje.
