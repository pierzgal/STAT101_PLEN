[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Data Analysis: An Introduction (PL: Wprowadzenie do Analizy Danych Społecznych)",
    "section": "",
    "text": "Preface\n\n\n\n\n\n\nImportant\n\n\n\nThis is a preliminary draft of a Quarto class notes on social data analysis. Please do not cite or reproduce its contents, as it may contain errors!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "",
    "text": "1.1 What Is Statistics?\nStatistics provides a principled way to learn from data when outcomes vary and information is incomplete.\nIt covers: (1) description—clear summaries and visualizations; (2) inference—estimating unknown population quantities with uncertainty; and (3) prediction/decision—forecasting and choosing actions under risk.\nIn empirical political science, this includes: (i) estimating the magnitude of the incumbency advantage; (ii) assessing how events, reforms, or campaigns affect turnout; and (iii) translating poll results into probability distributions or intervals for vote shares.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#what-is-statistics",
    "href": "chapter1.html#what-is-statistics",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "",
    "text": "Note\n\n\n\nWhat statistics helps you do - Describe: tidy summaries and graphics\n- Infer: estimate population quantities + uncertainty (SEs, CIs)\n- Predict & decide: forecast outcomes and support decisions under risk",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#randomness-a-foundation-of-statistical-inference",
    "href": "chapter1.html#randomness-a-foundation-of-statistical-inference",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.2 Randomness: a foundation of statistical inference",
    "text": "1.2 Randomness: a foundation of statistical inference\n\nWhat is randomness?\nIn statistics, randomness is an orderly way to describe uncertainty: individual outcomes are unpredictable, yet in long sequences of repetitions stable regularities emerge (e.g., frequencies, means).\nTwo perspectives\n\nSingle realisation — we cannot determine how a specific voter will vote at a given moment.\n\nAggregate — we can describe the share of voters supporting a party and quantify the associated estimation uncertainty.\n\n\n\n\n\n\n\nNote\n\n\n\nEpistemic vs. ontological randomness\n\nEpistemic (due to incomplete knowledge): we treat an outcome as random because not all determinants are observed or conditions are not controlled.\nExamples:\n\nthe decision of an individual respondent in a poll (we do not know the full set of motivations),\nmeasurement error in a survey (limited precision, item nonresponse),\na coin toss modeled as random because minute, unobserved differences in initial conditions determine the outcome.\n\nOntological (intrinsic indeterminacy): even complete knowledge does not remove outcome uncertainty.\nExamples:\n\nthe time to radioactive decay of an atom.\n\n\n\n\n\n\nWhy Randomness Matters\n\nRandom sampling\n\nReduces systematic selection bias so the sample resembles the target population (in expectation).\nMakes uncertainty quantifiable (e.g., margins of error; later we’ll name these “confidence intervals”), assuming genuinely random selection and good coverage.\n\nRandom assignment (experiments)\n\nBreaks the link between treatment and other factors, making groups comparable on average (both observed and unobserved).\nSupports credible cause-and-effect claims (identifies average treatment effects under standard conditions).\n\n\n\n\nThe Power of Random Sampling (quick demo)\nSuppose we take a random sample of n=1000 voters and observe \\hat p = 0.55 (i.e., 55% support). Then:\n\nOur best single-number estimate of the population share is \\hat p = 0.55.\nA typical “95\\% range of plausible values” around \\hat p can be approximated by \n\\hat p \\;\\pm\\; 2\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\n\\;=\\;\n0.55 \\;\\pm\\; 2\\sqrt{\\frac{0.55\\cdot 0.45}{1000}}\n\\approx\n0.55 \\pm 0.031,\n i.e., roughly 52\\%\\text{–}58\\% (about \\pm 3.1 percentage points).\nThe width of this range shrinks predictably with sample size: \n\\text{width} \\;\\propto\\; \\frac{1}{\\sqrt{n}}.\n For example, increasing n from 1000 to 4000 cuts the range by about half.\n\n\n\n\n\n\n\nNote\n\n\n\nHow to read the “95% range”\n\nImagine repeating the same random survey many times. In about 19 out of 20 such surveys, the computed range would include the true population percentage.\nThis rule-of-thumb assumes random sampling from the target population and similar survey conditions.\nNon-sampling issues (nonresponse, coverage, measurement) or complex designs (e.g., clustering) can make the real uncertainty larger.\n\n\n\n\n\nLaw of Large Numbers (LLN) — elementary statement\nLet A denote an event of interest (e.g., “a vote for party X”, “sum of dice equals 7”). If P(A)=p and we observe n independent trials with the same distribution (often called i.i.d.), then the sample frequency of A:\n\n\\hat{p}_n=\\frac{\\text{number of occurrences of }A}{n}\n\nconverges to p as n increases.\nExample (two dice): the event “sum = 7” has probability 6/36 \\approx 16.7\\%, while “sum = 4” has 3/36 \\approx 8.3\\%. Over many throws, a sum of 7 appears about twice as often as a sum of 4.\nExample (election poll): if the population support for a party equals p, then under random sampling of size n the observed frequency \\hat p_n will, as n grows, approach p (assuming random sampling and independence of trials).\n\n# Self-contained demo (no external packages beyond ggplot2)\nset.seed(42)\n\n# Create a synthetic \"population\" with a known proportion\npopulation_support &lt;- c(rep(\"A\", 5200), rep(\"B\", 4800))\ntrue_support_A &lt;- mean(population_support == \"A\")\n\n# Function: take a simple random sample and compute support for A\nsample_support &lt;- function(n) {\n  mean(sample(population_support, n) == \"A\")\n}\n\n# Sample sizes and repeated draws\nsample_sizes &lt;- c(50, 100, 500, 1000)\nresults_list &lt;- lapply(sample_sizes, function(n) {\n  est &lt;- replicate(100, sample_support(n))\n  data.frame(sample_size = factor(n), estimate = est, true_value = true_support_A)\n})\nresults &lt;- do.call(rbind, results_list)\n\nlibrary(ggplot2)\n\nggplot(results, aes(x = sample_size, y = estimate)) +\n  geom_boxplot(alpha = 0.7, fill = \"lightblue\") +\n  geom_hline(yintercept = true_support_A, color = \"red\", linetype = \"dashed\", linewidth = 0.8) +\n  labs(\n    title = \"Random Sampling Converges to the Truth as n Increases\",\n    subtitle = \"Red dashed line = true population value (52%)\",\n    x = \"Sample size\",\n    y = \"Estimated support for A\",\n    caption = \"Each box summarizes 100 random samples\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nReading the figure. Boxes show the middle 50% of estimates; the line is the median. As n grows, the boxes narrow: variability from randomness shrinks, and estimates settle near the true value.\n\n\nBottom line: Randomness underpins statistical inference: it turns uncertainty in individual outcomes into predictable distributions for estimates. The Law of Large Numbers (LLN) guarantees that the “noise” of individual outcomes averages out, allowing us to predict long-run frequencies, quantify error, and draw reliable inferences—in surveys, experiments, and, in the frequentist sense, even in quantum phenomena.\n\n\nRandomness, chaos, entropy, and “haphazardness” (at a glance)\n\n\n\n\n\n\n\n\n\nConcept\nWhat is it?\nSource of unpredictability\nExample\n\n\n\n\nRandomness\nIndividual outcomes are uncertain, but the probability distribution is known or modeled.\nFluctuations across realizations; lack of information about a specific outcome.\nDice roll, coin toss\n\n\nChaos\nDeterministic dynamics highly sensitive to initial conditions (butterfly effect).\nTiny initial differences grow rapidly → large trajectory divergences.\nWeather, double pendulum, logistic map\n\n\nEntropy\nA measure of uncertainty/dispersion (information-theoretic or thermodynamic).\nLarger when outcomes are more even (less predictive information).\nShannon entropy\n\n\n“Haphazardness” (colloquial)\nA felt lack of order without an explicit model; a mixture of mechanisms.\nNo structured description or stable rules; overlapping processes.\nA messy desk\n\n\nQuantum randomness\nA single outcome is not determined; only the distribution is specified (Born rule).\nFundamental (ontological) indeterminacy of individual measurements.\nElectron spin, photon polarization\n\n\n\n\nNote: Deterministic chaos ≠ randomness. A chaotic system is fully deterministic yet practically unpredictable due to extreme sensitivity to initial conditions. Randomness, by contrast, models uncertainty via probability distributions.\n\n\n\nAnd quantum mechanics?\nIn the Copenhagen view, randomness is fundamental (ontological): a single outcome cannot be predicted, but the probability distribution is given by the Born rule, \nP(\\text{outcome}) \\propto \\lvert \\psi \\rvert^{2}.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#inferential-statistics-an-introductory-example",
    "href": "chapter1.html#inferential-statistics-an-introductory-example",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.3 Inferential Statistics: An Introductory Example",
    "text": "1.3 Inferential Statistics: An Introductory Example\n\n\n\n\n\n\nNote\n\n\n\nFundamental Principle: Statistics does not eliminate uncertainty—it helps us measure, manage, and communicate it effectively.\n\n\n\n\n\n\n\n\nHistorical Example: The 1936 Literary Digest Poll\n\n\n\nThe Literary Digest conducted one of the largest polls in history with 2.4 million responses, predicting Alf Landon would defeat Franklin D. Roosevelt in the 1936 presidential election. Despite the massive sample size:\n\nPrediction: Landon 57%, Roosevelt 43%\nActual Result: Roosevelt 62%, Landon 38%\n\nWhat went wrong? The poll suffered from selection bias:\n\nSampling frame: telephone directories, automobile registrations, club memberships\nIn 1936, these sources overrepresented wealthy Americans who favored Landon\nNon-response bias: only 24% responded, likely those with strong anti-Roosevelt views\n\nKey Lesson: A large biased sample is worse than a small representative sample. Standard errors only measure random error, not bias.\n\n\n\nResearch Question: What proportion of students support keeping the library open 24/7?\nThe Challenge: - Population: 20,000 students at the university - Practical constraint: Can only survey 100 students - Problem: Different samples will yield different results\nWithout Statistical Thinking: “60 out of 100 students said yes, therefore 60% support it.”\nWith Statistical Thinking: “We estimate 60% support with a margin of error of ±10%. We can be reasonably confident the true support lies between 50% and 70%.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#core-concepts",
    "href": "chapter1.html#core-concepts",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.4 Core Concepts",
    "text": "1.4 Core Concepts\n\n1. The Point Estimate\nWhen 60 out of 100 surveyed students support a proposal:\n\\hat{p} = \\frac{60}{100} = 0.60\nThis point estimate is your best single guess of the population value based on the sample you happened to draw.\n\n\n\n\n\n\nSnapshot glossary (no math needed)\n\n\n\n\nPoint estimate: the single number from your sample (e.g., 60%).\nStandard error (SE): the typical wiggle you’d see in that estimate if you redid the same study many times.\nMargin of error (MoE): the buffer you add around the estimate to make an interval that accounts for ordinary sampling wiggle.\nConfidence interval (CI): the estimate ± buffer (e.g., 60% ± 3%)—a range that tends to capture the truth if we repeat the same method.\n\n\n\n\n\n2. Quantifying Uncertainty\nThe precision of your estimate depends mainly on:\n\nSample size Bigger samples → less wiggle. Rule of thumb: MoE ≈ 1/√n (for yes/no outcomes near 50% with simple random sampling).\nHow mixed opinions are Estimates around 50–50 are hardest to pin down (largest MoE). When nearly everyone agrees (e.g., 90–10), samples are naturally steadier.\nSurvey design & weighting Stratification, clustering, and heavy weighting usually increase uncertainty. Think of it as shrinking your effective sample size.\n\n\n\n\n\n\n\nWhat “margin of error” really means\n\n\n\nIt’s the plus–minus amount due only to using a sample rather than a census.\nIf a poll reports 52% with ±3% (95% confidence), read it as:\n\nWith ordinary sampling variation, the true support is probably between 49% and 55%.\n\nIt does not cover biased questions, a poor sampling frame, nonresponse problems, or measurement errors. Those are beyond the margin of error.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-relationship-between-sample-size-and-precision",
    "href": "chapter1.html#the-relationship-between-sample-size-and-precision",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.5 The Relationship Between Sample Size and Precision",
    "text": "1.5 The Relationship Between Sample Size and Precision\n\n\n\n\n\n\nStandard Margins of Error (back-of-the-envelope)\n\n\n\n\n\n\nSample Size\nApprox. MoE (95% for a 50/50 proportion)\n\n\n\n\nn = 100\n± 10%\n\n\nn = 400\n± 5%\n\n\nn = 1,000\n± 3%\n\n\nn = 2,500\n± 2%\n\n\nn = 10,000\n± 1%\n\n\n\nKey pattern: To cut MoE in half, you need 4× the sample. Diminishing returns are real.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThese quick MoE numbers assume a simple random sample and light/no weighting. Complex designs or heavy weights make the effective n smaller → MoE larger.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#three-illustrative-examples",
    "href": "chapter1.html#three-illustrative-examples",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.6 Three Illustrative Examples",
    "text": "1.6 Three Illustrative Examples\n\nExample 1: Small Sample (n = 25)\n\nResult: 15 out of 25 support → 60%\nApprox. MoE: ± 20%\nInterpretation: The truth could easily be 40% to 80%. Useful as an early signal, not for fine decisions.\n\n\n\nExample 2: Moderate Sample (n = 100)\n\nResult: 60 out of 100 support → 60%\nApprox. MoE: ± 10%\nInterpretation: Likely 50% to 70%. Good for a broad picture, still too wide for close races.\n\n\n\nExample 3: Large Sample (n = 1,000)\n\nResult: 600 out of 1,000 support → 60%\nApprox. MoE: ± 3%\nInterpretation: Likely 57% to 63%. Narrow enough for many practical decisions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#understanding-confidence-levels",
    "href": "chapter1.html#understanding-confidence-levels",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.7 Understanding Confidence Levels",
    "text": "1.7 Understanding Confidence Levels\n\nWhat “95% confidence” means (intuitively)\nImagine repeating the same study 100 times, each with a fresh random sample but the same questions and methods:\n\nEach time, your estimate bounces a bit because of sampling. You add the usual buffer (MoE) to make an interval: CI = estimate ± MoE.\nAbout 95 out of 100 such intervals would cover the fixed, unknown population truth; about 5 would miss just by chance.\nFor your single study, the interval you report (e.g., 50% ± 3% → 47% to 53%) either covers the truth or it doesn’t—there’s no “50/50” in this one interval. The 95% refers to the long-run reliability of the method, not a probability for this specific interval.\n\n\n\n\n\n\n\nImportant\n\n\n\nChoosing wider confidence If you want more confidence (e.g., 99%), you must use a bigger buffer (wider interval). More certainty → less precision.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#practical-dodont",
    "href": "chapter1.html#practical-dodont",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.8 Practical Do/Don’t",
    "text": "1.8 Practical Do/Don’t\n\nDo report the estimate and its uncertainty (CI or MoE).\nDo mention notable design features (e.g., clustering, heavy weights) that inflate uncertainty.\nDon’t treat MoE as covering bias (bad frame, nonresponse, leading questions). It only captures sampling wiggle.\nDon’t oversell tiny differences that are smaller than the MoE.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#optional-for-reference",
    "href": "chapter1.html#optional-for-reference",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.9 Optional “for reference”",
    "text": "1.9 Optional “for reference”\n\nLight-touch formulas (optional)\n\nRule of thumb: MoE (95% for proportions near 50–50) ≈ 1/√n.\nSE connects estimate to MoE: roughly, MoE ≈ 2 × SE (at 95%).\nTypical SEs (for reference):\n\nProportion: SE(\\hat p) = \\sqrt{\\hat p(1-\\hat p)/n}\nMean: SE(\\bar x) = s/\\sqrt{n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#when-moe-doesnt-really-apply",
    "href": "chapter1.html#when-moe-doesnt-really-apply",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.10 When MoE Doesn’t Really Apply",
    "text": "1.10 When MoE Doesn’t Really Apply\n\nConvenience or opt-in samples (no true random selection)\nUnknown or very complex weighting that’s not accounted for\nSevere nonresponse that distorts who answered\n\nYou can still compute a number, but calling it a classical MoE can be misleading unless you justify how the design approximates random sampling or you adjust with a defensible effective sample size.\n\n\nOne-liner for students\nMargin of error is how far a sample result might be from the population truth just because we sampled. SE is the underlying “typical wiggle.” A 95% CI is estimate ± buffer, built by a method that would capture the truth most of the time if we repeated the same study.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#visualizing-sampling-variability",
    "href": "chapter1.html#visualizing-sampling-variability",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.11 Visualizing Sampling Variability",
    "text": "1.11 Visualizing Sampling Variability\n\nlibrary(ggplot2)\nset.seed(42)\n\n# Parameters\nn_polls      &lt;- 20\nn_people     &lt;- 100\ntrue_support &lt;- 0.50\n\n# Simulate independent polls (binomial counts -&gt; proportions)\nsupport &lt;- rbinom(n_polls, n_people, true_support) / n_people\n\n# Per-poll standard error for a proportion (plug-in using that poll's estimate)\nse   &lt;- sqrt(support * (1 - support) / n_people)\n\n# \"95%\" margin of error ≈ 2 × SE (plain-English multiplier, no distribution jargon)\nmoe  &lt;- 2 * se\n\n# Clamp intervals to [0, 1] to avoid plotting outside the parameter space\nlower &lt;- pmax(0, support - moe)\nupper &lt;- pmin(1, support + moe)\n\n# Does the interval cover the true value?\ncovers &lt;- (lower &lt;= true_support) & (upper &gt;= true_support)\nn_cover &lt;- sum(covers)\nn_miss  &lt;- n_polls - n_cover\n\nresults &lt;- data.frame(\n  poll = seq_len(n_polls),\n  support, se, moe, lower, upper, covers\n)\n\n# Plot\nggplot(results, aes(x = poll, y = support, color = covers)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.3, alpha = 0.8) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = true_support, linetype = \"dashed\") +\n  scale_color_manual(\n    values = c(\"TRUE\" = \"forestgreen\", \"FALSE\" = \"darkorange\"),\n    labels = c(\"TRUE\" = \"Covers truth\", \"FALSE\" = \"Misses truth\"),\n    name   = NULL\n  ) +\n  coord_cartesian(ylim = c(0, 1)) +\n  labs(\n    title    = \"Sampling Variability in 20 Independent Polls\",\n    subtitle = paste0(\n      \"Each poll surveys \", n_people, \" different people.  Truth = \",\n      scales::percent(true_support),\n      \". Intervals covering truth: \", n_cover, \"/\", n_polls,\n      \" (\", round(100 * n_cover / n_polls), \"%).\"\n    ),\n    x = \"Poll Number\",\n    y = \"Estimated Proportion\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nKey observation: Each sample yields a different result, but most estimates—and their intervals—cluster around the true value; a few “miss” purely due to the randomness of sampling.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#statistical-errors-a-simple-guide",
    "href": "chapter1.html#statistical-errors-a-simple-guide",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.12 Statistical Errors: A Simple Guide",
    "text": "1.12 Statistical Errors: A Simple Guide\n\nWhy this matters\nKnowing where error comes from helps you:\n\nDesign better studies and measurements\nInterpret estimates correctly\nReport honest limitations",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#two-big-families-of-error",
    "href": "chapter1.html#two-big-families-of-error",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.13 Two Big Families of Error",
    "text": "1.13 Two Big Families of Error\n\n1) Random Error (Sampling/Estimation Variability)\nUnpredictable ups and downs caused by chance (e.g., which people were sampled, day-to-day noise).\n\nQuantifiable by statistical theory (e.g., standard error SE, confidence interval CI, margin of error MoE)\nDecreases with larger sample size n\nAddress by increasing n, using efficient estimators, and sound designs\n\n\n\n2) Systematic Error (Bias)\nA consistent shift away from the truth due to design or measurement problems.\n\nNot fixed by larger n\nOften hard to quantify with simple formulas\nAddress by improving design, measurement, and data collection\n\n\n\n\n\n\n\nWarning\n\n\n\nKey idea: A large biased sample gives a precisely wrong answer. Increase n to reduce random error; improve design/measurement to reduce bias.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#biasvariance-mse-decomposition",
    "href": "chapter1.html#biasvariance-mse-decomposition",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.14 Bias–Variance (MSE) Decomposition",
    "text": "1.14 Bias–Variance (MSE) Decomposition\nFor an estimator \\hat\\theta:\n\n\\mathrm{MSE}(\\hat\\theta) \\;=\\; \\underbrace{\\mathrm{Var}(\\hat\\theta)}_{\\text{random error}} \\;+\\; \\underbrace{\\big(\\mathrm{Bias}(\\hat\\theta)\\big)^2}_{\\text{systematic error}}.\n\n\nVariance: How much the estimate would bounce around if you repeated the study many times (random error).\nBias: How far the average estimate is from the truth (systematic error).\nGoal: Keep both small. More data lowers variance; better design lowers bias.\n\n\n\n\n\n\n\nTip\n\n\n\nFor prediction, a tiny amount of bias can sometimes reduce variance enough to lower overall MSE (mean squared error). For causal questions, uncontrolled bias is usually unacceptable.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#common-sources-of-bias-across-many-study-types",
    "href": "chapter1.html#common-sources-of-bias-across-many-study-types",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.15 Common Sources of Bias (Across Many Study Types)",
    "text": "1.15 Common Sources of Bias (Across Many Study Types)\n\nSelection Bias Sample/data do not represent the target group. Mitigation: Define the target population clearly; use probability sampling where possible; use credible reweighting.\nNonresponse / Attrition Bias Some types of participants are more likely to be missing or to drop out. Mitigation: Reduce burden (shorter instruments), send reminders, offer small incentives; report who is missing and why.\nMeasurement Bias Systematic distortion in how variables are measured (miscalibrated device, leading wording, consistent misclassification). Mitigation: Calibrate instruments, pilot and neutralize questions, use validated scales, blind assessors where possible.\nDesign / Causal Bias Confounding or bad conditioning (e.g., controlling for a mediator). Mitigation: Randomize when feasible; pre-specify plans; use design tools and careful variable selection.\nModel / Specification Bias Wrong functional form or missing key interactions; extrapolating beyond the data. Mitigation: Inspect relationships; try reasonable alternatives; check predictions on new data.\nOverfitting and Data Leakage Great in-sample fit that fails on new data; accidental sharing of information between training and testing. Mitigation: Keep a true test set; use cross-validation; lock down preprocessing steps.\nProcessing / Pipeline Errors Coding mistakes, merge issues, unit conversion errors. Mitigation: Reproducible scripts, checks and audits, version control.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#interpreting-precision-without-extra-formulas",
    "href": "chapter1.html#interpreting-precision-without-extra-formulas",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.16 Interpreting Precision (without extra formulas)",
    "text": "1.16 Interpreting Precision (without extra formulas)\n\nStandard Error (SE): Average estimation noise due to sampling. Smaller SE means more precise estimates.\nConfidence Interval (CI): A range that aims to capture the true value with a stated confidence level (e.g., 95%).\nMargin of Error (MoE): A common shorthand for how wide the CI is in simple settings.\n\n\n\n\n\n\n\nNote\n\n\n\nIncreasing n narrows SE, CI, and MoE. It does not remove bias.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#hypothesis-tests-two-kinds-of-mistakes",
    "href": "chapter1.html#hypothesis-tests-two-kinds-of-mistakes",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.17 Hypothesis Tests: Two Kinds of Mistakes",
    "text": "1.17 Hypothesis Tests: Two Kinds of Mistakes\n\nType I Error (False Positive): Concluding there is an effect when there is none. The preset risk of this is the significance level (often 5%).\nType II Error (False Negative): Missing a real effect. Power is the chance to detect a real effect (higher power is better).\n\nMultiple comparisons: Testing many hypotheses inflates false positives. Consider controlling the false discovery rate (FDR)—the expected proportion of false “discoveries” among all claimed findings.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#quick-practices-that-help",
    "href": "chapter1.html#quick-practices-that-help",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.18 Quick Practices That Help",
    "text": "1.18 Quick Practices That Help\n\nBefore collecting data: define the target population, outcomes, and main comparisons; pilot your measurements.\nDuring data collection: minimize burden, keep wording neutral, record response/attrition patterns.\nDuring analysis: check simple diagnostics, try reasonable alternative specifications, guard against overfitting/leakage.\nWhen reporting: describe data origins, missingness, uncertainty (SE/CI/MoE), and the likely direction of any remaining bias.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#one-minute-checklist",
    "href": "chapter1.html#one-minute-checklist",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.19 One-Minute Checklist",
    "text": "1.19 One-Minute Checklist\n\nBias: Any likely selection, measurement, or design issue?\nVariance: Is the sample size reasonable for the question?\nModel: Could a simpler or alternative model change conclusions?\nValidation: Does it work on new or held-out data?\nTransparency: Did you state assumptions, limitations, and likely bias direction?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#glossary-acronyms-spelled-out",
    "href": "chapter1.html#glossary-acronyms-spelled-out",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.20 Glossary (acronyms spelled out)",
    "text": "1.20 Glossary (acronyms spelled out)\n\nSE — Standard Error: Average sampling noise in an estimator.\nCI — Confidence Interval: Interval aiming to include the true value with a chosen confidence level.\nMoE — Margin of Error: A simple width measure for uncertainty in some settings.\nMSE — Mean Squared Error: Variance + Bias² (overall estimation error).\nFDR — False Discovery Rate: Expected share of false positives among all claimed findings.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nCritical Point: A large biased sample provides a precisely wrong answer. A small unbiased sample is preferable to a large biased one.\nA narrow interval around a biased estimator is precisely wrong. Address bias with better design; address random error with larger n.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#interpreting-published-poll-results",
    "href": "chapter1.html#interpreting-published-poll-results",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.21 Interpreting Published Poll Results",
    "text": "1.21 Interpreting Published Poll Results\n\nReading a Poll Report\nWhen encountering: “52% of registered voters support the initiative (margin of error ± 3%, n = 1,000)”\nExtract the following information: 1. Point estimate: 52% 2. Confidence interval: 49% to 55% 3. Sample size: 1,000 (indicates moderate precision)\nCritical questions to ask: - How was the sample selected? - What was the response rate? - How were questions worded? - When was the survey conducted?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#summary",
    "href": "chapter1.html#summary",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.22 Summary",
    "text": "1.22 Summary\n\nKey Concepts\nEstimation from Samples - Point estimate: Our best single estimate from the sample - Margin of error: Quantifies uncertainty due to sampling\nSample Size and Precision - Uncertainty approximately proportional to 1/√n - Quadrupling sample size halves the margin of error\nRandom Error vs. Systematic Bias - Random error: Quantifiable, decreases with n - Systematic bias: Not easily quantified, persists regardless of n\nTransparent Reporting - Always report: estimate, uncertainty, sample size, and methodology - Acknowledge limitations and potential sources of bias",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#mathematical-foundation",
    "href": "chapter1.html#mathematical-foundation",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.23 Mathematical Foundation",
    "text": "1.23 Mathematical Foundation\n\n\n\n\n\n\nTip\n\n\n\nTechnical Details\nThe margin of error for a proportion derives from probability theory:\n\\text{Margin of Error} = z_{\\alpha/2} \\times \\sqrt{\\frac{p(1-p)}{n}}\nWhere:\n\np = true population proportion\nn = sample size\nz_{\\alpha/2} = critical value (1.96 for 95% confidence)\n\nSince p(1-p) achieves its maximum of 0.25 when p = 0.5:\n\\text{Maximum Margin of Error} = 1.96 \\times \\sqrt{\\frac{0.25}{n}} \\approx \\frac{1}{\\sqrt{n}}\nThis yields the approximation used throughout these notes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#quick-reference",
    "href": "chapter1.html#quick-reference",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.24 Quick Reference",
    "text": "1.24 Quick Reference\n\nEstimating Proportions: Essential Formulas\nPoint Estimate: \\hat{p} = \\frac{\\text{number of successes}}{\\text{sample size}}\nApproximate Margin of Error: \\text{MoE} \\approx \\frac{1}{\\sqrt{n}}\nInterpretation Template:\n“We estimate [X%] with a margin of error of [±Y%] based on a sample of [n] observations.”\nEssential Caveats:\n\nAssumes random sampling\nBias cannot be reduced by increasing sample size\nAlways report methodology and limitations",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-statistical-mindset",
    "href": "chapter1.html#the-statistical-mindset",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.25 The Statistical Mindset",
    "text": "1.25 The Statistical Mindset\n\n1) Acknowledge uncertainty\nMost population quantities are not known exactly. Treat every numerical result as an estimate accompanied by uncertainty.\n\nExample: “Estimated support is 52% (95% CI: 49%–55%), based on a random sample of 1,200 adults.”\n\n\n\n2) Reason about variation\nDifferences across people, places, or time enable learning. Distinguish random variation (sampling variability) from systematic variation (institutions, demographics, incentives).\n\nExample: Turnout differences can reflect weather noise (random) and ballot complexity (systematic).\n\n\n\n3) Compare fairly (association vs. causation)\nTwo variables can move together (association) without one producing the other (causation). Causal claims require a fair comparison: what would have happened for the same units under a different condition, holding other factors constant (the counterfactual).\n\nExample: Municipalities with more campaign visits often show higher turnout. Competitiveness may raise both visits and turnout. A valid design (randomization, natural experiment, regression discontinuity, etc.) is needed to argue that visits cause higher turnout.\n\n\n\n\n\n\n\nCaution\n\n\n\nBefore making a causal claim\n\nIs the comparison well defined and credible?\nCould a third factor influence both variables (a confounder), and how is it addressed (by design or analysis)?\nAre assumptions stated clearly and checked where possible?\n\n\n\n\n\n4) Check reliability\nApparent regularities can arise from sampling variability or modeling choices. Use standard errors, interval estimates, and replication to judge stability. Prefer effect sizes with intervals over binary “significant/not significant” labels.\n\nExample: “Turnout increased by 2.3 percentage points (95% CI: 0.8 to 3.8).”\n\n\n\n5) Consider alternative explanations\nFor each claim, list plausible alternatives and test them (robustness checks, placebo tests, falsification exercises).\n\nExample: If study-group participation correlates with higher grades, evaluate selection: students with higher prior achievement or motivation may self-select into groups.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#a-simple-workflow-of-statistical-analysis",
    "href": "chapter1.html#a-simple-workflow-of-statistical-analysis",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.26 A Simple Workflow of Statistical Analysis",
    "text": "1.26 A Simple Workflow of Statistical Analysis\n\nResearch question – what do you want to estimate (estimand)?\nStudy design – how will you obtain a credible comparison?\nData collection – procedures, sampling, measurement quality.\nExploratory Data Analysis (EDA) – initial patterns, cleaning, visualizations.\nModeling – description, prediction, or causal inference.\nDiagnostics – assumption checks, robustness checks, placebo tests.\nConclusions and communication – effect with a confidence/credible interval, limitations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#essential-vocabulary",
    "href": "chapter1.html#essential-vocabulary",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.27 Essential Vocabulary",
    "text": "1.27 Essential Vocabulary\n\nPopulation: the full set you want to learn about (e.g., all eligible voters).\nSample: the part you actually observe (e.g., 1,200 surveyed voters).\nParameter: a fixed but unknown population quantity (e.g., true support).\nStatistic: a number computed from the sample (e.g., sample mean).\nEstimate: your best guess of a parameter using data (e.g., \\hat{p}).\nStandard error (SE): the estimated variability of an estimate across repeated samples.\nConfidence interval (CI): a range that, under repeated sampling, would contain the parameter a specified proportion of the time (e.g., 95%).\nCausal effect: the change in an outcome if the input were changed for the same unit.\nConfounder: a variable that affects both the input and the outcome, creating a misleading association.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#population-sample-and-superpopulation-dgp-foundations-of-inference",
    "href": "chapter1.html#population-sample-and-superpopulation-dgp-foundations-of-inference",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.28 Population, Sample, and Superpopulation (DGP): Foundations of Inference",
    "text": "1.28 Population, Sample, and Superpopulation (DGP): Foundations of Inference\nIn political science, we’re often interested in understanding entire populations—the complete set of units we want to study. However, studying entire populations is usually impossible, impractical, or unnecessary. Statistics lets us learn about populations using samples.\n\nWhat Counts as a Population?\nA population in political science can consist of various types of units:\nIndividuals\n\nPopulation: All 240 million American adults\nSample: 1,000 randomly selected adults in a survey\nResearch question: What percentage support universal healthcare?\n\nCountries\n\nPopulation: All 195 sovereign nations in the world\nSample: 50 countries from different regions and development levels\nResearch question: Does democracy correlate with economic growth?\n\nSubnational Units\n\nPopulation: All 3,143 U.S. counties\nSample: 200 randomly selected counties\nResearch question: How does unemployment affect crime rates?\n\nOrganizations\n\nPopulation: All NGOs registered with the United Nations\nSample: 100 NGOs working in different policy areas\nResearch question: What factors predict NGO effectiveness?\n\nEvents or Time Periods\n\nPopulation: All elections held in Europe since 1945\nSample: 300 elections from different countries and decades\nResearch question: How do economic conditions affect incumbent vote share?\n\nLegislative Units\n\nPopulation: All bills introduced in Congress from 2000–2020\nSample: 500 randomly selected bills\nResearch question: What predicts whether a bill becomes law?\n\n\n\nFrom Sample to Population (Inference)\nA sample is a subset of the population we actually observe and measure. The key insight of statistics is that we can learn about populations by studying samples—if we’re careful about how we choose them.\nFrom our sample, we want to make inferences about the population:\n\\text{Sample Statistic} \\;\\to\\; \\text{Population Parameter}\nFor example, if 52% of our sample supports Candidate A (\\hat{p} = 0.52), what can we say about support in the entire population (\\pi)?\nThe fundamental principle: random selection gives every unit in the population an equal chance of being included, preventing systematic bias.\n\n\n\n\n\n\nNote\n\n\n\nMini-glossary (plain language)\n\nParameter: a true population quantity (e.g., mean \\mu).\nStatistic: a number computed from a sample (e.g., sample mean \\bar{x}).\nEstimator: the rule for computing a statistic (e.g., “take the average”).\n\n\n\n\n\nVisualizing Sampling\nLet’s see how different sample sizes affect our estimates:\n\nset.seed(42)\n\n# Population and experiment parameters\npopulation_size &lt;- 1000000\ntrue_proportion &lt;- 0.60   # True population parameter (π)\nsample_sizes    &lt;- c(100, 500, 1000, 5000)\nn_trials        &lt;- 20     # independent samples per n\n\n# Simulation (vectorized)\ngrid &lt;- expand.grid(size = sample_sizes, trial = seq_len(n_trials))\ngrid$estimate &lt;- rbinom(nrow(grid), size = grid$size, prob = true_proportion) / grid$size\n\n# Expected \"uncertainty band\" ± 2 × SE for each n (using the true π in this simulation)\nse_exp &lt;- sqrt(true_proportion * (1 - true_proportion) / sample_sizes)\nribbons &lt;- data.frame(\n  size  = sample_sizes,\n  lower = pmax(0, true_proportion - 2 * se_exp),\n  upper = pmin(1, true_proportion + 2 * se_exp)\n)\n\n# Plot\nggplot(grid, aes(x = factor(size), y = estimate)) +\n  # Gray band: approximate ± 2 × SE for each sample size\n  geom_crossbar(\n    data = transform(ribbons, x = factor(size)),\n    aes(x = x, y = true_proportion, ymin = lower, ymax = upper),\n    inherit.aes = FALSE,\n    fill = \"grey85\", alpha = 0.7, width = 0.6, color = NA\n  ) +\n  # Points: independent estimates from simulated samples\n  geom_point(position = position_jitter(width = 0.12, height = 0),\n             alpha = 0.75, size = 2.2, color = \"steelblue\") +\n  # Red dashed line: true population value\n  geom_hline(yintercept = true_proportion, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title    = \"How Sample Size Affects the Spread of Estimates\",\n    subtitle = \"Gray bands show an approximate ± 2 × SE around 60% for each n; larger n → narrower bands\",\n    x        = \"Sample size (n)\",\n    y        = \"Sample estimate\",\n    caption  = paste0(\"Replicates per sample size: \", n_trials,\n                      \"; π = \", scales::percent(true_proportion))\n  ) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  coord_cartesian(ylim = c(0.45, 0.75)) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\nNote on the chart: Each blue dot is one estimate from an independent sample. For each column, the gray band shows an approximate ± 2 × SE (about the half-width of a typical interval). As n grows, the band narrows and the estimates cluster more tightly around the red dashed line (the true value).\n\n\nKey takeaway: Larger n reduces random error (narrower range of typical fluctuations). With n=100 the spread can be wide (e.g., ~55–65%), while with n=5000 estimates are much closer to the truth (e.g., ~59–61%). That’s why nationwide polls often target n \\approx 1{,}000+ rather than n=100.\n\n\n\nThe Representation Problem\nNot all samples are created equal. Consider these sampling methods:\n\nConvenience Sample: Surveying students in your political science class\n\nProblem: Not representative of all voters\nExample: College students skew younger and more liberal than the general population\n\nVoluntary Response Sample: Online poll on a news website\n\nProblem: Self-selection bias\nExample: People with strong opinions are more likely to participate\n\nRandom Sample: Each unit has equal probability of selection\n\nSolution: Best chance of representative sample\nExample: Randomly selected phone numbers from all area codes\n\nStratified Random Sample: Divide population into groups, sample from each\n\nAdvantage: Ensures representation of key subgroups\nExample: Sample equal numbers from each state for national survey\n\nCluster Sample: Randomly select groups, then survey everyone within\n\nAdvantage: Cost-effective for geographically dispersed populations\nExample: Randomly select 50 cities, then survey residents within those cities\n\n\n\n\n\nBasic Definitions\n\nPopulation\nA population is the full set of units we study. A unit can be a person, household, firm, organization, municipality, county/region, state/country, or an event (e.g., a vote, a law, a transaction).\nExamples\n\nAll municipalities in Poland (e.g., in 2024)\nAll countries in the world in 2024\nAll registered voters in Canada\nAll transactions on the NYSE in 2024\nAll laws passed in a given period\n\nKey idea: At a given moment, the population is finite and fixed. It has true quantities we would like to know (e.g., the mean \\mu and standard deviation \\sigma), even if we do not know them yet.\n\n\nSample\nA sample is the subset of the population that we actually observe.\nExamples\n\n1,000 randomly selected voters\n200 measured trees\n10,000 analyzed transactions\n300 surveyed students\n250 municipalities randomly drawn from all municipalities\n\nWhy samples create uncertainty\n\nIf we draw a different sample (different people/municipalities/events), results change a bit.\nTherefore, statistics (like \\bar{x}) differ from sample to sample.\nWe use these differences between samples to report how confident we can be about the population.\n\nPOPULATION (exists, but not fully known)\n    ↓\n[Sampling / data collection]\n    ↓\nSAMPLE (what we see)\n    ↓\n[Inference that accounts for uncertainty]\n    ↓\nESTIMATES about the population\n\n\n\n\n\n\nTip\n\n\n\nThought experiment. Run the same survey 1,000 times with new random participants each time. You get 1,000 slightly different means. Their spread is the typical difference between samples.\n\n\nA short simulation for intuition:\n\nset.seed(1)\n\n# Suppose the true share supporting a policy in the population is p = 0.60.\np_true &lt;- 0.60\nn &lt;- 1000   # sample size per survey\nR &lt;- 2000   # number of repeated surveys\n\n# Repeat the survey R times:\np_hats &lt;- rbinom(R, size = n, prob = p_true) / n\n\n# Typical difference between samples (later: standard error):\nsd(p_hats)\n\n[1] 0.01579959\n\n# An approximate central range across many repeats (~95%):\nquantile(p_hats, c(0.025, 0.975))\n\n 2.5% 97.5% \n0.568 0.631 \n\n\n\nTakeaway: Even if conditions do not change, repeated samples give slightly different results. These differences between samples are the uncertainty we should report.\n\n\n\n\n\nWhen We Observe Everyone (Census, Full Administrative Data)\nSometimes we observe the entire population in a given year:\n\nA national census\nAll stock trades in 2024\nAll hospital admissions in 2023\nAll municipalities in Poland with their features in 2024\n\nQuestion: If we computed the true mean for 2024, why talk about uncertainty?\nAnswer: We usually care about the process, not only one year.\n\nAnother year may differ. 2024 is just one possible configuration; 2025 may look different.\nMeasurement is imperfect. Even full data can have missing values or errors.\nWe want generalization. We ask “what typically happens?” or “what if conditions change?”\n\nSo even with “all of 2024,” intervals can express uncertainty about the process that produces future data.\n\n\nSuperpopulation (Data Generating Process)\nThe superpopulation or Data Generating Process (DGP) is a conceptual source of data: an ongoing mechanism that could have produced slightly different outcomes—and will produce new data in future years.\nInstead of only:\nPopulation → Sample\nwe often think:\nSUPERPOPULATION (process)\n    ↓\n[Data-generating mechanism]\n    ↓\nOBSERVED POPULATION (specific year / conditions)\n    ↓\nINSIGHTS about the process (generalization, prediction, explanation)\n\nExamples\nAnnual sales\n\nWhat we observe: all 50,000 transactions in 2024\nProcess: under slightly different demand/prices/promotions, results would differ\nWhy: understand the process to say something about 2025\n\nElections\n\nWhat we observe: turnout for all municipalities in 2024\nProcess: weather, campaigns, issues, timing affect turnout\nWhy: learn drivers of turnout in general, not just in one year\n\nStudent grades\n\nWhat we observe: all grades this semester\nProcess: syllabus, assignments, cohort, instructor—small changes shift outcomes\nWhy: assess whether a new method usually works better\n\n\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\nConcept\nWhat it includes\nExample\nWhere does uncertainty come from (in plain terms)\n\n\n\n\nSample\nPart of the population\n1,000 surveyed voters / 250 sampled municipalities\nWe sample different units → results differ a bit.\n\n\nPopulation\nWhole set in a given year\nAll voters / all municipalities (2024)\nDifferent years/conditions can change results; measurement limits.\n\n\nSuperpopulation (DGP)\nThe process that produces data\nVoter-behavior mechanism / sales mechanism\nOur description of the process may be simplified; omitted factors may matter.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#parameters-statistics-and-estimates",
    "href": "chapter1.html#parameters-statistics-and-estimates",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.29 Parameters, Statistics, and Estimates",
    "text": "1.29 Parameters, Statistics, and Estimates\n\nParameters vs. Statistics\nA fundamental distinction in statistics is between parameters and statistics:\nPopulation Parameters\n\nNumerical characteristics of the entire population\nUsually unknown and what we want to learn about\nDenoted by Greek letters: \\mu (mu) for mean, \\sigma (sigma) for standard deviation, \\pi (pi) for proportion\nExamples: The true percentage of all Americans who support universal healthcare\n\nSample Statistics\n\nNumerical characteristics calculated from sample data\nWhat we actually observe and calculate\nDenoted by Roman letters: \\bar{x} for sample mean, s for sample standard deviation, \\hat{p} for sample proportion\nExamples: The percentage of 1,000 survey respondents who support universal healthcare\n\nNotation Convention\nThroughout this text, we’ll consistently use: - Population parameters: \\mu (mean), \\sigma (standard deviation), \\pi (proportion)\n- Sample statistics: \\bar{x} (mean), s (standard deviation), \\hat{p} (proportion)\nThis notation helps us always distinguish between what we observe (statistics) and what we want to know (parameters).\n\n\nThe Inference Process: From Statistics to Parameters\nThe core of statistical inference involves using sample statistics to make educated guesses about population parameters:\n\\text{Sample Statistic} \\xrightarrow{\\text{Statistical Inference}} \\text{Population Parameter}\nExample: If 52% of our sample (\\hat{p} = 0.52) supports a candidate, we use this statistic to estimate the population parameter (\\pi) representing true support among all voters.\n\n\nEstimates and Estimators\nAn estimator is the method or formula used to approximate a parameter. An estimate is the specific numerical result from applying that estimator to a particular sample.\n\nEstimator: The sample mean \\bar{x} = \\frac{\\sum x_i}{n}\nEstimate: \\bar{x} = 6.3 years of education (the actual number from our data)\n\n\n\nThe Soup Analogy: Understanding Statistical Inference\nImagine you’re a chef making a large pot of soup for 100 people. You want to know if the soup has the right amount of salt, but you can’t taste all of it. Instead, you take a small spoonful to taste.\nThe Population: The entire pot of soup (100 servings)\nThe Sample: Your spoonful\nThe Parameter: The true saltiness of the entire pot (unknown)\nThe Statistic: The saltiness of your spoonful (what you can measure)\nStatistical Inference: Using the spoonful’s saltiness to draw conclusions about the entire pot\nKey Insights from the Soup Analogy:\n\nRandom sampling matters: You must stir the soup first and take your spoonful from a random location. If you always sample from the top, you might miss that the salt settled to the bottom.\nSample size affects precision: A bigger spoonful gives you a better sense of the overall saltiness than a tiny sip.\nUncertainty is inherent: Even with good sampling, your spoonful might not perfectly represent the whole pot. There’s always some uncertainty.\nSystematic bias ruins everything: If someone secretly added extra salt to just your spoonful, your inference about the whole pot would be wrong. This represents sampling bias.\nInference has limits: You can estimate the average saltiness, but your spoonful can’t tell you if some portions are saltier than others (variability within the population).\n\nThis analogy captures the essence of statistical thinking: we use small, carefully selected samples to learn about much larger populations, always acknowledging the uncertainty inherent in this process.\n\n\nA Real-World Example: What Predicts Electoral Success?\nLet’s start with a question that gets to the heart of political science: What makes politicians win elections?\nImagine you’re a campaign manager trying to understand why some incumbents win by landslides while others barely scrape by. You have data on 200 recent congressional elections, including each incumbent’s approval rating, the state of the local economy, and their victory margin.\n\n# Create realistic electoral data\nset.seed(42)  # Consistent with initial setup\nn_elections &lt;- 200\n\n# Generate correlated predictors (realistic scenario)\napproval_rating &lt;- runif(n_elections, 35, 85)\neconomic_growth &lt;- rnorm(n_elections, 2.5, 1.5)\ncampaign_spending_100k &lt;- rnorm(n_elections, 8, 2)  # In units of $100,000 for clarity\n\n# Create victory margin with realistic relationships\nvictory_margin &lt;- -15 + \n  0.6 * approval_rating +           # Strong approval effect\n  2.5 * economic_growth +           # Economic voting\n  0.3 * campaign_spending_100k +    # Money helps (effect per $100k)\n  rnorm(n_elections, 0, 8)          # Random factors\n\n# Create dataset\nelection_data &lt;- data.frame(\n  district = 1:n_elections,\n  approval = approval_rating,\n  econ_growth = economic_growth,\n  spending_100k = campaign_spending_100k,\n  victory_margin = victory_margin,\n  won = victory_margin &gt; 0\n)\n\n# Quick visualization\np1 &lt;- ggplot(election_data, aes(x = approval, y = victory_margin)) +\n  geom_point(alpha = 0.6, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.7) +\n  labs(title = \"Approval Rating vs. Victory Margin\",\n       x = \"Approval Rating (%)\",\n       y = \"Victory Margin (percentage points)\",\n       subtitle = \"Points above the dashed line represent wins\")\n\nprint(p1)\n\n\n\n\n\n\n\n# Run the regression\nsimple_model &lt;- lm(victory_margin ~ approval, data = election_data)\nsummary(simple_model)\n\n\nCall:\nlm(formula = victory_margin ~ approval, data = election_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.9948  -6.1420   0.5653   5.9218  28.4974 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) -9.78570    2.63382  -3.715             0.000264 ***\napproval     0.64728    0.04192  15.439 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.635 on 198 degrees of freedom\nMultiple R-squared:  0.5462,    Adjusted R-squared:  0.544 \nF-statistic: 238.4 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\n\n\nFigure Note: This scatter plot shows the relationship between approval ratings (x-axis) and electoral victory margins (y-axis). Each point represents one election. The red line shows the “line of best fit” from linear regression, with the gray band indicating uncertainty. Points above the dashed horizontal line (y=0) represent electoral victories.\n\nReading the Output: The “Estimate” for approval (approximately 0.60) means each 1-point increase in approval rating is associated with a 0.60-point increase in victory margin. The p-value (&lt;0.001) indicates this relationship is statistically significant—very unlikely to be due to chance alone.\nWhat we just discovered: Each 1-point increase in approval rating is associated with about a 0.65-point increase in victory margin. With an approval rating below 15.1%, incumbents typically lose.\nHowever, approval rating represents only one factor in electoral success. A more comprehensive analysis requires examining multiple variables simultaneously:\n\n# Multiple regression model\nfull_model &lt;- lm(victory_margin ~ approval + econ_growth + spending_100k, data = election_data)\n\n# Clean presentation of results\nmodel_results &lt;- tidy(full_model) %&gt;%\n  mutate(\n    estimate = round(estimate, 4),\n    p.value = round(p.value, 3),\n    significant = ifelse(p.value &lt; 0.05, \"Yes\", \"No\"),\n    term = recode(term,\n                  \"(Intercept)\" = \"Baseline\",\n                  \"approval\" = \"Approval Rating\",\n                  \"econ_growth\" = \"Economic Growth (%)\",\n                  \"spending_100k\" = \"Campaign Spending (per $100k)\")\n  )\n\nkable(model_results, \n      col.names = c(\"Variable\", \"Effect Size\", \"Std Error\", \"t-statistic\", \"p-value\", \"Significant?\"),\n      caption = \"What Really Drives Electoral Success?\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nWhat Really Drives Electoral Success?\n\n\nVariable\nEffect Size\nStd Error\nt-statistic\np-value\nSignificant?\n\n\n\n\nBaseline\n-18.8368\n3.7738929\n-4.991343\n0.00\nYes\n\n\nApproval Rating\n0.6541\n0.0397821\n16.441115\n0.00\nYes\n\n\nEconomic Growth (%)\n1.9619\n0.4004247\n4.899426\n0.00\nYes\n\n\nCampaign Spending (per $100k)\n0.4897\n0.3054328\n1.603246\n0.11\nNo\n\n\n\n\n\nWhen we account for multiple factors simultaneously, we see that:\n\nApproval rating remains the strongest predictor (0.6 points per 1% approval)\nEconomic growth also matters significantly (2.5 points per 1% GDP growth)\nCampaign spending has a modest effect (0.3 points per $100,000 spent)\n\nThis is the power of regression analysis—it helps us disentangle complex relationships and understand what really matters in politics.\n\n\n\n\n\n\nCommon Statistical Pitfalls in Political Science\n\n\n\n\nEcological fallacy: Assuming group-level patterns apply to individuals\nSelection bias: Non-random samples that systematically exclude certain groups\n\nConfounding: Failing to account for variables that affect both X and Y\nP-hacking: Testing multiple hypotheses until finding significance\nOvergeneralization: Extending findings beyond the studied population\n\n\n\n\n\nThe Political World is Full of Data\nPolitical science has evolved from a primarily theoretical discipline to one that increasingly relies on empirical evidence. Whether we’re studying:\n\nElection outcomes: Why do people vote the way they do?\nPublic opinion: What shapes attitudes toward immigration or climate policy?\nInternational relations: What factors predict conflict between nations?\nPolicy effectiveness: Did a new education policy actually improve outcomes?\n\nWe need systematic ways to analyze data and draw conclusions that go beyond anecdotes and personal impressions.\nConsider this question: “Does democracy lead to economic growth?”\nYour intuition might suggest yes—democratic countries tend to be wealthier. But is this causation or correlation? Are there exceptions? How confident can we be in our conclusions?\nStatistics provides the tools to move from hunches to evidence-based answers, helping us distinguish between what seems true and what actually is true.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#measurement-transforming-concepts-into-numbers",
    "href": "chapter1.html#measurement-transforming-concepts-into-numbers",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.30 Measurement: Transforming Concepts into Numbers",
    "text": "1.30 Measurement: Transforming Concepts into Numbers\n\nThe Challenge of Measurement in Social Sciences\nIn social sciences, we often struggle with the fact that key concepts do not translate directly into numbers:\n\nHow do we measure “democracy”?\nWhat number captures “political ideology”?\nHow do we quantify “institutional strength”?\nHow do we measure “political participation”?\n\n\n\nLevels of Measurement\nNominal (categories without order)\n\nParty affiliation: Democratic, Republican, Independent\nCountry: Poland, Germany, France\nVoting choice: Candidate A, Candidate B, Did not vote\n\nPermitted operations: frequency counts, mode, cross-tabulation, chi-square test.\nOrdinal (ordered categories)\n\nEducation level: elementary &lt; high school &lt; bachelor’s &lt; master’s &lt; doctoral degree\nLikert scales: Strongly disagree &lt; Disagree &lt; Neutral &lt; Agree &lt; Strongly agree\nPolitical knowledge level: low &lt; medium &lt; high\n\nPermitted operations: ordering, median, quartiles, Spearman’s rank correlation, non-parametric tests (e.g., Mann-Whitney).\n\n\n\n\n\n\nImportant\n\n\n\nKey characteristic: Distances between categories do not have to be equal. For example, the difference in knowledge between “low” and “medium” levels may be much larger or smaller than the difference between “medium” and “high” levels. We only know that one level is higher than another, but not “by how much.”\n\n\nInterval (equal intervals, arbitrary zero)\n\nCalendar years: difference between 2020–2021 = difference between 2023–2024\nTemperature in °C or °F\nStandardized scores based on linear transformation (e.g., z-score, T-score)\n\nPermitted operations: addition, subtraction, arithmetic mean, standard deviation, Pearson correlation, linear regression.\n\n\n\n\n\n\nWarning\n\n\n\nLimitation: Comparisons like “twice as much” make no sense because the zero point is arbitrary. For example: 20°C is not “twice as warm” as 10°C. If we used the Fahrenheit scale, these same temperatures would be 68°F and 50°F – suddenly one is no longer “twice” the other.\n\n\nRatio (equal intervals + true zero)\n\nNumber of votes cast (0 = actually zero votes)\nAge, income, campaign expenditures\nNumber of correct answers on a test, percentage of correct answers\n\nPermitted operations: all operations, including ratios (“twice as many votes”).\n\n\n\nSpecial Case: Psychometric Test Results\n\n\n\n\n\n\n\n\nType of Score\nLevel of Measurement\nNote\n\n\n\n\nLetter grades (A/B/C), stanines, categories\nOrdinal\nOnly ordering, no equal intervals\n\n\nPercentiles\nOrdinal\nSame percentile increase means different change in actual scores\n\n\nIQ scores\nOrdinal\nRank-ordered and transformed to normal distribution\n\n\nz-score, T-score\nInterval*\n*Only if original scores truly have equal intervals\n\n\nRaw number of points, % correct\nRatio\nTrue zero, constant increment\n\n\n\nExample of the percentile problem: Moving from the 50th to 60th percentile might mean a change of 2-3 points on a test, while moving from the 90th to 95th percentile might mean a change of 10 points. Percentiles only tell us what percentage of people scored worse, but they don’t tell us about the actual magnitude of differences in abilities.\n\n\n\nReality: IQ is Fundamentally an Ordinal Scale\nHow IQ scores are created – step by step:\n\nCollecting raw scores: People take a test and receive a number of correct answers (e.g., 45 out of 60 questions)\nOrdering: All raw scores are arranged from worst to best\nAssigning ranks: Each score is assigned a position in the ranking\nTransformation to IQ scale: Ranks are mathematically transformed so that the mean equals 100 and standard deviation equals 15\n\nKey problem: This process forces a normal distribution onto data that may not have been normal in its original form. This means that equal differences in IQ points (e.g., difference between IQ 100 and 115 vs. difference between IQ 115 and 130) may not correspond to equal differences in actual cognitive abilities.\n\n\n\n\n\n\nKey Point\n\n\n\nIQ 130 does not mean “twice the intelligence” of IQ 65. IQ points only show a person’s position relative to other people in the sample, not the actual amount of intelligence. This is similar to places in a competition – the winner might win by a hair or by miles, but will still be in first place.\n\n\nIn research practice: why do we sometimes treat IQ as an interval scale?\nThis is a methodological compromise that allows for the use of more precise statistical tools:\n✅ Treating IQ as an interval scale is acceptable when: - Using standard statistical tests (correlations, regressions, t-tests) - Comparing groups within the same test and population - Being aware of the limitations of this approach - Our conclusions don’t depend on differences being exactly equal\n⚠️ Remember the limitations: - This is a simplification of reality - The assumption works better for scores near the mean (IQ 85-115) than at the extremes - Results must be interpreted carefully\n❌ Never: - Say that IQ differences mean equal differences in intelligence - Use statements like “twice as intelligent” - Forget that the normal distribution was imposed, not discovered in the data\n\n\nPractical Guidelines for Researchers\n\nBe transparent:\n\nClearly state: “We treat IQ as an interval scale for statistical purposes, remembering that it is fundamentally an ordinal scale”\n\nConsider alternatives:\n\nUse non-parametric tests when sample size allows\nCompare results from different analytical methods\n\nInterpret cautiously:\n\nFocus on statements about order (“group A achieved higher scores than group B”)\nAvoid precise statements about the magnitude of differences\nRemember: a 15-point IQ difference means “one standard deviation in the sample,” not “a specific amount of additional intelligence”\n\n\n\n\n\n\n\n\nTip\n\n\n\nIQ is an ordinal scale that has been transformed to look like an interval scale. It can be used in statistical analyses requiring an interval scale, but one must always remember its true nature when interpreting results. The key is understanding that IQ points tell us about position in a group, not about the absolute amount of intelligence.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#statistical-significance-making-sense-of-uncertain-evidence",
    "href": "chapter1.html#statistical-significance-making-sense-of-uncertain-evidence",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.31 Statistical Significance: Making Sense of Uncertain Evidence",
    "text": "1.31 Statistical Significance: Making Sense of Uncertain Evidence\n\nThe Courtroom Analogy for Hypothesis Testing\nStatistical hypothesis testing follows a logic analogous to legal proceedings:\n\nNull hypothesis (H_0): The defendant is innocent (no real effect exists)\nAlternative hypothesis (H_1): The defendant is guilty (a real effect exists)\nEvidence: Our data and statistical test\nVerdict: Reject H_0 (find significance) or fail to reject H_0 (no significance)\n\nAs in legal proceedings, we require strong evidence to reject the presumption of innocence (no effect). This framework leads to two types of potential errors:\n\nType I error (false positive): Convicting an innocent person (rejecting H_0 when H_0 is true), controlled by significance level \\alpha (typically 0.05)\nType II error (false negative): Acquitting a guilty person (failing to reject H_0 when H_1 is true), with probability \\beta and power 1-\\beta\n\n\n\nWhat is Statistical Significance?\nWhen we observe a difference in our data, we face a fundamental question: Does this difference reflect a true population characteristic or merely sampling variability?\nStatistical significance provides a framework for answering:\n\nIs the observed pattern likely due to a real effect, or could it plausibly arise from random chance alone?\n\nThis framework distinguishes between:\n\nSignal: Real patterns reflecting true relationships in the population\nNoise: Random variation arising from sampling\n\n\n\nThe Logic of Hypothesis Testing\nThe null hypothesis represents our default assumption—typically that no effect or relationship exists:\n\nNo difference between groups\nNo relationship between variables\nNo treatment effect\n\nWe maintain this skeptical stance until the data provide sufficient evidence to reject it.\n\n\nUnderstanding p-values: Three Complementary Perspectives\nThe p-value remains one of the most misunderstood concepts in statistics. Consider three complementary interpretations:\n\n1. The Surprise Metric\nThe p-value quantifies how surprised we should be to observe our data if nothing systematic were occurring:\n\nSmall p-value (&lt; 0.05): Very surprising under the null → Evidence for an effect\nLarge p-value (&gt; 0.05): Not surprising under the null → Insufficient evidence\n\n\n\n2. The Coin Flip Illustration\nConsider testing whether a coin is fair. You flip it 10 times and observe 8 heads.\nThe p-value answers: If the coin were actually fair, how often would we observe 8 or more heads in 10 flips?\nCalculation: P(X \\geq 8) = \\sum_{k=8}^{10} \\binom{10}{k} 0.5^{10} = \\frac{56}{1024} \\approx 0.0547\nSince this probability is relatively small (5.47%), we have moderate evidence against fairness.\n\n\n3. The Formal Definition\nA p-value is:\n\nThe probability of observing data at least as extreme as what we obtained, assuming the null hypothesis is true.\n\nFormally, for test statistic T and observed value t_{\\text{obs}}:\n\nOne-sided: p = P(T \\geq t_{\\text{obs}} \\mid H_0) or P(T \\leq t_{\\text{obs}} \\mid H_0)\nTwo-sided: p = 2 \\min\\{P(T \\geq |t_{\\text{obs}}| \\mid H_0), P(T \\leq -|t_{\\text{obs}}| \\mid H_0)\\}\n\nCritical clarification: The p-value assumes the null hypothesis is true—it does not provide the probability that the null hypothesis is true.\n\n\n\nA Visual Understanding of p-values\n\n# Simulate what happens under the null hypothesis\nset.seed(789)\nnull_distribution &lt;- rnorm(10000, mean = 0, sd = 1)\n\n# Our observed test statistic\nobserved &lt;- 2.1\n\n# Create data frame for visualization\nhist_data &lt;- data.frame(values = null_distribution)\n\n# Create the visualization\nggplot(hist_data, aes(x = values)) +\n  geom_histogram(aes(y = ..density..), bins = 50, \n                 fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_density(color = \"darkblue\", linewidth = 1) +\n  geom_vline(xintercept = observed, color = \"red\", linewidth = 1.5) +\n  geom_vline(xintercept = -observed, color = \"red\", linewidth = 1.5, \n             linetype = \"dashed\") +\n  geom_area(stat = \"function\", fun = dnorm, \n            xlim = c(observed, 4), fill = \"red\", alpha = 0.3) +\n  geom_area(stat = \"function\", fun = dnorm, \n            xlim = c(-4, -observed), fill = \"red\", alpha = 0.3) +\n  labs(title = \"What the p-value Measures\",\n       subtitle = \"Distribution of possible results if the null hypothesis were true\",\n       x = \"Test Statistic Values\",\n       y = \"Probability Density\") +\n  annotate(\"text\", x = 2.5, y = 0.15, \n           label = \"p-value:\\nProbability of\\nresults this extreme\\nor more extreme\", \n           color = \"red\", fontface = \"bold\", size = 3) +\n  annotate(\"text\", x = 0, y = 0.2, \n           label = \"Most likely\\nresults if\\nno effect\", \n           color = \"darkblue\", size = 3)\n\n\n\n\n\n\n\n\nThe blue distribution represents expected outcomes under the null hypothesis. The red lines mark our observed result, and the red shaded areas show the p-value—the probability of obtaining results at least this extreme by chance alone.\n\n\nExamples: Understanding p-values in Context\n\nExample 1: Campaign Advertisement Effectiveness\nResearch Question: Do television advertisements increase candidate vote share?\nDesign: A candidate runs TV ads in 20 randomly selected cities but not in 20 other similar cities.\n\n# Simulate the campaign ad experiment\nset.seed(123)\n\n# Generate realistic data\nad_cities &lt;- c(rep(\"With Ads\", 20), rep(\"No Ads\", 20))\nvote_share &lt;- c(\n  rnorm(20, 0.58, 0.08),  # Cities with ads: mean 58%, SD 8%\n  rnorm(20, 0.54, 0.08)   # Cities without ads: mean 54%, SD 8%\n)\n\ncampaign_data &lt;- data.frame(\n  treatment = factor(ad_cities, levels = c(\"No Ads\", \"With Ads\")),\n  vote_share = vote_share\n)\n\n# Calculate the observed difference\nmean_with_ads &lt;- mean(campaign_data$vote_share[campaign_data$treatment == \"With Ads\"])\nmean_no_ads  &lt;- mean(campaign_data$vote_share[campaign_data$treatment == \"No Ads\"])\nobserved_diff &lt;- mean_with_ads - mean_no_ads\n\n# Perform t-test (two-sided by default)\nt_test_result &lt;- t.test(vote_share ~ treatment, data = campaign_data)\np_val &lt;- t_test_result$p.value\n\n# Create visualization\nggplot(campaign_data, aes(x = treatment, y = vote_share, fill = treatment)) +\n  geom_boxplot(alpha = 0.7, width = 0.5) +\n  geom_jitter(width = 0.15, alpha = 0.6, size = 2.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4, \n               fill = \"red\", color = \"darkred\") +\n  labs(\n    title = \"Do Campaign Ads Increase Vote Share?\",\n    subtitle = paste0(\"Observed difference: \", round(observed_diff*100, 1), \n                     \" percentage points, p-value = \", round(p_val, 3)),\n    x = \"Treatment Condition\",\n    y = \"Vote Share (%)\",\n    caption = \"Red diamonds show group means. Each dot represents one city.\"\n  ) +\n  scale_y_continuous(labels = function(x) paste0(x*100, \"%\")) +\n  scale_fill_manual(values = c(\"No Ads\" = \"#E8E8E8\", \"With Ads\" = \"#4CAF50\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nInterpretation:\n\nThe boxes display the middle 50% of cities in each group\nRed diamonds indicate group means\nIndividual dots represent individual cities\nIf p-value &lt; 0.05, the observed difference is unlikely due to chance alone\n\n\n\nExample 2: Weather Effects on Voter Turnout\nResearch Question: Does rain decrease voter turnout?\n\n# Simulate weather and turnout data\nset.seed(456)\noptions(scipen = 999)\n\n# Create realistic data with clear difference\nn_elections &lt;- 30\nweather_data &lt;- data.frame(\n  weather = factor(c(rep(\"Rainy\", n_elections), rep(\"Sunny\", n_elections)),\n                   levels = c(\"Sunny\", \"Rainy\")),\n  turnout = c(\n    rnorm(n_elections, 0.62, 0.06),  # Rainy days: lower turnout\n    rnorm(n_elections, 0.68, 0.06)   # Sunny days: higher turnout\n  )\n)\n\n# Calculate statistics\nrain_turnout  &lt;- mean(weather_data$turnout[weather_data$weather == \"Rainy\"])\nsunny_turnout &lt;- mean(weather_data$turnout[weather_data$weather == \"Sunny\"])\nweather_diff  &lt;- sunny_turnout - rain_turnout  # Sunny minus Rainy\n\n# Statistical test (two-sided)\nweather_test &lt;- t.test(turnout ~ weather, data = weather_data)\nweather_p    &lt;- weather_test$p.value\nci_lower     &lt;- weather_test$conf.int[1]  # CI corresponds to Sunny - Rainy\nci_upper     &lt;- weather_test$conf.int[2]\n\n# Create enhanced visualization\nggplot(weather_data, aes(x = weather, y = turnout, fill = weather)) +\n  geom_boxplot(alpha = 0.7, width = 0.5) +\n  geom_jitter(width = 0.15, alpha = 0.6, size = 2.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4, \n               fill = \"black\", color = \"black\") +\n  labs(\n    title = \"Does Weather Affect Voter Turnout?\",\n    subtitle = paste0(\"Difference: \", round(weather_diff*100, 1), \n                     \" percentage points (95% CI: [\", \n                     round(ci_lower*100, 1), \", \", round(ci_upper*100, 1), \n                     \"]), p = \", round(weather_p, 3)),\n    x = \"Weather Condition\",\n    y = \"Voter Turnout Rate (%)\",\n    caption = \"Black diamonds show means. Each dot represents one election.\"\n  ) +\n  scale_y_continuous(labels = function(x) paste0(x*100, \"%\")) +\n  scale_fill_manual(values = c(\"Rainy\" = \"#B3D9FF\", \"Sunny\" = \"#FFD700\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nResults Summary:\n\nSunny day turnout: 68.9%\nRainy day turnout: 63.4%\nDifference: 5.5 percentage points\np-value: 0.0008\n\nInterpretation: If weather had no effect on turnout, there would be only a 0.08% chance of observing a difference this large or larger. This provides strong evidence that weather affects turnout.\n\n\nExample 3: Non-Significant Results\nResearch Question: Does time spent on social media predict political knowledge?\n\n# Simulate a case with no meaningful relationship\nset.seed(999)\nn_people &lt;- 150\n\n# Create data with essentially no relationship\nsocial_media_data &lt;- data.frame(\n  social_media_hours = runif(n_people, 0, 8),\n  political_knowledge = rnorm(n_people, 50, 15)\n)\n\n# Add tiny, undetectable relationship\nsocial_media_data$political_knowledge &lt;- social_media_data$political_knowledge + \n  0.5 * social_media_data$social_media_hours + rnorm(n_people, 0, 14)\n\n# Fit linear model\nsm_model  &lt;- lm(political_knowledge ~ social_media_hours, data = social_media_data)\nsm_summary &lt;- summary(sm_model)\nsm_coef &lt;- coef(sm_model)[2]\nsm_p    &lt;- sm_summary$coefficients[2, 4]\nsm_se   &lt;- sm_summary$coefficients[2, 2]\nr_squared &lt;- sm_summary$r.squared\n\n# Create scatter plot with regression line\nggplot(social_media_data, aes(x = social_media_hours, y = political_knowledge)) +\n  geom_point(alpha = 0.5, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\", fill = \"pink\", alpha = 0.3) +\n  labs(\n    title = \"Social Media Use and Political Knowledge\",\n    subtitle = paste0(\"Effect: \", round(sm_coef, 2), \" points per hour (SE = \", \n                     round(sm_se, 2), \"), p = \", round(sm_p, 3),\n                     \", R² = \", round(r_squared, 3)),\n    x = \"Daily Social Media Hours\",\n    y = \"Political Knowledge Score (0-100)\",\n    caption = \"Wide confidence band indicates high uncertainty about the relationship\"\n  ) +\n  theme_minimal() +\n  annotate(\"text\", x = 6, y = 20, \n           label = \"Not statistically\\nsignificant\", \n           color = \"red\", fontface = \"bold\", size = 4)\n\n\n\n\n\n\n\n\nCritical points about non-significant results:\n\nWe cannot conclude there is no relationship\nWe can only state we lack sufficient evidence for a relationship\nPossible explanations for non-significance:\n\nThe effect truly does not exist\nThe effect is too small to detect with our sample size\nMeasurement error obscures the true relationship\n\n\n\n\n\nThe 0.05 Threshold: Convention, Not Natural Law\nThe conventional threshold of p &lt; 0.05 for “statistical significance” is merely a historical convention established by Ronald Fisher in the 1920s.\n\n# Create a visual showing the continuous nature of p-values\np_values &lt;- seq(0.001, 0.2, by = 0.001)\np_data &lt;- data.frame(\n  p = p_values,\n  significant = ifelse(p_values &lt; 0.05, \"Significant\", \"Not Significant\")\n)\n\nggplot(p_data, aes(x = p, y = 1, fill = significant)) +\n  geom_tile(aes(height = 1)) +\n  geom_vline(xintercept = 0.05, color = \"black\", linewidth = 1.5) +\n  scale_fill_manual(values = c(\"Significant\" = \"#4CAF50\", \n                               \"Not Significant\" = \"#FF6B6B\")) +\n  scale_x_continuous(breaks = c(0.001, 0.01, 0.05, 0.1, 0.15, 0.2),\n                     labels = c(\"0.001\", \"0.01\", \"0.05\", \"0.10\", \"0.15\", \"0.20\")) +\n  labs(\n    title = \"The Arbitrary Nature of the 0.05 Threshold\",\n    subtitle = \"p = 0.049 and p = 0.051 are practically identical, yet conventionally treated differently\",\n    x = \"p-value\",\n    y = \"\",\n    fill = \"Conventional\\nInterpretation\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank()) +\n  annotate(\"text\", x = 0.025, y = 1, label = \"Strong\\nEvidence\", \n           color = \"white\", fontface = \"bold\") +\n  annotate(\"text\", x = 0.125, y = 1, label = \"Weak\\nEvidence\", \n           color = \"white\", fontface = \"bold\") +\n  annotate(\"text\", x = 0.05, y = 0.5, label = \"Arbitrary\\nCutoff\", \n           color = \"black\", fontface = \"bold\", size = 3)\n\n\n\n\n\n\n\n\nKey Considerations:\n\nNothing fundamentally changes at p = 0.05\nDifferent fields adopt different thresholds (physics: p ≈ 3 × 10⁻⁷, “5 sigma”)\nModern practice emphasizes reporting exact p-values and effect sizes\nThe dichotomization into “significant” versus “not significant” can be misleading\n\n\n\nCommon Misconceptions About p-values\n\nIncorrect Interpretations:\n\n“p = 0.03 means there’s a 97% chance our treatment works”\n\nError: p-values do not provide the probability that a hypothesis is true\n\n“p = 0.20 means the effect is small”\n\nError: p-values measure evidence strength, not effect magnitude\n\n“p &gt; 0.05 proves there’s no effect”\n\nError: Absence of evidence does not constitute evidence of absence\n\n\n\n\nCorrect Interpretations:\n\n“p = 0.03 means: If there were no effect, we would observe data this extreme only 3% of the time”\n“p = 0.20 means we have weak evidence against the null hypothesis”\n“p &gt; 0.05 means we cannot confidently distinguish signal from noise”\n\n\n\n\nStatistical Significance versus Practical Significance\n\n# Demonstrate the difference between statistical and practical significance\nset.seed(42)\n\n# Small but statistically significant effect (large sample)\nlarge_n &lt;- 10000\ngroup_a_large &lt;- rnorm(large_n, mean = 100, sd = 15)\ngroup_b_large &lt;- rnorm(large_n, mean = 100.5, sd = 15)  # Tiny difference\n\n# Large but not statistically significant effect (small sample)\nsmall_n &lt;- 20\ngroup_a_small &lt;- rnorm(small_n, mean = 100, sd = 15)\ngroup_b_small &lt;- rnorm(small_n, mean = 105, sd = 15)  # Large difference\n\n# Tests\ntest_large &lt;- t.test(group_a_large, group_b_large)\ntest_small &lt;- t.test(group_a_small, group_b_small)\n\n# Create comparison visualization\ncomparison_data &lt;- data.frame(\n  Scenario = c(\"Large Sample\\n(n=10,000)\", \"Small Sample\\n(n=20)\"),\n  Effect_Size = c(mean(group_b_large) - mean(group_a_large),\n                  mean(group_b_small) - mean(group_a_small)),\n  P_Value = c(test_large$p.value, test_small$p.value),\n  Significant = c(test_large$p.value &lt; 0.05, test_small$p.value &lt; 0.05)\n)\n\nggplot(comparison_data, aes(x = Effect_Size, y = -log10(P_Value))) +\n  geom_point(aes(color = Significant, shape = Scenario), size = 8) +\n  geom_hline(yintercept = -log10(0.05), linetype = \"dashed\", color = \"red\") +\n  geom_text(aes(label = Scenario), vjust = -1.5, size = 3) +\n  scale_color_manual(values = c(\"FALSE\" = \"gray60\", \"TRUE\" = \"darkgreen\")) +\n  labs(\n    title = \"Statistical versus Practical Significance\",\n    subtitle = \"Large samples detect tiny effects; small samples may miss large effects\",\n    x = \"Effect Size (Difference in Means)\",\n    y = \"Statistical Significance\\n(-log10 p-value)\",\n    caption = \"Points above red line are statistically significant (p &lt; 0.05)\"\n  ) +\n  theme_minimal() +\n  annotate(\"text\", x = 0.5, y = -log10(0.05), \n           label = \"p = 0.05\", color = \"red\", size = 3) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nComparison Results:\nLarge Sample (n=10,000 per group):\n\nDifference: 0.68 units\np-value: 1.412962e-03\nStatistically significant: Yes\nPractically important: Likely not (difference is minimal)\n\nSmall Sample (n=20 per group):\n\nDifference: 4.42 units\np-value: 0.344\nStatistically significant: No\nPractically important: Possibly yes (difference is substantial)\n\nKey Lesson: Always evaluate both statistical significance and effect size.\n\n\nThe Relationship Between p-values and Confidence Intervals\nA direct correspondence exists between p-values and confidence intervals:\n\nIf p &lt; 0.05 for testing “no difference,” the 95% CI excludes zero\nIf p &gt; 0.05, the 95% CI includes zero\n\n\n# Demonstrate the relationship\nset.seed(789)\n\n# Generate several studies with different effect sizes\nstudies &lt;- data.frame(\n  study = LETTERS[1:6],\n  effect = c(2.5, 2.2, 0.9, 0.3, -0.2, -1.5),\n  se = rep(1, 6)\n)\n\nstudies$ci_lower &lt;- studies$effect - 1.96 * studies$se\nstudies$ci_upper &lt;- studies$effect + 1.96 * studies$se\nstudies$p_value  &lt;- 2 * pnorm(-abs(studies$effect/studies$se))\nstudies$significant &lt;- studies$p_value &lt; 0.05\n\nggplot(studies, aes(x = study, y = effect, color = significant)) +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"gray50\", linewidth = 1) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2, linewidth = 1) +\n  geom_point(size = 4) +\n  scale_color_manual(values = c(\"FALSE\" = \"gray60\", \"TRUE\" = \"darkgreen\"),\n                     labels = c(\"Not Significant\", \"Significant\")) +\n  labs(\n    title = \"Confidence Intervals and Statistical Significance\",\n    subtitle = \"CIs that exclude zero correspond to p &lt; 0.05\",\n    x = \"Study\",\n    y = \"Effect Size\",\n    color = \"Statistical Significance\",\n    caption = \"Error bars show 95% confidence intervals\"\n  ) +\n  theme_minimal() +\n  annotate(\"text\", x = 0.5, y = 0.2, label = \"No effect\", \n           color = \"gray50\", fontface = \"italic\", size = 3) +\n  geom_text(aes(label = paste0(\"p=\", round(p_value, 3))), \n            vjust = -2, size = 3)\n\n\n\n\n\n\n\n\nObservations:\n\nStudies A, B: CIs exclude zero → p &lt; 0.05\nStudies C, D, E, F: CIs include zero → p &gt; 0.05\nDistance from zero correlates inversely with p-value magnitude\n\n\n\nComplete Example: Analyzing a Political Experiment\nResearch Question: Does providing voters with fact-checking information reduce belief in misinformation?\n\n# Simulate a fact-checking experiment\nset.seed(2024)\n\n# Create experimental data\nn_per_group &lt;- 100\nexperiment_data &lt;- data.frame(\n  group = c(rep(\"Control\", n_per_group), rep(\"Fact-Check\", n_per_group)),\n  misinformation_belief = c(\n    rnorm(n_per_group, mean = 65, sd = 12),  # Control: higher belief\n    rnorm(n_per_group, mean = 58, sd = 12)   # Treatment: lower belief\n  )\n)\n\n# Step 1: Calculate descriptive statistics\ndesc_stats &lt;- experiment_data %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    n = n(),\n    mean = mean(misinformation_belief),\n    sd = sd(misinformation_belief),\n    se = sd / sqrt(n),\n    .groups = \"drop\"\n  )\n\n# Extract means\nmean_control &lt;- desc_stats$mean[desc_stats$group == \"Control\"]\nmean_fact    &lt;- desc_stats$mean[desc_stats$group == \"Fact-Check\"]\ndifference_fc &lt;- mean_fact - mean_control\n\n# Step 2: Perform t-test\nt_result &lt;- t.test(misinformation_belief ~ group, data = experiment_data)\n\n# Convert CI to match (Fact-Check - Control)\nci_fc_lower &lt;- -t_result$conf.int[2]\nci_fc_upper &lt;- -t_result$conf.int[1]\n\n# Step 3: Calculate effect size (Cohen's d)\npooled_sd &lt;- sqrt(((n_per_group - 1) * desc_stats$sd[desc_stats$group==\"Control\"]^2 + \n                   (n_per_group - 1) * desc_stats$sd[desc_stats$group==\"Fact-Check\"]^2) / \n                  (2 * n_per_group - 2))\ncohens_d &lt;- difference_fc / pooled_sd\n\n# Step 4: Visualize results\nggplot(experiment_data, aes(x = group, y = misinformation_belief, fill = group)) +\n  geom_violin(alpha = 0.3) +\n  geom_boxplot(width = 0.3, alpha = 0.7) +\n  geom_jitter(width = 0.1, alpha = 0.3, size = 1) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4, \n               fill = \"yellow\", color = \"black\") +\n  scale_fill_manual(values = c(\"Control\" = \"#FF6B6B\", \"Fact-Check\" = \"#4ECDC4\")) +\n  labs(\n    title = \"Does Fact-Checking Reduce Belief in Misinformation?\",\n    subtitle = paste0(\"Difference (Fact-Check − Control): \", round(difference_fc, 1), \n                     \" points, p = \", round(t_result$p.value, 3),\n                     \", Cohen's d = \", round(cohens_d, 2)),\n    x = \"Experimental Condition\",\n    y = \"Misinformation Belief Score (0-100)\",\n    caption = \"Yellow diamonds show group means\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Step 5: Report results\ncat(\"HYPOTHESIS TEST RESULTS\\n\")\n\nHYPOTHESIS TEST RESULTS\n\ncat(\"Null Hypothesis: Fact-checking has no effect on misinformation belief\\n\")\n\nNull Hypothesis: Fact-checking has no effect on misinformation belief\n\ncat(\"Alternative: Fact-checking affects misinformation belief\\n\\n\")\n\nAlternative: Fact-checking affects misinformation belief\n\ncat(\"Descriptive Statistics:\\n\")\n\nDescriptive Statistics:\n\nprint(desc_stats)\n\n# A tibble: 2 × 5\n  group          n  mean    sd    se\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Control      100  64.0  12.3  1.23\n2 Fact-Check   100  59.6  12.3  1.23\n\ncat(\"\\nInferential Statistics:\\n\")\n\n\nInferential Statistics:\n\ncat(paste(\"Difference (Fact-Check − Control):\", round(difference_fc, 2), \"points\\n\"))\n\nDifference (Fact-Check − Control): -4.4 points\n\ncat(paste(\"95% CI: [\", round(ci_fc_lower, 2), \",\", round(ci_fc_upper, 2), \"]\\n\"))\n\n95% CI: [ -7.82 , -0.98 ]\n\ncat(paste(\"t-statistic:\", round(t_result$statistic, 2), \"\\n\"))\n\nt-statistic: 2.53 \n\ncat(paste(\"p-value:\", round(t_result$p.value, 4), \"\\n\"))\n\np-value: 0.012 \n\neffect_size_label &lt;- ifelse(abs(cohens_d) &lt; 0.2, \"negligible\",\n                           ifelse(abs(cohens_d) &lt; 0.5, \"small\",\n                                  ifelse(abs(cohens_d) &lt; 0.8, \"medium\", \"large\")))\ncat(paste(\"Cohen's d:\", round(cohens_d, 2), \"(\", effect_size_label, \"effect)\\n\\n\"))\n\nCohen's d: -0.36 ( small effect)\n\ncat(\"Conclusion:\\n\")\n\nConclusion:\n\nif(t_result$p.value &lt; 0.05) {\n  cat(\"We reject the null hypothesis. The evidence suggests that\\n\")\n  cat(\"fact-checking significantly reduces belief in misinformation.\\n\")\n} else {\n  cat(\"We fail to reject the null hypothesis. We lack sufficient\\n\")\n  cat(\"evidence that fact-checking affects misinformation belief.\\n\")\n}\n\nWe reject the null hypothesis. The evidence suggests that\nfact-checking significantly reduces belief in misinformation.\n\n\nNote on directional hypotheses: When theoretical predictions specify direction (e.g., fact-checking reduces misinformation belief), one-sided tests may be appropriate. Set alternative = \"greater\" for \\mu_{\\text{Control}} &gt; \\mu_{\\text{Fact-Check}} or alternative = \"less\" for the opposite. Two-sided tests remain standard when any difference would be theoretically meaningful.\n\n\nSummary: Practical Guidelines for Statistical Significance\nWhen interpreting statistical tests, follow this systematic approach:\n\nEvaluate the effect size first\n\nAssess the magnitude of the difference or relationship\nDetermine practical meaningfulness\n\nExamine the p-value\n\np &lt; 0.05: Evidence against the null hypothesis\np &gt; 0.05: Insufficient evidence to reject the null\n\nInterpret confidence intervals\n\nThese indicate the range of plausible effect sizes\nWider intervals reflect greater uncertainty\n\nConsider the research context\n\nSample size affects statistical power\nStudy quality outweighs p-value magnitude\nMultiple testing increases false positive risk\n\n\nFundamental Principles:\nStatistical significance does not equal practical importance. The p-value measures surprise under the null hypothesis, not the probability of truth. Absence of evidence does not constitute evidence of absence. Effect sizes should always accompany p-values in research reports.\nStatistical significance serves as a tool for distinguishing signal from noise in data, not as a measure of importance or truth. Apply it judiciously within the broader context of substantive and practical significance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#regression-the-workhorse-of-political-science",
    "href": "chapter1.html#regression-the-workhorse-of-political-science",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.32 Regression: The Workhorse of Political Science",
    "text": "1.32 Regression: The Workhorse of Political Science\nConsider a typical pre-election news headline: “Candidate Smith’s approval rating reaches 68%.” Your immediate inference likely suggests favorable electoral prospects for Smith—not guaranteed victory, but a strong position.\nThis intuitive assessment exemplifies the essence of regression analysis. You utilized one piece of information (approval rating) to predict another outcome (electoral success), automatically recognizing that higher approval ratings correlate with better electoral performance, despite an imperfect relationship.\nRegression analysis systematizes this intuitive process, enabling researchers to:\n\nGenerate predictions based on available information\nIdentify which factors matter most\nQuantify uncertainty in predictions\nTest theoretical propositions with empirical data",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#variables-and-variation",
    "href": "chapter1.html#variables-and-variation",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.33 Variables and Variation",
    "text": "1.33 Variables and Variation\n\nDefining Variables\nA variable is any characteristic that can take different values across units of observation. In political science:\n\nUnits of analysis: Countries, individuals, elections, policies, years\nVariables: GDP, voting preference, democracy score, conflict occurrence",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#what-is-regression",
    "href": "chapter1.html#what-is-regression",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.34 What is Regression?",
    "text": "1.34 What is Regression?\nRegression analysis constitutes the foundational statistical tool in political science. It models relationships between variables and operationalizes our fundamental statistical model.\n\nThe Fundamental Model\nA model represents an object, person, or system in an informative way. Models divide into physical representations (such as architectural models) and abstract representations (such as mathematical equations describing atmospheric dynamics).\nThe core of statistical thinking can be expressed as:\nY = f(X) + \\text{error}\nThis equation states that our outcome (Y) equals some function of our predictors (X), plus unpredictable variation.\nComponents:\n\nY = Dependent variable (the phenomenon we seek to explain)\nX = Independent variable(s) (explanatory factors)\nf() = The functional relationship (often assumed linear)\nerror (\\epsilon) = Unexplained variation\n\nThis model provides the foundation for all statistical analysis—from simple correlations to complex machine learning algorithms.\nRegression helps answer fundamental questions such as:\n\nHow much does education increase political participation?\nWhat factors predict electoral success?\nDo democratic institutions promote economic growth?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#building-intuition-a-sports-analogy",
    "href": "chapter1.html#building-intuition-a-sports-analogy",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.35 Building Intuition: A Sports Analogy",
    "text": "1.35 Building Intuition: A Sports Analogy\nBefore examining political applications, consider a simpler context. Suppose you want to predict basketball players’ scoring based on their height. Expected patterns include:\n\nTaller players generally score more points\nHeight alone does not determine scoring (skill, position, and playing time matter)\nSubstantial variation exists—some shorter players excel at scoring\n\nPlotting height (x-axis) versus points scored (y-axis) would likely reveal:\n\nAn upward trend in points as height increases\nConsiderable scatter around that trend\nA line capturing the general relationship\n\nThis illustrates regression’s essence: finding the line that best summarizes relationships between variables while acknowledging imperfect correlations.\n\n# Create basketball example for intuition\nset.seed(123)\nn_players &lt;- 100\n\n# Generate realistic basketball data\nheight_inches &lt;- rnorm(n_players, 78, 4)  # Average NBA height ~6'6\"\n# Scoring increases with height, but with substantial variation\npoints_per_game &lt;- 2 + 0.3 * (height_inches - 70) + rnorm(n_players, 0, 5)\npoints_per_game &lt;- pmax(0, points_per_game)  # No negative scoring\n\nbasketball_data &lt;- data.frame(\n  height = height_inches,\n  points = points_per_game\n)\n\n# Visualization\nggplot(basketball_data, aes(x = height, y = points)) +\n  geom_point(alpha = 0.6, color = \"orange\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"blue\", size = 1.2) +\n  labs(\n    title = \"Height versus Points Scored: The Basic Concept of Regression\",\n    subtitle = \"The blue line shows the general relationship; points show individual players\",\n    x = \"Height (inches)\",\n    y = \"Points Per Game\",\n    caption = \"Each point represents one player; the line summarizes the overall pattern\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretation: Each orange point represents one player. The blue line indicates the overall trend—taller players score more points on average. The variation around the line reflects other unmeasured factors: skill, position, minutes played, team system, and other determinants of scoring ability.\nKey Insight: The line does not pass through every point because height represents only one factor affecting scoring. The scatter around the line captures all other influential factors not included in the model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#simple-linear-regression",
    "href": "chapter1.html#simple-linear-regression",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.36 Simple Linear Regression",
    "text": "1.36 Simple Linear Regression\nThe basic regression equation formalizes this relationship:\nY_i = \\alpha + \\beta X_i + \\epsilon_i\nWhere:\n\nY_i = outcome for observation i\nX_i = predictor for observation i\n\\alpha = intercept (expected value of Y when X = 0)\n\\beta = slope (change in Y for one-unit change in X)\n\\epsilon_i = error term\n\nApplied to the basketball example:\n\nY_i = points scored by player i\nX_i = height of player i\n\\alpha = baseline scoring (mathematical construct when height = 0)\n\\beta = additional points expected per inch of height\n\\epsilon_i = all other factors affecting player i’s scoring\n\n\nExample: Education and Political Participation\nConsider a classic political science question: Does education increase political participation?\n\n\n\n\n\n\n\n\n\nStatistical Results:\n\n\n• Each additional year of education increases participation by 0.029 points on average\n\n\n• Education explains 9.2 % of variation in participation\n\n\n• Remaining 90.8 % is explained by unmeasured factors\n\n\nR² (R-squared) Interpretation: This statistic indicates the percentage of variation in the outcome variable explained by predictor variables. R² = 0.3 means the model explains 30% of variation in political participation, leaving 70% unexplained.\n\n\nDecomposing the Regression Equation\nThe formal equation applies to our education-participation study as follows:\nY_i = \\alpha + \\beta X_i + \\epsilon_i\nTranslation to substantive terms:\n\nY_i: Political participation for person i\n\\alpha (intercept): Expected participation for someone with zero years of education\n\\beta (slope): Change in participation per additional year of education\nX_i: Years of education for person i\n\\epsilon_i: All other factors affecting person i’s participation (income, age, political interest, etc.)\n\nConceptual Framework: An individual’s political participation equals a baseline level (\\alpha) plus the effect of education (\\beta \\times education) plus unexplained factors (\\epsilon).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#multiple-regression-accounting-for-complexity",
    "href": "chapter1.html#multiple-regression-accounting-for-complexity",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.37 Multiple Regression: Accounting for Complexity",
    "text": "1.37 Multiple Regression: Accounting for Complexity\nReal-world phenomena rarely have single causes. Education affects participation, but so do income, age, and political interest. Multiple regression accounts for several factors simultaneously.\n\nUnderstanding “Controlling For”\nThis concept proves challenging but essential. Consider an analogy:\nSchool Comparison Example: To assess whether private schools outperform public schools, comparing raw test scores proves inadequate. Private school students often have wealthier, more educated parents. The observed difference might reflect family background rather than school quality.\nFair comparison requires comparing students from similar backgrounds—wealthy students across school types and middle-class students across school types.\nStatistical “control” achieves this comparison mathematically. When stating “education increases political participation by 0.04 points, controlling for income and age,” this means:\n\nAmong people with identical income and age\nThose with one additional year of education participate 0.04 points more on average\nThe comparison adjusts for confounding factors\n\nThe multiple regression equation formalizes this:\nY_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_k X_{ki} + \\epsilon_i\nEach \\beta_j represents the effect of X_j holding all other variables constant.\n\n\nExample: Determinants of Electoral Success\nReturning to electoral prediction, consider multiple factors simultaneously:\n\n\n\nImpact of Including Control Variables\n\n\nModel\nApproval Effect\np-value\n\n\n\n\nSimple (approval only)\n0.781\n0\n\n\nMultiple (controlling for economy & spending)\n0.750\n0\n\n\n\n\n\nDeterminants of Electoral Success:\n\n\n• 1% increase in approval → +0.7 point victory margin \n• 1% economic growth → +2.3 point victory margin \n• $1M in spending → +0.03 point victory margin \n\n\n\nThese factors jointly explain 71.5% of election outcomes\n\n\nRemaining 28.5% attributable to unmeasured factors\n\n\nCritical Observation: The effect of approval rating changes when additional variables are included. This demonstrates why controlling for confounders matters—omitted variables can substantially bias conclusions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-causal-inference-challenge-does-money-buy-elections",
    "href": "chapter1.html#the-causal-inference-challenge-does-money-buy-elections",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.38 The Causal Inference Challenge: Does Money Buy Elections?",
    "text": "1.38 The Causal Inference Challenge: Does Money Buy Elections?\nThis question illustrates regression’s limitations and the distinction between correlation and causation.\nThe Observed Pattern: Candidates who spend more typically receive more votes. Does spending cause votes?\nAlternative Explanations:\n\nReverse causation: Popular candidates attract more donations\nCommon cause: Charismatic candidates both inspire donations and win votes\nSelection bias: Only well-funded candidates run competitive races\n\nThis illustrates the central challenge: correlation does not imply causation.\n\nThe Fundamental Problem of Causal Inference\nTo establish causation, we would need to observe the same candidate in parallel scenarios:\n\nScenario A: Spending $5 million\nScenario B: Spending $1 million\nCausal effect = Difference in vote share\n\nThe Problem: We observe only one scenario per candidate. This constitutes the “Fundamental Problem of Causal Inference.”\n\n\nApproaches to Causal Identification\nResearchers employ several strategies to approximate causal effects:\n1. Randomized Experiments (Gold Standard)\n\nRandom assignment to treatment/control groups\nGroups identical except for treatment\nDifferences attributable to treatment\n\n2. Natural Experiments\n\nClose elections create quasi-random variation\nPolicy changes affect some areas but not others\nNatural disasters provide exogenous shocks\n\n3. Statistical Control\n\nInclude confounding variables in regression\nInterpret coefficients as causal under strong assumptions\nLimitation: Requires measuring all confounders\n\n\n# Demonstrate confounding in campaign spending\nset.seed(789)\nn_candidates &lt;- 500\n\n# Candidate quality affects both spending and votes\ncandidate_quality &lt;- rnorm(n_candidates, 0, 1)\n\n# Quality influences fundraising\nspending &lt;- 50 + 20 * candidate_quality + rnorm(n_candidates, 0, 10)\nspending &lt;- pmax(0, spending)\n\n# Votes depend on spending AND quality\nvote_share &lt;- 30 + 0.1 * spending + 15 * candidate_quality + rnorm(n_candidates, 0, 5)\nvote_share &lt;- pmax(0, pmin(100, vote_share))\n\ncampaign_data &lt;- data.frame(\n  spending = spending,\n  quality = candidate_quality,\n  vote_share = vote_share\n)\n\n# Compare analyses\nnaive_model &lt;- lm(vote_share ~ spending, data = campaign_data)\ncontrolled_model &lt;- lm(vote_share ~ spending + quality, data = campaign_data)\n\n# True effect is 0.1\ncomparison_results &lt;- data.frame(\n  Model = c(\"Naive (no controls)\", \"Proper (controlling for quality)\"),\n  Spending_Effect = c(coef(naive_model)[2], coef(controlled_model)[2]),\n  True_Effect = c(0.1, 0.1)\n) %&gt;%\n  mutate(\n    Error = Spending_Effect - True_Effect,\n    Bias_Direction = case_when(\n      abs(Error) &lt; 0.05 ~ \"Unbiased\",\n      Error &gt; 0 ~ \"Upward bias\",\n      Error &lt; 0 ~ \"Downward bias\"\n    )\n  )\n\nkable(comparison_results, \n      digits = 3,\n      caption = \"Importance of Controlling for Confounders\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nImportance of Controlling for Confounders\n\n\nModel\nSpending_Effect\nTrue_Effect\nError\nBias_Direction\n\n\n\n\nNaive (no controls)\n0.681\n0.1\n0.581\nUpward bias\n\n\nProper (controlling for quality)\n0.155\n0.1\n0.055\nUpward bias\n\n\n\n\n# Visualize confounding\nggplot(campaign_data, aes(x = spending, y = vote_share, color = quality)) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linetype = \"dashed\") +\n  scale_color_gradient2(low = \"blue\", mid = \"gray\", high = \"red\", \n                       midpoint = 0, name = \"Candidate\\nQuality\") +\n  labs(\n    title = \"Confounding in Campaign Finance\",\n    subtitle = \"Red line shows naive correlation; true effect requires quality control\",\n    x = \"Campaign Spending ($1000s)\",\n    y = \"Vote Share (%)\",\n    caption = \"Color indicates candidate quality—higher quality correlates with both spending and votes\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nKey Lesson: Without controlling for candidate quality, we overestimate spending’s effect. The naive analysis conflates quality’s effect with spending’s effect.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#common-pitfalls-in-regression-analysis",
    "href": "chapter1.html#common-pitfalls-in-regression-analysis",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.39 Common Pitfalls in Regression Analysis (*)",
    "text": "1.39 Common Pitfalls in Regression Analysis (*)\n\nPitfall 1: Confusing Statistical and Practical Significance\nThe Problem: Mistaking small but statistically significant effects for meaningful findings.\nWhy It Occurs: Large samples make tiny effects statistically significant. A study of 100,000 voters might detect that negative ads reduce turnout by 0.0001 percentage points with p &lt; 0.001.\nExample:\n\nReported: “Negative Ads Significantly Reduce Voter Turnout”\nReality: Effect = -0.001 percentage points per ad\nPractical Impact: Negligible\n\n\n# Demonstrate statistical vs practical significance\nset.seed(123)\n\n# Large sample, tiny effect\nn_large &lt;- 10000\ntreatment_large &lt;- rep(c(0, 1), each = n_large/2)\noutcome_large &lt;- 0.5 + 0.001 * treatment_large + rnorm(n_large, 0, 0.1)\nresult_large &lt;- t.test(outcome_large ~ treatment_large)\n\n# Small sample, larger effect\nn_small &lt;- 100\ntreatment_small &lt;- rep(c(0, 1), each = n_small/2)\noutcome_small &lt;- 0.5 + 0.05 * treatment_small + rnorm(n_small, 0, 0.1)\nresult_small &lt;- t.test(outcome_small ~ treatment_small)\n\ncat(\"Statistical versus Practical Significance:\\n\\n\")\n\nStatistical versus Practical Significance:\n\ncat(\"Large sample (n = 10,000):\\n\")\n\nLarge sample (n = 10,000):\n\ncat(\"  Effect size:\", round(diff(result_large$estimate), 5), \"\\n\")\n\n  Effect size: 0.00064 \n\ncat(\"  P-value:\", format(result_large$p.value, scientific = TRUE), \"\\n\")\n\n  P-value: 7.488156e-01 \n\ncat(\"  Assessment: Statistically significant but practically meaningless\\n\\n\")\n\n  Assessment: Statistically significant but practically meaningless\n\ncat(\"Small sample (n = 100):\\n\")\n\nSmall sample (n = 100):\n\ncat(\"  Effect size:\", round(diff(result_small$estimate), 3), \"\\n\")\n\n  Effect size: 0.031 \n\ncat(\"  P-value:\", round(result_small$p.value, 3), \"\\n\")\n\n  P-value: 0.117 \n\ncat(\"  Assessment: Not statistically significant but practically meaningful\\n\")\n\n  Assessment: Not statistically significant but practically meaningful\n\n\nEffect Size Guidelines (Cohen’s conventions):\n\nNegligible: &lt; 0.1 standard deviations\nSmall: 0.1-0.3 standard deviations\nMedium: 0.3-0.8 standard deviations\nLarge: &gt; 0.8 standard deviations\n\nKey Principle: Always evaluate whether effect sizes are large enough to matter substantively.\n\n\nPitfall 2: Overfitting\nThe Problem: Creating models too complex for available data, causing the model to memorize rather than generalize.\nConsequences: Artificially inflated performance metrics that fail to replicate with new data.\n\n# Compare simple versus complex models\nset.seed(456)\nn_obs &lt;- 50\n\n# Create data where only X1 matters\nx1 &lt;- rnorm(n_obs)\nx2 &lt;- rnorm(n_obs)  # Irrelevant\nx3 &lt;- rnorm(n_obs)  # Irrelevant\nx4 &lt;- rnorm(n_obs)  # Irrelevant\nx5 &lt;- rnorm(n_obs)  # Irrelevant\n\ny &lt;- 2 + 1.5 * x1 + rnorm(n_obs, 0, 1)  # Only X1 actually matters\n\n# Compare models\nsimple_model &lt;- lm(y ~ x1)\ncomplex_model &lt;- lm(y ~ x1 + x2 + x3 + x4 + x5)\n\ncat(\"Model Comparison:\\n\")\n\nModel Comparison:\n\ncat(\"Simple model R²:\", round(summary(simple_model)$r.squared, 3), \"\\n\")\n\nSimple model R²: 0.719 \n\ncat(\"Complex model R²:\", round(summary(complex_model)$r.squared, 3), \"\\n\")\n\nComplex model R²: 0.73 \n\ncat(\"Complex adjusted R²:\", round(summary(complex_model)$adj.r.squared, 3), \"\\n\\n\")\n\nComplex adjusted R²: 0.699 \n\ncat(\"Note: Complex model's higher R² is misleading—adjusted R² reveals minimal improvement\\n\")\n\nNote: Complex model's higher R² is misleading—adjusted R² reveals minimal improvement\n\n\nWarning Signs:\n\nMore variables than justified theoretically\nR² increases but adjusted R² stagnates\nPoor out-of-sample prediction\nInclusion of variables without theoretical justification\n\nAdjusted R² Explanation: Unlike regular R², adjusted R² penalizes model complexity, providing honest assessment of model improvement.\nPrevention Strategies:\n\nInclude only theoretically justified variables\nMonitor adjusted R² rather than R²\nUse cross-validation\nMaintain parsimony\n\n\n\nPitfall 3: Multiple Testing Problem\nThe Problem: Testing numerous relationships and reporting only significant ones.\nStatistical Reality: With α = 0.05, expect 5% false positives. Testing 20 relationships yields approximately one spurious “significant” result.\n\n# Simulate multiple testing problem\nset.seed(789)\nn_tests &lt;- 20\np_values &lt;- numeric(n_tests)\n\n# Run tests where no true effect exists\nfor(i in 1:n_tests) {\n  x &lt;- rnorm(100)\n  y &lt;- rnorm(100)  # Y unrelated to X\n  test_result &lt;- cor.test(x, y)\n  p_values[i] &lt;- test_result$p.value\n}\n\nsignificant_tests &lt;- sum(p_values &lt; 0.05)\n\ncat(\"Multiple Testing Demonstration:\\n\")\n\nMultiple Testing Demonstration:\n\ncat(\"Tests conducted:\", n_tests, \"\\n\")\n\nTests conducted: 20 \n\ncat(\"'Significant' results:\", significant_tests, \"\\n\")\n\n'Significant' results: 0 \n\ncat(\"Expected by chance:\", round(n_tests * 0.05), \"\\n\")\n\nExpected by chance: 1 \n\ncat(\"Smallest p-value:\", round(min(p_values), 4), \"\\n\\n\")\n\nSmallest p-value: 0.0541 \n\nif(significant_tests &gt; 0) {\n  cat(\"Selective reporting of significant results would create false positives\\n\")\n}\n\nSolutions:\n\nPre-registration of hypotheses\nBonferroni or false discovery rate corrections\nComplete reporting of all tests\nTheory-driven hypothesis testing\n\n\n\nPitfall 4: Ecological Fallacy\nThe Problem: Inferring individual-level relationships from group-level data.\nClassic Example: “Wealthy states vote Democratic, therefore wealthy individuals vote Democratic” Reality: Within states, wealth often correlates with Republican voting\n\n# Demonstrate ecological fallacy\nset.seed(101)\n\n# State-level: wealth correlates with Democratic voting\nstate_income &lt;- runif(50, 40000, 80000)\nstate_dem_vote &lt;- 30 + 0.0005 * state_income + rnorm(50, 0, 5)\nstate_correlation &lt;- cor(state_income, state_dem_vote)\n\n# Individual-level: opposite relationship\nindividual_data &lt;- data.frame()\nfor(i in 1:50) {\n  n_people &lt;- 200\n  indiv_income &lt;- rnorm(n_people, state_income[i], 15000)\n  prob_dem &lt;- plogis((state_dem_vote[i]/100) - 0.00002 * (indiv_income - state_income[i]))\n  vote_dem &lt;- rbinom(n_people, 1, prob_dem)\n  \n  state_data &lt;- data.frame(\n    state = i,\n    income = indiv_income,\n    dem_vote = vote_dem * 100\n  )\n  individual_data &lt;- rbind(individual_data, state_data)\n}\n\nindividual_correlation &lt;- cor(individual_data$income, individual_data$dem_vote)\n\ncat(\"State-level correlation (income vs. Democratic vote):\", round(state_correlation, 3), \"\\n\")\n\nState-level correlation (income vs. Democratic vote): 0.75 \n\ncat(\"Individual-level correlation (income vs. Democratic vote):\", round(individual_correlation, 3), \"\\n\")\n\nIndividual-level correlation (income vs. Democratic vote): -0.09 \n\ncat(\"\\nOpposite signs demonstrate the ecological fallacy\\n\")\n\n\nOpposite signs demonstrate the ecological fallacy\n\n\n\n\nPitfall 5: Selection Bias\nThe Problem: Drawing inferences from non-representative samples.\nCommon Sources:\n\nSurveying only landline users (age bias)\nStudying only volunteers (motivation bias)\nAnalyzing only successful cases (survivorship bias)\nUsing convenience samples (accessibility bias)\n\nConsequence: Systematic bias that statistical techniques cannot correct.\n\n\nPitfall 6: Ignoring Uncertainty\nThe Problem: Treating point estimates as precise values.\nIncorrect: “Support is 52%” Better: “Support is 52% ± 3%” Best: “95% confidence interval: [49%, 55%]”\nImportance: Acknowledging uncertainty prevents overconfident conclusions.\n\n\nPitfall 7: Spurious Correlations\nThe Problem: Variables correlate by coincidence, particularly with time trends.\n\n# Demonstrate spurious correlation\nset.seed(321)\nyears &lt;- 1990:2020\nn_years &lt;- length(years)\n\n# Unrelated variables with time trends\ninternet_users &lt;- 10 + 2.5 * (years - 1990) + rnorm(n_years, 0, 3)\npizza_consumption &lt;- 50 + 1.2 * (years - 1990) + rnorm(n_years, 0, 2)\n\nspurious_corr &lt;- cor(internet_users, pizza_consumption)\n\n# Remove time trends\ninternet_detrended &lt;- residuals(lm(internet_users ~ years))\npizza_detrended &lt;- residuals(lm(pizza_consumption ~ years))\ntrue_corr &lt;- cor(internet_detrended, pizza_detrended)\n\ncat(\"Spurious Correlation Analysis:\\n\")\n\nSpurious Correlation Analysis:\n\ncat(\"Correlation with time trends:\", round(spurious_corr, 3), \"\\n\")\n\nCorrelation with time trends: 0.982 \n\ncat(\"Correlation after detrending:\", round(true_corr, 3), \"\\n\")\n\nCorrelation after detrending: 0.136 \n\ncat(\"Conclusion: Apparent correlation driven by common time trends\\n\")\n\nConclusion: Apparent correlation driven by common time trends\n\n\nDetection Methods:\n\nExamine theoretical plausibility\nCheck for common time trends\nLook for third variables\nInspect scatterplots\n\n\n\nPitfall 8: Confounding Variables\nThe Problem: A third variable influences both predictor and outcome, creating misleading relationships.\nThe Confounding Structure:\n\nZ → X (confounder affects predictor)\nZ → Y (confounder affects outcome)\nCreates spurious X → Y relationship\n\n\n# Demonstrate confounding\nset.seed(456)\nn_districts &lt;- 500\n\n# Socioeconomic status confounds spending-votes relationship\nses &lt;- rnorm(n_districts, 0, 1)\n\n# SES affects both variables\ncampaign_spending &lt;- 100 + 50 * ses + rnorm(n_districts, 0, 20)\ncampaign_spending &lt;- pmax(10, campaign_spending)\n\nvote_share &lt;- 50 + 8 * ses + 0.02 * campaign_spending + rnorm(n_districts, 0, 5)\nvote_share &lt;- pmax(0, pmin(100, vote_share))\n\n# Compare models\nnaive_model &lt;- lm(vote_share ~ campaign_spending, data = data.frame(campaign_spending, vote_share))\ncontrolled_model &lt;- lm(vote_share ~ campaign_spending + ses, \n                      data = data.frame(campaign_spending, vote_share, ses))\n\ncat(\"Campaign Spending Effects:\\n\")\n\nCampaign Spending Effects:\n\ncat(\"Without SES control:\", round(coef(naive_model)[2], 4), \"\\n\")\n\nWithout SES control: 0.1601 \n\ncat(\"With SES control:\", round(coef(controlled_model)[2], 4), \"\\n\")\n\nWith SES control: 0.0052 \n\ncat(\"True effect: 0.02\\n\")\n\nTrue effect: 0.02\n\n\nTypes of Confounding:\n\nPositive Confounding: Exaggerates relationships\nNegative Confounding: Masks relationships\nSign Reversal: Reverses relationship direction\n\nSolutions:\n\nTheoretical identification of confounders\nCausal diagrams\nStatistical control\nResearch design solutions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#guidelines-for-research-consumers",
    "href": "chapter1.html#guidelines-for-research-consumers",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.40 Guidelines for Research Consumers",
    "text": "1.40 Guidelines for Research Consumers\nWhen evaluating research, consider:\n\nEffect Size: Is the magnitude practically meaningful?\nSample: Who is included/excluded? How might this bias results?\nMultiple Testing: Were many tests conducted? Is there cherry-picking?\nCausation: What is the identification strategy?\nModel Complexity: Does complexity match data availability?\nUncertainty: Are confidence intervals reported?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#complete-analysis-example",
    "href": "chapter1.html#complete-analysis-example",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.41 Complete Analysis Example",
    "text": "1.41 Complete Analysis Example\n\n# Research Question: What predicts voter turnout in US counties?\n\nset.seed(101)\nn_counties &lt;- 500\n\ncounty_data &lt;- data.frame(\n  county_id = 1:n_counties,\n  median_income = runif(n_counties, 35000, 85000),\n  college_percent = runif(n_counties, 15, 65),\n  unemployment = runif(n_counties, 2, 15),\n  rural_percent = runif(n_counties, 0, 95)\n)\n\n# Generate turnout based on multiple factors\ncounty_data$turnout &lt;- 45 + \n  0.0003 * county_data$median_income +\n  0.4 * county_data$college_percent +\n  -0.8 * county_data$unemployment +\n  -0.1 * county_data$rural_percent +\n  rnorm(n_counties, 0, 8)\n\ncounty_data$turnout &lt;- pmax(30, pmin(85, county_data$turnout))\n\n# Step 1: Data exploration\ncat(\"Step 1: Data Summary\\n\")\n\nStep 1: Data Summary\n\ncat(\"Sample size:\", nrow(county_data), \"counties\\n\")\n\nSample size: 500 counties\n\ncat(\"Turnout range:\", round(min(county_data$turnout), 1), \"% to\", \n    round(max(county_data$turnout), 1), \"%\\n\\n\")\n\nTurnout range: 33.5 % to 85 %\n\n# Step 2: Model estimation\nfull_turnout_model &lt;- lm(turnout ~ median_income + college_percent + \n                        unemployment + rural_percent, data = county_data)\n\n# Step 3: Results interpretation\ncat(\"Step 3: Results Interpretation\\n\\n\")\n\nStep 3: Results Interpretation\n\nmodel_results &lt;- tidy(full_turnout_model) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(\n    interpretation = case_when(\n      term == \"median_income\" ~ paste0(\"$10k income increase → +\", \n                                       round(estimate * 10000, 1), \"% turnout\"),\n      term == \"college_percent\" ~ paste0(\"10% more college graduates → +\", \n                                         round(estimate * 10, 1), \"% turnout\"),\n      term == \"unemployment\" ~ paste0(\"1% higher unemployment → \", \n                                      round(estimate, 1), \"% turnout\"),\n      term == \"rural_percent\" ~ paste0(\"10% more rural → \", \n                                       round(estimate * 10, 1), \"% turnout\")\n    ),\n    significance = ifelse(p.value &lt; 0.05, \"Significant\", \"Not significant\")\n  )\n\nfor(i in 1:nrow(model_results)) {\n  cat(\"•\", model_results$interpretation[i], \"(\", model_results$significance[i], \")\\n\")\n}\n\n• $10k income increase → +3.1% turnout ( Significant )\n• 10% more college graduates → +4% turnout ( Significant )\n• 1% higher unemployment → -0.7% turnout ( Significant )\n• 10% more rural → -0.8% turnout ( Significant )\n\n# Step 4: Model assessment\nr_squared &lt;- summary(full_turnout_model)$r.squared\ncat(paste0(\"\\nModel explains \", round(r_squared * 100, 1), \"% of turnout variation\\n\"))\n\n\nModel explains 49.9% of turnout variation\n\ncat(paste0(\"Remaining \", round((1-r_squared) * 100, 1), \"% due to unmeasured factors\\n\\n\"))\n\nRemaining 50.1% due to unmeasured factors\n\n# Step 5: Diagnostics\ncat(\"Step 5: Assumption Diagnostics\\n\")\n\nStep 5: Assumption Diagnostics\n\nplot(full_turnout_model, which = 1, main = \"Residuals versus Fitted Values\")\n\n\n\n\n\n\n\n\n\nAnalysis Summary\nFindings:\n\nEducation levels strongly predict county turnout\nEconomic factors (income, unemployment) show significant effects\nRural areas exhibit lower turnout\nThese factors explain approximately 60% of turnout variation\n\nLimitations:\n\nCorrelational analysis cannot establish causation\n40% of variation remains unexplained\nCounty-level patterns may not reflect individual behavior\nImportant variables may be omitted",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#conclusion",
    "href": "chapter1.html#conclusion",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.42 Conclusion",
    "text": "1.42 Conclusion\nRegression analysis provides systematic methods for testing theoretical propositions against empirical data. While it cannot definitively establish causation without appropriate research designs, it offers valuable tools for understanding relationships in observational data.\nKey Principles:\n\nRegression identifies best-fitting linear relationships\nMultiple regression controls for confounding variables\nCorrelation does not establish causation\nEffect sizes matter more than statistical significance\nAll analyses have limitations requiring acknowledgment\n\nThese analytical skills enable critical evaluation of empirical claims in academic research, policy debates, and political discourse. Understanding regression means not only conducting analyses but also recognizing the strengths and limitations of statistical evidence in social science research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#practical-advice-for-political-science-research",
    "href": "chapter1.html#practical-advice-for-political-science-research",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.43 Practical Advice for Political Science Research",
    "text": "1.43 Practical Advice for Political Science Research\n\n1. Start with Theory\nStatistics is a tool, not a substitute for thinking:\n\nWhat relationship do you expect and why?\nWhat would falsify your hypothesis?\nWhat alternative explanations exist?\n\n\n\n2. Know Your Data\nBefore any analysis:\n\n# Essential diagnostic steps\nsummary(data)           # Basic statistics\ntable(data$variable)    # Frequency tables\nhist(data$variable)     # Distribution\nplot(x, y)             # Scatterplots\ncor(data)              # Correlation matrix\n\n\n\n3. Match Method to Question\n\nDescribing: Means, proportions, distributions\nPredicting: Regression, machine learning\nCausal inference: Experiments, quasi-experiments, panel methods\n\n\n\n4. Interpret Substantively\nAlways translate statistics back to political science:\n\nWhat does a one-unit change mean substantively?\nIs the effect politically meaningful?\nWhat are the policy implications?\n\n\n\n5. Be Transparent\n\nReport all analyses, not just significant results\nShare data and code when possible\nAcknowledge limitations\nDescribe robustness checks",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#practice-problems",
    "href": "chapter1.html#practice-problems",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.44 Practice Problems",
    "text": "1.44 Practice Problems\n\nProblem 1: Identifying Populations and Samples\nYou want to understand what factors influence democratic transitions.\n\nWhat could be your population?\nHow would you select a sample?\nWhat biases might you face?\n\n\n\nProblem 2: Interpreting Results\nA study finds: “Education increases voter turnout by 2.3 percentage points per year of schooling (p = 0.02)”\n\nWhat does the p = 0.02 mean in plain English?\nIf someone has 4 more years of education than another person, how much more likely are they to vote?\nIs this a big or small effect? (Consider: typical turnout is around 60%)\n\n\n\nProblem 3: Correlation vs. Causation\n“Countries with more McDonald’s restaurants have lower infant mortality rates”\n\nGive three possible explanations for this pattern\nHow could you test which explanation is correct?\nWhat data would you need?\n\n\n\nProblem 4: Regression Interpretation\nYou run a regression predicting congressional vote share with these results:\nVote Share = 45.2 + 0.31*Approval + 2.1*Economy - 0.05*Age\n             (0.8)  (0.04)         (0.6)       (0.02)\n\nR² = 0.67, n = 435\nStandard errors in parentheses\nInterpret each coefficient substantively and assess statistical significance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#essential-r-code-for-getting-started",
    "href": "chapter1.html#essential-r-code-for-getting-started",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.45 Essential R Code for Getting Started",
    "text": "1.45 Essential R Code for Getting Started\n\n# Reading data\ndata &lt;- read.csv(\"yourfile.csv\")     # Load a CSV file\n\n# Basic exploration\nsummary(data)                         # See basic statistics for all variables\nhead(data)                           # Look at first few rows\ntable(data$party)                    # Count how many in each category\n\n# Simple analysis\nmean(data$age)                       # Calculate average age\ncor(data$income, data$turnout)      # Correlation between two variables\n\n# Basic visualization\nhist(data$age)                       # Histogram of age distribution\nplot(data$education, data$turnout)  # Scatterplot of two variables\n\n# Difference between groups\nt.test(income ~ gender, data = data) # Compare average income by gender\n\n# Simple regression\nmodel &lt;- lm(turnout ~ education, data = data)  # Run regression\nsummary(model)                                  # See results\n\n# Multiple regression\nmodel2 &lt;- lm(turnout ~ education + age + income, data = data)\nsummary(model2)\n\n# Create nice plots with ggplot2\nlibrary(ggplot2)\nggplot(data, aes(x = education, y = turnout)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Education and Turnout\",\n       x = \"Years of Education\", \n       y = \"Voter Turnout\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#final-thoughts",
    "href": "chapter1.html#final-thoughts",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.46 Final Thoughts",
    "text": "1.46 Final Thoughts\nStatistics is not just a tool—it’s a way of thinking about evidence, uncertainty, and inference. As citizens and scholars, developing statistical intuition helps us:\n\nCritically evaluate political claims\nDesign better research\nMake more informed decisions\nUnderstand the limits of what we can know\n\nRemember: Every number tells a story, but not every story told by numbers is true. Your job is to develop the skills to tell the difference.\nThe goal isn’t to become a statistician, but to become a political scientist who can evaluate and produce rigorous evidence. Statistics helps us move from hunches to hypotheses to evidence-based conclusions about the political world.\nAs you continue your journey in political science, always remember that behind every statistical analysis are real people, real policies, and real consequences. The tools you’ve learned here will help you contribute to our understanding of politics and hopefully make the world a bit better informed.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-a-sampling-methods",
    "href": "chapter1.html#appendix-a-sampling-methods",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.47 Appendix A: Sampling Methods",
    "text": "1.47 Appendix A: Sampling Methods\n\nProbability Sampling\nProbability sampling methods involve random selection, giving each member of the population a known, non-zero chance of being selected. These methods allow researchers to calculate sampling error and make statistical inferences about the population.\n\nSimple Random Sampling (SRS)\n\nDefinition: Each member of the population has an equal chance of being selected.\nAdvantages: Minimizes selection bias; allows straightforward statistical analysis.\nDisadvantages: Requires a complete sampling frame; may not capture enough members of smaller subgroups.\nExample: To select 100 students from a university with 10,000 students, assign each student a number and use a random number generator to select 100 numbers.\nBest used when: The population is relatively homogeneous and a complete list of population members is available.\n\nStratified Random Sampling\n\nDefinition: The population is divided into mutually exclusive subgroups (strata) based on shared characteristics, then samples are randomly selected from each stratum.\nAdvantages: Ensures representation of key subgroups; can improve precision for same sample size as SRS; allows analysis within and between strata.\nDisadvantages: Requires prior knowledge of population characteristics for stratification; more complex analysis.\nExample: In a national political survey, divide the population into strata based on geographic regions (Northeast, Midwest, South, West) and then randomly sample from each region proportionally to their population size.\nBest used when: The population contains distinct subgroups that might respond differently to the research question.\n\nCluster Sampling\n\nDefinition: The population is divided into clusters (usually geographic), some clusters are randomly selected, and all members within those clusters are studied.\nAdvantages: More cost-effective when population is geographically dispersed; doesn’t require a complete list of population members.\nDisadvantages: Lower statistical precision than SRS or stratified sampling; clusters must be representative.\nExample: To study high school students’ study habits, randomly select 20 high schools from across the country and survey all students in those schools.\nBest used when: The population is widely dispersed geographically and traveling to all units would be costly.\n\nSystematic Sampling\n\nDefinition: Selecting every kth item from a list after a random start.\nAdvantages: Simple to implement; often more practical than SRS; can avoid neighbor effects.\nDisadvantages: Can introduce bias if there’s a periodic pattern in the list.\nExample: At a busy shopping mall, survey every 20th person who enters, starting with a randomly chosen number between 1 and 20.\nBest used when: The population is ordered randomly or in a way unrelated to the study variables.\n\nMulti-stage Sampling\n\nDefinition: Combining multiple sampling methods in stages.\nAdvantages: Practical for large-scale surveys; balances cost and precision.\nDisadvantages: Complex design and analysis; multiple stages can compound sampling errors.\nExample: First randomly select counties (cluster sampling), then randomly select households within those counties (simple random sampling), and finally select one adult from each household (systematic sampling).\nBest used when: Studying large, complex populations across wide geographical areas.\n\n\n\n\nNon-probability Sampling\nNon-probability sampling doesn’t involve random selection, which means statistical inferences about the population must be made with caution. While it can introduce bias, it may be necessary in certain situations.\n\nConvenience Sampling\n\nDefinition: Selecting easily accessible subjects.\nAdvantages: Fast, inexpensive, and easy to implement.\nDisadvantages: High risk of selection bias; limits generalizability.\nExample: A researcher studying college students’ sleep patterns might survey students in their own classes.\nBest used for: Pilot studies, exploratory research, or when resources are severely limited.\n\nPurposive Sampling\n\nDefinition: Selecting subjects based on specific characteristics relevant to the research question.\nAdvantages: Focuses on relevant cases; useful for in-depth studies of specific groups.\nDisadvantages: Researcher bias in selection; limited generalizability.\nExample: For a study on the experiences of CEOs in the tech industry, intentionally seek out and interview CEOs from various tech companies.\nBest used for: Qualitative research, case studies, or studying unique populations.\n\nSnowball Sampling\n\nDefinition: Participants recruit other participants from their networks.\nAdvantages: Access to hard-to-reach or hidden populations; builds on social networks.\nDisadvantages: Sample biased toward those in certain social networks; cannot calculate selection probabilities.\nExample: In a study of undocumented immigrants’ access to healthcare, researchers ask initial participants to refer other potential participants.\nBest used for: Studying rare populations or sensitive topics where no sampling frame exists.\n\nQuota Sampling\n\nDefinition: Selecting participants to meet specific quotas for certain characteristics to match known population parameters.\nAdvantages: Ensures representation of key demographic groups; faster and cheaper than probability sampling; doesn’t require sampling frame.\nDisadvantages: Non-random selection within quotas can introduce bias; inference is limited.\nExample: In a market research study, researchers ensure they interview specific numbers of people from different age groups, genders, and income levels.\nBest used for: Commercial polling, market research, or when probability sampling is not feasible.\n\n\n\n\nWhy Pollsters Increasingly Use Quota Sampling\nIn recent years, many polling organizations have shifted toward quota sampling approaches for several key reasons:\n\nDeclining Response Rates: Traditional probability-based telephone polls have seen response rates drop from about 36% in the 1990s to less than 10% today. This increases costs and potentially introduces non-response bias that can be worse than selection bias from non-probability methods.\nCoverage Issues: Random digit dialing no longer reaches a representative sample of the population as many people have abandoned landlines for cell phones, and many don’t answer calls from unknown numbers.\nCost Efficiency: Probability-based polls have become prohibitively expensive as response rates decline, while online panels and quota sampling are more affordable.\nSpeed: In fast-moving political campaigns or rapidly evolving public opinion situations, quota sampling can deliver results much faster than probability methods.\nWeighting and Modeling Improvements: Modern statistical techniques allow pollsters to adjust quota samples to better represent the target population by weighting responses based on known population parameters.\nHybrid Approaches: Many pollsters now use hybrid methods that combine elements of probability and non-probability sampling, with sophisticated weighting and modeling to improve accuracy.\n\nThe 2016 US presidential election, where many polls failed to predict the outcome accurately, led to considerable soul-searching among pollsters. Rather than abandoning quota sampling, many organizations have refined their methods, focusing on better quota definitions, improved weighting techniques, and more transparent reporting of methodological limitations.\nDespite these trends, it’s important to note that probability sampling remains the gold standard for statistical inference. Well-designed probability samples still provide the most reliable foundation for generalizing from sample to population, especially for academic research where accuracy is prioritized over cost and speed.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-a-measuring-uncertainty-in-data",
    "href": "chapter1.html#appendix-a-measuring-uncertainty-in-data",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.48 Appendix A: Measuring Uncertainty in Data (*)",
    "text": "1.48 Appendix A: Measuring Uncertainty in Data (*)\n\nFundamental Principle\nStatistics does not eliminate uncertainty—it helps us measure, manage, and communicate it effectively.\n\n\n\n1. Two Types of Error: Random Error and Bias\nRandom Error (Sampling Variability)\n\nVaries unpredictably from sample to sample\nDecreases with larger sample sizes\nCan be quantified using standard errors and confidence intervals\nExample: Different samples of 100 voters will yield slightly different percentages supporting a candidate\n\nBias (Systematic Error)\n\nConsistent deviation from the true value\nDoes NOT decrease with larger sample sizes\nCannot be quantified through standard statistical formulas\nExample: Conducting an online survey about household income excludes households without internet access, likely underestimating poverty rates\n\n\n\n\n2. Two Types of Variability\nStandard Deviation (SD)\n\nMeasures the spread of individual data points around the mean\nQuantifies the typical distance between observations and their average\nFormula: s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\n\nStandard Error (SE)\n\nMeasures the precision of a sample statistic (such as the mean)\nQuantifies random sampling variability only (not bias)\nFormula: SE(\\bar{x}) = \\frac{s}{\\sqrt{n}}\n\nKey Observation: As sample size increases, the standard deviation remains relatively stable while the standard error decreases.\n\n\n\n3. Core Statistical Formulas\n\nNumerical Data (Continuous Variables)\n\n\n\n\n\n\n\n\nStatistic\nFormula\nApplication\n\n\n\n\nSample mean\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\nPoint estimate of population mean\n\n\nSample variance\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2\nMeasure of data dispersion\n\n\nStandard deviation\ns = \\sqrt{s^2}\nSpread in original units\n\n\nStandard error of mean\nSE(\\bar{x}) = \\frac{s}{\\sqrt{n}}\nPrecision of mean estimate\n\n\n95% Confidence interval\n\\bar{x} \\pm 1.96 \\times SE(\\bar{x})\nInterval estimate (large samples)\n\n\n\n\n\nCategorical Data (Proportions)\n\n\n\n\n\n\n\n\nStatistic\nFormula\nApplication\n\n\n\n\nSample proportion\n\\hat{p} = \\frac{\\text{number of successes}}{n}\nPoint estimate of population proportion\n\n\nStandard error\nSE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\nPrecision of proportion estimate\n\n\n95% Confidence interval\n\\hat{p} \\pm 1.96 \\times SE(\\hat{p})\nInterval estimate\n\n\n\nValidity conditions: - For means: sample size n ≥ 30 or approximately symmetric data distribution - For proportions: n\\hat{p} \\geq 10 and n(1-\\hat{p}) \\geq 10 - For small samples: replace 1.96 with the appropriate t-value - Critical assumption: Random sampling or random assignment (to avoid bias)\n\n\n\n\n4. The Multiplier 1.96: An Empirical Constant\nThe value 1.96 (often rounded to 2) is an empirically-derived constant that ensures approximately 95% coverage in repeated sampling. This multiplier has been validated through extensive statistical practice and simulation studies.\nPractical interpretation: When we construct intervals using \\text{estimate} \\pm 1.96 \\times SE: - Empirical studies show that about 95 out of 100 such intervals will contain the true population value - This provides a standardized way to communicate uncertainty - The choice of 95% is a widely-adopted convention in scientific research\nAlternative multipliers for different coverage levels: - 90% coverage: use 1.645 - 99% coverage: use 2.576 - Quick approximation: use 2 for roughly 95% coverage\n\n\n\n5. Worked Example: Analysis of Study Hours\nData: Survey of n = 40 students regarding weekly study hours\nSample statistics: \\bar{x} = 10.8 hours, s = 2.1 hours\nCalculations: 1. Standard error: SE = \\frac{2.1}{\\sqrt{40}} = 0.332 hours 2. Margin of error (95%): MoE = 1.96 \\times 0.332 = 0.651 hours 3. 95% Confidence interval: [10.8 - 0.651, 10.8 + 0.651] = [10.15, 11.45] hours\nInterpretation: Based on this sample, we estimate the mean study time to be 10.8 hours per week, with a margin of error of ±0.65 hours at the 95% confidence level. This accounts for random sampling error only, assuming no selection bias.\n\n\n\n6. Bootstrap Methods: Detailed Explanation\nThe bootstrap is a computer-intensive resampling technique that works regardless of the shape or pattern of your data. It simulates the process of taking many samples from the population by resampling from our single sample.\nConceptual Foundation:\n\nOur sample is treated as a “mini-population”\nWe draw many new samples from this mini-population (with replacement)\nThe variation across these resamples mimics what would happen if we could collect many real samples\nThis works because our sample contains information about the variability in the population\nNo need to assume data follows a bell curve or any other specific shape\n\nStep-by-Step Procedure:\n\nStart with your original sample of size n\n\nExample: [7, 9, 10, 11, 12] (n = 5 study hours)\n\nCreate a bootstrap sample by randomly selecting n values WITH replacement\n\nBootstrap sample 1: [7, 10, 10, 11, 7] → mean = 9.0\nBootstrap sample 2: [12, 9, 11, 11, 10] → mean = 10.6\nBootstrap sample 3: [9, 7, 12, 9, 11] → mean = 9.6\n\nRepeat B times (typically B = 1,000 to 10,000)\n\nCalculate your statistic (e.g., mean) for each bootstrap sample\nStore all B statistics\n\nConstruct confidence interval using percentiles\n\nSort the B statistics from smallest to largest\nFor 95% CI: take the 2.5th and 97.5th percentiles\nIf B = 1,000: CI = [25th smallest value, 975th smallest value]\n\n\nAdvantages:\n\nWorks for any statistic (median, correlation, ratio, etc.)\nNo formula needed for complex statistics\nCaptures the actual shape of the sampling distribution\nParticularly useful for small samples or skewed data\nDoes not require data to follow any specific pattern (bell-shaped, symmetric, etc.)\n\n\n\n\n7. Strategies for Managing Uncertainty\nMinimize Bias Through Study Design - Use random sampling from a well-defined population - Ensure sampling frame matches target population - Minimize non-response through follow-up efforts - Use blinding when appropriate\nReduce Random Error - Increase sample size (reduces SE by factor of 1/\\sqrt{n}) - Use stratification to ensure representation - Improve measurement precision - Note: These strategies do NOT reduce bias\nPre-Analysis Planning - Specify hypotheses and analysis methods before data collection - Determine required sample size based on desired precision - Document decision rules to maintain objectivity\n\n\n\n8. Sample Size Determination for Proportions\nFor estimating a proportion with margin of error m at 95% confidence:\nn \\approx \\frac{0.96}{m^2}\nThis formula assumes maximum variability (p = 0.5) and uses the approximation (1.96)^2 \\times 0.25 \\approx 0.96.\nExamples:\n\n\n\nDesired Margin of Error\nRequired Sample Size\n\n\n\n\n±1%\n~9,600\n\n\n±3%\n~1,067\n\n\n±5%\n~384\n\n\n±10%\n~96\n\n\n\n\n\n\n9. Implementation in R\n\n# Data: Weekly study hours for 40 students\nstudy_hours &lt;- c(9,12,10,14,7,11,13,9,8,12,10,11,9,15,13,7,8,10,14,12,\n                 11,9,10,8,12,13,9,7,10,11,12,8,9,13,14,10,11,10,9,12)\n\n# Calculate sample statistics\nn &lt;- length(study_hours)\nmean_hours &lt;- mean(study_hours)\nsd_hours &lt;- sd(study_hours)\nse_hours &lt;- sd_hours / sqrt(n)\n\n# Construct 95% confidence interval using normal approximation\nz_critical &lt;- 1.96\nmargin_of_error &lt;- z_critical * se_hours\nci_lower &lt;- mean_hours - margin_of_error\nci_upper &lt;- mean_hours + margin_of_error\n\n# Display results\ncat(\"SAMPLE STATISTICS\\n\")\n\nSAMPLE STATISTICS\n\ncat(sprintf(\"Sample size: n = %d\\n\", n))\n\nSample size: n = 40\n\ncat(sprintf(\"Sample mean: %.2f hours\\n\", mean_hours))\n\nSample mean: 10.55 hours\n\ncat(sprintf(\"Sample standard deviation: %.2f hours\\n\", sd_hours))\n\nSample standard deviation: 2.12 hours\n\ncat(sprintf(\"Standard error: %.3f hours\\n\", se_hours))\n\nStandard error: 0.336 hours\n\ncat(sprintf(\"Margin of error (95%%): %.3f hours\\n\", margin_of_error))\n\nMargin of error (95%): 0.658 hours\n\ncat(sprintf(\"95%% Confidence interval: [%.2f, %.2f]\\n\\n\", ci_lower, ci_upper))\n\n95% Confidence interval: [9.89, 11.21]\n\n# Bootstrap confidence interval with visualization\nset.seed(123)\nB &lt;- 5000\n\n# Generate bootstrap samples and calculate means\nbootstrap_means &lt;- replicate(B, {\n  resample &lt;- sample(study_hours, size = n, replace = TRUE)\n  mean(resample)\n})\n\n# Calculate bootstrap confidence interval\nboot_ci &lt;- quantile(bootstrap_means, probs = c(0.025, 0.975))\n\ncat(\"BOOTSTRAP RESULTS\\n\")\n\nBOOTSTRAP RESULTS\n\ncat(sprintf(\"Bootstrap 95%% CI: [%.2f, %.2f]\\n\", boot_ci[1], boot_ci[2]))\n\nBootstrap 95% CI: [9.90, 11.22]\n\ncat(sprintf(\"Number of bootstrap samples: B = %d\\n\\n\", B))\n\nNumber of bootstrap samples: B = 5000\n\n# Visualize bootstrap distribution\nlibrary(ggplot2)\nbootstrap_df &lt;- data.frame(means = bootstrap_means)\n\nggplot(bootstrap_df, aes(x = means)) +\n  geom_histogram(aes(y = ..density..), bins = 50, \n                 fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  geom_vline(xintercept = mean_hours, color = \"red\", \n             linetype = \"solid\", linewidth = 1) +\n  geom_vline(xintercept = boot_ci, color = \"darkgreen\", \n             linetype = \"dashed\", linewidth = 1) +\n  annotate(\"text\", x = mean_hours, y = 0, label = \"Sample Mean\", \n           vjust = -1, color = \"red\") +\n  annotate(\"text\", x = boot_ci[1], y = 0, label = \"2.5%\", \n           vjust = -2, color = \"darkgreen\") +\n  annotate(\"text\", x = boot_ci[2], y = 0, label = \"97.5%\", \n           vjust = -2, color = \"darkgreen\") +\n  labs(title = \"Bootstrap Distribution of Sample Means\",\n       subtitle = sprintf(\"B = %d bootstrap samples\", B),\n       x = \"Bootstrap Sample Means (hours)\",\n       y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\nSummary\nEssential Concepts:\n\nDistinguish between bias and random error: Large samples reduce random error but not bias\nThe Literary Digest failure: Even 2.4 million responses could not overcome selection bias\nStandard errors measure precision, not accuracy: They quantify random variation only\nBootstrap provides a universal method: Works without needing formulas or requiring data to follow specific patterns\n\nReporting Guidelines:\n\nAlways report point estimates with measures of uncertainty\nAcknowledge potential sources of bias in study limitations\nUse the format: “estimate (95% CI: lower bound–upper bound)”\nInclude sample size and sampling method when presenting results\n\nMethodological Considerations:\n\nRandom sampling is essential for valid inference\nVerify that validity conditions are met before applying formulas\nConsider bootstrap methods when your data is skewed or when standard formulas don’t exist\nRemember that no amount of statistical analysis can fix a biased sample",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-b-quantifying-uncertainty-for-a-proportion",
    "href": "chapter1.html#appendix-b-quantifying-uncertainty-for-a-proportion",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.49 Appendix B: Quantifying Uncertainty for a Proportion (*)",
    "text": "1.49 Appendix B: Quantifying Uncertainty for a Proportion (*)\n\nSetting and Notation\nWe observe a binary outcome (e.g., Heads vs. Tails; Support vs. No support). Let n denote the number of independent trials and x the number of “successes.” The sample proportion is \\hat p = x/n, which estimates the population proportion p.\n\n\n\n\n\n\nImportant\n\n\n\nAssumptions for the formulas below\n\nSampling: simple random sample (or randomized experiment) with observations that are independent (or close to independent).\nPopulation size: large relative to n; if sampling without replacement from a finite population of size N, a finite-population correction may apply: \\mathrm{SE}\\_\\text{FPC}=\\mathrm{SE}\\sqrt{(N-n)/(N-1)}.\nMeasurement: outcome is correctly recorded.\n\n\n\n\n\nEstimation, Precision, and a 95% Interval\nEstimator. \\hat p = x/n.\nStandard error (precision).\n\n\\mathrm{SE}(\\hat p) \\approx \\sqrt{\\frac{\\hat p(1-\\hat p)}{n}},\n\nwhich follows from \\mathrm{Var}(\\hat p)=p(1-p)/n for Bernoulli outcomes; replacing p with \\hat p yields a practical estimate of uncertainty.\n“Wald” 95% confidence interval (quick approximation).\n\n\\hat p \\ \\pm\\ 1.96 \\times \\mathrm{SE}(\\hat p).\n\nThis normal-approximation interval is serviceable for moderate to large n and p not too close to 0 or 1, but it can under-cover when n is small.\nWilson interval (recommended for small–moderate n). The Wilson score interval provides more reliable coverage for small samples; the simulation below uses it by default.\n\n\n\n\n\n\nTip\n\n\n\nMental arithmetic for polls (worst case near p=0.5). \\text{MoE} \\approx \\dfrac{1}{\\sqrt{n}} (since 1.96\\sqrt{0.25/n}\\approx 0.98/\\sqrt{n}).\n\nn=100 \\Rightarrow \\pm 10\\% • n=400 \\Rightarrow \\pm 5\\% • n=1000 \\Rightarrow \\pm 3.2\\%\nn=1600 \\Rightarrow \\pm 2.5\\% • n=2500 \\Rightarrow \\pm 2\\% • n=10000 \\Rightarrow \\pm 1\\%\n\n\n\n\n\nWorked Examples (hand calculation)\n\nTiny sample: n=10, x=7 ⇒ \\hat p=0.70. \\mathrm{SE}\\approx \\sqrt{0.7\\cdot0.3/10}\\approx 0.145 ⇒ Wald 95% CI \\approx 0.70 \\pm 0.29 = [0.41,\\,0.99]. (Very wide; small n implies high sampling variability.)\nSmall sample: n=30, x=18 ⇒ \\hat p=0.60. \\mathrm{SE}\\approx \\sqrt{0.6\\cdot0.4/30}\\approx 0.089 ⇒ Wald 95% CI \\approx [0.42,\\,0.78].\nModerate sample: n=100, x=56 ⇒ \\hat p=0.56. \\mathrm{SE}\\approx \\sqrt{0.56\\cdot0.44/100}\\approx 0.050 ⇒ Wald 95% CI \\approx [0.46,\\,0.66].\n\nReporting template (concise). “Estimate p\\approx \\hat p with a 95% CI [L, U]; report n, method (Wald/Wilson), and any design features affecting bias.”\n\n\nMini-Poll Illustration\nPoll with n=100 respondents and x=56 supporters:\n\n\\hat p=0.56\n\\mathrm{SE}\\approx 0.050\nWald 95% CI \\approx [0.46,\\,0.66].\n\nIf \\hat p=0.56 but n=1000, then \\mathrm{SE}\\approx 0.0157 and the 95% CI shrinks to [0.529,\\,0.591].\n\n\nSimulation: Sampling Variability and Coverage\nThe following code simulates 30 independent samples (10 trials each) and plots their 95% CIs. By default it uses the Wilson interval, which has better small-sample coverage than the quick Wald interval.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(tibble)\n\nset.seed(1234)\n\n# Choose interval method: \"wilson\" (recommended for small n) or \"wald\" (quick approximation)\nmethod &lt;- \"wilson\"\n\nm &lt;- 30      # number of repeated samples\nn &lt;- 10      # trials per sample\np_true &lt;- 0.5\n\nx     &lt;- rbinom(m, size = n, prob = p_true)\nphat  &lt;- x / n\nz     &lt;- 1.96\n\nwald_ci &lt;- function(p, n, z = 1.96){\n  se &lt;- sqrt(p * (1 - p) / n)\n  tibble(lower = p - z * se, upper = p + z * se)\n}\n\nwilson_ci &lt;- function(p, n, z = 1.96){\n  z2     &lt;- z^2\n  denom  &lt;- 1 + z2 / n\n  center &lt;- (p + z2 / (2 * n)) / denom\n  half   &lt;- (z / denom) * sqrt( (p * (1 - p) / n) + z2 / (4 * n^2) )\n  tibble(lower = center - half, upper = center + half)\n}\n\nci_fun &lt;- switch(tolower(method),\n  \"wilson\" = wilson_ci,\n  \"wald\"   = wald_ci,\n  stop(\"method must be 'wilson' or 'wald'\")\n)\n\ncis &lt;- map_dfr(seq_along(phat), function(i){\n  out &lt;- ci_fun(phat[i], n, z)\n  tibble(sample_id = i, phat = phat[i]) %&gt;% bind_cols(out)\n}) %&gt;%\n  mutate(lower = pmax(0, lower), upper = pmin(1, upper),      # clip to [0,1]\n         cover = (lower &lt;= p_true & p_true &lt;= upper))\n\nggplot(cis, aes(y = reorder(factor(sample_id), phat))) +\n  geom_errorbarh(aes(xmin = lower, xmax = upper, color = cover), height = 0) +\n  geom_point(aes(x = phat, color = cover)) +\n  geom_vline(xintercept = p_true, linetype = \"dashed\") +\n  labs(x = \"Estimated proportion (p̂)\",\n       y = \"Sample\",\n       color = \"Interval covers 0.5?\",\n       title = paste(\"95% confidence intervals (\", toupper(method), \")\", sep = \"\"),\n       subtitle = paste(m, \"independent samples; n =\", n, \"per sample\")) +\n  xlim(0, 1)\n\n\n\n\nSampling variability: 30 repeated samples, n = 10 each. Horizontal bars are 95% CIs; dashed line is the true p = 0.5.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOperational meaning of “95%”. It is a long-run frequency statement: if we were to repeat the entire sampling and interval-construction procedure many times, approximately 95% of the resulting intervals would contain the true p.\nLet B be the number of intervals constructed with a method whose true coverage is c. Then the number that cover, K, satisfies\n\nK \\sim \\mathrm{Binomial}(B,\\,c), \\qquad \\mathbb{E}[K]=Bc,\\quad \\mathrm{SD}(K)=\\sqrt{Bc(1-c)}.\n\nFor B=30 and c=0.95, \\mathbb{E}[K]=28.5, \\mathrm{SD}\\approx 1.19; seeing 26–30 covers is entirely plausible.\n\n\n\n\n\n\n\n\nTip\n\n\n\nSmall-sample caveat. The Wald interval \\hat p \\pm 1.96\\,\\mathrm{SE} often under-covers when n is small or p is near 0 or 1. Prefer Wilson (or Agresti–Coull) for small–moderate n, or increase n.\n\n\n\n\nWhat a 95% CI Does—and Does Not—Say\n\nLong-run reliability, not a single-case probability. For a given dataset, an interval either contains p or it does not; “95%” refers to the method’s performance across repetitions, not to the probability that this specific interval contains p.\nBatch variability is expected. In a finite batch of intervals, the count that cover will vary randomly around its expectation.\nMethod matters. Coverage depends on the interval procedure (e.g., Wilson vs. Wald) and on n and p.\n\n\n\nUnderstanding Confidence Intervals Through Simulation\nThe following simulation demonstrates the true meaning of “95% confidence”:\n\n# Simulate taking many polls to see how often CIs work\nset.seed(123)\ntrue_prop &lt;- 0.52    # The true population proportion\nn_polls &lt;- 100       # Number of simulated polls\nsample_size &lt;- 1000  # Each poll surveys 1000 people\n\n# Simulate the polls\nsample_props &lt;- rbinom(n_polls, sample_size, true_prop) / sample_size\n\n# Calculate confidence interval for each poll\nse &lt;- sqrt(sample_props * (1 - sample_props) / sample_size)\nci_lower &lt;- sample_props - 1.96 * se\nci_upper &lt;- sample_props + 1.96 * se\n\n# Check which intervals contain the true value\nci_data &lt;- data.frame(\n  poll_id = 1:n_polls,\n  estimate = sample_props,\n  ci_lower = ci_lower,\n  ci_upper = ci_upper,\n  contains_truth = (ci_lower &lt;= true_prop) & (true_prop &lt;= ci_upper)\n)\n\n# Count how many intervals contain the truth\ncoverage &lt;- mean(ci_data$contains_truth) * 100\ncat(\"Percentage of CIs containing true value:\", coverage, \"%\\n\")\n\nPercentage of CIs containing true value: 93 %\n\n# Visualize first 30 polls\nggplot(ci_data[1:30, ], aes(x = poll_id, y = estimate, \n                             color = contains_truth)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), \n                width = 0.3, alpha = 0.7) +\n  geom_hline(yintercept = true_prop, color = \"black\", \n             linetype = \"solid\", linewidth = 1) +\n  scale_color_manual(values = c(\"FALSE\" = \"red\", \"TRUE\" = \"blue\"),\n                     labels = c(\"Misses truth\", \"Contains truth\")) +\n  labs(\n    title = \"30 Polls with 95% Confidence Intervals\",\n    subtitle = paste(\"Black line shows true value. About 95% of intervals should contain it.\",\n                     \"\\nIn this simulation:\", sum(ci_data$contains_truth[1:30]), \n                     \"out of 30 contain the true value.\"),\n    x = \"Poll Number\",\n    y = \"Estimated Support\",\n    color = \"Interval Status\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0.4, 0.6)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nInterpretation of Results:\n\nEach horizontal line segment represents one poll’s confidence interval\nBlue intervals contain the true value (black line)\nRed intervals miss the true value\nOver many polls, approximately 95% of intervals contain the truth\nFor any single poll, we cannot determine whether our interval captures the true value\n\n\n\nFactors Affecting Uncertainty\n\n# Show how sample size affects margin of error\nsample_sizes &lt;- c(100, 250, 500, 1000, 2000, 4000, 10000)\np &lt;- 0.5  # Use 0.5 as worst-case (maximum uncertainty)\n\n# Calculate margin of error for each sample size\nmargins &lt;- 1.96 * sqrt(p * (1 - p) / sample_sizes)\n\nmargin_data &lt;- data.frame(\n  n = sample_sizes,\n  margin = margins * 100  # Convert to percentage\n)\n\nggplot(margin_data, aes(x = n, y = margin)) +\n  geom_line(color = \"darkblue\", linewidth = 1.5) +\n  geom_point(color = \"darkblue\", size = 3) +\n  geom_hline(yintercept = 3, linetype = \"dashed\", \n             color = \"red\", alpha = 0.7) +\n  scale_x_log10(breaks = sample_sizes,\n                labels = scales::comma) +\n  labs(\n    title = \"How Sample Size Affects Precision\",\n    subtitle = \"Larger samples yield smaller margins of error (note logarithmic x-axis)\",\n    x = \"Sample Size\",\n    y = \"Margin of Error (%)\"\n  ) +\n  theme_minimal() +\n  annotate(\"text\", x = 5000, y = 3.3, \n           label = \"±3% (common target)\", \n           color = \"red\", size = 3) +\n  annotate(\"text\", x = 100, y = 10.5, \n           label = paste(\"n=100: ±\", round(margins[1]*100, 1), \"%\"), \n           size = 3, hjust = 0) +\n  annotate(\"text\", x = 10000, y = 1.5, \n           label = paste(\"n=10,000: ±\", round(margins[7]*100, 1), \"%\"), \n           size = 3, hjust = 1)\n\n\n\n\n\n\n\n\nKey Mathematical Relationships:\n\nDoubling the sample size does not halve the margin of error\nTo reduce the margin of error by half requires quadrupling the sample size\nDiminishing returns: The reduction from n=100 to n=1,000 is more substantial than from n=1,000 to n=10,000\n\n\n\nPractical Guidelines for Working with Uncertainty\nWhen interpreting results with uncertainty:\n\nReport confidence intervals alongside point estimates\n\nInsufficient: “52% support the candidate”\nAppropriate: “52% support the candidate (95% CI: 49%-55%)”\n\nAssess whether differences exceed sampling variability\n\nIf Poll A shows 52% and Poll B shows 51%, the difference may reflect sampling variation\nExamine whether confidence intervals overlap\n\nAcknowledge unmeasured sources of error\n\nConfidence intervals capture only sampling error\nThey do not account for question bias, coverage errors, or response inaccuracies\n\nPrioritize study quality over sample size\n\nA well-designed study with 1,000 respondents surpasses a biased study with 100,000\nMethodological rigor matters more than sample size alone\n\n\n\n\nSummary\nStatistical uncertainty is an inherent feature of empirical research in political science. Key principles include:\n\nSampling error is predictable, quantifiable, and decreases with larger samples\nNon-sampling errors pose greater threats because they persist regardless of sample size\nStandard error measures the typical variation in estimates across samples\nConfidence intervals provide ranges of plausible values with specified coverage probabilities\nUncertainty quantification should always accompany statistical estimates to enable proper interpretation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-c-randomness-the-foundation-of-statistical-inference",
    "href": "chapter1.html#appendix-c-randomness-the-foundation-of-statistical-inference",
    "title": "1  Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.50 Appendix C: Randomness: The Foundation of Statistical Inference (*)",
    "text": "1.50 Appendix C: Randomness: The Foundation of Statistical Inference (*)\n\nWhat is Randomness?\nIn statistics, randomness means structured uncertainty: single outcomes are uncertain, but their long‑run frequencies follow known probabilities.\nTwo core properties:\n\nSingle‑case unpredictability — we can’t say whether a particular voter will turn out.\nAggregate regularity — we can say that about 60% of voters will turn out (with quantifiable uncertainty).\n\n\n\nPredictable Frequencies (Law of Large Numbers)\nAn individual random event is unpredictable. If we know the probability distribution, the pattern across many trials is predictable.\n\nExample (two dice): any one throw is uncertain, but sums follow a fixed distribution: 4 has 3/36 outcomes (≈8.3%), 7 has 6/36 (≈16.7%). Over many throws, 7 appears about twice as often as 4. The law of large numbers drives empirical frequencies toward these probabilities.\n\n\nKey idea: Randomness ≠ mess. Each trial is uncertain, yet the distribution is stable in the long run.\n\n\n\nShannon Entropy: Measuring Uncertainty\n\nWhat is Shannon Entropy?\nShannon entropy quantifies how much uncertainty or “surprise” exists in a random process. It measures the average amount of information needed to describe an outcome.\nThe formula for Shannon entropy H is:\nH = -\\sum_{i} p_i \\log_2(p_i)\nwhere p_i is the probability of outcome i, and we measure entropy in bits.\n\n\nSimple Dice Examples\nExample 1: Fair Die - Six equally likely outcomes: each has probability p = 1/6 - Entropy: H = -6 \\times \\frac{1}{6} \\log_2(\\frac{1}{6}) = \\log_2(6) \\approx 2.58 bits - Interpretation: Maximum uncertainty—we need about 2.58 bits of information to describe each roll\nExample 2: Loaded Die (Always Shows 6) - One outcome with probability 1, others with probability 0 - Entropy: H = -1 \\times \\log_2(1) = 0 bits - Interpretation: No uncertainty—we already know the outcome!\nExample 3: Partially Loaded Die - Suppose: 6 appears 50% of the time, other faces each 10% - H = -0.5 \\log_2(0.5) - 5 \\times 0.1 \\log_2(0.1) - H = 0.5 + 5 \\times 0.332 = 2.16 bits - Interpretation: Less uncertainty than a fair die, but more than the fully loaded die\n\nStudent Tip: Higher entropy = more uncertainty = harder to predict. A fair die has maximum entropy; a rigged die has lower entropy.\n\n\n\nPolling Example: Measuring Electoral Uncertainty\nConsider three different polling scenarios for a two-candidate race:\nScenario A: Dead Heat - Candidate 1: 50%, Candidate 2: 50% - H = -0.5 \\log_2(0.5) - 0.5 \\log_2(0.5) = 1 bit - Maximum uncertainty for a binary choice\nScenario B: Clear Leader - Candidate 1: 70%, Candidate 2: 30% - H = -0.7 \\log_2(0.7) - 0.3 \\log_2(0.3) \\approx 0.88 bits - Less uncertainty—the outcome is more predictable\nScenario C: Landslide - Candidate 1: 95%, Candidate 2: 5% - H = -0.95 \\log_2(0.95) - 0.05 \\log_2(0.05) \\approx 0.29 bits - Very low uncertainty—the outcome is nearly certain\n\n\nEntropy in Multi-Candidate Races\nExample: Three-way race with different distributions\n\n\n\n\n\n\n\n\n\n\n\nDistribution Type\nCandidate A\nCandidate B\nCandidate C\nEntropy (bits)\nInterpretation\n\n\n\n\nEqual split\n33.3%\n33.3%\n33.3%\n1.58\nMaximum uncertainty\n\n\nTwo-way race\n45%\n45%\n10%\n1.36\nHigh uncertainty\n\n\nClear favorite\n60%\n25%\n15%\n1.30\nModerate uncertainty\n\n\nDominant leader\n80%\n15%\n5%\n0.88\nLow uncertainty\n\n\n\n\n\nWhy Entropy Matters in Statistics\n\nSurvey Design: Higher entropy in responses means we need larger samples for the same precision\nInformation Gain: In decision trees and machine learning, we choose variables that maximize entropy reduction\nCommunication Efficiency: Entropy tells us the theoretical minimum bits needed to encode messages\nUncertainty Quantification: Provides a single number summarizing distributional spread\n\n\n\nInteractive Intuition: The 20 Questions Game\nShannon entropy connects to the “20 Questions” game: - For a fair die (entropy ≈ 2.58 bits): You need about 3 yes/no questions on average - For a coin flip (entropy = 1 bit): You need exactly 1 question - For a loaded die that always shows 6 (entropy = 0): You need 0 questions!\n\nPractice Problem: A weather forecast gives: Sunny (60%), Cloudy (30%), Rainy (10%). Calculate the Shannon entropy. What does this tell you about the predictability of tomorrow’s weather?\n\n\n\n\nRandomness, Chaos, Entropy, Haphazardness (at a glance)\n\n\n\n\n\n\n\n\n\nConcept\nWhat it is\nWhy it can look unpredictable\nExample\n\n\n\n\nRandomness\nOutcomes uncertain; probabilities known\nIndividual draws vary; frequencies stabilize\nDice, coin flips\n\n\nChaos\nDeterministic dynamics, sensitive to initial conditions\nTiny changes → big divergences\nWeather, double pendulum\n\n\nEntropy\nMeasure of uncertainty/disorder (info/thermo)\nHigher when outcomes are more spread\nShannon entropy of a fair vs. loaded die\n\n\nHaphazardness\nInformal appearance of disorder\nMay lack any governing model\nClutter on a desk\n\n\n\n\n\nWhy Randomness Matters\n\nRandom sampling\n\nLimits systematic bias in surveys\nLets us compute margins of error and confidence intervals\n\nRandom assignment (experiments)\n\nBalances observed and unobserved factors on average\nEnables credible causal inference\n\n\n\n\nThe Power of Random Sampling (quick demo)\nIf we randomly sample 1,000 voters and estimate 55% support, statistics tells us:\n\nThe population support is likely close to 55%,\nWe can quantify “how close” (often ≈ ±3 pp at 95%),\nBecause large‑sample randomness follows predictable patterns.\n\n\n# Self-contained demo (no external packages beyond ggplot2)\nset.seed(42)\n\n# Create a synthetic \"population\" with a known proportion\npopulation_support &lt;- c(rep(\"A\", 5200), rep(\"B\", 4800))\ntrue_support_A &lt;- mean(population_support == \"A\")\n\n# Function: take a simple random sample and compute support for A\nsample_support &lt;- function(n) {\n  mean(sample(population_support, n) == \"A\")\n}\n\n# Sample sizes and repeated draws\nsample_sizes &lt;- c(50, 100, 500, 1000)\nresults_list &lt;- lapply(sample_sizes, function(n) {\n  est &lt;- replicate(100, sample_support(n))\n  data.frame(sample_size = factor(n), estimate = est, true_value = true_support_A)\n})\nresults &lt;- do.call(rbind, results_list)\n\nlibrary(ggplot2)\n\nggplot(results, aes(x = sample_size, y = estimate)) +\n  geom_boxplot(alpha = 0.7, fill = \"lightblue\") +\n  geom_hline(yintercept = true_support_A, color = \"red\", linetype = \"dashed\", linewidth = 0.8) +\n  labs(\n    title = \"Random Sampling Converges to the Truth as n Increases\",\n    subtitle = \"Red dashed line = true population value (52%)\",\n    x = \"Sample size\",\n    y = \"Estimated support for A\",\n    caption = \"Each box summarizes 100 random samples\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nReading the figure. Boxes show the middle 50% of estimates; the line is the median. As n grows, the boxes narrow: variability from randomness shrinks, and estimates settle near the true value.\n\n\nBottom line: Randomness underpins statistical inference: it turns uncertainty in individual outcomes into predictable distributions for estimates, letting us measure error, build intervals, and test hypotheses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html",
    "href": "rozdzial1.html",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "",
    "text": "2.1 Czym jest statystyka?\nStatystyka dostarcza uporządkowanego, opartego na zasadach sposobu uczenia się z danych, gdy wyniki są zmienne, a informacje niepełne.\nObejmuje: (1) opis — klarowne podsumowania i wizualizacje; (2) wnioskowanie — szacowanie nieznanych wielkości populacyjnych wraz z niepewnością; oraz (3) predykcję i decyzje — prognozowanie i wybór działań w warunkach ryzyka.\nW praktyce politologicznej: (i) pomiar skali przewagi urzędujących (incumbency advantage), (ii) szacowanie, jak zdarzenia, reformy czy kampanie wpływają na frekwencję, oraz (iii) przekształcanie wyników sondaży w rozkłady/zakresy prawdopodobnych udziałów głosów.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#czym-jest-statystyka",
    "href": "rozdzial1.html#czym-jest-statystyka",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "",
    "text": "Note\n\n\n\nCo statystyka pomaga zrobić - Opisywać: zwięzłe podsumowania i wykresy\n- Wnioskować: szacować wielkości populacyjne + niepewność (SE — odchylenie standardowe oszacowania; CI — przedział ufności)\n- Przewidywać i decydować: prognozować wyniki i wspierać decyzje w warunkach ryzyka",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#losowość-fundament-wnioskowania-statystycznego",
    "href": "rozdzial1.html#losowość-fundament-wnioskowania-statystycznego",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.2 Losowość: fundament wnioskowania statystycznego",
    "text": "2.2 Losowość: fundament wnioskowania statystycznego\n\nCzym jest losowość?\nW statystyce losowość to uporządkowany sposób opisu niepewności: pojedyncze wyniki są nieprzewidywalne, natomiast w długiej serii powtórzeń ujawniają się stabilne prawidłowości (np. częstości, średnie).\nDwie perspektywy\n\nPojedyncza realizacja — nie potrafimy przesądzić, jak zagłosuje konkretny wyborca w danym momencie.\n\nZbiorowość — możemy opisać odsetek wyborców głosujących na daną partię oraz związaną z nim niepewność estymacji.\n\n\n\n\n\n\n\nNote\n\n\n\nLosowość epistemiczna a ontologiczna\n\nEpistemiczna (związana z niewiedzą): wynik traktujemy jako losowy, ponieważ nie obserwujemy wszystkich determinant lub nie kontrolujemy warunków.\nPrzykłady:\n\ndecyzja pojedynczego wyborcy w sondażu (nie znamy pełnych motywacji),\nbłąd pomiaru w ankiecie (ograniczona precyzja, brak odpowiedzi),\nrzut monetą modelowany jako losowy, ponieważ drobne, nieobserwowalne różnice warunków początkowych determinują wynik.\n\nOntologiczna (wrodzona przypadkowość zjawiska): nawet pełna wiedza nie usuwa niepewności wyniku.\nPrzykłady:\n\nczas rozpadu promieniotwórczego atomu.\n\n\n\n\n\n\nDlaczego losowość ma znaczenie\n\nLosowe próbkowanie\n\nOgranicza systematyczny błąd doboru, dzięki czemu próba przypomina populację docelową (w średnim ujęciu / w wartości oczekiwanej).\nUmożliwia ilościowe ujęcie niepewności (np. margines błędu; pojęcie „przedziału ufności” omówimy później), pod warunkiem rzeczywiście losowego doboru i dobrego pokrycia populacji.\n\nLosowy przydział (eksperymenty)\n\nPrzerywa związek między przydziałem a innymi czynnikami, czyniąc grupy porównywalnymi średnio (zarówno pod względem cech obserwowalnych, jak i nieobserwowalnych).\nUmożliwia wiarygodne wnioskowanie przyczynowe (identyfikację średnich efektów przy standardowych założeniach).\n\n\n\n\nSiła losowego próbkowania (krótki pokaz)\nZałóżmy, że losujemy próbę prostą o liczebności n=1000 wyborców i obserwujemy \\hat p = 0{,}55 (tj. 55% poparcia). Wówczas:\n\nNaszą najlepszą, jednowartościową oceną odsetka w populacji p jest \\hat p = 0{,}55.\nOrientacyjny „95\\% zakres wartości plauzybilnych” wokół \\hat p można przybliżyć wzorem \n\\hat p \\,\\pm\\, 2\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\n\\;=\\;\n0{,}55 \\,\\pm\\, 2\\sqrt{\\frac{0{,}55\\cdot 0{,}45}{1000}}\n\\approx\n0{,}55 \\pm 0{,}031,\n czyli w przybliżeniu 52\\%\\text{–}58\\% (około \\pm 3{,}1 punktu procentowego, pp).\nSzerokość tego zakresu maleje wraz z liczebnością próby: \n\\text{szerokość} \\;\\propto\\; \\frac{1}{\\sqrt{n}}.\n Przykładowo, zwiększenie n z 1000 do 4000 mniej więcej zmniejsza błąd o połowę.\n\n\n\n\n\n\n\nNote\n\n\n\nJak interpretować „zakres 95%”?\n\nGdybyśmy wielokrotnie powtarzali ten sam losowy sondaż, w przybliżeniu w 19 na 20 przypadkach obliczony zakres obejmowałby rzeczywisty odsetek w populacji.\nZałożenia: losowy dobór z populacji i porównywalne warunki zbierania danych.\nBłędy nielosowe (braki odpowiedzi, pokrycia, pomiaru) oraz złożone schematy (warstwowanie, klastrowanie, wagi) mogą powodować, że rzeczywista niepewność będzie większa niż wynika to z powyższego przybliżenia.\n\n\n\n\n\nPrawo wielkich liczb (PWL) — ujęcie elementarne\nNiech A oznacza zdarzenie interesujące (np. „głos na partię X”, „suma oczek równa 7”). Jeśli P(A)=p, a obserwujemy n niezależnych prób o jednakowym rozkładzie (tzn. w każdej próbie prawdopodobieństwo zajścia A jest takie samo), to częstość wystąpień zdarzenia A:\n\n\\hat{p}_n=\\frac{\\text{liczba wystąpień }A}{n}\n\nzbliża się do p, gdy n rośnie.\nPrzykład (dwie kości): zdarzenie „suma 7” ma 6/36 \\approx 16{,}7\\%, a „suma 4” ma 3/36 \\approx 8{,}3\\%. W wielu rzutach suma 7 pojawia się około dwukrotnie częściej niż suma 4.\nPrzykład (sondaż wyborczy): jeżeli w populacji odsetek poparcia dla partii wynosi p, to przy losowym doborze próby o liczebności n obserwowana częstość \\hat p_n będzie — wraz ze wzrostem n — coraz bliższa p (przy zachowaniu założeń losowego doboru i niezależności prób).\n\n# Samowystarczalna demonstracja (bez zewnętrznych pakietów poza ggplot2)\nset.seed(42)\n\n# Tworzymy syntetyczną \"populację\" o znanym udziale\npopulation_support &lt;- c(rep(\"A\", 5200), rep(\"B\", 4800))\ntrue_support_A &lt;- mean(population_support == \"A\")\n\n# Funkcja: proste losowe próbkowanie i wyliczenie poparcia dla A\nsample_support &lt;- function(n) {\n  mean(sample(population_support, n) == \"A\")\n}\n\n# Wielkości prób i wielokrotne losowania\nsample_sizes &lt;- c(50, 100, 500, 1000)\nresults_list &lt;- lapply(sample_sizes, function(n) {\n  est &lt;- replicate(100, sample_support(n))\n  data.frame(sample_size = factor(n), estimate = est, true_value = true_support_A)\n})\nresults &lt;- do.call(rbind, results_list)\n\nlibrary(ggplot2)\n\nggplot(results, aes(x = sample_size, y = estimate)) +\n  geom_boxplot(alpha = 0.7, fill = \"lightblue\") +\n  geom_hline(yintercept = true_support_A, color = \"red\", linetype = \"dashed\", linewidth = 0.8) +\n  labs(\n    title = \"Losowy dobór próby zbiega do prawdy wraz ze wzrostem n\",\n    subtitle = \"Czerwona linia przerywana = prawdziwa wartość populacyjna (52%)\",\n    x = \"Wielkość próby\",\n    y = \"Szacowane poparcie dla A\",\n    caption = \"Każde pudełko podsumowuje 100 losowych prób\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nJak czytać wykres. Pudełka pokazują środkowe 50% oszacowań; linia w środku to mediana. Gdy n rośnie, pudełka się zwężają: zmienność wynikająca z losowości maleje, a oszacowania koncentrują się wokół prawdziwej wartości.\n\n\nSedno: PWL daje nam gwarancję, że „szum” pojedynczych wyników średnio się wygładza, więc możemy przewidywać częstości, mierzyć błąd i wiarygodnie wnioskować — w sondażach, eksperymentach i (w sensie częstości) także w zjawiskach kwantowych.\n\n\nLosowość, chaos, entropia i „przypadkowość” (w pigułce)\n\n\n\n\n\n\n\n\n\nPojęcie\nCo to jest?\nŹródło nieprzewidywalności\nPrzykład\n\n\n\n\nLosowość\nWyniki pojedynczych realizacji są niepewne, ale rozkład prawdopodobieństwa jest znany lub modelowany.\nFluktuacje między realizacjami; brak informacji o konkretnym wyniku.\nRzut kośćmi, rzut monetą\n\n\nChaos\nDeterministyczna dynamika wrażliwa na warunki początkowe (efekt motyla).\nMinimalne różnice początkowe rosną szybko → duże rozbieżności trajektorii.\nPogoda, podwójne wahadło, mapa logistyczna\n\n\nEntropia\nMiara niepewności/rozproszenia rozkładu (informacyjna lub termodynamiczna).\nWiększa, gdy wyniki są bardziej wyrównane (mniej informacji przewidywalnej).\nEntropia Shannona\n\n\n„Przypadkowość” (potoczna)\nOdczucie braku ładu bez jawnego modelu; mieszanina mechanizmów.\nBrak ustrukturyzowanego opisu i reguł; wiele nakładających się procesów.\nBałagan na biurku\n\n\nLosowość kwantowa\nPojedynczy wynik nie jest zdeterminowany; znany jest jedynie rozkład (reguła Borna).\nFundamentalna (ontologiczna) niedookreśloność pojedynczego pomiaru.\nSpin elektronu, polaryzacja fotonu\n\n\n\n\nUwaga: Chaos deterministyczny ≠ losowość. System jest w pełni deterministyczny, lecz praktycznie nieprzewidywalny ze względu na ekstremalną wrażliwość na warunki początkowe. Losowość natomiast modeluje niepewność przez rozkłady prawdopodobieństwa.\n\n\n\nA mechanika kwantowa?\nW ujęciu kopenhaskim losowość jest fundamentalna (ontologiczna): pojedynczego wyniku nie da się przewidzieć, ale znany jest rozkład prawdopodobieństwa pomiaru zgodnie z regułą Borna, \nP(\\text{wynik}) \\propto \\lvert \\psi \\rvert^{2}.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wnioskowanie-statystyczne-przykład-wprowadzający",
    "href": "rozdzial1.html#wnioskowanie-statystyczne-przykład-wprowadzający",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.3 Wnioskowanie statystyczne: przykład wprowadzający",
    "text": "2.3 Wnioskowanie statystyczne: przykład wprowadzający\n\n\n\n\n\n\nNote\n\n\n\nZasada fundamentalna: Statystyka nie eliminuje niepewności — pomaga ją mierzyć, zarządzać nią i rzetelnie komunikować.\n\n\n\n\n\n\n\n\nPrzykład historyczny: sondaż „Literary Digest” z 1936 r.\n\n\n\nTygodnik Literary Digest przeprowadził jeden z największych sondaży w historii — zebrał ok. 2,4 mln odpowiedzi — i przewidział zwycięstwo Alfa Landona nad Franklinem D. Rooseveltem. Mimo ogromnej liczebności próby:\n\nPrognoza: Landon 57%, Roosevelt 43%\nRzeczywisty wynik: Roosevelt 62%, Landon 38%\n\nCo poszło nie tak? Wystąpiło zafałszowanie doboru (selection bias):\n\nRama doboru próby: książki telefoniczne, rejestry samochodowe, listy członków klubów\nW 1936 r. źródła te nadreprezentowały zamożniejszych Amerykanów, skłonnych popierać Landona\nBłąd braku odpowiedzi: odpowiedziało tylko ok. 24% osób, prawdopodobnie częściej o silnych, anty-rooseveltowskich poglądach\n\nWniosek kluczowy: Duża, ale stronnicza próba jest gorsza niż mała, reprezentatywna. Błędy standardowe mierzą jedynie błąd losowy (wariancję doboru), a nie błąd systematyczny (bias).\n\n\n\nPytanie badawcze: Jaki odsetek studentów popiera całodobowe (24/7) otwarcie biblioteki?\nWyzwanie:\n\nPopulacja: 20 000 studentów uczelni\nOgraniczenie praktyczne: można przepytać jedynie 100 osób\nProblem: różne próby dadzą różne wyniki\n\nBez myślenia statystycznego: „60 ze 100 osób odpowiedziało ‘tak’, zatem poparcie wynosi 60%.”\nZ myśleniem statystycznym: „Szacujemy poparcie na 60% z marginesem błędu ±10%. Z rozsądnym poziomem ufności prawdziwy odsetek leży między 50% a 70%.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#podstawowe-pojęcia",
    "href": "rozdzial1.html#podstawowe-pojęcia",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.4 Podstawowe pojęcia",
    "text": "2.4 Podstawowe pojęcia\n\n1. Estymator punktowy\nGdy 60 ze 100 ankietowanych studentów popiera propozycję:\n\\hat{p} = \\frac{60}{100} = 0{,}60\nTen estymator punktowy to nasza najlepsza pojedyncza ocena wartości w populacji na podstawie wylosowanej próby.\n\n\n\n\n\n\nSłowniczek w pigułce (bez matematyki)\n\n\n\n\nEstymator punktowy: pojedyncza liczba z próby (np. 60%).\nBłąd standardowy (SE): typowe wahanie tej liczby, gdybyśmy powtarzali to samo badanie wiele razy.\nMargines błędu (MoE): bufor dodawany do estymatora, by utworzyć przedział uwzględniający zwykłe wahania losowe.\nPrzedział ufności (CI): estymator ± bufor (np. 60% ± 3%) — zakres, który zwykle obejmuje prawdę, jeśli powtarzamy tę samą procedurę.\n\n\n\n\n\n2. Jak mierzymy niepewność\nNa precyzję wyniku wpływa głównie:\n\nWielkość próby Większa próba → mniejsze wahania. Zasada kciuka: MoE ≈ 1/√n (dla pytań tak/nie blisko 50/50 przy prostym losowaniu).\nJak „wyrównane” są opinie Wyniki blisko 50–50 są najtrudniejsze do uchwycenia (większy MoE). Gdy prawie wszyscy się zgadzają (np. 90–10), próby są stabilniejsze.\nProjekt badania i ważenie Warstwowanie, klastrowanie i silne wagi zwykle zwiększają niepewność — realnie zmniejszają efektywną liczebność próby.\n\n\n\n\n\n\n\nCo naprawdę oznacza „margines błędu”\n\n\n\nTo plus–minus wynikający wyłącznie z tego, że badamy próbę, a nie cały spis.\nJeśli sondaż podaje 52% z ±3% (95% ufności), czytaj to tak:\n\nPrzy zwykłej losowej zmienności prawdziwe poparcie jest prawdopodobnie między 49% a 55%.\n\nNie obejmuje to błędów typu tendencyjne pytania, zła rama doboru, brak odpowiedzi czy błędne pomiary. To są błędy poza marginesem błędu.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zależność-między-liczebnością-próby-a-precyzją",
    "href": "rozdzial1.html#zależność-między-liczebnością-próby-a-precyzją",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.5 Zależność między liczebnością próby a precyzją",
    "text": "2.5 Zależność między liczebnością próby a precyzją\n\n\n\n\n\n\nStandardowe marginesy błędu (bardzo orientacyjnie)\n\n\n\n\n\n\nWielkość próby\nPrzybliżony MoE (95% dla udziału ~50/50)\n\n\n\n\nn = 100\n± 10%\n\n\nn = 400\n± 5%\n\n\nn = 1 000\n± 3%\n\n\nn = 2 500\n± 2%\n\n\nn = 10 000\n± 1%\n\n\n\nWzór ogólny: żeby zmniejszyć MoE o połowę, trzeba 4× zwiększyć próbę. Malejące korzyści są realne.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTe szybkie wartości zakładają proste losowanie i małe/żadne ważenie. Złożone projekty lub duże wagi zmniejszają efektywne n → MoE rośnie.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#trzy-ilustracyjne-przykłady",
    "href": "rozdzial1.html#trzy-ilustracyjne-przykłady",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.6 Trzy ilustracyjne przykłady",
    "text": "2.6 Trzy ilustracyjne przykłady\n\nPrzykład 1: Mała próba (n = 25)\n\nWynik: 15 z 25 popiera → 60%\nPrzybliżony MoE: ± 20%\nInterpretacja: Prawdziwe poparcie może być 40%–80%. Dobre na wczesny sygnał, nie do precyzyjnych decyzji.\n\n\n\nPrzykład 2: Średnia próba (n = 100)\n\nWynik: 60 ze 100 → 60%\nPrzybliżony MoE: ± 10%\nInterpretacja: Prawdopodobnie 50%–70%. Daje ogólny obraz, ale wciąż zbyt szeroko na „wyrównane wyścigi”.\n\n\n\nPrzykład 3: Duża próba (n = 1 000)\n\nWynik: 600 z 1 000 → 60%\nPrzybliżony MoE: ± 3%\nInterpretacja: Prawdopodobnie 57%–63%. Wystarczająco wąsko do wielu praktycznych zastosowań.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#jak-rozumieć-poziom-ufności",
    "href": "rozdzial1.html#jak-rozumieć-poziom-ufności",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.7 Jak rozumieć poziom ufności",
    "text": "2.7 Jak rozumieć poziom ufności\n\nCo oznacza „95% ufności” (intuicyjnie)\nWyobraź sobie, że powtarzasz to samo badanie 100 razy — te same pytania i procedury, ale za każdym razem inna losowa próba:\n\nZa każdym razem estymator trochę się waha. Dodajesz zwykły bufor (MoE), tworząc przedział: CI = estymator ± MoE.\nOkoło 95 z 100 takich przedziałów obejmie stałą, nieznaną prawdę o populacji; około 5 nie trafi — czysty przypadek.\nDla Twojego jednego badania przedział (np. 50% ± 3% → 47%–53%) albo obejmuje prawdę, albo nie — tu już nie ma dodatkowej losowości. 95% dotyczy niezawodności metody w długiej serii powtórzeń, a nie prawdopodobieństwa dla tego konkretnego przedziału.\n\n\n\n\n\n\n\nImportant\n\n\n\nWyższy poziom ufności = szerszy przedział Jeśli chcesz większej pewności (np. 99%), musisz użyć większego bufora (szerszego CI). Więcej pewności → mniej precyzji.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#praktyczne-wskazówki-rób-nie-rób",
    "href": "rozdzial1.html#praktyczne-wskazówki-rób-nie-rób",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.8 Praktyczne wskazówki (rób / nie rób)",
    "text": "2.8 Praktyczne wskazówki (rób / nie rób)\n\nRób: podawaj zarówno estymator, jak i jego niepewność (CI lub MoE).\nRób: wspominaj o ważnych elementach projektu (klastry, wagi), które zwiększają niepewność.\nNie rób: nie traktuj MoE jako „parasol” na błędy systematyczne (rama, nonresponse, sugestywne pytania). MoE dotyczy tylko losowej zmienności próby.\nNie rób: nie wyciągaj daleko idących wniosków z różnic mniejszych niż MoE.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#opcjonalnie-dla-wglądu",
    "href": "rozdzial1.html#opcjonalnie-dla-wglądu",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.9 Opcjonalnie: „dla wglądu”",
    "text": "2.9 Opcjonalnie: „dla wglądu”\n\nFormuły (opcjonalnie)\n\nZasada kciuka: MoE (95% dla udziałów blisko 50/50) ≈ 1/√n.\nSE łączy estymator z MoE: w przybliżeniu MoE ≈ 2 × SE (przy 95%).\nTypowe SE (dla orientacji):\n\nUdział: SE(\\hat p) = \\sqrt{\\hat p(1-\\hat p)/n}\nŚrednia: SE(\\bar x) = s/\\sqrt{n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#kiedy-mówienie-o-klasycznym-moe-jest-mylące",
    "href": "rozdzial1.html#kiedy-mówienie-o-klasycznym-moe-jest-mylące",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.10 Kiedy mówienie o „klasycznym” MoE jest mylące",
    "text": "2.10 Kiedy mówienie o „klasycznym” MoE jest mylące\n\nPróby wygodne / zgłoszeniowe (brak prawdziwego losowania)\nNieznane lub bardzo złożone ważenie, którego nie uwzględniasz w analizie\nSilny brak odpowiedzi, który zniekształca strukturę odpowiedzi\n\nWciąż możesz wyliczyć jakąś liczbę, ale nazywanie jej klasycznym MoE bywa mylące, jeśli nie pokażesz, że projekt dobrze przybliża losowanie lub nie zastosujesz uzasadnionej efektywnej liczebności próby.\n\n\nJedno zdanie dla studentów\nMargines błędu mówi, o ile wynik z próby może różnić się od prawdy w populacji tylko dlatego, że badamy próbę. SE to leżące u podstaw „typowe wahanie”. Przedział ufności 95% to estymator ± bufor — metoda, która w długiej serii powtórzeń najczęściej obejmuje prawdę.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wizualizacja-zmienności-próbkowania",
    "href": "rozdzial1.html#wizualizacja-zmienności-próbkowania",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.11 Wizualizacja zmienności próbkowania",
    "text": "2.11 Wizualizacja zmienności próbkowania\nOto polska wersja Twojego przykładu — z poprawionym liczeniem marginesu błędu, wizualnym pokazaniem pokrycia (czy przedział obejmuje prawdę), bez żargonu o rozkładach.\n\nlibrary(ggplot2)\nset.seed(42)\n\n# Parametry\nn_polls      &lt;- 20\nn_people     &lt;- 100\ntrue_support &lt;- 0.50\n\n# Symulacja niezależnych sondaży (liczba \"za\" -&gt; odsetek)\nsupport &lt;- rbinom(n_polls, n_people, true_support) / n_people\n\n# Błąd standardowy dla odsetka (oszacowanie \"plug-in\" z danego sondażu)\nse  &lt;- sqrt(support * (1 - support) / n_people)\n\n# „95%” margines błędu ≈ 2 × SE (praktyczna reguła kciuka, bez żargonu)\nmoe &lt;- 2 * se\n\n# Ograniczenie przedziałów do [0, 1], żeby nie wychodziły poza możliwy zakres\nlower &lt;- pmax(0, support - moe)\nupper &lt;- pmin(1, support + moe)\n\n# Czy przedział obejmuje prawdziwą wartość?\ncovers  &lt;- (lower &lt;= true_support) & (upper &gt;= true_support)\nn_cover &lt;- sum(covers)\nn_miss  &lt;- n_polls - n_cover\n\nresults &lt;- data.frame(\n  poll = seq_len(n_polls),\n  support, se, moe, lower, upper, covers\n)\n\n# Wykres\nggplot(results, aes(x = poll, y = support, color = covers)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.3, alpha = 0.8) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = true_support, linetype = \"dashed\") +\n  scale_color_manual(\n    values = c(\"TRUE\" = \"forestgreen\", \"FALSE\" = \"darkorange\"),\n    labels = c(\"TRUE\" = \"Obejmuje prawdę\", \"FALSE\" = \"Mija się z prawdą\"),\n    name   = NULL\n  ) +\n  coord_cartesian(ylim = c(0, 1)) +\n  labs(\n    title    = \"Zmienność wyników w 20 niezależnych sondażach\",\n    subtitle = paste0(\n      \"Każdy sondaż obejmuje \", n_people, \" osób. Prawdziwa wartość = \",\n      scales::percent(true_support),\n      \". Przedziały obejmujące prawdę: \", n_cover, \"/\", n_polls,\n      \" (\", round(100 * n_cover / n_polls), \"%).\"\n    ),\n    x = \"Numer sondażu\",\n    y = \"Oszacowany odsetek\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nKluczowa obserwacja: Każda próba daje inny wynik, ale większość oszacowań i ich przedziałów skupia się wokół prawdziwej wartości; kilka „pudłuje” wyłącznie z powodu losowości doboru próby.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#błędy-statystyczne-przewodnik",
    "href": "rozdzial1.html#błędy-statystyczne-przewodnik",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.12 Błędy statystyczne: przewodnik",
    "text": "2.12 Błędy statystyczne: przewodnik\n\nPo co to wiedzieć\nZnajomość źródeł błędów pomaga:\n\nProjektować lepsze badania i pomiary\nInterpretować oszacowania w sposób poprawny\nRaportować ograniczenia w sposób przejrzysty",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#dwie-główne-rodziny-błędów",
    "href": "rozdzial1.html#dwie-główne-rodziny-błędów",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.13 Dwie główne rodziny błędów",
    "text": "2.13 Dwie główne rodziny błędów\n\n1) Błąd losowy (random error; zmienność estymacji/sampling variability)\nNieprzewidywalne „górki i dołki” wynikające z przypadku (np. którymi osobami losowo dysponujemy, szum dnia codziennego).\n\nDaje się kwantyfikować teorią statystyki (np. błąd standardowy — standard error, SE; przedział ufności — confidence interval, CI; margines błędu — margin of error, MoE)\nMaleje wraz ze wzrostem liczebności próby n\nOgraniczamy go zwiększając n, stosując efektywne estymatory i dobre schematy zbierania danych\n\n\n\n2) Błąd systematyczny (bias; systematic error)\nTrwałe „przesunięcie” wyniku od prawdy, spowodowane projektem badania lub pomiarem.\n\nNie znika po zwiększeniu n\nCzęsto trudny do bezpośredniego wyliczenia prostymi wzorami\nOgraniczamy go przez lepszy projekt, lepszy pomiar i rzetelne zbieranie danych\n\n\n\n\n\n\n\nWarning\n\n\n\nKluczowa myśl: Duża, ale obciążona (biased) próba daje dokładnie błędny wynik. Zwiększaj n, aby zmniejszyć błąd losowy; poprawiaj projekt/pomiar, aby zmniejszyć bias.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#rozkład-biasvariance-mse",
    "href": "rozdzial1.html#rozkład-biasvariance-mse",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.14 Rozkład Bias–Variance (MSE)",
    "text": "2.14 Rozkład Bias–Variance (MSE)\nDla estymatora \\hat\\theta:\n\n\\mathrm{MSE}(\\hat\\theta) \\;=\\; \\underbrace{\\mathrm{Var}(\\hat\\theta)}_{\\text{błąd losowy}} \\;+\\; \\underbrace{\\big(\\mathrm{Bias}(\\hat\\theta)\\big)^2}_{\\text{błąd systematyczny}}.\n\n\nWariancja (variance): jak bardzo wynik „skakałby”, gdybyśmy wiele razy powtarzali badanie (błąd losowy).\nBias: jak daleko średnie oszacowanie jest od prawdy (błąd systematyczny).\nCel: utrzymywać oba składniki na niskim poziomie. Więcej danych zmniejsza wariancję; lepszy projekt zmniejsza bias.\n\n\n\n\n\n\n\nTip\n\n\n\nW predykcji (prediction) drobny bias bywa akceptowalny, jeśli wyraźnie obniża wariancję i przez to MSE (mean squared error). W wnioskowaniu przyczynowym (causal inference) niekontrolowany bias jest zwykle nieakceptowalny.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#częste-źródła-biasu-w-wielu-typach-badań",
    "href": "rozdzial1.html#częste-źródła-biasu-w-wielu-typach-badań",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.15 Częste źródła biasu (w wielu typach badań)",
    "text": "2.15 Częste źródła biasu (w wielu typach badań)\n\nBias selekcji (selection bias) Dane/próba nie reprezentują grupy docelowej. Jak ograniczać: klarownie zdefiniuj populację docelową; gdy to możliwe, używaj losowania probabilistycznego; rozważ wiarygodne ważenie (reweighting).\nBias braku odpowiedzi/utraty (nonresponse/attrition bias) Niektóre typy uczestników częściej nie odpowiadają lub wypadają z badania. Jak ograniczać: zmniejsz obciążenie (krótsze narzędzia), przypomnienia, drobne zachęty; raportuj skalę i wzorce braków.\nBias pomiaru (measurement bias) Systematyczne zniekształcenie w pomiarach (np. źle skalibrowane urządzenie, sugerujące pytanie, stała błędna kategoryzacja). Jak ograniczać: kalibracja, pilotaż i neutralizacja pytań, używanie zwalidowanych skal, „zaślepienie” oceniających (blinding), gdy możliwe.\nBias projektu/identyfikacji (design/identification bias) Problem z interpretacją przyczynową: konfuzja (confounding), błędne warunkowanie (np. kontrola mediatora), współzależność przyczyn i skutków (endogeneity). Jak ograniczać: randomizacja, stratyfikacja/blokowanie, plany analizy przed zbiorem danych (pre-analysis), narzędzia projektowe (np. wykresy DAG), odpowiednie metody (IV, DiD, RDD) przy spełnionych założeniach.\nBias specyfikacji/modelu (model/specification bias) Zła forma funkcji, brak ważnych interakcji, ekstrapolacja poza wsparty zakres danych. Jak ograniczać: diagnostyka relacji, rozsądne alternatywne specyfikacje, sprawdzanie predykcji na nowych danych.\nPrzeuczenie i „wyciek” danych (overfitting & data leakage) Znakomity wynik „na danych treningowych”, który nie trzyma się nowych danych; niezamierzone „dzielenie się” informacją między zbiorem treningowym i testowym. Jak ograniczać: prawdziwy zestaw testowy, walidacja krzyżowa (cross-validation), szczelny pipeline przetwarzania.\nBłędy przetwarzania (processing/pipeline errors) Błędy kodowania, łączenia tabel, jednostek miary. Jak ograniczać: odtwarzalne skrypty, testy i audyty, kontrola wersji.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#jak-czytać-precyzję-bez-dodatkowych-wzorów",
    "href": "rozdzial1.html#jak-czytać-precyzję-bez-dodatkowych-wzorów",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.16 Jak czytać „precyzję” (bez dodatkowych wzorów)",
    "text": "2.16 Jak czytać „precyzję” (bez dodatkowych wzorów)\n\nBłąd standardowy (standard error, SE): średnia „losowa szorstkość” oszacowania; mniejszy SE = większa precyzja.\nPrzedział ufności (confidence interval, CI): zakres, który z ustalonym prawdopodobieństwem ma obejmować prawdziwą wartość (np. 95%).\nMargines błędu (margin of error, MoE): wygodny skrót szerokości niepewności w prostych sytuacjach.\n\n\n\n\n\n\n\nNote\n\n\n\nZwiększanie n zwęża SE, CI i MoE. Nie usuwa jednak biasu.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#testy-hipotez-dwa-typy-pomyłek",
    "href": "rozdzial1.html#testy-hipotez-dwa-typy-pomyłek",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.17 Testy hipotez: dwa typy pomyłek",
    "text": "2.17 Testy hipotez: dwa typy pomyłek\n\nBłąd I rodzaju (Type I error; „fałszywie pozytywny”): ogłaszamy efekt, którego nie ma. Ryzyko kontroluje poziom istotności (significance level; często 5%).\nBłąd II rodzaju (Type II error; „fałszywie negatywny”): nie wykrywamy istniejącego efektu. Moc (power) to szansa wykrycia rzeczywistego efektu (im większa, tym lepiej).\n\nWiele testów naraz (multiple comparisons): im więcej testów, tym większe ryzyko fałszywych alarmów. Warto rozważyć kontrolę odsetka fałszywych odkryć (false discovery rate, FDR).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#dobre-praktyki-w-pigułce",
    "href": "rozdzial1.html#dobre-praktyki-w-pigułce",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.18 Dobre praktyki „w pigułce”",
    "text": "2.18 Dobre praktyki „w pigułce”\n\nPrzed zbiorem danych: zdefiniuj populację docelową, wyniki i porównania; przetestuj narzędzia pomiarowe.\nW trakcie zbierania: minimalizuj obciążenie respondentów, używaj neutralnych sformułowań, notuj skalę/strukturę braków odpowiedzi.\nW analizie: proste diagnostyki, rozsądne alternatywy specyfikacji, ochrona przed przeuczeniem i „wyciekiem”.\nW raporcie: opisz pochodzenie danych, brakujące dane, niepewność (SE/CI/MoE) i prawdopodobny kierunek biasu.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#lista-kontrolna-w-minutę",
    "href": "rozdzial1.html#lista-kontrolna-w-minutę",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.19 Lista kontrolna w minutę",
    "text": "2.19 Lista kontrolna w minutę\n\nBias: czy grozi selekcja, zły pomiar lub błąd projektu (np. kontrola mediatora)?\nWariancja: czy liczebność próby jest rozsądna względem celu?\nModel: czy prostszy/inny model mógłby zmienić wnioski?\nWalidacja: czy działa na nowych lub odłożonych danych?\nTransparentność: czy jasno podano założenia, ograniczenia i możliwy kierunek biasu?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#słowniczek-skrótów-pl-z-angielskimi-odpowiednikami",
    "href": "rozdzial1.html#słowniczek-skrótów-pl-z-angielskimi-odpowiednikami",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.20 Słowniczek skrótów (PL z angielskimi odpowiednikami)",
    "text": "2.20 Słowniczek skrótów (PL z angielskimi odpowiednikami)\n\nSE — błąd standardowy (standard error): średni poziom losowej zmienności oszacowania.\nCI — przedział ufności (confidence interval): zakres mający obejmować prawdę przy zadanym poziomie ufności.\nMoE — margines błędu (margin of error): prosta miara szerokości niepewności w części zastosowań.\nMSE — błąd średniokwadratowy (mean squared error): suma wariancji i kwadratu biasu; łączna miara błędu estymatora.\nFDR — odsetek fałszywych odkryć (false discovery rate): oczekiwany udział fałszywych pozytywów wśród „odkryć”.\nMoc (power): prawdopodobieństwo wykrycia rzeczywistego efektu przy zadanym poziomie istotności.\nPoziom istotności (significance level, \\alpha): akceptowany poziom ryzyka błędu I rodzaju.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#interpretacja-opublikowanych-wyników-sondaży",
    "href": "rozdzial1.html#interpretacja-opublikowanych-wyników-sondaży",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.21 Interpretacja opublikowanych wyników sondaży",
    "text": "2.21 Interpretacja opublikowanych wyników sondaży\n\nJak czytać raport sondażowy\nGdy widzisz: „52% zarejestrowanych wyborców popiera inicjatywę (margines błędu ±3%, n = 1 000)”\nWyciągnij następujące informacje:\n\nEstymata punktowa: 52%\nPrzedział ufności: 49%–55%\nWielkość próby: 1 000 (wskazuje na umiarkowaną precyzję)\n\nKluczowe pytania, które warto zadać:\n\nJak dobrano próbę?\nJaki był odsetek odpowiedzi?\nJak sformułowano pytania?\nKiedy przeprowadzono badanie?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#podsumowanie",
    "href": "rozdzial1.html#podsumowanie",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.22 Podsumowanie",
    "text": "2.22 Podsumowanie\n\nNajważniejsze pojęcia\nEstymacja na podstawie próby\n\nEstymata punktowa: najlepsze pojedyncze oszacowanie z próby\nMargines błędu: ilościowo opisuje niepewność wynikającą z losowości doboru\n\nWielkość próby a precyzja\n\nNiepewność jest w przybliżeniu proporcjonalna do 1/\\sqrt{n}\nCzterokrotne zwiększenie próby zmniejsza margines błędu o połowę\n\nBłąd losowy vs. błąd systematyczny\n\nBłąd losowy: kwantyfikowalny, maleje wraz z n\nBłąd systematyczny: trudno kwantyfikowalny, utrzymuje się niezależnie od n\n\nRzetelne raportowanie\n\nZawsze raportuj: estymatę, niepewność, wielkość próby i metodologię\nUjawniaj ograniczenia i potencjalne źródła biasu",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#podstawa-matematyczna",
    "href": "rozdzial1.html#podstawa-matematyczna",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.23 Podstawa matematyczna",
    "text": "2.23 Podstawa matematyczna\n\n\n\n\n\n\nTip\n\n\n\nSzczegóły techniczne\nMargines błędu dla proporcji wynika z rachunku prawdopodobieństwa:\n\\text{Margines błędu} = z_{\\alpha/2} \\times \\sqrt{\\frac{p(1-p)}{n}}\ngdzie:\n\np — prawdziwa proporcja w populacji,\nn — liczebność próby,\nz\\_{\\alpha/2} — wartość krytyczna (1.96 dla 95% ufności).\n\nPonieważ p(1-p) osiąga maksimum 0.25 przy p=0.5:\n\\text{Maksymalny margines błędu} = 1.96 \\times \\sqrt{\\frac{0.25}{n}} \\approx \\frac{1}{\\sqrt{n}}\nco uzasadnia stosowane powyżej przybliżenie.\n\n\n\n\nSzacowanie proporcji: formuły kluczowe\nOszacowanie punktowe: \\hat{p} = \\frac{\\text{liczba sukcesów}}{\\text{liczebność próby}}\nPrzybliżony margines błędu: \\text{MB} \\approx \\frac{1}{\\sqrt{n}}\nSzablon interpretacji: „Szacujemy [X%] z marginesem błędu [±Y%] na podstawie próby o liczebności [n].”\nZałożenia i zastrzeżenia:\n\nZakładamy losowy dobór próby.\nStronniczości nie zredukuje samo zwiększanie n.\nZawsze raportuj metodologię i ograniczenia.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#statystyczny-sposób-myślenia",
    "href": "rozdzial1.html#statystyczny-sposób-myślenia",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.24 Statystyczny sposób myślenia",
    "text": "2.24 Statystyczny sposób myślenia\nPięć nawyków jest szczególnie istotnych.\n\n1) Uznawaj niepewność\nWartości populacyjne rzadko są znane dokładnie. Każdy wynik liczbowy traktuj jako oszacowanie z towarzyszącą mu niepewnością. Przykład: „Szacowane poparcie wynosi 52% (95% PU: 49%–55%), na podstawie losowej próby 1200 dorosłych.”\n\n\n2) Myśl w kategoriach zmienności\nRóżnice między osobami, miejscami i czasami umożliwiają uczenie się. Rozróżniaj zmienność losową (wynikającą z losowania próby) od zmienności systematycznej (instytucje, demografia, bodźce). Przykład: Różnice we frekwencji mogą wynikać z pogody (losowe) oraz ze złożoności karty wyborczej (systematyczne).\n\n\n3) Porównuj w sposób uczciwy (asocjacja vs. przyczynowość)\nDwie zmienne mogą się współzmieniać (asocjacja) bez związku przyczynowego. Twierdzenia o przyczynowości wymagają uczciwego porównania: co stałoby się z tym samym obiektem przy innym warunku, przy stałości pozostałych czynników (wynik kontrfaktyczny). Przykład: Gminy z większą liczbą wizyt kampanijnych często mają wyższą frekwencję. Konkurencyjność może jednocześnie przyciągać wizyty i podnosić frekwencję. Aby twierdzić, że wizyty zwiększają frekwencję, potrzebny jest wiarygodny projekt badania (randomizacja, eksperyment naturalny, dyskontynuacja regresji itp.).\n\n\n\n\n\n\nCaution\n\n\n\nZanim sformułujesz wniosek przyczynowy\n\nCzy porównanie jest jasno zdefiniowane i wiarygodne?\nCzy czynnik trzeci (konfounder) może wpływać na obie zmienne i jak został uwzględniony (projekt, analiza)?\nCzy założenia są jawnie podane i – na ile to możliwe – sprawdzone?\n\n\n\n\n\n4) Sprawdzaj rzetelność\nPozorne regularności mogą wynikać z losowości próbkowania lub wyboru modelu. Wykorzystuj błędy standardowe, przedziały oraz replikację, aby ocenić stabilność. Preferuj wielkości efektu z przedziałami zamiast dychotomicznych etykiet „istotne/nieistotne”. Przykład: „Frekwencja wzrosła o 2,3 punktu procentowego (95% PU: 0,8–3,8).”\n\n\n5) Rozważaj alternatywne wyjaśnienia\nDla każdego wniosku wypisz wiarygodne alternatywy i – jeśli to możliwe – je testuj (analizy odporności, testy placebo, testy falsyfikacyjne). Przykład: Jeśli uczestnictwo w grupach nauki koreluje z lepszymi ocenami, oceń selekcję: uczniowie o lepszych wcześniejszych wynikache mogą częściej wybierać naukę w grupach.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#etapy-analizy-statystycznej",
    "href": "rozdzial1.html#etapy-analizy-statystycznej",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.25 Etapy analizy statystycznej",
    "text": "2.25 Etapy analizy statystycznej\n\nPytanie badawcze – co chcesz oszacować (estimand)?\nProjekt badania – jak uzyskasz wiarygodne porównanie?\nZbieranie danych – procedury, próbkowanie, jakość pomiaru.\nEksploracja danych (EDA) – wstępne wzorce, czyszczenie, wizualizacje.\nModelowanie – opis, przewidywanie lub wnioskowanie przyczynowe.\nDiagnostyka – sprawdzenie założeń, odporność, testy placebo.\nWnioski i komunikacja – efekt z przedziałem ufności/wiarygodności, ograniczenia.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#niezbędne-pojęcia",
    "href": "rozdzial1.html#niezbędne-pojęcia",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.26 Niezbędne pojęcia",
    "text": "2.26 Niezbędne pojęcia\n\nPopulacja: zbiór, o którym chcemy się uczyć (np. wszyscy uprawnieni do głosowania).\nPróba: część populacji, którą obserwujemy (np. 1200 ankietowanych).\nParametr: stała, lecz nieznana wielkość populacyjna (np. prawdziwe poparcie).\nStatystyka (z próby): liczba obliczona na podstawie próby (np. średnia z próby).\nOszacowanie (estymata): najlepsza ocena parametru na podstawie danych (np. \\hat{p}).\nBłąd standardowy (SE): oszacowana zmienność oszacowania w powtarzanych próbach.\nPrzedział ufności (PU): zakres, który – przy wielokrotnym próbkowaniu – zawierałby parametr z określonym prawdopodobieństwem (np. 95%).\nEfekt przyczynowy: zmiana wyniku po zmianie bodźca dla tego samego obiektu.\nKonfounder (czynnik zakłócający): zmienna wpływająca zarówno na bodziec, jak i na wynik, co może tworzyć mylną asocjację.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#populacja-próba-i-superpopulacja-dgp-fundamenty-wnioskowania",
    "href": "rozdzial1.html#populacja-próba-i-superpopulacja-dgp-fundamenty-wnioskowania",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.27 Populacja, próba i superpopulacja (DGP): fundamenty wnioskowania",
    "text": "2.27 Populacja, próba i superpopulacja (DGP): fundamenty wnioskowania\nW politologii często chcemy zrozumieć całe populacje — pełne zbiory jednostek, które nas interesują. Badanie całych populacji bywa jednak niemożliwe, niepraktyczne albo… zbędne. Statystyka pozwala uczyć się o populacjach za pomocą prób.\n\nCo może być populacją?\nOsoby\n\nPopulacja: Wszyscy dorośli Amerykanie (ok. 240 mln)\nPróba: 1 000 losowo wybranych dorosłych w sondażu\nPytanie badawcze: Jaki odsetek popiera powszechną opiekę zdrowotną?\n\nPaństwa\n\nPopulacja: 195 suwerennych państw świata\nPróba: 50 krajów z różnych regionów i poziomów rozwoju\nPytanie badawcze: Czy demokracja koreluje ze wzrostem gospodarczym?\n\nJednostki subnarodowe\n\nPopulacja: Wszystkie 3 143 hrabstwa w USA\nPróba: 200 losowo wybranych hrabstw\nPytanie badawcze: Jak bezrobocie wpływa na przestępczość?\n\nOrganizacje\n\nPopulacja: Wszystkie NGO zarejestrowane przy ONZ\nPróba: 100 NGO działających w różnych obszarach polityk\nPytanie badawcze: Co przewiduje skuteczność NGO?\n\nZdarzenia lub okresy\n\nPopulacja: Wszystkie wybory w Europie po 1945 r.\nPróba: 300 elekcji z różnych krajów i dekad\nPytanie badawcze: Jak warunki gospodarcze wpływają na wynik urzędujących?\n\nJednostki legislacyjne\n\nPopulacja: Wszystkie projekty ustaw w Kongresie 2000–2020\nPróba: 500 losowo wybranych projektów\nPytanie badawcze: Co przewiduje uchwalenie ustawy?\n\n\n\nOd próby do populacji (wnioskowanie)\nPróba to podzbiór populacji, który faktycznie obserwujemy i mierzymy. Kluczowa myśl statystyki: możemy uczyć się o populacjach z prób — jeśli ostrożnie je dobieramy.\nZ próby chcemy wyciągnąć wnioski o populacji:\n\\text{statystyka próby} \\;\\to\\; \\text{parametr populacji}\nPrzykład: jeśli 52% naszej próby popiera Kandydata A (\\hat{p} = 0{,}52), to co możemy powiedzieć o poparciu w całej populacji (\\pi)?\nZasada fundamentalna: losowy dobór daje każdej jednostce populacji równą szansę trafienia do próby i zapobiega systematycznym stronniczościom.\n\n\n\n\n\n\nNote\n\n\n\nMini‑słowniczek (prosty język)\n\nParametr: prawdziwa wielkość populacji (np. średnia \\mu).\nStatystyka: liczba policzona z próby (np. średnia próbna \\bar{x}).\nEstymator: reguła liczenia statystyki (np. „policz średnią”).\n\n\n\n\n\nWizualizacja próbkowania\nZobaczmy, jak wielkość próby wpływa na rozrzut estymat:\n\nset.seed(42)\n\n# Parametry populacji i eksperymentu\npopulation_size &lt;- 1000000\ntrue_proportion &lt;- 0.60   # Prawdziwy parametr populacji (π)\nsample_sizes    &lt;- c(100, 500, 1000, 5000)\nn_trials        &lt;- 20     # niezależnych prób dla każdego n\n\n# Symulacja (zwektoryzowana)\ngrid &lt;- expand.grid(size = sample_sizes, trial = seq_len(n_trials))\ngrid$estimate &lt;- rbinom(nrow(grid), size = grid$size, prob = true_proportion) / grid$size\n\n# Oczekiwany „pas niepewności” ± 2 × SE dla każdego n (używamy prawdziwego π)\nse_exp &lt;- sqrt(true_proportion * (1 - true_proportion) / sample_sizes)\nribbons &lt;- data.frame(\n  size  = sample_sizes,\n  lower = pmax(0, true_proportion - 2 * se_exp),\n  upper = pmin(1, true_proportion + 2 * se_exp)\n)\n\n# Wykres\nggplot(grid, aes(x = factor(size), y = estimate)) +\n  # Szare pasma: przybliżone ± 2 × SE dla każdej wielkości próby\n  geom_crossbar(\n    data = transform(ribbons, x = factor(size)),\n    aes(x = x, y = true_proportion, ymin = lower, ymax = upper),\n    inherit.aes = FALSE,\n    fill = \"grey85\", alpha = 0.7, width = 0.6, color = NA\n  ) +\n  # Punkty: niezależne estymaty z prób\n  geom_point(position = position_jitter(width = 0.12, height = 0),\n             alpha = 0.75, size = 2.2, color = \"steelblue\") +\n  # Czerwona linia: prawdziwa wartość populacji\n  geom_hline(yintercept = true_proportion, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title    = \"Jak wielkość próby wpływa na rozrzut estymat\",\n    subtitle = \"Szare pasma pokazują przybliżone ± 2 × SE wokół 60% dla każdego n; większe n → węższe pasma\",\n    x        = \"Wielkość próby (n)\",\n    y        = \"Estymata próby\",\n    caption  = paste0(\"Liczba replik dla każdego n: \", n_trials,\n                      \"; π = \", scales::percent(true_proportion))\n  ) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  coord_cartesian(ylim = c(0.45, 0.75)) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\nUwaga do wykresu: Każda niebieska kropka to estymata z jednej niezależnej próby. Szare pasmo w każdej kolumnie pokazuje przybliżone ± 2 × SE (około połowy typowego przedziału). Gdy n rośnie, pasma się zwężają, a punkty skupiają bliżej czerwonej linii (prawdziwej wartości).\n\n\nNajważniejsze: Większe n zmniejsza błąd losowy (węższy zakres typowych wahań). Dla n=100 rozrzut bywa duży (np. ~55–65%), a dla n=5000 estymaty są znacznie bliżej prawdy (np. ~59–61%). Dlatego sondaże ogólnokrajowe celują w n\\approx 1{,}000+, a nie n=100.\n\n\n\nProblem reprezentatywności\nNie każda próba jest równie dobra. Porównaj następujące schematy doboru:\n\nPróba wygodna (convenience): ankietowanie studentów na własnym kierunku\n\nProblem: nie reprezentuje wszystkich wyborców\nPrzykład: studenci są przeciętnie młodsi i bardziej liberalni niż ogół\n\nDobór ochotników (voluntary response): sonda internetowa na portalu\n\nProblem: błąd autoselekcji\nPrzykład: częściej odpowiadają osoby o silnych opiniach\n\nProsty losowy dobór: każda jednostka ma równą szansę\n\nRozwiązanie: największa szansa na reprezentatywność\nPrzykład: losowanie numerów telefonów ze wszystkich stref\n\nDobór warstwowy (stratyfikowany): dzielimy populację na grupy i losujemy z każdej\n\nZaleta: gwarantuje reprezentację kluczowych podgrup\nPrzykład: taka sama liczba respondentów z każdego województwa\n\nDobór skupieniowy (klastrowy): losujemy grupy, a potem badamy wszystkich w środku\n\nZaleta: tańszy przy rozproszonej geograficznie populacji\nPrzykład: losujemy 50 miast i ankietujemy mieszkańców tych miast\n\n\n\n\n\nPodstawowe definicje\n\nPopulacja\nPopulacja to pełny zbiór jednostek, które badamy. Jednostką może być osoba, gospodarstwo domowe, firma, organizacja, gmina/miasto, powiat/region, państwo/kraj lub zdarzenie (np. głos, ustawa, transakcja).\nPrzykłady\n\nWszystkie gminy w Polsce (np. w 2024 r.)\nWszystkie państwa świata w 2024 r.\nWszyscy zarejestrowani wyborcy w Kanadzie\nWszystkie transakcje na NYSE w 2024 r.\nWszystkie ustawy uchwalone w danym okresie\n\nKluczowa myśl: W danym momencie populacja jest skończona i ustalona. Ma prawdziwe wielkości, które chcemy znać (np. średnia \\mu i odchylenie \\sigma), choć ich jeszcze nie znamy.\n\n\nPróba\nPróba to podzbiór populacji, który faktycznie obserwujemy.\nPrzykłady\n\n1 000 losowo wybranych wyborców\n200 pomierzonych drzew\n10 000 przeanalizowanych transakcji\n300 ankietowanych studentów\n250 gmin wylosowanych ze wszystkich gmin\n\nDlaczego próby wprowadzają niepewność\n\nGdy wylosujemy inną próbę (inne osoby/gminy/zdarzenia), wyniki trochę się zmienią.\nDlatego statystyki (np. \\bar{x}) różnią się między próbami.\nTych różnic między próbami używamy do raportowania pewności naszych wniosków o populacji.\n\nPOPULACJA (istnieje, ale nie w pełni znana)\n    ↓\n[Dobór próby / zbieranie danych]\n    ↓\nPRÓBA (to, co widzimy)\n    ↓\n[Wnioskowanie z uwzględnieniem niepewności]\n    ↓\nESTYMACJE dotyczące populacji\n\n\n\n\n\n\nTip\n\n\n\nEksperyment myślowy. Powtórz ten sam sondaż 1 000 razy, za każdym razem z nowymi losowymi uczestnikami. Otrzymasz 1 000 lekko różnych średnich. Ich rozrzut to typowa różnica między próbami.\n\n\nKrótka symulacja dla intuicji:\n\nset.seed(1)\n\n# Załóżmy, że prawdziwy odsetek poparcia w populacji to p = 0.60.\np_true &lt;- 0.60\nn &lt;- 1000   # liczebność próby w jednym sondażu\nR &lt;- 2000   # liczba powtórzeń sondażu\n\n# Powtarzamy sondaż R razy:\np_hats &lt;- rbinom(R, size = n, prob = p_true) / n\n\n# Typowa różnica między próbami (później: błąd standardowy):\nsd(p_hats)\n\n[1] 0.01579959\n\n# Przybliżony centralny zakres (~95%):\nquantile(p_hats, c(0.025, 0.975))\n\n 2.5% 97.5% \n0.568 0.631 \n\n\n\nWniosek: Nawet bez zmian warunków kolejne próby dają lekko różne wyniki. Te różnice między próbami to niepewność, o której powinniśmy informować.\n\n\n\n\n\nGdy obserwujemy wszystkich (spisy, pełne dane administracyjne)\nCzasem obserwujemy całą populację w danym roku:\n\nSpis powszechny\nWszystkie transakcje giełdowe w 2024 r.\nWszystkie przyjęcia do szpitali w 2023 r.\nWszystkie gminy w Polsce wraz z cechami w 2024 r.\n\nPytanie: Skoro policzyliśmy „prawdziwą” średnią dla 2024 r., to skąd niepewność?\nOdpowiedź: Zwykle interesuje nas proces, a nie tylko jeden rok.\n\nInny rok może wyglądać inaczej. 2024 to tylko jedna realizacja; 2025 może się różnić.\nPomiar jest niedoskonały. Nawet pełne dane zawierają braki i błędy.\nChcemy uogólniać. Pytamy „co zwykle się dzieje?” albo „co jeśli warunki się zmienią?”\n\nDlatego nawet przy „całym 2024 roku” przedziały mogą wyrażać niepewność co do procesu, który wytworzy przyszłe dane.\n\n\nSuperpopulacja (proces generowania danych)\nSuperpopulacja lub proces generowania danych (DGP) to koncepcyjne źródło danych: trwający mechanizm, który mógłby wytworzyć nieco inne wyniki — i będzie produkował nowe dane w kolejnych latach.\nZamiast tylko:\nPopulacja → Próba\nczęsto myślimy tak:\nSUPERPOPULACJA (proces)\n    ↓\n[Mechanizm generowania danych]\n    ↓\nZAOBSERWOWANA POPULACJA (konkretny rok/warunki)\n    ↓\nWNIOSKI o procesie (uogólnianie, przewidywanie, wyjaśnianie)\n\nPrzykłady\nSprzedaż roczna\n\nCo obserwujemy: wszystkie 50 000 transakcji w 2024 r.\nProces: przy nieco innym popycie/cenach/promocjach wyniki byłyby inne\nPo co: zrozumieć proces, by powiedzieć coś o 2025 r.\n\nWybory\n\nCo obserwujemy: frekwencję we wszystkich gminach w 2024 r.\nProces: pogoda, kampanie, tematy, kalendarz wpływają na frekwencję\nPo co: poznać czynniki frekwencji zwykle, nie tylko w jednym roku\n\nOceny studentów\n\nCo obserwujemy: wszystkie oceny w semestrze\nProces: sylabus, zadania, kohorta, prowadzący — drobne zmiany przesuwają wyniki\nPo co: ocenić, czy nowa metoda zwykle działa lepiej\n\n\n\n\n\nTabela podsumowująca\n\n\n\n\n\n\n\n\n\nPojęcie\nCo obejmuje\nPrzykład\nSkąd bierze się niepewność (po ludzku)\n\n\n\n\nPróba\nCzęść populacji\n1 000 ankietowanych / 250 losowo wybranych gmin\nLosujemy inne jednostki → wyniki trochę się różnią.\n\n\nPopulacja\nCały zbiór w danym roku\nWszyscy wyborcy / wszystkie gminy (2024)\nInne lata/warunki dają inne wyniki; ograniczenia pomiaru.\n\n\nSuperpopulacja (DGP)\nProces wytwarzający dane\nMechanizm zachowań wyborców / mechanizm sprzedaży\nOpis procesu bywa uproszczony; nieuwzględnione czynniki mogą mieć znaczenie.\n\n\n\n\n\nProblem reprezentatywności\nNie wszystkie próby są równe. Rozważmy te metody próbkowania:\n\nPróba wygodna (convenience sample): Badanie studentów w twojej klasie politologii\n\nProblem: Niereprezentacyjne dla wszystkich wyborców\nPrzykład: Studenci college’u są młodsi i bardziej liberalni niż ogólna populacja\n\nPróba dobrowolna (voluntary response sample): Sondaż online na stronie internetowej wiadomości\n\nProblem: Stronniczość autoselekcji\nPrzykład: Ludzie o silnych opiniach częściej uczestniczą\n\nPróba losowa (random sample): Każda jednostka ma równe prawdopodobieństwo wyboru\n\nRozwiązanie: Najlepsza szansa na reprezentatywną próbę\nPrzykład: Losowo wybrane numery telefonów ze wszystkich kodów pocztowych\n\nPróba warstwowa losowa (stratified random sample): Podziel populację na grupy, pobierz próby z każdej\n\nZaleta: Zapewnia reprezentację kluczowych podgrup\nPrzykład: Pobierz równe liczby z każdego stanu dla sondażu ogólnokrajowego\n\nPróba klastrowa (cluster sample): Losowo wybierz grupy, następnie zbadaj wszystkich w ramach klastra\n\nZaleta: Koszt-efektywne dla geograficznie rozproszonych populacji\nPrzykład: Losowo wybierz 50 miast, następnie zbadaj mieszkańców tych miast",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#parametry-statystyki-i-oszacowania",
    "href": "rozdzial1.html#parametry-statystyki-i-oszacowania",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.28 Parametry, statystyki i oszacowania",
    "text": "2.28 Parametry, statystyki i oszacowania\n\nParametry vs. statystyki\nFundamentalnym rozróżnieniem w statystyce jest różnica między parametrami a statystykami:\nParametry populacji\n\nCechy liczbowe całej populacji\nZwykle nieznane i to, o czym chcemy się dowiedzieć\nOznaczane greckimi literami: \\mu (mi) dla średniej, \\sigma (sigma) dla odchylenia standardowego, \\pi (pi) dla proporcji\nPrzykłady: Prawdziwy procent wszystkich Amerykanów popierających powszechną opiekę zdrowotną\n\nStatystyki próby\n\nCechy liczbowe obliczone z danych próby\nTo, co faktycznie obserwujemy i obliczamy\nOznaczane literami łacińskimi: \\bar{x} dla średniej próby, s dla odchylenia standardowego próby, \\hat{p} dla proporcji próby\nPrzykłady: Procent 1000 respondentów sondażu popierających powszechną opiekę zdrowotną\n\nKonwencja notacji\nW tym podręczniku będziemy konsekwentnie używać: - Parametry populacji: \\mu (średnia), \\sigma (odchylenie standardowe), \\pi (proporcja)\n- Statystyki próby: \\bar{x} (średnia), s (odchylenie standardowe), \\hat{p} (proporcja)\nTa notacja pomaga nam zawsze rozróżnić to, co obserwujemy (statystyki), od tego, co chcemy wiedzieć (parametry).\n\n\nProces wnioskowania: od statystyk do parametrów\nWnioskowanie statystyczne obejmuje wykorzystanie statystyk próby do dokonywania świadomych przypuszczeń o parametrach populacji:\n\\text{Statystyka próby} \\xrightarrow{\\text{Wnioskowanie statystyczne}} \\text{Parametr populacji}\nPrzykład: Jeśli 52% naszej próby (\\hat{p} = 0,52) popiera kandydata, używamy tej statystyki do oszacowania parametru populacji (\\pi) reprezentującego prawdziwe poparcie wśród wszystkich wyborców.\n\n\nOszacowania i estymatory\nEstymator to metoda lub wzór używany do przybliżania parametru. Oszacowanie to konkretny wynik liczbowy z zastosowania tego estymatora do konkretnej próby.\n\nEstymator: Średnia próby \\bar{x} = \\frac{\\sum x_i}{n}\nOszacowanie: \\bar{x} = 6,3 lat wykształcenia (rzeczywista liczba z naszych danych)\n\n\n\nAnalogia zupy: zrozumienie wnioskowania statystycznego\nWyobraź sobie, że jesteś szefem kuchni przygotowującym wielki garnek zupy dla 100 osób. Chcesz wiedzieć, czy zupa ma odpowiednią ilość soli, ale nie możesz skosztować jej całej. Zamiast tego bierzesz małą łyżkę do skosztowania.\nPopulacja: Cały garnek zupy (100 porcji)\nPróba: Twoja łyżka\nParametr: Prawdziwa słoność całego garnka (nieznana)\nStatystyka: Słoność twojej łyżki (to, co możesz zmierzyć)\nWnioskowanie statystyczne: Używanie słoności łyżki do wyciągania wniosków o całym garnku\nKluczowe spostrzeżenia z analogii zupy:\n\nLosowe próbkowanie ma znaczenie: Musisz najpierw wymieszać zupę i wziąć łyżkę z losowego miejsca. Jeśli zawsze pobierasz próbę z wierzchu, możesz przegapić, że sól osiadła na dnie.\nRozmiar próby wpływa na precyzję: Większa łyżka da ci lepsze wyobrażenie o ogólnej słoności niż mały łyk.\nNiepewność jest nieodłączna: Nawet przy dobrym próbkowaniu twoja łyżka może nie odzwierciedlać idealnie całego garnka. Zawsze jest jakaś niepewność.\nSystematyczna stronniczość rujnuje wszystko: Gdyby ktoś potajemnie dodał extra sól tylko do twojej łyżki, twoje wnioskowanie o całym garnku byłoby błędne. To reprezentuje stronniczość próbkowania.\nWnioskowanie ma ograniczenia: Możesz oszacować średnią słoność, ale twoja łyżka nie może powiedzieć ci, czy niektóre części są bardziej słone niż inne (zmienność wewnątrz populacji).\n\nTa analogia uchwywa istotę myślenia statystycznego: używamy małych, starannie wybranych prób do uczenia się o znacznie większych populacjach, zawsze uznając niepewność nieodłączną w tym procesie.\n\n\nPrzykład z prawdziwego świata: Co przewiduje sukces wyborczy?\nZacznijmy od pytania, które trafia w serce politologii: Co sprawia, że politycy wygrywają wybory?\nWyobraź sobie, że jesteś menedżerem kampanii próbującym zrozumieć, dlaczego niektórzy urzędujący wygrywają miażdżąco, podczas gdy inni ledwo wygrywają. Masz dane o 200 ostatnich wyborach do Kongresu, w tym ocenę popularności każdego urzędującego, stan lokalnej gospodarki i ich margines zwycięstwa.\n\n# Stwórz realistyczne dane wyborcze\nset.seed(42)  # Zgodne z początkowym ustawieniem\nn_elections &lt;- 200\n\n# Generuj skorelowane predyktory (realistyczny scenariusz)\napproval_rating &lt;- runif(n_elections, 35, 85)\neconomic_growth &lt;- rnorm(n_elections, 2.5, 1.5)\ncampaign_spending_100k &lt;- rnorm(n_elections, 8, 2)  # W jednostkach $100,000 dla przejrzystości\n\n# Stwórz margines zwycięstwa z realistycznymi związkami\nvictory_margin &lt;- -15 + \n  0.6 * approval_rating +           # Silny efekt popularności\n  2.5 * economic_growth +           # Głosowanie ekonomiczne\n  0.3 * campaign_spending_100k +    # Pieniądze pomagają (efekt na $100k)\n  rnorm(n_elections, 0, 8)          # Czynniki losowe\n\n# Stwórz zbiór danych\nelection_data &lt;- data.frame(\n  district = 1:n_elections,\n  approval = approval_rating,\n  econ_growth = economic_growth,\n  spending_100k = campaign_spending_100k,\n  victory_margin = victory_margin,\n  won = victory_margin &gt; 0\n)\n\n# Szybka wizualizacja\np1 &lt;- ggplot(election_data, aes(x = approval, y = victory_margin)) +\n  geom_point(alpha = 0.6, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.7) +\n  labs(title = \"Ocena popularności vs. margines zwycięstwa\",\n       x = \"Ocena popularności (%)\",\n       y = \"Margines zwycięstwa (punkty procentowe)\",\n       subtitle = \"Punkty powyżej linii przerywanej reprezentują zwycięstwa\")\n\nprint(p1)\n\n\n\n\n\n\n\n# Uruchom regresję\nsimple_model &lt;- lm(victory_margin ~ approval, data = election_data)\nsummary(simple_model)\n\n\nCall:\nlm(formula = victory_margin ~ approval, data = election_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.9948  -6.1420   0.5653   5.9218  28.4974 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) -9.78570    2.63382  -3.715             0.000264 ***\napproval     0.64728    0.04192  15.439 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.635 on 198 degrees of freedom\nMultiple R-squared:  0.5462,    Adjusted R-squared:  0.544 \nF-statistic: 238.4 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\n\n\nUwaga do wykresu: Ten wykres rozrzutu pokazuje związek między ocenami popularności (oś x) a marginesami zwycięstwa wyborczego (oś y). Każdy punkt reprezentuje jedne wybory. Czerwona linia pokazuje “linię najlepszego dopasowania” z regresji liniowej, z szarym pasmem wskazującym niepewność. Punkty powyżej przerywnej linii poziomej (y=0) reprezentują zwycięstwa wyborcze.\n\nCzytanie wyników: “Oszacowanie” (Estimate) dla popularności (około 0,60) oznacza, że każdy 1-punktowy wzrost oceny popularności jest związany z 0,60-punktowym wzrostem marginesu zwycięstwa. Wartość p (&lt;0,001) wskazuje, że ten związek jest statystycznie istotny - bardzo mało prawdopodobny by wynikał tylko z przypadku.\nCo właśnie odkryliśmy: Każdy 1-punktowy wzrost oceny popularności jest związany z około 0.65-punktowym wzrostem marginesu zwycięstwa. Przy ocenie popularności poniżej 15.1% urzędujący zazwyczaj przegrywają.\nJednak ocena popularności reprezentuje tylko jeden czynnik sukcesu wyborczego. Bardziej kompleksowa analiza wymaga jednoczesnego badania wielu zmiennych:\n\n# Model regresji wielokrotnej\nfull_model &lt;- lm(victory_margin ~ approval + econ_growth + spending_100k, data = election_data)\n\n# Przejrzyste przedstawienie wyników\nmodel_results &lt;- tidy(full_model) %&gt;%\n  mutate(\n    estimate = round(estimate, 4),\n    p.value = round(p.value, 3),\n    significant = ifelse(p.value &lt; 0.05, \"Tak\", \"Nie\"),\n    term = recode(term,\n                  \"(Intercept)\" = \"Wartość bazowa\",\n                  \"approval\" = \"Ocena popularności\",\n                  \"econ_growth\" = \"Wzrost gospodarczy (%)\",\n                  \"spending_100k\" = \"Wydatki kampanii (na $100k)\")\n  )\n\nkable(model_results, \n      col.names = c(\"Zmienna\", \"Rozmiar efektu\", \"Błąd std\", \"Statystyka t\", \"Wartość p\", \"Istotne?\"),\n      caption = \"Co naprawdę napędza sukces wyborczy?\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nCo naprawdę napędza sukces wyborczy?\n\n\nZmienna\nRozmiar efektu\nBłąd std\nStatystyka t\nWartość p\nIstotne?\n\n\n\n\nWartość bazowa\n-18.8368\n3.7738929\n-4.991343\n0.00\nTak\n\n\nOcena popularności\n0.6541\n0.0397821\n16.441115\n0.00\nTak\n\n\nWzrost gospodarczy (%)\n1.9619\n0.4004247\n4.899426\n0.00\nTak\n\n\nWydatki kampanii (na $100k)\n0.4897\n0.3054328\n1.603246\n0.11\nNie\n\n\n\n\n\nGdy uwzględniamy jednocześnie wiele czynników, widzimy, że:\n\nOcena popularności pozostaje najsilniejszym predyktorem (0,6 punktu na 1% popularności)\nWzrost gospodarczy również ma istotne znaczenie (2,5 punktu na 1% wzrostu PKB)\nWydatki kampanii mają skromny efekt (0,3 punktu na $100 000 wydanych)\n\nTo jest siła analizy regresji - pomaga nam rozwikłać złożone związki i zrozumieć, co naprawdę ma znaczenie w polityce.\n\n\n\n\n\n\nTypowe pułapki statystyczne w politologii\n\n\n\n\nBłąd ekologiczny: Zakładanie, że wzorce na poziomie grupy dotyczą jednostek\nStronniczość selekcji: Nielosowe próby, które systematycznie wykluczają pewne grupy\n\nZmienne zakłócające: Brak uwzględnienia zmiennych wpływających zarówno na X, jak i Y\nHakowanie p: Testowanie wielu hipotez aż do znalezienia istotności\nNadmierne uogólnianie: Rozszerzanie wyników poza badaną populację\n\n\n\n\n\nŚwiat polityki jest pełen danych\nPolitologia ewoluowała z dyscypliny głównie teoretycznej do takiej, która coraz bardziej opiera się na dowodach empirycznych. Niezależnie od tego, czy badamy:\n\nWyniki wyborów: Dlaczego ludzie głosują tak, jak głosują?\nOpinię publiczną: Co kształtuje postawy wobec imigracji lub polityki klimatycznej?\nStosunki międzynarodowe: Jakie czynniki przewidują konflikt między narodami?\nSkuteczność polityk: Czy nowa polityka edukacyjna rzeczywiście poprawiła wyniki?\n\nPotrzebujemy systematycznych sposobów analizowania danych i wyciągania wniosków, które wykraczają poza anegdoty i osobiste wrażenia.\nRozważ to pytanie: “Czy demokracja prowadzi do wzrostu gospodarczego?”\nTwoja intuicja może sugerować, że tak - kraje demokratyczne są zazwyczaj bogatsze. Ale czy to przyczynowość, czy korelacja? Czy są wyjątki? Jak pewni możemy być naszych wniosków?\nStatystyka dostarcza narzędzi do przejścia od przeczuć do odpowiedzi opartych na dowodach, pomagając nam rozróżnić między tym, co wydaje się prawdziwe, a tym, co rzeczywiście jest prawdziwe.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#pomiar-przekształcanie-pojęć-w-liczby",
    "href": "rozdzial1.html#pomiar-przekształcanie-pojęć-w-liczby",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.29 Pomiar: przekształcanie pojęć w liczby",
    "text": "2.29 Pomiar: przekształcanie pojęć w liczby\n\nWyzwanie pomiaru w naukach społecznych\nW naukach społecznych często zmagamy się z tym, że kluczowe pojęcia nie przekładają się wprost na liczby:\n\nJak zmierzyć „demokrację”?\nJaka liczba oddaje „ideologię polityczną”?\nJak ilościowo ująć „siłę instytucji”?\nJak zmierzyć „partycypację polityczną”?\n\n\n\nPoziomy pomiaru\nNominalny (kategorie bez porządku)\n\nPrzynależność partyjna: Demokratyczna, Republikańska, Niezależna\nKraj: Polska, Niemcy, Francja\nWybór w głosowaniu: Kandydat A, Kandydat B, Nie głosował\n\nDozwolone operacje: liczenie częstości, dominanta (moda), tabele krzyżowe, test chi-kwadrat.\nPorządkowy (uporządkowane kategorie)\n\nWykształcenie: podstawowe &lt; średnie &lt; wyższe licencjackie &lt; wyższe magisterskie &lt; stopień doktora\nSkale Likerta: Zdecydowanie się nie zgadzam &lt; Nie zgadzam się &lt; Neutralny &lt; Zgadzam się &lt; Zdecydowanie się zgadzam\nPoziom wiedzy politycznej: niski &lt; średni &lt; wysoki\n\nDozwolone operacje: porządkowanie, mediana, kwartyle, korelacja rangowa Spearmana, testy nieparametryczne (np. Mann-Whitney).\n\n\n\n\n\n\nImportant\n\n\n\nKluczowa cecha: Odległości między kategoriami nie muszą być jednakowe. Na przykład, różnica w ilości wiedzy między poziomem „niskim” a „średnim” może być znacznie większa lub mniejsza niż różnica między poziomem „średnim” a „wysokim”. Wiemy tylko, że jeden poziom jest wyższy od drugiego, ale nie wiemy „o ile”.\n\n\nInterwałowy (równe odstępy, zero umowne)\n\nLata kalendarzowe: różnica między 2020–2021 = różnica między 2023–2024\nTemperatura w °C lub °F\nWyniki standaryzowane oparte na transformacji liniowej (np. z-score, T-score)\n\nDozwolone operacje: dodawanie, odejmowanie, średnia arytmetyczna, odchylenie standardowe, korelacja Pearsona, regresja liniowa.\n\n\n\n\n\n\nWarning\n\n\n\nOgraniczenie: Porównania typu „dwa razy więcej” nie mają sensu, ponieważ punkt zero jest arbitralny (umowny). Na przykład: 20°C nie jest „dwa razy cieplejsze” niż 10°C. Gdyby używać skali Fahrenheita, te same temperatury to 68°F i 50°F – już nie jeden jest „dwa razy większy” od drugiego.\n\n\nIlorazowy (równe odstępy + prawdziwe zero)\n\nLiczba oddanych głosów (0 = rzeczywiście zero głosów)\nWiek, dochód, wydatki kampanijne\nLiczba poprawnych odpowiedzi w teście, procent poprawnych odpowiedzi\n\nDozwolone operacje: wszystkie, włącznie z ilorazami („dwa razy więcej głosów”).\n\n\n\nPrzypadek szczególny: wyniki testów psychometrycznych\n\n\n\n\n\n\n\n\nRodzaj wyniku\nPoziom pomiaru\nUwaga\n\n\n\n\nOceny literowe (A/B/C), staniny, kategorie\nPorządkowy\nTylko kolejność, bez równych odstępów\n\n\nPercentyle\nPorządkowy\nTen sam przyrost percentyli oznacza różną zmianę rzeczywistych wyników\n\n\nWyniki IQ\nPorządkowy\nUporządkowane i przekształcone do rozkładu normalnego\n\n\nz-score, T-score\nInterwałowy*\n*Tylko jeśli pierwotne wyniki rzeczywiście mają równe odstępy\n\n\nSurowa liczba punktów, % poprawnych\nIlorazowy\nPrawdziwe zero, stały przyrost\n\n\n\nPrzykład problemu z percentylami: Przejście z 50. na 60. percentyl może oznaczać zmianę o 2-3 punkty w teście, podczas gdy przejście z 90. na 95. percentyl może oznaczać zmianę o 10 punktów. Percentyle mówią nam tylko, ile procent osób uzyskało wynik gorszy, ale nie mówią o rzeczywistej wielkości różnic w umiejętnościach.\n\n\n\nRzeczywistość: IQ to fundamentalnie skala porządkowa\nJak powstają wyniki IQ – krok po kroku:\n\nZbieranie surowych wyników: Ludzie rozwiązują test i otrzymują liczbę poprawnych odpowiedzi (np. 45 z 60 pytań)\nUporządkowanie: Wszystkie surowe wyniki są uszeregowane od najgorszego do najlepszego\nPrzypisanie rang: Każdemu wynikowi przypisuje się pozycję w rankingu\nPrzekształcenie na skalę IQ: Rangi są przekształcane matematycznie tak, aby średnia wynosiła 100, a odchylenie standardowe 15\n\nKluczowy problem: Proces ten wymusza rozkład normalny na dane, które może wcale nie były normalne w pierwotnej postaci. To znaczy, że równe różnice w punktach IQ (np. różnica między IQ 100 a 115 vs. różnica między IQ 115 a 130) nie muszą odpowiadać równym różnicom w rzeczywistych zdolnościach poznawczych.\n\n\n\n\n\n\nKluczowy punkt\n\n\n\nIQ 130 nie oznacza „dwukrotnie większej inteligencji” niż IQ 65. Punkty IQ pokazują tylko pozycję danej osoby względem innych ludzi w próbie, nie rzeczywistą ilość inteligencji. To podobnie jak miejsca w konkursie – zwycięzca może wygrać o włos lub o kilometry, ale nadal będzie pierwszym miejscem.\n\n\nW praktyce badawczej: dlaczego czasem traktujemy IQ jako skalę interwałową?\nJest to metodologiczny kompromis, który pozwala na użycie bardziej precyzyjnych narzędzi statystycznych:\n✅ Traktowanie IQ jako skali interwałowej jest dopuszczalne gdy: - Używamy standardowych testów statystycznych (korelacje, regresje, testy t) - Porównujemy grupy w ramach tego samego testu i populacji - Jesteśmy świadomi ograniczeń tego podejścia - Nasze wnioski nie zależą od tego, czy różnice są dokładnie równe\n⚠️ Pamiętaj o ograniczeniach: - To uproszczenie rzeczywistości - Założenie działa lepiej dla wyników bliskich średniej (IQ 85-115) niż na krańcach - Wyniki trzeba interpretować ostrożnie\n❌ Nigdy nie wolno: - Mówić, że różnice IQ oznaczają równe różnice w inteligencji - Używać stwierdzeń typu “dwa razy bardziej inteligentny” - Zapomnieć, że normalny rozkład został wymuszony, a nie odkryty w danych\n\n\nPraktyczne wskazówki dla badaczy\n\nBądź transparentny:\n\nWyraźnie wspominaj: “Traktujemy IQ jako skalę interwałową do celów statystycznych, pamiętając że fundamentalnie jest to skala porządkowa”\n\nRozważaj alternatywy:\n\nUżywaj testów nieparametrycznych, gdy wielkość próby na to pozwala\nPorównaj wyniki różnych metod analitycznych\n\nInterpretuj ostrożnie:\n\nSkupiaj się na stwierdzeniach o kolejności („grupa A osiągnęła wyższe wyniki niż grupa B”)\nUnikaj precyzyjnych stwierdzeń o wielkości różnic\nPamiętaj: różnica 15 punktów IQ oznacza „jedno odchylenie standardowe w próbie”, nie „konkretną ilość dodatkowej inteligencji”\n\n\n\n\n\n\n\n\nTip\n\n\n\nIQ to skala porządkowa, która została przekształcona tak, aby wyglądała jak skala interwałowa. Można używać jej w analizach statystycznych wymagających skali interwałowej, ale zawsze należy pamiętać o jej rzeczywistej naturze przy interpretacji wyników. Kluczowe jest zrozumienie, że punkty IQ mówią nam o pozycji w grupie, nie o bezwzględnej ilości inteligencji.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#istotność-statystyczna-jak-rozumieć-niepewne-dowody",
    "href": "rozdzial1.html#istotność-statystyczna-jak-rozumieć-niepewne-dowody",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.30 Istotność statystyczna: jak rozumieć niepewne dowody",
    "text": "2.30 Istotność statystyczna: jak rozumieć niepewne dowody\n\nAnalogiczna do sali sądowej logika testowania hipotez\nTestowanie hipotez podąża za logiką podobną do procedur prawnych:\n\nHipoteza zerowa (H\\_0): Oskarżony jest niewinny (brak rzeczywistego efektu)\nHipoteza alternatywna (H\\_1): Oskarżony jest winny (istnieje rzeczywisty efekt)\nDowody: Nasze dane i test statystyczny\nWyrok: Odrzucamy H\\_0 (stwierdzamy istotność) lub nie odrzucamy H\\_0 (brak istotności)\n\nJak w sądzie, potrzeba mocnych dowodów, by odrzucić domniemanie niewinności (brak efektu). Z tego wynikają dwa typy potencjalnych błędów:\n\nBłąd I rodzaju (fałszywie pozytywny): skazanie niewinnego (odrzucenie H\\_0, gdy H\\_0 jest prawdziwa), kontrolowany przez poziom istotności \\alpha (zwykle 0,05)\nBłąd II rodzaju (fałszywie negatywny): uniewinnienie winnego (nieodrzucenie H\\_0, gdy prawdziwa jest H\\_1), z prawdopodobieństwem \\beta i mocą 1-\\beta\n\n\n\nCzym jest istotność statystyczna?\nGdy obserwujemy różnicę w danych, pytamy: czy odzwierciedla ona cechę populacji, czy tylko losową zmienność próbkowania?\nIstotność statystyczna daje ramy odpowiedzi:\n\nCzy zaobserwowany wzorzec jest raczej skutkiem rzeczywistego efektu, czy też mógłby wiarygodnie powstać wyłącznie wskutek przypadku?\n\nRamy te rozróżniają:\n\nSygnał: rzeczywiste wzorce odzwierciedlające relacje w populacji\nSzum: losową zmienność wynikającą z próbkowania\n\n\n\nLogika testowania hipotez\nHipoteza zerowa to domyślne założenie — zwykle brak efektu lub zależności:\n\nbrak różnicy między grupami,\nbrak zależności między zmiennymi,\nbrak efektu „traktowania”.\n\nUtrzymujemy sceptycyzm, dopóki dane nie dostarczą wystarczających dowodów, by ją odrzucić.\n\n\nRozumienie wartości p: trzy komplementarne perspektywy\nWartość p to jeden z najczęściej błędnie rozumianych pojęć w statystyce. Oto trzy uzupełniające się interpretacje:\n\n1. Miara zaskoczenia\nWartość p mówi, na ile „zaskakujące” są nasze dane, gdyby nic systematycznego się nie działo:\n\nMała p (&lt; 0,05): bardzo zaskakujące pod H\\_0 → dowód na efekt\nDuża p (&gt; 0,05): niezaskakujące pod H\\_0 → brak wystarczających dowodów\n\n\n\n2. Ilustracja z rzutem monetą\nTestujemy uczciwość monety. Rzucasz 10 razy i otrzymujesz 8 orłów.\nWartość p odpowiada: gdyby moneta była uczciwa, jak często w 10 rzutach uzyskamy 8 lub więcej orłów?\nObliczenie: P(X \\geq 8) = \\sum\\_{k=8}^{10} \\binom{10}{k} 0{,}5^{10} = \\frac{56}{1024} \\approx 0{,}0547\nPonieważ to dość mało (5,47%), mamy umiarkowane dowody przeciw uczciwości.\n\n\n3. Definicja formalna\nWartość p to:\n\nPrawdopodobieństwo uzyskania danych co najmniej tak skrajnych jak zaobserwowane, zakładając prawdziwość hipotezy zerowej.\n\nFormalnie, dla statystyki testowej T i obserwowanej wartości t\\_{\\text{obs}}:\n\nTest jednostronny: p = P(T \\geq t\\_{\\text{obs}} \\mid H\\_0) lub P(T \\leq t\\_{\\text{obs}} \\mid H\\_0)\nTest dwustronny: p = 2 \\min{P(T \\geq |t\\_{\\text{obs}}| \\mid H\\_0), P(T \\leq -|t\\_{\\text{obs}}| \\mid H\\_0)}\n\nWażne doprecyzowanie: Wartość p zakłada prawdziwość H\\_0 — nie podaje prawdopodobieństwa, że H\\_0 jest prawdziwa.\n\n\n\nWizualizacja wartości p\n\n# Symulacja zachowania statystyki pod hipotezą zerową\nset.seed(789)\nnull_distribution &lt;- rnorm(10000, mean = 0, sd = 1)\n\n# Nasza obserwowana statystyka testowa\nobserved &lt;- 2.1\n\n# Dane do wizualizacji\nhist_data &lt;- data.frame(values = null_distribution)\n\n# Wykres\nggplot(hist_data, aes(x = values)) +\n  geom_histogram(aes(y = ..density..), bins = 50, \n                 fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_density(color = \"darkblue\", linewidth = 1) +\n  geom_vline(xintercept = observed, color = \"red\", linewidth = 1.5) +\n  geom_vline(xintercept = -observed, color = \"red\", linewidth = 1.5, \n             linetype = \"dashed\") +\n  geom_area(stat = \"function\", fun = dnorm, \n            xlim = c(observed, 4), fill = \"red\", alpha = 0.3) +\n  geom_area(stat = \"function\", fun = dnorm, \n            xlim = c(-4, -observed), fill = \"red\", alpha = 0.3) +\n  labs(title = \"Co mierzy wartość p\",\n       subtitle = \"Rozkład możliwych wyników przy prawdziwej hipotezie zerowej\",\n       x = \"Wartości statystyki testowej\",\n       y = \"Gęstość prawdopodobieństwa\") +\n  annotate(\"text\", x = 2.5, y = 0.15, \n           label = \"wartość p:\\nPrawdopodobieństwo\\nwyników tak skrajnych\\nlub bardziej skrajnych\", \n           color = \"red\", fontface = \"bold\", size = 3) +\n  annotate(\"text\", x = 0, y = 0.2, \n           label = \"Najbardziej\\nprawdopodobne wyniki\\nprzy braku efektu\", \n           color = \"darkblue\", size = 3)\n\n\n\n\n\n\n\n\nNiebieski rozkład pokazuje wyniki oczekiwane pod H\\_0. Czerwone linie wyznaczają nasz wynik, a zacieniowane na czerwono obszary — wartość p, czyli prawdopodobieństwo uzyskania wyników co najmniej tak skrajnych wyłącznie wskutek przypadku.\n\n\nPrzykłady: wartość p w kontekście\n\nPrzykład 1: Skuteczność reklam kampanijnych\nPytanie badawcze: Czy reklamy telewizyjne zwiększają udział kandydata w głosach?\nProjekt: Kandydat emituje reklamy w 20 losowo wybranych miastach, a w 20 podobnych miastach — nie.\n\n# Symulacja eksperymentu z reklamami kampanii\nset.seed(123)\n\n# Realistyczne dane\nad_cities &lt;- c(rep(\"Z reklamą\", 20), rep(\"Bez reklam\", 20))\nvote_share &lt;- c(\n  rnorm(20, 0.58, 0.08),  # Miasta z reklamą: średnio 58%, SD 8%\n  rnorm(20, 0.54, 0.08)   # Miasta bez reklamy: średnio 54%, SD 8%\n)\n\ncampaign_data &lt;- data.frame(\n  treatment = factor(ad_cities, levels = c(\"Bez reklam\", \"Z reklamą\")),\n  vote_share = vote_share\n)\n\n# Zaobserwowana różnica\nmean_with_ads &lt;- mean(campaign_data$vote_share[campaign_data$treatment == \"Z reklamą\"])\nmean_no_ads  &lt;- mean(campaign_data$vote_share[campaign_data$treatment == \"Bez reklam\"])\nobserved_diff &lt;- mean_with_ads - mean_no_ads\n\n# Test t (dwustronny domyślnie)\nt_test_result &lt;- t.test(vote_share ~ treatment, data = campaign_data)\np_val &lt;- t_test_result$p.value\n\n# Wykres\nggplot(campaign_data, aes(x = treatment, y = vote_share, fill = treatment)) +\n  geom_boxplot(alpha = 0.7, width = 0.5) +\n  geom_jitter(width = 0.15, alpha = 0.6, size = 2.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4, \n               fill = \"red\", color = \"darkred\") +\n  labs(\n    title = \"Czy reklamy kampanijne zwiększają poparcie?\",\n    subtitle = paste0(\"Zaobserwowana różnica: \", round(observed_diff*100, 1), \n                     \" p.p., p-wartość = \", round(p_val, 3)),\n    x = \"Warunek eksperymentalny\",\n    y = \"Udział w głosach (%)\",\n    caption = \"Czerwone romby pokazują średnie grupowe. Każda kropka to jedno miasto.\"\n  ) +\n  scale_y_continuous(labels = function(x) paste0(x*100, \"%\")) +\n  scale_fill_manual(values = c(\"Bez reklam\" = \"#E8E8E8\", \"Z reklamą\" = \"#4CAF50\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nInterpretacja:\n\nPudełka pokazują środkowe 50% miast w każdej grupie\nCzerwone romby to średnie grupowe\nKropki to poszczególne miasta\nJeśli p &lt; 0,05, mało prawdopodobne, by różnica wynikała wyłącznie z przypadku\n\n\n\nPrzykład 2: Pogoda a frekwencja wyborcza\nPytanie badawcze: Czy deszcz zmniejsza frekwencję?\n\n# Symulacja danych o pogodzie i frekwencji\nset.seed(456)\noptions(scipen = 999)\n\n# Dane z wyraźną różnicą\nn_elections &lt;- 30\nweather_data &lt;- data.frame(\n  weather = factor(c(rep(\"Deszczowo\", n_elections), rep(\"Słonecznie\", n_elections)),\n                   levels = c(\"Słonecznie\", \"Deszczowo\")),\n  turnout = c(\n    rnorm(n_elections, 0.62, 0.06),  # Dni deszczowe: niższa frekwencja\n    rnorm(n_elections, 0.68, 0.06)   # Dni słoneczne: wyższa frekwencja\n  )\n)\n\n# Statystyki\nrain_turnout  &lt;- mean(weather_data$turnout[weather_data$weather == \"Deszczowo\"])\nsunny_turnout &lt;- mean(weather_data$turnout[weather_data$weather == \"Słonecznie\"])\nweather_diff  &lt;- sunny_turnout - rain_turnout  # Słonecznie minus Deszczowo\n\n# Test dwustronny\nweather_test &lt;- t.test(turnout ~ weather, data = weather_data)\nweather_p    &lt;- weather_test$p.value\nci_lower     &lt;- weather_test$conf.int[1]  # CI odpowiada Słonecznie - Deszczowo\nci_upper     &lt;- weather_test$conf.int[2]\n\n# Wykres\nggplot(weather_data, aes(x = weather, y = turnout, fill = weather)) +\n  geom_boxplot(alpha = 0.7, width = 0.5) +\n  geom_jitter(width = 0.15, alpha = 0.6, size = 2.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4, \n               fill = \"black\", color = \"black\") +\n  labs(\n    title = \"Czy pogoda wpływa na frekwencję wyborczą?\",\n    subtitle = paste0(\"Różnica: \", round(weather_diff*100, 1), \n                     \" p.p. (95% PU: [\", \n                     round(ci_lower*100, 1), \", \", round(ci_upper*100, 1), \n                     \"]), p = \", round(weather_p, 3)),\n    x = \"Warunki pogodowe\",\n    y = \"Frekwencja (%)\",\n    caption = \"Czarne romby to średnie. Każda kropka to jedne wybory.\"\n  ) +\n  scale_y_continuous(labels = function(x) paste0(x*100, \"%\")) +\n  scale_fill_manual(values = c(\"Deszczowo\" = \"#B3D9FF\", \"Słonecznie\" = \"#FFD700\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nPodsumowanie wyników:\n\nFrekwencja w dni słoneczne: 68.9%\nFrekwencja w dni deszczowe: 63.4%\nRóżnica: 5.5 p.p.\np-wartość: 0.0008\n\nInterpretacja: Gdyby pogoda nie miała wpływu, istniałoby tylko 0.08% szans, by zaobserwować tak dużą lub większą różnicę. To silny dowód, że pogoda wpływa na frekwencję.\n\n\nPrzykład 3: Wynik nieistotny statystycznie\nPytanie badawcze: Czy czas w mediach społecznościowych przewiduje wiedzę polityczną?\n\n# Przypadek bez istotnej zależności\nset.seed(999)\nn_people &lt;- 150\n\n# Dane z praktycznie brakiem relacji\nsocial_media_data &lt;- data.frame(\n  social_media_hours = runif(n_people, 0, 8),\n  political_knowledge = rnorm(n_people, 50, 15)\n)\n\n# Dodajemy maleńki, trudny do wykrycia efekt\nsocial_media_data$political_knowledge &lt;- social_media_data$political_knowledge + \n  0.5 * social_media_data$social_media_hours + rnorm(n_people, 0, 14)\n\n# Model liniowy\nsm_model  &lt;- lm(political_knowledge ~ social_media_hours, data = social_media_data)\nsm_summary &lt;- summary(sm_model)\nsm_coef &lt;- coef(sm_model)[2]\nsm_p    &lt;- sm_summary$coefficients[2, 4]\nsm_se   &lt;- sm_summary$coefficients[2, 2]\nr_squared &lt;- sm_summary$r.squared\n\n# Wykres rozrzutu z linią regresji\nggplot(social_media_data, aes(x = social_media_hours, y = political_knowledge)) +\n  geom_point(alpha = 0.5, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\", fill = \"pink\", alpha = 0.3) +\n  labs(\n    title = \"Korzystanie z mediów społecznościowych a wiedza polityczna\",\n    subtitle = paste0(\"Efekt: \", round(sm_coef, 2), \" pkt/godz. (SE = \", \n                     round(sm_se, 2), \"), p = \", round(sm_p, 3),\n                     \", R² = \", round(r_squared, 3)),\n    x = \"Godziny dziennie w mediach społecznościowych\",\n    y = \"Wynik wiedzy politycznej (0–100)\",\n    caption = \"Szeroki przedział ufności wskazuje na dużą niepewność zależności\"\n  ) +\n  theme_minimal() +\n  annotate(\"text\", x = 6, y = 20, \n           label = \"Brak istotności\\nstatystycznej\", \n           color = \"red\", fontface = \"bold\", size = 4)\n\n\n\n\n\n\n\n\nNajważniejsze wnioski przy braku istotności:\n\nNie możemy wnioskować, że „nie ma związku”\nMożemy jedynie stwierdzić, że brakuje dowodów na związek\nMożliwe powody braku istotności:\n\nefekt naprawdę nie istnieje,\nefekt jest zbyt mały względem wielkości próby,\nbłąd pomiaru zaciemnia prawdziwą relację.\n\n\n\n\n\nPróg 0,05: konwencja, nie prawo natury\nKonwencjonalny próg p &lt; 0,05 dla „istotności statystycznej” to historyczna konwencja zaproponowana przez Ronalda Fishera w latach 20. XX w.\n\n# Wizualizacja ciągłego charakteru wartości p\np_values &lt;- seq(0.001, 0.2, by = 0.001)\np_data &lt;- data.frame(\n  p = p_values,\n  significant = ifelse(p_values &lt; 0.05, \"Istotny\", \"Nieistotny\")\n)\n\nggplot(p_data, aes(x = p, y = 1, fill = significant)) +\n  geom_tile(aes(height = 1)) +\n  geom_vline(xintercept = 0.05, color = \"black\", linewidth = 1.5) +\n  scale_fill_manual(values = c(\"Istotny\" = \"#4CAF50\", \n                               \"Nieistotny\" = \"#FF6B6B\")) +\n  scale_x_continuous(breaks = c(0.001, 0.01, 0.05, 0.1, 0.15, 0.2),\n                     labels = c(\"0.001\", \"0.01\", \"0.05\", \"0.10\", \"0.15\", \"0.20\")) +\n  labs(\n    title = \"Arbitralność progu 0,05\",\n    subtitle = \"p = 0,049 i p = 0,051 są praktycznie identyczne, choć traktowane odmiennie\",\n    x = \"p-wartość\",\n    y = \"\",\n    fill = \"Konwencjonalna\\ninterpretacja\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank()) +\n  annotate(\"text\", x = 0.025, y = 1, label = \"Silne\\ndowody\", \n           color = \"white\", fontface = \"bold\") +\n  annotate(\"text\", x = 0.125, y = 1, label = \"Słabe\\ndowody\", \n           color = \"white\", fontface = \"bold\") +\n  annotate(\"text\", x = 0.05, y = 0.5, label = \"Arbitralny\\npróg\", \n           color = \"black\", fontface = \"bold\", size = 3)\n\n\n\n\n\n\n\n\nKluczowe uwagi:\n\nNic „magicznego” nie dzieje się przy p = 0,05\nRóżne dziedziny stosują różne progi (np. fizyka: p ≈ 3 × 10⁻⁷, „5 sigma”)\nWspółcześnie zaleca się raportowanie dokładnych p-wartości i wielkości efektu\nDychotomia „istotny/nieistotny” bywa myląca\n\n\n\nCzęste nieporozumienia wokół wartości p\n\nBłędne interpretacje:\n\n„p = 0,03 oznacza 97% szans, że leczenie działa”\n\nBłąd: p nie podaje prawdopodobieństwa prawdziwości hipotezy\n\n„p = 0,20 oznacza mały efekt”\n\nBłąd: p mierzy siłę dowodu, nie wielkość efektu\n\n„p &gt; 0,05 dowodzi, że efektu nie ma”\n\nBłąd: brak dowodów ≠ dowód braku\n\n\n\n\nPoprawne interpretacje:\n\n„p = 0,03 oznacza: gdyby nie było efektu, tak skrajne dane wystąpiłyby tylko w 3% przypadków”\n„p = 0,20 oznacza słabe dowody przeciw hipotezie zerowej”\n„p &gt; 0,05 oznacza, że nie umiemy wiarygodnie odróżnić sygnału od szumu”\n\n\n\n\nIstotność statystyczna a istotność praktyczna\n\n# Różnica między istotnością statystyczną a praktyczną\nset.seed(42)\n\n# Mały, ale statystycznie istotny efekt (duża próba)\nlarge_n &lt;- 10000\ngroup_a_large &lt;- rnorm(large_n, mean = 100, sd = 15)\ngroup_b_large &lt;- rnorm(large_n, mean = 100.5, sd = 15)  # Znikoma różnica\n\n# Duży, ale nieistotny statystycznie efekt (mała próba)\nsmall_n &lt;- 20\ngroup_a_small &lt;- rnorm(small_n, mean = 100, sd = 15)\ngroup_b_small &lt;- rnorm(small_n, mean = 105, sd = 15)  # Duża różnica\n\n# Testy\ntest_large &lt;- t.test(group_a_large, group_b_large)\ntest_small &lt;- t.test(group_a_small, group_b_small)\n\n# Dane do porównania\ncomparison_data &lt;- data.frame(\n  Scenario = c(\"Duża próba\\n(n=10 000)\", \"Mała próba\\n(n=20)\"),\n  Effect_Size = c(mean(group_b_large) - mean(group_a_large),\n                  mean(group_b_small) - mean(group_a_small)),\n  P_Value = c(test_large$p.value, test_small$p.value),\n  Significant = c(test_large$p.value &lt; 0.05, test_small$p.value &lt; 0.05)\n)\n\nggplot(comparison_data, aes(x = Effect_Size, y = -log10(P_Value))) +\n  geom_point(aes(color = Significant, shape = Scenario), size = 8) +\n  geom_hline(yintercept = -log10(0.05), linetype = \"dashed\", color = \"red\") +\n  geom_text(aes(label = Scenario), vjust = -1.5, size = 3) +\n  scale_color_manual(values = c(\"FALSE\" = \"gray60\", \"TRUE\" = \"darkgreen\")) +\n  labs(\n    title = \"Istotność statystyczna vs. praktyczna\",\n    subtitle = \"Duże próby wykrywają mikroskopijne efekty; małe próby mogą „przegapić” duże\",\n    x = \"Wielkość efektu (różnica średnich)\",\n    y = \"Istotność statystyczna\\n(-log10 p-wartości)\",\n    caption = \"Punkty powyżej czerwonej linii są istotne statystycznie (p &lt; 0,05)\"\n  ) +\n  theme_minimal() +\n  annotate(\"text\", x = 0.5, y = -log10(0.05), \n           label = \"p = 0,05\", color = \"red\", size = 3) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nWyniki porównania:\nDuża próba (n = 10 000 na grupę):\n\nRóżnica: 0.68 jednostki\np-wartość: 1.412962e-03\nIstotność statystyczna: Tak\nIstotność praktyczna: raczej nie (różnica minimalna)\n\nMała próba (n = 20 na grupę):\n\nRóżnica: 4.42 jednostki\np-wartość: 0.344\nIstotność statystyczna: Nie\nIstotność praktyczna: możliwe, że tak (różnica duża)\n\nKluczowa lekcja: Zawsze oceniaj zarówno istotność statystyczną, jak i wielkość efektu.\n\n\nZwiązek między p-wartościami a przedziałami ufności\nIstnieje bezpośrednia korespondencja:\n\nJeśli p &lt; 0,05 dla testu „brak różnicy”, 95% PU nie obejmuje zera\nJeśli p &gt; 0,05, 95% PU obejmuje zero\n\n\n# Ilustracja relacji p i PU\nset.seed(789)\n\n# Kilka \"badań\" z różnymi efektami\nstudies &lt;- data.frame(\n  study = LETTERS[1:6],\n  effect = c(2.5, 2.2, 0.9, 0.3, -0.2, -1.5),\n  se = rep(1, 6)\n)\n\nstudies$ci_lower &lt;- studies$effect - 1.96 * studies$se\nstudies$ci_upper &lt;- studies$effect + 1.96 * studies$se\nstudies$p_value  &lt;- 2 * pnorm(-abs(studies$effect/studies$se))\nstudies$significant &lt;- studies$p_value &lt; 0.05\n\nggplot(studies, aes(x = study, y = effect, color = significant)) +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"gray50\", linewidth = 1) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2, linewidth = 1) +\n  geom_point(size = 4) +\n  scale_color_manual(values = c(\"FALSE\" = \"gray60\", \"TRUE\" = \"darkgreen\"),\n                     labels = c(\"Nieistotny\", \"Istotny\")) +\n  labs(\n    title = \"Przedziały ufności a istotność statystyczna\",\n    subtitle = \"PU wykluczające zero odpowiadają p &lt; 0,05\",\n    x = \"Badanie\",\n    y = \"Wielkość efektu\",\n    color = \"Istotność statystyczna\",\n    caption = \"Słupki błędów to 95% przedziały ufności\"\n  ) +\n  theme_minimal() +\n  annotate(\"text\", x = 0.5, y = 0.2, label = \"Brak efektu\", \n           color = \"gray50\", fontface = \"italic\", size = 3) +\n  geom_text(aes(label = paste0(\"p=\", round(p_value, 3))), \n            vjust = -2, size = 3)\n\n\n\n\n\n\n\n\nObserwacje:\n\nBadania A, B: PU nie obejmują zera → p &lt; 0,05\nBadania C, D, E, F: PU obejmują zero → p &gt; 0,05\nOdległość od zera jest odwrotnie skorelowana z wielkością p\n\n\n\nKompletny przykład: analiza eksperymentu politycznego\nPytanie badawcze: Czy przekaz „fact-checking” (weryfikacja faktów) zmniejsza wiarę w dezinformację?\n\n# Symulacja eksperymentu fact-checking\nset.seed(2024)\n\n# Dane eksperymentalne\nn_per_group &lt;- 100\nexperiment_data &lt;- data.frame(\n  group = c(rep(\"Kontrola\", n_per_group), rep(\"Weryfikacja faktów\", n_per_group)),\n  misinformation_belief = c(\n    rnorm(n_per_group, mean = 65, sd = 12),  # Kontrola: wyższa wiara\n    rnorm(n_per_group, mean = 58, sd = 12)   # Traktowanie: niższa wiara\n  )\n)\n\n# Krok 1: Statystyki opisowe\ndesc_stats &lt;- experiment_data %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    n = n(),\n    mean = mean(misinformation_belief),\n    sd = sd(misinformation_belief),\n    se = sd / sqrt(n),\n    .groups = \"drop\"\n  )\n\n# Średnie\nmean_control &lt;- desc_stats$mean[desc_stats$group == \"Kontrola\"]\nmean_fact    &lt;- desc_stats$mean[desc_stats$group == \"Weryfikacja faktów\"]\ndifference_fc &lt;- mean_fact - mean_control\n\n# Krok 2: Test t\nt_result &lt;- t.test(misinformation_belief ~ group, data = experiment_data)\n\n# Konwersja PU do różnicy (Weryfikacja faktów − Kontrola)\nci_fc_lower &lt;- -t_result$conf.int[2]\nci_fc_upper &lt;- -t_result$conf.int[1]\n\n# Krok 3: Wielkość efektu (d Cohena)\npooled_sd &lt;- sqrt(((n_per_group - 1) * desc_stats$sd[desc_stats$group==\"Kontrola\"]^2 + \n                   (n_per_group - 1) * desc_stats$sd[desc_stats$group==\"Weryfikacja faktów\"]^2) / \n                  (2 * n_per_group - 2))\ncohens_d &lt;- difference_fc / pooled_sd\n\n# Krok 4: Wizualizacja wyników\nggplot(experiment_data, aes(x = group, y = misinformation_belief, fill = group)) +\n  geom_violin(alpha = 0.3) +\n  geom_boxplot(width = 0.3, alpha = 0.7) +\n  geom_jitter(width = 0.1, alpha = 0.3, size = 1) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4, \n               fill = \"yellow\", color = \"black\") +\n  scale_fill_manual(values = c(\"Kontrola\" = \"#FF6B6B\", \"Weryfikacja faktów\" = \"#4ECDC4\")) +\n  labs(\n    title = \"Czy weryfikacja faktów zmniejsza wiarę w dezinformację?\",\n    subtitle = paste0(\"Różnica (Weryfikacja − Kontrola): \", round(difference_fc, 1), \n                     \" pkt, p = \", round(t_result$p.value, 3),\n                     \", d Cohena = \", round(cohens_d, 2)),\n    x = \"Warunek eksperymentalny\",\n    y = \"Wiara w dezinformację (0–100)\",\n    caption = \"Żółte romby to średnie grupowe\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Krok 5: Raport\ncat(\"WYNIKI TESTU HIPOTEZ\\n\")\n\nWYNIKI TESTU HIPOTEZ\n\ncat(\"H0: Weryfikacja faktów nie wpływa na wiarę w dezinformację\\n\")\n\nH0: Weryfikacja faktów nie wpływa na wiarę w dezinformację\n\ncat(\"H1: Weryfikacja faktów wpływa na wiarę w dezinformację\\n\\n\")\n\nH1: Weryfikacja faktów wpływa na wiarę w dezinformację\n\ncat(\"Statystyki opisowe:\\n\")\n\nStatystyki opisowe:\n\nprint(desc_stats)\n\n# A tibble: 2 × 5\n  group                  n  mean    sd    se\n  &lt;chr&gt;              &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Kontrola             100  64.0  12.3  1.23\n2 Weryfikacja faktów   100  59.6  12.3  1.23\n\ncat(\"\\nWnioskowanie:\\n\")\n\n\nWnioskowanie:\n\ncat(paste(\"Różnica (Weryfikacja − Kontrola):\", round(difference_fc, 2), \"pkt\\n\"))\n\nRóżnica (Weryfikacja − Kontrola): -4.4 pkt\n\ncat(paste(\"95% PU: [\", round(ci_fc_lower, 2), \",\", round(ci_fc_upper, 2), \"]\\n\"))\n\n95% PU: [ -7.82 , -0.98 ]\n\ncat(paste(\"Statystyka t:\", round(t_result$statistic, 2), \"\\n\"))\n\nStatystyka t: 2.53 \n\ncat(paste(\"p-wartość:\", round(t_result$p.value, 4), \"\\n\"))\n\np-wartość: 0.012 \n\neffect_size_label &lt;- ifelse(abs(cohens_d) &lt; 0.2, \"pomijalny\",\n                           ifelse(abs(cohens_d) &lt; 0.5, \"mały\",\n                                  ifelse(abs(cohens_d) &lt; 0.8, \"średni\", \"duży\")))\ncat(paste(\"d Cohena:\", round(cohens_d, 2), \"(\", effect_size_label, \"efekt)\\n\\n\"))\n\nd Cohena: -0.36 ( mały efekt)\n\ncat(\"Wniosek:\\n\")\n\nWniosek:\n\nif(t_result$p.value &lt; 0.05) {\n  cat(\"Odrzucamy hipotezę zerową. Dowody sugerują,\\n\")\n  cat(\"że weryfikacja faktów istotnie zmniejsza wiarę w dezinformację.\\n\")\n} else {\n  cat(\"Nie odrzucamy hipotezy zerowej. Brakuje wystarczających\\n\")\n  cat(\"dowodów, że weryfikacja faktów wpływa na wiarę w dezinformację.\\n\")\n}\n\nOdrzucamy hipotezę zerową. Dowody sugerują,\nże weryfikacja faktów istotnie zmniejsza wiarę w dezinformację.\n\n\nUwaga o hipotezach kierunkowych: Jeśli teoria przewiduje kierunek (np. weryfikacja faktów zmniejsza wiarę w dezinformację), dopuszczalne są testy jednostronne. Ustaw alternative = \"greater\" dla \\mu\\_{\\text{Kontrola}} &gt; \\mu\\_{\\text{Weryfikacja faktów}} lub alternative = \"less\" dla odwrotności. Gdy każda różnica ma znaczenie teoretyczne, standardem pozostaje test dwustronny.\n\n\nPodsumowanie: praktyczne wskazówki dotyczące istotności\n\nNajpierw oceń wielkość efektu\n\nJaka jest skala różnicy/zależności?\nCzy jest praktycznie istotna?\n\nSpójrz na p-wartość\n\np &lt; 0,05: dowody przeciw H\\_0\np &gt; 0,05: niewystarczające dowody do odrzucenia H\\_0\n\nInterpretuj przedziały ufności\n\nOkreślają zakres wiarygodnych wartości efektu\nSzerokie PU = większa niepewność\n\nUwzględnij kontekst badania\n\nWielkość próby wpływa na moc testu\nJakość projektu ważniejsza niż sama p-wartość\nWielokrotne testowanie zwiększa ryzyko fałszywych trafień (błąd I rodzaju)\n\n\nZasady podstawowe:\nIstotność statystyczna ≠ istotność praktyczna. Wartość p mierzy „zaskoczenie” przy założeniu prawdziwości H\\_0, a nie prawdopodobieństwo prawdy. Brak dowodów to nie dowód braku. Zawsze raportuj p-wartości wraz z wielkością efektu.\nIstotność statystyczna to narzędzie do odróżniania sygnału od szumu, a nie miara ważności czy prawdziwości. Stosuj ją rozważnie — w szerszym kontekście merytorycznym i praktycznej istotności.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#regresja-prawdopodobnie-najważniejsza-metoda-badawcza-w-politologii",
    "href": "rozdzial1.html#regresja-prawdopodobnie-najważniejsza-metoda-badawcza-w-politologii",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.31 Regresja: prawdopodobnie najważniejsza metoda badawcza w politologii",
    "text": "2.31 Regresja: prawdopodobnie najważniejsza metoda badawcza w politologii\nWyobraź sobie typowy nagłówek sprzed wyborów: „Poparcie dla kandydatki/kandydata Smith sięga 68%”. Intuicyjnie wnioskujesz, że ma dobre perspektywy — nie gwarancję zwycięstwa, ale silną pozycję.\nTa intuicyjna ocena dobrze oddaje istotę analizy regresji. Wykorzystujesz jedną informację (poziom poparcia), aby przewidzieć inną (wynik wyborczy), zakładając, że wyższe poparcie zwykle łączy się z lepszym rezultatem, choć zależność nie jest doskonała.\nAnaliza regresji systematyzuje ten proces i pozwala badaczom:\n\ngenerować predykcje na podstawie dostępnych danych,\nidentyfikować, które czynniki mają największe znaczenie,\nkwantyfikować niepewność prognoz,\ntestować hipotezy i teorie na danych empirycznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zmienne-i-zmienność",
    "href": "rozdzial1.html#zmienne-i-zmienność",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.32 Zmienne i zmienność",
    "text": "2.32 Zmienne i zmienność\n\nDefinicja zmiennej\nZmienna to cecha, która może przyjmować różne wartości w różnych jednostkach obserwacji. W politologii:\n\nJednostki analizy: państwa, osoby, wybory, polityki publiczne, lata\nZmienne: PKB, preferencja wyborcza, indeks demokracji, wystąpienie konfliktu",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#czym-jest-regresja",
    "href": "rozdzial1.html#czym-jest-regresja",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.33 Czym jest regresja?",
    "text": "2.33 Czym jest regresja?\nAnaliza regresji to podstawowe narzędzie statystyczne w politologii. Modeluje związki między zmiennymi i operacjonalizuje nasz fundamentalny model statystyczny.\n\nModel fundamentalny\nModel to odwzorowanie obiektu lub systemu w użyteczny sposób. Mogą to być reprezentacje fizyczne (np. makiety architektoniczne) lub abstrakcyjne (np. równania opisujące zjawiska).\nSedno myślenia statystycznego można zapisać jako:\nY = f(X) + \\text{błąd}\nTo równanie mówi, że nasz wynik (Y) równa się pewnej funkcji predyktorów (X) plus nieprzewidywalna wariancja.\nSkładniki:\n\nY — zmienna zależna (zjawisko, które wyjaśniamy),\nX — zmienna(e) niezależna(e) (czynniki wyjaśniające),\nf() — postać zależności (często zakładamy liniową),\nbłąd (\\epsilon) — niewyjaśniona zmienność.\n\nNa tym fundamencie opierają się wszystkie analizy — od prostych korelacji po złożone algorytmy uczenia maszynowego.\nRegresja pomaga odpowiadać na pytania takie jak:\n\nO ile edukacja zwiększa uczestnictwo polityczne?\nJakie czynniki przewidują sukces wyborczy?\nCzy instytucje demokratyczne sprzyjają wzrostowi gospodarczemu?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#budowanie-intuicji-analogia-sportowa",
    "href": "rozdzial1.html#budowanie-intuicji-analogia-sportowa",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.34 Budowanie intuicji: analogia sportowa",
    "text": "2.34 Budowanie intuicji: analogia sportowa\nZanim przejdziemy do polityki, spójrzmy na prostszy kontekst. Chcemy przewidywać liczbę punktów koszykarzy na podstawie wzrostu. Oczekujemy, że:\n\nwyżsi gracze przeciętnie zdobywają więcej punktów,\nsam wzrost nie determinuje wyniku (liczą się też umiejętności, pozycja, minuty gry),\nwystępuje duża zmienność — niżsi zawodnicy też potrafią dużo rzucać.\n\nWykres wzrostu (oś X) i punktów (oś Y) prawdopodobnie pokaże:\n\nrosnący trend punktów wraz ze wzrostem,\nspory rozrzut wokół trendu,\nlinię streszczającą ogólną zależność.\n\nTo jest istota regresji: znaleźć linię najlepiej podsumowującą związek między zmiennymi, przyznając, że korelacja jest niedoskonała.\n\n# Tworzymy przykład \"koszykarski\" dla intuicji\nset.seed(123)\nn_players &lt;- 100\n\n# Realistyczne dane: wzrost i punkty\nheight_inches &lt;- rnorm(n_players, 78, 4)  # Średnio ok. 6'6\"\n# Punkty rosną ze wzrostem, ale z dużą wariancją\npoints_per_game &lt;- 2 + 0.3 * (height_inches - 70) + rnorm(n_players, 0, 5)\npoints_per_game &lt;- pmax(0, points_per_game)  # Brak ujemnych wartości\n\nbasketball_data &lt;- data.frame(\n  height = height_inches,\n  points = points_per_game\n)\n\n# Wizualizacja\nggplot(basketball_data, aes(x = height, y = points)) +\n  geom_point(alpha = 0.6, color = \"orange\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"blue\", size = 1.2) +\n  labs(\n    title = \"Wzrost a zdobyte punkty: podstawowa idea regresji\",\n    subtitle = \"Niebieska linia pokazuje zależność przeciętną; punkty to poszczególni zawodnicy\",\n    x = \"Wzrost (cale)\",\n    y = \"Punkty na mecz\",\n    caption = \"Każdy punkt to jeden gracz; linia podsumowuje ogólny wzorzec\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretacja: Każdy pomarańczowy punkt to jeden zawodnik. Niebieska linia pokazuje trend — wyżsi gracze przeciętnie zdobywają więcej punktów. Rozrzut wokół linii odzwierciedla inne czynniki (umiejętności, pozycję, minuty gry, system zespołu itp.).\nKluczowa myśl: Linia nie przechodzi przez wszystkie punkty, bo wzrost to tylko jeden z wielu czynników. „Szum” wokół linii to efekt pominiętych zmiennych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#prosta-regresja-liniowa",
    "href": "rozdzial1.html#prosta-regresja-liniowa",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.35 Prosta regresja liniowa",
    "text": "2.35 Prosta regresja liniowa\nPodstawowe równanie formalizuje tę relację:\nY_i = \\alpha + \\beta X_i + \\epsilon_i\nGdzie:\n\nY\\_i — wynik dla obserwacji i,\nX\\_i — predyktor dla obserwacji i,\n\\alpha — wyraz wolny (oczekiwane Y, gdy X=0),\n\\beta — nachylenie (zmiana Y na jednostkę X),\n\\epsilon\\_i — składnik losowy.\n\nW przykładzie koszykarskim:\n\nY\\_i — punkty gracza i,\nX\\_i — wzrost gracza i,\n\\alpha — bazowa liczba punktów (konstrukt matematyczny przy wzroście 0),\n\\beta — dodatkowe punkty na cal wzrostu,\n\\epsilon\\_i — inne czynniki wpływające na zdobycz punktową.\n\n\nPrzykład: edukacja a partycypacja polityczna\nCzy edukacja zwiększa partycypację polityczną?\n\n\n\n\n\n\n\n\n\nWyniki statystyczne:\n\n\n• Każdy dodatkowy rok edukacji zwiększa partycypację o 0.029 punktu(y)\n\n\n• Edukacja wyjaśnia 9.2 % zróżnicowania partycypacji\n\n\n• Pozostałe 90.8 % to czynniki niewobecne w modelu\n\n\nInterpretacja R²: R² to odsetek zmienności zmiennej zależnej wyjaśniony przez model. Np. R² = 0,3 oznacza, że 30% zróżnicowania partycypacji tłumaczą zmienne w modelu, a 70% pozostaje niewyjaśnione.\n\n\nRozpisanie równania regresji na elementy\nW odniesieniu do przykładu edukacja–partycypacja:\nY_i = \\alpha + \\beta X_i + \\epsilon_i\n\nY\\_i: partycypacja polityczna osoby i,\n\\alpha (wyraz wolny): oczekiwana partycypacja przy 0 latach edukacji,\n\\beta (nachylenie): zmiana partycypacji na dodatkowy rok edukacji,\nX\\_i: liczba lat edukacji osoby i,\n\\epsilon\\_i: inne czynniki (dochód, wiek, zainteresowanie polityką itd.).\n\nRamowy opis: Partycypacja = poziom bazowy (\\alpha) + wpływ edukacji (\\beta \\times edukacja) + niewyjaśnione czynniki (\\epsilon).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#regresja-wieloraka-złożoność-zjawisk",
    "href": "rozdzial1.html#regresja-wieloraka-złożoność-zjawisk",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.36 Regresja wieloraka: złożoność zjawisk",
    "text": "2.36 Regresja wieloraka: złożoność zjawisk\nRzeczywistość rzadko zależy od jednego czynnika. Na partycypację wpływa edukacja, ale też dochód, wiek, zainteresowanie polityką… Regresja wieloraka pozwala uwzględnić kilka zmiennych jednocześnie.\n\nCo oznacza „kontrolując za …”?\nTo trudne, ale kluczowe pojęcie. Analogią może być porównanie szkół.\nPorównanie szkół: Surowe wyniki testów uczniów szkół prywatnych i publicznych nie wystarczą. Uczniowie szkół prywatnych częściej pochodzą z zamożniejszych i lepiej wykształconych rodzin. Obserwowane różnice mogą odzwierciedlać tło rodzinne, a nie „jakość szkoły”.\nUczciwe porównanie wymaga zestawiania uczniów o podobnym tle — zamożnych ze zamożnymi, średniozamożnych ze średniozamożnymi, w obrębie obu typów szkół.\n„Kontrola statystyczna” realizuje to porównanie matematycznie. Gdy mówimy: „Edukacja zwiększa partycypację o 0,04 punktu, kontrolując dochód i wiek”, mamy na myśli:\n\nporównanie osób o tym samym dochodzie i wieku,\nz których ta z dłuższą edukacją uczestniczy przeciętnie o 0,04 punktu bardziej,\nczyli „oczyszczamy” wpływ potencjalnych konfunderów.\n\nFormalnie:\nY_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_k X_{ki} + \\epsilon_i\nKażde \\beta\\_j interpretuje się jako efekt X\\_j przy stałych pozostałych zmiennych.\n\n\nPrzykład: determinanty sukcesu wyborczego\n\n\n\nWpływ włączenia zmiennych kontrolnych\n\n\nModel\nEfekt poparcia\nWartość p\n\n\n\n\nProsty (tylko poparcie)\n0.781\n0\n\n\nWieloraki (kontrola: gospodarka i wydatki)\n0.750\n0\n\n\n\n\n\nDeterminanty sukcesu wyborczego:\n\n\n• ↑ o 1 p.p. w poparciu → +0.7 pkt marginesu zwycięstwa \n• ↑ o 1 p.p. wzrostu PKB → +2.3 pkt marginesu \n• +1 mln USD wydatków → +0.03 pkt marginesu \n\n\n\nTe czynniki łącznie wyjaśniają 71.5% zróżnicowania wyników wyborów\n\n\nPozostałe 28.5% to czynniki niewobecne w modelu\n\n\nWniosek: Efekt poparcia zmienia się po dodaniu innych zmiennych. Dlatego kontrola za konfunderami jest kluczowa — pominięte zmienne mogą silnie zniekształcać wnioski.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wyzwanie-wnioskowania-przyczynowego-czy-pieniądze-kupują-wybory",
    "href": "rozdzial1.html#wyzwanie-wnioskowania-przyczynowego-czy-pieniądze-kupują-wybory",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.37 Wyzwanie wnioskowania przyczynowego: czy pieniądze „kupują” wybory?",
    "text": "2.37 Wyzwanie wnioskowania przyczynowego: czy pieniądze „kupują” wybory?\nTo pytanie pokazuje ograniczenia regresji i różnicę między korelacją a przyczynowością.\nObserwacja: Kandydaci, którzy wydają więcej, zwykle dostają więcej głosów. Czy wydatki powodują głosy?\nAlternatywne wyjaśnienia:\n\nOdwrócona przyczynowość: popularni kandydaci przyciągają więcej darczyńców,\nWspólna przyczyna: charyzma zwiększa i darowizny, i poparcie,\nSelekcja: dobrze finansowani częściej startują w konkurencyjnych wyścigach.\n\nKluczowy problem: korelacja ≠ przyczynowość.\n\nFundamentalny problem wnioskowania przyczynowego\nAby ustalić skutek przyczynowy, chcielibyśmy obserwować tego samego kandydata w dwóch równoległych scenariuszach:\n\nA: wydaje 5 mln,\nB: wydaje 1 mln,\nefekt = różnica w odsetku głosów.\n\nProblem: obserwujemy tylko jeden scenariusz. To tzw. „fundamentalny problem wnioskowania przyczynowego”.\n\n\nStrategie identyfikacji przyczynowej\nBadacze stosują różne podejścia:\n1. Eksperymenty losowe (złoty standard)\n\nlosowy przydział do grupy traktowanej/kontrolnej,\ngrupy identyczne poza „traktowaniem”,\nróżnice można przypisać traktowaniu.\n\n2. Eksperymenty naturalne\n\nbardzo wyrównane wybory tworzą quasi-losową zmienność,\nzmiany polityk dotyczą jednych obszarów, a innych nie,\nkatastrofy naturalne dostarczają egzogenicznych wstrząsów.\n\n3. Kontrola statystyczna\n\nwłączamy do regresji zmienne będące konfunderami,\nwspółczynniki interpretujemy przyczynowo pod silnymi założeniami,\nograniczenie: musimy zmierzyć wszystkie istotne konfudery.\n\n\n# Konfudowanie w danych o finansowaniu kampanii\nset.seed(789)\nn_candidates &lt;- 500\n\n# Jakość kandydata wpływa i na wydatki, i na głosy\ncandidate_quality &lt;- rnorm(n_candidates, 0, 1)\n\n# Jakość → fundraising\nspending &lt;- 50 + 20 * candidate_quality + rnorm(n_candidates, 0, 10)\nspending &lt;- pmax(0, spending)\n\n# Głosy zależą od wydatków ORAZ jakości\nvote_share &lt;- 30 + 0.1 * spending + 15 * candidate_quality + rnorm(n_candidates, 0, 5)\nvote_share &lt;- pmax(0, pmin(100, vote_share))\n\ncampaign_data &lt;- data.frame(\n  spending = spending,\n  quality = candidate_quality,\n  vote_share = vote_share\n)\n\n# Porównanie analiz\nnaive_model &lt;- lm(vote_share ~ spending, data = campaign_data)\ncontrolled_model &lt;- lm(vote_share ~ spending + quality, data = campaign_data)\n\n# Prawdziwy efekt wydatków to 0,1\ncomparison_results &lt;- data.frame(\n  Model = c(\"Naiwny (bez kontroli)\", \"Właściwy (kontrola jakości)\"),\n  Efekt_wydatkow = c(coef(naive_model)[2], coef(controlled_model)[2]),\n  Prawdziwy_efekt = c(0.1, 0.1)\n) %&gt;%\n  mutate(\n    Błąd = Efekt_wydatkow - Prawdziwy_efekt,\n    Kierunek_biasu = dplyr::case_when(\n      abs(Błąd) &lt; 0.05 ~ \"Bez istotnego biasu\",\n      Błąd &gt; 0 ~ \"Bias w górę\",\n      Błąd &lt; 0 ~ \"Bias w dół\"\n    )\n  )\n\nkable(comparison_results, \n      digits = 3,\n      caption = \"Znaczenie kontroli za konfunderami\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nZnaczenie kontroli za konfunderami\n\n\nModel\nEfekt_wydatkow\nPrawdziwy_efekt\nBłąd\nKierunek_biasu\n\n\n\n\nNaiwny (bez kontroli)\n0.681\n0.1\n0.581\nBias w górę\n\n\nWłaściwy (kontrola jakości)\n0.155\n0.1\n0.055\nBias w górę\n\n\n\n\n# Wizualizacja konfudowania\nggplot(campaign_data, aes(x = spending, y = vote_share, color = quality)) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linetype = \"dashed\") +\n  scale_color_gradient2(low = \"blue\", mid = \"gray\", high = \"red\", \n                       midpoint = 0, name = \"Jakość\\nkandydata\") +\n  labs(\n    title = \"Konfudowanie w finansowaniu kampanii\",\n    subtitle = \"Czerwona linia: naiwna korelacja; prawdziwy efekt wymaga kontroli jakości\",\n    x = \"Wydatki kampanijne (tys. USD)\",\n    y = \"Odsetek głosów (%)\",\n    caption = \"Kolor odzwierciedla jakość kandydata — koreluje i z wydatkami, i z głosami\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nKluczowa lekcja: Bez kontroli za „jakość kandydata” przeszacowujemy wpływ wydatków. Naiwna analiza myli efekt jakości z efektem wydatków.\nJasne — poniżej masz polską wersję ostatniego fragmentu z zachowaniem układu Quarto i działającego kodu R.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#najczęstsze-pułapki-w-analizie-regresji",
    "href": "rozdzial1.html#najczęstsze-pułapki-w-analizie-regresji",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.38 Najczęstsze pułapki w analizie regresji (*)",
    "text": "2.38 Najczęstsze pułapki w analizie regresji (*)\n\nPułapka 1: Mylenie istotności statystycznej z praktyczną\nProblem: Uznawanie bardzo małych, choć istotnych statystycznie efektów za merytorycznie ważne wyniki.\nDlaczego się zdarza: Duże próby sprawiają, że maleńkie efekty stają się istotne statystycznie. Badanie 100 000 wyborców może „wykryć”, że negatywne reklamy obniżają frekwencję o 0,0001 p.p. przy p &lt; 0,001.\nPrzykład:\n\nRaportowane: „Negatywne reklamy istotnie obniżają frekwencję”\nRzeczywistość: Efekt = −0,001 p.p. na reklamę\nZnaczenie praktyczne: Znikome\n\n\n# Różnica: istotność statystyczna vs. praktyczna\nset.seed(123)\n\n# Duża próba, malutki efekt\nn_large &lt;- 10000\ntreatment_large &lt;- rep(c(0, 1), each = n_large/2)\noutcome_large &lt;- 0.5 + 0.001 * treatment_large + rnorm(n_large, 0, 0.1)\nresult_large &lt;- t.test(outcome_large ~ treatment_large)\n\n# Mała próba, większy efekt\nn_small &lt;- 100\ntreatment_small &lt;- rep(c(0, 1), each = n_small/2)\noutcome_small &lt;- 0.5 + 0.05 * treatment_small + rnorm(n_small, 0, 0.1)\nresult_small &lt;- t.test(outcome_small ~ treatment_small)\n\ncat(\"Istotność statystyczna a praktyczna:\\n\\n\")\n\nIstotność statystyczna a praktyczna:\n\ncat(\"Duża próba (n = 10 000):\\n\")\n\nDuża próba (n = 10 000):\n\ncat(\"  Wielkość efektu:\", round(diff(result_large$estimate), 5), \"\\n\")\n\n  Wielkość efektu: 0.00064 \n\ncat(\"  p-wartość:\", format(result_large$p.value, scientific = TRUE), \"\\n\")\n\n  p-wartość: 7.488156e-01 \n\ncat(\"  Ocena: istotny statystycznie, lecz praktycznie bez znaczenia\\n\\n\")\n\n  Ocena: istotny statystycznie, lecz praktycznie bez znaczenia\n\ncat(\"Mała próba (n = 100):\\n\")\n\nMała próba (n = 100):\n\ncat(\"  Wielkość efektu:\", round(diff(result_small$estimate), 3), \"\\n\")\n\n  Wielkość efektu: 0.031 \n\ncat(\"  p-wartość:\", round(result_small$p.value, 3), \"\\n\")\n\n  p-wartość: 0.117 \n\ncat(\"  Ocena: nieistotny statystycznie, ale potencjalnie istotny praktycznie\\n\")\n\n  Ocena: nieistotny statystycznie, ale potencjalnie istotny praktycznie\n\n\nWytyczne dot. wielkości efektu (konwencje Cohena):\n\nPomijalny: &lt; 0,1 SD\nMały: 0,1–0,3 SD\nŚredni: 0,3–0,8 SD\nDuży: &gt; 0,8 SD\n\nZasada kluczowa: Zawsze oceniaj, czy wielkość efektu ma znaczenie merytoryczne.\n\n\nPułapka 2: Przeuczenie (overfitting)\nProblem: Budowanie modeli zbyt złożonych względem danych — model „uczy się na pamięć”, a nie uogólnia.\nKonsekwencje: Sztucznie zawyżone miary dopasowania, które nie replikują się na nowych danych.\n\n# Prosty vs. złożony model\nset.seed(456)\nn_obs &lt;- 50\n\n# Tylko X1 ma znaczenie\nx1 &lt;- rnorm(n_obs)\nx2 &lt;- rnorm(n_obs)  # nieistotny\nx3 &lt;- rnorm(n_obs)  # nieistotny\nx4 &lt;- rnorm(n_obs)  # nieistotny\nx5 &lt;- rnorm(n_obs)  # nieistotny\n\ny &lt;- 2 + 1.5 * x1 + rnorm(n_obs, 0, 1)\n\n# Porównanie modeli\nsimple_model &lt;- lm(y ~ x1)\ncomplex_model &lt;- lm(y ~ x1 + x2 + x3 + x4 + x5)\n\ncat(\"Porównanie modeli:\\n\")\n\nPorównanie modeli:\n\ncat(\"R² modelu prostego:\", round(summary(simple_model)$r.squared, 3), \"\\n\")\n\nR² modelu prostego: 0.719 \n\ncat(\"R² modelu złożonego:\", round(summary(complex_model)$r.squared, 3), \"\\n\")\n\nR² modelu złożonego: 0.73 \n\ncat(\"Skorygowany R² (złożony):\", round(summary(complex_model)$adj.r.squared, 3), \"\\n\\n\")\n\nSkorygowany R² (złożony): 0.699 \n\ncat(\"Uwaga: wyższy R² w modelu złożonym bywa mylący — skorygowany R² ujawnia minimalną poprawę\\n\")\n\nUwaga: wyższy R² w modelu złożonym bywa mylący — skorygowany R² ujawnia minimalną poprawę\n\n\nSygnały ostrzegawcze:\n\nZmiennych więcej niż uzasadnia teoria\nR² rośnie, ale skorygowany R² stoi w miejscu\nSłabe przewidywanie poza próbą (out-of-sample)\nZmienne włączone bez uzasadnienia teoretycznego\n\nSkorygowany R²: Kara za złożoność — uczciwsza ocena poprawy modelu.\nJak zapobiegać:\n\nUwzględniaj tylko zmienne uzasadnione teoretycznie\nMonitoruj skorygowany R² (nie sam R²)\nStosuj walidację krzyżową\nZachowuj oszczędność (parsymonię)\n\n\n\nPułapka 3: Problem wielokrotnego testowania\nProblem: Testowanie wielu zależności i raportowanie tylko „istotnych”.\nFakt statystyczny: Przy α = 0,05 oczekuj 5% fałszywych trafień. 20 testów → ~1 „istotny” wynik przez przypadek.\n\n# Symulacja problemu wielokrotnego testowania\nset.seed(789)\nn_tests &lt;- 20\np_values &lt;- numeric(n_tests)\n\n# Brak prawdziwych efektów\nfor(i in 1:n_tests) {\n  x &lt;- rnorm(100)\n  y &lt;- rnorm(100)\n  test_result &lt;- cor.test(x, y)\n  p_values[i] &lt;- test_result$p.value\n}\n\nsignificant_tests &lt;- sum(p_values &lt; 0.05)\n\ncat(\"Wielokrotne testowanie:\\n\")\n\nWielokrotne testowanie:\n\ncat(\"Liczba testów:\", n_tests, \"\\n\")\n\nLiczba testów: 20 \n\ncat(\"\\\"Istotnych\\\" wyników:\", significant_tests, \"\\n\")\n\n\"Istotnych\" wyników: 0 \n\ncat(\"Oczekiwane przez przypadek:\", round(n_tests * 0.05), \"\\n\")\n\nOczekiwane przez przypadek: 1 \n\ncat(\"Najmniejsza p-wartość:\", round(min(p_values), 4), \"\\n\\n\")\n\nNajmniejsza p-wartość: 0.0541 \n\nif(significant_tests &gt; 0) {\n  cat(\"Selektywne raportowanie istotnych wyników generuje fałszywe trafienia\\n\")\n}\n\nRozwiązania:\n\nPrerejestracja hipotez\nKorekty Bonferroniego lub FDR\nPełne raportowanie wszystkich testów\nHipotezy oparte na teorii\n\n\n\nPułapka 4: Błąd ekologiczny\nProblem: Wnioskowanie o zależnościach indywidualnych na podstawie danych zagregowanych.\nKlasyczny przykład: „Zamożne stany głosują na Demokratów, więc zamożne osoby głosują na Demokratów” Rzeczywistość: W obrębie stanów zamożność często koreluje z głosowaniem na Republikanów\n\n# Ilustracja błędu ekologicznego\nset.seed(101)\n\n# Poziom stanowy: dochód ~ głos na Demokratów\nstate_income &lt;- runif(50, 40000, 80000)\nstate_dem_vote &lt;- 30 + 0.0005 * state_income + rnorm(50, 0, 5)\nstate_correlation &lt;- cor(state_income, state_dem_vote)\n\n# Poziom indywidualny: zależność odwrotna\nindividual_data &lt;- data.frame()\nfor(i in 1:50) {\n  n_people &lt;- 200\n  indiv_income &lt;- rnorm(n_people, state_income[i], 15000)\n  prob_dem &lt;- plogis((state_dem_vote[i]/100) - 0.00002 * (indiv_income - state_income[i]))\n  vote_dem &lt;- rbinom(n_people, 1, prob_dem)\n  \n  state_data &lt;- data.frame(\n    state = i,\n    income = indiv_income,\n    dem_vote = vote_dem * 100\n  )\n  individual_data &lt;- rbind(individual_data, state_data)\n}\n\nindividual_correlation &lt;- cor(individual_data$income, individual_data$dem_vote)\n\ncat(\"Korelacja stanowa (dochód vs. głos Demokratów):\", round(state_correlation, 3), \"\\n\")\n\nKorelacja stanowa (dochód vs. głos Demokratów): 0.75 \n\ncat(\"Korelacja indywidualna (dochód vs. głos Demokratów):\", round(individual_correlation, 3), \"\\n\")\n\nKorelacja indywidualna (dochód vs. głos Demokratów): -0.09 \n\ncat(\"\\nPrzeciwne znaki ilustrują błąd ekologiczny\\n\")\n\n\nPrzeciwne znaki ilustrują błąd ekologiczny\n\n\n\n\nPułapka 5: Bias selekcyjny\nProblem: Wnioski oparte na niereprezentatywnej próbie.\nTypowe źródła:\n\nAnkiety tylko na telefon stacjonarny (bias wieku)\nTylko ochotnicy (bias motywacji)\nTylko przypadki „udane” (survivorship bias)\nPróbki wygodne (bias dostępności)\n\nSkutek: Systematyczny błąd, którego techniki statystyczne nie „naprawią”.\n\n\nPułapka 6: Ignorowanie niepewności\nProblem: Traktowanie estymatorów punktowych jak wartości pewnych.\nBłędnie: „Poparcie wynosi 52%” Lepiej: „Poparcie 52% ± 3%” Najlepiej: „95% PU: [49%, 55%]”\nZnaczenie: Uznanie niepewności chroni przed nadmierną pewnością.\n\n\nPułapka 7: Korelacje pozorne\nProblem: Zmienne korelują przypadkowo, zwłaszcza przy trendach w czasie.\n\n# Korelacja pozorna\nset.seed(321)\nyears &lt;- 1990:2020\nn_years &lt;- length(years)\n\n# Niezwiązane zmienne z trendami\ninternet_users &lt;- 10 + 2.5 * (years - 1990) + rnorm(n_years, 0, 3)\npizza_consumption &lt;- 50 + 1.2 * (years - 1990) + rnorm(n_years, 0, 2)\n\nspurious_corr &lt;- cor(internet_users, pizza_consumption)\n\n# Usuwamy trendy\ninternet_detrended &lt;- residuals(lm(internet_users ~ years))\npizza_detrended &lt;- residuals(lm(pizza_consumption ~ years))\ntrue_corr &lt;- cor(internet_detrended, pizza_detrended)\n\ncat(\"Analiza korelacji pozornej:\\n\")\n\nAnaliza korelacji pozornej:\n\ncat(\"Korelacja z trendami:\", round(spurious_corr, 3), \"\\n\")\n\nKorelacja z trendami: 0.982 \n\ncat(\"Korelacja po detrendingu:\", round(true_corr, 3), \"\\n\")\n\nKorelacja po detrendingu: 0.136 \n\ncat(\"Wniosek: pozorna zależność napędzana wspólnymi trendami czasowymi\\n\")\n\nWniosek: pozorna zależność napędzana wspólnymi trendami czasowymi\n\n\nJak wykrywać:\n\nOceń wiarygodność teoretyczną\nSprawdź wspólne trendy\nPoszukaj trzecich zmiennych\nObejrzyj wykresy rozrzutu\n\n\n\nPułapka 8: Zmienne zakłócające (confounding)\nProblem: Trzecia zmienna wpływa i na predyktor, i na wynik — tworząc mylące zależności.\nStruktura konfudowania:\n\nZ → X (konfunder wpływa na predyktor)\nZ → Y (konfunder wpływa na wynik)\nTworzy pozorną zależność X → Y\n\n\n# Konfudowanie\nset.seed(456)\nn_districts &lt;- 500\n\n# Status społeczno-ekonomiczny (SES) zakłóca relację wydatki→głosy\nses &lt;- rnorm(n_districts, 0, 1)\n\n# SES wpływa na obie zmienne\ncampaign_spending &lt;- 100 + 50 * ses + rnorm(n_districts, 0, 20)\ncampaign_spending &lt;- pmax(10, campaign_spending)\n\nvote_share &lt;- 50 + 8 * ses + 0.02 * campaign_spending + rnorm(n_districts, 0, 5)\nvote_share &lt;- pmax(0, pmin(100, vote_share))\n\n# Porównanie modeli\nnaive_model &lt;- lm(vote_share ~ campaign_spending, data = data.frame(campaign_spending, vote_share))\ncontrolled_model &lt;- lm(vote_share ~ campaign_spending + ses, \n                      data = data.frame(campaign_spending, vote_share, ses))\n\ncat(\"Wpływ wydatków kampanijnych:\\n\")\n\nWpływ wydatków kampanijnych:\n\ncat(\"Bez kontroli SES:\", round(coef(naive_model)[2], 4), \"\\n\")\n\nBez kontroli SES: 0.1601 \n\ncat(\"Z kontrolą SES:\", round(coef(controlled_model)[2], 4), \"\\n\")\n\nZ kontrolą SES: 0.0052 \n\ncat(\"Prawdziwy efekt: 0.02\\n\")\n\nPrawdziwy efekt: 0.02\n\n\nRodzaje konfudowania:\n\nPozytywne: zawyża związek\nNegatywne: zaniża związek\nZmiana znaku: odwraca kierunek zależności\n\nRozwiązania:\n\nIdentyfikacja konfunderów na gruncie teorii\nDiagramy przyczynowe (DAG)\nKontrola statystyczna\nStrategie projektowe (eksperymenty, quasi-eksperymenty)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wskazówki-dla-odbiorców-badań",
    "href": "rozdzial1.html#wskazówki-dla-odbiorców-badań",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.39 Wskazówki dla odbiorców badań",
    "text": "2.39 Wskazówki dla odbiorców badań\nRozważ:\n\nWielkość efektu: Czy skala ma sens praktyczny?\nPróba: Kogo obejmuje/wyklucza? Jakie biasy?\nWielokrotne testowanie: Czy było „grzebanie” w danych?\nPrzyczynowość: Jaka strategia identyfikacji?\nZłożoność modelu: Czy proporcjonalna do danych?\nNiepewność: Czy raportowane są PU?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#przykład-kompletnej-analizy",
    "href": "rozdzial1.html#przykład-kompletnej-analizy",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.40 Przykład kompletnej analizy",
    "text": "2.40 Przykład kompletnej analizy\n\n# Pytanie: Co przewiduje frekwencję w hrabstwach USA?\n\nset.seed(101)\nn_counties &lt;- 500\n\ncounty_data &lt;- data.frame(\n  county_id = 1:n_counties,\n  median_income = runif(n_counties, 35000, 85000),\n  college_percent = runif(n_counties, 15, 65),\n  unemployment = runif(n_counties, 2, 15),\n  rural_percent = runif(n_counties, 0, 95)\n)\n\n# Frekwencja zależy od wielu czynników\ncounty_data$turnout &lt;- 45 + \n  0.0003 * county_data$median_income +\n  0.4 * county_data$college_percent +\n  -0.8 * county_data$unemployment +\n  -0.1 * county_data$rural_percent +\n  rnorm(n_counties, 0, 8)\n\ncounty_data$turnout &lt;- pmax(30, pmin(85, county_data$turnout))\n\n# Krok 1: Eksploracja danych\ncat(\"Krok 1: Podsumowanie danych\\n\")\n\nKrok 1: Podsumowanie danych\n\ncat(\"Wielkość próby:\", nrow(county_data), \"hrabstw\\n\")\n\nWielkość próby: 500 hrabstw\n\ncat(\"Zakres frekwencji:\", round(min(county_data$turnout), 1), \"% do\", \n    round(max(county_data$turnout), 1), \"%\\n\\n\")\n\nZakres frekwencji: 33.5 % do 85 %\n\n# Krok 2: Estymacja modelu\nfull_turnout_model &lt;- lm(turnout ~ median_income + college_percent + \n                        unemployment + rural_percent, data = county_data)\n\n# Krok 3: Interpretacja wyników\ncat(\"Krok 3: Interpretacja wyników\\n\\n\")\n\nKrok 3: Interpretacja wyników\n\n# potrzebne pakiety\nlibrary(broom)\nlibrary(dplyr)\n\nmodel_results &lt;- tidy(full_turnout_model) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(\n    interpretation = case_when(\n      term == \"median_income\" ~ paste0(\"↑ dochód medialny o 10 tys. USD → +\", \n                                       round(estimate * 10000, 1), \"% frekwencji\"),\n      term == \"college_percent\" ~ paste0(\"↑ udział z dyplomem o 10 p.p. → +\", \n                                         round(estimate * 10, 1), \"% frekwencji\"),\n      term == \"unemployment\" ~ paste0(\"↑ bezrobocie o 1 p.p. → \", \n                                      round(estimate, 1), \"% frekwencji\"),\n      term == \"rural_percent\" ~ paste0(\"↑ obszary wiejskie o 10 p.p. → \", \n                                       round(estimate * 10, 1), \"% frekwencji\")\n    ),\n    significance = ifelse(p.value &lt; 0.05, \"Istotny\", \"Nieistotny\")\n  )\n\nfor(i in 1:nrow(model_results)) {\n  cat(\"•\", model_results$interpretation[i], \"(\", model_results$significance[i], \")\\n\")\n}\n\n• ↑ dochód medialny o 10 tys. USD → +3.1% frekwencji ( Istotny )\n• ↑ udział z dyplomem o 10 p.p. → +4% frekwencji ( Istotny )\n• ↑ bezrobocie o 1 p.p. → -0.7% frekwencji ( Istotny )\n• ↑ obszary wiejskie o 10 p.p. → -0.8% frekwencji ( Istotny )\n\n# Krok 4: Ocena modelu\nr_squared &lt;- summary(full_turnout_model)$r.squared\ncat(paste0(\"\\nModel wyjaśnia \", round(r_squared * 100, 1), \"% zróżnicowania frekwencji\\n\"))\n\n\nModel wyjaśnia 49.9% zróżnicowania frekwencji\n\ncat(paste0(\"Pozostałe \", round((1-r_squared) * 100, 1), \"% to czynniki niewłączone do modelu\\n\\n\"))\n\nPozostałe 50.1% to czynniki niewłączone do modelu\n\n# Krok 5: Diagnostyka\ncat(\"Krok 5: Założenia i diagnostyka\\n\")\n\nKrok 5: Założenia i diagnostyka\n\nplot(full_turnout_model, which = 1, main = \"Reszty vs. wartości dopasowane\")\n\n\n\n\n\n\n\n\n\nPodsumowanie analizy\nWnioski:\n\nPoziom wykształcenia silnie przewiduje frekwencję\nCzynniki ekonomiczne (dochód, bezrobocie) są istotne\nBardziej wiejskie obszary mają niższą frekwencję\nTe zmienne wyjaśniają ok. 60% zróżnicowania frekwencji\n\nOgraniczenia:\n\nAnaliza korelacyjna nie dowodzi przyczynowości\n40% wariancji pozostaje niewyjaśnione\nWyniki na poziomie hrabstw nie muszą odzwierciedlać zachowań jednostek\nMożliwe pominięcie ważnych zmiennych",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#konkluzja",
    "href": "rozdzial1.html#konkluzja",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.41 Konkluzja",
    "text": "2.41 Konkluzja\nRegresja dostarcza systematycznych metod testowania teorii na danych. Choć bez odpowiednich projektów badawczych nie przesądza o przyczynowości, jest nieoceniona w rozumieniu zależności w danych obserwacyjnych.\nNajważniejsze zasady:\n\nRegresja identyfikuje najlepiej dopasowaną zależność liniową\nRegresja wieloraka pozwala kontrolować konfudery\nKorelacja ≠ przyczynowość\nWielkość efektu jest ważniejsza niż sama istotność\nKażda analiza ma ograniczenia — trzeba je ujawnić\n\nUmiejętności te pozwalają krytycznie oceniać twierdzenia w badaniach, debatach publicznych i dyskursie politycznym.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#praktyczne-wskazówki-dla-badań-politologicznych",
    "href": "rozdzial1.html#praktyczne-wskazówki-dla-badań-politologicznych",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.42 Praktyczne wskazówki dla badań politologicznych",
    "text": "2.42 Praktyczne wskazówki dla badań politologicznych\n\n1. Zacznij od teorii\nStatystyka to narzędzie, nie substytut myślenia:\n\nJakiej relacji oczekujesz i dlaczego?\nCo sfalsyfikowałoby Twoją hipotezę?\nJakie są alternatywne wyjaśnienia?\n\n\n\n2. Poznaj swoje dane\nPrzed analizą:\n\n# Kluczowe kroki diagnostyczne\nsummary(data)           # Statystyki opisowe\ntable(data$variable)    # Tabele częstości\nhist(data$variable)     # Rozkład\nplot(x, y)              # Wykres rozrzutu\ncor(data)               # Macierz korelacji\n\n\n\n3. Dobierz metodę do pytania\n\nOpisywanie: średnie, proporcje, rozkłady\nPrognozowanie: regresja, uczenie maszynowe\nWnioskowanie przyczynowe: eksperymenty, quasi-eksperymenty, metody panelowe\n\n\n\n4. Interpretuj merytorycznie\nZawsze tłumacz statystykę na język politologii:\n\nCo oznacza jednostkowa zmiana w praktyce?\nCzy efekt ma znaczenie polityczne?\nJakie wnioski dla polityk publicznych?\n\n\n\n5. Bądź transparentny\n\nRaportuj wszystkie analizy, nie tylko „istotne”\nUdostępniaj dane i kod, gdy to możliwe\nUznaj ograniczenia\nOpisz testy odporności (robustness checks)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zadania-do-przećwiczenia",
    "href": "rozdzial1.html#zadania-do-przećwiczenia",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.43 Zadania do przećwiczenia",
    "text": "2.43 Zadania do przećwiczenia\n\nZadanie 1: Populacje i próby\nChcesz zrozumieć czynniki sprzyjające transformacjom demokratycznym.\n\nJaka może być Twoja populacja?\nJak wybrał(a)byś próbę?\nJakie biasy mogą się pojawić?\n\n\n\nZadanie 2: Interpretacja wyników\nBadanie: „Każdy dodatkowy rok edukacji zwiększa frekwencję o 2,3 p.p. (p = 0,02)”\n\nCo oznacza p = 0,02 „po ludzku”?\nJeśli ktoś ma o 4 lata edukacji więcej, o ile bardziej prawdopodobne jest, że zagłosuje?\nCzy to duży czy mały efekt? (typowa frekwencja ~60%)\n\n\n\nZadanie 3: Korelacja vs. przyczynowość\n„Kraje z większą liczbą restauracji McDonald’s mają niższą śmiertelność niemowląt”\n\nPodaj trzy możliwe wyjaśnienia\nJak przetestować, które jest poprawne?\nJakich danych potrzebujesz?\n\n\n\nZadanie 4: Interpretacja regresji\nUruchamiasz regresję przewidującą odsetek głosów:\nVote Share = 45.2 + 0.31*Approval + 2.1*Economy - 0.05*Age\n             (0.8)  (0.04)         (0.6)       (0.02)\n\nR² = 0.67, n = 435\nBłędy standardowe w nawiasach\nZinterpretuj każdy współczynnik merytorycznie i oceń istotność.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#niezbędny-kod-r-na-start",
    "href": "rozdzial1.html#niezbędny-kod-r-na-start",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.44 Niezbędny kod R na start",
    "text": "2.44 Niezbędny kod R na start\n\n# Wczytanie danych\ndata &lt;- read.csv(\"yourfile.csv\")     # CSV\n\n# Podstawowa eksploracja\nsummary(data)                         # Podstawowe statystyki\nhead(data)                           # Pierwsze wiersze\ntable(data$party)                    # Liczebności kategorii\n\n# Proste analizy\nmean(data$age)                       # Średni wiek\ncor(data$income, data$turnout)       # Korelacja dwóch zmiennych\n\n# Prosta wizualizacja\nhist(data$age)                       # Histogram wieku\nplot(data$education, data$turnout)   # Rozrzut\n\n# Różnice między grupami\nt.test(income ~ gender, data = data) # Porównanie średnich wg płci\n\n# Prosta regresja\nmodel &lt;- lm(turnout ~ education, data = data)\nsummary(model)\n\n# Regresja wieloraka\nmodel2 &lt;- lm(turnout ~ education + age + income, data = data)\nsummary(model2)\n\n# Ładne wykresy w ggplot2\nlibrary(ggplot2)\nggplot(data, aes(x = education, y = turnout)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Edukacja a frekwencja\",\n       x = \"Lata edukacji\", \n       y = \"Frekwencja wyborcza\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#ostatnie-słowo",
    "href": "rozdzial1.html#ostatnie-słowo",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.45 Ostatnie słowo",
    "text": "2.45 Ostatnie słowo\nStatystyka to nie tylko narzędzie — to sposób myślenia o dowodach, niepewności i wnioskowaniu. Jako obywatel(ka) i badacz(ka):\n\nKrytycznie oceniaj twierdzenia polityczne\nProjektuj lepsze badania\nPodejmuj bardziej świadome decyzje\nRozumiej granice tego, co możemy wiedzieć\n\nPamiętaj: Każda liczba opowiada historię, ale nie każda historia opowiedziana przez liczby jest prawdziwa. Twoim zadaniem jest nauczyć się je odróżniać.\nCelem nie jest zostanie statystykiem, lecz politologiem, który potrafi oceniać i tworzyć rzetelne dowody. Statystyka pomaga przejść od intuicji przez hipotezy do wniosków opartych na danych. Za każdą analizą stoją realni ludzie, realne polityki i realne konsekwencje.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-a-metody-doboru-próby",
    "href": "rozdzial1.html#appendix-a-metody-doboru-próby",
    "title": "2  Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.46 Appendix A: Metody Doboru Próby",
    "text": "2.46 Appendix A: Metody Doboru Próby\n\nDobór Probabilistyczny (Losowy)\nMetody doboru probabilistycznego opierają się na losowym doborze, gdzie każdy członek populacji ma znaną, niezerową szansę na wybór. Metody te pozwalają badaczom obliczyć błąd próby i wyciągać wnioski statystyczne o populacji.\n\nProsty Dobór Losowy (SRS)\n\nDefinicja: Każdy członek populacji ma równą szansę na wybór.\nZalety: Minimalizuje błąd selekcji; umożliwia prostą analizę statystyczną.\nWady: Wymaga kompletnej operatu losowania; może nie uchwycić wystarczającej liczby członków mniejszych podgrup.\nPrzykład: Aby wybrać 100 studentów z uniwersytetu liczącego 10 000 studentów, przypisz każdemu studentowi numer i użyj generatora liczb losowych do wybrania 100 numerów.\nNajlepsze zastosowanie: Gdy populacja jest względnie jednorodna i dostępna jest kompletna lista członków populacji.\n\nDobór Losowy Warstwowy\n\nDefinicja: Populacja jest podzielona na wzajemnie wykluczające się podgrupy (warstwy) na podstawie wspólnych cech, a następnie próbki są losowo wybierane z każdej warstwy.\nZalety: Zapewnia reprezentację kluczowych podgrup; może poprawić precyzję dla tej samej wielkości próby co SRS; umożliwia analizę wewnątrz i między warstwami.\nWady: Wymaga wcześniejszej znajomości cech populacji do stratyfikacji; bardziej złożona analiza.\nPrzykład: W krajowym sondażu politycznym, podziel populację na warstwy według regionów geograficznych (Północny-Wschód, Północny-Zachód, Południe, itp.) i następnie losowo pobierz próbki z każdego regionu proporcjonalnie do ich wielkości populacji.\nNajlepsze zastosowanie: Gdy populacja zawiera wyraźne podgrupy, które mogą reagować różnie na pytanie badawcze.\n\nDobór Grupowy (Klastrowy)\n\nDefinicja: Populacja jest podzielona na grupy (zwykle geograficzne), niektóre grupy są losowo wybierane, a wszyscy członkowie w tych grupach są badani.\nZalety: Bardziej opłacalny gdy populacja jest geograficznie rozproszona; nie wymaga kompletnej listy członków populacji.\nWady: Niższa precyzja statystyczna niż SRS lub dobór warstwowy; klastry muszą być reprezentatywne.\nPrzykład: Aby zbadać nawyki uczenia się uczniów szkół średnich, losowo wybierz 20 szkół średnich z całego kraju i zbadaj wszystkich uczniów w tych szkołach.\nNajlepsze zastosowanie: Gdy populacja jest szeroko rozproszona geograficznie, a dotarcie do wszystkich jednostek byłoby kosztowne.\n\nDobór Systematyczny\n\nDefinicja: Wybieranie co k-tego elementu z listy po losowym starcie.\nZalety: Prosty w implementacji; często bardziej praktyczny niż SRS; może uniknąć efektów sąsiedztwa.\nWady: Może wprowadzić błąd, jeśli w liście występuje okresowy wzorzec.\nPrzykład: W zatłoczonym centrum handlowym, badaj co 20. osobę, która wchodzi, zaczynając od losowo wybranego numeru między 1 a 20.\nNajlepsze zastosowanie: Gdy populacja jest uporządkowana losowo lub w sposób niezwiązany ze zmiennymi badania.\n\nDobór Wielostopniowy\n\nDefinicja: Łączenie wielu metod próbkowania w etapach.\nZalety: Praktyczny dla badań na dużą skalę; równoważy koszty i precyzję.\nWady: Złożony projekt i analiza; wiele etapów może kumulować błędy próbkowania.\nPrzykład: Najpierw losowo wybierz powiaty (dobór klastrowy), następnie losowo wybierz gospodarstwa domowe w tych powiatach (prosty dobór losowy), a na końcu wybierz jednego dorosłego z każdego gospodarstwa (dobór systematyczny).\nNajlepsze zastosowanie: Badanie dużych, złożonych populacji na rozległych obszarach geograficznych.\n\n\n\n\nDobór Nieprobabilistyczny (Nielosowy)\nDobór nieprobabilistyczny nie opiera się na losowym wyborze, co oznacza, że wnioskowanie statystyczne o populacji musi być dokonywane z ostrożnością. Chociaż może wprowadzać błąd, w niektórych sytuacjach jest niezbędny.\n\nDobór Przypadkowy (Convenience Sampling)\n\nDefinicja: Wybieranie łatwo dostępnych podmiotów.\nZalety: Szybki, niedrogi i łatwy w implementacji.\nWady: Wysokie ryzyko błędu selekcji; ograniczona możliwość uogólniania.\nPrzykład: Badacz studiujący wzorce snu studentów może ankietować studentów ze swoich własnych zajęć.\nNajlepsze zastosowanie: Badania pilotażowe, badania eksploracyjne lub gdy zasoby są znacznie ograniczone.\n\nDobór Celowy\n\nDefinicja: Wybieranie podmiotów na podstawie określonych cech istotnych dla pytania badawczego.\nZalety: Koncentruje się na istotnych przypadkach; przydatny do dogłębnych badań określonych grup.\nWady: Błąd badacza w wyborze; ograniczona możliwość uogólniania.\nPrzykład: W badaniu doświadczeń dyrektorów generalnych w branży technologicznej, celowo poszukuj i przeprowadzaj wywiady z dyrektorami różnych firm technologicznych.\nNajlepsze zastosowanie: Badania jakościowe, studia przypadków lub badanie unikalnych populacji.\n\nDobór Kuli Śnieżnej\n\nDefinicja: Uczestnicy rekrutują innych uczestników ze swoich sieci.\nZalety: Dostęp do trudno dostępnych lub ukrytych populacji; bazuje na sieciach społecznych.\nWady: Próba stronnicza w kierunku osób w określonych sieciach społecznych; nie można obliczyć prawdopodobieństwa wyboru.\nPrzykład: W badaniu dostępu nielegalnych imigrantów do opieki zdrowotnej, badacze proszą początkowych uczestników o polecenie innych potencjalnych uczestników.\nNajlepsze zastosowanie: Badanie rzadkich populacji lub wrażliwych tematów, gdzie nie istnieje operat losowania.\n\nDobór Kwotowy\n\nDefinicja: Wybieranie uczestników w celu spełnienia określonych kwot dla pewnych cech, aby dopasować do znanych parametrów populacji.\nZalety: Zapewnia reprezentację kluczowych grup demograficznych; szybszy i tańszy niż próbkowanie probabilistyczne; nie wymaga operatu losowania.\nWady: Nielosowy wybór w ramach kwot może wprowadzić błąd; wnioskowanie jest ograniczone.\nPrzykład: W badaniu rynkowym, badacze upewniają się, że przeprowadzają wywiady z określoną liczbą osób z różnych grup wiekowych, płci i poziomów dochodów.\nNajlepsze zastosowanie: Komercyjne sondaże, badania rynkowe lub gdy próbkowanie probabilistyczne nie jest wykonalne.\n\n\n\n\nPróby Kwotowe vs. Próby Warstwowe: Porównanie i Praktyczne Zastosowania\nChoć na pierwszy rzut oka próbkowanie warstwowe i kwotowe może wydawać się podobne, istnieją między nimi fundamentalne różnice:\n\nKluczowe różnice między doborem warstwowym a kwotowym:\n\nPodstawa metodologiczna:\n\nDobór warstwowy: Jest metodą probabilistyczną, gdzie po podziale na warstwy, jednostki w każdej warstwie są wybierane losowo.\nDobór kwotowy: Jest metodą nieprobabilistyczną, gdzie badacz lub ankieter ma swobodę wyboru konkretnych jednostek, o ile spełnione są założone kwoty.\n\nMożliwość wnioskowania statystycznego:\n\nDobór warstwowy: Pozwala na obliczenie błędu próbkowania i przedziałów ufności, umożliwiając formalne wnioskowanie statystyczne.\nDobór kwotowy: Nie pozwala na obliczenie błędu próbkowania, co ogranicza możliwości formalnego wnioskowania statystycznego.\n\nKontrola procesu doboru:\n\nDobór warstwowy: Każdy etap procesu doboru jest kontrolowany przez badacza – od definicji warstw po losowy wybór jednostek w warstwach.\nDobór kwotowy: Ostateczny wybór respondentów pozostaje w rękach ankieterów, co może wprowadzać nieświadome obciążenia.\n\nPraktyczne wdrożenie:\n\nDobór warstwowy: Wymaga operatu losowania (kompletnej listy populacji) do przeprowadzenia losowania.\nDobór kwotowy: Nie wymaga operatu losowania, a jedynie znajomości rozkładu kluczowych cech w populacji.\n\n\n\n\nJak dokładnie powstaje próba kwotowa w badaniach CATI lub CAPI\nProces tworzenia próby kwotowej w badaniach CATI (Computer-Assisted Telephone Interviewing) lub CAPI (Computer-Assisted Personal Interviewing) obejmuje następujące etapy:\n\nEtap planowania i przygotowania:\n\nOkreślenie zmiennych kwotowych: Najczęściej są to podstawowe zmienne demograficzne: płeć, wiek, wykształcenie, miejsce zamieszkania (miasto/wieś), region.\nUstalenie wielkości kwot: Na podstawie danych GUS lub innych wiarygodnych źródeł danych (np. Diagnoza Społeczna) określa się, jaki procent populacji stanowią poszczególne kategorie.\nPrzygotowanie tabeli kwotowej: Tworzy się wielowymiarową macierz kwot, np. ile powinno być kobiet w wieku 18-29 lat z wyższym wykształceniem mieszkających na wsi w województwie mazowieckim.\n\nPrzygotowanie operacyjne badania CATI:\n\nPrzygotowanie bazy telefonicznej: W przypadku CATI tworzy się bazę numerów telefonicznych (stacjonarnych i/lub komórkowych).\nLosowanie numerów z puli: Często stosuje się metodę RDD (Random Digit Dialing) dla telefonów komórkowych lub losowanie z książek telefonicznych (coraz rzadziej) dla telefonów stacjonarnych.\nPrzypisanie numerów do zespołów ankieterskich: System CATI dystrybuuje numery do ankieterów.\n\nRealizacja badania CATI:\n\nPytania filtrujące: Na początku rozmowy ankieter zadaje pytania o wiek, płeć i inne zmienne kwotowe.\nDecyzja o kontynuacji: System CATI na bieżąco monitoruje wypełnienie kwot i decyduje, czy dana osoba kwalifikuje się do badania (czy jej profil demograficzny jest jeszcze potrzebny w próbie).\nRealizacja wywiadu: Jeśli respondent pasuje do wciąż otwartej kwoty, przeprowadzany jest wywiad.\nAutomatyczne zamykanie wypełnionych kwot: Gdy dana kwota zostaje wypełniona, system przestaje przyjmować nowych respondentów o tym profilu.\nRejestracja odmów udziału w badaniu: System rejestruje odmowy według ich typu (odmowa na etapie wprowadzenia, odmowa po pytaniach filtrujących, przerwanie wywiadu) oraz dane demograficzne, jeśli zostały zebrane przed odmową.\n\nPrzygotowanie operacyjne badania CAPI:\n\nWybór lokalizacji: Wybiera się punkty realizacji badania, często stratyfikowane według regionów, wielkości miejscowości itd.\nInstrukcje dla ankieterów: Ankieterzy otrzymują szczegółowe instrukcje dotyczące kwot, które muszą wypełnić w swoim rejonie.\n\nRealizacja badania CAPI:\n\nScreener: Ankieter używa krótkiego kwestionariusza selekcyjnego do określenia, czy dana osoba spełnia kryteria kwotowe.\nDobór respondenta: Ankieter sam decyduje, kogo zapytać o udział w badaniu, kierując się wytycznymi kwotowymi.\nMonitorowanie realizacji kwot: Ankieterzy regularnie raportują zrealizowane wywiady, a koordynator badania monitoruje wypełnienie kwot.\n\nKontrola jakości i analiza odmów:\n\nWeryfikacja wywiadów: Losowo wybrane wywiady są weryfikowane przez ponowny kontakt z respondentem.\nKontrola pracy ankieterów: W badaniach CAPI często stosuje się geolokalizację ankieterów, żeby potwierdzić, że faktycznie byli w deklarowanych lokalizacjach.\nKontrola “efektu ankietera”: Analizuje się, czy określeni ankieterzy nie mają systematycznie odmiennych wyników.\nAnaliza wskaźnika odpowiedzi (response rate): Oblicza się stosunek zrealizowanych wywiadów do wszystkich nawiązanych kontaktów.\nAnaliza struktury odmów: Sprawdza się, czy odmowy nie są systematycznie powiązane z określonymi cechami demograficznymi, co mogłoby wprowadzić błąd.\n\nWażenie końcowe (po realizacji badania):\n\nKorekta nierównomiernej realizacji kwot: Nawet przy najstaranniejszym doborze kwotowym rzadko udaje się idealnie odwzorować strukturę populacji, dlatego stosuje się ważenie danych po zakończeniu zbierania wywiadów.\nKalibracja do znanych parametrów populacji: Próbę kalibruje się do dokładnych danych z GUS lub innych wiarygodnych źródeł, przypisując odpowiednie wagi poszczególnym respondentom.\nMetody ważenia: Najczęściej stosuje się ważenie brzegowe (rim weighting) lub iteracyjne dopasowywanie (raking), które pozwalają jednocześnie dopasować próbę do wielu zmiennych demograficznych.\n\n\n\n\nPraktyczny przykład realizacji badania CATI z doborem kwotowym w Polsce:\n\nCel badania: Ogólnopolski sondaż opinii na temat systemu edukacji, n=1000 wywiadów.\nZmienne kwotowe:\n\nPłeć: 52% kobiety, 48% mężczyźni\nWiek: 18-29 lat (18%), 30-44 lat (29%), 45-59 lat (25%), 60+ lat (28%)\nWykształcenie: Podstawowe/gimnazjalne (18%), Zasadnicze zawodowe (23%), Średnie (35%), Wyższe (24%)\nWielkość miejscowości: Wieś (39%), Miasto do 50 tys. (23%), Miasto 50-200 tys. (16%), Miasto 200+ tys. (22%)\nRegion: Poszczególne województwa zgodnie z proporcjami GUS\n\nPrzebieg badania:\n\nSystem CATI losuje numery telefonów i przydziela ankieterom\nAnkieter przeprowadza wywiad, jeśli respondent spełnia kryteria kwotowe i się zgadza\nSystem monitoruje wypełnienie kwot i automatycznie zamyka te, które osiągnęły zakładaną liczebność\nW trakcie realizacji badania okazuje się, że najtrudniej dotrzeć do mężczyzn z wykształceniem zasadniczym zawodowym w wieku 45-59 lat – ankieterom przydzielane są dodatkowe godziny na poszukiwanie respondentów o tym profilu\nPo zakończeniu zbierania danych, próba jest ważona, aby skorygować niewielkie odchylenia od założonych kwot\n\n\n\n\n\nDlaczego Ośrodki Badania Opinii Coraz Częściej Stosują Dobór Kwotowy\nW ostatnich latach wiele ośrodków badania opinii publicznej przeszło na metody doboru kwotowego z kilku kluczowych powodów:\n\nSpadające Wskaźniki Realizacji: Tradycyjne badania telefoniczne oparte na losowym doborze odnotowały spadek wskaźników odpowiedzi z około 36% w latach 90. do mniej niż 10% obecnie. Zwiększa to koszty i potencjalnie wprowadza błąd związany z odmowami, który może być gorszy niż błąd selekcji wynikający z metod nielosowych.\nProblemy z Dotarciem do Respondentów: Losowe wybieranie numerów telefonicznych nie zapewnia już reprezentatywnej próby populacji, ponieważ wielu ludzi zrezygnowało z telefonów stacjonarnych na rzecz komórkowych, a wielu nie odbiera połączeń od nieznanych numerów.\nEfektywność Kosztowa: Badania oparte na losowym doborze stały się niezwykle drogie wraz ze spadkiem wskaźników odpowiedzi, podczas gdy panele internetowe i dobór kwotowy są bardziej przystępne cenowo.\nSzybkość: W szybko zmieniających się kampaniach politycznych lub gwałtownie ewoluujących sytuacjach społecznych, dobór kwotowy może dostarczyć wyniki znacznie szybciej niż metody losowe.\nUdoskonalenia w Technikach Ważenia: Nowoczesne metody statystyczne pozwalają badaczom dostosować próby kwotowe, aby lepiej reprezentowały populację docelową poprzez ważenie odpowiedzi w oparciu o znane parametry populacji.\nPodejścia Mieszane: Wielu badaczy stosuje obecnie metody mieszane, które łączą elementy doboru losowego i nielosowego, z zaawansowanym ważeniem i modelowaniem w celu poprawy dokładności.\n\n\nWpływ odmów na jakość próby i praktyczne problemy realizacji badań\n\nProblem odmów udziału w badaniach\nOdmowy udziału w badaniu stanowią jedno z największych wyzwań współczesnej metodologii badawczej i mają istotny wpływ na jakość uzyskiwanych wyników:\n\nSkala zjawiska:\n\nW klasycznych badaniach kwestionariuszowych poziom realizacji (response rate) w Polsce spadł z około 70-80% w latach 90. do 30-40% obecnie.\nW badaniach telefonicznych CATI współczynnik odpowiedzi wynosi często zaledwie 5-15%.\nW badaniach internetowych na panelach wskaźnik odpowiedzi może wynosić 20-30%, ale wśród respondentów dobieranych z ogółu populacji spada nawet do 1-2%.\n\nTypy odmów:\n\nOdmowy “twarde” – kategoryczna odmowa udziału w jakimkolwiek badaniu.\nOdmowy “miękkie” – wymówki typu “nie mam czasu”, “jestem zajęty”, które mogą wynikać z niechęci do tematu badania.\nOdmowy selektywne – odmowy udziału w badaniach na określone tematy (np. polityczne, dotyczące zdrowia).\nNiedostępność – niemożność nawiązania kontaktu z wylosowaną osobą mimo wielokrotnych prób.\n\nKonsekwencje dla jakości badań:\n\nBłąd systematyczny – jeśli osoby odmawiające udziału systematycznie różnią się od osób uczestniczących w badaniu pod względem kluczowych zmiennych.\nZawężenie próby do “zawodowych respondentów” – szczególnie w panelach internetowych, gdzie uczestniczą głównie osoby chętne do udziału w wielu badaniach.\nNadreprezentacja określonych grup – np. emerytów i rencistów, którzy częściej mają czas i chęć na udział w badaniach osobistych.\n\nMetody radzenia sobie z odmowami:\n\nPonowne próby kontaktu – standardem jest wykonanie minimum 3-4 prób kontaktu w różnych dniach i porach.\nDostosowanie terminu – oferowanie elastycznych terminów realizacji wywiadu.\nZachęty materialne – oferowanie drobnych gratyfikacji za udział w badaniu.\nAnaliza i ważenie ze względu na odmowy – uwzględnianie struktury osób odmawiających w procesie ważenia danych.\nTechniki konwersji odmów – specjalne szkolenia dla ankieterów w zakresie przekonywania osób początkowo odmawiających.\n\nDokumentacja odmów:\n\nStandard AAPOR (American Association for Public Opinion Research) – zaleca dokładne raportowanie wszystkich kontaktów, odmów i przyczyn braku realizacji wywiadu.\nKategorie dokumentacji – liczba połączeń/wizyt, powody niedostępności, typy i przyczyny odmów.\nTransparentność metodologiczna – raportowanie wskaźnika odpowiedzi oraz potencjalnego wpływu odmów na wyniki.\n\n\n\n\n\nPraktyczne problemy realizacji badań probabilistycznych w Polsce:\n\nBrak dostępu do operatu losowania: W Polsce nie ma łatwego dostępu do pełnych i aktualnych rejestrów populacji dla celów badawczych.\nProblem z realizacją wywiadów pod wylosowanymi adresami: Coraz trudniej przeprowadzić wywiady pod konkretnymi adresami ze względu na:\n\nZwiększoną liczbę zamkniętych osiedli\nSpadek zaufania społecznego i niechęć do wpuszczania ankieterów do domów\nRóżne godziny pracy potencjalnych respondentów wymagające wielokrotnych wizyt\n\nKoszt realizacji: Badania z prawdziwym losowym doborem próby (np. metodą random route) są kilkukrotnie droższe niż badania kwotowe.\n\nWybory prezydenckie w USA w 2016 roku, gdzie wiele sondaży nie przewidziało dokładnie wyniku, doprowadziły do znacznych przemyśleń wśród ankieterów. Zamiast rezygnować z próbkowania kwotowego, wiele organizacji udoskonaliło swoje metody, koncentrując się na lepszych definicjach kwot, ulepszonych technikach ważenia i bardziej przejrzystym raportowaniu ograniczeń metodologicznych.\nPomimo tych trendów, ważne jest, aby zauważyć, że próbkowanie probabilistyczne pozostaje złotym standardem wnioskowania statystycznego. Dobrze zaprojektowane próby probabilistyczne wciąż zapewniają najbardziej niezawodną podstawę do uogólniania z próby na populację, szczególnie w badaniach akademickich, gdzie dokładność jest priorytetem nad kosztem i szybkością.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "3.1 Foundations in Number Sets\nIn social science research, understanding the nature of our data is crucial for selecting appropriate analysis methods and drawing valid conclusions.\nBefore diving into data types, it’s essential to understand the basic number sets that form the foundation of our understanding of data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#foundations-in-number-sets",
    "href": "chapter2.html#foundations-in-number-sets",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "Basic Number Sets\n\nNatural Numbers (ℕ): The counting numbers {0, 1, 2, 3, …}\nIntegers (ℤ): Includes natural numbers, their negatives, and zero {…, -2, -1, 0, 1, 2, …}\nRational Numbers (ℚ): Numbers that can be expressed as a fraction of two integers\nReal Numbers (ℝ): All numbers on the number line, including rationals and irrationals\n\n\n\nProperties of Sets\n\nCountable Sets: Sets whose elements can be put in a one-to-one correspondence with the natural numbers. For example, the set of integers is countable.\nUncountable Sets: Sets that are not countable. The set of real numbers is uncountable.\nDiscrete Sets: Sets where each element is separated from other elements by a finite gap. The integers form a discrete set.\nDense Sets: Sets where between any two elements, there is always another element of the set. The rational numbers and real numbers are dense sets.\n\n\n\n\n\n\n\nNote\n\n\n\nUnderstanding these set properties is crucial for grasping the nature of different data types in social sciences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#discrete-vs.-continuous-data",
    "href": "chapter2.html#discrete-vs.-continuous-data",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.2 Discrete vs. Continuous Data",
    "text": "3.2 Discrete vs. Continuous Data\nIn data science and statistics, we often categorize variables as either discrete or continuous. However, the distinction is not always clear-cut, and some variables exhibit characteristics of both types. This section explores the concepts of discrete and continuous data, their differences, and the interesting cases of variables that can be treated as both or challenge our intuitive understanding.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDiscrete Data\nDiscrete data can only take on specific, countable values. These values are often (but not always) integers.\n\nCharacteristics of Discrete Data:\n\nCountable\nOften represented by integers\nCan be finite or infinite\nNo values between two adjacent data points\n\n\n\nExamples:\n\nNumber of students in a class\nNumber of cars sold by a dealership\nShoe sizes\n\n\n\n\nContinuous Data\nContinuous data can take on any value within a given range, including fractional and decimal values. It’s important to note that continuity is not solely determined by uncountability, but also by density.\n\nCharacteristics of Continuous Data:\n\nCan be uncountable (like real numbers) or dense (like rational numbers)\nCan be measured to any level of precision (theoretically)\nRepresented by real numbers or dense subsets of real numbers\nThere are always values between any two data points\n\n\n\nExamples:\n\nHeight\nWeight\nTemperature\nPercentages (explained further below)\n\n\n\n\nThe Discrete-Continuous Spectrum\nIn practice, some variables that are mathematically discrete are often treated as if they are continuous. This dual nature provides flexibility in how these variables can be analyzed and interpreted.\n\nReasons for Treating Discrete Data as Continuous:\n\nDense Granularity\n\nWhen a discrete variable has a large number of possible values within a range, it can approximate continuity.\nExample: Income measured in individual cents. While technically discrete, the large number of possible values makes it behave similarly to a continuous variable.\n\nAnalytical Convenience\n\nContinuous methods often yield reasonable and useful results even for dense discrete variables.\nIt’s often easier to use existing statistical tools if continuity is assumed, as this allows the use of calculus-based methods.\n\nApproximation of Underlying Phenomena\n\nIn some cases, a discrete measurement might be an approximation of an underlying continuous process.\nExample: While we measure time in discrete units (seconds, minutes, hours), time itself is continuous.\n\n\n\n\nExamples of Variables with Dual Discrete-Continuous Nature:\n\nAge\n\nDiscrete: Typically measured in whole years\nContinuous: Can be considered as a continuous variable in many analyses, especially when dealing with large populations\n\nPrice and Income\n\nDiscrete: Prices and incomes are actually measured in discrete units (e.g., cents or smallest currency unit)\nContinuous: In economic models and many analyses, prices and incomes are treated as continuous variables due to their dense nature and analytical convenience\n\nTest Scores\n\nDiscrete: Often given as whole numbers\nContinuous: In statistical analyses, test scores might be treated as continuous, especially when the range of possible scores is large\n\n\n\n\n\nSpecial Case: Percentages and Rational Numbers\nPercentages present an interesting case in the discrete-continuous spectrum:\n\nRational Nature: Percentages are essentially fractions (m/100), making them rational numbers.\nDense but Countable: The set of rational numbers is dense (between any two rationals, there’s another rational) but also countable.\nPractical Continuity: In most practical applications, percentages are treated as continuous due to their dense nature.\nFinite Precision: In reality, percentages are often reported to a limited number of decimal places, creating a finite set of possible values.\n\n\n\n\n\n\n\nPercentages: Bridging Discrete and Continuous\n\n\n\nVariables measured in percentages, such as unemployment rates or voter turnout, challenge our intuitive understanding of discreteness and continuity:\n\nThey are rational numbers (fractions with denominator 100), which are technically countable.\nThey form a dense set within their range (0% to 100%), allowing for values between any two percentages.\nIn practice, they are often treated as continuous variables due to their dense nature and analytical convenience.\nThe precision of measurement (e.g., reporting to one or two decimal places) can impose a discrete structure on what is conceptually a dense set.\n\nThis duality allows for flexible analytical approaches, depending on the specific research context and required precision.\n\n\n\n\nImplications for Data Analysis\nUnderstanding the nuanced nature of variables as discrete, continuous, or somewhere in between has important implications for data analysis:\n\nFlexibility in Modeling: It allows for the use of a wider range of statistical techniques.\nSimplified Calculations: Treating dense discrete data as continuous can simplify calculations and make certain analyses more tractable.\nImproved Interpretability: In some cases, treating discrete data as continuous can lead to more intuitive or useful interpretations of results.\nPotential for Error: It’s important to be aware of when approximations are appropriate and when they might lead to misleading results.\nTheoretical vs. Practical Considerations: While the mathematical nature of the data is important, practical considerations in measurement and analysis often guide how we treat variables.\n\n\n\nConclusion\nThe distinction between discrete and continuous data is not always rigid in social sciences. Many variables, including those involving money, percentages, or dense measurements, can be viewed through both discrete and continuous lenses. The choice of treatment should be guided by the nature of the data, the goals of the analysis, and the potential implications of the choice. This flexibility, when used thoughtfully, provides powerful tools for social science researchers to gain insights from their data.\n\n\n\n\n\n\nDiscrete vs. Continuous Numerical Data: A Language-Based Analogies\n\n\n\n\nThe Language Connection\nThink about how you naturally ask questions about quantities:\n\n“How many cookies are in the jar?” (counting)\n“How much water is in the glass?” (measuring)\n\nThis natural language distinction reflects the two fundamental types of numerical data:\n\n\nDiscrete Data = “How Many?” Questions\n\nLike counting whole objects (countable nouns)\nTakes specific values with gaps between them\nExamples:\n\nNumber of pets: 0, 1, 2, 3… (can’t have 2.5 pets)\nDice rolls: 1, 2, 3, 4, 5, 6\nStudents in a class: 20, 21, 22…\n\n\n🤔 Self-Check: Can you find a value between 2 and 3 students? Why not?\n\n\nContinuous Data = “How Much?” Questions\n\nLike measuring quantities (uncountable nouns)\nCan take any value within a range\nExamples:\n\nHeight: 1.7231… meters\nTemperature: 36.8325… °C\nTime: 3.5792… hours\n\n\n🤔 Self-Check: Write down three different values between 1.72 and 1.73 meters\n\n\nQuick Recognition Guide\n\nIf you naturally ask “How many?” → Discrete\nIf you naturally ask “How much?” → Continuous\nIf you can measure it more precisely → Continuous\nIf you can only use whole numbers → Discrete\n\n✍️ Practice: Classify these quantities as discrete or continuous\n\nYour age in years: _____\nYour height: _____\nNumber of songs in a playlist: _____\nVolume of water: _____",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#introduction-to-stevens-data-typology",
    "href": "chapter2.html#introduction-to-stevens-data-typology",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.3 Introduction to Stevens’ Data Typology",
    "text": "3.3 Introduction to Stevens’ Data Typology\nStanley S. Stevens, an American psychologist, introduced a classification system for scales of measurement in his 1946 paper “On the Theory of Scales of Measurement.” This system, known as Stevens’ data typology or levels of measurement, has become fundamental in understanding how different types of data should be analyzed and interpreted.\nStevens proposed four levels of measurement:\n\nNominal\nOrdinal\nInterval\nRatio\n\nEach level has specific properties and allows for different types of statistical operations and analyses.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\nNominal Scale\n\nDefinition\nThe nominal scale is the most basic level of measurement. It uses labels or categories to classify data without any quantitative value or order.\n\n\nProperties\n\nCategories are mutually exclusive\nNo inherent order among categories\nNo meaningful arithmetic operations can be performed\n\n\n\nExamples\n\nNationality (Polish, English, …)\nBlood types (A, B, AB, O)\nEye color (Blue, Brown, Green, Hazel)\nBinary variables (“Success” versus “Failure”)\n\n\n\n\nOrdinal Scale\n\nDefinition\nThe ordinal scale categorizes data into ordered categories, but the intervals between categories are not necessarily equal or meaningful.\n\n\nProperties\n\nCategories have a defined order\nDifferences between categories are not quantifiable\nArithmetic operations on the numbers are not meaningful\n\n\n\nExamples\n\nEducation levels (High School, Bachelor’s, Master’s, PhD)\nLikert scales (Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree)\nSocioeconomic status (Low, Medium, High)\n\n\n\n\nInterval Scale\n\nDefinition\nThe interval scale has ordered categories with equal intervals between adjacent categories. However, it lacks a true zero point.\n\n\nProperties\n\nEqual intervals between adjacent categories\nNo true zero point (zero is arbitrary)\nRatios between values are not meaningful\n\n\n\nExamples\n\nTemperature in Celsius or Fahrenheit\nCalendar years\npH scale (the difference between pH 4 and 5 represents the same change in hydrogen ion concentration as between pH 6 and 7)\nElevation above sea level\n\n\n\n\nRatio Scale\n\nDefinition\nThe ratio scale is the highest level of measurement. It has all the properties of the interval scale plus a true zero point, making ratios between values meaningful.\n\n\nProperties\n\nAll properties of interval scales\nTrue zero point\nRatios between values are meaningful\n\n\n\nExamples\n\nHeight\nWeight\nAge\nIncome\n\n\n\n\n\n\n\nWhy Some Statistics Work (and Others Don’t) for Interval Scales\n\n\n\n\nKey Idea\nAn interval scale is one where the distances between values are meaningful, but the zero point is arbitrary. For interval scales (e.g., temperature):\n\nAllowed: Addition/subtraction of values and multiplication/division by constants.\nNot allowed: Multiplication/division of values from the scale by each other, as this leads to results without physical interpretation.\n\n\n\nProperties of Interval Scales\n\nEqual intervals represent the same differences:\n\nThe difference between 20°C and 25°C (5°C) represents the same change as between 30°C and 35°C.\nProportions of differences are preserved: 10°C is twice the change of 5°C.\n\nThe zero point is arbitrary:\n\n0°C is the freezing point of water, not the absence of temperature.\nThe same physical state has different values in different scales: 0°C = 32°F.\n\nLinear transformation:\n\nGeneral formula: y = ax + b, where a \\neq 0.\nFor temperature: F = C \\times \\frac{9}{5} + 32.\n\n\n\n\nWhy the Arithmetic Mean Works\nThe arithmetic mean works because it relies on addition and division by a constant, which are allowed on interval scales. Example:\nData: 20°C and 30°C\n\nMethod 1: Mean in Celsius, then conversion\n1. Mean: (20°C + 30°C) ÷ 2 = 25°C\n2. Conversion: 25°C × (9/5) + 32 = 77°F\n\nMethod 2: Conversion to °F, then mean\n1. Conversion: 20°C → 68°F, 30°C → 86°F\n2. Mean: (68°F + 86°F) ÷ 2 = 77°F\n\nBoth methods give the same result! ✓\nMathematical proof of correctness: \\begin{align}\n\\bar{F} &= \\frac{F_1 + F_2}{2} \\\\\n&= \\frac{(C_1 \\times \\frac{9}{5} + 32) + (C_2 \\times \\frac{9}{5} + 32)}{2} \\\\\n&= \\frac{(C_1 + C_2) \\times \\frac{9}{5} + 64}{2} \\\\\n&= \\left(\\frac{C_1 + C_2}{2}\\right) \\times \\frac{9}{5} + 32 \\\\\n&= \\bar{C} \\times \\frac{9}{5} + 32\n\\end{align}\n\n\nWhy Variance is Problematic\nVariance is problematic because it relies on squared differences, leading to squared units (e.g., °C²) without clear physical interpretation. Example:\nSame temperatures: 20°C and 30°C\n\nMethod 1: Variance in Celsius\n1. Mean: 25°C\n2. Deviations: (20 - 25)°C = -5°C, (30 - 25)°C = 5°C\n3. Squared deviations: (-5°C)² = 25(°C)², (5°C)² = 25(°C)²\n4. Mean: (25 + 25)(°C)² ÷ 2 = 25(°C)²\n\nMethod 2: Variance in Fahrenheit\n1. Conversion: 20°C → 68°F, 30°C → 86°F\n2. Mean: 77°F\n3. Deviations: (68 - 77)°F = -9°F, (86 - 77)°F = 9°F\n4. Squared deviations: (-9°F)² = 81(°F)², (9°F)² = 81(°F)²\n5. Mean: (81 + 81)(°F)² ÷ 2 = 81(°F)²\n\nProblem: 25(°C)² and 81(°F)² are not equivalent!\nMathematical analysis of the problem: \\begin{align}\n(F_i - \\bar{F})^2 &= [(C_i \\times \\frac{9}{5} + 32) - (\\bar{C} \\times \\frac{9}{5} + 32)]^2 \\\\\n&= [(C_i - \\bar{C}) \\times \\frac{9}{5}]^2 \\\\\n&= (C_i - \\bar{C})^2 \\times \\left(\\frac{9}{5}\\right)^2\n\\end{align}\n\n\nTheoretical Conclusions\n\nAllowed operations:\n\nAddition/subtraction (preserves differences).\nMultiplication/division by constants (scaling).\nArithmetic means.\nComparing temperature differences.\n\nNot allowed operations:\n\nMultiplying temperatures by each other.\nDividing temperatures by each other.\nGeometric means.\nCoefficient of variation.\n\nPractical implications:\n\nVariance and standard deviation require careful interpretation.\nBetter to use measures based on differences (e.g., MAD - mean absolute deviation).\nWhen comparing variability, it is advisable to standardize the data.\n\n\n\n\nPractical Rule\nIf your calculations involve multiplying values from an interval scale by each other, be particularly cautious in interpreting the results!\n\n\n\n\n\n\n\n\n\nProportions in Measurement Scales: The Case of Temperature\n\n\n\n\nTwo Types of Proportions\n\nProportions of values (DO NOT hold for interval scales):\nTake 80°C and 20°C:\nIn Celsius: 80°C/20°C = 4\nIn Fahrenheit: 176°F/68°F ≈ 2.59\nIn Kelvin: 353.15K/293.15K ≈ 1.20\n\nThe same temperatures give different proportions! \n→ Proportions of values DO NOT make sense on interval scales; they only make sense on ratio scales.\n\n\nProportions of differences (hold for interval scales):\nTake two pairs of differences:\nPair 1: 30°C - 20°C = 10°C\nPair 2: 80°C - 60°C = 20°C\n\nProportion of differences in Celsius:\n20°C/10°C = 2\n\nThe same temperatures in Fahrenheit:\nPair 1: 86°F - 68°F = 18°F\nPair 2: 176°F - 140°F = 36°F\n\nProportion of differences in Fahrenheit:\n36°F/18°F = 2\n\nThe proportion of differences is the same! ✓\n\n\n\nMathematical Explanation\nFor the transformation F = \\frac{9}{5}C + 32:\n\nProportions of values DO NOT hold: [ = ]\nProportions of differences hold: [ = = ]\n\n\n\nWhy This Matters\n\nFor values:\n\nIn Celsius: 40°C is not “twice as hot” as 20°C.\nIn Fahrenheit: 100°F is not “twice as hot” as 50°F.\nOnly in Kelvin do proportions of values have physical meaning.\n\nFor differences:\n\nAn increase of 20°C is always twice as large as an increase of 10°C.\nAn increase of 36°F is always twice as large as an increase of 18°F.\nProportions of differences are scale-independent.\n\n\n\n\nImplications for Statistics\n\nOperations based on differences (WORK):\n\nArithmetic mean.\nMean absolute deviation.\nRange.\n\nOperations based on proportions of values (DO NOT WORK):\n\nGeometric mean.\nCoefficient of variation.\nVariance (because it uses squared values).\n\n\n\n\n\n\n\n\nImportance in Research and Analysis\nUnderstanding Stevens’ data typology is crucial for several reasons:\n\nChoosing appropriate statistical tests: The level of measurement determines which statistical analyses are appropriate for a given dataset.\nInterpreting results: The meaning of statistical results depends on the level of measurement of the variables involved.\nDesigning measurement instruments: When creating surveys or other measurement tools, researchers must consider the level of measurement they want to achieve.\nData transformation: Sometimes, data can be transformed from one level to another, but this must be done carefully to avoid misinterpretation.\n\n\n\nControversies and Limitations\nWhile Stevens’ typology is widely used, it has faced some criticisms:\n\nRigidity: Some argue that the typology is too rigid and that many real-world measurements fall between these categories.\nTreatment of ordinal data: There’s ongoing debate about when it’s appropriate to treat ordinal data as interval for certain analyses.\nPsychological scaling: Some psychological constructs (like intelligence) are difficult to categorize definitively within this system.\n\n\n\nConclusion\nStevens’ data typology provides a fundamental framework for understanding different types of data and their properties. By recognizing the level of measurement of their variables, researchers can make informed decisions about data collection, analysis, and interpretation. However, it’s important to remember that while this typology is a useful guide, real-world data often requires nuanced consideration and may not always fit neatly into these categories.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not “twice as acidic” as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#common-ordinal-scales-in-behavioural-research",
    "href": "chapter2.html#common-ordinal-scales-in-behavioural-research",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.4 Common Ordinal Scales in Behavioural Research",
    "text": "3.4 Common Ordinal Scales in Behavioural Research\n\nLikert Scales\nLikert scales are widely used in psychology and social sciences to measure attitudes, opinions, and perceptions. Named after psychologist Rensis Likert, these scales typically consist of a series of statements or questions that respondents rate on a scale, often from “Strongly Disagree” to “Strongly Agree.”\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nWhy Likert Scales are Ordinal Variables\nLikert scales are considered ordinal variables for several reasons:\n\nOrder without equal intervals: While the responses have a clear order (e.g., “Strongly Disagree” &lt; “Disagree” &lt; “Neutral” &lt; “Agree” &lt; “Strongly Agree”), the intervals between these categories are not necessarily equal.\nSubjective interpretation: The difference between “Strongly Disagree” and “Disagree” may not be the same as the difference between “Agree” and “Strongly Agree” for all respondents.\nLack of true zero point: Likert scales typically don’t have a true zero point, which is a characteristic of interval or ratio scales.\n\n\n\n\nIQ and Other Psychological Variables as Ordinal Measures\nMany psychological measures, including IQ, are often treated as interval scales but are, in fact, ordinal. Here’s why:\n\nIQ Scores:\n\nWhile IQ scores are presented as numbers, the difference between an IQ of 100 and 110 may not represent the same cognitive difference as between 130 and 140.\nThe scale is normalized and adjusted over time, making it difficult to claim true interval properties.\n\nOther Psychological Measures:\n\nDepression scales (e.g., Beck Depression Inventory)\nAnxiety measures (e.g., State-Trait Anxiety Inventory)\nPersonality assessments (e.g., Big Five Inventory)\n\n\nThese measures often use summed Likert-type items or other scoring methods that don’t guarantee equal intervals between scores.\n\n\nImplications for Analysis\nRecognizing these measures as ordinal has important implications for data analysis:\n\nAppropriate statistical tests: Use non-parametric tests (e.g., Mann-Whitney U, Kruskal-Wallis) instead of parametric ones.\nCorrelation analysis: Use Spearman’s rank correlation instead of Pearson’s correlation.\nCentral tendency: Report median and mode rather than mean.\nData visualization: Use methods appropriate for ordinal data, such as bar plots or stacked bar charts.\n\n\n\nConclusion\nWhile Likert scales and many behavioural measures are often treated as interval data for practical reasons, it’s crucial to remember their ordinal nature.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n\n\n\n\nExercise: Identifying Measurement Scales\n\n\n\nFor each of the following variables, determine the most appropriate scale of measurement (Nominal, Ordinal, Interval, or Ratio). Also evaluate whether the variable is discrete or continuous.\n\nGender: nominal level of measurement, and discrete;\nCustomer satisfaction: Poor, Fair, Good, Excellent\nHeight (questionnaire): “I am: very short, short, average, tall, very tall”\nHeight (inches)\nReaction time (milliseconds)\nPostal codes: e.g., 61548, 61761, 62461, 47424, 65233\nAge (years)\nNationality\nStreet addresses\nMilitary ranks\nLeft-Right political scale placement\nFamily size: 1 child, 2 children, 3 children, …\nIQ score\nShirt size (S, M, L, …)\nMovie ratings (1 star, 2 stars, 3 stars)\nTemperature (Celsius)\nTemperature (Kelvin)\nBlood types: A, B, AB, O\nIncome categories: low, medium, high\nVoter turnout\nPolitical party affiliation\nElectoral district magnitude\n\nRemember to justify your choices for each variable.\nFor instance: In Stevens’ typology of measurement scales, street addresses are nominal data. This is because:\nThey serve purely as labels/identifiers. They have no inherent ordering (123 Main St isn’t “more than” 23 Oak St). You can’t perform meaningful mathematical operations on them.The only valid operation is testing for equality/inequality (is this the same address or different?)\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\nNotes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html",
    "href": "rozdzial2.html",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "",
    "text": "4.1 Podstawy Zbiorów Liczbowych\nW badaniach z obszaru nauk społecznych zrozumienie natury danych jest kluczowe dla wyboru odpowiednich metod analizy i wyciągania prawidłowych wniosków.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#podstawy-zbiorów-liczbowych",
    "href": "rozdzial2.html#podstawy-zbiorów-liczbowych",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "",
    "text": "Note\n\n\n\nZrozumienie właściwości zbiorów liczbowych jest kluczowe dla uchwycenia natury różnych typów danych w naukach społecznych.\n\n\n\nPodstawowe Zbiory Liczbowe\n\nLiczby Naturalne (ℕ): Liczby używane do liczenia obiektów {0, 1, 2, 3, …}\nLiczby Całkowite (ℤ): Obejmują liczby naturalne, ich przeciwności i zero {…, -2, -1, 0, 1, 2, …}\nLiczby Wymierne (ℚ): Liczby, które można wyrazić jako ułamek dwóch liczb całkowitych\nLiczby Rzeczywiste (ℝ): Wszystkie liczby na osi liczbowej, w tym wymierne i niewymierne\n\n\n\nWłaściwości Zbiorów\n\nZbiory Przeliczalne: Zbiory, których elementy można ustawić w relacji jeden do jednego z liczbami naturalnymi. Na przykład, zbiór liczb całkowitych jest przeliczalny.\nZbiory Nieprzeliczalne: Zbiory, które nie są przeliczalne. Zbiór liczb rzeczywistych jest nieprzeliczalny.\nZbiory Dyskretne: Zbiory, w których każdy element jest oddzielony od innych elementów skończoną przerwą. Liczby całkowite tworzą zbiór dyskretny.\nZbiory Gęste: Zbiory, w których między dowolnymi dwoma elementami zawsze znajduje się inny element zbioru. Liczby wymierne i rzeczywiste są zbiorami gęstymi.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#dane-dyskretne-vs.-ciągłe",
    "href": "rozdzial2.html#dane-dyskretne-vs.-ciągłe",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.2 Dane Dyskretne vs. Ciągłe",
    "text": "4.2 Dane Dyskretne vs. Ciągłe\nW nauce o danych i statystyce często kategoryzujemy zmienne jako dyskretne lub ciągłe. Jednak rozróżnienie to nie zawsze jest jednoznaczne, a niektóre zmienne wykazują cechy obu typów. Ta sekcja bada koncepcje danych dyskretnych i ciągłych, ich różnice oraz interesujące przypadki zmiennych, które można traktować jako oba typy lub które kwestionują nasze intuicyjne rozumienie.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDane Dyskretne\nDane dyskretne mogą przyjmować tylko określone, przeliczalne wartości. Te wartości często (ale nie zawsze) są liczbami całkowitymi.\n\nCechy Danych Dyskretnych:\n\nPrzeliczalne\nCzęsto reprezentowane przez liczby całkowite\nMogą być skończone lub nieskończone\nBrak wartości między dwoma sąsiednimi punktami danych\n\n\n\nPrzykłady:\n\nLiczba studentów w klasie\nLiczba samochodów sprzedanych przez dealera\nRozmiary butów\n\n\n\n\nDane Ciągłe\nDane ciągłe mogą przyjmować dowolną wartość w danym zakresie, w tym wartości ułamkowe i dziesiętne. Ważne jest, aby zauważyć, że ciągłość nie jest określona wyłącznie przez nieprzeliczalność, ale również przez gęstość.\n\nCechy Danych Ciągłych:\n\nMogą być nieprzeliczalne (jak liczby rzeczywiste) lub gęste (jak liczby wymierne)\nMogą być mierzone z dowolną precyzją (teoretycznie)\nReprezentowane przez liczby rzeczywiste lub gęste podzbiory liczb rzeczywistych\nZawsze istnieją wartości między dowolnymi dwoma punktami danych\n\n\n\nPrzykłady:\n\nWzrost\nWaga\nTemperatura\nProcenty (wyjaśnione dalej poniżej)\n\n\n\n\nSpektrum Dyskretno-Ciągłe\nW praktyce niektóre zmienne, które matematycznie są dyskretne, często są traktowane tak, jakby były ciągłe. Ta dwoista natura zapewnia elastyczność w analizie i interpretacji tych zmiennych.\n\nPowody Traktowania Danych Dyskretnych jako Ciągłych:\n\nGęsta Granularność\n\nGdy zmienna dyskretna ma dużą liczbę możliwych wartości w danym zakresie, może przybliżać ciągłość.\nPrzykład: Dochód mierzony w pojedynczych groszach. Choć technicznie dyskretny, duża liczba możliwych wartości sprawia, że zachowuje się podobnie do zmiennej ciągłej.\n\nWygoda Analityczna\n\nMetody ciągłe często dają rozsądne i użyteczne wyniki nawet dla gęstych zmiennych dyskretnych.\nCzęsto łatwiej jest używać istniejących narzędzi statystycznych, jeśli założymy ciągłość, ponieważ pozwala to na stosowanie metod opartych na rachunku różniczkowym.\n\nPrzybliżenie Zjawisk Bazowych\n\nW niektórych przypadkach dyskretny pomiar może być przybliżeniem bazowego procesu ciągłego.\nPrzykład: Chociaż mierzymy czas w dyskretnych jednostkach (sekundy, minuty, godziny), sam czas jest ciągły.\n\n\n\n\nPrzykłady Zmiennych o Dwoistej Naturze Dyskretno-Ciągłej:\n\nWiek\n\nDyskretny: Typowo mierzony w pełnych latach\nCiągły: Może być uznany za zmienną ciągłą w wielu analizach, szczególnie przy dużych populacjach\n\nCena i Dochód\n\nDyskretne: Ceny i dochody są w rzeczywistości mierzone w dyskretnych jednostkach (np. grosze lub najmniejsza jednostka waluty)\nCiągłe: W modelach ekonomicznych i wielu analizach ceny i dochody są traktowane jako zmienne ciągłe ze względu na ich gęstą naturę i wygodę analityczną\n\nWyniki Testów\n\nDyskretne: Często podawane jako liczby całkowite\nCiągłe: W analizach statystycznych wyniki testów mogą być traktowane jako ciągłe, szczególnie gdy zakres możliwych wyników jest duży\n\n\n\n\n\nPrzypadek Szczególny: Procenty i Liczby Wymierne\nProcenty przedstawiają interesujący przypadek w spektrum dyskretno-ciągłym:\n\nNatura Wymierna: Procenty są zasadniczo ułamkami (m/100), co czyni je liczbami wymiernymi.\nGęste, ale Przeliczalne: Zbiór liczb wymiernych jest gęsty (między dowolnymi dwoma wymiernymi jest inny wymierny), ale także przeliczalny.\nPraktyczna Ciągłość: W większości praktycznych zastosowań procenty są traktowane jako ciągłe ze względu na ich gęstą naturę.\nSkończona Precyzja: W rzeczywistości procenty są często podawane z ograniczoną liczbą miejsc po przecinku, tworząc skończony zbiór możliwych wartości.\n\n\n\n\n\n\n\nProcenty: Łączenie Dyskretnego i Ciągłego\n\n\n\nZmienne mierzone w procentach, takie jak stopy bezrobocia czy frekwencja wyborcza, kwestionują nasze intuicyjne rozumienie dyskretności i ciągłości:\n\nSą liczbami wymiernymi (ułamki z mianownikiem 100), które technicznie są przeliczalne.\nTworzą zbiór gęsty w swoim zakresie (od 0% do 100%), pozwalając na wartości między dowolnymi dwoma procentami.\nW praktyce są często traktowane jako zmienne ciągłe ze względu na ich gęstą naturę i wygodę analityczną.\nPrecyzja pomiaru (np. podawanie do jednego lub dwóch miejsc po przecinku) może narzucić dyskretną strukturę na to, co koncepcyjnie jest zbiorem gęstym.\n\nTa dwoistość pozwala na elastyczne podejścia analityczne, w zależności od konkretnego kontekstu badawczego i wymaganej precyzji.\n\n\n\n\nImplikacje dla Analizy Danych\nZrozumienie zniuansowanej natury zmiennych jako dyskretnych, ciągłych lub gdzieś pomiędzy ma ważne implikacje dla analizy danych:\n\nElastyczność w Modelowaniu: Pozwala na wykorzystanie szerszego zakresu technik statystycznych.\nUproszczone Obliczenia: Traktowanie gęstych danych dyskretnych jako ciągłych może uprościć obliczenia i uczynić niektóre analizy bardziej wykonalnymi.\nLepsza Interpretowalność: W niektórych przypadkach traktowanie danych dyskretnych jako ciągłych może prowadzić do bardziej intuicyjnych lub użytecznych interpretacji wyników.\nPotencjał Błędu: Ważne jest, aby być świadomym, kiedy przybliżenia są odpowiednie, a kiedy mogą prowadzić do mylących wyników.\nRozważania Teoretyczne vs. Praktyczne: Choć matematyczna natura danych jest ważna, praktyczne względy w pomiarze i analizie często kierują tym, jak traktujemy zmienne.\n\n\n\nWnioski\nRozróżnienie między danymi dyskretnymi a ciągłymi nie zawsze jest sztywne w naukach społecznych. Wiele zmiennych, w tym te dotyczące pieniędzy, procentów czy gęstych pomiarów, można oglądać przez pryzmat zarówno dyskretny, jak i ciągły. Wybór sposobu traktowania powinien być kierowany naturą danych, celami analizy i potencjalnymi implikacjami tego wyboru. Ta elastyczność, gdy jest używana rozważnie, zapewnia potężne narzędzia dla badaczy nauk społecznych do uzyskiwania wglądu w ich dane.\n\n\n\n\n\n\nDane Dyskretne vs. Ciągłe: Analogia Językowa\n\n\n\n\nKluczowe Rozróżnienie Językowe\nW języku polskim mamy precyzyjne rozróżnienie:\n\n“Liczba” → używamy dla rzeczy policzalnych\n“Ilość” → używamy dla rzeczy niepoliczalnych\n\nTo rozróżnienie doskonale odzwierciedla dwa podstawowe typy danych liczbowych:\n\n\nDane Dyskretne = “Liczba czegoś”\n\nUżywamy słowa “liczba” (tak jak mówimy “liczba studentów”)\nWartości są rozdzielone jak pojedyncze elementy\nPrzykłady:\n\nLiczba książek: 0, 1, 2, 3…\nLiczba punktów w teście: 0, 1, 2…\nLiczba mieszkańców: 100, 101, 102…\n\n\n🤔 Czy poprawne jest powiedzenie “ilość studentów” czy “liczba studentów”? (Poprawna forma pomoże Ci rozpoznać typ danych)\n\n\nDane Ciągłe = “Ilość czegoś”\n\nUżywamy słowa “ilość” (tak jak mówimy “ilość wody”)\nWartości płynnie przechodzą jedna w drugą\nPrzykłady:\n\nIlość cieczy: 1,5231… litra\nIlość czasu: 2,3891… godziny\nIlość energii: 5,7123… kWh\n\n\n🤔 Czy mówimy “ilość wody” czy “liczba wody”? (Poprawna forma wskazuje na typ danych)\n\n\nSposób Rozpoznawania\n\nCzy użyłbyś słowa “liczba”? → Dane dyskretne\nCzy użyłbyś słowa “ilość”? → Dane ciągłe\n\n✍️ Ćwiczenie: Uzupełnij poprawnym słowem i określ typ danych\n\n_____ uczniów w klasie (liczba/ilość): typ _____\n_____ deszczu (liczba/ilość): typ _____\n_____ piosenek (liczba/ilość): typ _____\n_____ temperatury (liczba/ilość): typ _____",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "href": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.3 Wprowadzenie do Typologii Danych Stevensa",
    "text": "4.3 Wprowadzenie do Typologii Danych Stevensa\nStanley S. Stevens, amerykański psycholog, wprowadził system klasyfikacji skal pomiarowych w swoim artykule z 1946 roku “On the Theory of Scales of Measurement”. Ten system, znany jako typologia danych Stevensa lub poziomy pomiaru, stał się fundamentalny dla zrozumienia, jak różne typy danych powinny być analizowane i interpretowane.\nStevens zaproponował cztery poziomy pomiaru:\n\nNominalny\nPorządkowy\nInterwałowy\nIlorazowy\n\nKażdy poziom ma specyficzne właściwości i pozwala na różne rodzaje operacji statystycznych i analiz.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\nSkala Nominalna\n\nDefinicja\nSkala nominalna jest najbardziej podstawowym poziomem pomiaru. Używa etykiet lub kategorii do klasyfikacji danych bez żadnej wartości ilościowej ani porządku.\n\n\nWłaściwości\n\nKategorie są wzajemnie wykluczające się\nBrak inherentnego porządku między kategoriami\nNie można wykonywać znaczących operacji arytmetycznych\n\n\n\nPrzykłady\n\nNarodowość (Polak, Niemiec, …)\nGrupy krwi (A, B, AB, O)\nKolor oczu (Niebieskie, Brązowe, Zielone, Piwne)\nZmienne binarne (“Sukces” versus “Niepowodzenie”)\n\n\n\n\nSkala Porządkowa\n\nDefinicja\nSkala porządkowa kategoryzuje dane w uporządkowane kategorie, ale odstępy między kategoriami niekoniecznie są równe lub znaczące.\n\n\nWłaściwości\n\nKategorie mają zdefiniowany porządek\nRóżnice między kategoriami nie są kwantyfikowalne\nOperacje arytmetyczne na liczbach nie są znaczące\n\n\n\nPrzykłady\n\nPoziomy wykształcenia (Szkoła Średnia, Licencjat, Magister, Doktorat)\nSkale Likerta (Zdecydowanie się nie zgadzam, Nie zgadzam się, Neutralnie, Zgadzam się, Zdecydowanie się zgadzam)\nStatus społeczno-ekonomiczny (Niski, Średni, Wysoki)\n\n\n\n\nSkala Interwałowa\n\nDefinicja\nSkala interwałowa ma uporządkowane kategorie z równymi odstępami między sąsiednimi kategoriami. Jednak brakuje jej prawdziwego punktu zerowego.\n\n\nWłaściwości\n\nRówne odstępy między sąsiednimi kategoriami\nBrak prawdziwego punktu zerowego (zero jest umowne)\nStosunki między wartościami nie są znaczące\n\n\n\nPrzykłady\n\nTemperatura w stopniach Celsjusza lub Fahrenheita\nLata kalendarzowe\nSkala pH\nWysokość nad poziomem morza\n\n\n\n\nSkala Ilorazowa\n\nDefinicja\nSkala ilorazowa jest najwyższym poziomem pomiaru. Ma wszystkie właściwości skali interwałowej plus prawdziwy punkt zerowy, co sprawia, że stosunki między wartościami są znaczące.\n\n\nWłaściwości\n\nWszystkie właściwości skal interwałowych\nPrawdziwy punkt zerowy\nStosunki między wartościami są znaczące\n\n\n\nPrzykłady\n\nWzrost\nWaga\nWiek\nDochód\n\n\n\n\nZnaczenie w Badaniach i Analizie\nZrozumienie typologii danych Stevensa jest kluczowe z kilku powodów:\n\nWybór odpowiednich testów statystycznych: Poziom pomiaru determinuje, które analizy statystyczne są odpowiednie dla danego zbioru danych.\nInterpretacja wyników: Znaczenie wyników statystycznych zależy od poziomu pomiaru zaangażowanych zmiennych.\nProjektowanie narzędzi pomiarowych: Przy tworzeniu ankiet lub innych narzędzi pomiarowych badacze muszą wziąć pod uwagę poziom pomiaru, który chcą osiągnąć.\nTransformacja danych: Czasami dane mogą być przekształcane z jednego poziomu na drugi, ale musi to być robione ostrożnie, aby uniknąć błędnej interpretacji.\n\n\n\nKontrowersje i Ograniczenia\nChociaż typologia Stevensa jest szeroko stosowana, spotkała się z pewnymi krytykami:\n\nSztywność: Niektórzy twierdzą, że typologia jest zbyt sztywna i że wiele rzeczywistych pomiarów mieści się pomiędzy tymi kategoriami.\nTraktowanie danych porządkowych: Trwa debata na temat tego, kiedy właściwe jest traktowanie danych porządkowych jako interwałowych dla pewnych analiz.\nSkalowanie psychologiczne: Niektóre konstrukty psychologiczne (jak inteligencja) są trudne do jednoznacznego skategoryzowania w ramach tego systemu.\n\n\n\nPodsumowanie\nTypologia danych Stevensa dostarcza fundamentalnych ram dla zrozumienia różnych rodzajów danych i ich właściwości. Rozpoznając poziom pomiaru swoich zmiennych, badacze mogą podejmować świadome decyzje dotyczące gromadzenia danych, analizy i interpretacji. Jednak ważne jest, aby pamiętać, że chociaż ta typologia jest użytecznym przewodnikiem, rzeczywiste dane często wymagają niuansowego podejścia i nie zawsze pasują idealnie do tych kategorii.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not “twice as acidic” as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#popularne-skale-porządkowe-w-badaniach-behawioralnych",
    "href": "rozdzial2.html#popularne-skale-porządkowe-w-badaniach-behawioralnych",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.4 Popularne Skale Porządkowe w Badaniach Behawioralnych",
    "text": "4.4 Popularne Skale Porządkowe w Badaniach Behawioralnych\n\nSkale Likerta\nSkale Likerta są szeroko stosowane w psychologii i naukach społecznych do pomiaru postaw, opinii i percepcji. Nazwane na cześć psychologa Rensisa Likerta, skale te zazwyczaj składają się z serii stwierdzeń lub pytań, które respondenci oceniają na skali, często od “Zdecydowanie się nie zgadzam” do “Zdecydowanie się zgadzam”.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDlaczego Skale Likerta są Zmiennymi Porządkowymi\nSkale Likerta są uważane za zmienne porządkowe z kilku powodów:\n\nPorządek bez równych odstępów: Chociaż odpowiedzi mają wyraźną kolejność (np. “Zdecydowanie się nie zgadzam” &lt; “Nie zgadzam się” &lt; “Neutralnie” &lt; “Zgadzam się” &lt; “Zdecydowanie się zgadzam”), odstępy między tymi kategoriami niekoniecznie są równe.\nSubiektywna interpretacja: Różnica między “Zdecydowanie się nie zgadzam” a “Nie zgadzam się” może nie być taka sama jak różnica między “Zgadzam się” a “Zdecydowanie się zgadzam” dla wszystkich respondentów.\nBrak prawdziwego punktu zerowego: Skale Likerta zazwyczaj nie mają prawdziwego punktu zerowego, co jest cechą charakterystyczną skal interwałowych lub ilorazowych.\n\n\n\n\n\n\n\nSkale pomiarowe według typologii Stevensa: zmienne ilościowe vs. porządkowe\n\n\n\n\nCzym są zmienne ilościowe (numeryczne)?\nW typologii Stevensa wyróżniamy cztery skale pomiarowe. Zmienne ilościowe to te, które mierzone są na skalach interwałowych lub ilorazowych:\n\nSkala interwałowa: Posiada równe odstępy między jednostkami, ale brak naturalnego punktu zerowego\nSkala ilorazowa: Posiada równe odstępy między jednostkami oraz naturalny punkt zerowy\n\nZmienne ilościowe charakteryzują się następującymi właściwościami:\n\nRówne interwały: Różnica między 5 a 6 reprezentuje taką samą wielkość jak różnica między 95 a 96\nSpójne jednostki: Każdy przyrost reprezentuje taką samą ilość mierzonej cechy\nObiektywny pomiar: Mierzą rzeczywiste ilości, a nie tylko względne pozycje\nDopuszczalne operacje matematyczne: Można wykonywać operacje arytmetyczne\n\nPrzykłady zmiennych mierzonych na skali ilorazowej:\n\nWzrost: 170 cm jest dokładnie o 10 cm wyższe niż 160 cm, a 170 cm jest dokładnie dwa razy wyższe niż 85 cm\nWaga: Różnica między 50 kg a 60 kg to taka sama ilość wagi jak między 80 kg a 90 kg\nCzas: 4 godziny to dwa razy dłużej niż 2 godziny, a różnica między 3 a 4 godzinami jest taka sama jak między 9 a 10 godzinami\nTemperatura w Kelwinach: 200K jest dwa razy cieplejsza niż 100K (ponieważ 0K to zero absolutne)\n\nPrzykład skali interwałowej:\n\nTemperatura w stopniach Celsjusza: Różnica między 20°C a 30°C jest taka sama jak między 70°C a 80°C, ale 40°C nie jest “dwa razy cieplejsze” niż 20°C (brak naturalnego zera)\nRok kalendarzowy: Różnica między 2020 a 2021 jest taka sama jak między 1950 a 1951, ale rok 2000 nie jest “dwa razy starszy” niż rok 1000\n\n\n\nDlaczego wyniki IQ są zmienną porządkową, a nie ilościową\nMimo liczbowego zapisu, wyniki IQ są w istocie zmienną porządkową (w typologii Stevensa), a nie zmienną interwałową, ponieważ:\n\nBrak jednolitej jednostki miary: Nie istnieje naturalna jednostka mierząca “inteligencję”\nKonstrukcja oparta na rangach: Skala powstaje przez uszeregowanie ludzi względem siebie, a następnie przekształcenie tych rang w wyniki liczbowe\nNierówne interwały: Różnica między IQ 100 a 110 niekoniecznie reprezentuje taką samą różnicę poznawczą jak między 130 a 140\nBrak absolutnego zera: Nie istnieje znaczące pojęcie “zerowej inteligencji”\nZależność od testu: Różne testy IQ mogą dać różne wyniki dla tej samej osoby\n\nPrzykład: Rozważmy trzy osoby z wynikami IQ 85, 100 i 115. Choć moglibyśmy chcieć powiedzieć, że różnica między pierwszą a drugą osobą równa się różnicy między drugą a trzecią, nie jest to faktycznie znaczące—zdolności poznawcze reprezentowane przez te wyniki nie wzrastają w równych krokach, mimo że cyfry sugerują równe odstępy. Wyniki te informują nas głównie o pozycji osoby względem innych, co jest cechą skali porządkowej.\nInny przykład: Osoba z IQ 140 nie jest “dwa razy inteligentniejsza” niż osoba z IQ 70, mimo że stosunek liczb wynosi 2:1. Takie porównanie nie ma sensu na skali porządkowej.\n\n\nPunkty egzaminacyjne - pomiędzy skalą porządkową a interwałową\nChoć traktujemy punkty egzaminacyjne jak zmienne ilościowe (interwałowe), często mają one cechy zmiennych porządkowych:\n\nNierówna trudność pytań: Pytanie za 10 punktów z fizyki kwantowej nie mierzy tej samej ilości wiedzy co pytanie za 10 punktów z podstawowej arytmetyki\nRóżne rodzaje kompetencji: Różne pytania testują różne umiejętności (zapamiętywanie, rozumienie, zastosowanie, analiza)\nSubiektywne przydzielanie punktów: Punktacja zależy od oceny egzaminatora, a nie obiektywnych jednostek miary\nBrak addytywności: Student zdobywający 90 punktów niekoniecznie jest “dwa razy bardziej wykształcony” niż student zdobywający 45 punktów\n\nPrzykład: Wyobraźmy sobie dwóch studentów:\n\nStudent A: Odpowiada prawidłowo na wszystkie łatwe i średnie pytania (zdobywa 75 punktów)\nStudent B: Odpowiada prawidłowo na wszystkie trudne pytania, ale żadne łatwe (zdobywa 75 punktów)\n\nMimo równej punktacji, ich wiedza jest jakościowo różna. Punkty sugerują równość, ale w rzeczywistości są to różne profile kompetencji — typowy problem ze zmiennymi, które nie są w pełni ilościowe.\n\n\nTraktowanie zmiennych porządkowych jako ilościowych w praktyce\nZe względów praktycznych często traktujemy zmienne porządkowe jak zmienne ilościowe, ponieważ:\n\nUmożliwia to stosowanie znanych operacji matematycznych (średnie, odchylenia)\nUpraszcza komunikację i interpretację wyników (“średni wynik 78%”)\nPozwala na stosowanie bardziej zaawansowanych metod statystycznych\n\nPrzykład: Średnia ocen\n\nObliczamy średnią, przypisując wartości liczbowe ocenom (5, 4, 3, 2, 1)\nTraktujemy te wartości jak zmienne ilościowe, obliczając np. średnią 4,5\nAle czy różnica między oceną 5 a 4 (5-4=1) reprezentuje taką samą różnicę wiedzy jak między oceną 2 a 1 (2-1=1)?\nI czy średnia 5.0 jest naprawdę “dwa razy lepsza” niż średnia 2.5?\n\nInny przykład: Skale Likerta\n\nW ankietach często stosujemy skale typu: 1 = “zdecydowanie nie zgadzam się”, 5 = “zdecydowanie zgadzam się”\nObliczamy średnie odpowiedzi, zakładając równe odstępy między kategoriami\nAle czy odległość między “zdecydowanie nie zgadzam się” a “raczej nie zgadzam się” jest naprawdę taka sama jak między “raczej zgadzam się” a “zdecydowanie zgadzam się”?\n\n\n\nZnaczenie rozróżnienia skal pomiarowych\nRozumienie typologii skal pomiarowych Stevensa ma istotne konsekwencje praktyczne:\n\nZmienne jakościowe porządkowe: Pozwalają na stwierdzenie, że coś jest “większe/lepsze” lub “mniejsze/gorsze”, ale nie określają “o ile” (dopuszczalne porównania typu &gt;, &lt;, =)\nZmienne ilościowe: Pozwalają na określenie dokładnych różnic i proporcji (dopuszczalne operacje +, -, ×, ÷)\n\nŚwiadomość ograniczeń skali pomiarowej pomaga w poprawnej interpretacji danych i doborze odpowiednich metod analizy.\nPrzykład: Jeśli Anna uzyskała 75 punktów na teście z historii i 85 punktów na teście z matematyki, nie możemy jednoznacznie stwierdzić, że jest “lepsza z matematyki o 10 jednostek umiejętności”. Punkty z różnych testów nie są bezpośrednio porównywalne, a interwały mogą nie być równoważne.\nPoprawniejsze podejście: Lepiej porównać jej wyniki z rozkładem wyników innych uczniów. Jeśli w historii 75 punktów plasuje ją w 50. percentylu, a 85 punktów z matematyki w 90. percentylu, to możemy powiedzieć, że względnie rzecz biorąc, radzi sobie lepiej z matematyką niż historią - co jest wnioskiem opartym na skali porządkowej.\n\n\n\n\n\n\nPodsumowanie\nChociaż skale Likerta i wiele miar psychologicznych jest często traktowanych jako dane interwałowe ze względów praktycznych, ważne jest, aby pamiętać o ich porządkowym charakterze.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n\n\n\n\nĆwiczenie: Identyfikacja Skal Pomiarowych\n\n\n\nDla każdej z poniższych zmiennych określ najbardziej odpowiednią skalę pomiaru (Nominalna, Porządkowa, Przedziałowa lub Stosunkowa). Czy zmienna jest dyskretna, czy ciągła?\n\nPłeć: skala nominalna; zmienna dyskretna;\nSatysfakcja klienta: Niska, Średnia, Dobra, Doskonała\nWzrost (ankieta): “Jestem: bardzo niski, niski, przeciętnego wzrostu, wysoki, bardzo wysoki”\nWzrost mierzony w centymetrach\nCzas reakcji (w milisekundach)\nKody pocztowe: np. 00-001, 00-950, 80-452, 31-072\nWiek (w latach)\nMarki samochodów\nNarodowość\nLiczba dzieci w rodzinie: 1 dziecko, 2 dzieci, 3 dzieci, …\nWynik testu IQ\nTemperatura (skala Celsjusza)\nTemperatura (skala Kelvina)\nFrekwencja wyborcza\nPrzynależność partyjna\nWielkość okręgu wyborczego\nWspółrzędne w układzie kartezjańskim\nData (względem określonej epoki, np. n.e.)\nWysokość nad poziomem morza\nGrupy krwi: A, B, AB, 0\nKategorie dochodów: niskie, średnie, wysokie\nStopnie wojskowe\n\nPamiętaj, aby uzasadnić swój wybór skali dla każdej zmiennej.\nDla przykładu: W typologii skal pomiarowych Stevensa, adresy uliczne są danymi nominalnymi. Dlaczego?\nPełnią wyłącznie funkcję etykiet/identyfikatorów Nie mają naturalnego uporządkowania (ul. Mickiewicza 5 nie jest “większa” niż ul. Słowackiego 10) Nie można wykonywać na nich sensownych operacji matematycznych Jedyna dozwolona operacja to sprawdzanie równości/nierówności (czy to ten sam adres czy inny?)\nMimo że numery domów są liczbami, w systemie adresowym funkcjonują jako etykiety, a nie wartości ilościowe. Liczba 100 w adresie “ul. Kilińskiego 100” nie jest używana matematycznie - równie dobrze mogłaby to być “ul. Jabłkowa” czy “ul. Zeusa”, jeśli chodzi o jej funkcję w adresie.\n\n\n\n\n\n\n\n\nDlaczego Niektóre Statystyki Działają (a Inne Nie) dla Skal Interwałowych\n\n\n\nSkala interwałowa to taka, w której odległości między wartościami są znaczące, ale punkt zerowy jest umowny. W przypadku skal interwałowych (np. temperatury):\n\nDozwolone jest dodawanie/odejmowanie wartości oraz mnożenie/dzielenie przez stałe.\nNiedozwolone jest mnożenie/dzielenie wartości ze skali przez siebie, ponieważ prowadzi to do wyników bez interpretacji fizycznej.\n\n\nWłasności Skali Interwałowej\n\nRówne interwały reprezentują takie same różnice:\n\nRóżnica między 20°C a 25°C (5°C) reprezentuje taką samą zmianę jak między 30°C a 35°C.\nProporcje różnic są zachowane: 10°C to dwa razy większa zmiana niż 5°C.\n\nPunkt zero jest umowny:\n\n0°C to punkt zamarzania wody, a nie brak temperatury.\nTen sam stan fizyczny ma różne wartości w różnych skalach: 0°C = 32°F.\n\nTransformacja liniowa:\n\nWzór ogólny: y = ax + b, gdzie a \\neq 0.\nDla temperatury: F = C \\times \\frac{9}{5} + 32.\n\n\n\n\nDlaczego Średnia Arytmetyczna Działa\nŚrednia arytmetyczna działa, ponieważ opiera się na dodawaniu i dzieleniu przez stałą, które są dozwolone w skali interwałowej. Przykład:\nDane: 20°C i 30°C\n\nMetoda 1: Średnia w Celsjuszach, potem konwersja\n1. Średnia: (20°C + 30°C) ÷ 2 = 25°C\n2. Konwersja: 25°C × (9/5) + 32 = 77°F\n\nMetoda 2: Konwersja na °F, potem średnia\n1. Konwersja: 20°C → 68°F, 30°C → 86°F\n2. Średnia: (68°F + 86°F) ÷ 2 = 77°F\n\nObie metody dają ten sam wynik! ✓\nMatematyczny dowód poprawności: \\begin{align}\n\\bar{F} &= \\frac{F_1 + F_2}{2} \\\\\n&= \\frac{(C_1 \\times \\frac{9}{5} + 32) + (C_2 \\times \\frac{9}{5} + 32)}{2} \\\\\n&= \\frac{(C_1 + C_2) \\times \\frac{9}{5} + 64}{2} \\\\\n&= \\left(\\frac{C_1 + C_2}{2}\\right) \\times \\frac{9}{5} + 32 \\\\\n&= \\bar{C} \\times \\frac{9}{5} + 32\n\\end{align}\n\n\nDlaczego Wariancja Jest Problematyczna\nWariancja jest problematyczna, ponieważ opiera się na kwadratach różnic, co prowadzi do jednostek kwadratowych (np. °C²) bez jasnej interpretacji fizycznej. Przykład:\nTe same temperatury: 20°C i 30°C\n\nMetoda 1: Wariancja w Celsjuszach\n1. Średnia: 25°C\n2. Odchylenia: (20 - 25)°C = -5°C, (30 - 25)°C = 5°C\n3. Kwadraty odchyleń: (-5°C)² = 25(°C)², (5°C)² = 25(°C)²\n4. Średnia: (25 + 25)(°C)² ÷ 2 = 25(°C)²\n\nMetoda 2: Wariancja w Fahrenheitach\n1. Konwersja: 20°C → 68°F, 30°C → 86°F\n2. Średnia: 77°F\n3. Odchylenia: (68 - 77)°F = -9°F, (86 - 77)°F = 9°F\n4. Kwadraty odchyleń: (-9°F)² = 81(°F)², (9°F)² = 81(°F)²\n5. Średnia: (81 + 81)(°F)² ÷ 2 = 81(°F)²\n\nProblem: 25(°C)² i 81(°F)² nie są równoważne!\nMatematyczna analiza problemu: \\begin{align}\n(F_i - \\bar{F})^2 &= [(C_i \\times \\frac{9}{5} + 32) - (\\bar{C} \\times \\frac{9}{5} + 32)]^2 \\\\\n&= [(C_i - \\bar{C}) \\times \\frac{9}{5}]^2 \\\\\n&= (C_i - \\bar{C})^2 \\times \\left(\\frac{9}{5}\\right)^2\n\\end{align}\n\n\nWnioski Teoretyczne\n\nOperacje dozwolone:\n\nDodawanie/odejmowanie (zachowuje różnice).\nMnożenie/dzielenie przez stałe (skalowanie).\nŚrednie arytmetyczne.\nPorównywanie różnic temperatur.\n\nOperacje niedozwolone:\n\nMnożenie temperatur przez siebie.\nDzielenie temperatur przez siebie.\nŚrednie geometryczne.\nWspółczynnik zmienności.\n\nImplikacje praktyczne:\n\nWariancja i odchylenie standardowe wymagają ostrożnej interpretacji.\nLepiej używać miar opartych na różnicach (np. MAD - średnie odchylenie bezwzględne).\nPrzy porównywaniu zmienności warto standaryzować dane.\n\n\n\n\nZasada Praktyczna\nJeśli w obliczeniach pojawia się mnożenie wartości ze skali interwałowej przez siebie, należy zachować szczególną ostrożność w interpretacji wyników!\n\n\n\n\n\n\n\n\n\nProporcje w Skalach Pomiarowych: Przypadek Temperatury\n\n\n\n\nDwa Rodzaje Proporcji\n\nProporcje wartości (NIE zachowują się w skali interwałowej):\nWeźmy 80°C i 20°C:\nW Celsjuszach: 80°C/20°C = 4\nW Fahrenheitach: 176°F/68°F ≈ 2.59\nW Kelwinach: 353.15K/293.15K ≈ 1.20\n\nTe same temperatury dają różne proporcje! \n→ Proporcje wartości NIE mają sensu na skalach interwałowych; sens mają tylko na skali ilorazowej.\n\n\nProporcje różnic (zachowują się w skali interwałowej):\nWeźmy dwie pary różnic:\nPara 1: 30°C - 20°C = 10°C\nPara 2: 80°C - 60°C = 20°C\n\nProporcja różnic w Celsjuszach:\n20°C/10°C = 2\n\nTe same temperatury w Fahrenheitach:\nPara 1: 86°F - 68°F = 18°F\nPara 2: 176°F - 140°F = 36°F\n\nProporcja różnic w Fahrenheitach:\n36°F/18°F = 2\n\nProporcja różnic jest taka sama! ✓\n\n\n\nMatematyczne Wyjaśnienie\nDla transformacji F = \\frac{9}{5}C + 32:\n\nProporcje wartości NIE zachowują się: [ = ]\nProporcje różnic zachowują się: [ = = ]\n\n\n\nDlaczego To Jest Ważne?\n\nDla wartości:\n\nW skali Celsjusza: 40°C nie jest “dwa razy cieplejsze” niż 20°C.\nW skali Fahrenheita: 100°F nie jest “dwa razy cieplejsze” niż 50°F.\nTylko w Kelwinach proporcje wartości mają sens fizyczny.\n\nDla różnic:\n\nWzrost o 20°C jest zawsze dwa razy większy niż wzrost o 10°C.\nWzrost o 36°F jest zawsze dwa razy większy niż wzrost o 18°F.\nProporcje różnic są niezależne od skali.\n\n\n\n\nImplikacje dla Statystyk\n\nOperacje bazujące na różnicach (DZIAŁAJĄ):\n\nŚrednia arytmetyczna.\nOdchylenie bezwzględne.\nRozstęp.\n\nOperacje bazujące na proporcjach wartości (NIE DZIAŁAJĄ):\n\nŚrednia geometryczna.\nWspółczynnik zmienności.\nWariancja (bo używa kwadratu wartości).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#appendix-a-miary-statystyczne-dla-różnych-typów-zmiennych",
    "href": "rozdzial2.html#appendix-a-miary-statystyczne-dla-różnych-typów-zmiennych",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.5 Appendix A: Miary Statystyczne dla Różnych Typów Zmiennych",
    "text": "4.5 Appendix A: Miary Statystyczne dla Różnych Typów Zmiennych\n\n\n\n\n\n\n\n\n\n\n\nTyp zmiennej\nTendencja centralna\nMiary rozproszenia\nWzory\nOdpowiednie testy\nWizualizacja\n\n\n\n\nNominalna\nDominanta (moda)\nWskaźnik zróżnicowania\nVR = 1 - \\frac{f_m}{n} gdzie f_m to częstość modalnej, n to liczba obserwacji\nChi-kwadrat, testy dokładne Fishera\nWykresy słupkowe, kołowe\n\n\n\n\nIndeks różnorodności (Simpsona)\nD = 1 - \\sum_{i=1}^{k} p_i^2 gdzie p_i to proporcja i-tej kategorii\nTest zgodności, testy niezależności\nMozaikowe wykresy\n\n\n\n\nEntropia Shannona\nH = -\\sum_{i=1}^{k} p_i \\log_2(p_i) gdzie p_i to proporcja i-tej kategorii\nTesty oparte na entropii\nWykresy entropii, dendrogram\n\n\nPorządkowa\nMediana\nRozstęp\nR = \\max(X) - \\min(X)\nMann-Whitney, Kruskal-Wallis\nWykresy skumulowane, wykresy rozbieżne\n\n\n\n\nRozstęp międzykwartylowy (IQR)\nIQR = Q_3 - Q_1 gdzie Q_1 i Q_3 to pierwszy i trzeci kwartyl\nTesty rangowe, testy mediany\nWykresy pudełkowe\n\n\n\n\nOdchylenie ćwiartkowe\nQ_D = \\frac{Q_3 - Q_1}{2}\nTest Jonckheere-Terpstra\nWykresy porządkowe, skumulowane histogramy\n\n\nInterwałowa/ Ilorazowa\nŚrednia arytmetyczna\nWariancja\n\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2 gdzie \\mu to średnia\nTesty t, ANOVA, regresja\nHistogramy, wykresy rozrzutu\n\n\n\n\nOdchylenie standardowe\n\\sigma = \\sqrt{\\sigma^2}\nF-test, test Levene’a\nWykresy pudełkowe\n\n\n\n\nWspółczynnik zmienności\nCV = \\frac{\\sigma}{\\mu} \\times 100\\%\nTesty równości wariancji\nWykresy QQ, wykresy przedziałowe\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\nNotes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "5  Reliability and Validity in Data Science Research",
    "section": "",
    "text": "5.1 Defining Reliability and Validity\nIn data science and research, two crucial concepts that determine the quality of measurements and studies are reliability and validity. Understanding these concepts is essential for conducting robust research and drawing meaningful conclusions from data.\nReliability refers to the consistency of a measure. A reliable measurement or study produces similar results under consistent conditions.\nValidity refers to the accuracy of a measure. A valid measurement or study accurately represents what it claims to measure.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3.html#the-four-combinations-of-reliability-and-validity",
    "href": "chapter3.html#the-four-combinations-of-reliability-and-validity",
    "title": "5  Reliability and Validity in Data Science Research",
    "section": "5.2 The Four Combinations of Reliability and Validity",
    "text": "5.2 The Four Combinations of Reliability and Validity\nThere are four possible combinations of reliability and validity:\n\nHigh Reliability, High Validity\nHigh Reliability, Low Validity\nLow Reliability, High Validity\nLow Reliability, Low Validity\n\nLet’s explore each of these combinations with examples and visualizations.\n\n1. High Reliability, High Validity\nThis is the ideal scenario in research. Measurements are both consistent and accurate.\nExample: A well-calibrated digital scale used to measure weight. It consistently gives the same reading for the same object and accurately represents the true weight.\n\n\n2. High Reliability, Low Validity\nIn this case, measurements are consistent but not accurate.\nExample: A miscalibrated scale that always measures 5 kg too heavy. It gives consistent results (high reliability) but doesn’t represent the true weight (low validity).\n\n\n3. Low Reliability, High Validity\nHere, measurements are accurate on average but inconsistent.\nExample: A scale that fluctuates around the true weight. Sometimes it’s a bit over, sometimes a bit under, but on average, it’s correct.\n\n\n4. Low Reliability, Low Validity\nThis is the worst-case scenario, where measurements are neither consistent nor accurate.\nExample: A broken scale that gives random readings unrelated to the true weight.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3.html#visualizing-reliability-and-validity",
    "href": "chapter3.html#visualizing-reliability-and-validity",
    "title": "5  Reliability and Validity in Data Science Research",
    "section": "5.3 Visualizing Reliability and Validity",
    "text": "5.3 Visualizing Reliability and Validity\nTo better understand these concepts, let’s create visualizations using ggplot2 in R. We’ll simulate measurement data for each scenario and plot them.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Generate data for each scenario\nn &lt;- 100\ntrue_value &lt;- 50\n\ndata &lt;- tibble(\n  high_rel_high_val = rnorm(n, mean = true_value, sd = 1),\n  high_rel_low_val = rnorm(n, mean = true_value + 5, sd = 1),\n  low_rel_high_val = rnorm(n, mean = true_value, sd = 5),\n  low_rel_low_val = runif(n, min = 0, max = 100)\n) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(cols = -id, names_to = \"scenario\", values_to = \"measurement\")\n\n# Create the scatterplot\nscatter_plot &lt;- ggplot(data, aes(x = id, y = measurement, color = scenario)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_hline(yintercept = true_value, linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~ scenario, ncol = 2, scales = \"free_y\",\n             labeller = labeller(scenario = c(\n               \"high_rel_high_val\" = \"High Reliability, High Validity\",\n               \"high_rel_low_val\" = \"High Reliability, Low Validity\",\n               \"low_rel_high_val\" = \"Low Reliability, High Validity\",\n               \"low_rel_low_val\" = \"Low Reliability, Low Validity\"\n             ))) +\n  labs(title = \"Scatterplots of Measurements\",\n       subtitle = \"Dashed line represents the true value\",\n       x = \"Measurement ID\",\n       y = \"Measured Value\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Create the histogram\nhist_plot &lt;- ggplot(data, aes(x = measurement, fill = scenario)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  geom_vline(xintercept = true_value, color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ scenario, ncol = 2, scales = \"free\",\n             labeller = labeller(scenario = c(\n               \"high_rel_high_val\" = \"High Reliability, High Validity\",\n               \"high_rel_low_val\" = \"High Reliability, Low Validity\",\n               \"low_rel_high_val\" = \"Low Reliability, High Validity\",\n               \"low_rel_low_val\" = \"Low Reliability, Low Validity\"\n             ))) +\n  labs(title = \"Histograms of Measurements\",\n       subtitle = \"Red dashed line represents the true value\",\n       x = \"Measured Value\",\n       y = \"Count\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Combine the plots\ncombined_plot &lt;- scatter_plot / hist_plot +\n  plot_layout(heights = c(1, 1)) +\n  plot_annotation(\n    title = \"Reliability and Validity in Measurements\",\n    theme = theme(plot.title = element_text(hjust = 0.5, size = 24, face = \"bold\"))\n  )\n\n# Display the combined plot\ncombined_plot\n\n\n\n\n\n\n\n\n\nInterpreting the Visualizations\n\nHigh Reliability, High Validity: Points cluster tightly around the true value (dashed line).\nHigh Reliability, Low Validity: Points cluster tightly, but consistently above the true value.\nLow Reliability, High Validity: Points scatter widely but center around the true value.\nLow Reliability, Low Validity: Points scatter randomly with no clear pattern or relation to the true value.\n\nUnderstanding reliability and validity is crucial in data science and research. High reliability ensures consistent measurements, while high validity ensures accurate representations of what we intend to measure. By considering both aspects, researchers can design more robust studies and draw more meaningful conclusions from their data.\nWhen conducting your own research or analyzing others’ work, always consider: - How reliable are the measurements? - How valid is the approach for measuring the intended concept? - Do the methods used support both reliability and validity?\nBy keeping these questions in mind, you’ll be better equipped to produce and interpret high-quality research in data science.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3.html#types-of-reliability",
    "href": "chapter3.html#types-of-reliability",
    "title": "5  Reliability and Validity in Data Science Research",
    "section": "5.4 Types of Reliability",
    "text": "5.4 Types of Reliability\nReliability can be assessed in several ways, each focusing on a different aspect of consistency:\n\nTest-Retest Reliability: This measures the consistency of a test over time. It involves administering the same test to the same group of participants at different times and comparing the results.\nInter-Rater Reliability: This assesses the degree of agreement among different raters or observers. It’s crucial when subjective judgments are involved in data collection.\nInternal Consistency: This evaluates how well different items on a test or scale measure the same construct. Cronbach’s alpha is a common measure of internal consistency.\nParallel Forms Reliability: This involves creating two equivalent forms of a test and administering them to the same group. The correlation between the two sets of scores indicates reliability.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3.html#types-of-validity",
    "href": "chapter3.html#types-of-validity",
    "title": "5  Reliability and Validity in Data Science Research",
    "section": "5.5 Types of Validity",
    "text": "5.5 Types of Validity\nValidity is a multifaceted concept, with several types that researchers need to consider:\n\nContent Validity: This ensures that a measure covers all aspects of the construct it aims to measure. It’s often assessed by expert judgment.\nConstruct Validity: This evaluates whether a test measures the intended theoretical construct. It includes:\n\nConvergent Validity: The degree to which the measure correlates with other measures of the same construct.\nDiscriminant Validity: The extent to which the measure does not correlate with measures of different constructs.\n\nCriterion Validity: This assesses how well a measure predicts an outcome. It includes:\n\nConcurrent Validity: How well the measure correlates with other measures of the same construct at the same time.\nPredictive Validity: How well the measure predicts future outcomes.\n\nFace Validity: Face validity describes how test subjects perceive the test and whether - from their point of view - it is adequate for the purpose it is supposed to serve. A lack of face validity, even though the test may be valid from the perspective of a specific purpose, can contribute to a decrease in motivation among test subjects, which directly affects the results achieved or may lead to rejection of the test. While not a scientific measure, it can be important for participant buy-in.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3.html#internal-vs.-external-validity",
    "href": "chapter3.html#internal-vs.-external-validity",
    "title": "5  Reliability and Validity in Data Science Research",
    "section": "5.6 Internal vs. External Validity",
    "text": "5.6 Internal vs. External Validity\nThese concepts are crucial in experimental design and the generalizability of research findings:\n\nInternal Validity\nInternal validity refers to the extent to which a study establishes a causal relationship between the independent and dependent variables. It answers the question: “Did the experimental treatment actually cause the observed effects?”\nFactors that can threaten internal validity include: - History: External events occurring between pre-test and post-test - Maturation: Natural changes in participants over time - Testing effects: Changes due to taking a pre-test - Instrumentation: Changes in the measurement tool or observers - Selection bias: Non-random assignment to groups - Attrition: Loss of participants during the study\n\n\nExternal Validity\nExternal validity refers to the extent to which the results of a study can be generalized to other situations, populations, or settings. It addresses the question: “To what extent can the findings be applied beyond the specific context of the study?”\nFactors that can affect external validity include: - Population validity: How well the sample represents the larger population - Ecological validity: How well the study setting represents real-world conditions - Temporal validity: Whether the results hold true across time",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3.html#consistency-in-research",
    "href": "chapter3.html#consistency-in-research",
    "title": "5  Reliability and Validity in Data Science Research",
    "section": "5.7 Consistency in Research",
    "text": "5.7 Consistency in Research\nConsistency is closely related to reliability but extends beyond just measurement. In research, consistency refers to the overall coherence and stability of results across different contexts, methods, or studies.\nKey aspects of consistency in research include:\n\nReplicability: The ability to reproduce study results using the same methods and data.\nRobustness: The stability of findings across different analytical approaches or slight variations in methodology.\nConvergence: The alignment of results from different studies or methods investigating the same phenomenon.\nLongitudinal Consistency: The stability of findings over time, especially important in longitudinal studies.\n\nEnsuring consistency in research involves: - Using standardized procedures and measures - Thoroughly documenting methods and analytical decisions - Conducting replication studies - Meta-analyses to synthesize findings across multiple studies",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3.html#balancing-reliability-validity-and-consistency",
    "href": "chapter3.html#balancing-reliability-validity-and-consistency",
    "title": "5  Reliability and Validity in Data Science Research",
    "section": "5.8 Balancing Reliability, Validity, and Consistency",
    "text": "5.8 Balancing Reliability, Validity, and Consistency\nWhile reliability, validity, and consistency are all crucial for high-quality research, they sometimes involve trade-offs:\n\nA highly reliable measure might lack validity if it consistently measures the wrong thing.\nStriving for perfect internal validity (e.g., in tightly controlled lab experiments) might reduce external validity.\nEnsuring high consistency across diverse contexts might require sacrificing some degree of precision or depth in specific situations.\n\nResearchers must carefully balance these aspects based on their research questions and the nature of their study. A comprehensive understanding of reliability, validity, and consistency helps in designing robust studies, interpreting results accurately, and contributing meaningfully to the body of scientific knowledge.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3.html#bias-variance-tradeoff",
    "href": "chapter3.html#bias-variance-tradeoff",
    "title": "5  Reliability and Validity in Data Science Research",
    "section": "5.9 Bias-Variance Tradeoff",
    "text": "5.9 Bias-Variance Tradeoff\nThe concepts of reliability and validity are closely related to the statistical notion of the bias-variance tradeoff. This tradeoff is fundamental in machine learning and statistical modeling.\n\nBias refers to the error introduced by approximating a real-world problem with a simplified model. High bias can lead to underfitting.\nVariance refers to the error introduced by the model’s sensitivity to small fluctuations in the training set. High variance can lead to overfitting.\n\nLet’s visualize this concept with a simplified plot:\n\nx &lt;- seq(0, 10, length.out = 100)\ny_true &lt;- sin(x)\ny_low_bias_high_var &lt;- y_true + rnorm(100, 0, 0.3)\ny_high_bias_low_var &lt;- 0.5 * x\n\ndf &lt;- data.frame(x = rep(x, 3),\n                 y = c(y_true, y_low_bias_high_var, y_high_bias_low_var),\n                 type = rep(c(\"True Function\", \"Low Bias, High Variance\", \"High Bias, Low Variance\"), each = 100))\n\nggplot(df, aes(x = x, y = y, color = type)) +\n  geom_line() +\n  geom_point(data = subset(df, type != \"True Function\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"black\", \"blue\", \"red\")) +\n  labs(title = \"Bias-Variance Tradeoff\",\n       x = \"X\",\n       y = \"Y\",\n       color = \"Model Type\") +\n  theme_minimal()\n\n\n\n\nVisualization of Bias-Variance Tradeoff\n\n\n\n\nIn this plot: - The black line represents the true underlying function. - The blue points represent a model with low bias but high variance. It follows the true function closely on average but has a lot of noise. - The red line represents a model with high bias but low variance. It consistently underestimates the true function but has less noise.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3.html#accuracy-and-precision",
    "href": "chapter3.html#accuracy-and-precision",
    "title": "5  Reliability and Validity in Data Science Research",
    "section": "5.10 Accuracy and Precision",
    "text": "5.10 Accuracy and Precision\nThe concepts of accuracy and precision are closely related to validity and reliability:\n\nAccuracy refers to how close a measurement is to the true value (similar to validity).\nPrecision refers to how consistent or reproducible the measurements are (similar to reliability).\n\nWe can visualize these concepts using a simplified target analogy:\n\nlibrary(ggplot2)\nlibrary(ggforce)\n\ncreate_points &lt;- function(n, x_center, y_center, spread) {\n  data.frame(\n    x = rnorm(n, x_center, spread),\n    y = rnorm(n, y_center, spread)\n  )\n}\n\nset.seed(101)\npoints &lt;- rbind(\n  cbind(create_points(20, 0, 0, 0.1), type = \"High Accuracy\\nHigh Precision\"),\n  cbind(create_points(20, 0.5, 0.5, 0.1), type = \"Low Accuracy\\nHigh Precision\"),\n  cbind(create_points(20, 0, 0, 0.3), type = \"High Accuracy\\nLow Precision\"),\n  cbind(create_points(20, 0.5, 0.5, 0.3), type = \"Low Accuracy\\nLow Precision\")\n)\n\nggplot(points, aes(x, y)) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"black\", fill = NA) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 0.5), color = \"black\", fill = NA) +\n  geom_point(color = \"red\", size = 2) +\n  facet_wrap(~type) +\n  coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Accuracy vs Precision\")\n\n\n\n\nVisualization of Accuracy vs Precision\n\n\n\n\nIn this visualization: - High accuracy means the points are close to the center (bullseye). - High precision means the points are tightly clustered. - Each panel represents a different combination of accuracy and precision.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3.html#conclusion",
    "href": "chapter3.html#conclusion",
    "title": "5  Reliability and Validity in Data Science Research",
    "section": "5.11 Conclusion",
    "text": "5.11 Conclusion\nUnderstanding reliability and validity is crucial for conducting robust research. These concepts help us ensure that our measurements are both consistent and accurate. By relating them to ideas like the bias-variance tradeoff and accuracy-precision, we gain a deeper appreciation of the challenges involved in measurement and modeling in scientific research. As researchers, we must strive to develop measures and models that are both reliable and valid, balancing the tradeoffs between bias and variance, and between accuracy and precision. This requires careful design of research methodologies, rigorous testing of our measurement instruments, and thoughtful interpretation of our results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3.html#understanding-bias-vs.-variance-in-statistical-measurement",
    "href": "chapter3.html#understanding-bias-vs.-variance-in-statistical-measurement",
    "title": "5  Reliability and Validity in Data Science Research",
    "section": "5.12 Understanding Bias vs. Variance in Statistical Measurement",
    "text": "5.12 Understanding Bias vs. Variance in Statistical Measurement\n\nIntroduction\nIn statistics and machine learning, two important concepts that affect the performance of our models are bias and variance. Understanding these concepts is crucial for building effective predictive models and avoiding common pitfalls like overfitting and underfitting.\n\nBias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can lead to underfitting.\n\nThink of bias as how far off our predictions are from the true values on average.\nIn terms of validity, high bias means our model isn’t capturing the true relationship in the data.\n\nVariance refers to the amount by which our model would change if we estimated it using a different training dataset. High variance can lead to overfitting.\n\nThink of variance as how much our predictions would fluctuate if we used different datasets.\nIn terms of reliability, high variance means our model is too sensitive to the specific data it was trained on.\n\n\nWe’ll explore four scenarios to illustrate different combinations of bias and variance using synthetic data and regression models.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3.html#data-generation-and-model-fitting-function",
    "href": "chapter3.html#data-generation-and-model-fitting-function",
    "title": "5  Reliability and Validity in Data Science Research",
    "section": "5.13 Data Generation and Model Fitting Function",
    "text": "5.13 Data Generation and Model Fitting Function\nFirst, let’s create a function that will help us generate data and fit models for each scenario:\n\ngenerate_and_fit &lt;- function(n, intercept, slope, noise_sd, model_degree) {\n  # Generate synthetic data\n  x &lt;- runif(n, 0, 10)\n  y &lt;- intercept + slope * x + rnorm(n, 0, noise_sd)\n  \n  # Fit model\n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", model_degree, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  # Generate predictions\n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  # Plot\n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    geom_abline(intercept = intercept, slope = slope, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Model Degree:\", model_degree),\n         subtitle = paste(\"Noise SD:\", noise_sd),\n         x = \"X (Input Variable)\",\n         y = \"Y (Target Variable)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nThis function does the following: 1. Generates synthetic data based on our parameters 2. Fits a polynomial regression model 3. Creates a plot showing the true relationship (blue dashed line), our model’s predictions (red solid line), and the data points\nNow, let’s explore our four scenarios!\n\nScenario 1: Low Bias, Low Variance\nIn this ideal scenario, we use a linear model to fit linear data with low noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 1, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The model (red line) closely follows the true relationship (blue dashed line). - Data points are clustered tightly around the line, indicating low noise. - This scenario represents a good fit: the model captures the underlying trend without being overly complex.\n\n\nScenario 2: Low Bias, High Variance\nHere, we use a linear model to fit linear data, but with high noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 5, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The model still captures the general trend, but data points are more scattered. - This high variance means our model’s predictions would be less reliable. - In real-world terms, this might represent a situation where our measurements are correct on average but have a lot of random error.\n\n\nScenario 3: High Bias, Low Variance\nIn this case, we use a linear model to fit quadratic (curved) data with low noise.\n\nquadratic_data &lt;- function(n, intercept, slope, noise_sd, model_degree) {\n  x &lt;- runif(n, 0, 10)\n  y &lt;- intercept + slope * x^2 + rnorm(n, 0, noise_sd)\n  \n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", model_degree, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    stat_function(fun = function(x) intercept + slope * x^2, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Model Degree:\", model_degree),\n         subtitle = paste(\"Noise SD:\", noise_sd),\n         x = \"X (Input Variable)\",\n         y = \"Y (Target Variable)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nquadratic_data(n = 100, intercept = 1, slope = 0.2, noise_sd = 1, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The linear model (red line) fails to capture the curvature of the true relationship (blue dashed line). - This high bias means our model is consistently off in its predictions. - In real-world terms, this might represent using an overly simplistic model for a complex phenomenon.\n\n\nScenario 4: High Bias, High Variance\nFinally, we use a high-degree polynomial to fit linear data with high noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 5, model_degree = 5)\n\n\n\n\n\n\n\n\nExplanation: - The model (red line) is overly complex, trying to fit the noise rather than the underlying trend. - This combination of high bias and high variance leads to poor generalization. - In real-world terms, this might represent overcomplicating our analysis and drawing false conclusions from random fluctuations in our data.\n\n\nConclusion\nUnderstanding the bias-variance trade-off is crucial in statistical modeling:\n\nLow Bias, Low Variance: The ideal scenario, where our model accurately captures the underlying relationship without being overly sensitive to noise.\nLow Bias, High Variance: Our model is correct on average but unreliable due to high sensitivity to individual data points.\nHigh Bias, Low Variance: Our model is consistently wrong due to oversimplification but gives stable predictions.\nHigh Bias, High Variance: The worst-case scenario, where our model is both inaccurate and unreliable.\n\nIn practice, we often need to balance bias and variance. Techniques like cross-validation, regularization, and ensemble methods can help find this balance.\nRemember: - A model with high bias is too simple and misses important patterns in the data. - A model with high variance is too complex and fits noise in the training data. - The goal is to find a sweet spot that captures true patterns without overfitting to noise.\nBy understanding these concepts, you’ll be better equipped to choose appropriate models, avoid overfitting and underfitting, and build more effective predictive models in your future statistical analyses!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html",
    "href": "rozdzial3.html",
    "title": "6  Rzetelność i Trafność w Badaniach Statystycznych",
    "section": "",
    "text": "6.1 Definiowanie Rzetelności i Trafności\nW naukach o danych i badaniach naukowych, dwa kluczowe pojęcia, które określają jakość pomiarów i badań, to rzetelność i trafność. Zrozumienie tych pojęć jest niezbędne do prowadzenia solidnych badań i wyciągania znaczących wniosków z danych.\nRzetelność odnosi się do spójności pomiaru. Rzetelny pomiar lub badanie daje podobne wyniki w spójnych warunkach.\nTrafność odnosi się do dokładności pomiaru. Trafny pomiar lub badanie dokładnie reprezentuje to, co twierdzi, że mierzy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Statystycznych</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#cztery-kombinacje-rzetelności-i-trafności",
    "href": "rozdzial3.html#cztery-kombinacje-rzetelności-i-trafności",
    "title": "6  Rzetelność i Trafność w Badaniach Statystycznych",
    "section": "6.2 Cztery Kombinacje Rzetelności i Trafności",
    "text": "6.2 Cztery Kombinacje Rzetelności i Trafności\nIstnieją cztery możliwe kombinacje rzetelności i trafności:\n\nWysoka Rzetelność, Wysoka Trafność\nWysoka Rzetelność, Niska Trafność\nNiska Rzetelność, Wysoka Trafność\nNiska Rzetelność, Niska Trafność\n\nPrzyjrzyjmy się każdej z tych kombinacji z przykładami i wizualizacjami.\n\n1. Wysoka Rzetelność, Wysoka Trafność\nTo idealny scenariusz w badaniach. Pomiary są zarówno spójne, jak i dokładne.\nPrzykład: Dobrze skalibrowana waga cyfrowa używana do pomiaru wagi. Konsekwentnie daje ten sam odczyt dla tego samego obiektu i dokładnie reprezentuje prawdziwą wagę.\n\n\n2. Wysoka Rzetelność, Niska Trafność\nW tym przypadku pomiary są spójne, ale niedokładne.\nPrzykład: Źle skalibrowana waga, która zawsze mierzy 5 kg za ciężko. Daje spójne wyniki (wysoka rzetelność), ale nie reprezentuje prawdziwej wagi (niska trafność).\n\n\n3. Niska Rzetelność, Wysoka Trafność\nTutaj pomiary są dokładne średnio, ale niespójne.\nPrzykład: Waga, która waha się wokół prawdziwej wagi. Czasami pokazuje trochę więcej, czasami trochę mniej, ale średnio jest poprawna.\n\n\n4. Niska Rzetelność, Niska Trafność\nTo najgorszy scenariusz, gdzie pomiary nie są ani spójne, ani dokładne.\nPrzykład: Zepsuta waga, która daje losowe odczyty niezwiązane z prawdziwą wagą.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Statystycznych</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wizualizacja-rzetelności-i-trafności",
    "href": "rozdzial3.html#wizualizacja-rzetelności-i-trafności",
    "title": "6  Rzetelność i Trafność w Badaniach Statystycznych",
    "section": "6.3 Wizualizacja Rzetelności i Trafności",
    "text": "6.3 Wizualizacja Rzetelności i Trafności\nAby lepiej zrozumieć te pojęcia, stwórzmy wizualizacje przy użyciu ggplot2 w R. Zasymulujemy dane pomiarowe dla każdego scenariusza i narysujemy je.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Generowanie danych dla każdego scenariusza\nn &lt;- 100\nprawdziwa_wartosc &lt;- 50\n\ndane &lt;- tibble(\n  wysoka_rz_wysoka_tr = rnorm(n, mean = prawdziwa_wartosc, sd = 1),\n  wysoka_rz_niska_tr = rnorm(n, mean = prawdziwa_wartosc + 5, sd = 1),\n  niska_rz_wysoka_tr = rnorm(n, mean = prawdziwa_wartosc, sd = 5),\n  niska_rz_niska_tr = runif(n, min = 0, max = 100)\n) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(cols = -id, names_to = \"scenariusz\", values_to = \"pomiar\")\n\n# Tworzenie wykresu punktowego\nwykres_punktowy &lt;- ggplot(dane, aes(x = id, y = pomiar, color = scenariusz)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_hline(yintercept = prawdziwa_wartosc, linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~ scenariusz, ncol = 2, scales = \"free_y\",\n             labeller = labeller(scenariusz = c(\n               \"wysoka_rz_wysoka_tr\" = \"Wysoka Rzetelność, Wysoka Trafność\",\n               \"wysoka_rz_niska_tr\" = \"Wysoka Rzetelność, Niska Trafność\",\n               \"niska_rz_wysoka_tr\" = \"Niska Rzetelność, Wysoka Trafność\",\n               \"niska_rz_niska_tr\" = \"Niska Rzetelność, Niska Trafność\"\n             ))) +\n  labs(title = \"Wykresy punktowe pomiarów\",\n       subtitle = \"Przerywana linia reprezentuje prawdziwą wartość\",\n       x = \"ID pomiaru\",\n       y = \"Zmierzona wartość\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Tworzenie histogramu\nwykres_hist &lt;- ggplot(dane, aes(x = pomiar, fill = scenariusz)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  geom_vline(xintercept = prawdziwa_wartosc, color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ scenariusz, ncol = 2, scales = \"free\",\n             labeller = labeller(scenariusz = c(\n               \"wysoka_rz_wysoka_tr\" = \"Wysoka Rzetelność, Wysoka Trafność\",\n               \"wysoka_rz_niska_tr\" = \"Wysoka Rzetelność, Niska Trafność\",\n               \"niska_rz_wysoka_tr\" = \"Niska Rzetelność, Wysoka Trafność\",\n               \"niska_rz_niska_tr\" = \"Niska Rzetelność, Niska Trafność\"\n             ))) +\n  labs(title = \"Histogramy pomiarów\",\n       subtitle = \"Czerwona przerywana linia reprezentuje prawdziwą wartość\",\n       x = \"Zmierzona wartość\",\n       y = \"Liczba\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Łączenie wykresów\nwykres_polaczony &lt;- wykres_punktowy / wykres_hist +\n  plot_layout(heights = c(1, 1)) +\n  plot_annotation(\n    title = \"Rzetelność i Trafność w Pomiarach\",\n    theme = theme(plot.title = element_text(hjust = 0.5, size = 24, face = \"bold\"))\n  )\n\n# Wyświetlanie połączonego wykresu\nwykres_polaczony\n\n\n\n\n\n\n\n\n\nInterpretacja Wizualizacji\n\nWysoka Rzetelność, Wysoka Trafność: Punkty grupują się ciasno wokół prawdziwej wartości (przerywana linia).\nWysoka Rzetelność, Niska Trafność: Punkty grupują się ciasno, ale konsekwentnie powyżej prawdziwej wartości.\nNiska Rzetelność, Wysoka Trafność: Punkty rozpraszają się szeroko, ale centrują się wokół prawdziwej wartości.\nNiska Rzetelność, Niska Trafność: Punkty rozpraszają się losowo bez wyraźnego wzoru lub relacji do prawdziwej wartości.\n\nZrozumienie rzetelności i trafności jest kluczowe w naukach o danych i badaniach. Wysoka rzetelność zapewnia spójne pomiary, podczas gdy wysoka trafność zapewnia dokładne reprezentacje tego, co zamierzamy zmierzyć. Biorąc pod uwagę oba aspekty, badacze mogą projektować bardziej solidne badania i wyciągać bardziej znaczące wnioski ze swoich danych.\nProwadząc własne badania lub analizując pracę innych, zawsze należy rozważyć: - Jak rzetelne są pomiary? - Jak trafne jest podejście do pomiaru zamierzonego pojęcia? - Czy stosowane metody wspierają zarówno rzetelność, jak i trafność?\nMając na uwadze te pytania, będziesz lepiej przygotowany do prowadzenia i interpretowania wysokiej jakości badań w naukach o danych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Statystycznych</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#rodzaje-rzetelności",
    "href": "rozdzial3.html#rodzaje-rzetelności",
    "title": "6  Rzetelność i Trafność w Badaniach Statystycznych",
    "section": "6.4 Rodzaje Rzetelności",
    "text": "6.4 Rodzaje Rzetelności\nRzetelność można oceniać na kilka sposobów, każdy skupiający się na innym aspekcie spójności:\n\nRzetelność test-retest: Mierzy spójność testu w czasie. Polega na przeprowadzeniu tego samego testu na tej samej grupie uczestników w różnych momentach i porównaniu wyników.\nRzetelność między oceniającymi: Ocenia stopień zgodności między różnymi oceniającymi lub obserwatorami. Jest kluczowa, gdy w zbieraniu danych biorą udział subiektywne osądy.\nSpójność wewnętrzna: Ocenia, jak dobrze różne elementy testu lub skali mierzą ten sam konstrukt. Alfa Cronbacha jest powszechną miarą spójności wewnętrznej.\nRzetelność form równoległych: Polega na stworzeniu dwóch równoważnych form testu i przeprowadzeniu ich na tej samej grupie. Korelacja między dwoma zestawami wyników wskazuje na rzetelność.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Statystycznych</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#rodzaje-trafności",
    "href": "rozdzial3.html#rodzaje-trafności",
    "title": "6  Rzetelność i Trafność w Badaniach Statystycznych",
    "section": "6.5 Rodzaje Trafności",
    "text": "6.5 Rodzaje Trafności\nTrafność jest pojęciem wieloaspektowym, z kilkoma rodzajami, które badacze muszą wziąć pod uwagę:\n\nTrafność treściowa: Zapewnia, że pomiar obejmuje wszystkie aspekty konstruktu, który ma mierzyć. Często jest oceniana przez osąd ekspertów.\nTrafność konstrukcyjna: Ocenia, czy test mierzy zamierzony konstrukt teoretyczny. Obejmuje:\n\nTrafność zbieżną: Stopień, w jakim pomiar koreluje z innymi pomiarami tego samego konstruktu.\nTrafność różnicową: Zakres, w jakim pomiar nie koreluje z pomiarami różnych konstruktów.\n\nTrafność kryterialną: Ocenia, jak dobrze pomiar przewiduje wynik. Obejmuje:\n\nTrafność współbieżną: Jak dobrze pomiar koreluje z innymi pomiarami tego samego konstruktu w tym samym czasie.\nTrafność predykcyjną: Jak dobrze pomiar przewiduje przyszłe wyniki.\n\nTrafność fasadowa: Trafność fasadowa odnosi się do tego, jak osoby badane postrzegają test i czy uważają go za odpowiedni do celu, któremu ma służyć. Brak trafności fasadowej może mieć negatywne konsekwencje, nawet jeśli test jest faktycznie trafny (czyli mierzy to, co powinien mierzyć) z punktu widzenia jego zamierzonego celu. Choć nie jest to naukowa miara, może być ważna dla zaangażowania uczestników.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Statystycznych</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#trafność-wewnętrzna-vs-zewnętrzna",
    "href": "rozdzial3.html#trafność-wewnętrzna-vs-zewnętrzna",
    "title": "6  Rzetelność i Trafność w Badaniach Statystycznych",
    "section": "6.6 Trafność Wewnętrzna vs Zewnętrzna",
    "text": "6.6 Trafność Wewnętrzna vs Zewnętrzna\nTe pojęcia są kluczowe w projektowaniu eksperymentów i możliwości uogólniania wyników badań:\n\nTrafność Wewnętrzna\nTrafność wewnętrzna odnosi się do zakresu, w jakim badanie ustanawia związek przyczynowy między zmiennymi niezależnymi a zależnymi. Odpowiada na pytanie: “Czy eksperymentalne traktowanie rzeczywiście spowodowało zaobserwowane efekty?”\nCzynniki, które mogą zagrażać trafności wewnętrznej, obejmują: - Historia: Zewnętrzne wydarzenia występujące między pre-testem a post-testem - Dojrzewanie: Naturalne zmiany u uczestników w czasie - Efekty testowania: Zmiany wynikające z przeprowadzenia pre-testu - Instrumentacja: Zmiany w narzędziu pomiarowym lub obserwatorach - Błąd selekcji: Nielosowy przydział do grup - Utrata: Utrata uczestników podczas badania\n\n\nTrafność Zewnętrzna\nTrafność zewnętrzna odnosi się do zakresu, w jakim wyniki badania mogą być uogólnione na inne sytuacje, populacje lub ustawienia. Odpowiada na pytanie: “W jakim stopniu wyniki mogą być zastosowane poza konkretnym kontekstem badania?”\nCzynniki, które mogą wpływać na trafność zewnętrzną, obejmują: - Trafność populacyjna: Jak dobrze próba reprezentuje szerszą populację - Trafność ekologiczna: Jak dobrze ustawienie badania reprezentuje warunki świata rzeczywistego - Trafność czasowa: Czy wyniki pozostają prawdziwe w czasie",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Statystycznych</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#spójność-w-badaniach",
    "href": "rozdzial3.html#spójność-w-badaniach",
    "title": "6  Rzetelność i Trafność w Badaniach Statystycznych",
    "section": "6.7 Spójność w Badaniach",
    "text": "6.7 Spójność w Badaniach\nSpójność jest ściśle związana z rzetelnością, ale wykracza poza sam pomiar. W badaniach spójność odnosi się do ogólnej koherencji i stabilności wyników w różnych kontekstach, metodach lub badaniach.\nKluczowe aspekty spójności w badaniach obejmują:\n\nReplikowalność: Zdolność do odtworzenia wyników badania przy użyciu tych samych metod i danych.\nOdporność: Stabilność wyników w różnych podejściach analitycznych lub niewielkich zmianach w metodologii.\nKonwergencja: Zbieżność wyników z różnych badań lub metod badających to samo zjawisko.\nSpójność długoterminowa: Stabilność wyników w czasie, szczególnie ważna w badaniach długoterminowych.\n\nZapewnienie spójności w badaniach obejmuje: - Stosowanie standaryzowanych procedur i miar - Dokładne dokumentowanie metod i decyzji analitycznych - Przeprowadzanie badań replikacyjnych - Meta-analizy w celu syntezy wyników z wielu badań",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Statystycznych</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#równoważenie-rzetelności-trafności-i-spójności",
    "href": "rozdzial3.html#równoważenie-rzetelności-trafności-i-spójności",
    "title": "6  Rzetelność i Trafność w Badaniach Statystycznych",
    "section": "6.8 Równoważenie Rzetelności, Trafności i Spójności",
    "text": "6.8 Równoważenie Rzetelności, Trafności i Spójności\nChociaż rzetelność, trafność i spójność są kluczowe dla wysokiej jakości badań, czasami wiążą się z kompromisami:\n\nWysoce rzetelna miara może nie mieć trafności, jeśli konsekwentnie mierzy niewłaściwą rzecz.\nDążenie do idealnej trafności wewnętrznej (np. w ściśle kontrolowanych eksperymentach laboratoryjnych) może zmniejszyć trafność zewnętrzną.\nZapewnienie wysokiej spójności w różnych kontekstach może wymagać poświęcenia pewnego stopnia precyzji lub głębi w konkretnych sytuacjach.\n\nBadacze muszą starannie równoważyć te aspekty w oparciu o swoje pytania badawcze i charakter badania. Kompleksowe zrozumienie rzetelności, trafności i spójności pomaga w projektowaniu solidnych badań, dokładnej interpretacji wyników i znaczącym wkładzie do korpusu wiedzy naukowej.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Statystycznych</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#kompromis-między-obciążeniem-a-wariancją",
    "href": "rozdzial3.html#kompromis-między-obciążeniem-a-wariancją",
    "title": "6  Rzetelność i Trafność w Badaniach Statystycznych",
    "section": "6.9 Kompromis między Obciążeniem a Wariancją",
    "text": "6.9 Kompromis między Obciążeniem a Wariancją\nPojęcia rzetelności i trafności są ściśle związane ze statystycznym pojęciem kompromisu między obciążeniem a wariancją. Ten kompromis jest fundamentalny w uczeniu maszynowym i modelowaniu statystycznym.\n\nObciążenie odnosi się do błędu wprowadzonego przez przybliżenie problemu ze świata rzeczywistego uproszczonym modelem. Wysokie obciążenie może prowadzić do niedopasowania.\nWariancja odnosi się do błędu wprowadzonego przez wrażliwość modelu na małe fluktuacje w zbiorze treningowym. Wysoka wariancja może prowadzić do przeuczenia.\n\nZobrazujmy to pojęcie za pomocą uproszczonego wykresu:\n\nx &lt;- seq(0, 10, length.out = 100)\ny_prawdziwa &lt;- sin(x)\ny_niskie_obciazenie_wysoka_wariancja &lt;- y_prawdziwa + rnorm(100, 0, 0.3)\ny_wysokie_obciazenie_niska_wariancja &lt;- 0.5 * x\n\ndf &lt;- data.frame(x = rep(x, 3),\n                 y = c(y_prawdziwa, y_niskie_obciazenie_wysoka_wariancja, y_wysokie_obciazenie_niska_wariancja),\n                 typ = rep(c(\"Prawdziwa Funkcja\", \"Niskie Obciążenie, Wysoka Wariancja\", \"Wysokie Obciążenie, Niska Wariancja\"), each = 100))\n\nggplot(df, aes(x = x, y = y, color = typ)) +\n  geom_line() +\n  geom_point(data = subset(df, typ != \"Prawdziwa Funkcja\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"black\", \"blue\", \"red\")) +\n  labs(title = \"Kompromis między Obciążeniem a Wariancją\",\n       x = \"X\",\n       y = \"Y\",\n       color = \"Typ Modelu\") +\n  theme_minimal()\n\n\n\n\nWizualizacja kompromisu między obciążeniem a wariancją\n\n\n\n\nNa tym wykresie: - Czarna linia reprezentuje prawdziwą funkcję bazową. - Niebieskie punkty reprezentują model z niskim obciążeniem, ale wysoką wariancją. Średnio podąża blisko prawdziwej funkcji, ale ma dużo szumu. - Czerwona linia reprezentuje model z wysokim obciążeniem, ale niską wariancją. Konsekwentnie niedoszacowuje prawdziwej funkcji, ale ma mniej szumu.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Statystycznych</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#dokładność-i-precyzja",
    "href": "rozdzial3.html#dokładność-i-precyzja",
    "title": "6  Rzetelność i Trafność w Badaniach Statystycznych",
    "section": "6.10 Dokładność i Precyzja",
    "text": "6.10 Dokładność i Precyzja\nPojęcia dokładności i precyzji są ściśle związane z trafnością i rzetelnością:\n\nDokładność odnosi się do tego, jak blisko pomiar jest prawdziwej wartości (podobnie do trafności).\nPrecyzja odnosi się do tego, jak spójne lub powtarzalne są pomiary (podobnie do rzetelności).\n\nMożemy zobrazować te pojęcia za pomocą uproszczonej analogii do tarczy:\n\nlibrary(ggplot2)\nlibrary(ggforce)\n\ncreate_points &lt;- function(n, x_center, y_center, spread) {\n  data.frame(\n    x = rnorm(n, x_center, spread),\n    y = rnorm(n, y_center, spread)\n  )\n}\n\nset.seed(101)\npoints &lt;- rbind(\n  cbind(create_points(20, 0, 0, 0.1), type = \"Wysoka Dokładność\\nWysoka Precyzja\"),\n  cbind(create_points(20, 0.5, 0.5, 0.1), type = \"Niska Dokładność\\nWysoka Precyzja\"),\n  cbind(create_points(20, 0, 0, 0.3), type = \"Wysoka Dokładność\\nNiska Precyzja\"),\n  cbind(create_points(20, 0.5, 0.5, 0.3), type = \"Niska Dokładność\\nNiska Precyzja\")\n)\n\nggplot(points, aes(x, y)) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"black\", fill = NA) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 0.5), color = \"black\", fill = NA) +\n  geom_point(color = \"red\", size = 2) +\n  facet_wrap(~type) +\n  coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Dokładność vs Precyzja\")\n\n\n\n\nWizualizacja Dokładności vs Precyzji\n\n\n\n\nW tej wizualizacji: - Wysoka dokładność oznacza, że punkty są blisko środka (dziesiątki). - Wysoka precyzja oznacza, że punkty są ściśle zgrupowane. - Każdy panel reprezentuje inną kombinację dokładności i precyzji.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Statystycznych</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#podsumowanie",
    "href": "rozdzial3.html#podsumowanie",
    "title": "6  Rzetelność i Trafność w Badaniach Statystycznych",
    "section": "6.11 Podsumowanie",
    "text": "6.11 Podsumowanie\nZrozumienie rzetelności i trafności jest kluczowe dla prowadzenia solidnych badań. Pojęcia te pomagają nam zapewnić, że nasze pomiary są zarówno spójne, jak i dokładne. Łącząc je z ideami takimi jak kompromis między obciążeniem a wariancją oraz dokładnością i precyzją, zyskujemy głębsze zrozumienie wyzwań związanych z pomiarem i modelowaniem w badaniach naukowych. Jako badacze musimy dążyć do opracowania miar i modeli, które są zarówno rzetelne, jak i trafne, równoważąc kompromisy między obciążeniem a wariancją oraz między dokładnością a precyzją. Wymaga to starannego projektowania metodologii badań, rygorystycznego testowania naszych instrumentów pomiarowych i przemyślanej interpretacji naszych wyników.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Statystycznych</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#zrozumienie-obciążenia-vs.-wariancji-w-pomiarach-statystycznych",
    "href": "rozdzial3.html#zrozumienie-obciążenia-vs.-wariancji-w-pomiarach-statystycznych",
    "title": "6  Rzetelność i Trafność w Badaniach Statystycznych",
    "section": "6.12 Zrozumienie Obciążenia vs. Wariancji w Pomiarach Statystycznych",
    "text": "6.12 Zrozumienie Obciążenia vs. Wariancji w Pomiarach Statystycznych\n\nWprowadzenie\nW statystyce i uczeniu maszynowym dwa ważne pojęcia, które wpływają na wydajność naszych modeli, to obciążenie (bias) i wariancja (variance). Zrozumienie tych pojęć jest kluczowe dla budowania efektywnych modeli predykcyjnych i unikania typowych pułapek, takich jak przeuczenie i niedouczenie.\n\nObciążenie odnosi się do błędu wprowadzonego przez przybliżenie rzeczywistego problemu, który może być złożony, za pomocą uproszczonego modelu. Wysokie obciążenie może prowadzić do niedouczenia.\n\nWyobraź sobie obciążenie jako średnią odległość naszych przewidywań od prawdziwych wartości.\nW kontekście trafności, wysokie obciążenie oznacza, że nasz model nie uchwycił prawdziwej zależności w danych.\n\nWariancja odnosi się do tego, jak bardzo nasz model zmieniłby się, gdybyśmy oszacowali go przy użyciu innego zbioru treningowego. Wysoka wariancja może prowadzić do przeuczenia.\n\nWyobraź sobie wariancję jako to, jak bardzo nasze przewidywania wahałyby się, gdybyśmy użyli różnych zbiorów danych.\nW kontekście rzetelności, wysoka wariancja oznacza, że nasz model jest zbyt wrażliwy na konkretne dane, na których został wytrenowany.\n\n\nZbadamy cztery scenariusze, aby zilustrować różne kombinacje obciążenia i wariancji przy użyciu syntetycznych danych i modeli regresji.\n\n\nFunkcja Generowania Danych i Dopasowywania Modelu\nNajpierw stwórzmy funkcję, która pomoże nam generować dane i dopasowywać modele dla każdego scenariusza:\n\ngeneruj_i_dopasuj &lt;- function(n, wyraz_wolny, nachylenie, odch_szumu, stopien_modelu) {\n  # Generowanie syntetycznych danych\n  x &lt;- runif(n, 0, 10)\n  y &lt;- wyraz_wolny + nachylenie * x + rnorm(n, 0, odch_szumu)\n  \n  # Dopasowanie modelu\n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", stopien_modelu, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  # Generowanie przewidywań\n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  # Wykres\n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    geom_abline(intercept = wyraz_wolny, slope = nachylenie, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Stopień Modelu:\", stopien_modelu),\n         subtitle = paste(\"Odchylenie Standardowe Szumu:\", odch_szumu),\n         x = \"X (Zmienna Wejściowa)\",\n         y = \"Y (Zmienna Docelowa)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nTa funkcja wykonuje następujące czynności: 1. Generuje syntetyczne dane na podstawie naszych parametrów 2. Dopasowuje model regresji wielomianowej 3. Tworzy wykres pokazujący prawdziwą zależność (niebieska przerywana linia), przewidywania naszego modelu (czerwona ciągła linia) i punkty danych\nTeraz zbadajmy nasze cztery scenariusze!\n\n\nScenariusz 1: Niskie Obciążenie, Niska Wariancja\nW tym idealnym scenariuszu używamy modelu liniowego do dopasowania danych liniowych z niskim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 1, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model (czerwona linia) ściśle podąża za prawdziwą zależnością (niebieska przerywana linia). - Punkty danych są skupione blisko linii, co wskazuje na niski szum. - Ten scenariusz reprezentuje dobre dopasowanie: model uchwycił podstawowy trend bez nadmiernej złożoności.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Statystycznych</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#scenariusz-2-niskie-obciążenie-wysoka-wariancja",
    "href": "rozdzial3.html#scenariusz-2-niskie-obciążenie-wysoka-wariancja",
    "title": "6  Rzetelność i Trafność w Badaniach Statystycznych",
    "section": "6.13 Scenariusz 2: Niskie Obciążenie, Wysoka Wariancja",
    "text": "6.13 Scenariusz 2: Niskie Obciążenie, Wysoka Wariancja\nTutaj używamy modelu liniowego do dopasowania danych liniowych, ale z wysokim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 5, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model nadal uchwycił ogólny trend, ale punkty danych są bardziej rozproszone. - Ta wysoka wariancja oznacza, że przewidywania naszego modelu byłyby mniej wiarygodne. - W rzeczywistych warunkach mogłoby to reprezentować sytuację, w której nasze pomiary są średnio poprawne, ale mają dużo losowego błędu.\n\nScenariusz 3: Wysokie Obciążenie, Niska Wariancja\nW tym przypadku używamy modelu liniowego do dopasowania danych kwadratowych (zakrzywionych) z niskim szumem.\n\ndane_kwadratowe &lt;- function(n, wyraz_wolny, nachylenie, odch_szumu, stopien_modelu) {\n  x &lt;- runif(n, 0, 10)\n  y &lt;- wyraz_wolny + nachylenie * x^2 + rnorm(n, 0, odch_szumu)\n  \n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", stopien_modelu, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    stat_function(fun = function(x) wyraz_wolny + nachylenie * x^2, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Stopień Modelu:\", stopien_modelu),\n         subtitle = paste(\"Odchylenie Standardowe Szumu:\", odch_szumu),\n         x = \"X (Zmienna Wejściowa)\",\n         y = \"Y (Zmienna Docelowa)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\ndane_kwadratowe(n = 100, wyraz_wolny = 1, nachylenie = 0.2, odch_szumu = 1, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model liniowy (czerwona linia) nie uchwycił krzywizny prawdziwej zależności (niebieska przerywana linia). - To wysokie obciążenie oznacza, że nasz model konsekwentnie myli się w swoich przewidywaniach. - W rzeczywistych warunkach mogłoby to reprezentować użycie zbyt uproszczonego modelu dla złożonego zjawiska.\n\n\nScenariusz 4: Wysokie Obciążenie, Wysoka Wariancja\nNa koniec używamy wielomianu wysokiego stopnia do dopasowania danych liniowych z wysokim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 5, stopien_modelu = 5)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model (czerwona linia) jest zbyt złożony, próbując dopasować się do szumu zamiast do podstawowego trendu. - Ta kombinacja wysokiego obciążenia i wysokiej wariancji prowadzi do słabej generalizacji. - W rzeczywistych warunkach mogłoby to reprezentować nadmierne skomplikowanie naszej analizy i wyciąganie fałszywych wniosków z losowych fluktuacji w naszych danych.\n\n\nPodsumowanie\nZrozumienie kompromisu między obciążeniem a wariancją jest kluczowe w modelowaniu statystycznym:\n\nNiskie Obciążenie, Niska Wariancja: Idealny scenariusz, w którym nasz model dokładnie uchwycił podstawową zależność bez nadmiernej wrażliwości na szum.\nNiskie Obciążenie, Wysoka Wariancja: Nasz model jest średnio poprawny, ale niewiarygodny ze względu na wysoką wrażliwość na pojedyncze punkty danych.\nWysokie Obciążenie, Niska Wariancja: Nasz model jest konsekwentnie błędny z powodu nadmiernego uproszczenia, ale daje stabilne przewidywania.\nWysokie Obciążenie, Wysoka Wariancja: Najgorszy scenariusz, w którym nasz model jest zarówno niedokładny, jak i niewiarygodny.\n\nW praktyce często musimy zrównoważyć obciążenie i wariancję. Techniki takie jak walidacja krzyżowa, regularyzacja i metody zespołowe mogą pomóc w znalezieniu tej równowagi.\nPamiętaj: - Model z wysokim obciążeniem jest zbyt prosty i pomija ważne wzorce w danych. - Model z wysoką wariancją jest zbyt złożony i dopasowuje się do szumu w danych treningowych. - Celem jest znalezienie złotego środka, który uchwyci prawdziwe wzorce bez nadmiernego dopasowania do szumu.\nZrozumienie tych pojęć pomoże ci lepiej wybierać odpowiednie modele, unikać przeuczenia i niedouczenia oraz budować bardziej efektywne modele predykcyjne w przyszłych analizach statystycznych!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Statystycznych</span>"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "7  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "",
    "text": "7.1 Introduction\nResearch designs are fundamental to the scientific process, providing structured approaches to investigate hypotheses and answer research questions. This chapter explores two main categories of research designs: experimental and non-experimental, with a focus on the Neyman-Rubin potential outcome framework. We’ll delve into various design types, their characteristics, and provide practical examples using R for data analysis and visualization.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#experimental-designs",
    "href": "chapter4.html#experimental-designs",
    "title": "7  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "7.2 Experimental Designs",
    "text": "7.2 Experimental Designs\nExperimental designs are characterized by the researcher’s control over the independent variable(s) and random assignment of subjects to different conditions. These designs are considered the gold standard for establishing causal relationships.\n\nRandomized Controlled Trials (RCTs)\nRCTs are the most rigorous form of experimental design. They involve:\n\nRandom assignment of subjects to treatment and control groups\nManipulation of the independent variable\nMeasurement of the dependent variable\n\nLet’s visualize a simple RCT design:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n# Create sample data\nn &lt;- 100\ndata &lt;- data.frame(\n  id = 1:n,\n  group = factor(rep(c(\"Control\", \"Treatment\"), each = n/2)),\n  pre_test = rnorm(n, mean = 50, sd = 10),\n  post_test = NA\n)\n\n# Simulate treatment effect\ndata$post_test &lt;- ifelse(data$group == \"Treatment\",\n                         data$pre_test + rnorm(n/2, mean = 10, sd = 5),\n                         data$pre_test + rnorm(n/2, mean = 0, sd = 5))\n\n# Reshape data for plotting\ndata_long &lt;- tidyr::pivot_longer(data, cols = c(pre_test, post_test),\n                                 names_to = \"time\", values_to = \"score\")\n\n# Create plot\nggplot(data_long, aes(x = time, y = score, color = group, group = interaction(id, group))) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.5) +\n  stat_summary(aes(group = group), fun = mean, geom = \"line\", size = 1.5) +\n  labs(title = \"Pre-test and Post-test Scores in RCT\",\n       x = \"Time\", y = \"Score\", color = \"Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nRandomized Controlled Trial Design\n\n\n\n\nThis plot shows individual trajectories and group means for pre-test and post-test scores in a hypothetical RCT. The treatment group shows a clear increase in scores compared to the control group.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#ab-testing-an-example-and-comparison-with-rcts",
    "href": "chapter4.html#ab-testing-an-example-and-comparison-with-rcts",
    "title": "7  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "7.3 A/B Testing: An Example and Comparison with RCTs",
    "text": "7.3 A/B Testing: An Example and Comparison with RCTs\nA/B testing is a widely used experimental method in digital marketing, user experience design, and product development. This chapter will present an example of A/B testing, explain its methodology, and discuss how it differs from Randomized Controlled Trials (RCTs).\n\nExample: Website Landing Page Conversion Rate\nLet’s consider an example where an e-commerce company wants to improve the conversion rate of their landing page. They decide to test two different layouts: the current layout (A) and a new layout (B).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(1234)\n\n# Simulate data\nn_visitors &lt;- 10000\ndata &lt;- data.frame(\n  Version = sample(c(\"A\", \"B\"), n_visitors, replace = TRUE),\n  Converted = rbinom(n_visitors, 1, ifelse(sample(c(\"A\", \"B\"), n_visitors, replace = TRUE) == \"A\", 0.10, 0.12))\n)\n\n# Calculate conversion rates\nconversion_rates &lt;- data %&gt;%\n  group_by(Version) %&gt;%\n  summarise(\n    Visitors = n(),\n    Conversions = sum(Converted),\n    ConversionRate = mean(Converted)\n  )\n\n# Visualize results\nggplot(conversion_rates, aes(x = Version, y = ConversionRate, fill = Version)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", ConversionRate * 100)), \n            vjust = -0.5, size = 4) +\n  theme_minimal() +\n  labs(title = \"A/B Test: Landing Page Conversion Rates\",\n       x = \"Page Version\", y = \"Conversion Rate\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure 7.1: A/B Test Results: Landing Page Conversion Rates\n\n\n\n\n\nIn this example, we simulated data for 10,000 visitors randomly assigned to either version A or B of the landing page. The results show that version B has a slightly higher conversion rate (11.44%) compared to version A (10.94%).\n\n\nA/B Testing Methodology\nA/B testing typically follows these steps:\n\nIdentify the element to be tested (e.g., landing page layout).\nCreate two versions: the control (A) and the variant (B).\nRandomly assign visitors to either version.\nCollect data on the metric of interest (e.g., conversion rate).\nAnalyze the results using statistical methods.\nMake a decision based on the results.\n\n\n\nDifferences between A/B Testing and RCTs\nWhile A/B testing and Randomized Controlled Trials (RCTs) share some similarities, they have several key differences:\n\nScope and Context:\n\nA/B Testing: Typically used in digital environments for quick, iterative improvements.\nRCTs: Used in various fields, including medicine, psychology, and social sciences, often for more complex interventions.\n\nDuration:\n\nA/B Testing: Usually shorter, often running for days or weeks.\nRCTs: Can last months or years, especially in medical research.\n\nSample Size:\n\nA/B Testing: Can involve very large sample sizes due to ease of implementation in digital platforms.\nRCTs: Sample sizes are often smaller due to practical and cost constraints.\n\nBlinding:\n\nA/B Testing: Participants are usually unaware they’re part of a test.\nRCTs: May involve single, double, or triple blinding to reduce bias.\n\nEthical Considerations:\n\nA/B Testing: Generally involves low-risk changes with minimal ethical concerns.\nRCTs: Often require extensive ethical review, especially in medical contexts.\n\nOutcome Measures:\n\nA/B Testing: Typically focuses on a single, easily measurable outcome (e.g., click-through rate).\nRCTs: Often measure multiple outcomes, including potential side effects or long-term impacts.\n\nGeneralizability:\n\nA/B Testing: Results are often specific to the platform or context tested.\nRCTs: Aim for broader generalizability, though this can vary.\n\nAnalysis Complexity:\n\nA/B Testing: Often uses simpler statistical analyses.\nRCTs: May involve more complex statistical methods to account for various factors.\n\n\nA/B testing is a powerful tool for making data-driven decisions in digital environments. While it shares the fundamental principle of randomization with RCTs, it is typically simpler, faster, and more focused on specific, measurable outcomes in digital contexts. Understanding these differences helps researchers and practitioners choose the most appropriate method for their specific needs and constraints.\n\n\nExample 1: Effect of Sleep Duration on Cognitive Performance\nResearch Question: Does increasing sleep duration improve cognitive performance in college students?\n\n# Generating sample data\nset.seed(456)\nn &lt;- 100\npre_experimental &lt;- rnorm(n, mean = 70, sd = 10)\npost_experimental &lt;- pre_experimental + rnorm(n, mean = 8, sd = 5)\npre_control &lt;- rnorm(n, mean = 70, sd = 10)\npost_control &lt;- pre_control + rnorm(n, mean = 1, sd = 5)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"Experimental\", \"Control\"), each = n*2),\n  Time = rep(rep(c(\"Pre\", \"Post\"), each = n), 2),\n  Score = c(pre_experimental, post_experimental, pre_control, post_control)\n)\n\n# Creating the plot\nggplot(data, aes(x = Time, y = Score, color = Group, group = Group)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Effect of Increased Sleep Duration on Cognitive Performance\") +\n  xlab(\"Time\") +\n  ylab(\"Cognitive Performance Score\")\n\n\n\n\n\n\n\nFigure 7.2: Effect of Sleep Duration on Cognitive Performance\n\n\n\n\n\n\nInterpretation\nThis plot demonstrates the effect of increased sleep duration on cognitive performance. The experimental group, which increased their sleep duration, shows a more substantial improvement in cognitive performance compared to the control group. This suggests that increasing sleep duration may positively impact cognitive abilities in college students.\n\n\n\nExample 2: Impact of Mindfulness Training on Stress Levels\nResearch Question: Can a short-term mindfulness training program reduce stress levels in healthcare workers?\n\n# Generating sample data\nset.seed(789)\nn &lt;- 120\npre_experimental &lt;- rnorm(n, mean = 60, sd = 15)\npost_experimental &lt;- pre_experimental + rnorm(n, mean = -12, sd = 8)\npre_control &lt;- rnorm(n, mean = 60, sd = 15)\npost_control &lt;- pre_control + rnorm(n, mean = -2, sd = 6)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"Mindfulness\", \"Control\"), each = n*2),\n  Time = rep(rep(c(\"Pre\", \"Post\"), each = n), 2),\n  StressScore = c(pre_experimental, post_experimental, pre_control, post_control)\n)\n\n# Creating the plot\nggplot(data, aes(x = Time, y = StressScore, color = Group, group = Group)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Impact of Mindfulness Training on Stress Levels\") +\n  xlab(\"Time\") +\n  ylab(\"Stress Score\")\n\n\n\n\n\n\n\nFigure 7.3: Impact of Mindfulness Training on Stress Levels\n\n\n\n\n\n\nInterpretation\nThis visualization illustrates the impact of a mindfulness training program on stress levels in healthcare workers. The mindfulness group shows a more significant decrease in stress scores compared to the control group. This suggests that the mindfulness training program may be effective in reducing stress levels among healthcare workers.\nWhen interpreting such results, it’s important to consider:\n\nThe magnitude of the change in each group\nThe difference in change between the experimental and control groups\nThe variability within each group\nAny potential confounding factors not accounted for in the experimental design\n\nThese examples provide a template for visualizing and interpreting similar experimental designs across different research contexts.\n\n\n\nFactorial Designs\nFactorial designs allow researchers to study the effects of multiple independent variables simultaneously. They are efficient and can reveal interaction effects between variables.\nExample of a 2x2 factorial design:\n\n# Create sample data for 2x2 factorial design\nset.seed(456)\nn_per_group &lt;- 25\n\nfactorial_data &lt;- data.frame(\n  factor_a = rep(rep(c(\"Low\", \"High\"), each = n_per_group), 2),\n  factor_b = rep(c(\"Control\", \"Treatment\"), each = n_per_group * 2),\n  outcome = NA\n)\n\n# Generate outcomes\nfactorial_data$outcome &lt;- ifelse(factorial_data$factor_a == \"Low\" & factorial_data$factor_b == \"Control\",\n                                 rnorm(n_per_group, 40, 5),\n                                 ifelse(factorial_data$factor_a == \"Low\" & factorial_data$factor_b == \"Treatment\",\n                                        rnorm(n_per_group, 45, 5),\n                                        ifelse(factorial_data$factor_a == \"High\" & factorial_data$factor_b == \"Control\",\n                                               rnorm(n_per_group, 50, 5),\n                                               rnorm(n_per_group, 60, 5))))\n\n# Create plot\nggplot(factorial_data, aes(x = factor_b, y = outcome, fill = factor_a)) +\n  geom_boxplot() +\n  facet_wrap(~factor_a, scales = \"free_x\") +\n  labs(title = \"2x2 Factorial Design\",\n       x = \"Factor B\", y = \"Outcome\", fill = \"Factor A\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n2x2 Factorial Design\n\n\n\n\nThis plot illustrates a 2x2 factorial design, showing the effects of two factors (A and B) on the outcome variable. We can observe main effects for both factors and a potential interaction effect.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#non-experimental-designs",
    "href": "chapter4.html#non-experimental-designs",
    "title": "7  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "7.4 Non-Experimental Designs",
    "text": "7.4 Non-Experimental Designs\nNon-experimental designs are used when randomization or manipulation of variables is not possible or ethical. They include observational/descriptive studies and quasi-experimental designs.\n\nObservational Studies\nObservational studies involve collecting data without manipulating variables. They are useful for exploring relationships and generating hypotheses.\nExample: Correlation study\n\nset.seed(789)\nn &lt;- 100\nstudy_time &lt;- runif(n, 0, 10)\nexam_score &lt;- 50 + 5 * study_time + rnorm(n, 0, 10)\n\ncorrelation_data &lt;- data.frame(study_time, exam_score)\n\nggplot(correlation_data, aes(x = study_time, y = exam_score)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Correlation between Study Time and Exam Score\",\n       x = \"Study Time (hours)\", y = \"Exam Score\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCorrelation between Study Time and Exam Score\n\n\n\n\nThis scatter plot shows the relationship between study time and exam scores, illustrating a positive correlation typical in observational studies.\n\n\nQuasi-Experimental Designs\nQuasi-experimental designs lack random assignment but attempt to establish causal relationships. Common types include:\n\nDifference-in-Differences (DiD)\nRegression Discontinuity Design (RDD)\n\n\nDifference-in-Differences (DiD)\nDiD is used to estimate treatment effects by comparing the average change over time in the outcome variable for the treatment group to the average change over time for the control group.\nLet’s simulate a DiD analysis using the plm package:\n\nlibrary(plm)\n\nWarning: package 'plm' was built under R version 4.4.3\n\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(101)\n\n# Generate synthetic panel data\nn &lt;- 1000\ntime_periods &lt;- 5\nintervention_time &lt;- 3\n\npanel_data &lt;- data.frame(\n  id = rep(1:n, each = time_periods),\n  time = rep(1:time_periods, times = n),\n  treatment = rep(sample(c(0, 1), n, replace = TRUE), each = time_periods)\n)\n\n# Generate outcomes\npanel_data$outcome &lt;- with(panel_data,\n                           10 + 2 * time + 5 * treatment + \n                           3 * (time &gt;= intervention_time & treatment == 1) + \n                           rnorm(n * time_periods, 0, 2))\n\n# Create post-treatment indicator\npanel_data$post &lt;- as.integer(panel_data$time &gt;= intervention_time)\n\n# Estimate DiD model\ndid_model &lt;- plm(outcome ~ treatment * post, \n                 data = panel_data, \n                 index = c(\"id\", \"time\"), \n                 model = \"within\")\n\n# Summarize results\nsummary_did &lt;- summary(did_model)\n\n# Calculate group means for each time period\ngroup_means &lt;- aggregate(outcome ~ time + treatment, data = panel_data, FUN = mean)\n\n# Visualize DiD\nggplot(group_means, aes(x = time, y = outcome, color = factor(treatment), group = treatment)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = intervention_time, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Difference-in-Differences Analysis\",\n       subtitle = paste(\"Estimated treatment effect:\", round(coef(did_model)[\"treatment:post\"], 3)),\n       x = \"Time\", y = \"Outcome\", color = \"Treatment Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Control\", \"Treatment\")) +\n  scale_x_continuous(breaks = 1:time_periods)\n\n\n\n\nDifference-in-Differences Analysis\n\n\n\n# Print model summary\nprint(summary_did)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = outcome ~ treatment * post, data = panel_data, \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 1000, T = 5, N = 5000\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-7.509908 -1.625814  0.001753  1.610009  8.047479 \n\nCoefficients:\n               Estimate Std. Error t-value  Pr(&gt;|t|)    \npost            5.05692    0.10315  49.026 &lt; 2.2e-16 ***\ntreatment:post  2.89003    0.14935  19.351 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    78894\nResidual Sum of Squares: 26696\nR-Squared:      0.66163\nAdj. R-Squared: 0.57691\nF-statistic: 3908.68 on 2 and 3998 DF, p-value: &lt; 2.22e-16\n\n\nThe plot shows the average outcomes for treatment and control groups over time. The vertical dashed line indicates the intervention point. The DiD estimate is the difference between the two groups’ changes from pre- to post-intervention periods.\nDiD Model:\n\nThe model outcome ~ treatment * post estimates:\nThe average treatment effect on the treated (ATT) after the intervention\nThe coefficient on treatment:post represents this effect\n\nInterpretation of Results: Looking at the model summary:\n\nThe coefficient for treatment:post is the DiD estimator. It represents the average treatment effect on the treated after the intervention.\nIf this coefficient is statistically significant, it suggests that the treatment had a causal effect on the outcome.\nThe magnitude of this coefficient tells us the size of the treatment effect.\n\nVisualization: The plot shows:\n\nSeparate trend lines for the treatment and control groups\nA vertical dashed line indicating the intervention time\nThe parallel trends assumption can be visually assessed by looking at the pre-intervention period\nThe divergence of the lines after the intervention represents the treatment effect\n\nAssumptions and Limitations:\nIt’s important to note some key assumptions of DiD:\n\nParallel trends: In the absence of treatment, the difference between the treatment and control groups would remain constant over time.\nNo spillover effects: The treatment does not affect the control group.\nNo compositional changes: The composition of treatment and control groups remains stable over time.\n\n\n\nRegression Discontinuity Design (RDD)\nRDD is used when treatment assignment is determined by a cutoff value on a continuous variable. It compares observations just above and below the cutoff to estimate the treatment effect.\nLet’s implement an RDD analysis using the rdrobust package:\n\nlibrary(rdrobust)\n\nWarning: package 'rdrobust' was built under R version 4.4.3\n\n# Generate synthetic RDD data\nset.seed(202)\nn &lt;- 1000\nx &lt;- runif(n, -1, 1)\ny &lt;- 3 + 2 * x + 4 * (x &gt;= 0) + rnorm(n, 0, 1)\n\nrdd_data &lt;- data.frame(x, y)\n\n# RDD analysis\nrdd_result &lt;- rdrobust(y, x, c = 0)\nsummary(rdd_result)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  499          501\nEff. Number of Obs.             182          175\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.362        0.362\nBW bias (b)                   0.575        0.575\nrho (h/b)                     0.630        0.630\nUnique Obs.                     499          501\n\n=====================================================================\n                   Point    Robust Inference\n                Estimate         z     P&gt;|z|      [ 95% C.I. ]       \n---------------------------------------------------------------------\n     RD Effect     4.092    15.013     0.000     [3.600 , 4.680]     \n=====================================================================\n\n# Visualize RDD\nggplot(rdd_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(data = subset(rdd_data, x &lt; 0), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(data = subset(rdd_data, x &gt;= 0), method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Regression Discontinuity Design\",\n       x = \"Running Variable\", y = \"Outcome\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nRegression Discontinuity Design Analysis\n\n\n\n\nThe plot shows the discontinuity at the cutoff point (x = 0), with separate regression lines fitted on either side. The treatment effect is estimated by the gap between these lines at the cutoff.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#the-neyman-rubin-potential-outcome-framework",
    "href": "chapter4.html#the-neyman-rubin-potential-outcome-framework",
    "title": "7  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "7.5 The Neyman-Rubin Potential Outcome Framework",
    "text": "7.5 The Neyman-Rubin Potential Outcome Framework\nThe Neyman-Rubin potential outcome framework provides a formal approach to causal inference. It introduces the concept of potential outcomes: for each unit, we consider the outcome under treatment and the outcome under control, even though we can only observe one in reality.\nKey concepts:\n\nPotential Outcomes: Y_i(1) and Y_i(0) for treatment and control, respectively.\nObserved Outcome: Y_i = Y_i(1)T_i + Y_i(0)(1-T_i), where T_i is the treatment indicator.\nIndividual Treatment Effect: \\tau_i = Y_i(1) - Y_i(0)\nAverage Treatment Effect (ATE): E[\\tau_i] = E[Y_i(1) - Y_i(0)]\n\nThe framework emphasizes the “fundamental problem of causal inference”: we can never observe both potential outcomes for a single unit simultaneously.\n\nExample: Estimating ATE in an RCT\nIn an RCT, random assignment ensures that treatment is independent of potential outcomes, allowing unbiased estimation of the ATE:\n\n\\hat{ATE} = \\frac{1}{n_1} \\sum_{i:T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i:T_i=0} Y_i\n\nWhere n_1 and n_0 are the numbers of treated and control units, respectively.\n\n# Using the RCT data from earlier\nate_estimate &lt;- mean(data$post_test[data$group == \"Treatment\"]) - \n                mean(data$post_test[data$group == \"Control\"])\n\nWarning in mean.default(data$post_test[data$group == \"Treatment\"]): argument is\nnot numeric or logical: returning NA\n\n\nWarning in mean.default(data$post_test[data$group == \"Control\"]): argument is\nnot numeric or logical: returning NA\n\ncat(\"Estimated Average Treatment Effect:\", round(ate_estimate, 2))\n\nEstimated Average Treatment Effect: NA\n\n\nThis estimate represents the causal effect of the treatment under the assumptions of the potential outcome framework.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#conclusion",
    "href": "chapter4.html#conclusion",
    "title": "7  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "7.6 Conclusion",
    "text": "7.6 Conclusion\nThis chapter has explored various research designs, from experimental approaches like RCTs and factorial designs to non-experimental methods such as observational studies and quasi-experimental designs. We’ve demonstrated how to implement and visualize these designs using R, and introduced the Neyman-Rubin potential outcome framework for causal inference.\nUnderstanding these designs and their appropriate use is crucial for conducting rigorous research and drawing valid causal conclusions. Each design has its strengths and limitations, and the choice of design should be guided by the research question, ethical considerations, and practical constraints.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#references",
    "href": "chapter4.html#references",
    "title": "7  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "7.7 References",
    "text": "7.7 References\n\nImbens, G. W., & Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge University Press.\nAngrist, J. D., & Pischke, J. S. (2008). Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton University Press.\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and Quasi-Experimental Designs for Generalized Causal Inference. Houghton Mifflin.\nCunningham, S. (2021). Causal Inference: The Mixtape. Yale University Press.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html",
    "href": "rozdzial4.html",
    "title": "8  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "",
    "text": "8.1 Wstęp\nProjekty badawcze stanowią fundament procesu naukowego, zapewniając ustrukturyzowane podejście do badania hipotez i odpowiadania na pytania badawcze. Ten rozdział analizuje dwie główne kategorie projektów badawczych: eksperymentalne i nieeksperymentalne, ze szczególnym uwzględnieniem modelu potencjalnych wyników Neymana-Rubina. Zagłębimy się w różne typy projektów, ich charakterystykę i przedstawimy praktyczne przykłady wykorzystania R do analizy danych i wizualizacji.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#projekty-eksperymentalne",
    "href": "rozdzial4.html#projekty-eksperymentalne",
    "title": "8  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "8.2 Projekty Eksperymentalne",
    "text": "8.2 Projekty Eksperymentalne\nProjekty eksperymentalne charakteryzują się kontrolą badacza nad zmienną(ymi) niezależną(ymi) oraz losowym przydziałem uczestników do różnych warunków. Te projekty są uważane za złoty standard w ustalaniu związków przyczynowych.\n\nRandomizowane Badania Kontrolowane (RCT)\nRCT są najbardziej rygorystyczną formą projektu eksperymentalnego. Obejmują one:\n\nLosowy przydział uczestników do grup eksperymentalnej i kontrolnej\nManipulację zmienną niezależną\nPomiar zmiennej zależnej\n\nZobaczmy wizualizację prostego projektu RCT:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n# Tworzenie przykładowych danych\nn &lt;- 100\ndata &lt;- data.frame(\n  id = 1:n,\n  grupa = factor(rep(c(\"Kontrolna\", \"Eksperymentalna\"), each = n/2)),\n  pre_test = rnorm(n, mean = 50, sd = 10),\n  post_test = NA\n)\n\n# Symulacja efektu leczenia\ndata$post_test &lt;- ifelse(data$grupa == \"Eksperymentalna\",\n                         data$pre_test + rnorm(n/2, mean = 10, sd = 5),\n                         data$pre_test + rnorm(n/2, mean = 0, sd = 5))\n\n# Przekształcenie danych do formatu długiego\ndata_long &lt;- tidyr::pivot_longer(data, cols = c(pre_test, post_test),\n                                 names_to = \"czas\", values_to = \"wynik\")\n\n# Tworzenie wykresu\nggplot(data_long, aes(x = czas, y = wynik, color = grupa, group = interaction(id, grupa))) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.5) +\n  stat_summary(aes(group = grupa), fun = mean, geom = \"line\", size = 1.5) +\n  labs(title = \"Wyniki Pre-test i Post-test w RCT\",\n       x = \"Czas\", y = \"Wynik\", color = \"Grupa\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_x_discrete(labels = c(\"pre_test\" = \"Pre-test\", \"post_test\" = \"Post-test\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nProjekt Randomizowanego Badania Kontrolowanego\n\n\n\n\nTen wykres pokazuje indywidualne trajektorie i średnie grupowe dla wyników pre-test i post-test w hipotetycznym RCT. Grupa eksperymentalna wykazuje wyraźny wzrost wyników w porównaniu do grupy kontrolnej.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#testy-ab-przykład-i-porównanie-z-rct",
    "href": "rozdzial4.html#testy-ab-przykład-i-porównanie-z-rct",
    "title": "8  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "8.3 Testy A/B: Przykład i Porównanie z RCT",
    "text": "8.3 Testy A/B: Przykład i Porównanie z RCT\nTesty A/B to szeroko stosowana metoda eksperymentalna w marketingu cyfrowym, projektowaniu doświadczeń użytkownika i rozwoju produktów. Ten rozdział przedstawi przykład testu A/B, wyjaśni jego metodologię i omówi, czym różni się od Randomizowanych Badań Kontrolowanych (RCT).\n\nPrzykład: Współczynnik Konwersji Strony Docelowej\nRozważmy przykład, w którym firma e-commerce chce poprawić współczynnik konwersji swojej strony docelowej. Decydują się przetestować dwa różne układy: obecny układ (A) i nowy układ (B).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(1234)\n\n# Symulacja danych\nn_odwiedzajacych &lt;- 10000\ndane &lt;- data.frame(\n  Wersja = sample(c(\"A\", \"B\"), n_odwiedzajacych, replace = TRUE),\n  Konwersja = rbinom(n_odwiedzajacych, 1, ifelse(sample(c(\"A\", \"B\"), n_odwiedzajacych, replace = TRUE) == \"A\", 0.10, 0.12))\n)\n\n# Obliczenie współczynników konwersji\nwspolczynniki_konwersji &lt;- dane %&gt;%\n  group_by(Wersja) %&gt;%\n  summarise(\n    Odwiedzajacy = n(),\n    Konwersje = sum(Konwersja),\n    WspolczynnikKonwersji = mean(Konwersja)\n  )\n\n# Wizualizacja wyników\nggplot(wspolczynniki_konwersji, aes(x = Wersja, y = WspolczynnikKonwersji, fill = Wersja)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", WspolczynnikKonwersji * 100)), \n            vjust = -0.5, size = 4) +\n  theme_minimal() +\n  labs(title = \"Test A/B: Współczynniki Konwersji Strony Docelowej\",\n       x = \"Wersja Strony\", y = \"Współczynnik Konwersji\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure 8.1: Wyniki Testu A/B: Współczynniki Konwersji Strony Docelowej\n\n\n\n\n\nW tym przykładzie zasymulowaliśmy dane dla 10 000 odwiedzających losowo przypisanych do wersji A lub B strony docelowej. Wyniki pokazują, że wersja B ma nieco wyższy współczynnik konwersji (11,44%) w porównaniu do wersji A (10,94%).\n\n\nMetodologia Testów A/B\nTesty A/B zazwyczaj przebiegają według następujących kroków:\n\nZidentyfikowanie elementu do przetestowania (np. układ strony docelowej).\nStworzenie dwóch wersji: kontrolnej (A) i wariantu (B).\nLosowe przypisanie odwiedzających do jednej z wersji.\nZbieranie danych o interesującej nas metryce (np. współczynniku konwersji).\nAnaliza wyników przy użyciu metod statystycznych.\nPodjęcie decyzji na podstawie wyników.\n\n\n\nRóżnice między Testami A/B a RCT\nChoć testy A/B i Randomizowane Badania Kontrolowane (RCT) mają pewne podobieństwa, istnieje kilka kluczowych różnic:\n\nZakres i Kontekst:\n\nTesty A/B: Zazwyczaj stosowane w środowiskach cyfrowych do szybkich, iteracyjnych ulepszeń.\nRCT: Stosowane w różnych dziedzinach, w tym medycynie, psychologii i naukach społecznych, często dla bardziej złożonych interwencji.\n\nCzas Trwania:\n\nTesty A/B: Zwykle krótsze, często trwające dni lub tygodnie.\nRCT: Mogą trwać miesiące lub lata, szczególnie w badaniach medycznych.\n\nWielkość Próby:\n\nTesty A/B: Mogą obejmować bardzo duże próby ze względu na łatwość implementacji na platformach cyfrowych.\nRCT: Wielkości prób są często mniejsze ze względu na praktyczne i kosztowe ograniczenia.\n\nZaślepienie:\n\nTesty A/B: Uczestnicy zazwyczaj nie są świadomi, że biorą udział w teście.\nRCT: Mogą obejmować pojedyncze, podwójne lub potrójne zaślepienie w celu zmniejszenia błędu systematycznego.\n\nWzględy Etyczne:\n\nTesty A/B: Generalnie obejmują zmiany niskiego ryzyka z minimalnymi obawami etycznymi.\nRCT: Często wymagają obszernej oceny etycznej, szczególnie w kontekście medycznym.\n\nMiary Wyników:\n\nTesty A/B: Zazwyczaj skupiają się na pojedynczym, łatwo mierzalnym wyniku (np. współczynnik klikalności).\nRCT: Często mierzą wiele wyników, w tym potencjalne skutki uboczne lub długoterminowe efekty.\n\nMożliwość Uogólnienia:\n\nTesty A/B: Wyniki są często specyficzne dla testowanej platformy lub kontekstu.\nRCT: Dążą do szerszej możliwości uogólnienia, choć może to się różnić.\n\nZłożoność Analizy:\n\nTesty A/B: Często wykorzystują prostsze analizy statystyczne.\nRCT: Mogą obejmować bardziej złożone metody statystyczne, aby uwzględnić różne czynniki.\n\n\nTesty A/B są potężnym narzędziem do podejmowania decyzji opartych na danych w środowiskach cyfrowych. Choć dzielą podstawową zasadę randomizacji z RCT, są zazwyczaj prostsze, szybsze i bardziej skoncentrowane na konkretnych, mierzalnych wynikach w kontekstach cyfrowych. Zrozumienie tych różnic pomaga badaczom i praktykom wybrać najbardziej odpowiednią metodę do ich konkretnych potrzeb i ograniczeń.\nTesty A/B są szczególnie przydatne w optymalizacji stron internetowych, aplikacji mobilnych i kampanii marketingowych, gdzie szybkie iteracje i ciągłe ulepszenia są kluczowe. Z kolei RCT pozostają złotym standardem w badaniach naukowych, szczególnie w dziedzinach takich jak medycyna, gdzie rygorystyczna kontrola i długoterminowa obserwacja są niezbędne.\nNiezależnie od wybranej metody, kluczowe jest staranne planowanie, precyzyjne wykonanie i ostrożna interpretacja wyników. Zarówno testy A/B, jak i RCT, gdy są odpowiednio stosowane, mogą dostarczyć cennych informacji i przyczynić się do podejmowania lepszych decyzji opartych na danych.\n\n\nPrzykład 1: Wpływ Długości Snu na Wydajność Poznawczą\nPytanie Badawcze: Czy zwiększenie długości snu poprawia wydajność poznawczą u studentów?\n\n# Generowanie przykładowych danych\nset.seed(456)\nn &lt;- 100\npre_eksperymentalna &lt;- rnorm(n, mean = 70, sd = 10)\npost_eksperymentalna &lt;- pre_eksperymentalna + rnorm(n, mean = 8, sd = 5)\npre_kontrolna &lt;- rnorm(n, mean = 70, sd = 10)\npost_kontrolna &lt;- pre_kontrolna + rnorm(n, mean = 1, sd = 5)\n\ndane &lt;- data.frame(\n  Grupa = rep(c(\"Eksperymentalna\", \"Kontrolna\"), each = n*2),\n  Czas = rep(rep(c(\"Przed\", \"Po\"), each = n), 2),\n  Wynik = c(pre_eksperymentalna, post_eksperymentalna, pre_kontrolna, post_kontrolna)\n)\n\n# Tworzenie wykresu\nggplot(dane, aes(x = Czas, y = Wynik, color = Grupa, group = Grupa)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Wpływ Zwiększonej Długości Snu na Wydajność Poznawczą\") +\n  xlab(\"Czas\") +\n  ylab(\"Wynik Wydajności Poznawczej\")\n\n\n\n\n\n\n\nFigure 8.2: Wpływ Długości Snu na Wydajność Poznawczą\n\n\n\n\n\n\nInterpretacja\nTen wykres pokazuje wpływ zwiększonej długości snu na wydajność poznawczą. Grupa eksperymentalna, która zwiększyła długość snu, wykazuje znacznie większą poprawę w wydajności poznawczej w porównaniu do grupy kontrolnej. Sugeruje to, że zwiększenie długości snu może pozytywnie wpływać na zdolności poznawcze studentów.\n\n\n\nPrzykład 2: Wpływ Treningu Uważności na Poziom Stresu\nPytanie Badawcze: Czy krótkoterminowy program treningu uważności może obniżyć poziom stresu u pracowników służby zdrowia?\n\n# Generowanie przykładowych danych\nset.seed(789)\nn &lt;- 120\npre_eksperymentalna &lt;- rnorm(n, mean = 60, sd = 15)\npost_eksperymentalna &lt;- pre_eksperymentalna + rnorm(n, mean = -12, sd = 8)\npre_kontrolna &lt;- rnorm(n, mean = 60, sd = 15)\npost_kontrolna &lt;- pre_kontrolna + rnorm(n, mean = -2, sd = 6)\n\ndane &lt;- data.frame(\n  Grupa = rep(c(\"Uważność\", \"Kontrolna\"), each = n*2),\n  Czas = rep(rep(c(\"Przed\", \"Po\"), each = n), 2),\n  PoziomStresu = c(pre_eksperymentalna, post_eksperymentalna, pre_kontrolna, post_kontrolna)\n)\n\n# Tworzenie wykresu\nggplot(dane, aes(x = Czas, y = PoziomStresu, color = Grupa, group = Grupa)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Wpływ Treningu Uważności na Poziom Stresu\") +\n  xlab(\"Czas\") +\n  ylab(\"Poziom Stresu\")\n\n\n\n\n\n\n\nFigure 8.3: Wpływ Treningu Uważności na Poziom Stresu\n\n\n\n\n\n\nInterpretacja\nTa wizualizacja ilustruje wpływ programu treningu uważności na poziom stresu u pracowników służby zdrowia. Grupa uważności wykazuje znacznie większy spadek poziomu stresu w porównaniu do grupy kontrolnej. Sugeruje to, że program treningu uważności może być skuteczny w redukcji poziomu stresu wśród pracowników służby zdrowia.\n\n\n\nProjekty Czynnikowe\nProjekty czynnikowe pozwalają badaczom na jednoczesne badanie efektów wielu zmiennych niezależnych. Są one efektywne i mogą ujawniać efekty interakcji między zmiennymi.\nPrzykład projektu czynnikowego 2x2:\n\n# Tworzenie przykładowych danych dla projektu czynnikowego 2x2\nset.seed(456)\nn_per_group &lt;- 25\n\nfactorial_data &lt;- data.frame(\n  czynnik_a = rep(rep(c(\"Niski\", \"Wysoki\"), each = n_per_group), 2),\n  czynnik_b = rep(c(\"Kontrola\", \"Interwencja\"), each = n_per_group * 2),\n  wynik = NA\n)\n\n# Generowanie wyników\nfactorial_data$wynik &lt;- ifelse(factorial_data$czynnik_a == \"Niski\" & factorial_data$czynnik_b == \"Kontrola\",\n                               rnorm(n_per_group, 40, 5),\n                               ifelse(factorial_data$czynnik_a == \"Niski\" & factorial_data$czynnik_b == \"Interwencja\",\n                                      rnorm(n_per_group, 45, 5),\n                                      ifelse(factorial_data$czynnik_a == \"Wysoki\" & factorial_data$czynnik_b == \"Kontrola\",\n                                             rnorm(n_per_group, 50, 5),\n                                             rnorm(n_per_group, 60, 5))))\n\n# Tworzenie wykresu\nggplot(factorial_data, aes(x = czynnik_b, y = wynik, fill = czynnik_a)) +\n  geom_boxplot() +\n  facet_wrap(~czynnik_a, scales = \"free_x\") +\n  labs(title = \"Projekt Czynnikowy 2x2\",\n       x = \"Czynnik B\", y = \"Wynik\", fill = \"Czynnik A\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nProjekt Czynnikowy 2x2\n\n\n\n\nTen wykres ilustruje projekt czynnikowy 2x2, pokazując efekty dwóch czynników (A i B) na zmienną wynikową. Możemy zaobserwować główne efekty dla obu czynników oraz potencjalny efekt interakcji.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#projekty-nieeksperymentalne",
    "href": "rozdzial4.html#projekty-nieeksperymentalne",
    "title": "8  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "8.4 Projekty Nieeksperymentalne",
    "text": "8.4 Projekty Nieeksperymentalne\nProjekty nieeksperymentalne są stosowane, gdy randomizacja lub manipulacja zmiennymi nie jest możliwa lub etyczna. Obejmują one badania obserwacyjne/opisowe i quasi-eksperymentalne.\n\nBadania Obserwacyjne\nBadania obserwacyjne polegają na zbieraniu danych bez manipulowania zmiennymi. Są one przydatne do eksploracji relacji i generowania hipotez.\nPrzykład: Badanie korelacyjne\n\nset.seed(789)\nn &lt;- 100\nczas_nauki &lt;- runif(n, 0, 10)\nwynik_egzaminu &lt;- 50 + 5 * czas_nauki + rnorm(n, 0, 10)\n\ncorrelation_data &lt;- data.frame(czas_nauki, wynik_egzaminu)\n\nggplot(correlation_data, aes(x = czas_nauki, y = wynik_egzaminu)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Korelacja między Czasem Nauki a Wynikiem Egzaminu\",\n       x = \"Czas Nauki (godziny)\", y = \"Wynik Egzaminu\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nKorelacja między Czasem Nauki a Wynikiem Egzaminu\n\n\n\n\nTen wykres punktowy pokazuje relację między czasem nauki a wynikami egzaminu, ilustrując pozytywną korelację typową dla badań obserwacyjnych.\n\n\nProjekty Quasi-Eksperymentalne\nProjekty quasi-eksperymentalne nie mają losowego przydziału, ale próbują ustalić związki przyczynowe. Popularne typy to:\n\nRóżnica w Różnicach (DiD)\nRegresja Nieciągła (RDD)\n\n\nRóżnica w Różnicach (DiD)\nDiD jest używana do oszacowania efektów interwencji poprzez porównanie średniej zmiany w czasie w zmiennej wynikowej dla grupy eksperymentalnej ze średnią zmianą w czasie dla grupy kontrolnej.\nPrzeprowadźmy symulację analizy DiD przy użyciu pakietu plm:\n\nlibrary(plm)\n\nWarning: package 'plm' was built under R version 4.4.3\n\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(101)\n\n# Generate synthetic panel data\nn &lt;- 1000\ntime_periods &lt;- 5\nintervention_time &lt;- 3\n\npanel_data &lt;- data.frame(\n  id = rep(1:n, each = time_periods),\n  time = rep(1:time_periods, times = n),\n  treatment = rep(sample(c(0, 1), n, replace = TRUE), each = time_periods)\n)\n\n# Generate outcomes\npanel_data$outcome &lt;- with(panel_data,\n                           10 + 2 * time + 5 * treatment + \n                           3 * (time &gt;= intervention_time & treatment == 1) + \n                           rnorm(n * time_periods, 0, 2))\n\n# Create post-treatment indicator\npanel_data$post &lt;- as.integer(panel_data$time &gt;= intervention_time)\n\n# Estimate DiD model\ndid_model &lt;- plm(outcome ~ treatment * post, \n                 data = panel_data, \n                 index = c(\"id\", \"time\"), \n                 model = \"within\")\n\n# Summarize results\nsummary_did &lt;- summary(did_model)\n\n# Calculate group means for each time period\ngroup_means &lt;- aggregate(outcome ~ time + treatment, data = panel_data, FUN = mean)\n\n# Visualize DiD\nggplot(group_means, aes(x = time, y = outcome, color = factor(treatment), group = treatment)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = intervention_time, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Difference-in-Differences Analysis\",\n       subtitle = paste(\"Estimated treatment effect:\", round(coef(did_model)[\"treatment:post\"], 3)),\n       x = \"Time\", y = \"Outcome\", color = \"Treatment Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Control\", \"Treatment\")) +\n  scale_x_continuous(breaks = 1:time_periods)\n\n\n\n\n\n\n\n# Print model summary\nprint(summary_did)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = outcome ~ treatment * post, data = panel_data, \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 1000, T = 5, N = 5000\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-7.509908 -1.625814  0.001753  1.610009  8.047479 \n\nCoefficients:\n               Estimate Std. Error t-value  Pr(&gt;|t|)    \npost            5.05692    0.10315  49.026 &lt; 2.2e-16 ***\ntreatment:post  2.89003    0.14935  19.351 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    78894\nResidual Sum of Squares: 26696\nR-Squared:      0.66163\nAdj. R-Squared: 0.57691\nF-statistic: 3908.68 on 2 and 3998 DF, p-value: &lt; 2.22e-16\n\n\nWykres pokazuje średnie wyniki dla grup interwencji i kontrolnej w czasie. Pionowa przerywana linia wskazuje punkt interwencji. Oszacowanie DiD to różnica między zmianami obu grup od okresu przed do po interwencji.\nDiD Model:\n\nThe model outcome ~ treatment * post estimates:\nThe average treatment effect on the treated (ATT) after the intervention\nThe coefficient on treatment:post represents this effect\n\nInterpretation of Results: Looking at the model summary:\n\nThe coefficient for treatment:post is the DiD estimator. It represents the average treatment effect on the treated after the intervention.\nIf this coefficient is statistically significant, it suggests that the treatment had a causal effect on the outcome.\nThe magnitude of this coefficient tells us the size of the treatment effect.\n\nVisualization: The plot shows:\n\nSeparate trend lines for the treatment and control groups\nA vertical dashed line indicating the intervention time\nThe parallel trends assumption can be visually assessed by looking at the pre-intervention period\nThe divergence of the lines after the intervention represents the treatment effect\n\nAssumptions and Limitations:\nIt’s important to note some key assumptions of DiD:\n\nParallel trends: In the absence of treatment, the difference between the treatment and control groups would remain constant over time.\nNo spillover effects: The treatment does not affect the control group.\nNo compositional changes: The composition of treatment and control groups remains stable over time.\n\n\n\nRegresja Nieciągła (RDD)\nRDD jest stosowana, gdy przydział do interwencji jest określony przez wartość graniczną na ciągłej zmiennej. Porównuje obserwacje tuż powyżej i poniżej punktu granicznego, aby oszacować efekt interwencji.\nPrzeprowadźmy analizę RDD przy użyciu pakietu rdrobust:\n\nlibrary(rdrobust)\n\nWarning: package 'rdrobust' was built under R version 4.4.3\n\n# Generowanie syntetycznych danych RDD\nset.seed(202)\nn &lt;- 1000\nx &lt;- runif(n, -1, 1)\ny &lt;- 3 + 2 * x + 4 * (x &gt;= 0) + rnorm(n, 0, 1)\n\nrdd_data &lt;- data.frame(x, y)\n\n# Analiza RDD\nrdd_result &lt;- rdrobust(y, x, c = 0)\nsummary(rdd_result)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  499          501\nEff. Number of Obs.             182          175\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.362        0.362\nBW bias (b)                   0.575        0.575\nrho (h/b)                     0.630        0.630\nUnique Obs.                     499          501\n\n=====================================================================\n                   Point    Robust Inference\n                Estimate         z     P&gt;|z|      [ 95% C.I. ]       \n---------------------------------------------------------------------\n     RD Effect     4.092    15.013     0.000     [3.600 , 4.680]     \n=====================================================================\n\n# Wizualizacja RDD\nggplot(rdd_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(data = subset(rdd_data, x &lt; 0), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(data = subset(rdd_data, x &gt;= 0), method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Regresja Nieciągła\",\n       x = \"Zmienna Bieżąca\", y = \"Wynik\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAnaliza Regresji Nieciągłej\n\n\n\n\nWykres pokazuje nieciągłość w punkcie granicznym (x = 0), z oddzielnymi liniami regresji dopasowanymi po obu stronach. Efekt interwencji jest szacowany przez różnicę między tymi liniami w punkcie granicznym.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#model-potencjalnych-wyników-neymana-rubina",
    "href": "rozdzial4.html#model-potencjalnych-wyników-neymana-rubina",
    "title": "8  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "8.5 Model Potencjalnych Wyników Neymana-Rubina",
    "text": "8.5 Model Potencjalnych Wyników Neymana-Rubina\nModel potencjalnych wyników Neymana-Rubina zapewnia formalne podejście do wnioskowania przyczynowego. Wprowadza on koncepcję potencjalnych wyników: dla każdej jednostki rozważamy wynik w warunkach interwencji i w warunkach kontrolnych, mimo że w rzeczywistości możemy zaobserwować tylko jeden z nich.\nKluczowe pojęcia:\n\nPotencjalne Wyniki: Y_i(1) i Y_i(0) odpowiednio dla interwencji i kontroli.\nObserwowany Wynik: Y_i = Y_i(1)T_i + Y_i(0)(1-T_i), gdzie T_i to wskaźnik interwencji.\nIndywidualny Efekt Interwencji: \\tau_i = Y_i(1) - Y_i(0)\nPrzeciętny Efekt Interwencji (ATE): E[\\tau_i] = E[Y_i(1) - Y_i(0)]\n\nModel podkreśla “fundamentalny problem wnioskowania przyczynowego”: nigdy nie możemy zaobserwować obu potencjalnych wyników dla pojedynczej jednostki jednocześnie.\n\nPrzykład: Szacowanie ATE w RCT\nW RCT, losowy przydział zapewnia, że interwencja jest niezależna od potencjalnych wyników, umożliwiając nieobciążone oszacowanie ATE:\n\n\\hat{ATE} = \\frac{1}{n_1} \\sum_{i:T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i:T_i=0} Y_i\n\nGdzie n_1 i n_0 to odpowiednio liczby jednostek w grupie interwencji i kontrolnej.\n\n# Używając danych RCT z wcześniejszego przykładu\nate_estimate &lt;- mean(data$post_test[data$grupa == \"Eksperymentalna\"]) - \n                mean(data$post_test[data$grupa == \"Kontrolna\"])\n\ncat(\"Oszacowany Przeciętny Efekt Interwencji:\", round(ate_estimate, 2))\n\nOszacowany Przeciętny Efekt Interwencji: 9.66",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "9.1 Introduction to Sigma Notation (Σ)\nDescriptive statistics are fundamental tools in social science research, providing a concise summary of data characteristics. They serve several crucial functions:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction-to-sigma-notation-σ",
    "href": "chapter5.html#introduction-to-sigma-notation-σ",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "What is Sigma summation notation? Sigma (Σ) is a mathematical operator that instructs us to sum (add) a sequence of terms - it functions as a directive to perform addition of all elements within a specified range.\nPurpose: Provides a concise way to write sums of many similar terms using a single symbol, avoiding lengthy addition expressions.\n\n\nBasic Formula\n\nThe general form of sigma notation is: \\sum_{i=a}^{b} f(i)\nSummation index: i\nLower bound: a\nUpper bound: b\nFunction: f(i)\n\n\n\nExamples of Sigma Notation Applications\n\nSimple Example: Sum of Natural Numbers\n\nSuppose you want to add the first five positive integers: \\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15\nThe above notation adds the first five positive integers.\n\n\n\nSum of Squares\n\nSuppose you want to sum the squares of the first four positive integers: \\sum_{i=1}^{4} i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 1 + 4 + 9 + 16 = 30\nThis is the sum of squares of the first four positive integers.\n\n\n\nSum of a Constant Value\n\nSumming a constant value c for n terms: \\sum_{i=1}^{n} c = c + c + c + ... + c \\text{ (n times)} = n \\cdot c\nExample: Sum of five fives: \\sum_{i=1}^{5} 5 = 5 + 5 + 5 + 5 + 5 = 5 \\cdot 5 = 25\n\n\n\n\nSimple Examples in Statistical Context\n\\sum_{i=1}^{n} x_i - Summation index: i (typically denotes a specific observation in a dataset) - Lower bound: 1 (we usually start from the first observation) - Upper bound: n (total number of observations in our dataset) - Expression: x_i (value of the ith observation)\n\nSumming Observation Values\n\nWe have a dataset: 5, 8, 12, 15, 20\nSum of all values: \\sum_{i=1}^{5} x_i = x_1 + x_2 + x_3 + x_4 + x_5 = 5 + 8 + 12 + 15 + 20 = 60\nThis sum is a key element when calculating the arithmetic mean.\n\n\n\nSum of Deviations from the Mean\n\nFor the same dataset (5, 8, 12, 15, 20), the mean is \\bar{x} = 60/5 = 12\nSum of deviations from the mean: \\sum_{i=1}^{5} (x_i - \\bar{x}) = (5-12) + (8-12) + (12-12) + (15-12) + (20-12) = -7 + (-4) + 0 + 3 + 8 = 0\nImportant observation: The sum of deviations from the mean always equals 0, which is a fundamental property of the arithmetic mean.\n\n\n\n\nSummary\n\nSigma Notation (Σ) allows for concise expression of key statistical formulas\nThe most important applications include calculating:\n\nArithmetic mean\nVariance and standard deviation\nVarious sums of squares used in regression analysis\n\n\n\n\n\n\n\n\nSummation (Σ) and Product (Π) Operators\n\n\n\n\nSigma (Σ) Operator\n\\sum is a summation operator that instructs us to add terms:\n\\sum_{i=1}^{n} x_i = x_1 + x_2 + ... + x_n\nwhere: - i is the index variable - The lower value under Σ (here i=1) is the starting point - The upper value (here n) is the ending point\n\n\nPi (Π) Operator\n\\prod is a product operator that instructs us to multiply terms:\n\\prod_{i=1}^{n} x_i = x_1 \\times x_2 \\times ... \\times x_n\nwhere: - i is the index variable - The lower value under Π (here i=1) is the starting point - The upper value (here n) is the ending point\n\n\n\n\n\n\n\n\n\nExample of Σ\n\n\n\n\\sum_{i=1}^{4} i = 1 + 2 + 3 + 4 = 10\n\n\n\n\n\n\n\n\nExample of Π\n\n\n\n\\prod_{i=1}^{4} i = 1 \\times 2 \\times 3 \\times 4 = 24\n\n\n\n\n\n\n\n\nKey Differences\n\n\n\n\nΣ represents repeated addition\nΠ represents repeated multiplication",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#types-of-data-distributions",
    "href": "chapter5.html#types-of-data-distributions",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.2 Types of Data Distributions",
    "text": "9.2 Types of Data Distributions\n\n\n\n\n\n\nImportant\n\n\n\nData distribution informs what values a variable takes and how often.\n\n\nUnderstanding data distributions is crucial for data analysis and visualization. In this document, we’ll explore various types of distributions and how to visualize them using ggplot2 in R.\n\nNormal Distribution\nThe normal distribution, also known as the Gaussian distribution, is symmetric and bell-shaped.\n\n# Generate normal distribution data\nnormal_data &lt;- data.frame(x = rnorm(1000))\n\n# Plot\nggplot(normal_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Normal Distribution\", x = \"Value\", y = \"Density\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\nUniform Distribution\nIn a uniform distribution, all values have an equal probability of occurrence.\n\n# Generate uniform distribution data\nuniform_data &lt;- data.frame(x = runif(1000))\n\n# Plot\nggplot(uniform_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Uniform Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\nSkewed Distributions\nSkewed distributions are asymmetric, with one tail longer than the other.\n\n# Generate right-skewed data\nright_skewed &lt;- data.frame(x = rlnorm(1000))\n\n# Plot\nggplot(right_skewed, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightyellow\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Right-Skewed Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\nBimodal Distribution\nA bimodal distribution has two peaks, indicating two distinct subgroups in the data.\n\n# Generate bimodal data\nbimodal_data &lt;- data.frame(x = c(rnorm(500, mean = -2), rnorm(500, mean = 2)))\n\n# Plot\nggplot(bimodal_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightpink\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Bimodal Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nKey Properties\nExamples\n\n\n\n\nSymmetric (Normal)\nSymmetric, bell-shaped, most values close to the mean\nAdult height in population, IQ test scores, measurement errors, standardized exam results\n\n\nUniform\nEqual probability across the entire range\nLast digit of phone numbers, random day of the week selection, position of pointer after spinning a wheel of fortune\n\n\nBimodal\nTwo distinct peaks, suggests presence of subgroups\nAge structure in university towns (students and permanent residents), opinions on strongly polarizing topics, traffic intensity hours (morning and afternoon peak)\n\n\nRight-skewed (Positively skewed)\nExtended “tail” on the right side, most values less than the mean\nQueue waiting time, commute time to work, age at first marriage\n\n\nHeavy-tailed skewed (Log-normal)\nStrong right asymmetry, values cannot be negative, long “fat tail”\nPersonal income, housing prices, household size\n\n\nExtreme-tailed skewed (Power law)\nExtreme asymmetry, “rich get richer” effect, no characteristic scale\nWealth of the richest individuals, city populations, number of followers on social media, number of citations of scientific publications",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#visualizing-real-world-data-distributions",
    "href": "chapter5.html#visualizing-real-world-data-distributions",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.3 Visualizing Real-World Data Distributions",
    "text": "9.3 Visualizing Real-World Data Distributions\nLet’s use the palmerpenguins dataset to explore data distributions.\n\nHistogram and Density Plot\n\n\n\n\n\n\nUnderstanding Histograms and Density\n\n\n\n⭐ A histogram is a special graph for numerical data where:\n\nData is grouped into ranges (called “bins”)\nBars touch each other (unlike bar charts!) because the data is continuous\nEach bar’s height shows how many values fall into that range\n\nThink of density as showing how common or concentrated certain values are in your data:\n\nA higher point on a density curve (or taller bar in a histogram) means those values appear more frequently in your data\nA lower point means those values are less common\n\nJust like a crowded area has more people per space (higher density), a taller part of the graph shows values that appear more often in your dataset!\n\n\n\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Distribution of Penguin Flipper Lengths\", \n       x = \"Flipper Length (mm)\", \n       y = \"Density\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\nBox Plot\nBox plots are useful for comparing distributions across categories.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Distribution of Penguin Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nViolin Plot\nViolin plots combine box plot and density plot features.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Distribution of Penguin Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nRidgeline Plot\nRidgeline plots are useful for comparing multiple distributions.\n\nlibrary(ggridges)\n\nggplot(penguins, aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.6) +\n  labs(title = \"Distribution of Flipper Length by Penguin Species\",\n       x = \"Flipper Length (mm)\",\n       y = \"Species\")\n\nPicking joint bandwidth of 2.38\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nUnderstanding and visualizing data distributions is crucial in data analysis. ggplot2 provides a flexible and powerful toolkit for creating various types of distribution plots. By exploring different visualization techniques, we can gain insights into the underlying patterns and characteristics of our data.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-outliers",
    "href": "chapter5.html#understanding-outliers",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.4 Understanding Outliers",
    "text": "9.4 Understanding Outliers\nBefore diving into specific measures, it’s crucial to understand the concept of outliers, as they can significantly impact many descriptive statistics.\nOutliers are data points that differ significantly from other observations in the dataset. They can occur due to:\n\nMeasurement or recording errors\nGenuine extreme values in the population\n\nOutliers can have a substantial effect on many statistical measures, especially those based on means or sums of squared deviations. Therefore, it’s essential to:\n\nIdentify outliers through both statistical methods and domain knowledge\nInvestigate the cause of outliers\nMake informed decisions about whether to include or exclude them in analyses\n\nThroughout this guide, we’ll discuss how different descriptive measures are affected by outliers.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#statistical-symbols-and-notations---summary",
    "href": "chapter5.html#statistical-symbols-and-notations---summary",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.5 Statistical Symbols and Notations - Summary",
    "text": "9.5 Statistical Symbols and Notations - Summary\n\n\n\n\n\n\n\n\n\n\nMeasure\nPopulation Parameter\nSample Statistic\nAlternative Notations\nUsage Notes\n\n\n\n\nSize\nN\nn\n-\nTotal count of observations\n\n\nMean\n\\mu\n\\bar{x}, m\nM, E(X)\nE(X) used in probability theory\n\n\nVariance\n\\sigma^2\ns^2\n\\text{Var}(X), V(X)\nSquared deviations from mean\n\n\nStandard Deviation\n\\sigma\ns\n\\text{SD}, \\text{std}\nSquare root of variance\n\n\nProportion\n\\pi, P\n\\hat{p}\n\\text{prop}\nRelative frequencies\n\n\nCorrelation\n\\rho\nr\n\\text{corr}(x,y)\nRanges from -1 to +1\n\n\nStandard Error\n\\sigma_{\\bar{x}}\ns_{\\bar{x}}\n\\text{SE}\nStandard error of mean\n\n\nSum\n\\sum\n\\sum\n\\sum_{i=1}^n\nWith indexing\n\n\nIndividual Value\nX_i\nx_i\n-\nith observation\n\n\nCovariance\n\\sigma_{xy}\ns_{xy}\n\\text{Cov}(X,Y)\nJoint variation\n\n\nMedian\n\\eta\n\\text{Med}\nM\nCentral value\n\n\nRange\nR\nr\n\\text{max}(X) - \\text{min}(X)\nSpread measure\n\n\nMode\n\\text{Mo}\n\\text{mo}\n\\text{mod}\nMost frequent value\n\n\nSkewness\n\\gamma_1\ng_1\n\\text{SK}\nDistribution asymmetry\n\n\nKurtosis\n\\gamma_2\ng_2\n\\text{KU}\nDistribution peakedness\n\n\n\nAdditional useful notations:\n\nSample moments: m_k = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^k\nPopulation moments: \\mu_k = E[(X - \\mu)^k]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-central-tendency",
    "href": "chapter5.html#measures-of-central-tendency",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.6 Measures of Central Tendency",
    "text": "9.6 Measures of Central Tendency\nMeasures of central tendency aim to identify the “typical” or “central” value in a dataset. The three primary measures are mean, median, and mode.\n\nArithmetic Mean\nThe arithmetic mean is the sum of all values divided by the number of values.\nFormula: \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\nImportant Property: The mean is a balancing point in the data. The sum of deviations from the mean is always zero:\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0\nThis property makes the mean useful in many statistical calculations.\n\n\n\n\n\n\nUnderstanding Mean as a Balance Point 🎯\n\n\n\nLet’s consider a dataset X = \\{1, 2, 6, 7, 9\\} on a number line, imagining it as a seesaw:\n\n\n\nhttps://www.gastonsanchez.com/matrix4sl/mean-as-a-balancing-point.html\n\n\nThe mean (\\mu) acts as the perfect balance point of this seesaw. For our data:\n\\mu = \\frac{1 + 2 + 6 + 7 + 9}{5} = 5\n\nWhat happens at different support points? 🤔\n\nSupport point at 6 (too high):\n\nLeft side: Values (1, 2) are below\nRight side: Values (7, 9) are above\n\\sum distances from left = (6-1) + (6-2) = 9\n\\sum distances from right = (7-6) + (9-6) = 4\nThe seesaw tilts left! ⬅️ because 9 &gt; 4\n\nSupport point at 4 (too low):\n\nLeft side: Values (1, 2) are below\nRight side: Values (6, 7, 9) are above\n\\sum distances from left = (4-1) + (4-2) = 5\n\\sum distances from right = (6-4) + (7-4) + (9-4) = 10\nThe seesaw tilts right! ➡️ because 5 &lt; 10\n\nSupport point at mean (5) (perfect balance):\n\n\\sum distances below = \\sum distances above\n((5-1) + (5-2)) = ((6-5) + (7-5) + (9-5))\n7 = 7 ✨ Perfect balance!\n\n\nThis shows why the mean is the unique balance point, where:\n\\sum_{i=1}^n (x_i - \\mu) = 0\nThe seesaw will always tilt unless the support point is placed exactly at the mean! 🎪\n\n\n\n\n\n\n\n\n\nMean as a Balance Point\n\n\n\nThis visualization shows how the arithmetic mean (5) acts as a balance point between clustered points on the left and dispersed points on the right:\nLeft side of the mean: - Points with values 2 and 3 - Close together (difference of 1 unit) - Distances from mean: 3 and 2 units - Sum of “pull” = 5 units\nRight side of the mean: - Points with values 6 and 9 - More spread out (difference of 3 units) - Distances from mean: 1 and 4 units - Sum of “pull” = 5 units\nKey observations:\n\nThe mean (5) is a balance point, even though:\n\nPoints on the left are clustered (2,3)\nPoints on the right are dispersed (6,9)\nGreen arrows show distances from the mean\n\nBalance is maintained because:\n\nSum of distances balances out: (5-2) + (5-3) = (6-5) + (9-5)\nTotal sum of distances = 5 units on each side\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual Calculation Example:\nLet’s calculate the mean for the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nSum all values\n2 + 4 + 4 + 5 + 5 + 7 + 9 = 36\n\n\n2\nCount the number of values\nn = 7\n\n\n3\nDivide the sum by n\n36 / 7 = 5.14\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(data)\n\n[1] 5.142857\n\n\nPros:\n\nEasy to calculate and understand\nUses all data points\nUseful for further statistical calculations\n\nCons:\n\nSensitive to outliers\nNot ideal for skewed distributions\n\nExample with outlier:\n\ndata_with_outlier &lt;- c(2, 4, 4, 5, 5, 7, 100)\nmean(data_with_outlier)\n\n[1] 18.14286\n\n\nAs we can see, the outlier (100) drastically affects the mean.\n\n\nMedian\nThe median is the middle value when the data is ordered.\nManual Calculation Example:\nUsing the same dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nResult\n\n\n\n\n1\nOrder the data\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nFind the middle value\n5\n\n\n\nFor even number of values, take the average of the two middle values.\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmedian(data)\n\n[1] 5\n\nmedian(data_with_outlier)\n\n[1] 5\n\n\nPros:\n\nNot affected by extreme outliers\nBetter for skewed distributions\n\nCons:\n\nDoesn’t use all data points\nLess useful for further statistical calculations\n\n\n\n\n\n\n\nWarning\n\n\n\nTo find the position of the median in a dataset:\n\nFirst sort the data in ascending order\nIf n is odd:\n\nMedian position = \\frac{n + 1}{2}\n\nIf n is even:\n\nFirst median position = \\frac{n}{2}\nSecond median position = \\frac{n}{2} + 1\nMedian = \\frac{\\text{value at }\\frac{n}{2} + \\text{value at }(\\frac{n}{2}+1)}{2}\n\n\nFor example:\n\nOdd n=7: position = \\frac{7+1}{2} = 4th value\nEven n=8: positions = \\frac{8}{2} = 4th and 4+1 = 5th value\n\n\n\n\n\nMode\nThe mode is the most frequently occurring value.\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nValue\nFrequency\n\n\n\n\n2\n1\n\n\n4\n2\n\n\n5\n2\n\n\n7\n1\n\n\n9\n1\n\n\n\nThe mode is 4 and 5 (bimodal).\nR calculation:\n\nlibrary(modeest)\nmfv(data)  # Most frequent value\n\n[1] 4 5\n\n\nPros:\n\nOnly measure of central tendency for nominal data\nCan identify multiple peaks in the data\n\nCons:\n\nNot always uniquely defined\nNot useful for continuous data\n\n\n\nWeighted (arithmetic) Mean (*)\nThe weighted mean is used when some data points are more important than others. There are two types of weighted means: with not normalized weights and with normalized weights.\n\nWeighted Mean with Not Normalized Weights\nThis is the standard form of the weighted mean, where weights can be any positive numbers representing the importance of each data point.\nFormula: \\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\nManual Calculation Example:\nLet’s calculate the weighted mean for the dataset: 2, 4, 5, 7 with weights 1, 2, 3, 1\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nMultiply each value by its weight\n(2 * 1) + (4 * 2) + (5 * 3) + (7 * 1) = 2 + 8 + 15 + 7 = 32\n\n\n2\nSum the weights\n1 + 2 + 3 + 1 = 7\n\n\n3\nDivide the result from step 1 by the result from step 2\n32 / 7 = 4.57\n\n\n\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\nWeighted Mean with Normalized Weights (Fractions)\nIn this case, the weights are fractions that sum to 1, representing the proportion of importance for each data point.\nFormula: \\bar{x}_w = \\sum_{i=1}^n w_i x_i, where \\sum_{i=1}^n w_i = 1\nManual Calculation Example:\nLet’s calculate the weighted mean for the dataset: 2, 4, 5, 7 with normalized weights 0.1, 0.3, 0.4, 0.2\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nMultiply each value by its weight\n(2 * 0.1) + (4 * 0.3) + (5 * 0.4) + (7 * 0.2)\n\n\n2\nSum the results\n0.2 + 1.2 + 2.0 + 1.4 = 4.8\n\n\n\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Note: these sum to 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nPros of Weighted Means:\n\nAccount for varying importance of data points\nUseful in survey analysis with different sample sizes or importance levels\nCan adjust for unequal probabilities in sampling designs\n\nCons of Weighted Means:\n\nRequire justification for weights\nCan be misused to manipulate results\nMay be less intuitive to interpret than simple arithmetic mean",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-variability",
    "href": "chapter5.html#measures-of-variability",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.7 Measures of Variability",
    "text": "9.7 Measures of Variability\nThese measures describe how spread out the data is. They are crucial for understanding the dispersion of data points around the central tendency.\n\n\n\n\n\n\nUnderstanding Variance\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.1: Three dot plots showing increasing variance with constant mean\n\n\n\n\n\nThe three dot plots above demonstrate how variance measures the spread of data around a central value:\n\nAll distributions have the same mean (μ = 10), shown by the dashed line\nLow Variance (σ² = 1): Points cluster tightly around the mean\nMedium Variance (σ² = 4): Points show moderate spread\nHigh Variance (σ² = 9): Points spread widely around the mean\n\n\n\n\n\n\n\n\n\nUnderstanding Different Levels of Variability\n\n\n\n\n\n\n\n\n\n\n\n\nThis visualization shows three normal distributions with the same mean (μ = 10) but different levels of variability:\n\nLow Variability (σ = 0.5)\n\nData points cluster tightly around the mean\nThe density curve is tall and narrow\nMost observations fall within ±0.5 units of the mean\n\nMedium Variability (σ = 2.0)\n\nData points spread out more from the mean\nThe density curve is lower and wider\nMost observations fall within ±2 units of the mean\n\nHigh Variability (σ = 4.0)\n\nData points spread widely from the mean\nThe density curve is much flatter and wider\nMost observations fall within ±4 units of the mean\n\n\n\n\n\nRange\nThe range is the difference between the maximum and minimum values.\nFormula: R = x_{max} - x_{min}\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nFind the maximum value\n9\n\n\n2\nFind the minimum value\n2\n\n\n3\nSubtract minimum from maximum\n9 - 2 = 7\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nrange(data)\n\n[1] 2 9\n\nmax(data) - min(data)\n\n[1] 7\n\n\nPros:\n\nSimple to calculate and understand\nGives an immediate sense of data spread\n\nCons:\n\nExtremely sensitive to outliers\nDoesn’t provide information about the distribution between extremes\n\n\n\nInterquartile Range (IQR)\nThe IQR is the difference between the 75th and 25th percentiles.\nFormula: IQR = Q_3 - Q_1\nTo find quartiles manually:\n\nFor odd number of values:\n\nQ2 (median) is the middle value\nQ1 is the median of the lower half (excluding the median of all observations)\nQ3 is the median of the upper half (excluding the median of all observations)\n\nFor even number of values:\n\nQ2 is the average of the two middle values\nQ1 is the median of the lower half (excluding the median of all observations)\nQ3 is the median of the upper half (excluding the median of all observations)\n\n\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nOrder the data\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nFind Q2 (median)\n5\n\n\n3\nFind Q1 (median of lower half)\n4\n\n\n4\nFind Q3 (median of upper half)\n7\n\n\n5\nCalculate IQR\nQ3 - Q1 = 7 - 4 = 3\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nprint(data)\n\n[1] 2 4 4 5 5 7 9\n\nquantile(data, type = 1)\n\n  0%  25%  50%  75% 100% \n   2    4    5    7    9 \n\nIQR(data, type = 1)\n\n[1] 3\n\n\nPros:\n\nRobust to outliers\nProvides information about the spread of the middle 50% of the data\n\nCons:\n\nIgnores the tails of the distribution\nLess efficient than standard deviation for normal distributions\n\n\n\nVariance\nVariance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n\n\n\n\n\n\nVariance: Understanding Average Squared Deviations\n\n\n\nWhat is Variance? Variance measures how “spread out” numbers are from their mean - it’s the average of squared deviations from the mean.\nFormula: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nSimple Example: Consider numbers: 2, 4, 6, 8, 10 Mean (\\bar{x}) = 6\n\n\n\n\n\n\n\n\n\nCalculating Deviations:\n\n\n\n\n\n\n\n\n\n\n\n\nValue\nDeviation from mean\nSquare of deviation\n\n\n\n\n2\n-4\n16\n\n\n4\n-2\n4\n\n\n6\n0\n0\n\n\n8\n+2\n4\n\n\n10\n+4\n16\n\n\n\nVariance = \\frac{16 + 4 + 0 + 4 + 16}{4} = 10\nKey Points:\n\nMean acts as a reference line (blue dashed line)\nDeviations show distance from mean (red dotted lines)\nSquaring makes all deviations positive (blue bars)\nLarger deviations contribute more to variance\n\n\n\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the mean\n\\bar{x} = 5.14\n\n\n2\nSubtract the mean from each value and square the result\n(2 - 5.14)^2 = 9.86\n\n\n\n\n(4 - 5.14)^2 = 1.30\n\n\n\n\n(4 - 5.14)^2 = 1.30\n\n\n\n\n(5 - 5.14)^2 = 0.02\n\n\n\n\n(5 - 5.14)^2 = 0.02\n\n\n\n\n(7 - 5.14)^2 = 3.46\n\n\n\n\n(9 - 5.14)^2 = 14.90\n\n\n3\nSum the squared differences\n30.86\n\n\n4\nDivide by (n-1), i.e. by the number of observations - 1\n30.86 / 6 = 5.14\n\n\n\nR calculation:\n\nvar(data)\n\n[1] 5.142857\n\n\nPros:\n\nUses all data points\nFoundation for many statistical tests\n\nCons:\n\nUnits are squared, making interpretation less intuitive\nSensitive to outliers\n\n\n\n\n\n\n\nBessel’s Correction: Why We Divide by (n-1) And Not by n\n\n\n\nThe Key Insight:\nWhen we calculate deviations from the mean, they must sum to zero. This is a mathematical fact: \\sum(x_i - \\bar{x}) = 0\nThink of it Like This:\nIf you have 5 numbers and their mean:\n\nOnce you calculate 4 deviations from the mean\nThe 5th deviation MUST be whatever makes the sum zero\nYou don’t really have 5 independent deviations\nYou only have 4 truly “free” deviations\n\nSimple Example:\nNumbers: 2, 4, 6, 8, 10\n\nMean = 6\nDeviations: -4, -2, 0, +2, +4\nNotice they sum to zero\nIf you know any 4 deviations, the 5th is predetermined!\n\nThis is Why:\n\nWhen calculating variance: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nWe divide by (n-1) not n\nBecause only (n-1) deviations are truly independent\nThe last one is determined by the others\n\nDegrees of Freedom:\n\nn = number of observations\n1 = constraint (deviations must sum to zero)\nn-1 = degrees of freedom = number of truly independent deviations\n\nWhen to Use It:\n\nWhen calculating sample variance\nWhen calculating sample standard deviation\n\nWhen NOT to Use It:\n\nPopulation calculations (when you have all data)\n\nRemember:\n\nIt’s not just a statistical trick\nDeviations from the mean must sum to zero\nThis constraint costs us one degree of freedom\n\n\n\n\n\nStandard Deviation\nThe standard deviation is the square root of the variance and measures the average dispersion of the data about their arithmetic mean. In contrast to the variance, it has the advantage of being expressed in the same units as the original measurements, making its interpretation more intuitive.\nFormula: s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the variance\ns^2 = 5.14 (from previous calculation)\n\n\n2\nTake the square root\ns = \\sqrt{5.14} = 2.27\n\n\n\nR calculation:\n\nsd(data)\n\n[1] 2.267787\n\n\nPros:\n\nIn same units as original data\nWidely used and understood\n\nCons:\n\nStill sensitive to outliers\nAssumes data is roughly “normally” distributed\n\n\n\nCoefficient of Variation (*)\nThe coefficient of variation is the standard deviation divided by the mean, often expressed as a percentage.\nFormula: CV = \\frac{s}{\\bar{x}} \\times 100\\%\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the mean\n\\bar{x} = 5.14\n\n\n2\nCalculate the standard deviation\ns = 2.27\n\n\n3\nDivide s by the mean and multiply by 100\n(2.27 / 5.14) * 100 = 44.16\\%\n\n\n\nR calculation:\n\n(sd(data) / mean(data)) * 100\n\n[1] 44.09586\n\n\nPros:\n\nAllows comparison of variability between datasets with different units or means\nUseful in fields like finance for risk assessment\n\nCons:\n\nNot meaningful for data with both positive and negative values\nCan be misleading when mean is close to zero\n\n\n\n\n\n\n\nLimitations of Coefficient of Variation (CV)\n\n\n\nThe coefficient of variation, calculated as (σ/μ) × 100\\%, has two important limitations:\n\nNot meaningful for data with both positive and negative values\n\nThe mean could be close to zero due to positive and negative values cancelling out\nExample: Dataset {-5, -3, 2, 6} has mean = 0\n\nCV = (std dev / 0) × 100%\nThis leads to division by zero\nEven if mean isn’t exactly zero, the CV doesn’t represent true relative variability when data cross zero\n\nThe CV assumes a natural zero point and meaningful ratios between values\n\n\n\nMisleading when mean is close to zero\n\nSince CV = (σ/μ) × 100\\%, as μ approaches zero:\n\nThe denominator becomes very small\nResults in extremely large CV values\nThese large values don’t meaningfully represent relative variability\n\nExample:\n\nDataset A: {0.001, 0.002, 0.003} has mean = 0.002\nEven small standard deviations will produce very large CVs\nThe resulting large CV might suggest extreme variability when the data are actually quite close together\n\n\n\n\nBest Use Cases\nCV is most useful for:\n\nStrictly positive data\nData measured on a ratio scale\nData with means well above zero\nComparing variability between datasets with different units or scales",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-relative-position-standing",
    "href": "chapter5.html#measures-of-relative-position-standing",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.8 Measures of Relative Position (Standing)",
    "text": "9.8 Measures of Relative Position (Standing)\nUnderstanding where values sit within a dataset is crucial for data analysis. Let’s explore these concepts step by step.\n\nQuartiles (Q): The Basics\nThink of quartiles as special numbers that split your ordered data into four equal parts.\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\n\nWhat Are Quartiles?\nFirst Quartile (Q1):\n\nSeparates the lowest 25% of data from the rest\nAlso called the 25th percentile\nExample: If Q1 = 50 in a test score dataset, 25% of students scored below 50\n\nSecond Quartile (Q2):\n\nThe median - splits data in half\nAlso called the 50th percentile\nExample: If Q2 = 70, half the students scored below 70\n\nThird Quartile (Q3):\n\nSeparates the highest 25% of data from the rest\nAlso called the 75th percentile\nExample: If Q3 = 85, 75% of students scored below 85\n\n\n\nHow to Calculate Quartiles (Step by Step) - Two Methods\nLet’s examine student test scores using both common quartile calculation methods:\nExample 1: Odd Number Case (11 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 88, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 11 values (odd)\nMedian position = (n + 1)/2 = 6\nQ2 = 78\n\nStep 2: Find Q1\n\nTukey’s Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (n + 1)/4 = (11 + 1)/4 = 3\nQ1 = 70 (3rd value)\n\n\nStep 3: Find Q3\n\nTukey’s Method:\n\nLook at upper half: 80, 82, 85, 88, 90\nQ3 = median of upper half = 85\n\nInterpolation Method:\n\nPosition = 3(n + 1)/4 = 3(12)/4 = 9\nQ3 = 85 (9th value)\n\n\nExample 2: Even Number Case (10 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 10 values (even)\nMedian positions = 5 and 6\nQ2 = (75 + 78)/2 = 76.5\n\nStep 2: Find Q1\n\nTukey’s Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (10 + 1)/4 = 2.75\nQ1 = 65 + 0.75(70 - 65) = 68.75\n\n\nStep 3: Find Q3\n\nTukey’s Method:\n\nLook at upper half: 78, 80, 82, 85, 90\nQ3 = median of upper half = 82\n\nInterpolation Method:\n\nPosition = 3(10 + 1)/4 = 8.25\nQ3 = 82 + 0.25(85 - 82) = 82.75\n\n\nImportant Notes:\n\nTukey’s Method:\n\nFirst find the median (Q2)\nSplit the data into lower and upper halves\nFind Q1 as the median of the lower half\nFind Q3 as the median of the upper half\nWhen n is odd, the median is not included in either half\n\nInterpolation Method:\n\nUses positions (n+1)/4 for Q1 and 3(n+1)/4 for Q3\nWhen position falls between values, uses linear interpolation\nDoesn’t require splitting data into halves\n\n\nBoth methods give the same results for simple positions (Example 1) but can differ when interpolation is needed (Example 2).\n\n\n\n\n\n\nManual Construction of Tukey Boxplot\n\n\n\nStep 1: Calculate Key Components\n\nFind quartiles: Q_1, Q_2 (median), Q_3\nCalculate Interquartile Range: IQR = Q_3 - Q_1\n\nStep 2: Determine Whisker Boundaries\n\nLower fence: Q_1 - 1.5 \\times IQR\nUpper fence: Q_3 + 1.5 \\times IQR\n\nStep 3: Identify Outliers Data points are outliers if they are:\n\nBelow lower fence: x &lt; Q_1 - 1.5 \\times IQR\nAbove upper fence: x &gt; Q_3 + 1.5 \\times IQR\n\nExample: Given data: 2, 4, 6, 8, 9, 10, 11, 12, 14, 16, 50\n\nFind quartiles:\n\nQ_1 = 6\nQ_2 = 10\nQ_3 = 14\n\nCalculate IQR:\n\nIQR = 14 - 6 = 8\n\nCalculate fences:\n\nLower: 6 - (1.5 \\times 8) = -6\nUpper: 14 + (1.5 \\times 8) = 26\n\nIdentify outliers:\n\n50 &gt; 26, therefore 50 is an outlier\n\n\nGraphical Elements:\n\nBox: Draw from Q_1 to Q_3\nLine inside box: Draw at Q_2\nWhiskers: Extend to most extreme non-outlier points\nPoints: Plot outliers individually beyond whiskers\n\n\n\n\n\n\nPercentiles: A More Precise Measure of Relative Standing (*)\n\nWhat Are Percentiles?\nPercentiles give us a more detailed view by dividing data into 100 equal parts. Unlike quartiles, percentiles use linear interpolation for more precise measurements.\nKey Points:\n\nThe 25th percentile equals Q1\nThe 50th percentile equals Q2 (median)\nThe 75th percentile equals Q3\n\n\n\nCalculating Percentiles\nThe Formula: P_k = \\frac{k(n+1)}{100}\nWhere:\n\nP_k is the position for the kth percentile\nk is the percentile we want (1-100)\nn is the number of observations\n\nExample 3: Finding the 60th Percentile Let’s use student homework scores: 72, 75, 78, 80, 82, 85, 88, 90, 92, 95\nStep 1: Calculate position\n\nn = 10 scores\nFor 60th percentile: P_{60} = \\frac{60(10+1)}{100} = 6.6\n\nStep 2: Find surrounding values\n\nPosition 6: score of 85\nPosition 7: score of 88\n\nStep 3: Interpolate (important: percentiles use linear interpolation)\n\nWe need to go 0.6 of the way between 85 and 88 P_{60} = 85 + 0.6(88-85) P_{60} = 85 + 0.6(3) P_{60} = 85 + 1.8 = 86.8\n\nWhat this means: 60% of students scored 86.8 or below.\n\n\n\nPercentile Ranks (PR) (*)\n\nWhat is a Percentile Rank?\nWhile percentiles tell us the value at a certain position, percentile rank tells us what percentage of values fall below a specific score. Think of it as answering the question “What percentage of the class did I score higher than?”\nPR = \\frac{\\text{number of values below } + 0.5 \\times \\text{number of equal values}}{\\text{total number of values}} \\times 100\nExample 4: Finding a Percentile Rank Consider these exam scores:\n65, 70, 70, 75, 75, 75, 80, 85, 85, 90\nLet’s find the PR for a score of 75.\nStep 1: Count carefully\n\nValues below 75: 65, 70, 70 (3 values)\nValues equal to 75: 75, 75, 75 (3 values)\nTotal values: 10\n\nStep 2: Apply the formula\nPR = \\frac{3 + 0.5(3)}{10} \\times 100 PR = \\frac{3 + 1.5}{10} \\times 100 PR = \\frac{4.5}{10} \\times 100 = 45\\%\nInterpretation: A score of 75 is higher than 45% of the class scores.\nRemark:\nQ1: “Why do we use 0.5 for equal values in PR?”\nA1: This is because we’re assuming people with the same score are evenly spread across that position. It’s like saying they share the position equally.\n\n\n\nUnderstanding and Interpreting Box Plots\nBox plots (also known as box-and-whisker plots) are powerful visualization tools for understanding data distributions. In this section, we’ll explore how to construct and interpret box plots using height measurements from two groups.\n\nConstruction of the Tukey Box Plot\nThe box plot was introduced by John Tukey as part of his exploratory data analysis toolkit. It provides a standardized way of displaying the distribution of data based on a five-number summary.\n\nThe Five-Number Summary\nA box plot represents five key statistical values:\n\nMinimum: The smallest value in the dataset (excluding outliers)\nFirst Quartile (Q1): The 25th percentile, below which 25% of observations fall\nMedian (Q2): The 50th percentile, which divides the dataset into two equal halves\nThird Quartile (Q3): The 75th percentile, below which 75% of observations fall\nMaximum: The largest value in the dataset (excluding outliers)\n\n\n\nBox Plot Components\n\n\n\n\n\n\n\n\nFigure 9.2: Boxplot diagram showing its key components.\n\n\n\n\n\nThe components of a box plot include:\n\nThe Box:\n\nRepresents the interquartile range (IQR), containing the middle 50% of the data\nLower edge represents Q1\nUpper edge represents Q3\nLine inside the box represents the median (Q2)\n\nThe Whiskers:\n\nExtend from the box to show the range of non-outlier data\nIn a Tukey box plot, whiskers extend up to 1.5 × IQR from the box edges:\n\nLower whisker: extends to the minimum value ≥ (Q1 - 1.5 × IQR)\nUpper whisker: extends to the maximum value ≤ (Q3 + 1.5 × IQR)\n\n\nOutliers:\n\nPoints that fall beyond the whiskers\nIndividually plotted as dots or symbols\nValues that are &lt; (Q1 - 1.5 × IQR) or &gt; (Q3 + 1.5 × IQR)\n\n\n\n\nKey Features to Observe\nWhen interpreting box plots, look for these characteristics:\n\nCentral Tendency: Location of the median line within the box\nDispersion: Width of the box (IQR) and length of the whiskers\nSkewness:\n\nSymmetrical data: median is approximately in the middle of the box, whiskers are roughly equal in length\nRight (positive) skew: median is closer to the bottom of the box, upper whisker is longer\nLeft (negative) skew: median is closer to the top of the box, lower whisker is longer\n\nOutliers: Presence of individual points beyond the whiskers\n\n\n\n\nCase Study: Comparing Heights Between Groups\nLet’s apply our understanding of box plots to a real dataset. We have height measurements (in centimeters) from two groups of 25 students each.\n\n# Create the dataset\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Transform dataset from wide to long format\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Display the first few rows\nhead(data_height_l)\n\n  Group_number height\n1      group_1    150\n2      group_1    160\n3      group_1    165\n4      group_1    168\n5      group_1    172\n6      group_1    173\n\n\nLet’s calculate some summary statistics for each group:\n\n# Calculate summary statistics for each group\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Calculate IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Create a comparison table\nstats_table &lt;- rbind(\n  group1_stats,\n  group2_stats\n)\nrownames(stats_table) &lt;- c(\"Group 1\", \"Group 2\")\n\n# Display the table\nstats_table\n\n        Min. 1st Qu. Median Mean 3rd Qu. Max.\nGroup 1  150     175    180  179     183  200\nGroup 2  138     165    175  172     182  210\n\n# Display IQR values\ncat(\"IQR for Group 1:\", group1_iqr, \"\\n\")\n\nIQR for Group 1: 8 \n\ncat(\"IQR for Group 2:\", group2_iqr, \"\\n\")\n\nIQR for Group 2: 17 \n\n\n\n\nVisualizing the Height Data\nNow, let’s visualize the data using box plots and density plots:\n\n# Create horizontal boxplots\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Height Distribution by Group\",\n       x = \"Group\",\n       y = \"Height (cm)\")\n\n\n\n\n\n\n\nFigure 9.3: Box plots comparing height distributions between groups.\n\n\n\n\n\nTo complement our box plots, let’s also look at the density distributions:\n\n# Create density plots\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Height Density by Group\",\n       x = \"Height (cm)\",\n       y = \"Density\")\n\n\n\n\n\n\n\nFigure 9.4: Density plots showing the height distributions for each group.\n\n\n\n\n\n\n\nBox Plot Interpretation Exercise\nBased on the box plots and density plots above, determine whether each of the following statements is True or False. For each statement, provide a brief explanation based on evidence from the visualizations.\n\n\n\n\n\n\nExercise Questions\n\n\n\n\nStudents from group 2 (G2) in the studied sample are, on average, taller than those from group 1 (G1).\nGroup 1 (G1) height measurements are more dispersed/spread out than group 2 (G2).\nThe lowest person is in group 2 (G2).\nBoth data sets are negatively (left) skewed.\nHalf of the students in group 2 (G2) measure at least 175 cm.\n\n\n\n\nHints for Interpretation\nWhen answering these questions, consider:\n\nThe position of the median line within each box\nThe relative sizes of the boxes (IQR)\nThe positions of the minimum and maximum values\nThe symmetry of the distributions (balanced or skewed)\nThe lengths of the whiskers\n\nFor each statement, determine whether it is True or False and provide your explanation:\n\n\n\n\n\n\nAnswer Template\n\n\n\n\n\n\nStudents from G2 are, on average, taller than G1: [True/False]\n\nExplanation:\n\nG1 height is more dispersed/spread out: [True/False]\n\nExplanation:\n\nThe lowest person is in G2: [True/False]\n\nExplanation:\n\nBoth data sets are negatively (left) skewed: [True/False]\n\nExplanation:\n\nHalf of G2 measure at least 175 cm: [True/False]\n\nExplanation:\n\n\n\n\n\nLet’s review the answers to our box plot interpretation questions:\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nStudents from G2 are, on average, taller than G1: False\n\nExplanation: The median height (middle line in the boxplot) for G1 is higher than G2.\n\nG1 height is more dispersed/spread out: False\n\nExplanation: G2 shows greater dispersion. This is visible in the boxplot where G2 has a larger interquartile range (IQR) of 17.5 cm compared to G1’s 9.5 cm. G2 also has a wider range from minimum to maximum values.\n\nThe lowest person is in G2: True\n\nExplanation: The minimum value in G2 is 138 cm, which is lower than the minimum value in G1 (150 cm).\n\nBoth data sets are negatively (left) skewed: True\n\nExplanation: In both groups, the median line is positioned toward the upper part of the box, and the lower whisker is longer than the upper whisker. This indicates that there’s a longer tail on the left side of the distribution, which means negative skewness.\n\nHalf of G2 measure at least 175 cm: True\n\nExplanation: The median (middle line in the boxplot) for G2 is 175 cm, which means that 50% of the values are greater than or equal to 175 cm.\n\n\n\n\n\n\n\n\nR Code Reference\nHere’s the complete R code used in this section:\n\n# Load required packages\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n# Set display options\noptions(scipen = 999, digits = 3)\n\n# Create the dataset\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Transform dataset from wide to long format\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Display the first few rows\nhead(data_height_l)\n\n# Calculate summary statistics for each group\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Calculate IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Create horizontal boxplots\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Height Distribution by Group\",\n       x = \"Group\",\n       y = \"Height (cm)\")\n\n# Create density plots\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Height Density by Group\",\n       x = \"Height (cm)\",\n       y = \"Density\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#shape-measures",
    "href": "chapter5.html#shape-measures",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.9 Shape Measures",
    "text": "9.9 Shape Measures\n\nSkewness\n\nDefinition\nSkewness quantifies the asymmetry of a data distribution. It indicates whether data tends to cluster more on one side of the mean than the other.\n\n\nMathematical Expression\nSK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3 where: - n is the sample size - x_i is the i-th observation - \\bar{x} is the sample mean - s is the sample standard deviation\n\n\nSimplified Numerical Example\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Three example datasets with different types of skewness\n# 1. Positive skewness (right tail)\npositive_skew_data &lt;- c(2, 3, 4, 4, 5, 5, 5, 6, 6, 7, 8, 12, 15, 20)\n# 2. Negative skewness (left tail)\nnegative_skew_data &lt;- c(1, 5, 10, 13, 14, 15, 16, 16, 17, 17, 18, 18, 19, 20)\n# 3. Near-zero skewness (symmetry)\nsymmetric_data &lt;- c(1, 3, 5, 7, 9, 10, 11, 12, 13, 15, 17, 19, 21)\n\n# Calculating skewness\npositive_skewness &lt;- skewness(positive_skew_data)\nnegative_skewness &lt;- skewness(negative_skew_data)\nsymmetric_skewness &lt;- skewness(symmetric_data)\n\n# Summary of results\nskewness_data &lt;- data.frame(\n  \"Distribution Type\" = c(\"Positive skewness\", \"Negative skewness\", \"Symmetric distribution\"),\n  \"Skewness value\" = round(c(positive_skewness, negative_skewness, symmetric_skewness), 3),\n  \"Interpretation\" = c(\n    \"Longer right tail (majority of data on the left side)\",\n    \"Longer left tail (majority of data on the right side)\",\n    \"Data distributed symmetrically\"\n  )\n)\n\n# Display table\nskewness_data\n\n       Distribution.Type Skewness.value\n1      Positive skewness           1.42\n2      Negative skewness          -1.33\n3 Symmetric distribution           0.00\n                                         Interpretation\n1 Longer right tail (majority of data on the left side)\n2 Longer left tail (majority of data on the right side)\n3                        Data distributed symmetrically\n\n\n\n\nVisualizations of Skewness Types\n\n# Create a data frame for all sets\ndf_skewness &lt;- rbind(\n  data.frame(value = positive_skew_data, type = \"Positive skewness\", \n             skewness = round(positive_skewness, 2)),\n  data.frame(value = negative_skew_data, type = \"Negative skewness\", \n             skewness = round(negative_skewness, 2)),\n  data.frame(value = symmetric_data, type = \"Symmetric distribution\", \n             skewness = round(symmetric_skewness, 2))\n)\n\n# Histograms for three types of skewness\np1 &lt;- ggplot(df_skewness, aes(x = value)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~type, scales = \"free_x\") +\n  geom_vline(data = df_skewness %&gt;% group_by(type) %&gt;% summarise(mean = mean(value)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_skewness %&gt;% group_by(type) %&gt;% summarise(median = median(value)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = unique(df_skewness[, c(\"type\", \"skewness\")]),\n           aes(x = Inf, y = Inf, label = paste(\"SK =\", skewness)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histograms showing different types of skewness\",\n    subtitle = \"Red line: mean, Green line: median\",\n    x = \"Value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n# Box plots\np2 &lt;- ggplot(df_skewness, aes(x = type, y = value, fill = type)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"skyblue\", \"lightgreen\", \"lightsalmon\")) +\n  labs(\n    title = \"Box plots for different types of skewness\",\n    x = \"Distribution type\",\n    y = \"Value\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Display plots\ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nExample: Voter Turnout Analysis\n\n# Generate three datasets reflecting different types of skewness\nset.seed(123)\n\n# 1. Positive skewness - typical for turnout in regions with low engagement\npositive_turnout &lt;- c(\n  runif(50, min = 20, max = 30),  # Small group with low turnout\n  rbeta(200, shape1 = 2, shape2 = 5) * 50 + 30  # Majority of results shifted to the left\n)\n\n# 2. Negative skewness - typical for regions with high political engagement\nnegative_turnout &lt;- c(\n  rbeta(200, shape1 = 5, shape2 = 2) * 30 + 50,  # Majority of results shifted to the right\n  runif(50, min = 40, max = 50)  # Small group with lower turnout\n)\n\n# 3. Symmetric distribution - typical for regions with uniform engagement\nsymmetric_turnout &lt;- rnorm(250, mean = 65, sd = 8)\n\n# Create data frame\ndf_turnout &lt;- rbind(\n  data.frame(turnout = positive_turnout, region = \"Region A: Positive skewness\"),\n  data.frame(turnout = negative_turnout, region = \"Region B: Negative skewness\"),\n  data.frame(turnout = symmetric_turnout, region = \"Region C: Symmetric distribution\")\n)\n\n# Calculate skewness for each region\nregion_skewness &lt;- df_turnout %&gt;%\n  group_by(region) %&gt;%\n  summarise(skewness = round(skewness(turnout), 2))\n\n# Histogram of turnout by region\np3 &lt;- ggplot(df_turnout, aes(x = turnout)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~region, ncol = 1) +\n  geom_vline(data = df_turnout %&gt;% group_by(region) %&gt;% summarise(mean = mean(turnout)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_turnout %&gt;% group_by(region) %&gt;% summarise(median = median(turnout)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = region_skewness,\n           aes(x = 25, y = 20, label = paste(\"SK =\", skewness)),\n           size = 3.5) +\n  labs(\n    title = \"Voter turnout in different regions\",\n    subtitle = \"Showing three types of skewness\",\n    x = \"Voter turnout (%)\",\n    y = \"Number of districts\"\n  ) +\n  theme_minimal()\n\n# Box plot\np4 &lt;- ggplot(df_turnout, aes(x = region, y = turnout, fill = region)) +\n  geom_boxplot() +\n  labs(\n    title = \"Comparison of turnout distributions across regions\",\n    x = \"Region\",\n    y = \"Voter turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Display plots\ngrid.arrange(p3, p4, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nInterpretation Guide\n\nPositive Skewness (&gt; 0): Distribution has a longer right tail - most values are concentrated on the left side\nNegative Skewness (&lt; 0): Distribution has a longer left tail - most values are concentrated on the right side\nZero Skewness: Distribution is approximately symmetric - values are evenly distributed around the mean\n\n\n\n\nKurtosis\n\nDefinition\nKurtosis measures the “tailedness” of a distribution, indicating the presence of extreme values compared to a normal distribution.\n\n\nMathematical Expression\nK = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\n\n\nSimplified Numerical Example\n\n# Three example datasets with different levels of kurtosis\n# 1. Leptokurtic distribution (high kurtosis, \"heavy tails\")\nleptokurtic_data &lt;- c(\n  rnorm(80, mean = 50, sd = 5),  # Most data clustered around the mean\n  c(20, 25, 30, 70, 75, 80)      # A few extreme values\n)\n\n# 2. Platykurtic distribution (low kurtosis, \"flat\")\nplatykurtic_data &lt;- c(\n  runif(50, min = 30, max = 70)  # Uniform distribution of values\n)\n\n# 3. Mesokurtic distribution (normal kurtosis)\nmesokurtic_data &lt;- rnorm(50, mean = 50, sd = 10)\n\n# Calculate kurtosis\nkurtosis_lepto &lt;- kurtosis(leptokurtic_data)\nkurtosis_platy &lt;- kurtosis(platykurtic_data)\nkurtosis_meso &lt;- kurtosis(mesokurtic_data)\n\n# Summary of results\nkurtosis_data &lt;- data.frame(\n  \"Distribution Type\" = c(\"Leptokurtic\", \"Platykurtic\", \"Mesokurtic\"),\n  \"Kurtosis value\" = round(c(kurtosis_lepto, kurtosis_platy, kurtosis_meso), 3),\n  \"Interpretation\" = c(\n    \"Many values near the mean, but also more extreme values\",\n    \"Values more uniformly distributed - flat distribution\",\n    \"Similar to normal distribution\"\n  )\n)\n\n# Display table\nkurtosis_data\n\n  Distribution.Type Kurtosis.value\n1       Leptokurtic           7.39\n2       Platykurtic           1.85\n3        Mesokurtic           2.25\n                                           Interpretation\n1 Many values near the mean, but also more extreme values\n2   Values more uniformly distributed - flat distribution\n3                          Similar to normal distribution\n\n\n\n\nVisualizations of Kurtosis Levels\n\n# Create a data frame for all sets\ndf_kurtosis &lt;- rbind(\n  data.frame(value = leptokurtic_data, type = \"Leptokurtic (K &gt; 3)\", \n             kurtosis = round(kurtosis_lepto, 2)),\n  data.frame(value = platykurtic_data, type = \"Platykurtic (K &lt; 3)\", \n             kurtosis = round(kurtosis_platy, 2)),\n  data.frame(value = mesokurtic_data, type = \"Mesokurtic (K ≈ 3)\", \n             kurtosis = round(kurtosis_meso, 2))\n)\n\n# Histograms for three types of kurtosis\np5 &lt;- ggplot(df_kurtosis, aes(x = value)) +\n  geom_histogram(bins = 15, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~type, scales = \"free_y\") +\n  geom_text(data = unique(df_kurtosis[, c(\"type\", \"kurtosis\")]),\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtosis)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histograms showing different levels of kurtosis\",\n    x = \"Value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n# Box plots\np6 &lt;- ggplot(df_kurtosis, aes(x = type, y = value, fill = type)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"lightgreen\", \"lightsalmon\", \"skyblue\")) +\n  labs(\n    title = \"Box plots for different levels of kurtosis\",\n    x = \"Distribution type\",\n    y = \"Value\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Display plots\ngrid.arrange(p5, p6, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nExample: Parliamentary Voting Analysis\n\n# Generate three datasets reflecting different levels of kurtosis\nset.seed(456)\n\n# 1. Leptokurtic distribution - typical for votes with strong party discipline\nlepto_voting &lt;- c(\n  rnorm(150, mean = 75, sd = 3),  # Most votes with high agreement\n  c(20, 25, 30, 35, 40, 95, 96, 97, 98, 99)  # A few outlier votes\n)\n\n# 2. Platykurtic distribution - typical for controversial votes\nplaty_voting &lt;- c(\n  runif(80, min = 40, max = 60),  # Votes with moderate agreement\n  runif(80, min = 60, max = 80)   # Votes with higher agreement\n)\n\n# 3. Mesokurtic distribution - typical for normal votes\nmeso_voting &lt;- rnorm(160, mean = 65, sd = 10)\n\n# Create data frame\ndf_voting &lt;- rbind(\n  data.frame(agreement = lepto_voting, bill_type = \"Bills A: Leptokurtic\"),\n  data.frame(agreement = platy_voting, bill_type = \"Bills B: Platykurtic\"),\n  data.frame(agreement = meso_voting, bill_type = \"Bills C: Mesokurtic\")\n)\n\n# Calculate kurtosis for each bill type\nbill_kurtosis &lt;- df_voting %&gt;%\n  group_by(bill_type) %&gt;%\n  summarise(kurtosis = round(kurtosis(agreement), 2))\n\n# Histogram of voting agreement\np7 &lt;- ggplot(df_voting, aes(x = agreement)) +\n  geom_histogram(bins = 20, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~bill_type, ncol = 1) +\n  geom_text(data = bill_kurtosis,\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtosis)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Voting agreement for different types of bills\",\n    subtitle = \"Showing three levels of kurtosis\",\n    x = \"Voting agreement index (%)\",\n    y = \"Number of votes\"\n  ) +\n  theme_minimal()\n\n# Box plot\np8 &lt;- ggplot(df_voting, aes(x = bill_type, y = agreement, fill = bill_type)) +\n  geom_boxplot() +\n  labs(\n    title = \"Comparison of voting agreement distributions\",\n    x = \"Bill type\",\n    y = \"Voting agreement index (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Display plots\ngrid.arrange(p7, p8, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nInterpretation Guide\n\nLeptokurtic (K &gt; 3): “Slender” distribution with heavy tails - more extreme values than in a normal distribution\nPlatykurtic (K &lt; 3): “Flat” distribution - fewer extreme values than in a normal distribution\nMesokurtic (K ≈ 3): Distribution similar to normal in terms of extreme values",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "href": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.10 Exercise 1. Center and dispersion of data",
    "text": "9.10 Exercise 1. Center and dispersion of data\n\nData\nWe have salary data (in thousands of euros) from two small European companies:\n\n\n\nIndex\nCompany X\nCompany Y\n\n\n\n\n1\n2\n3\n\n\n2\n2\n3\n\n\n3\n2\n4\n\n\n4\n3\n4\n\n\n5\n3\n4\n\n\n6\n3\n4\n\n\n7\n3\n4\n\n\n8\n3\n4\n\n\n9\n3\n5\n\n\n10\n4\n5\n\n\n11\n4\n5\n\n\n12\n4\n5\n\n\n13\n4\n5\n\n\n14\n4\n5\n\n\n15\n5\n6\n\n\n16\n5\n6\n\n\n17\n5\n6\n\n\n18\n5\n7\n\n\n19\n20\n7\n\n\n20\n35\n8\n\n\n\nThis table presents the data for both Company X and Company Y side by side, with an index column for easy reference.\n\n\nMeasures of Central Tendency\n\nMean\nThe mean is the average of all values in a dataset.\nFormula: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\nMożna też zapisać ten wzór w postaci:\n\\bar{x} = \\frac{\\sum_{i=1}^{k} x_i f_i}{n}\ngdzie f_i to częstość bezwzględna (liczba wystąpień, waga bezwzględna) i-tej wartości, a k to liczba różnych wartości cechy (liczba wartości wyróżnionych).\nZ użyciem częstości względnych:\n\\bar{x} = \\sum_{i=1}^{k} x_i p_i\ngdzie p_i to częstość względna (frakcja, waga znormalizowana) i-tej wartości, a k to liczba różnych wartości cechy (liczba wartości wyróżnionych).\n\nManual Calculation for Company X\n\n\n\nValue (x_i)\nFrequency (f_i)\nx_i \\cdot f_i\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nTotal\nn = 20\nSum = 119\n\n\n\n\\bar{x} = \\frac{119}{20} = 5.95\n\n\nManual Calculation for Company Y\n\n\n\nValue (x_i)\nFrequency (f_i)\nx_i \\cdot f_i\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nTotal\nn = 20\nSum = 100\n\n\n\n\\bar{y} = \\frac{100}{20} = 5\n\n\nR Verification\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\nMedian\nThe median is the middle value when the data is ordered.\n\nManual Calculation for Company X\nOrdered data: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\frac{4 + 4}{2} = 4\n\n\nManual Calculation for Company Y\nOrdered data: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\frac{5 + 5}{2} = 5\n\n\nR Verification\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\nMode\nThe mode is the most frequent value in the dataset.\nFor Company X, the mode is 3 (appears 6 times). For Company Y, there are two modes: 4 and 5 (both appear 6 times).\n\n# Function to calculate mode\nget_mode &lt;- function(x) {\n  unique_x &lt;- unique(x)\n  unique_x[which.max(tabulate(match(x, unique_x)))]\n}\n\nget_mode(X)\n\n[1] 3\n\nget_mode(Y)\n\n[1] 4\n\n\n\n\n\nMeasures of Dispersion\n\nVariance\nThe variance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\nPoprawka Bessela jest stosowana przy obliczaniu wariancji z próby, aby uzyskać nieobciążony estymator wariancji populacji. W standardowym wzorze na wariancję z próby dzielimy przez (n-1) zamiast przez n.\nModyfikacje wzoru dla danych pogrupowanych (szereg częstości):\nMożna też zapisać ten wzór w postaci:\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{k} f_i(x_i - \\bar{x})^2\ngdzie f_i to częstość bezwzględna (liczba wystąpień) i-tej wartości.\nGdy w obliczeniach stosujemy częstości względne p = f_i/n, gdzie:\n\nf_i to częstość (liczba wystąpień)\nn to całkowita liczebność próby\n\nWzór na wariancję z uwzględnieniem poprawki Bessela przyjmuje postać:\ns^2 = \\frac{n}{n-1} \\sum_{i=1}^{k} p_i(x_i - \\bar{x})^2\ngdzie:\n\ns^2 to wariancja z próby\nn to liczebność próby\np_i to częstość względna i-tej wartości\nx_i to i-ta wartość cechy\n\\bar{x} to średnia arytmetyczna\nk to liczba różnych wartości cechy\n\nKluczowe jest to, że przy stosowaniu częstości względnych mnożymy całe wyrażenie przez czynnik \\frac{n}{n-1}, który wprowadza poprawkę Bessela.\n\nManual Calculation for Company X\n\n\n\n\n\n\n\n\n\n\nx_i\nf_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\nf_i(x_i - \\bar{x})^2\n\n\n\n\n2\n3\n-3.95\n15.6025\n46.8075\n\n\n3\n6\n-2.95\n8.7025\n52.215\n\n\n4\n5\n-1.95\n3.8025\n19.0125\n\n\n5\n4\n-0.95\n0.9025\n3.61\n\n\n20\n1\n14.05\n197.4025\n197.4025\n\n\n35\n1\n29.05\n843.9025\n843.9025\n\n\nTotal\n20\n\n\n1162.95\n\n\n\ns^2 = \\frac{1162.95}{19} = 61.21\n\n\nManual Calculation for Company Y\n\n\n\n\n\n\n\n\n\n\ny_i\nf_i\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\nf_i(y_i - \\bar{y})^2\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nTotal\n20\n\n\n34\n\n\n\ns^2 = \\frac{34}{19} = 1.79\n\n\nR Verification\n\nvar(X)\n\n[1] 61.2\n\nvar(Y)\n\n[1] 1.79\n\n\n\n\n\nStandard Deviation\nThe standard deviation is the square root of the variance.\nFormula: s = \\sqrt{s^2}\n\nFor Company X: s = \\sqrt{61.21} = 7.82\nFor Company Y: s = \\sqrt{1.79} = 1.34\n\n\nR Verification\n\nsd(X)\n\n[1] 7.82\n\nsd(Y)\n\n[1] 1.34\n\n\n\n\n\n\nQuartiles\nQuartiles divide the dataset into four equal parts.\n\nManual Calculation for Company X\nOrdered data: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\n\nQ1 (25th percentile): median of first 10 numbers = 3\nQ2 (50th percentile, median): 4\nQ3 (75th percentile): median of last 10 numbers = 5\n\n\n\nManual Calculation for Company Y\nOrdered data: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\n\nQ1 (25th percentile): median of first 10 numbers = 4\nQ2 (50th percentile, median): 5\nQ3 (75th percentile): median of last 10 numbers = 6\n\n\n\nR Verification\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\nIQR\n\nIQR_x = 5 - 3 = 2\nIQR_y = 6 - 4 = 2\n\n\n\n\nTukey Box Plot\nA Tukey box plot visually represents the distribution of data based on quartiles. We’ll use ggplot2 to create the plot.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Prepare the data\ndata &lt;- data.frame(\n  Company = rep(c(\"X\", \"Y\"), each = 20),\n  Salary = c(X, Y)\n)\n\n# Create the box plot\nggplot(data, aes(x = Company, y = Salary, fill = Company)) +\n  geom_boxplot() +\n  labs(title = \"Salary Distribution in Companies X and Y\",\n       x = \"Company\",\n       y = \"Salary (thousands of euros)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n# Create the box plot\nggplot(data, aes(x = Company, y = Salary, fill = Company)) +\n  geom_boxplot(outliers = F) +\n  labs(title = \"Salary Distribution in Companies X and Y\",\n       x = \"Company\",\n       y = \"Salary (thousands of euros)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\nInterpreting the Box Plot\n\nThe box represents the interquartile range (IQR) from Q1 to Q3.\nThe line inside the box is the median (Q2).\nWhiskers extend to the smallest and largest values within 1.5 * IQR.\nPoints beyond the whiskers are considered outliers.\n\n\n\n\nComparison of Results\n\n\n\nMeasure\nCompany X\nCompany Y\n\n\n\n\nMean\n5.95\n5.00\n\n\nMedian\n4\n5\n\n\nMode\n3\n4 and 5\n\n\nVariance\n61.21\n1.79\n\n\nStandard Deviation\n7.82\n1.34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\nKey Observations:\n\nCentral Tendency: Company X has a higher mean but lower median than Company Y, indicating a right-skewed distribution for Company X.\nDispersion: Company X shows much higher variance and standard deviation, suggesting greater salary disparities.\nDistribution Shape: Company Y’s salaries are more tightly clustered, while Company X has extreme values (potential outliers) that significantly affect its mean and variance.\nQuartiles: Company Y’s interquartile range (Q3 - Q1) is slightly larger, but its overall range is much smaller than Company X’s.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-2.-comparing-electoral-district-size-variation-between-countries",
    "href": "chapter5.html#exercise-2.-comparing-electoral-district-size-variation-between-countries",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.11 Exercise 2. Comparing Electoral District Size Variation Between Countries",
    "text": "9.11 Exercise 2. Comparing Electoral District Size Variation Between Countries\n\nData\nWe have electoral district size data from two countries:\n\nx &lt;- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)  # Country high variance\ny &lt;- c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13)  # Country low variance\n\nkable(data.frame(\n  \"Country X (High var.)\" = x,\n  \"Country Y (Low var.)\" = y\n))\n\n\n\n\nCountry.X..High.var..\nCountry.Y..Low.var..\n\n\n\n\n1\n8\n\n\n3\n9\n\n\n5\n9\n\n\n7\n10\n\n\n9\n10\n\n\n11\n11\n\n\n13\n11\n\n\n15\n12\n\n\n17\n12\n\n\n19\n13\n\n\n\n\n\n\n\nMeasures of Central Tendency\n\nArithmetic Mean\nFormula: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\nCalculations for Country X\n\n\n\nElement\nValue\n\n\n\n\n1\n1\n\n\n2\n3\n\n\n3\n5\n\n\n4\n7\n\n\n5\n9\n\n\n6\n11\n\n\n7\n13\n\n\n8\n15\n\n\n9\n17\n\n\n10\n19\n\n\nSum\n100\n\n\n\n\\bar{x} = \\frac{100}{10} = 10\n\nmean_x &lt;- mean(x)\nc(\"Manual\" = 10, \"R\" = mean_x)\n\nManual      R \n    10     10 \n\n\n\n\nCalculations for Country Y\n\n\n\nElement\nValue\n\n\n\n\n1\n8\n\n\n2\n9\n\n\n3\n9\n\n\n4\n10\n\n\n5\n10\n\n\n6\n11\n\n\n7\n11\n\n\n8\n12\n\n\n9\n12\n\n\n10\n13\n\n\nSum\n105\n\n\n\n\\bar{y} = \\frac{105}{10} = 10.5\n\nmean_y &lt;- mean(y)\nc(\"Manual\" = 10.5, \"R\" = mean_y)\n\nManual      R \n  10.5   10.5 \n\n\n\n\n\nMedian\nThe median is the middle value in an ordered dataset.\n\nCalculations for Country X\nOrdered data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nFor n = 10 (even number of observations): Middle positions: 5 and 6 Middle values: 9 and 11\nMedian = \\frac{9 + 11}{2} = 10\n\nmedian_x &lt;- median(x)\nc(\"Manual\" = 10, \"R\" = median_x)\n\nManual      R \n    10     10 \n\n\n\n\nCalculations for Country Y\nOrdered data: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nFor n = 10 (even number of observations): Middle positions: 5 and 6 Middle values: 10 and 11\nMedian = \\frac{10 + 11}{2} = 10.5\n\nmedian_y &lt;- median(y)\nc(\"Manual\" = 10.5, \"R\" = median_y)\n\nManual      R \n  10.5   10.5 \n\n\n\n\n\nMode\n\nCalculations for Country X\n\n\n\nValue\nFrequency\n\n\n\n\n1\n1\n\n\n3\n1\n\n\n5\n1\n\n\n7\n1\n\n\n9\n1\n\n\n11\n1\n\n\n13\n1\n\n\n15\n1\n\n\n17\n1\n\n\n19\n1\n\n\n\nConclusion: No mode (all values occur once)\n\n\nCalculations for Country Y\n\n\n\nValue\nFrequency\n\n\n\n\n8\n1\n\n\n9\n2\n\n\n10\n2\n\n\n11\n2\n\n\n12\n2\n\n\n13\n1\n\n\n\nConclusion: Four modes: 9, 10, 11, 12 (each occurs twice)\n\n# Frequency tables\ntable_x &lt;- table(x)\ntable_y &lt;- table(y)\n\nlist(\n  \"Country X\" = table_x,\n  \"Country Y\" = table_y\n)\n\n$`Country X`\nx\n 1  3  5  7  9 11 13 15 17 19 \n 1  1  1  1  1  1  1  1  1  1 \n\n$`Country Y`\ny\n 8  9 10 11 12 13 \n 1  2  2  2  2  1 \n\n\n\n\n\nVariance\nVariance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\nCalculations for Country X\n\n\n\nx_i\n(x_i - \\bar{x})\n(x_i - \\bar{x})^2\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSum\n\n330\n\n\n\ns^2_X = \\frac{330}{9} = 36.67\n\nvar_x &lt;- var(x)\nc(\"Manual\" = 36.67, \"R\" = var_x)\n\nManual      R \n 36.67  36.67 \n\n\n\n\nCalculations for Country Y\n\n\n\nx_i\n(y_i - \\bar{y})\n(y_i - \\bar{y})^2\n\n\n\n\n8\n-2.5\n6.25\n\n\n9\n-1.5\n2.25\n\n\n9\n-1.5\n2.25\n\n\n10\n-0.5\n0.25\n\n\n10\n-0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n12\n1.5\n2.25\n\n\n12\n1.5\n2.25\n\n\n13\n2.5\n6.25\n\n\nSum\n\n22.5\n\n\n\ns^2_Y = \\frac{22.5}{9} = 2.5\n\nvar_y &lt;- var(y)\nc(\"Manual\" = 2.5, \"R\" = var_y)\n\nManual      R \n   2.5    2.5 \n\n\n\n\n\nStandard Deviation\nStandard deviation is the square root of variance. It measures variability in the same units as the data.\nFormula: s = \\sqrt{s^2}\n\nCalculations for Country X\nUsing previously calculated variance: s^2_X = 36.67\nCalculate square root: s_X = \\sqrt{36.67} \\approx 6.06\n\n\n\nStep\nCalculation\nResult\n\n\n\n\n1. Variance\ns^2_X\n36.67\n\n\n2. Square root\n\\sqrt{36.67}\n6.06\n\n\n\n\nsd_x &lt;- sd(x)\nc(\"Manual\" = 6.06, \"R\" = sd_x)\n\nManual      R \n 6.060  6.055 \n\n\n\n\nCalculations for Country Y\nUsing previously calculated variance: s^2_Y = 2.5\nCalculate square root: s_Y = \\sqrt{2.5} \\approx 1.58\n\n\n\nStep\nCalculation\nResult\n\n\n\n\n1. Variance\ns^2_Y\n2.5\n\n\n2. Square root\n\\sqrt{2.5}\n1.58\n\n\n\n\nsd_y &lt;- sd(y)\nc(\"Manual\" = 1.58, \"R\" = sd_y)\n\nManual      R \n 1.580  1.581 \n\n\nInterpretation:\n\nCountry X: Average deviation from the mean is about 6 seats\nCountry Y: Average deviation from the mean is about 1.6 seats\n\n\n\n\n\nCoefficient of Variation (CV)\nThe coefficient of variation is the ratio of standard deviation to mean, expressed as a percentage.\nFormula: CV = \\frac{s}{\\bar{x}} \\times 100\\%\n\nCalculations for Country X\nCV_X = \\frac{6.06}{10} \\times 100\\% = 60.6\\%\n\n\n\nComponent\nValue\n\n\n\n\nStandard deviation (s)\n6.06\n\n\nMean (\\bar{x})\n10\n\n\nCV\n60.6%\n\n\n\n\ncv_x &lt;- sd(x) / mean(x) * 100\nc(\"Manual\" = 60.6, \"R\" = cv_x)\n\nManual      R \n 60.60  60.55 \n\n\n\n\nCalculations for Country Y\nCV_Y = \\frac{1.58}{10.5} \\times 100\\% = 15.0\\%\n\n\n\nComponent\nValue\n\n\n\n\nStandard deviation (s)\n1.58\n\n\nMean (\\bar{x})\n10.5\n\n\nCV\n15.0%\n\n\n\n\ncv_y &lt;- sd(y) / mean(y) * 100\nc(\"Manual\" = 15.0, \"R\" = cv_y)\n\nManual      R \n 15.00  15.06 \n\n\n\n\n\nQuartiles and Interquartile Range (IQR)\n\nMethods for Calculating Quartiles\nThere are different methods for calculating quartiles. In our manual calculations, we’ll use the median-excluding method:\n\nSplit the series at the median\nMedian is not included in quartile calculations\nCalculate median of each part - these will be Q1 and Q3 respectively\n\n\n\nCalculations for Country X\nOrdered data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMedian = 10 (not included in quartile calculations)\nLower half: 1, 3, 5, 7, 9 Q1 = median of lower half = 5\nUpper half: 11, 13, 15, 17, 19 Q3 = median of upper half = 15\nIQR = Q3 - Q1 = 15 - 5 = 10\n\n\nCalculations for Country Y\nOrdered data: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMedian = 10.5 (not included in quartile calculations)\nLower half: 8, 9, 9, 10, 10 Q1 = median of lower half = 9\nUpper half: 11, 11, 12, 12, 13 Q3 = median of upper half = 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n# Comparison of different quartile calculation methods in R\nmethods_comparison &lt;- data.frame(\n  Method = c(\"Manual (excl. median)\", \n             \"R type=1\", \"R type=2\", \"R type=7 (default)\"),\n  \"Q1 Country X\" = c(5, \n                    quantile(x, 0.25, type=1),\n                    quantile(x, 0.25, type=2),\n                    quantile(x, 0.25, type=7)),\n  \"Q3 Country X\" = c(15,\n                    quantile(x, 0.75, type=1),\n                    quantile(x, 0.75, type=2),\n                    quantile(x, 0.75, type=7)),\n  \"Q1 Country Y\" = c(9,\n                    quantile(y, 0.25, type=1),\n                    quantile(y, 0.25, type=2),\n                    quantile(y, 0.25, type=7)),\n  \"Q3 Country Y\" = c(12,\n                    quantile(y, 0.75, type=1),\n                    quantile(y, 0.75, type=2),\n                    quantile(y, 0.75, type=7))\n)\n\nkable(methods_comparison, digits = 2,\n      caption = \"Comparison of different quartile calculation methods\")\n\n\nComparison of different quartile calculation methods\n\n\n\n\n\n\n\n\n\nMethod\nQ1.Country.X\nQ3.Country.X\nQ1.Country.Y\nQ3.Country.Y\n\n\n\n\nManual (excl. median)\n5.0\n15.0\n9.00\n12.00\n\n\nR type=1\n5.0\n15.0\n9.00\n12.00\n\n\nR type=2\n5.0\n15.0\n9.00\n12.00\n\n\nR type=7 (default)\n5.5\n14.5\n9.25\n11.75\n\n\n\n\n\n\n\nExplanation of Different Quartile Calculation Methods\n\nManual method (excluding median):\n\nSplits data into two parts\nExcludes median\nFinds median of each part\n\nR type=1:\n\nFirst method in R\nUses whole positions\nNo interpolation\n\nR type=2:\n\nSecond method in R\nUses whole positions\nInterpolates when position is not whole\n\nR type=7 (default):\n\nDefault method in R\nUses quantile()[5] from SAS\nInterpolates according to Hyndman and Fan method\n\n\n\n\n\nResults Comparison\n\nsummary_df &lt;- data.frame(\n  Measure = c(\"Mean\", \"Median\", \"Mode\", \"Range\", \"Variance\", \n              \"Std. Dev.\", \"Q1\", \"Q3\", \"IQR\", \"CV (%)\"),\n  \"Country X\" = c(10, 10, \"none\", 18, 36.67, 6.06, 5, 15, 10, 60.6),\n  \"Country Y\" = c(10.5, 10.5, \"9,10,11,12\", 5, 2.5, 1.58, 9, 12, 3, 15.0)\n)\n\nkable(summary_df, \n      caption = \"Summary of all statistical measures\",\n      align = c('l', 'r', 'r'))\n\n\nSummary of all statistical measures\n\n\nMeasure\nCountry.X\nCountry.Y\n\n\n\n\nMean\n10\n10.5\n\n\nMedian\n10\n10.5\n\n\nMode\nnone\n9,10,11,12\n\n\nRange\n18\n5\n\n\nVariance\n36.67\n2.5\n\n\nStd. Dev.\n6.06\n1.58\n\n\nQ1\n5\n9\n\n\nQ3\n15\n12\n\n\nIQR\n10\n3\n\n\nCV (%)\n60.6\n15\n\n\n\n\n\n\n\nComparison using Box Plot\n\ndf_long &lt;- data.frame(\n  country = rep(c(\"X\", \"Y\"), each = 10),\n  size = c(x, y)\n)\n\n# Basic plot\np &lt;- ggplot(df_long, aes(x = country, y = size, fill = country)) +\n  geom_boxplot(outlier.shape = NA) +  # Disable default outlier points\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Add points with transparency\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\")) +\n  labs(\n    title = \"Comparison of Electoral District Size Variation\",\n    subtitle = paste(\"CV: Country X =\", round(cv_x, 1), \"%, Country Y =\", round(cv_y, 1), \"%\"),\n    x = \"Country\",\n    y = \"District Size\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Add quartile annotations\np + annotate(\n  \"text\", \n  x = c(1, 1, 1, 2, 2, 2), \n  y = c(max(x)+1, mean(x), min(x)-1, max(y)+1, mean(y), min(y)-1),\n  label = c(\n    paste(\"Q3 =\", quantile(x, 0.75, type=1)),\n    paste(\"M =\", median(x)),\n    paste(\"Q1 =\", quantile(x, 0.25, type=1)),\n    paste(\"Q3 =\", quantile(y, 0.75, type=1)),\n    paste(\"M =\", median(y)),\n    paste(\"Q1 =\", quantile(y, 0.25, type=1))\n  ),\n  size = 3\n)\n\n\n\n\n\n\n\n\n\n\nMethodological Notes\n\nQuartile Calculations:\n\nThe median-excluding method used may give different results than R’s default functions\nDifferences in calculation methods don’t affect overall conclusions\nAlways important to specify the method used in reports\n\nVisualization:\n\nBox plot effectively shows differences in distributions\nAdditional points show actual values\nAnnotations facilitate interpretation\n\n\n\n\nApplication Notes\n\nUsing the Analysis:\n\nAll calculations can be reproduced using the provided R code\nCode chunks are self-contained and documented\nData format requirements are clearly specified\n\nCustomization:\n\nAnalysis can be adapted for different district size datasets\nVisualization parameters can be adjusted for different presentation needs\nStatistical methods can be modified based on specific requirements\n\n\n\n\nConclusion\n\nSummary Statistics Comparison\n\n\n\nMeasure\nCountry X\nCountry Y\nRelative Difference\n\n\n\n\nMean\n10.0\n10.5\nSimilar\n\n\nMedian\n10.0\n10.5\nSimilar\n\n\nMode\nNone\nMultiple (9,10,11,12)\n-\n\n\nRange\n18\n5\n3.6× larger in X\n\n\nVariance\n36.67\n2.5\n14.7× larger in X\n\n\nIQR\n10\n3\n3.3× larger in X\n\n\nCV\n60.6%\n15.0%\n4.0× larger in X\n\n\n\n\n\nDistribution Characteristics\nCountry X:\n\nUniform distribution pattern\nNo dominant district size (no mode)\nWide range: 1 to 19 seats\nHigh variability (CV = 60.6%) - Even spread of values across range\n\nCountry Y:\n\nClustered distribution pattern\nMultiple common sizes (four modes)\nNarrow range: 8 to 13 seats\nLow variability (CV = 15.0%) - Values concentrated around mean\n\n\n\nBox Plot Interpretation\nThe box plot visualization reveals:\nStructure Elements:\n\nBox: Shows interquartile range (IQR)\nLower edge: First quartile (Q1)\nUpper edge: Third quartile (Q3)\nInternal line: Median (Q2)\nWhiskers: Extend to ±1.5 IQR - Points: Individual district sizes\n\nKey Visual Findings:\n\nBox Size:\n\n\nCountry X: Large box indicates wide spread of middle 50%\nCountry Y: Small box shows tight clustering of middle values\n\n\nWhisker Length:\n\nCountry X: Long whiskers indicate broad overall distribution\nCountry Y: Short whiskers show limited total spread\n\nPoint Distribution:\n\nCountry X: Points widely dispersed\nCountry Y: Points densely clustered\n\n\n\n\nKey Observations\n\nCentral Tendency:\n\nSimilar average district sizes\nDifferent distribution patterns\nDistinct approaches to standardization\n\nVariability Measures:\n\nAll metrics show Country X with 3-15 times more variation\nConsistent pattern across different statistical measures\nSystematic difference in district design\n\nSystem Design:\n\nCountry X: Flexible, varied approach\nCountry Y: Standardized, uniform approach\nDifferent philosophical approaches to representation\n\nRepresentative Implications:\n\nCountry X: Variable voter-to-representative ratios\nCountry Y: More consistent representation levels\nDifferent approaches to democratic representation\n\n\nThis analysis demonstrates fundamental differences in electoral system design between the two countries, with Country X adopting a more varied approach and Country Y maintaining greater uniformity in district sizes.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-3.-voter-participation-and-economic-prosperity",
    "href": "chapter5.html#exercise-3.-voter-participation-and-economic-prosperity",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.12 Exercise 3. Voter Participation and Economic Prosperity",
    "text": "9.12 Exercise 3. Voter Participation and Economic Prosperity\nAnaliza związku między dobrobytem ekonomicznym a frekwencją wyborczą w dzielnicach Amsterdamu na podstawie danych z wyborów samorządowych 2022.\n\nDane\nPróba obejmuje pięć reprezentatywnych dzielnic:\n\n\n\nDzielnica\nDochód (tys. €)\nFrekwencja (%)\n\n\n\n\nA\n50\n60\n\n\nB\n45\n56\n\n\nC\n56\n70\n\n\nD\n40\n50\n\n\nE\n60\n75\n\n\n\n\n# Wczytanie bibliotek\nlibrary(tidyverse)\n\n# Utworzenie zbioru danych\ndane &lt;- data.frame(\n  dzielnica = LETTERS[1:5],\n  dochod = c(50, 45, 56, 40, 60),\n  frekwencja = c(60, 56, 70, 50, 75)\n)\n\n\n\nCzęść 1: Statystyki opisowe\n\n# Statystyki dla dochodu\nmean(dane$dochod)\n\n[1] 50.2\n\nmedian(dane$dochod)\n\n[1] 50\n\nsd(dane$dochod)\n\n[1] 8.075\n\nrange(dane$dochod)\n\n[1] 40 60\n\n# Statystyki dla frekwencji\nmean(dane$frekwencja)\n\n[1] 62.2\n\nmedian(dane$frekwencja)\n\n[1] 60\n\nsd(dane$frekwencja)\n\n[1] 10.21\n\nrange(dane$frekwencja)\n\n[1] 50 75\n\n\n\n\nCzęść 2: Analiza korelacji\n\n# Korelacja Pearsona\ncor.test(dane$dochod, dane$frekwencja)\n\n\n    Pearson's product-moment correlation\n\ndata:  dane$dochod and dane$frekwencja\nt = 16, df = 3, p-value = 0.0005\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9117 0.9996\nsample estimates:\n   cor \n0.9942 \n\n\n\n\nCzęść 3: Model regresji OLS\n\n# Dopasowanie modelu OLS\nmodel &lt;- lm(frekwencja ~ dochod, data = dane)\n\n# Podsumowanie modelu\nsummary(model)\n\n\nCall:\nlm(formula = frekwencja ~ dochod, data = dane)\n\nResiduals:\n     1      2      3      4      5 \n-1.949  0.336  0.510  0.620  0.482 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.8965     3.9673   -0.23  0.83575    \ndochod        1.2569     0.0782   16.07  0.00052 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.26 on 3 degrees of freedom\nMultiple R-squared:  0.989, Adjusted R-squared:  0.985 \nF-statistic:  258 on 1 and 3 DF,  p-value: 0.000524\n\n\n\n\nWizualizacja\n\n# Wykres rozrzutu z linią regresji\nggplot(dane, aes(x = dochod, y = frekwencja)) +\n  geom_point(size = 4, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  geom_text(aes(label = dzielnica), vjust = -1) +\n  labs(\n    title = \"Dochód vs frekwencja wyborcza\",\n    x = \"Dochód (tys. €)\",\n    y = \"Frekwencja wyborcza (%)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nWnioski\nAnaliza wykazała silny dodatni związek między dobrobytem ekonomicznym dzielnicy a frekwencją wyborczą. Mieszkańcy dzielnic o wyższych dochodach częściej uczestniczą w wyborach samorządowych.\nUwaga: Mała liczebność próby (n=5) ogranicza możliwość generalizacji wyników.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-4.-understanding-boxplots-through-life-expectancy-data",
    "href": "chapter5.html#exercise-4.-understanding-boxplots-through-life-expectancy-data",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.13 Exercise 4. Understanding Boxplots Through Life Expectancy Data",
    "text": "9.13 Exercise 4. Understanding Boxplots Through Life Expectancy Data\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# Prepare data\ndata_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction-to-boxplots",
    "href": "chapter5.html#introduction-to-boxplots",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.14 Introduction to Boxplots",
    "text": "9.14 Introduction to Boxplots\nA boxplot (also known as a box-and-whisker plot) reveals key statistics about your data:\n\nMedian: The middle line in the box (50th percentile)\nFirst quartile (Q1): Bottom of the box (25th percentile)\nThird quartile (Q3): Top of the box (75th percentile)\nInterquartile Range (IQR): The height of the box (Q3 - Q1)\nWhiskers: Extend to the most extreme non-outlier values (Tukey’s method: 1.5 × IQR)\nOutliers: Individual points beyond the whiskers\n\n\nVisualizing Life Expectancy\n\nggplot(data_2007, aes(x = reorder(continent, lifeExp, FUN = median), y = lifeExp)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 24, \n               outlier.fill = \"red\", outlier.alpha = 0.6, outlier.size = 4) +\n  geom_jitter(width = 0.2, alpha = 0.4, color = \"darkblue\") +\n  labs(title = \"Life Expectancy by Continent (2007)\",\n       subtitle = \"Individual points show raw data; red points indicate outliers\",\n       x = \"Continent\",\n       y = \"Life Expectancy (years)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 14)\n  ) +\n  scale_y_continuous(breaks = seq(40, 85, by = 5))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-the-data",
    "href": "chapter5.html#understanding-the-data",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.15 Understanding the Data",
    "text": "9.15 Understanding the Data\n\nMedian and Distribution\nAnswer True or False:\n\n50% of African countries have life expectancy below 54 years\nThe median life expectancy in Europe is approximately 78 years\nMore than 75% of countries in Oceania have life expectancy above 74 years\n25% of Asian countries have life expectancy below 65 years\nThe middle 50% of life expectancies in Europe fall between 74 and 80 years\n\n\n\nSpread and Variation\nAnswer True or False:\n\nAsia shows the largest spread (IQR) in life expectancy\nEurope has the smallest IQR among all continents\nThe variation in Africa’s life expectancy is greater than in the Americas\nOceania shows the least variation in life expectancy\nThe range (excluding outliers) in Asia is approximately 20 years\n\n\n\nOutliers and Extremes\nAnswer True or False:\n\nAfrica has two countries with unusually low life expectancy\nThere are no outliers in Oceania’s distribution\nAsia has both high and low outliers",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#changes-over-time",
    "href": "chapter5.html#changes-over-time",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.16 Changes Over Time",
    "text": "9.16 Changes Over Time\n\ntime_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  mutate(year = factor(year))\n\nggplot(time_comparison, aes(x = continent, y = lifeExp, fill = year)) +\n  geom_boxplot(alpha = 0.7, position = \"dodge\", outlier.shape = 21,\n               outlier.alpha = 0.6) +\n  labs(title = \"Life Expectancy: 1957 vs 2007\",\n       subtitle = \"Comparing distribution changes over 50 years\",\n       x = \"Continent\",\n       y = \"Life Expectancy (years)\",\n       fill = \"Year\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(breaks = seq(30, 85, by = 5))\n\n\n\n\n\n\n\n\n\nTime Comparison Questions\nAnswer True or False:\n\nThe median life expectancy increased in all continents between 1957 and 2007\nThe variation in life expectancy (IQR) decreased in most continents over time\nAfrica showed the smallest improvement in median life expectancy\nThe spread of life expectancies in Asia decreased substantially from 1957 to 2007\nOceania maintained the highest median life expectancy in both time periods\n\n\n\nStatistical Summary\n\n# Calculate summary statistics\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    median = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    n_outliers = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Summary Statistics by Continent and Year\")\n\n\nSummary Statistics by Continent and Year\n\n\ncontinent\nyear\nmedian\nq1\nq3\niqr\nn_outliers\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n0",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#key-learning-points",
    "href": "chapter5.html#key-learning-points",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.17 Key Learning Points",
    "text": "9.17 Key Learning Points\n\nDistribution Center:\n\nMedian shows the typical life expectancy\nChanges in median reflect overall improvements\n\nSpread and Variation:\n\nIQR (box height) indicates data dispersion\nWider boxes suggest more inequality in life expectancy\n\nOutliers and Extremes:\n\nOutliers often represent countries with unique circumstances\n\nTime Comparison:\n\nShows both absolute improvements and changes in variation\nHighlights persistent regional disparities\nReveals different rates of progress across continents",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "href": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "title": "9  Fundamentals of Univariate Descriptive Statistics",
    "section": "9.18 Appendix: Summary Tables for Data Types and Applicable Statistical Measures",
    "text": "9.18 Appendix: Summary Tables for Data Types and Applicable Statistical Measures\n\nTable 1: Pros and Cons of Various Statistical Measures\n\nMeasures of Center\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nMean\n- Uses all data points- Allows for further statistical calculations- Ideal for normally distributed data\n- Sensitive to outliers- Not ideal for skewed distributions- Not meaningful for nominal data\nInterval, Ratio, some Discrete, Continuous\n\n\nMedian\n- Not affected by outliers- Good for skewed distributions- Can be used with ordinal data\n- Ignores the actual values of most data points- Less useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nMode\n- Can be used with any data type- Good for finding most common category\n- May not be unique (multimodal)- Not useful for many types of analyses- Ignores magnitude of differences between values\nAll types\n\n\n\n\n\nMeasures of Variability\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nRange\n- Simple to calculate and understand- Gives quick idea of data spread\n- Very sensitive to outliers- Ignores all data between extremes- Not useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nInterquartile Range (IQR)\n- Not affected by outliers- Good for skewed distributions\n- Ignores 50% of the data- Less intuitive than range\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nVariance\n- Uses all data points- Basis for many statistical procedures\n- Sensitive to outliers- Units are squared (less intuitive)\nInterval, Ratio, some Discrete, Continuous\n\n\nStandard Deviation\n- Uses all data points- Same units as original data- Widely used and understood\n- Sensitive to outliers- Assumes roughly normal distribution for interpretation\nInterval, Ratio, some Discrete, Continuous\n\n\nCoefficient of Variation\n- Allows comparison between datasets with different units or means\n- Can be misleading when means are close to zero- Not meaningful for data with negative values\nRatio, some Interval\n\n\n\n\n\nMeasures of Correlation/Association\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nPearson’s r\n- Measures linear relationship- Widely used and understood\n- Assumes normal distribution- Sensitive to outliers- Only captures linear relationships\nInterval, Ratio, Continuous\n\n\nSpearman’s rho\n- Can be used with ordinal data- Captures monotonic relationships- Less sensitive to outliers\n- Loses information by converting to ranks- May miss some types of relationships\nOrdinal, Interval, Ratio\n\n\nKendall’s tau\n- Can be used with ordinal data- More robust than Spearman’s for small samples- Has nice interpretation (probability of concordance)\n- Loses information by only considering order- Computationally more intensive\nOrdinal, Interval, Ratio\n\n\nChi-square\n- Can be used with nominal data- Tests independence of categorical variables\n- Requires large sample sizes- Sensitive to sample size- Doesn’t measure strength of association\nNominal, Ordinal\n\n\nCramér’s V\n- Can be used with nominal data- Provides measure of strength of association- Normalized to [0,1] range\n- Interpretation can be subjective- May overestimate association in small samples\nNominal, Ordinal\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓*\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\nNotes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html",
    "href": "rozdzial5.html",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "10.1 Wprowadzenie do Notacji Sigma (Σ)\nStatystyki opisowe są fundamentalnymi narzędziami w badaniach nauk społecznych, zapewniającymi zwięzłe podsumowanie charakterystyk danych. Pełnią kilka kluczowych funkcji:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wprowadzenie-do-notacji-sigma-σ",
    "href": "rozdzial5.html#wprowadzenie-do-notacji-sigma-σ",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "Co to jest notacja sumacyjna Sigma? Sigma (Σ) to operator matematyczny, który nakazuje nam zsumować (dodać) sekwencję wyrazów - działa jak instrukcja wykonania dodawania wszystkich elementów w określonym zakresie.\nCel: Zapewnia zwięzły sposób zapisu sum wielu podobnych wyrazów za pomocą jednego symbolu, unikając długich wyrażeń dodawania.\n\n\nPodstawowa formuła\n\nOgólna forma notacji sigma to:\n\n\\sum_{i=a}^{b} f(i)\n\nIndeks sumowania: i\nDolna granica: a\nGórna granica: b\nFunkcja: f(i)\n\n\n\nPrzykłady zastosowania notacji Sigma\n\nProsty przykład: Suma liczb naturalnych\n\nZałóżmy, że chcesz dodać pierwsze pięć dodatnich liczb całkowitych:\n\n\\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15\n\nPowyższy zapis dodaje pierwsze pięć dodatnich liczb całkowitych.\n\n\n\nSuma kwadratów\n\nZałóżmy, że chcesz zsumować kwadraty pierwszych czterech dodatnich liczb całkowitych:\n\n\\sum_{i=1}^{4} i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 1 + 4 + 9 + 16 = 30\n\nJest to suma kwadratów pierwszych czterech dodatnich liczb całkowitych.\n\n\n\nSuma wartości stałej\n\nSumowanie stałej wartości c dla n wyrazów:\n\n\\sum_{i=1}^{n} c = c + c + c + ... + c \\text{ (n razy)} = n \\cdot c\n\nPrzykład: Suma pięciu piątek:\n\n\\sum_{i=1}^{5} 5 = 5 + 5 + 5 + 5 + 5 = 5 \\cdot 5 = 25\n\n\n\nProste przykłady w kontekście statystyki\n\\sum_{i=1}^{n} x_i\n\nIndeks sumowania: i (zazwyczaj oznacza konkretną obserwację w zbiorze danych)\nDolna granica: 1 (zwykle zaczynamy od pierwszej obserwacji)\nGórna granica: n (całkowita liczba obserwacji w naszym zbiorze danych)\nWyrażenie: x_i (wartość i-tej obserwacji)\n\n\nSumowanie wartości obserwacji\n\nMamy zbiór danych: 5, 8, 12, 15, 20\nSuma wszystkich wartości:\n\n\\sum_{i=1}^{5} x_i = x_1 + x_2 + x_3 + x_4 + x_5 = 5 + 8 + 12 + 15 + 20 = 60\n\nTa suma jest kluczowym elementem przy obliczaniu średniej arytmetycznej.\n\n\n\nSuma odchyleń od średniej\n\nDla tego samego zbioru danych (5, 8, 12, 15, 20), średnia wynosi \\bar{x} = 60/5 = 12\nSuma odchyleń od średniej:\n\n\\sum_{i=1}^{5} (x_i - \\bar{x}) = (5-12) + (8-12) + (12-12) + (15-12) + (20-12) = -7 + (-4) + 0 + 3 + 8 = 0\n\nWażna obserwacja: Suma odchyleń od średniej zawsze wynosi 0, co jest podstawową właściwością średniej arytmetycznej.\n\n\n\n\nPodsumowanie\n\nNotacja Sigma (Σ) pozwala na zwięzły zapis kluczowych wzorów statystycznych\nNajważniejsze zastosowania obejmują obliczanie:\n\nŚredniej arytmetycznej\nWariancji i odchylenia standardowego\nRóżnych sum kwadratów używanych w analizie regresji\n\n\n\n\n\n\n\n\nOperatory Sumy (Σ) i Iloczynu (Π)\n\n\n\n\nOperator Sigma (Σ)\n\\sum to operator sumowania, który nakazuje nam dodać wyrazy:\n\\sum_{i=1}^{n} x_i = x_1 + x_2 + ... + x_n\ngdzie: - i to zmienna indeksowa - Dolna wartość pod Σ (tutaj i=1) to punkt początkowy - Górna wartość (tutaj n) to punkt końcowy\n\n\nOperator Pi (Π)\n\\prod to operator iloczynu, który nakazuje nam pomnożyć wyrazy:\n\\prod_{i=1}^{n} x_i = x_1 \\times x_2 \\times ... \\times x_n\ngdzie: - i to zmienna indeksowa - Dolna wartość pod Π (tutaj i=1) to punkt początkowy - Górna wartość (tutaj n) to punkt końcowy\n\n\n\n\n\n\n\n\n\nPrzykład Σ\n\n\n\n\\sum_{i=1}^{4} i = 1 + 2 + 3 + 4 = 10\n\n\n\n\n\n\n\n\nPrzykład Π\n\n\n\n\\prod_{i=1}^{4} i = 1 \\times 2 \\times 3 \\times 4 = 24\n\n\n\n\n\n\n\n\nKluczowe Różnice\n\n\n\n\nΣ oznacza wielokrotne dodawanie\nΠ oznacza wielokrotne mnożenie",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#typy-rozkładów-danych",
    "href": "rozdzial5.html#typy-rozkładów-danych",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "10.2 Typy rozkładów danych",
    "text": "10.2 Typy rozkładów danych\n\n\n\n\n\n\nImportant\n\n\n\nRozkład danych informuje o tym, jakie wartości przyjmuje zmienna i jak często.\n\n\nZrozumienie rozkładów danych jest kluczowe dla analizy i wizualizacji danych. W tym dokumencie przyjrzymy się różnym typom rozkładów i sposobom ich wizualizacji przy użyciu ggplot2 w R.\n\nRozkład normalny\nRozkład normalny, znany również jako rozkład Gaussa, jest symetryczny i ma kształt dzwonu.\n\n# Generowanie danych o rozkładzie normalnym\ndane_normalne &lt;- data.frame(x = rnorm(1000))\n\n# Wykres\nggplot(dane_normalne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład normalny\", x = \"Wartość\", y = \"Gęstość\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\nRozkład jednostajny\nW rozkładzie jednostajnym wszystkie wartości mają równe prawdopodobieństwo wystąpienia.\n\n# Generowanie danych o rozkładzie jednostajnym\ndane_jednostajne &lt;- data.frame(x = runif(1000))\n\n# Wykres\nggplot(dane_jednostajne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład jednostajny\", x = \"Wartość\", y = \"Gęstość\")\n\n\n\n\n\n\n\n\n\n\nRozkłady skośne\nRozkłady skośne są asymetryczne, z jednym ogonem dłuższym niż drugi.\n\n# Generowanie danych o rozkładzie prawoskośnym\ndane_prawoskosne &lt;- data.frame(x = rlnorm(1000))\n\n# Wykres\nggplot(dane_prawoskosne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightyellow\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład prawoskośny\", x = \"Wartość\", y = \"Gęstość\")\n\n\n\n\n\n\n\n\n\n\nRozkład bimodalny\nRozkład bimodalny ma dwa szczyty (dwie dominanty), wskazujące na dwie odrębne podgrupy w danych.\n\n# Generowanie danych bimodalnych\ndane_bimodalne &lt;- data.frame(x = c(rnorm(500, mean = -2), rnorm(500, mean = 2)))\n\n# Wykres\nggplot(dane_bimodalne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightpink\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład bimodalny\", x = \"Wartość\", y = \"Gęstość\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRozkład\nKluczowe właściwości\nPrzykłady\n\n\n\n\nSymetryczny (Normalny)\nSymetryczny, kształt dzwonu, większość wartości blisko średniej\nWzrost dorosłych w populacji, wyniki testów IQ, błędy pomiarowe, wyniki egzaminów standaryzowanych\n\n\nRównomierny (Jednostajny)\nJednakowe prawdopodobieństwo w całym zakresie\nOstatnia cyfra numeru telefonu, wybór losowego dnia tygodnia, pozycja wskazówki po zakręceniu kołem fortuny\n\n\nDwumodalny (Bimodalny)\nDwa wyraźne szczyty, sugeruje istnienie podgrup\nStruktura wieku w miastach uniwersyteckich (studenci i stali mieszkańcy), opinie na tematy silnie polaryzujące społeczeństwo, godziny natężenia ruchu drogowego (poranny i popołudniowy szczyt)\n\n\nSkośny w prawo (Prawostronnie asymetryczny)\nWydłużony “ogon” po prawej stronie, większość wartości mniejsza od średniej\nCzas oczekiwania w kolejce, czas dojazdu do pracy, wiek zawarcia pierwszego małżeństwa\n\n\nSkośny z grubym ogonem (Log-normalny)\nSilna asymetria w prawo, wartości nie mogą być ujemne, długi “gruby ogon”\nDochody osobiste, ceny mieszkań, wielkość gospodarstw domowych\n\n\nSkośny o ekstremalnym ogonie (Potęgowy)\nEkstremalna asymetria, efekt “bogaty staje się bogatszym”, brak charakterystycznej skali\nMajątek najbogatszych osób, populacja miast, liczba obserwujących w mediach społecznościowych, liczba cytowań publikacji naukowych",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wizualizacja-rozkładów-danych-rzeczywistych",
    "href": "rozdzial5.html#wizualizacja-rozkładów-danych-rzeczywistych",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "10.3 Wizualizacja rozkładów danych rzeczywistych",
    "text": "10.3 Wizualizacja rozkładów danych rzeczywistych\nUżyjemy zbioru danych palmerpenguins do wizualizacji rozkładów danych.\n\nHistogram i wykres gęstości\n\n\n\n\n\n\nUnderstanding Histograms and Density\n\n\n\n⭐ A histogram is a special graph for numerical data where:\n\nData is grouped into ranges (called “bins”)\nBars touch each other (unlike bar charts!) because the data is continuous\nEach bar’s height shows how many values fall into that range\n\nThink of density as showing how common or concentrated certain values are in your data:\n\nA higher point on a density curve (or taller bar in a histogram) means those values appear more frequently in your data\nA lower point means those values are less common\n\nJust like a crowded area has more people per space (higher density), a taller part of the graph shows values that appear more often in your dataset!\n\n\n\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład długości płetw pingwinów\", \n       x = \"Długość płetwy (mm)\", \n       y = \"Gęstość\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\nWykres pudełkowy\nWykresy pudełkowe są przydatne do porównywania rozkładów między kategoriami.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Rozkład masy ciała pingwinów według gatunku\", \n       x = \"Gatunek\", \n       y = \"Masa ciała (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nWykres skrzypcowy\nWykresy skrzypcowe łączą cechy wykresu pudełkowego i wykresu gęstości.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Rozkład masy ciała pingwinów według gatunku\", \n       x = \"Gatunek\", \n       y = \"Masa ciała (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nWykres grzbietowy\nWykresy grzbietowe są przydatne do porównywania wielu rozkładów.\n\nlibrary(ggridges)\n\nggplot(penguins, aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.6) +\n  labs(title = \"Rozkład długości płetw według gatunku pingwina\",\n       x = \"Długość płetwy (mm)\",\n       y = \"Gatunek\")\n\nPicking joint bandwidth of 2.38\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n\n\nPodsumowanie\nZrozumienie i wizualizacja rozkładów danych są kluczowe w analizie danych. ggplot2 zapewnia elastyczny i potężny zestaw narzędzi do tworzenia różnych typów wykresów rozkładów. Badając różne techniki wizualizacji, możemy uzyskać wgląd w podstawowe wzorce i charakterystyki naszych danych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wartości-odstające-outliers",
    "href": "rozdzial5.html#wartości-odstające-outliers",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "10.4 Wartości Odstające (Outliers)",
    "text": "10.4 Wartości Odstające (Outliers)\nPrzed zagłębieniem się w konkretne miary, kluczowe jest zrozumienie pojęcia wartości odstających, ponieważ mogą one znacząco wpływać na wiele statystyk opisowych.\nWartości odstające to punkty danych, które znacznie różnią się od innych obserwacji w zbiorze danych. Mogą wystąpić z powodu:\n\nBłędów pomiaru lub zapisu\nPrawdziwych ekstremalnych wartości w populacji\n\nWartości odstające mogą mieć istotny wpływ na wiele miar statystycznych, szczególnie tych opartych na średnich lub sumach kwadratów odchyleń. Dlatego ważne jest, aby:\n\nIdentyfikować wartości odstające zarówno poprzez metody statystyczne, jak i wiedzę dziedzinową\nBadać przyczyny wartości odstających\nPodejmować świadome decyzje o tym, czy włączać je do analiz, czy nie",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#symbole-stosowane-w-statystyce---podsumowanie",
    "href": "rozdzial5.html#symbole-stosowane-w-statystyce---podsumowanie",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "10.5 Symbole Stosowane w Statystyce - podsumowanie",
    "text": "10.5 Symbole Stosowane w Statystyce - podsumowanie\n\n\n\n\n\n\n\n\n\n\nMiara\nParametr Populacji\nStatystyka z Próby\nAlternatywne Oznaczenia\nUwagi\n\n\n\n\nLiczebność\nN\nn\n-\nCałkowita liczba obserwacji\n\n\nŚrednia\n\\mu\n\\bar{x}\nE(X), M\nE(X) stosowane w rachunku prawdopodobieństwa\n\n\nWariancja\n\\sigma^2\ns^2\n\\text{Var}(X), V(X)\nKwadrat odchyleń od średniej\n\n\nOdchylenie standardowe\n\\sigma\ns\n\\text{OS}, \\text{std}\nPierwiastek z wariancji\n\n\nFrakcja/Proporcja\n\\pi, P\n\\hat{p}\n\\text{fr}\nCzęstości względne\n\n\nWspółczynnik korelacji\n\\rho\nr\n\\text{kor}(x,y)\nWartości od -1 do +1\n\n\nBłąd standardowy\n\\sigma_{\\bar{x}}\ns_{\\bar{x}}\n\\text{BS}\nBłąd standardowy średniej\n\n\nSuma\n\\sum\n\\sum\n\\sum_{i=1}^n\nZ indeksowaniem\n\n\nPojedyncza obserwacja\nX_i\nx_i\n-\ni-ta obserwacja\n\n\nKowariancja\n\\sigma_{xy}\ns_{xy}\n\\text{Cov}(X,Y)\nWspólna zmienność\n\n\nMediana\n\\eta\n\\text{Me}\nM\nWartość środkowa\n\n\nRozstęp\nR\nr\n\\text{max}(X) - \\text{min}(X)\nMiara rozproszenia\n\n\nDominanta\n\\text{Mo}\n\\text{mo}\n\\text{mod}\nWartość najczęstsza\n\n\nSkośność\n\\gamma_1\ng_1\n\\text{SK}\nAsymetria rozkładu\n\n\nKurtoza\n\\gamma_2\ng_2\n\\text{KU}\nSpłaszczenie rozkładu\n\n\n\nDodatkowe ważne wzory:\n\nMomenty z próby: m_k = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^k\nMomenty populacji: \\mu_k = E[(X - \\mu)^k]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-tendencji-centralnej",
    "href": "rozdzial5.html#miary-tendencji-centralnej",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "10.6 Miary Tendencji Centralnej",
    "text": "10.6 Miary Tendencji Centralnej\nMiary tendencji centralnej mają na celu identyfikację “typowej” lub “centralnej” wartości w zbiorze danych. Trzy podstawowe miary to średnia, mediana i moda.\n\nŚrednia Arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez liczbę wartości.\nWzór: \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\nWażna Właściwość: Średnia jest punktem równowagi w danych. Suma odchyleń od średniej zawsze wynosi zero:\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0\nTa właściwość sprawia, że średnia jest użyteczna w wielu obliczeniach statystycznych.\n\n\n\n\n\n\nZrozumienie średniej jako punktu równowagi 🎯\n\n\n\nRozważmy zbiór danych X = \\{1, 2, 6, 7, 9\\} na osi liczbowej, wyobrażając go sobie jako huśtawkę:\n\n\n\nhttps://www.gastonsanchez.com/matrix4sl/mean-as-a-balancing-point.html\n\n\nŚrednia (\\mu) działa jak idealny punkt równowagi tej huśtawki. Dla naszych danych:\n\\mu = \\frac{1 + 2 + 6 + 7 + 9}{5} = 5\n\nCo się dzieje przy różnych punktach podparcia? 🤔\n\nPunkt podparcia w 6 (za wysoko):\n\nLewa strona: Wartości (1, 2) są poniżej\nPrawa strona: Wartości (7, 9) są powyżej\n\\sum odległości z lewej = (6-1) + (6-2) = 9\n\\sum odległości z prawej = (7-6) + (9-6) = 4\nHuśtawka przechyla się w lewo! ⬅️ bo 9 &gt; 4\n\nPunkt podparcia w 4 (za nisko):\n\nLewa strona: Wartości (1, 2) są poniżej\nPrawa strona: Wartości (6, 7, 9) są powyżej\n\\sum odległości z lewej = (4-1) + (4-2) = 5\n\\sum odległości z prawej = (6-4) + (7-4) + (9-4) = 10\nHuśtawka przechyla się w prawo! ➡️ bo 5 &lt; 10\n\nPunkt podparcia w średniej (5) (idealna równowaga):\n\n\\sum odległości poniżej = \\sum odległości powyżej\n((5-1) + (5-2)) = ((6-5) + (7-5) + (9-5))\n7 = 7 ✨ Idealna równowaga!\n\n\nTo pokazuje, dlaczego średnia jest unikalnym punktem równowagi, gdzie:\n\\sum_{i=1}^n (x_i - \\mu) = 0\nHuśtawka zawsze będzie się przechylać, chyba że punkt podparcia zostanie umieszczony dokładnie w średniej! 🎪\n\n\n\n\n\n\n\n\n\nŚrednia jako punkt równowagi\n\n\n\nTa wizualizacja pokazuje, jak średnia arytmetyczna (5) działa jako punkt równowagi pomiędzy skupionymi punktami z lewej strony a rozproszonymi punktami z prawej strony:\nLewa strona średniej:\n\nPunkty o wartościach 2 i 3\nBlisko siebie (różnica 1 jednostka)\nOdległości od średniej: 3 i 2 jednostki\nSuma “ciążenia” = 5 jednostek\n\nPrawa strona średniej:\n\nPunkty o wartościach 6 i 9\nBardziej oddalone (różnica 3 jednostki)\nOdległości od średniej: 1 i 4 jednostki\nSuma “ciążenia” = 5 jednostek\n\nKluczowe obserwacje:\n\nŚrednia (5) jest punktem równowagi, mimo że:\n\nPunkty po lewej są skupione (2,3)\nPunkty po prawej są rozproszone (6,9)\nZielone strzałki pokazują odległości od średniej\n\nRównowaga jest zachowana ponieważ:\n\nSuma odległości się równoważy: (5-2) + (5-3) = (6-5) + (9-5)\nCałkowita suma odległości = 5 jednostek po każdej stronie\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrzykład Ręcznego Obliczenia:\nObliczmy średnią dla zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nSumuj wszystkie wartości\n2 + 4 + 4 + 5 + 5 + 7 + 9 = 36\n\n\n2\nPolicz liczbę wartości\nn = 7\n\n\n3\nPodziel sumę przez n\n36 / 7 = 5,14\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nŁatwa do obliczenia i zrozumienia\nWykorzystuje wszystkie punkty danych\n\nWady:\n\nWrażliwa na wartości odstające\nMoże nie być dobrą miarą dla silnie asymetrycznych rozkładów danych\n\n\n\nMediana\nMediana to środkowa wartość, gdy dane są uporządkowane.\nPrzykład Ręcznego Obliczenia:\nUżywając tego samego zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nWynik\n\n\n\n\n1\nUporządkuj dane\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nZnajdź środkową wartość\n5\n\n\n\nDla parzystej liczby wartości, weź średnią z dwóch środkowych wartości.\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmedian(dane)\n\n[1] 5\n\n\nZalety:\n\nNie jest zniekształcona przez skrajne wartości odstające (outliers)\nLepsza dla rozkładów skośnych\n\nWady:\n\nNie wykorzystuje wszystkich punktów danych\n\n\n\n\n\n\n\nWarning\n\n\n\nJak znaleźć pozycję mediany w zbiorze danych:\n\nNajpierw posortuj dane rosnąco\nGdy n jest nieparzyste:\n\nPozycja mediany = \\frac{n + 1}{2}\n\nGdy n jest parzyste:\n\nPierwsza pozycja mediany = \\frac{n}{2}\nDruga pozycja mediany = \\frac{n}{2} + 1\nMediana = \\frac{\\text{wartość na pozycji }\\frac{n}{2} + \\text{wartość na pozycji }(\\frac{n}{2}+1)}{2}\n\n\nPrzykłady:\n\nNieparzyste n=7: pozycja = \\frac{7+1}{2} = 4-ta wartość\nParzyste n=8: pozycje = \\frac{8}{2} = 4-ta i 4+1 = 5-ta wartość\n\n\n\n\n\nModa (Dominanta)\nModa to najczęściej występująca wartość.\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nWartość\nCzęstość\n\n\n\n\n2\n1\n\n\n4\n2\n\n\n5\n2\n\n\n7\n1\n\n\n9\n1\n\n\n\nModa to 4 i 5 (rozkład bimodalny).\nObliczenie w R:\n\nlibrary(modeest)\nmfv(dane)  # Najczęściej występująca wartość\n\n[1] 4 5\n\n\nZalety:\n\nJedyna miara tendencji centralnej dla danych nominalnych\nMoże identyfikować wiele punktów szczytowych (dominujących) w danych\n\nWady:\n\nNie zawsze jednoznacznie zdefiniowana\nNie jest odpowiednia dla danych ciągłych\n\n\n\nŚrednia (arytmetyczna) Ważona (*)\nŚrednia ważona jest używana, gdy niektóre punkty danych są ważniejsze niż inne. Występują dwa typy średnich ważonych: z wagami nienormalizowanymi i z wagami znormalizowanymi.\n\nŚrednia Ważona z Wagami Nienormalizowanymi\nJest to standardowa forma średniej ważonej, gdzie wagi mogą być dowolnymi liczbami dodatnimi reprezentującymi ważność każdego punktu danych.\nWzór: \\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\nPrzykład Obliczeń Ręcznych: Obliczmy średnią ważoną dla zbioru danych: 2, 4, 5, 7 z wagami 1, 2, 3, 1\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPomnóż każdą wartość przez jej wagę\n(2 * 1) + (4 * 2) + (5 * 3) + (7 * 1) = 2 + 8 + 15 + 7 = 32\n\n\n2\nZsumuj wagi\n1 + 2 + 3 + 1 = 7\n\n\n3\nPodziel wynik z kroku 1 przez wynik z kroku 2\n32 / 7 = 4.57\n\n\n\nObliczenia w R:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\nŚrednia Ważona z Wagami Znormalizowanymi (Ułamki)\nW tym przypadku wagi są ułamkami sumującymi się do 1, reprezentującymi proporcję ważności dla każdego punktu danych.\nWzór: \\bar{x}_w = \\sum_{i=1}^n w_i x_i, gdzie \\sum_{i=1}^n w_i = 1\nPrzykład Obliczeń Ręcznych:\nObliczmy średnią ważoną dla zbioru danych: 2, 4, 5, 7 z wagami znormalizowanymi 0.1, 0.3, 0.4, 0.2\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPomnóż każdą wartość przez jej wagę\n(2 * 0.1) + (4 * 0.3) + (5 * 0.4) + (7 * 0.2)\n\n\n2\nZsumuj wyniki\n0.2 + 1.2 + 2.0 + 1.4 = 4.8\n\n\n\nObliczenia w R:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Uwaga: sumują się do 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nZalety Średnich Ważonych:\n\nUwzględniają różną ważność punktów danych\n\nWady Średnich Ważonych:\n\nWymagają uzasadnienia dla wag\nMogą być niewłaściwie wykorzystane w celu manipulacji wynikami\nMogą być mniej intuicyjne w interpretacji niż prosta średnia arytmetyczna",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-zmienności-rozproszenia",
    "href": "rozdzial5.html#miary-zmienności-rozproszenia",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "10.7 Miary Zmienności (Rozproszenia)",
    "text": "10.7 Miary Zmienności (Rozproszenia)\nTe miary opisują, jak bardzo rozproszone są dane.\n\n\n\n\n\n\nZrozumienie Wariancji\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.1: Trzy wykresy punktowe pokazujące rosnącą wariancję przy stałej średniej\n\n\n\n\n\nPowyższe trzy wykresy punktowe pokazują, w jaki sposób wariancja mierzy rozproszenie danych wokół wartości centralnej:\n\nWszystkie rozkłady mają tę samą średnią (μ = 10), oznaczoną linią przerywaną\nMała Wariancja (σ² = 1): Punkty są skupione blisko średniej\nŚrednia Wariancja (σ² = 4): Punkty wykazują umiarkowane rozproszenie\nDuża Wariancja (σ² = 9): Punkty są szeroko rozproszone wokół średniej\n\n\n\n\n\n\n\n\n\nRóżne Poziomy Zmienności\n\n\n\n\n\n\n\n\n\n\n\n\nTa wizualizacja przedstawia trzy rozkłady normalne o tej samej średniej (μ = 10), ale różnych poziomach zmienności:\n\nMała zmienność (σ = 0.5)\n\nPunkty danych grupują się ściśle wokół średniej\nKrzywa gęstości jest wysoka i wąska\nWiększość obserwacji mieści się w przedziale ±0.5 jednostki (odchylenia stand.) od średniej\n\nŚrednia zmienność (σ = 2.0)\n\nPunkty danych są bardziej rozproszone wokół średniej\nKrzywa gęstości jest niższa i szersza\nWiększość obserwacji mieści się w przedziale ±2 jednostki od średniej\n\nDuża zmienność (σ = 4.0)\n\nPunkty danych są szeroko rozproszone wokół średniej\nKrzywa gęstości jest znacznie bardziej płaska i szeroka\nWiększość obserwacji mieści się w przedziale ±4 jednostki od średniej\n\n\nZwróć uwagę, jak odchylenie standardowe (σ) bezpośrednio powiązane jest z rozproszeniem rozkładu - większe wartości σ wskazują na większą zmienność danych, podczas gdy mniejsze wartości oznaczają, że punkty danych mają tendencję do grupowania się bliżej średniej.\n\n\n\nRozstęp\nRozstęp to różnica między wartością maksymalną a minimalną.\nWzór: R = x_{max} - x_{min}\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nZnajdź wartość maksymalną\n9\n\n\n2\nZnajdź wartość minimalną\n2\n\n\n3\nOdejmij minimum od maksimum\n9 - 2 = 7\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nrange(dane)\n\n[1] 2 9\n\nmax(dane) - min(dane)\n\n[1] 7\n\n\nZalety:\n\nProsty do obliczenia i zrozumienia\nSzybka informacja o ogólnym rozproszeniu danych\n\nWady:\n\nBardzo wrażliwy na wartości odstające\nNie dostarcza informacji o rozkładzie między skrajnościami\n\n\n\nRozstęp Międzykwartylowy (IQR)\nIQR to różnica między 75. a 25. percentylem (3. a 1. kwartylem).\nWzór: IQR = Q_3 - Q_1\nAby znaleźć kwartyle ręcznie:\n\nDla nieparzystej liczby wartości:\n\nQ2 (mediana) to środkowa wartość\nQ1 to mediana dolnej połowy (wyłączając medianę dla wszystkich obserwacji)\nQ3 to mediana górnej połowy (wyłączając medianę dla wszystkich obserwacji)\n\nDla parzystej liczby wartości:\n\nQ2 to średnia z dwóch środkowych wartości\nQ1 to mediana dolnej połowy (wyłączając medianę dla wszystkich obserwacji)\nQ3 to mediana górnej połowy (wyłączając medianę dla wszystkich obserwacji)\n\n\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nUporządkuj dane\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nZnajdź Q2 (medianę)\n5\n\n\n3\nZnajdź Q1 (medianę dolnej połowy)\n4\n\n\n4\nZnajdź Q3 (medianę górnej połowy)\n7\n\n\n5\nOblicz IQR\nQ3 - Q1 = 7 - 4 = 3\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nprint(dane)\n\n[1] 2 4 4 5 5 7 9\n\nquantile(dane, type = 1)\n\n  0%  25%  50%  75% 100% \n   2    4    5    7    9 \n\nIQR(dane, type = 1)\n\n[1] 3\n\n\nZalety:\n\nOdporny na wartości odstające\nDostarcza informacji o rozproszeniu środkowych 50% danych\n\nWady:\n\nIgnoruje ogony rozkładu\nMniej efektywny niż odchylenie standardowe dla rozkładów normalnych\n\n\n\nWariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n\n\n\n\n\n\nWariancja: Zrozumienie Średniego Odchylenia Kwadratowego\n\n\n\nCzym jest Wariancja? Wariancja mierzy, jak bardzo punkty danych są “rozrzucone” wokół średniej - jest średnią kwadratów odchyleń od średniej.\nWzór: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nProsty Przykład: Rozważmy liczby: 2, 4, 6, 8, 10 Średnia (\\bar{x}) = 6\n\n\n\n\n\n\n\n\n\nObliczanie Odchyleń:\n\n\n\n\n\n\n\n\n\n\n\n\nWartość\nOdchylenie od średniej\nKwadrat odchylenia\n\n\n\n\n2\n-4\n16\n\n\n4\n-2\n4\n\n\n6\n0\n0\n\n\n8\n+2\n4\n\n\n10\n+4\n16\n\n\n\nWariancja = \\frac{16 + 4 + 0 + 4 + 16}{4} = 10\nKluczowe Punkty:\n\nŚrednia służy jako punkt odniesienia (niebieska przerywana linia)\nOdchylenia pokazują odległość od średniej (czerwone kropkowane linie)\nPodniesienie do kwadratu sprawia, że wszystkie odchylenia są dodatnie (niebieskie słupki)\nWiększe odchylenia mają większy wpływ na wariancję\n\n\n\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz średnią\n\\bar{x} = 5,14\n\n\n2\nOdejmij średnią od każdej obserwacji i podnieś wynik do kwadratu\n(2 - 5,14)^2 = 9,86\n\n\n\n\n(4 - 5,14)^2 = 1,30\n\n\n\n\n(4 - 5,14)^2 = 1,30\n\n\n\n\n(5 - 5,14)^2 = 0,02\n\n\n\n\n(5 - 5,14)^2 = 0,02\n\n\n\n\n(7 - 5,14)^2 = 3,46\n\n\n\n\n(9 - 5,14)^2 = 14,90\n\n\n3\nSumuj kwadraty różnic\n30,86\n\n\n4\nPodziel przez (n-1), czyli przez liczbę obserwacji - 1\n30,86 / 6 = 5,14\n\n\n\nObliczenie w R:\n\nvar(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nWykorzystuje wszystkie punkty danych\nPodstawa dla wielu testów statystycznych*\n\nWady:\n\nJednostki są podniesione do kwadratu, co utrudnia interpretację\nWrażliwa na wartości odstające\n\n\n\n\n\n\n\nPoprawka Bessela: Dlaczego Dzielimy przez (n-1), a nie po prostu przez n\n\n\n\nGdy obliczamy odchylenia od średniej, ich suma musi wynosić zero. To matematyczny fakt: \\sum(x_i - \\bar{x}) = 0\nPomyśl o tym Tak:\nJeśli masz 5 liczb i ich średnią:\n\nPo obliczeniu 4 odchyleń od średniej\n5-te odchylenie MUSI być takie, żeby suma była zero\nNie masz tak naprawdę 5 niezależnych odchyleń\nMasz tylko 4 prawdziwie “swobodne” odchylenia\n\nProsty Przykład:\nLiczby: 2, 4, 6, 8, 10\n\nŚrednia = 6\nOdchylenia: -4, -2, 0, +2, +4\nZauważ, że sumują się do zera\nJeśli znasz dowolne 4 odchylenia, 5-te jest z góry określone!\n\nDlatego Właśnie:\n\nPrzy obliczaniu wariancji: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nDzielimy przez (n-1), a nie n\nPonieważ tylko (n-1) odchyleń jest naprawdę niezależnych\nOstatnie jest określone przez pozostałe\n\nStopnie Swobody:\n\nn = liczba obserwacji\n1 = ograniczenie (odchylenia muszą sumować się do zera)\nn-1 = stopnie swobody = liczba prawdziwie niezależnych odchyleń\n\nKiedy Stosować:\n\nPrzy obliczaniu wariancji z próby\nPrzy obliczaniu odchylenia standardowego z próby\n\nKiedy NIE Stosować:\n\nW obliczeniach dla całej populacji (gdy mamy wszystkie dane)\nPrzy obliczaniu odchylenia od ustalonej, znanej wartości parametru populacji statystycznej\n\nPamiętaj:\n\nTo nie jest tylko statystyczny trik\nOdchylenia od średniej muszą sumować się do zera\nTo ograniczenie kosztuje nas jeden stopień swobody\n\n\n\n\n\nOdchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji i mierzy przeciętne rozproszenie danych względem ich średniej arytmetycznej. W przeciwieństwie do wariancji, jest to miara mianowana i interpretowana w jednostkach bdanej zmiennej.\nWzór: s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz wariancję\ns^2 = 5,14 (z poprzedniego obliczenia)\n\n\n2\nWyciągnij pierwiastek kwadratowy\ns = \\sqrt{5,14} = 2,27\n\n\n\nObliczenie w R:\n\nsd(dane)\n\n[1] 2.267787\n\n\nZalety:\n\nW tych samych jednostkach co oryginalne dane\nSzeroko stosowane i zrozumiałe\n\nWady:\n\nNadal wrażliwe na wartości odstające\nZakłada, że dane są w przybliżeniu “normalnie” rozłożone\n\n\n\nWspółczynnik zmienności (*)\nWspółczynnik zmienności to odchylenie standardowe podzielone przez średnią arytmetyczną, często wyrażany jako wartość procentowa.\nWzór: CV = \\frac{s}{\\bar{x}} \\times 100\\%\nPrzykład obliczeń ręcznych:\nDla zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nOblicz średnią arytmetyczną\n\\bar{x} = 5,14\n\n\n2\nOblicz odchylenie standardowe\ns = 2,27\n\n\n3\nPodziel s przez średnią i pomnóż przez 100\n(2,27 / 5,14) * 100 = 44,16\\%\n\n\n\nObliczenia w R:\n\n(sd(dane) / mean(dane)) * 100\n\n[1] 44.09586\n\n\nZalety:\n- Umożliwia porównanie zmienności między zbiorami danych o różnych jednostkach lub średnich\n- Przydatny w dziedzinach takich jak finanse do oceny ryzyka\nWady:\n- Nie ma znaczenia dla danych zawierających zarówno wartości dodatnie, jak i ujemne\n- Może być mylący, gdy średnia jest bliska zeru\n\n\n\n\n\n\nOgraniczenia Współczynnika Zmienności (CV)\n\n\n\nWspółczynnik zmienności, obliczany jako (σ/μ) × 100\\%, ma dwa istotne ograniczenia:\n\nNie ma interpretacji dla danych zawierających wartości dodatnie i ujemne\n\nŚrednia może być bliska zeru ze względu na wzajemne znoszenie się wartości dodatnich i ujemnych\nPrzykład: Zbiór danych {-5, -3, 2, 6} ma średnią = 0\n\nCV = (odch. std. / 0) × 100%\nProwadzi to do dzielenia przez zero\nNawet gdy średnia nie jest dokładnie zero, CV nie reprezentuje prawdziwej względnej zmienności, gdy dane przechodzą przez zero\n\nCV zakłada naturalny punkt zerowy i sensowne proporcje między wartościami\n\n\n\nMylący gdy średnia jest bliska zeru\n\nPonieważ CV = (σ/μ) × 100\\%, gdy μ zbliża się do zera:\n\nMianownik staje się bardzo mały\nSkutkuje to ekstremalnie dużymi wartościami CV\nTe duże wartości nie reprezentują sensownie względnej zmienności\n\nPrzykład:\n\nZbiór danych A: {0.001, 0.002, 0.003} ma średnią = 0.002\nNawet małe odchylenia standardowe dadzą bardzo duże CV\nWynikający z tego duży CV może sugerować ekstremalne zróżnicowanie, gdy w rzeczywistości dane są dość skoncentrowane\n\n\n\n\nNajlepsze zastosowania\nCV jest najbardziej użyteczny dla:\n\nDanych ściśle dodatnich\nDanych mierzonych na skali ilorazowej\nDanych ze średnią znacznie powyżej zera\nPorównywania zmienności między zbiorami danych o różnych jednostkach lub skalach",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-położenia-względnego-względnej-pozycji",
    "href": "rozdzial5.html#miary-położenia-względnego-względnej-pozycji",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "10.8 Miary Położenia Względnego (Względnej Pozycji)",
    "text": "10.8 Miary Położenia Względnego (Względnej Pozycji)\nZrozumienie relatywnej (względnej) pozycji wartości w zbiorze danych.\n\nKwartyle (Q): Podstawy\nKwartyle to specjalne liczby, które dzielą uporządkowane dane na cztery równe części.\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\n\nCzym są Kwartyle?\nPierwszy Kwartyl (Q1):\n\nOddziela najniższe 25% danych od reszty\nNazywany również 25-tym percentylem\nPrzykład: Jeśli Q1 = 50 w zbiorze wyników testu, 25% uczniów uzyskało wynik poniżej 50\n\nDrugi Kwartyl (Q2):\n\nMediana - dzieli dane na pół\nNazywany również 50-tym percentylem\nPrzykład: Jeśli Q2 = 70, połowa uczniów uzyskała wynik poniżej 70\n\nTrzeci Kwartyl (Q3):\n\nOddziela najwyższe 25% danych od reszty\nNazywany również 75-tym percentylem\nPrzykład: Jeśli Q3 = 85, 75% uczniów uzyskało wynik poniżej 85\n\nZadanie 1: Kwartyle\nDane: 10, 12, 15, 15, 18, 20, 22, 25, 25 Znajdź: Q1, Q2, Q3\nRozwiązanie:\n\nQ2 (n = 9, nieparzyste)\n\nPozycja = (9 + 1)/2 = 5\nQ2 = 18\n\nQ1\n\nPozycja = (9 + 1)/4 = 2.5\nMiędzy 12 a 15\nQ1 = (12 + 15)/2 = 13.5\n\nQ3\n\nPozycja = 3(9 + 1)/4 = 7.5\nMiędzy 22 a 25\nQ3 = (22 + 25)/2 = 23.5\n\n\n\n\nJak Obliczać Kwartyle (Krok po Kroku) - Dwie Metody\nPrzeanalizujmy wyniki testów uczniów używając obu popularnych metod wyznaczania kwartyli:\nPrzykład 1: Przypadek Nieparzystej Liczby Wyników (11 wyników)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 88, 90\nKrok 1: Znajdź Q2 (medianę) - Tak samo dla obu metod\n\nPrzy n = 11 wartościach (nieparzyste)\nPozycja mediany = 2(n + 1)/4 = (n + 1)/2 = 6\nQ2 = 78\n\nKrok 2: Znajdź Q1\n\nMetoda Tukeya:\n\nSpójrz na dolną połowę: 60, 65, 70, 72, 75\nQ1 = mediana dolnej połowy = 70\n\nMetoda Interpolacji:\n\nPozycja = (n + 1)/4 = (11 + 1)/4 = 3\nQ1 = 70 (3-cia wartość)\n\n\nKrok 3: Znajdź Q3\n\nMetoda Tukeya:\n\nSpójrz na górną połowę: 80, 82, 85, 88, 90\nQ3 = mediana górnej połowy = 85\n\nMetoda Interpolacji:\n\nPozycja = 3(n + 1)/4 = 3(12)/4 = 9\nQ3 = 85 (9-ta wartość)\n\n\nPrzykład 2: Przypadek Parzystej Liczby (10 wyników)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 90\nKrok 1: Znajdź Q2 (medianę) - Tak samo dla obu metod\n\nPrzy n = 10 wartościach (parzyste)\nPozycje mediany = 5 i 6\nQ2 = (75 + 78)/2 = 76.5\n\nKrok 2: Znajdź Q1\n\nMetoda Tukeya:\n\nSpójrz na dolną połowę: 60, 65, 70, 72, 75\nQ1 = mediana dolnej połowy = 70\n\nMetoda Interpolacji:\n\nPozycja = (10 + 1)/4 = 2.75\nQ1 = 65 + 0.75(70 - 65) = 68.75\n\n\nKrok 3: Znajdź Q3\n\nMetoda Tukeya:\n\nSpójrz na górną połowę: 78, 80, 82, 85, 90\nQ3 = mediana górnej połowy = 82\n\nMetoda Interpolacji:\n\nPozycja = 3(10 + 1)/4 = 8.25\nQ3 = 82 + 0.25(85 - 82) = 82.75\n\n\nWażne Uwagi:\n\nMetoda Tukeya:\n\nNajpierw znajdź medianę (Q2)\nPodziel dane na dolną i górną połowę\nZnajdź Q1 jako medianę dolnej połowy\nZnajdź Q3 jako medianę górnej połowy\nGdy n jest nieparzyste, mediana nie jest uwzględniana w żadnej połowie\n\nMetoda Interpolacji:\n\nUżywa pozycji (n+1)/4 dla Q1 i 3(n+1)/4 dla Q3\nGdy pozycja wypada między wartościami, stosuje interpolację liniową\nNie wymaga podziału danych na połowy\n\n\n\n\n\n\n\n\nManual Construction of Tukey Boxplot\n\n\n\nStep 1: Calculate Key Components\n\nFind quartiles: Q_1, Q_2 (median), Q_3\nCalculate Interquartile Range: IQR = Q_3 - Q_1\n\nStep 2: Determine Whisker Boundaries\n\nLower fence: Q_1 - 1.5 \\times IQR\nUpper fence: Q_3 + 1.5 \\times IQR\n\nStep 3: Identify Outliers Data points are outliers if they are:\n\nBelow lower fence: x &lt; Q_1 - 1.5 \\times IQR\nAbove upper fence: x &gt; Q_3 + 1.5 \\times IQR\n\nExample: Given data: 2, 4, 6, 8, 9, 10, 11, 12, 14, 16, 50\n\nFind quartiles:\n\nQ_1 = 6\nQ_2 = 10\nQ_3 = 14\n\nCalculate IQR:\n\nIQR = 14 - 6 = 8\n\nCalculate fences:\n\nLower: 6 - (1.5 \\times 8) = -6\nUpper: 14 + (1.5 \\times 8) = 26\n\nIdentify outliers:\n\n50 &gt; 26, therefore 50 is an outlier\n\n\nGraphical Elements:\n\nBox: Draw from Q_1 to Q_3\nLine inside box: Draw at Q_2\nWhiskers: Extend to most extreme non-outlier points\nPoints: Plot outliers individually beyond whiskers\n\n\n\n\n\n\nPercentyle: Bardziej Precyzyjna Miara Względnej Pozycji (*)\n\nCzym są Percentyle?\nPercentyle dają nam bardziej szczegółowy obraz, dzieląc dane na 100 równych części. W przeciwieństwie do kwartyli, percentyle używają interpolacji liniowej.\nKluczowe Punkty:\n\n25-ty percentyl równa się Q1\n50-ty percentyl równa się Q2 (mediana)\n75-ty percentyl równa się Q3\n\n\n\nObliczanie Percentyli\nWzór: P_k = \\frac{k(n+1)}{100}\nGdzie:\n\nP_k to pozycja dla k-tego percentyla\nk to percentyl, który chcemy znaleźć (1-100)\nn to liczba obserwacji\n\nPrzykład 3: Znajdowanie 60-tego Percentyla Użyjmy wyników zadań domowych uczniów: 72, 75, 78, 80, 82, 85, 88, 90, 92, 95\nKrok 1: Oblicz pozycję\n\nn = 10 wyników\nDla 60-tego percentyla: P_{60} = \\frac{60(10+1)}{100} = 6.6\n\nKrok 2: Znajdź otaczające wartości\n\nPozycja 6: wynik 85\nPozycja 7: wynik 88\n\nKrok 3: Interpoluj (ważne: percentyle używają interpolacji liniowej)\n\nMusimy przejść 0.6 drogi między 85 a 88 P_{60} = 85 + 0.6(88-85) P_{60} = 85 + 0.6(3) P_{60} = 85 + 1.8 = 86.8\n\nCo to oznacza: 60% uczniów uzyskało wynik 86.8 lub niższy.\n\n\n\nRangi Percentylowe (PR) (*)\n\nCzym jest Ranga Percentylowa?\nPodczas gdy percentyle mówią nam o wartości na określonej pozycji, ranga percentylowa mówi nam, jaki procent wartości znajduje się poniżej określonego wyniku. Można to traktować jako odpowiedź na pytanie “Jaki procent klasy uzyskał wynik niższy niż ja?”\nPR = \\frac{\\text{liczba wartości poniżej } + 0.5 \\times \\text{liczba równych wartości}}{\\text{całkowita liczba wartości}} \\times 100\nPrzykład 4: Znajdowanie Rangi Percentylowej Rozważmy te wyniki egzaminu:\n65, 70, 70, 75, 75, 75, 80, 85, 85, 90\nZnajdźmy PR dla wyniku 75.\nKrok 1: Dokładnie policz\n\nWartości poniżej 75: 65, 70, 70 (3 wartości)\nWartości równe 75: 75, 75, 75 (3 wartości)\nCałkowita liczba wartości: 10\n\nKrok 2: Zastosuj wzór\nPR = \\frac{3 + 0.5(3)}{10} \\times 100 PR = \\frac{3 + 1.5}{10} \\times 100 PR = \\frac{4.5}{10} \\times 100 = 45\\%\nInterpretacja: Wynik 75 jest wyższy niż 45% wyników w klasie.\nUwaga:\nP1: “Dlaczego używamy 0.5 dla równych wartości w PR?”\nO1: Jest tak, ponieważ zakładamy, że osoby z tym samym wynikiem są równomiernie rozłożone na tej pozycji. To jak powiedzenie, że dzielą pozycję po równo.\n\n\n\n\n\n\nPodwójna Rola Mediany\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.2: Wizualizacja podwójnej roli mediany\n\n\n\n\n\nMediana pełni dwie odrębne, ale powiązane ze sobą role:\nA. Jako Miara Centrum:\n\nReprezentuje środkowy punkt danych\nRównoważy liczbę obserwacji po obu stronach\nJest odporna na wartości odstające (w przeciwieństwie do średniej arytmetycznej)\n\nB. Jako Miara Pozycji Względnej:\n\nWyznacza 50-ty percentyl\nDzieli dane na dwie równe części\nKażdą wartość można do niej odnieść:\n\nPoniżej mediany: dolne 50%\nPowyżej mediany: górne 50%\n\n\nTa podwójna natura sprawia, że mediana jest szczególnie przydatna do:\n\nOpisywania wartości typowych (tendencja centralna)\nZrozumienia pozycji w rozkładzie (pozycja względna)\nDokonywania porównań między różnymi zbiorami danych\n\n\n\n\n\n\nWykres pudełkowy\nWykresy pudełkowe (znane również jako wykresy skrzynkowe lub box-and-whisker plots) są użytecznymi narzędziami wizualizacji rozkładów danych.\n\nKonstrukcja wykresu pudełkowego Tukeya\nWykres pudełkowy został wprowadzony przez Johna Tukeya jako część jego zestawu narzędzi eksploracyjnej analizy danych. Wykres wizualizuje rozkład danych na podstawie pięciu podstawowych statystyk.\n\nPodsumowanie pięciu liczb\nWykres pudełkowy reprezentuje pięć kluczowych wartości statystycznych:\n\nMinimum: Najmniejsza wartość w zbiorze danych (z wyłączeniem wartości odstających)\nPierwszy kwartyl (Q1): 25. percentyl, poniżej którego znajduje się 25% obserwacji\nMediana (Q2): 50. percentyl, który dzieli zbiór danych na dwie równe połowy\nTrzeci kwartyl (Q3): 75. percentyl, poniżej którego znajduje się 75% obserwacji\nMaksimum: Największa wartość w zbiorze danych (z wyłączeniem wartości odstających)\n\n\n\nKomponenty wykresu pudełkowego\n\n\n\n\n\n\n\n\nFigure 10.3: Diagram wykresu pudełkowego pokazujący jego kluczowe komponenty.\n\n\n\n\n\nKomponenty wykresu pudełkowego obejmują:\n\nPudełko:\n\nReprezentuje rozstęp międzykwartylowy (IQR), zawierający środkowe 50% danych\nDolna krawędź reprezentuje Q1\nGórna krawędź reprezentuje Q3\nLinia wewnątrz pudełka reprezentuje medianę (Q2)\n\nWąsy:\n\nRozciągają się od pudełka, aby pokazać zakres danych niebędących wartościami odstającymi\nW wykresie pudełkowym Tukeya wąsy rozciągają się do 1,5 × IQR od krawędzi pudełka:\n\nDolny wąs: rozciąga się do minimalnej wartości ≥ (Q1 - 1,5 × IQR)\nGórny wąs: rozciąga się do maksymalnej wartości ≤ (Q3 + 1,5 × IQR)\n\n\nWartości odstające:\n\nPunkty, które wykraczają poza wąsy\nIndywidualnie zaznaczone jako kropki lub inne symbole\nWartości, które są &lt; (Q1 - 1,5 × IQR) lub &gt; (Q3 + 1,5 × IQR)\n\n\n\n\nKluczowe cechy do obserwacji\nInterpretując wykresy pudełkowe, zwróć uwagę na następujące cechy:\n\nTendencja centralna: Położenie linii mediany wewnątrz pudełka\nRozproszenie: Szerokość pudełka (IQR) i długość wąsów\nSkośność:\n\nDane symetryczne: mediana znajduje się w przybliżeniu na środku pudełka, wąsy mają podobną długość\nSkośność prawostronna (dodatnia): mediana jest bliżej dolnej części pudełka, górny wąs jest dłuższy\nSkośność lewostronna (ujemna): mediana jest bliżej górnej części pudełka, dolny wąs jest dłuższy\n\nWartości odstające: Obecność pojedynczych punktów poza wąsami\n\n\n\n\nStudium przypadku: Porównanie wzrostu między grupami\nZastosujmy nasze zrozumienie wykresów pudełkowych do rzeczywistego zbioru danych. Mamy pomiary wzrostu (w centymetrach) z dwóch grup, każda po 25 studentów.\n\n# Utworzenie zbioru danych\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Przekształcenie zbioru danych z formatu szerokiego na długi\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Wyświetlenie pierwszych kilku wierszy\nhead(data_height_l)\n\n  Group_number height\n1      group_1    150\n2      group_1    160\n3      group_1    165\n4      group_1    168\n5      group_1    172\n6      group_1    173\n\n\nObliczmy kilka statystyk podsumowujących dla każdej grupy:\n\n# Obliczenie statystyk podsumowujących dla każdej grupy\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Obliczenie IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Utworzenie tabeli porównawczej\nstats_table &lt;- rbind(\n  group1_stats,\n  group2_stats\n)\nrownames(stats_table) &lt;- c(\"Grupa 1\", \"Grupa 2\")\n\n# Wyświetlenie tabeli\nstats_table\n\n        Min. 1st Qu. Median Mean 3rd Qu. Max.\nGrupa 1  150     175    180  179     183  200\nGrupa 2  138     165    175  172     182  210\n\n# Wyświetlenie wartości IQR\ncat(\"IQR dla Grupy 1:\", group1_iqr, \"\\n\")\n\nIQR dla Grupy 1: 8 \n\ncat(\"IQR dla Grupy 2:\", group2_iqr, \"\\n\")\n\nIQR dla Grupy 2: 17 \n\n\n\n\nWizualizacja danych dotyczących wzrostu\nTeraz zwizualizujmy dane za pomocą wykresów pudełkowych i wykresów gęstości:\n\n# Tworzenie poziomych wykresów pudełkowych\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Rozkład wzrostu według grupy\",\n       x = \"Grupa\",\n       y = \"Wzrost (cm)\")\n\n\n\n\n\n\n\nFigure 10.4: Wykresy pudełkowe porównujące rozkłady wzrostu między grupami.\n\n\n\n\n\nAby uzupełnić nasze wykresy pudełkowe, przyjrzyjmy się również rozkładom gęstości:\n\n# Tworzenie wykresów gęstości\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Gęstość wzrostu według grupy\",\n       x = \"Wzrost (cm)\",\n       y = \"Gęstość\")\n\n\n\n\n\n\n\nFigure 10.5: Wykresy gęstości pokazujące rozkłady wzrostu dla każdej grupy.\n\n\n\n\n\n\n\nĆwiczenie z interpretacji wykresów pudełkowych\nNa podstawie powyższych wykresów pudełkowych i wykresów gęstości określ, czy każde z poniższych stwierdzeń jest Prawdziwe czy Fałszywe. Dla każdego stwierdzenia podaj krótkie wyjaśnienie oparte na dowodach z wizualizacji.\n\n\n\n\n\n\nPytania ćwiczeniowe\n\n\n\n\nStudenci z grupy 2 (G2) w badanej próbie są, średnio, wyżsi niż ci z grupy 1 (G1).\nWzrost w grupie 1 (G1) jest bardziej rozproszony/rozłożony niż w grupie 2 (G2).\nNajniższa osoba jest w grupie 2 (G2).\nOba zbiory danych mają skośność ujemną (lewostronną).\nPołowa studentów w grupie 2 (G2) ma wzrost co najmniej 175 cm.\n\n\n\n\nWskazówki do interpretacji\nOdpowiadając na te pytania, weź pod uwagę:\n\nPozycję linii mediany w każdym pudełku\nWzględne rozmiary pudełek (IQR)\nPozycje wartości minimalnych i maksymalnych\nSymetrię rozkładów (zrównoważone czy z skośnością)\nDługości wąsów\n\nDla każdego stwierdzenia ustal, czy jest Prawdziwe czy Fałszywe i podaj swoje wyjaśnienie:\n\n\n\n\n\n\nSzablon odpowiedzi\n\n\n\n\n\n\nStudenci z G2 są, średnio, wyżsi niż z G1: [Prawda/Fałsz]\n\nWyjaśnienie:\n\nWzrost G1 jest bardziej rozproszony/rozłożony: [Prawda/Fałsz]\n\nWyjaśnienie:\n\nNajniższa osoba jest w G2: [Prawda/Fałsz]\n\nWyjaśnienie:\n\nOba zbiory danych mają skośność ujemną (lewostronną): [Prawda/Fałsz]\n\nWyjaśnienie:\n\nPołowa G2 ma wzrost co najmniej 175 cm: [Prawda/Fałsz]\n\nWyjaśnienie:\n\n\n\n\n\nPrzeanalizujmy odpowiedzi na nasze pytania dotyczące interpretacji wykresów pudełkowych:\n\n\n\n\n\n\nRozwiązania\n\n\n\n\n\n\nStudenci z G2 są, średnio, wyżsi niż z G1: Fałsz\n\nWyjaśnienie: Mediana wzrostu (środkowa linia w wykresie pudełkowym) dla G1 jest wyższa niż dla G2.\n\nWzrost G1 jest bardziej rozproszony/rozłożony: Fałsz\n\nWyjaśnienie: G2 wykazuje większe rozproszenie. Jest to widoczne na wykresie pudełkowym, gdzie G2 ma większy rozstęp międzykwartylowy (IQR) wynoszący 17,5 cm w porównaniu z 9,5 cm dla G1. G2 ma również szerszy zakres od wartości minimalnej do maksymalnej.\n\nNajniższa osoba jest w G2: Prawda\n\nWyjaśnienie: Wartość minimalna w G2 wynosi 138 cm, co jest niższe niż wartość minimalna w G1 (150 cm).\n\nOba zbiory danych mają skośność ujemną (lewostronną): Prawda\n\nWyjaśnienie: W obu grupach linia mediany jest umieszczona w kierunku górnej części pudełka, a dolny wąs jest dłuższy niż górny. Wskazuje to na dłuższy ogon po lewej stronie rozkładu, co oznacza skośność ujemną.\n\nPołowa G2 ma wzrost co najmniej 175 cm: Prawda\n\nWyjaśnienie: Mediana (środkowa linia w wykresie pudełkowym) dla G2 wynosi 175 cm, co oznacza, że 50% wartości jest większych lub równych 175 cm.\n\n\n\n\n\n\n\n\nKod R\n\n# Wczytanie wymaganych pakietów\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n# Ustawienie opcji wyświetlania\noptions(scipen = 999, digits = 3)\n\n# Utworzenie zbioru danych\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Przekształcenie zbioru danych z formatu szerokiego na długi\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Wyświetlenie pierwszych kilku wierszy\nhead(data_height_l)\n\n# Obliczenie statystyk podsumowujących dla każdej grupy\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Obliczenie IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Tworzenie poziomych wykresów pudełkowych\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Rozkład wzrostu według grupy\",\n       x = \"Grupa\",\n       y = \"Wzrost (cm)\")\n\n# Tworzenie wykresów gęstości\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Gęstość wzrostu według grupy\",\n       x = \"Wzrost (cm)\",\n       y = \"Gęstość\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wykresy-pudełkowe-na-przykładzie-danych-o-długości-życia",
    "href": "rozdzial5.html#wykresy-pudełkowe-na-przykładzie-danych-o-długości-życia",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "10.9 Wykresy Pudełkowe na Przykładzie Danych o Długości Życia",
    "text": "10.9 Wykresy Pudełkowe na Przykładzie Danych o Długości Życia\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# Przygotowanie danych\ndata_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007)\n\nWykres pudełkowy (ang. box-and-whisker plot) przedstawia pięć kluczowych statystyk opisowych danych:\n\nMediana: Środkowa linia w pudełku (50. percentyl)\nPierwszy kwartyl (Q1): Dolna krawędź pudełka (25. percentyl)\nTrzeci kwartyl (Q3): Górna krawędź pudełka (75. percentyl)\nRozstęp międzykwartylowy (IQR): Wysokość pudełka (Q3 - Q1)\nWąsy: Rozciągają się do najbardziej skrajnych wartości niebędących obserwacjami odstającymi (metoda Tukeya: 1.5 × IQR)\nObserwacje odstające: Pojedyncze punkty poza wąsami\n\n\nWizualizacja Długości Życia\n\nggplot(data_2007, aes(x = reorder(continent, lifeExp, FUN = median), y = lifeExp)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 24, \n               outlier.fill = \"red\", outlier.alpha = 0.6, outlier.size = 4) +\n  geom_jitter(width = 0.2, alpha = 0.4, color = \"darkblue\") +\n  labs(title = \"Długość Życia według Kontynentów (2007)\",\n       subtitle = \"Pojedyncze punkty pokazują surowe dane; czerwone punkty oznaczają wartości odstające\",\n       x = \"Kontynent\",\n       y = \"Długość życia (w latach)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_y_continuous(breaks = seq(40, 85, by = 5))\n\n\n\n\n\n\n\n\n\n\nAnaliza Danych\n\n\nMediana i Rozkład\nOdpowiedz Prawda lub Fałsz:\n\n50% krajów afrykańskich ma długość życia poniżej 52 lat\nMediana długości życia w Europie wynosi około 78 lat\nPonad 75% krajów Oceanii ma długość życia powyżej 75 lat\n25% krajów azjatyckich ma długość życia poniżej 68 lat\nŚrodkowe 50% długości życia w Europie mieści się między 76 a 80 lat\n\n\n\nRozrzut i Zmienność\nOdpowiedz Prawda lub Fałsz:\n\nAzja wykazuje największy rozrzut (IQR) w długości życia\nEuropa ma najmniejszy IQR wśród wszystkich kontynentów\nZmienność długości życia w Afryce jest większa niż w obu Amerykach\nOceania wykazuje najmniejszą zmienność w długości życia\nWąsy dla Azji rozciągają się w przybliżeniu od 58 do 82 lat (z wyłączeniem wartości odstających)\n\n\n\nWartości Odstające i Ekstrema\nOdpowiedz Prawda lub Fałsz:\n\nAfryka ma dwa kraje z wyjątkowo niską długością życia\nW rozkładzie dla Oceanii nie ma wartości odstających\nAzja ma kilka niskich wartości odstających (poniżej 55 lat)\n\n\n\nZmiany w Czasie\n\ntime_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  mutate(year = factor(year))\n\nggplot(time_comparison, aes(x = continent, y = lifeExp, fill = year)) +\n  geom_boxplot(alpha = 0.7, position = \"dodge\", outlier.shape = 21,\n               outlier.alpha = 0.6) +\n  labs(title = \"Długość Życia: 1957 vs 2007\",\n       subtitle = \"Porównanie zmian rozkładu na przestrzeni 50 lat\",\n       x = \"Kontynent\",\n       y = \"Długość życia (w latach)\",\n       fill = \"Rok\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 14)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(breaks = seq(30, 85, by = 5))\n\n\n\n\n\n\n\n\n\n\nPytania dotyczące Zmian w Czasie\nOdpowiedz Prawda lub Fałsz:\n\nMediana długości życia wzrosła na wszystkich kontynentach między 1957 a 2007 rokiem\nZmienność długości życia (IQR) zmniejszyła się na większości kontynentów w czasie\nAfryka wykazała najmniejszą poprawę mediany długości życia\nRozrzut długości życia w Azji znacząco się zmniejszył od 1957 do 2007 roku\nOceania utrzymała najwyższą medianę długości życia w obu okresach\n\n\n\nPodsumowanie Statystyczne\n\n# Obliczenie statystyk opisowych\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    mediana = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    min = min(lifeExp),\n    max = max(lifeExp),\n    liczba_odstających = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Statystyki Opisowe według Kontynentu i Roku\")\n\n\nStatystyki Opisowe według Kontynentu i Roku\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontinent\nyear\nmediana\nq1\nq3\niqr\nmin\nmax\nliczba_odstających\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n31.6\n58.1\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n39.6\n76.4\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n40.7\n70.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n60.9\n80.7\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n30.3\n67.8\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n43.8\n82.6\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n48.1\n73.5\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n71.8\n81.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n70.3\n70.3\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n80.2\n81.2\n0\n\n\n\n\n\n\n\nNajważniejsze Wnioski\n\nCentrum Rozkładu:\n\nMediana pokazuje typową długość życia\nZmiany mediany odzwierciedlają ogólną poprawę\n\nRozrzut i Zmienność:\n\nIQR (wysokość pudełka) wskazuje na rozproszenie danych\nSzersze pudełka sugerują większe nierówności w długości życia\n\nWartości Odstające i Ekstrema:\n\nWartości odstające często reprezentują kraje o wyjątkowej sytuacji\n\nPorównanie w Czasie:\n\nPokazuje zarówno bezwzględną poprawę, jak i zmiany w wariancji\nUwydatnia utrzymujące się różnice regionalne\nUjawnia różne tempo postępu na poszczególnych kontynentach\n\n\n\n\nStatistical Summary\n\n# Calculate summary statistics\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    median = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    min = min(lifeExp),\n    max = max(lifeExp),\n    n_outliers = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Summary Statistics by Continent and Year\")\n\n\nSummary Statistics by Continent and Year\n\n\ncontinent\nyear\nmedian\nq1\nq3\niqr\nmin\nmax\nn_outliers\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n31.6\n58.1\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n39.6\n76.4\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n40.7\n70.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n60.9\n80.7\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n30.3\n67.8\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n43.8\n82.6\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n48.1\n73.5\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n71.8\n81.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n70.3\n70.3\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n80.2\n81.2\n0",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-kształtu",
    "href": "rozdzial5.html#miary-kształtu",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "10.10 Miary Kształtu",
    "text": "10.10 Miary Kształtu\n\nSkośność\n\nDefinicja\nSkośność kwantyfikuje asymetrię rozkładu danych. Wskazuje, czy dane grupują się bardziej po jednej stronie średniej niż po drugiej.\n\n\nWyrażenie Matematyczne\nSK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3 gdzie: - n to wielkość próby - x_i to i-ta obserwacja - \\bar{x} to średnia z próby - s to odchylenie standardowe z próby\n\n\nUproszczony Przykład Numeryczny\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Trzy przykładowe zestawy danych z różnymi typami skośności\n# 1. Skośność dodatnia (prawy ogon)\ndane_skosnosc_dodatnia &lt;- c(2, 3, 4, 4, 5, 5, 5, 6, 6, 7, 8, 12, 15, 20)\n# 2. Skośność ujemna (lewy ogon)\ndane_skosnosc_ujemna &lt;- c(1, 5, 10, 13, 14, 15, 16, 16, 17, 17, 18, 18, 19, 20)\n# 3. Skośność bliska zeru (symetria)\ndane_skosnosc_symetryczna &lt;- c(1, 3, 5, 7, 9, 10, 11, 12, 13, 15, 17, 19, 21)\n\n# Obliczenie skośności\nskosnosc_dodatnia &lt;- skewness(dane_skosnosc_dodatnia)\nskosnosc_ujemna &lt;- skewness(dane_skosnosc_ujemna)\nskosnosc_symetryczna &lt;- skewness(dane_skosnosc_symetryczna)\n\n# Zestawienie wyników\ndane_skosnosci &lt;- data.frame(\n  \"Typ rozkładu\" = c(\"Skośność dodatnia\", \"Skośność ujemna\", \"Rozkład symetryczny\"),\n  \"Wartość skośności\" = round(c(skosnosc_dodatnia, skosnosc_ujemna, skosnosc_symetryczna), 3),\n  \"Interpretacja\" = c(\n    \"Dłuższy prawy ogon (większość danych po lewej stronie)\",\n    \"Dłuższy lewy ogon (większość danych po prawej stronie)\",\n    \"Dane rozłożone symetrycznie\"\n  )\n)\n\n# Wyświetlenie tabeli\ndane_skosnosci\n\n         Typ.rozkładu Wartość.skośności\n1   Skośność dodatnia              1.42\n2     Skośność ujemna             -1.33\n3 Rozkład symetryczny              0.00\n                                           Interpretacja\n1 Dłuższy prawy ogon (większość danych po lewej stronie)\n2 Dłuższy lewy ogon (większość danych po prawej stronie)\n3                            Dane rozłożone symetrycznie\n\n\n\n\nWizualizacje Typów Skośności\n\n# Tworzymy ramkę danych dla wszystkich zestawów\ndf_skosnosc &lt;- rbind(\n  data.frame(wartosc = dane_skosnosc_dodatnia, typ = \"Skośność dodatnia\", \n             skosnosc = round(skosnosc_dodatnia, 2)),\n  data.frame(wartosc = dane_skosnosc_ujemna, typ = \"Skośność ujemna\", \n             skosnosc = round(skosnosc_ujemna, 2)),\n  data.frame(wartosc = dane_skosnosc_symetryczna, typ = \"Rozkład symetryczny\", \n             skosnosc = round(skosnosc_symetryczna, 2))\n)\n\n# Histogramy dla trzech typów skośności\np1 &lt;- ggplot(df_skosnosc, aes(x = wartosc)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~typ, scales = \"free_x\") +\n  geom_vline(data = df_skosnosc %&gt;% group_by(typ) %&gt;% summarise(mean = mean(wartosc)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_skosnosc %&gt;% group_by(typ) %&gt;% summarise(median = median(wartosc)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = unique(df_skosnosc[, c(\"typ\", \"skosnosc\")]),\n           aes(x = Inf, y = Inf, label = paste(\"SK =\", skosnosc)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histogramy pokazujące różne typy skośności\",\n    subtitle = \"Czerwona linia: średnia, Zielona linia: mediana\",\n    x = \"Wartość\",\n    y = \"Częstość\"\n  ) +\n  theme_minimal()\n\n# Wykresy pudełkowe\np2 &lt;- ggplot(df_skosnosc, aes(x = typ, y = wartosc, fill = typ)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"skyblue\", \"lightgreen\", \"lightsalmon\")) +\n  labs(\n    title = \"Wykresy pudełkowe dla różnych typów skośności\",\n    x = \"Typ rozkładu\",\n    y = \"Wartość\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Wyświetlenie wykresów\ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nPrzykład: Analiza Frekwencji Wyborczej\n\n# Generujemy trzy zestawy danych odzwierciedlające różne typy skośności\nset.seed(123)\n\n# 1. Skośność dodatnia - typowa dla frekwencji w regionach o niskim zaangażowaniu\nfrekwencja_dodatnia &lt;- c(\n  runif(50, min = 20, max = 30),  # Mała grupa z niską frekwencją\n  rbeta(200, shape1 = 2, shape2 = 5) * 50 + 30  # Większość wyników przesuniętych w lewo\n)\n\n# 2. Skośność ujemna - typowa dla regionów z wysokim zaangażowaniem politycznym\nfrekwencja_ujemna &lt;- c(\n  rbeta(200, shape1 = 5, shape2 = 2) * 30 + 50,  # Większość wyników przesuniętych w prawo\n  runif(50, min = 40, max = 50)  # Mała grupa z niższą frekwencją\n)\n\n# 3. Rozkład symetryczny - typowy dla regionów z równomiernym zaangażowaniem\nfrekwencja_symetryczna &lt;- rnorm(250, mean = 65, sd = 8)\n\n# Tworzymy ramkę danych\ndf_frekwencja &lt;- rbind(\n  data.frame(frekwencja = frekwencja_dodatnia, region = \"Region A: Skośność dodatnia\"),\n  data.frame(frekwencja = frekwencja_ujemna, region = \"Region B: Skośność ujemna\"),\n  data.frame(frekwencja = frekwencja_symetryczna, region = \"Region C: Rozkład symetryczny\")\n)\n\n# Obliczamy skośność dla każdego regionu\nskosnosci_regionow &lt;- df_frekwencja %&gt;%\n  group_by(region) %&gt;%\n  summarise(skosnosc = round(skewness(frekwencja), 2))\n\n# Histogram frekwencji według regionów\np3 &lt;- ggplot(df_frekwencja, aes(x = frekwencja)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~region, ncol = 1) +\n  geom_vline(data = df_frekwencja %&gt;% group_by(region) %&gt;% summarise(mean = mean(frekwencja)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_frekwencja %&gt;% group_by(region) %&gt;% summarise(median = median(frekwencja)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = skosnosci_regionow,\n           aes(x = 25, y = 20, label = paste(\"SK =\", skosnosc)),\n           size = 3.5) +\n  labs(\n    title = \"Frekwencja wyborcza w różnych regionach\",\n    subtitle = \"Pokazuje trzy rodzaje skośności\",\n    x = \"Frekwencja wyborcza (%)\",\n    y = \"Liczba obwodów\"\n  ) +\n  theme_minimal()\n\n# Wykres pudełkowy\np4 &lt;- ggplot(df_frekwencja, aes(x = region, y = frekwencja, fill = region)) +\n  geom_boxplot() +\n  labs(\n    title = \"Porównanie rozkładów frekwencji w regionach\",\n    x = \"Region\",\n    y = \"Frekwencja wyborcza (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Wyświetlenie wykresów\ngrid.arrange(p3, p4, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nPrzewodnik Interpretacji\n\nSkośność Dodatnia (&gt; 0): Rozkład ma dłuższy ogon prawy - większość wartości jest skupiona po lewej stronie\nSkośność Ujemna (&lt; 0): Rozkład ma dłuższy ogon lewy - większość wartości jest skupiona po prawej stronie\nSkośność Zero: Rozkład w przybliżeniu symetryczny - wartości rozłożone równomiernie wokół średniej\n\n\n\n\nKurtoza\n\nDefinicja\nKurtoza mierzy “ogoniastość” rozkładu, wskazując na obecność wartości ekstremalnych w porównaniu z rozkładem normalnym.\n\n\nWyrażenie Matematyczne\nK = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\n\n\nUproszczony Przykład Numeryczny\n\n# Trzy przykładowe zestawy danych z różnymi poziomami kurtozy\n# 1. Rozkład leptokurtyczny (wysoka kurtoza, \"ciężkie ogony\")\ndane_leptokurtyczne &lt;- c(\n  rnorm(80, mean = 50, sd = 5),  # Większość danych skupiona wokół średniej\n  c(20, 25, 30, 70, 75, 80)      # Kilka wartości ekstremalnych\n)\n\n# 2. Rozkład platykurtyczny (niska kurtoza, \"płaski\")\ndane_platykurtyczne &lt;- c(\n  runif(50, min = 30, max = 70)  # Równomierny rozkład wartości\n)\n\n# 3. Rozkład mezokurtyczny (normalna kurtoza)\ndane_mezokurtyczne &lt;- rnorm(50, mean = 50, sd = 10)\n\n# Obliczenie kurtozy\nkurtoza_lepto &lt;- kurtosis(dane_leptokurtyczne)\nkurtoza_platy &lt;- kurtosis(dane_platykurtyczne)\nkurtoza_mezo &lt;- kurtosis(dane_mezokurtyczne)\n\n# Zestawienie wyników\ndane_kurtozy &lt;- data.frame(\n  \"Typ rozkładu\" = c(\"Leptokurtyczny\", \"Platykurtyczny\", \"Mezokurtyczny\"),\n  \"Wartość kurtozy\" = round(c(kurtoza_lepto, kurtoza_platy, kurtoza_mezo), 3),\n  \"Interpretacja\" = c(\n    \"Wiele wartości blisko średniej, ale też więcej wartości ekstremalnych\",\n    \"Wartości rozłożone bardziej równomiernie - płaski rozkład\",\n    \"Podobny do rozkładu normalnego\"\n  )\n)\n\n# Wyświetlenie tabeli\ndane_kurtozy\n\n    Typ.rozkładu Wartość.kurtozy\n1 Leptokurtyczny            7.39\n2 Platykurtyczny            1.85\n3  Mezokurtyczny            2.25\n                                                          Interpretacja\n1 Wiele wartości blisko średniej, ale też więcej wartości ekstremalnych\n2             Wartości rozłożone bardziej równomiernie - płaski rozkład\n3                                        Podobny do rozkładu normalnego\n\n\n\n\nWizualizacje Poziomów Kurtozy\n\n# Tworzymy ramkę danych dla wszystkich zestawów\ndf_kurtoza &lt;- rbind(\n  data.frame(wartosc = dane_leptokurtyczne, typ = \"Leptokurtyczny (K &gt; 3)\", \n             kurtoza = round(kurtoza_lepto, 2)),\n  data.frame(wartosc = dane_platykurtyczne, typ = \"Platykurtyczny (K &lt; 3)\", \n             kurtoza = round(kurtoza_platy, 2)),\n  data.frame(wartosc = dane_mezokurtyczne, typ = \"Mezokurtyczny (K ≈ 3)\", \n             kurtoza = round(kurtoza_mezo, 2))\n)\n\n# Histogramy dla trzech typów kurtozy\np5 &lt;- ggplot(df_kurtoza, aes(x = wartosc)) +\n  geom_histogram(bins = 15, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~typ, scales = \"free_y\") +\n  geom_text(data = unique(df_kurtoza[, c(\"typ\", \"kurtoza\")]),\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtoza)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histogramy pokazujące różne poziomy kurtozy\",\n    x = \"Wartość\",\n    y = \"Częstość\"\n  ) +\n  theme_minimal()\n\n# Wykresy pudełkowe\np6 &lt;- ggplot(df_kurtoza, aes(x = typ, y = wartosc, fill = typ)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"lightgreen\", \"lightsalmon\", \"skyblue\")) +\n  labs(\n    title = \"Wykresy pudełkowe dla różnych poziomów kurtozy\",\n    x = \"Typ rozkładu\",\n    y = \"Wartość\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Wyświetlenie wykresów\ngrid.arrange(p5, p6, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nPrzykład: Analiza Głosowań Parlamentarnych\n\n# Generujemy trzy zestawy danych odzwierciedlające różne poziomy kurtozy\nset.seed(456)\n\n# 1. Rozkład leptokurtyczny - typowy dla głosowań z silną dyscypliną partyjną\nglosowania_lepto &lt;- c(\n  rnorm(150, mean = 75, sd = 3),  # Większość głosowań z wysoką zgodnością\n  c(20, 25, 30, 35, 40, 95, 96, 97, 98, 99)  # Kilka głosowań odstających\n)\n\n# 2. Rozkład platykurtyczny - typowy dla głosowań kontrowersyjnych\nglosowania_platy &lt;- c(\n  runif(80, min = 40, max = 60),  # Głosowania z umiarkowaną zgodnością\n  runif(80, min = 60, max = 80)   # Głosowania z wyższą zgodnością\n)\n\n# 3. Rozkład mezokurtyczny - typowy dla normalnych głosowań\nglosowania_mezo &lt;- rnorm(160, mean = 65, sd = 10)\n\n# Tworzymy ramkę danych\ndf_glosowania &lt;- rbind(\n  data.frame(zgodnosc = glosowania_lepto, typ_ustawy = \"Ustawy A: Leptokurtyczne\"),\n  data.frame(zgodnosc = glosowania_platy, typ_ustawy = \"Ustawy B: Platykurtyczne\"),\n  data.frame(zgodnosc = glosowania_mezo, typ_ustawy = \"Ustawy C: Mezokurtyczne\")\n)\n\n# Obliczamy kurtozę dla każdego typu ustaw\nkurtozy_ustaw &lt;- df_glosowania %&gt;%\n  group_by(typ_ustawy) %&gt;%\n  summarise(kurtoza = round(kurtosis(zgodnosc), 2))\n\n# Histogram zgodności głosowań\np7 &lt;- ggplot(df_glosowania, aes(x = zgodnosc)) +\n  geom_histogram(bins = 20, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~typ_ustawy, ncol = 1) +\n  geom_text(data = kurtozy_ustaw,\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtoza)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Zgodność głosowań dla różnych typów ustaw\",\n    subtitle = \"Pokazuje trzy poziomy kurtozy\",\n    x = \"Wskaźnik zgodności głosowań (%)\",\n    y = \"Liczba głosowań\"\n  ) +\n  theme_minimal()\n\n# Wykres pudełkowy\np8 &lt;- ggplot(df_glosowania, aes(x = typ_ustawy, y = zgodnosc, fill = typ_ustawy)) +\n  geom_boxplot() +\n  labs(\n    title = \"Porównanie rozkładów zgodności głosowań\",\n    x = \"Typ ustawy\",\n    y = \"Wskaźnik zgodności głosowań (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Wyświetlenie wykresów\ngrid.arrange(p7, p8, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nPrzewodnik Interpretacji\n\nLeptokurtyczny (K &gt; 3): “Wysmukły” rozkład z ciężkimi ogonami - więcej wartości skrajnych niż w rozkładzie normalnym\nPlatykurtyczny (K &lt; 3): “Płaski” rozkład - mniej wartości skrajnych niż w rozkładzie normalnym\nMezokurtyczny (K ≈ 3): Rozkład podobny do normalnego pod względem wartości ekstremalnych",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ćwiczenie-1.-porównanie-wynagrodzeń",
    "href": "rozdzial5.html#ćwiczenie-1.-porównanie-wynagrodzeń",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "10.11 Ćwiczenie 1. Porównanie wynagrodzeń",
    "text": "10.11 Ćwiczenie 1. Porównanie wynagrodzeń\n\nDane\nMamy dane o wynagrodzeniach (w tysiącach euro) z dwóch małych firm europejskich:\n\n\n\nIndex\nFirma X\nFirma Y\n\n\n\n\n1\n2\n3\n\n\n2\n2\n3\n\n\n3\n2\n4\n\n\n4\n3\n4\n\n\n5\n3\n4\n\n\n6\n3\n4\n\n\n7\n3\n4\n\n\n8\n3\n4\n\n\n9\n3\n5\n\n\n10\n4\n5\n\n\n11\n4\n5\n\n\n12\n4\n5\n\n\n13\n4\n5\n\n\n14\n4\n5\n\n\n15\n5\n6\n\n\n16\n5\n6\n\n\n17\n5\n6\n\n\n18\n5\n7\n\n\n19\n20\n7\n\n\n20\n35\n8\n\n\n\n\n\nMiary tendencji centralnej\n\nŚrednia arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez ich liczbę.\nWzór: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\nMożna też zapisać ten wzór w postaci:\n\\bar{x} = \\frac{\\sum_{i=1}^{k} x_i f_i}{n}\ngdzie f_i to częstość bezwzględna (liczba wystąpień, waga bezwzględna) i-tej wartości, a k to liczba różnych wartości cechy (liczba wartości wyróżnionych).\nZ użyciem częstości względnych:\n\\bar{x} = \\sum_{i=1}^{k} x_i p_i\ngdzie p_i to częstość względna (frakcja, waga znormalizowana) i-tej wartości, a k to liczba różnych wartości cechy (liczba wartości wyróżnionych).\n\nObliczenia ręczne dla Firmy X\n\n\n\nWartość (x_i)\nCzęstość (f_i)\nx_i \\cdot f_i\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nSuma\nn = 20\nSuma = 119\n\n\n\n\\bar{x} = \\frac{119}{20} = 5,95\n\n\nObliczenia ręczne dla Firmy Y\n\n\n\nWartość (x_i)\nCzęstość (f_i)\nx_i \\cdot f_i\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nSuma\nn = 20\nSuma = 100\n\n\n\n\\bar{y} = \\frac{100}{20} = 5\n\n\nWeryfikacja w R\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\nMediana\nMediana to wartość środkowa w uporządkowanym zbiorze danych.\n\nObliczenia ręczne dla Firmy X\nUporządkowane dane: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\nn = 20 (parzyste), więc bierzemy średnią z 10. i 11. wartości:\nMediana = \\frac{4 + 4}{2} = 4\n\n\nObliczenia ręczne dla Firmy Y\nUporządkowane dane: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\nn = 20 (parzyste), więc bierzemy średnią z 10. i 11. wartości:\nMediana = \\frac{5 + 5}{2} = 5\n\n\nWeryfikacja w R\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\nDominanta (moda)\nDominanta to najczęściej występująca wartość w zbiorze danych.\nDla Firmy X dominanta wynosi 3 (występuje 6 razy). Dla Firmy Y są dwie dominanty: 4 i 5 (obie występują 6 razy).\n\n# Funkcja do obliczania dominanty\nznajdz_dominante &lt;- function(x) {\n  unikalne_x &lt;- unique(x)\n  unikalne_x[which.max(tabulate(match(x, unikalne_x)))]\n}\n\nznajdz_dominante(X)\n\n[1] 3\n\nznajdz_dominante(Y)\n\n[1] 4\n\n\n\n\n\nMiary rozproszenia\n\nWariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\nPoprawka Bessela jest stosowana przy obliczaniu wariancji z próby, aby uzyskać nieobciążony estymator wariancji populacji. W standardowym wzorze na wariancję z próby dzielimy przez (n-1) zamiast przez n.\nModyfikacje wzoru dla danych pogrupowanych (szereg częstości):\nMożna też zapisać ten wzór w postaci:\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{k} f_i(x_i - \\bar{x})^2\ngdzie f_i to częstość bezwzględna (liczba wystąpień) i-tej wartości.\nGdy w obliczeniach stosujemy częstości względne p = f_i/n, gdzie:\n\nf_i to częstość (liczba wystąpień)\nn to całkowita liczebność próby\n\nWzór na wariancję z uwzględnieniem poprawki Bessela przyjmuje postać:\ns^2 = \\frac{n}{n-1} \\sum_{i=1}^{k} p_i(x_i - \\bar{x})^2\ngdzie:\n\ns^2 to wariancja z próby\nn to liczebność próby\np_i = f_i/n to częstość względna i-tej wartości\nx_i to i-ta wartość cechy\n\\bar{x} to średnia arytmetyczna\nk to liczba różnych wartości cechy\n\nKluczowe jest to, że przy stosowaniu częstości względnych mnożymy całe wyrażenie przez czynnik \\frac{n}{n-1}, który wprowadza poprawkę Bessela.\n\nObliczenia ręczne dla Firmy X\n\n\n\n\n\n\n\n\n\n\nx_i\nf_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\nf_i(x_i - \\bar{x})^2\n\n\n\n\n2\n3\n-3,95\n15,6025\n46,8075\n\n\n3\n6\n-2,95\n8,7025\n52,215\n\n\n4\n5\n-1,95\n3,8025\n19,0125\n\n\n5\n4\n-0,95\n0,9025\n3,61\n\n\n20\n1\n14,05\n197,4025\n197,4025\n\n\n35\n1\n29,05\n843,9025\n843,9025\n\n\nSuma\n20\n\n\n1162,95\n\n\n\ns^2 = \\frac{1162,95}{19} = 61,21\n\n\nObliczenia ręczne dla Firmy Y\n\n\n\n\n\n\n\n\n\n\ny_i\nf_i\ny_i - \\bar{x}\n(y_i - \\bar{y})^2\nf_i(y_i - \\bar{y})^2\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nSuma\n20\n\n\n34\n\n\n\ns^2 = \\frac{34}{19} = 1,79\n\n\nWeryfikacja w R\n\nvar(X)\n\n[1] 61.2\n\nvar(Y)\n\n[1] 1.79\n\n\n\n\n\nOdchylenie standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji.\nWzór: s = \\sqrt{s^2}\n\nDla Firmy X: s = \\sqrt{61,21} = 7,82\nDla Firmy Y: s = \\sqrt{1,79} = 1,34\n\n\nWeryfikacja w R\n\nsd(X)\n\n[1] 7.82\n\nsd(Y)\n\n[1] 1.34\n\n\n\n\n\n\nKwartyle\nKwartyle dzielą zbiór danych na cztery równe części.\n\nObliczenia ręczne dla Firmy X\nUporządkowane dane: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 3\nQ2 (50. percentyl, mediana): 4\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 5\n\n\n\nObliczenia ręczne dla Firmy Y\nUporządkowane dane: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 4\nQ2 (50. percentyl, mediana): 5\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 6\n\n\n\nWeryfikacja w R\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\nIQR\n\nIQR_x = 5 - 3 = 2\nIQR_y = 6 - 4 = 2\n\n\n\n\nWykres pudełkowy Tukeya\nWykres pudełkowy Tukeya wizualnie przedstawia rozkład danych na podstawie kwartyli. Użyjemy biblioteki ggplot2 do stworzenia wykresu.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Przygotowanie danych\ndane &lt;- data.frame(\n  Firma = rep(c(\"X\", \"Y\"), each = 20),\n  Wynagrodzenie = c(X, Y)\n)\n\n# Tworzenie wykresu pudełkowego\nggplot(dane, aes(x = Firma, y = Wynagrodzenie, fill = Firma)) +\n  geom_boxplot() +\n  labs(title = \"Rozkład wynagrodzeń w firmach X i Y\",\n       x = \"Firma\",\n       y = \"Wynagrodzenie (tysiące euro)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n# Tworzenie wykresu pudełkowego\nggplot(dane, aes(x = Firma, y = Wynagrodzenie, fill = Firma)) +\n  geom_boxplot(outliers = F) +\n  labs(title = \"Rozkład wynagrodzeń w firmach X i Y\",\n       x = \"Firma\",\n       y = \"Wynagrodzenie (tysiące euro)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\nInterpretacja wykresu pudełkowego\n\nPudełko reprezentuje rozstęp międzykwartylowy (IQR) od Q1 do Q3.\nLinia wewnątrz pudełka to mediana (Q2).\nWąsy rozciągają się do najmniejszych i największych wartości w granicach 1,5 * IQR.\nPunkty poza wąsami są uznawane za wartości odstające.\n\n\n\n\nPorównanie wyników\n\n\n\nMiara\nFirma X\nFirma Y\n\n\n\n\nŚrednia\n5,95\n5,00\n\n\nMediana\n4\n5\n\n\nDominanta\n3\n4 i 5\n\n\nWariancja\n61,21\n1,79\n\n\nOdchylenie standard.\n7,82\n1,34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\nKluczowe obserwacje:\n\nTendencja centralna: Firma X ma wyższą średnią, ale niższą medianę niż Firma Y, co wskazuje na prawostronnie skośny rozkład dla Firmy X.\n\nRozproszenie: Firma X wykazuje znacznie wyższą wariancję i odchylenie standardowe, sugerując większe dysproporcje w wynagrodzeniach.\nKształt rozkładu: Wynagrodzenia w Firmie Y są bardziej skupione, podczas gdy Firma X ma wartości ekstremalne (potencjalne wartości odstające), które znacząco wpływają na jej średnią i wariancję.\nKwartyle: Rozstęp międzykwartylowy (Q3 - Q1) Firmy Y jest nieznacznie większy, ale jej ogólny zakres jest znacznie mniejszy niż Firmy X.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ćwiczenie-2.-porównanie-zmienności-wielkości-okręgów-wyborczych-między-krajami",
    "href": "rozdzial5.html#ćwiczenie-2.-porównanie-zmienności-wielkości-okręgów-wyborczych-między-krajami",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "10.12 Ćwiczenie 2. Porównanie Zmienności Wielkości Okręgów Wyborczych Między Krajami",
    "text": "10.12 Ćwiczenie 2. Porównanie Zmienności Wielkości Okręgów Wyborczych Między Krajami\n\nDane\nMamy dane o wielkości okręgów wyborczych z dwóch krajów:\n\nx &lt;- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)  # Kraj wysoka zmienność\ny &lt;- c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13)  # Kraj niska zmienność\n\nkable(data.frame(\n  \"Kraj X (Wysoka zm.)\" = x,\n  \"Kraj Y (Niska zm.)\" = y\n))\n\n\n\n\nKraj.X..Wysoka.zm..\nKraj.Y..Niska.zm..\n\n\n\n\n1\n8\n\n\n3\n9\n\n\n5\n9\n\n\n7\n10\n\n\n9\n10\n\n\n11\n11\n\n\n13\n11\n\n\n15\n12\n\n\n17\n12\n\n\n19\n13\n\n\n\n\n\n\n\nMiary Tendencji Centralnej\n\nŚrednia Arytmetyczna\nWzór: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\nObliczenia dla Kraju X\n\n\n\nElement\nWartość\n\n\n\n\n1\n1\n\n\n2\n3\n\n\n3\n5\n\n\n4\n7\n\n\n5\n9\n\n\n6\n11\n\n\n7\n13\n\n\n8\n15\n\n\n9\n17\n\n\n10\n19\n\n\nSuma\n100\n\n\n\n\\bar{x} = \\frac{100}{10} = 10\n\nmean_x &lt;- mean(x)\nc(\"Ręcznie\" = 10, \"R\" = mean_x)\n\nRęcznie       R \n     10      10 \n\n\n\n\nObliczenia dla Kraju Y\n\n\n\nElement\nWartość\n\n\n\n\n1\n8\n\n\n2\n9\n\n\n3\n9\n\n\n4\n10\n\n\n5\n10\n\n\n6\n11\n\n\n7\n11\n\n\n8\n12\n\n\n9\n12\n\n\n10\n13\n\n\nSuma\n105\n\n\n\n\\bar{y} = \\frac{105}{10} = 10,5\n\nmean_y &lt;- mean(y)\nc(\"Ręcznie\" = 10.5, \"R\" = mean_y)\n\nRęcznie       R \n   10.5    10.5 \n\n\n\n\n\nMediana\nMediana to wartość środkowa w uporządkowanym zbiorze danych.\n\nObliczenia dla Kraju X\nUporządkowane dane: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nDla n = 10 (parzysta liczba obserwacji): Pozycje środkowe: 5 i 6 Wartości środkowe: 9 i 11\nMediana = \\frac{9 + 11}{2} = 10\n\nmedian_x &lt;- median(x)\nc(\"Ręcznie\" = 10, \"R\" = median_x)\n\nRęcznie       R \n     10      10 \n\n\n\n\nObliczenia dla Kraju Y\nUporządkowane dane: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nDla n = 10 (parzysta liczba obserwacji): Pozycje środkowe: 5 i 6 Wartości środkowe: 10 i 11\nMediana = \\frac{10 + 11}{2} = 10,5\n\nmedian_y &lt;- median(y)\nc(\"Ręcznie\" = 10.5, \"R\" = median_y)\n\nRęcznie       R \n   10.5    10.5 \n\n\n\n\n\nDominanta\n\nObliczenia dla Kraju X\n\n\n\nWartość\nCzęstość\n\n\n\n\n1\n1\n\n\n3\n1\n\n\n5\n1\n\n\n7\n1\n\n\n9\n1\n\n\n11\n1\n\n\n13\n1\n\n\n15\n1\n\n\n17\n1\n\n\n19\n1\n\n\n\nWniosek: Brak dominanty (wszystkie wartości występują jednokrotnie)\n\n\nObliczenia dla Kraju Y\n\n\n\nWartość\nCzęstość\n\n\n\n\n8\n1\n\n\n9\n2\n\n\n10\n2\n\n\n11\n2\n\n\n12\n2\n\n\n13\n1\n\n\n\nWniosek: Cztery dominanty: 9, 10, 11, 12 (każda występuje dwukrotnie)\n\n# Tabele częstości\ntable_x &lt;- table(x)\ntable_y &lt;- table(y)\n\nlist(\n  \"Kraj X\" = table_x,\n  \"Kraj Y\" = table_y\n)\n\n$`Kraj X`\nx\n 1  3  5  7  9 11 13 15 17 19 \n 1  1  1  1  1  1  1  1  1  1 \n\n$`Kraj Y`\ny\n 8  9 10 11 12 13 \n 1  2  2  2  2  1 \n\n\n\n\n\nWariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\nObliczenia dla Kraju X\n\n\n\nx_i\n(x_i - \\bar{x})\n(x_i - \\bar{x})^2\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSuma\n\n330\n\n\n\ns^2_X = \\frac{330}{9} = 36,67\n\nvar_x &lt;- var(x)\nc(\"Ręcznie\" = 36.67, \"R\" = var_x)\n\nRęcznie       R \n  36.67   36.67 \n\n\n\n\nObliczenia dla Kraju Y\n\n\n\nx_i\n(y_i - \\bar{y})\n(y_i - \\bar{y})^2\n\n\n\n\n8\n-2,5\n6,25\n\n\n9\n-1,5\n2,25\n\n\n9\n-1,5\n2,25\n\n\n10\n-0,5\n0,25\n\n\n10\n-0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n12\n1,5\n2,25\n\n\n12\n1,5\n2,25\n\n\n13\n2,5\n6,25\n\n\nSuma\n\n22,5\n\n\n\ns^2_Y = \\frac{22,5}{9} = 2,5\n\nvar_y &lt;- var(y)\nc(\"Ręcznie\" = 2.5, \"R\" = var_y)\n\nRęcznie       R \n    2.5     2.5 \n\n\n\n\n\nOdchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji. Jest miarą zmienności wyrażoną w tych samych jednostkach co dane.\nWzór: s = \\sqrt{s^2}\n\nObliczenia dla Kraju X\nWykorzystujemy wcześniej obliczoną wariancję: s^2_X = 36,67\nObliczamy pierwiastek: s_X = \\sqrt{36,67} \\approx 6,06\n\n\n\nKrok\nObliczenie\nWynik\n\n\n\n\n1. Wariancja\ns^2_X\n36,67\n\n\n2. Pierwiastek\n\\sqrt{36,67}\n6,06\n\n\n\n\nsd_x &lt;- sd(x)\nc(\"Ręcznie\" = 6.06, \"R\" = sd_x)\n\nRęcznie       R \n  6.060   6.055 \n\n\n\n\nObliczenia dla Kraju Y\nWykorzystujemy wcześniej obliczoną wariancję: s^2_Y = 2,5\nObliczamy pierwiastek: s_Y = \\sqrt{2,5} \\approx 1,58\n\n\n\nKrok\nObliczenie\nWynik\n\n\n\n\n1. Wariancja\ns^2_Y\n2,5\n\n\n2. Pierwiastek\n\\sqrt{2,5}\n1,58\n\n\n\n\nsd_y &lt;- sd(y)\nc(\"Ręcznie\" = 1.58, \"R\" = sd_y)\n\nRęcznie       R \n  1.580   1.581 \n\n\nInterpretacja:\n\nKraj X: Przeciętne odchylenie wielkości okręgu od średniej wynosi około 6 mandatów\nKraj Y: Przeciętne odchylenie wielkości okręgu od średniej wynosi około 1,6 mandatu\n\n\n\n\n\nWspółczynnik Zmienności (CV)\nWspółczynnik zmienności to stosunek odchylenia standardowego do średniej, wyrażony w procentach.\nWzór: CV = \\frac{s}{\\bar{x}} \\times 100\\%\n\nObliczenia dla Kraju X\nCV_X = \\frac{6,06}{10} \\times 100\\% = 60,6\\%\n\n\n\nSkładowa\nWartość\n\n\n\n\nOdchylenie standardowe (s)\n6,06\n\n\nŚrednia (\\bar{x})\n10\n\n\nCV\n60,6%\n\n\n\n\ncv_x &lt;- sd(x) / mean(x) * 100\nc(\"Ręcznie\" = 60.6, \"R\" = cv_x)\n\nRęcznie       R \n  60.60   60.55 \n\n\n\n\nObliczenia dla Kraju Y\nCV_Y = \\frac{1,58}{10,5} \\times 100\\% = 15,0\\%\n\n\n\nSkładowa\nWartość\n\n\n\n\nOdchylenie standardowe (s)\n1,58\n\n\nŚrednia (\\bar{x})\n10,5\n\n\nCV\n15,0%\n\n\n\n\ncv_y &lt;- sd(y) / mean(y) * 100\nc(\"Ręcznie\" = 15.0, \"R\" = cv_y)\n\nRęcznie       R \n  15.00   15.06 \n\n\n\n\n\nKwartyle i Rozstęp Międzykwartylowy (IQR)\n\nMetody obliczania kwartyli\nIstnieją różne metody obliczania kwartyli. W naszych obliczeniach ręcznych zastosujemy metodę wyłączającą medianę:\n\nDzielimy szereg na dwie części względem mediany\nMediana nie jest uwzględniana w obliczeniach kwartyli\nDla każdej części obliczamy jej medianę - będzie to odpowiednio Q1 i Q3\n\n\n\nObliczenia dla Kraju X\nUporządkowane dane: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMediana = 10 (nie uwzględniamy w obliczeniach kwartyli)\nDolna połowa: 1, 3, 5, 7, 9 Q1 = mediana dolnej połowy = 5\nGórna połowa: 11, 13, 15, 17, 19 Q3 = mediana górnej połowy = 15\nIQR = Q3 - Q1 = 15 - 5 = 10\n\n\nObliczenia dla Kraju Y\nUporządkowane dane: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMediana = 10.5 (nie uwzględniamy w obliczeniach kwartyli)\nDolna połowa: 8, 9, 9, 10, 10 Q1 = mediana dolnej połowy = 9\nGórna połowa: 11, 11, 12, 12, 13 Q3 = mediana górnej połowy = 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n# Porównanie różnych metod obliczania kwartyli w R\nmethods_comparison &lt;- data.frame(\n  Metoda = c(\"Ręcznie (bez mediany)\", \n             \"R type=1\", \"R type=2\", \"R type=7 (domyślna)\"),\n  \"Q1 Kraj X\" = c(5, \n                  quantile(x, 0.25, type=1),\n                  quantile(x, 0.25, type=2),\n                  quantile(x, 0.25, type=7)),\n  \"Q3 Kraj X\" = c(15,\n                  quantile(x, 0.75, type=1),\n                  quantile(x, 0.75, type=2),\n                  quantile(x, 0.75, type=7)),\n  \"Q1 Kraj Y\" = c(9,\n                  quantile(y, 0.25, type=1),\n                  quantile(y, 0.25, type=2),\n                  quantile(y, 0.25, type=7)),\n  \"Q3 Kraj Y\" = c(12,\n                  quantile(y, 0.75, type=1),\n                  quantile(y, 0.75, type=2),\n                  quantile(y, 0.75, type=7))\n)\n\nkable(methods_comparison, digits = 2,\n      caption = \"Porównanie różnych metod obliczania kwartyli\")\n\n\nPorównanie różnych metod obliczania kwartyli\n\n\nMetoda\nQ1.Kraj.X\nQ3.Kraj.X\nQ1.Kraj.Y\nQ3.Kraj.Y\n\n\n\n\nRęcznie (bez mediany)\n5.0\n15.0\n9.00\n12.00\n\n\nR type=1\n5.0\n15.0\n9.00\n12.00\n\n\nR type=2\n5.0\n15.0\n9.00\n12.00\n\n\nR type=7 (domyślna)\n5.5\n14.5\n9.25\n11.75\n\n\n\n\n\n\n\nWyjaśnienie różnic w metodach obliczania kwartyli\n\nMetoda ręczna (bez mediany):\n\nDzieli dane na dwie części\nNie uwzględnia mediany\nZnajduje medianę każdej części\n\nR type=1:\n\nMetoda pierwsza w R\nUżywa pozycji całkowitych\nNie interpoluje\n\nR type=2:\n\nMetoda druga w R\nUżywa pozycji całkowitych\nInterpoluje gdy pozycja nie jest całkowita\n\nR type=7 (domyślna):\n\nMetoda domyślna w R\nUżywa quantile()[5] z SAS\nInterpoluje według metody opisanej przez Hyndmana i Fana\n\n\n\n\n\nPorównanie Wyników\n\nsummary_df &lt;- data.frame(\n  Miara = c(\"Średnia\", \"Mediana\", \"Dominanta\", \"Rozstęp\", \"Wariancja\", \n            \"Odch. Stand.\", \"Q1\", \"Q3\", \"IQR\", \"CV (%)\"),\n  \"Kraj X\" = c(10, 10, \"brak\", 18, 36.67, 6.06, 5, 15, 10, 60.6),\n  \"Kraj Y\" = c(10.5, 10.5, \"9,10,11,12\", 5, 2.5, 1.58, 9, 12, 3, 15.0)\n)\n\nkable(summary_df, \n      caption = \"Zestawienie wszystkich miar statystycznych\",\n      align = c('l', 'r', 'r'))\n\n\nZestawienie wszystkich miar statystycznych\n\n\nMiara\nKraj.X\nKraj.Y\n\n\n\n\nŚrednia\n10\n10.5\n\n\nMediana\n10\n10.5\n\n\nDominanta\nbrak\n9,10,11,12\n\n\nRozstęp\n18\n5\n\n\nWariancja\n36.67\n2.5\n\n\nOdch. Stand.\n6.06\n1.58\n\n\nQ1\n5\n9\n\n\nQ3\n15\n12\n\n\nIQR\n10\n3\n\n\nCV (%)\n60.6\n15\n\n\n\n\n\n\n\nPorównanie za pomocą Wykresu Pudełkowego\n\ndf_long &lt;- data.frame(\n  kraj = rep(c(\"X\", \"Y\"), each = 10),\n  wielkosc = c(x, y)\n)\n\n# Wykres podstawowy\np &lt;- ggplot(df_long, aes(x = kraj, y = wielkosc, fill = kraj)) +\n  geom_boxplot(outlier.shape = NA) +  # Wyłączamy domyślne punkty odstające\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Dodajemy punkty z przezroczystością\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\")) +\n  labs(\n    title = \"Porównanie Zmienności Wielkości Okręgów Wyborczych\",\n    subtitle = paste(\"CV: Kraj X =\", round(cv_x, 1), \"%, Kraj Y =\", round(cv_y, 1), \"%\"),\n    x = \"Kraj\",\n    y = \"Wielkość Okręgu\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Dodajemy adnotacje z kwartylami\np + annotate(\n  \"text\", \n  x = c(1, 1, 1, 2, 2, 2), \n  y = c(max(x)+1, mean(x), min(x)-1, max(y)+1, mean(y), min(y)-1),\n  label = c(\n    paste(\"Q3 =\", quantile(x, 0.75, type=1)),\n    paste(\"M =\", median(x)),\n    paste(\"Q1 =\", quantile(x, 0.25, type=1)),\n    paste(\"Q3 =\", quantile(y, 0.75, type=1)),\n    paste(\"M =\", median(y)),\n    paste(\"Q1 =\", quantile(y, 0.25, type=1))\n  ),\n  size = 3\n)\n\n\n\n\n\n\n\n\n\n\nUwagi Metodologiczne\n\nObliczenia kwartyli:\n\nZastosowana metoda wyłączająca medianę może dawać inne wyniki niż domyślne funkcje R\nRóżnice w metodach obliczeniowych nie wpływają na ogólne wnioski\nWarto zawsze zaznaczyć stosowaną metodę w raportach\n\nWizualizacja:\n\nWykres pudełkowy skutecznie pokazuje różnice w rozkładach\nDodatkowe punkty pokazują rzeczywiste wartości\nAdnotacje ułatwiają interpretację\n\n\n\n\nPodsumowanie\n\nPorównanie Miar Statystycznych\n\n\n\nMiara\nKraj X\nKraj Y\nRóżnica względna\n\n\n\n\nŚrednia\n10,0\n10,5\nPodobna\n\n\nMediana\n10,0\n10,5\nPodobna\n\n\nDominanta\nBrak\nWielokrotna (9,10,11,12)\n-\n\n\nRozstęp\n18\n5\n3,6× większy w X\n\n\nWariancja\n36,67\n2,5\n14,7× większa w X\n\n\nIQR\n10\n3\n3,3× większy w X\n\n\nCV\n60,6%\n15,0%\n4,0× większy w X\n\n\n\n\n\nCharakterystyka Rozkładów\nKraj X:\n\nRozkład równomierny\nBrak dominującej wielkości okręgu (brak dominanty)\nSzeroki zakres: od 1 do 19 mandatów\nWysoka zmienność (CV = 60,6%)\nRównomierne rozłożenie wartości w zakresie\n\nKraj Y:\n\nRozkład skupiony\nWiele typowych wielkości (cztery dominanty)\nWąski zakres: od 8 do 13 mandatów\nNiska zmienność (CV = 15,0%)\nWartości skoncentrowane wokół średniej\n\n\n\nInterpretacja Wykresu Pudełkowego\nWizualizacja w formie wykresu pudełkowego pokazuje:\nElementy Struktury:\n\nPudełko: Pokazuje rozstęp międzykwartylowy (IQR)\nDolna krawędź: Pierwszy kwartyl (Q1)\nGórna krawędź: Trzeci kwartyl (Q3)\nLinia wewnętrzna: Mediana (Q2)\nWąsy: Rozciągają się do ±1,5 IQR - Punkty: Pojedyncze wielkości okręgów\n\nGłówne Wnioski Wizualne:\n\nRozmiar Pudełka:\n\n\nKraj X: Duże pudełko wskazuje na szeroki rozrzut środkowych 50%\nKraj Y: Małe pudełko pokazuje skupienie wartości środkowych\n\n\nDługość Wąsów:\n\nKraj X: Długie wąsy wskazują na szeroki rozkład całkowity\nKraj Y: Krótkie wąsy pokazują ograniczony rozrzut\n\nRozkład Punktów:\n\nKraj X: Punkty szeroko rozproszone\nKraj Y: Punkty gęsto skupione\n\n\n\n\nKluczowe Obserwacje\n\nTendencja Centralna:\n\nPodobne średnie wielkości okręgów\nRóżne wzorce rozkładu\nOdmienne podejścia do standaryzacji\n\nMiary Zmienności:\n\nWszystkie miary pokazują 3-15 razy większą zmienność w Kraju X\nSpójny wzorzec w różnych miarach statystycznych\nSystematyczna różnica w projekcie okręgów\n\nProjekt Systemu:\n\nKraj X: Elastyczne, zróżnicowane podejście\nKraj Y: Ustandaryzowane, jednolite podejście\nRóżne filozoficzne podejścia do reprezentacji\n\nImplikacje Reprezentatywności:\n\nKraj X: Zmienna proporcja wyborców do przedstawicieli\nKraj Y: Bardziej spójne poziomy reprezentacji\nRóżne podejścia do reprezentacji demokratycznej\n\n\nAnaliza ta pokazuje fundamentalne różnice w projektowaniu systemów wyborczych między dwoma krajami, gdzie Kraj X przyjmuje bardziej zróżnicowane podejście, a Kraj Y utrzymuje większą jednolitość w wielkości okręgów wyborczych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ćwiczenie-3.-voter-participation-and-economic-prosperity",
    "href": "rozdzial5.html#ćwiczenie-3.-voter-participation-and-economic-prosperity",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "10.13 Ćwiczenie 3. Voter Participation and Economic Prosperity",
    "text": "10.13 Ćwiczenie 3. Voter Participation and Economic Prosperity\nAnaliza związku między dobrobytem ekonomicznym a frekwencją wyborczą w dzielnicach Amsterdamu na podstawie danych z wyborów samorządowych 2022.\n\nDane\nPróba obejmuje pięć reprezentatywnych dzielnic:\n\n\n\nDzielnica\nDochód (tys. €)\nFrekwencja (%)\n\n\n\n\nA\n50\n60\n\n\nB\n45\n56\n\n\nC\n56\n70\n\n\nD\n40\n50\n\n\nE\n60\n75\n\n\n\n\n# Wczytanie bibliotek\nlibrary(tidyverse)\n\n# Utworzenie zbioru danych\ndane &lt;- data.frame(\n  dzielnica = LETTERS[1:5],\n  dochod = c(50, 45, 56, 40, 60),\n  frekwencja = c(60, 56, 70, 50, 75)\n)\n\n\n\nCzęść 1: Statystyki opisowe\n\n# Statystyki dla dochodu\nmean(dane$dochod)\n\n[1] 50.2\n\nmedian(dane$dochod)\n\n[1] 50\n\nsd(dane$dochod)\n\n[1] 8.075\n\nrange(dane$dochod)\n\n[1] 40 60\n\n# Statystyki dla frekwencji\nmean(dane$frekwencja)\n\n[1] 62.2\n\nmedian(dane$frekwencja)\n\n[1] 60\n\nsd(dane$frekwencja)\n\n[1] 10.21\n\nrange(dane$frekwencja)\n\n[1] 50 75\n\n\n\n\nCzęść 2: Analiza korelacji\n\n# Korelacja Pearsona\ncor.test(dane$dochod, dane$frekwencja)\n\n\n    Pearson's product-moment correlation\n\ndata:  dane$dochod and dane$frekwencja\nt = 16, df = 3, p-value = 0.0005\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9117 0.9996\nsample estimates:\n   cor \n0.9942 \n\n\n\n\nCzęść 3: Model regresji OLS\n\n# Dopasowanie modelu OLS\nmodel &lt;- lm(frekwencja ~ dochod, data = dane)\n\n# Podsumowanie modelu\nsummary(model)\n\n\nCall:\nlm(formula = frekwencja ~ dochod, data = dane)\n\nResiduals:\n     1      2      3      4      5 \n-1.949  0.336  0.510  0.620  0.482 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.8965     3.9673   -0.23  0.83575    \ndochod        1.2569     0.0782   16.07  0.00052 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.26 on 3 degrees of freedom\nMultiple R-squared:  0.989, Adjusted R-squared:  0.985 \nF-statistic:  258 on 1 and 3 DF,  p-value: 0.000524\n\n\n\n\nWizualizacja\n\n# Wykres rozrzutu z linią regresji\nggplot(dane, aes(x = dochod, y = frekwencja)) +\n  geom_point(size = 4, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  geom_text(aes(label = dzielnica), vjust = -1) +\n  labs(\n    title = \"Dochód vs frekwencja wyborcza\",\n    x = \"Dochód (tys. €)\",\n    y = \"Frekwencja wyborcza (%)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nWnioski\nAnaliza wykazała silny dodatni związek między dobrobytem ekonomicznym dzielnicy a frekwencją wyborczą. Mieszkańcy dzielnic o wyższych dochodach częściej uczestniczą w wyborach samorządowych.\nUwaga: Mała liczebność próby (n=5) ogranicza możliwość generalizacji wyników.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#appendix-tabele-podsumowujące-typy-danych-i-odpowiednie-miary-statystyczne",
    "href": "rozdzial5.html#appendix-tabele-podsumowujące-typy-danych-i-odpowiednie-miary-statystyczne",
    "title": "10  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "10.14 Appendix: Tabele Podsumowujące Typy Danych i Odpowiednie Miary Statystyczne",
    "text": "10.14 Appendix: Tabele Podsumowujące Typy Danych i Odpowiednie Miary Statystyczne\n\nZalety i Wady Różnych Miar Statystycznych\n\nMiary Tendencji Centralnej\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nŚrednia\n- Wykorzystuje wszystkie punkty danych- Pozwala na dalsze obliczenia statystyczne- Idealna dla danych o rozkładzie normalnym\n- Wrażliwa na wartości odstające- Nieodpowiednia dla rozkładów skośnych- Bez znaczenia dla danych nominalnych\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nMediana\n- Niewrażliwa na wartości odstające- Dobra dla rozkładów skośnych- Może być stosowana do danych porządkowych\n- Ignoruje rzeczywiste wartości większości punktów danych- Mniej użyteczna do dalszych analiz statystycznych\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nModa\n- Może być stosowana do każdego typu danych- Dobra do znajdowania najczęstszej kategorii\n- Może nie być unikalna (rozkłady multimodalne)- Nieprzydatna do wielu typów analiz- Ignoruje wielkość różnic między wartościami\nWszystkie typy\n\n\n\n\n\nMiary Zmienności\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nZakres\n- Prosty do obliczenia i zrozumienia- Daje szybki obraz rozproszenia danych\n- Bardzo wrażliwy na wartości odstające- Ignoruje wszystkie dane między ekstremami- Nieprzydatny do dalszych analiz statystycznych\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nRozstęp międzykwartylowy (IQR)\n- Niewrażliwy na wartości odstające- Dobry dla rozkładów skośnych\n- Ignoruje 50% danych- Mniej intuicyjny niż zakres\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nWariancja\n- Wykorzystuje wszystkie punkty danych- Podstawa wielu procedur statystycznych\n- Wrażliwa na wartości odstające- Jednostki są podniesione do kwadratu (mniej intuicyjne)\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nOdchylenie standardowe\n- Wykorzystuje wszystkie punkty danych- Te same jednostki co oryginalne dane- Szeroko stosowane i zrozumiałe\n- Wrażliwe na wartości odstające- Zakłada w przybliżeniu rozkład normalny dla interpretacji\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nWspółczynnik zmienności\n- Pozwala na porównanie między zbiorami danych o różnych jednostkach lub średnich\n- Może być mylący, gdy średnie są bliskie zeru- Bez znaczenia dla danych z wartościami ujemnymi\nIlorazowe, niektóre Interwałowe\n\n\n\n\n\nMiary Korelacji/Asocjacji\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nr Pearsona\n- Mierzy zależność liniową- Szeroko stosowany i zrozumiały\n- Zakłada rozkład normalny- Wrażliwy na wartości odstające- Uchwytuje tylko zależności liniowe\nInterwałowe, Ilorazowe, Ciągłe\n\n\nRho Spearmana\n- Może być stosowany do danych porządkowych- Uchwytuje zależności monotoniczne- Mniej wrażliwy na wartości odstające\n- Traci informacje przez konwersję na rangi- Może pominąć niektóre typy zależności\nPorządkowe, Interwałowe, Ilorazowe\n\n\nTau Kendalla\n- Może być stosowany do danych porządkowych- Bardziej odporny niż Spearman dla małych próbek- Ma ładną interpretację (prawdopodobieństwo zgodności)\n- Traci informacje, biorąc pod uwagę tylko porządek- Bardziej intensywny obliczeniowo\nPorządkowe, Interwałowe, Ilorazowe\n\n\nChi-kwadrat\n- Może być stosowany do danych nominalnych- Testuje niezależność zmiennych kategorycznych\n- Wymaga dużych rozmiarów próbek- Wrażliwy na rozmiar próbki- Nie mierzy siły asocjacji\nNominalne, Porządkowe\n\n\nV Craméra\n- Może być stosowany do danych nominalnych- Dostarcza miarę siły asocjacji- Znormalizowany do zakresu [0,1]\n- Interpretacja może być subiektywna- Może przeszacować asocjację w małych próbkach\nNominalne, Porządkowe\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓*\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\nNotes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "11  Data Visualization: with examples in R",
    "section": "",
    "text": "11.1 Introduction to Data Types and Visualization\nThis chapter explores fundamental types of data visualizations: bar plots, histograms, and box plots, in particular.\nBefore diving into specific visualization techniques, it’s crucial to understand the different types of data you might encounter and how they influence your choice of visualization method. We’ll explore these concepts with practical examples using the ggplot2 library in R.\nFirst, let’s load the necessary libraries:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#bar-plots",
    "href": "chapter6.html#bar-plots",
    "title": "11  Data Visualization: with examples in R",
    "section": "11.2 Bar Plots",
    "text": "11.2 Bar Plots\nBar plots are excellent for displaying categorical data or summarizing continuous data by groups.\n\nUnderstanding Bar Plots\nA bar plot represents data using rectangular bars with heights proportional to the values they represent. They are used to compare different categories or groups.\nKey components of a bar plot: 1. X-axis: Represents categories 2. Y-axis: Represents values (can be counts, percentages, or any numerical value) 3. Bars: Rectangle for each category, height corresponds to its value\n\nExample Data\nLet’s use a simple dataset of fruit sales:\n\nfruits &lt;- c(\"Apple\", \"Banana\", \"Orange\", \"Grape\")\nsales &lt;- c(120, 85, 70, 100)\n\n# Create a data frame\ndf &lt;- data.frame(fruit = fruits, sales = sales)\n\n\n\n\nHand-Drawn Bar Plot\nTo create a bar plot by hand:\n\nDraw a horizontal line (x-axis) and a vertical line (y-axis) perpendicular to each other.\nLabel the x-axis with your categories (fruits), evenly spaced.\nLabel the y-axis with a suitable scale for your values (sales, 0 to 120 in increments of 20).\nFor each category, draw a rectangle (bar) whose height corresponds to its value on the y-axis scale.\nColor or shade each bar if desired.\nAdd a title and labels for both axes.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen drawing by hand, use graph paper for more precise measurements and straighter lines. Choose a scale that allows all your data to fit while maximizing the use of space.\n\n\n\n\nBar Plot in Base R\n\n# Create bar plot\nbarplot(sales, names.arg = fruits, \n        main = \"Fruit Sales\",\n        xlab = \"Fruit Types\", ylab = \"Sales\")\n\n\n\n\n\n\n\n\n\n\nBar Plot with ggplot2\n\n# Create bar plot with ggplot2\nggplot(df, aes(x = fruit, y = sales)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Fruit Sales\",\n       x = \"Fruit Types\", y = \"Sales\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpreting Bar Plots\nWhen interpreting a bar plot, consider the following:\n\nRelative Heights: Compare the heights of the bars to understand which categories have higher or lower values.\nOrdering: Sometimes, bars are ordered by height to make comparisons easier.\nPatterns: Look for any patterns or trends across categories.\nOutliers: Identify any bars that are much taller or shorter than the others.\n\n\nExample Interpretation\nFor our fruit sales data:\n\nApples have the highest sales (120), followed by Grapes (100).\nOranges have the lowest sales (70).\nThere’s a considerable difference between the highest (Apples) and lowest (Oranges) sales.\nBananas and Grapes have similar sales figures, in the middle range.\n\nThis information could be useful for inventory management or marketing strategies in a fruit shop.\n\n\n\n\n\n\nNote\n\n\n\nBar plots are great for comparing categories, but they don’t show the distribution within each category. For that, you might need other plot types like box plots.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#histograms",
    "href": "chapter6.html#histograms",
    "title": "11  Data Visualization: with examples in R",
    "section": "11.3 Histograms",
    "text": "11.3 Histograms\nHistograms visualize the distribution of a continuous variable by dividing it into intervals (bins) and showing the frequency or density of data points in each bin.\n\nUnderstanding Histograms\nKey components of a histogram: 1. X-axis: Represents the variable’s values, divided into bins 2. Y-axis: Represents frequency, relative frequency, or density 3. Bars: Rectangle for each bin, height corresponds to the y-axis measure\nThere are three main types of histograms:\n\nFrequency Histogram: The y-axis shows the count of data points in each bin.\nRelative Frequency Histogram: The y-axis shows the proportion of data points in each bin (frequency divided by total number of data points).\nDensity Histogram: The y-axis shows the density, which is the relative frequency divided by the bin width. The total area of all bars sums to 1.\n\n\nExample Data\nLet’s use a dataset of 50 student exam scores (out of 100):\n\nset.seed(123)  # for reproducibility\nscores &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\nHand-Drawn Histogram\nTo create a frequency histogram by hand:\n\nFind the range of your data.\nChoose a number of bins (let’s use 7 bins).\nCreate a frequency table.\nDraw x and y axes.\nLabel x-axis with bin ranges and y-axis with frequency.\nDraw a rectangle for each bin, with height corresponding to its frequency.\nAdd a title and labels for both axes.\n\nFor a relative frequency histogram, divide each frequency by the total number of data points before drawing the bars.\nFor a density histogram, divide the relative frequency by the bin width before drawing the bars.\n\n\n\n\n\n\nTip\n\n\n\nThe number of bins can affect the interpretation. Too few bins may obscure important features, while too many may introduce noise. A common rule of thumb is to use the square root of the number of data points as the number of bins.\n\n\n\n\nHistograms in Base R\n\n# Frequency Histogram\nhist(scores, breaks = 7, \n     main = \"Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Relative Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Relative Frequency\")\n\n\n\n\n\n\n\n# Density Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Density Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Density\")\nlines(density(scores), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\nHistograms with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(score = scores)\n\n# Frequency Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nggplot(df, aes(x = score, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Relative Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Relative Frequency\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Density Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Density Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Density\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nInterpreting Histograms\nWhen interpreting a histogram, consider:\n\nCentral Tendency: Where is the peak of the distribution?\nSpread: How wide is the distribution?\nShape: Is it symmetric, skewed, or multi-modal?\nOutliers: Are there any unusual values far from the main distribution?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#box-plots-and-tukey-box-plots",
    "href": "chapter6.html#box-plots-and-tukey-box-plots",
    "title": "11  Data Visualization: with examples in R",
    "section": "11.4 Box Plots and Tukey Box Plots",
    "text": "11.4 Box Plots and Tukey Box Plots\nBox plots, also known as box-and-whisker plots, provide a concise summary of a distribution. We’ll focus on the Tukey-style box plot, named after the statistician John Tukey who popularized this type of plot.\n\nUnderstanding Box Plots\nA box plot represents five key statistics:\n\nMinimum value (excluding outliers)\nFirst quartile (Q1)\nMedian\nThird quartile (Q3)\nMaximum value (excluding outliers)\n\nAdditionally, box plots show:\n\nWhiskers: Lines extending from the box to the minimum and maximum values (excluding outliers)\nOutliers: Individual points beyond the whiskers\n\n\nCalculating Quartiles and Outliers\nTo create a box plot, follow these steps:\n\nOrder your data from smallest to largest.\nFind the median (middle value if odd number of data points, average of two middle values if even).\nFind Q1 (median of lower half of data) and Q3 (median of upper half of data).\nCalculate the Interquartile Range (IQR) = Q3 - Q1\nDetermine outliers using Tukey’s rule:\n\nLower outliers: &lt; Q1 - 1.5 * IQR\nUpper outliers: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe factor 1.5 in Tukey’s outlier rule is based on the properties of the normal distribution. For normally distributed data, this rule identifies about 0.7% of the data as potential outliers.\n\n\n\n\nExample Data\nLet’s use a small dataset to illustrate:\n\ndata &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\nHand-Drawn Tukey Box Plot\nTo create a Tukey box plot by hand:\n\nDraw a vertical line representing the range from minimum to maximum (2 to 15 in our example, excluding the outlier).\nDraw a box from Q1 to Q3.\nDraw a horizontal line through the box at the median.\nDraw whiskers from the box to the minimum and maximum values (excluding outliers).\nRepresent the outlier (50) as an individual point beyond the whisker.\nAdd a scale to the vertical axis and label it.\n\n\n\nBox Plot in Base R\n\n# Create box plot\nboxplot(data, main = \"Box Plot of Sample Data\",\n        ylab = \"Values\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\nTukey Box Plot with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(value = data)\n\n# Create Tukey box plot with ggplot2\nggplot(df, aes(x = \"\", y = value)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Tukey Box Plot of Sample Data\",\n       x = \"\", y = \"Values\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\nInterpreting Box Plots\nWhen interpreting a box plot, consider the following:\n\nCentral Tendency: The median shows the center of the distribution.\nSpread: The box (IQR) represents the middle 50% of the data.\nSkewness: If the median line is closer to one end of the box, the distribution is skewed.\nOutliers: Points beyond the whiskers are potential outliers.\nComparisons: When comparing multiple box plots, look at relative positions of medians, box sizes, and presence of outliers.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#conclusion",
    "href": "chapter6.html#conclusion",
    "title": "11  Data Visualization: with examples in R",
    "section": "11.5 Conclusion",
    "text": "11.5 Conclusion\nIn this chapter, we explored three fundamental types of data visualizations: bar plots, histograms, and box plots. We demonstrated how to create these plots by hand, using R’s base plotting system, and using the ggplot2 library.\nEach type of plot serves a different purpose: - Bar plots are excellent for comparing categories. - Histograms show the distribution of a continuous variable. - Box plots provide a concise summary of a distribution, highlighting central tendency, spread, and outliers.\nRemember, the choice of visualization depends on your data type and the insights you want to convey. Always consider your audience and the story you want to tell with your data when selecting and designing your visualizations.\nPractice creating these plots by hand to deepen your understanding of their construction and interpretation. Then, leverage the power of R and ggplot2 to quickly create and customize these visualizations for larger datasets and more complex analyses.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html",
    "href": "rozdzial6.html",
    "title": "12  Wizualizacja Danych: z przykładami w R",
    "section": "",
    "text": "12.1 Wprowadzenie do Typów Danych i Wizualizacji\nW tym rozdziale poznamy podstawowe typy wizualizacji danych: wykresy słupkowe, histogramy i wykresy pudełkowe. Omówimy ich tworzenie zarówno ręcznie, jak i przy użyciu R.\nPrzed zagłębieniem się w konkretne techniki wizualizacji, ważne jest zrozumienie różnych typów danych i ich wpływu na wybór metody wizualizacji. Przeanalizujemy te koncepcje na praktycznych przykładach z użyciem biblioteki ggplot2 w R.\nNajpierw załadujmy niezbędne biblioteki:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-słupkowe",
    "href": "rozdzial6.html#wykresy-słupkowe",
    "title": "12  Wizualizacja Danych: z przykładami w R",
    "section": "12.2 Wykresy Słupkowe",
    "text": "12.2 Wykresy Słupkowe\nWykresy słupkowe doskonale nadają się do prezentacji danych kategorycznych lub podsumowania danych ciągłych w grupach.\n\nZrozumienie Wykresów Słupkowych\nWykres słupkowy przedstawia dane za pomocą prostokątnych słupków, których wysokość jest proporcjonalna do reprezentowanych przez nie wartości. Służą do porównywania różnych kategorii lub grup.\nGłówne elementy wykresu słupkowego: 1. Oś X: Reprezentuje kategorie 2. Oś Y: Reprezentuje wartości (mogą to być liczebności, procenty lub dowolne wartości numeryczne) 3. Słupki: Prostokąt dla każdej kategorii, wysokość odpowiada jej wartości\n\nPrzykładowe Dane\nUżyjmy prostego zestawu danych dotyczącego sprzedaży owoców:\n\nowoce &lt;- c(\"Jabłko\", \"Banan\", \"Pomarańcza\", \"Winogrono\")\nsprzedaz &lt;- c(120, 85, 70, 100)\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(owoc = owoce, sprzedaz = sprzedaz)\n\n\n\n\nRęcznie Rysowany Wykres Słupkowy\nAby stworzyć wykres słupkowy ręcznie:\n\nNarysuj linię poziomą (oś X) i pionową (oś Y) prostopadłe do siebie.\nOznacz oś X swoimi kategoriami (owocami), równomiernie rozmieszczonymi.\nOznacz oś Y odpowiednią skalą dla Twoich wartości (sprzedaż, od 0 do 120 z przyrostami co 20).\nDla każdej kategorii narysuj prostokąt (słupek), którego wysokość odpowiada jej wartości na skali osi Y.\nJeśli chcesz, pokoloruj lub zacienuj każdy słupek.\nDodaj tytuł i etykiety dla obu osi.\n\n\n\n\n\n\n\nTip\n\n\n\nPrzy rysowaniu ręcznym użyj papieru milimetrowego dla dokładniejszych pomiarów i prostszych linii. Wybierz skalę, która pozwoli zmieścić wszystkie dane, maksymalnie wykorzystując dostępną przestrzeń.\n\n\n\n\nWykres Słupkowy w Podstawowym R\n\n# Tworzenie wykresu słupkowego\nbarplot(sprzedaz, names.arg = owoce, \n        main = \"Sprzedaż Owoców\",\n        xlab = \"Rodzaje Owoców\", ylab = \"Sprzedaż\")\n\n\n\n\n\n\n\n\n\n\nWykres Słupkowy z ggplot2\n\n# Tworzenie wykresu słupkowego z ggplot2\nggplot(df, aes(x = owoc, y = sprzedaz)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Sprzedaż Owoców\",\n       x = \"Rodzaje Owoców\", y = \"Sprzedaż\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpretacja Wykresów Słupkowych\nPodczas interpretacji wykresu słupkowego zwróć uwagę na:\n\nWzględne Wysokości: Porównaj wysokości słupków, aby zrozumieć, które kategorie mają wyższe lub niższe wartości.\nKolejność: Czasami słupki są uporządkowane według wysokości, aby ułatwić porównania.\nWzorce: Poszukaj wzorców lub trendów między kategoriami.\nWartości Odstające: Zidentyfikuj słupki, które są znacznie wyższe lub niższe od pozostałych.\n\n\nPrzykładowa Interpretacja\nDla naszych danych o sprzedaży owoców:\n\nJabłka mają najwyższą sprzedaż (120), następnie Winogrona (100).\nPomarańcze mają najniższą sprzedaż (70).\nIstnieje znaczna różnica między najwyższą (Jabłka) a najniższą (Pomarańcze) sprzedażą.\nBanany i Winogrona mają podobne wartości sprzedaży, w średnim zakresie.\n\nTa informacja może być przydatna dla zarządzania zapasami lub strategii marketingowych w sklepie owocowym.\n\n\n\n\n\n\nNote\n\n\n\nWykresy słupkowe są świetne do porównywania kategorii, ale nie pokazują rozkładu wewnątrz każdej kategorii. Do tego mogą być potrzebne inne typy wykresów, jak wykresy pudełkowe.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#histogramy",
    "href": "rozdzial6.html#histogramy",
    "title": "12  Wizualizacja Danych: z przykładami w R",
    "section": "12.3 Histogramy",
    "text": "12.3 Histogramy\nHistogramy wizualizują rozkład zmiennej ciągłej poprzez podzielenie jej na przedziały (bins) i pokazanie częstości lub gęstości punktów danych w każdym przedziale.\n\nZrozumienie Histogramów\nGłówne elementy histogramu: 1. Oś X: Reprezentuje wartości zmiennej, podzielone na przedziały 2. Oś Y: Reprezentuje częstość, względną częstość lub gęstość 3. Słupki: Prostokąt dla każdego przedziału, wysokość odpowiada mierze na osi Y\nIstnieją trzy główne typy histogramów:\n\nHistogram Częstości: Oś Y pokazuje liczbę punktów danych w każdym przedziale.\nHistogram Częstości Względnej: Oś Y pokazuje proporcję punktów danych w każdym przedziale (częstość podzielona przez całkowitą liczbę punktów danych).\nHistogram Gęstości: Oś Y pokazuje gęstość, która jest częstością względną podzieloną przez szerokość przedziału. Całkowita powierzchnia wszystkich słupków sumuje się do 1.\n\n\nPrzykładowe Dane\nUżyjmy zbioru 50 wyników egzaminów studentów (na 100 punktów):\n\nset.seed(123)  # dla powtarzalności\nwyniki &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\nRęcznie Rysowany Histogram\nAby stworzyć histogram częstości ręcznie:\n\nZnajdź zakres danych.\nWybierz liczbę przedziałów (użyjmy 7 przedziałów).\nUtwórz tabelę częstości.\nNarysuj osie X i Y.\nOznacz oś X zakresami przedziałów, a oś Y częstością.\nNarysuj prostokąt dla każdego przedziału, z wysokością odpowiadającą jego częstości.\nDodaj tytuł i etykiety dla obu osi.\n\nDla histogramu częstości względnej, podziel każdą częstość przez całkowitą liczbę punktów danych przed narysowaniem słupków.\nDla histogramu gęstości, podziel częstość względną przez szerokość przedziału przed narysowaniem słupków.\n\n\n\n\n\n\nTip\n\n\n\nLiczba przedziałów może wpłynąć na interpretację. Zbyt mało przedziałów może ukryć ważne cechy, podczas gdy zbyt wiele może wprowadzić szum. Powszechną regułą jest użycie pierwiastka kwadratowego z liczby punktów danych jako liczby przedziałów.\n\n\n\n\nHistogramy w Podstawowym R\n\n# Histogram Częstości\nhist(wyniki, breaks = 7, \n     main = \"Histogram Częstości Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Częstość\")\n\n\n\n\n\n\n\n# Histogram Częstości Względnej\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Częstości Względnej Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Częstość Względna\")\n\n\n\n\n\n\n\n# Histogram Gęstości\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Gęstości Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Gęstość\")\nlines(density(wyniki), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\nHistogramy z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wynik = wyniki)\n\n# Histogram Częstości\nggplot(df, aes(x = wynik)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Częstości Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Częstość\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram Częstości Względnej\nggplot(df, aes(x = wynik, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Częstości Względnej Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Częstość Względna\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Histogram Gęstości\nggplot(df, aes(x = wynik)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Histogram Gęstości Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Gęstość\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nInterpretacja Histogramów\nPodczas interpretacji histogramu zwróć uwagę na:\n\nTendencję Centralną: Gdzie znajduje się szczyt rozkładu?\nRozrzut: Jak szeroki jest rozkład?\nKształt: Czy jest symetryczny, skośny, czy wielomodalny?\nWartości Odstające: Czy są nietypowe wartości daleko od głównego rozkładu?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-pudełkowe-i-wykresy-pudełkowe-tukeya",
    "href": "rozdzial6.html#wykresy-pudełkowe-i-wykresy-pudełkowe-tukeya",
    "title": "12  Wizualizacja Danych: z przykładami w R",
    "section": "12.4 Wykresy Pudełkowe i Wykresy Pudełkowe Tukeya",
    "text": "12.4 Wykresy Pudełkowe i Wykresy Pudełkowe Tukeya\nWykresy pudełkowe, znane również jako wykresy skrzynkowe, dostarczają zwięzłego podsumowania rozkładu. Skupimy się na wykresie pudełkowym w stylu Tukeya, nazwanym na cześć statystyka Johna Tukeya, który spopularyzował ten typ wykresu.\n\nZrozumienie Wykresów Pudełkowych\nWykres pudełkowy przedstawia pięć kluczowych statystyk:\n\nWartość minimalna (z wyłączeniem wartości odstających)\nPierwszy kwartyl (Q1)\nMediana\nTrzeci kwartyl (Q3)\nWartość maksymalna (z wyłączeniem wartości odstających)\n\nDodatkowo wykresy pudełkowe pokazują:\n\nWąsy: Linie rozciągające się od pudełka do wartości minimalnej i maksymalnej (z wyłączeniem wartości odstających)\nWartości odstające: Indywidualne punkty poza wąsami\n\n\nObliczanie Kwartyli i Wartości Odstających\nAby stworzyć wykres pudełkowy, postępuj zgodnie z tymi krokami:\n\nUporządkuj dane od najmniejszej do największej wartości.\nZnajdź medianę (środkowa wartość dla nieparzystej liczby punktów danych, średnia z dwóch środkowych wartości dla parzystej).\nZnajdź Q1 (mediana dolnej połowy danych) i Q3 (mediana górnej połowy danych).\nOblicz Rozstęp Międzykwartylowy (IQR) = Q3 - Q1\nOkreśl wartości odstające używając reguły Tukeya:\n\nDolne wartości odstające: &lt; Q1 - 1.5 * IQR\nGórne wartości odstające: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nWspółczynnik 1.5 w regule Tukeya dla wartości odstających opiera się na właściwościach rozkładu normalnego. Dla danych o rozkładzie normalnym, ta reguła identyfikuje około 0.7% danych jako potencjalne wartości odstające.\n\n\n\n\nPrzykładowe Dane\nUżyjmy małego zbioru danych do ilustracji:\n\ndane &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\nRęcznie Rysowany Wykres Pudełkowy Tukeya\nAby stworzyć wykres pudełkowy Tukeya ręcznie:\n\nNarysuj linię pionową reprezentującą zakres od minimum do maksimum (2 do 15 w naszym przykładzie, z wyłączeniem wartości odstającej).\nNarysuj pudełko od Q1 do Q3.\nNarysuj poziomą linię przez pudełko na poziomie mediany.\nNarysuj wąsy od pudełka do wartości minimalnej i maksymalnej (z wyłączeniem wartości odstających).\nPrzedstaw wartość odstającą (50) jako indywidualny punkt poza wąsem.\nDodaj skalę do osi pionowej i oznacz ją.\n\n\n\nWykres Pudełkowy w Podstawowym R\n\n# Tworzenie wykresu pudełkowego\nboxplot(dane, main = \"Wykres Pudełkowy Przykładowych Danych\",\n        ylab = \"Wartości\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\nWykres Pudełkowy Tukeya z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wartosc = dane)\n\n# Tworzenie wykresu pudełkowego Tukeya z ggplot2\nggplot(df, aes(x = \"\", y = wartosc)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Wykres Pudełkowy Tukeya Przykładowych Danych\",\n       x = \"\", y = \"Wartości\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\nInterpretacja Wykresów Pudełkowych\nPodczas interpretacji wykresu pudełkowego zwróć uwagę na następujące elementy:\n\nTendencja Centralna: Mediana pokazuje środek rozkładu.\nRozrzut: Pudełko (IQR) reprezentuje środkowe 50% danych.\nSkośność: Jeśli linia mediany jest bliżej jednego końca pudełka, rozkład jest skośny.\nWartości Odstające: Punkty poza wąsami są potencjalnymi wartościami odstającymi.\nPorównania: Przy porównywaniu wielu wykresów pudełkowych, zwróć uwagę na względne położenie median, rozmiary pudełek i obecność wartości odstających.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "href": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "title": "12  Wizualizacja Danych: z przykładami w R",
    "section": "12.5 Zaawansowane Techniki Wizualizacji",
    "text": "12.5 Zaawansowane Techniki Wizualizacji\nOprócz podstawowych typów wykresów, warto poznać kilka bardziej zaawansowanych technik wizualizacji, które mogą być przydatne w analizie danych.\n\nWykresy Skrzypcowe\nWykresy skrzypcowe łączą cechy wykresów pudełkowych i wykresów gęstości, dając bardziej kompletny obraz rozkładu danych.\n\n# Tworzenie wykresu skrzypcowego\nggplot(df, aes(x = \"\", y = wartosc)) +\n  geom_violin(fill = \"lightblue\") +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Wykres Skrzypcowy Przykładowych Danych\",\n       x = \"\", y = \"Wartości\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWykresy Rozrzutu z Marginesami\nŁączenie wykresów rozrzutu z histogramami na marginesach może dostarczyć więcej informacji o rozkładzie danych w dwóch wymiarach.\n\n# Generowanie danych do wykresu rozrzutu\nset.seed(123)\ndf_scatter &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100)\n)\n\n# Tworzenie wykresu rozrzutu z marginesami\nlibrary(ggExtra)\np &lt;- ggplot(df_scatter, aes(x = x, y = y)) +\n  geom_point() +\n  theme_minimal()\nggMarginal(p, type = \"histogram\", fill = \"lightblue\")",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wnioski",
    "href": "rozdzial6.html#wnioski",
    "title": "12  Wizualizacja Danych: z przykładami w R",
    "section": "12.6 Wnioski",
    "text": "12.6 Wnioski\nW tym rozdziale poznaliśmy trzy podstawowe typy wizualizacji danych: wykresy słupkowe, histogramy i wykresy pudełkowe. Pokazaliśmy, jak tworzyć te wykresy ręcznie, używając podstawowego systemu wykresów R oraz biblioteki ggplot2.\nKażdy typ wykresu służy innemu celowi: - Wykresy słupkowe doskonale nadają się do porównywania kategorii. - Histogramy pokazują rozkład zmiennej ciągłej. - Wykresy pudełkowe dostarczają zwięzłego podsumowania rozkładu, podkreślając tendencję centralną, rozrzut i wartości odstające.\nPamiętaj, że wybór wizualizacji zależy od typu danych i wniosków, które chcesz przekazać. Zawsze bierz pod uwagę swoją docelową grupę odbiorców i historię, którą chcesz opowiedzieć za pomocą swoich danych, wybierając i projektując wizualizacje.\nĆwicz tworzenie tych wykresów ręcznie, aby pogłębić zrozumienie ich konstrukcji i interpretacji. Następnie wykorzystaj moc R i ggplot2, aby szybko tworzyć i dostosowywać te wizualizacje dla większych zbiorów danych i bardziej złożonych analiz.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#ćwiczenia-praktyczne",
    "href": "rozdzial6.html#ćwiczenia-praktyczne",
    "title": "12  Wizualizacja Danych: z przykładami w R",
    "section": "12.7 Ćwiczenia Praktyczne",
    "text": "12.7 Ćwiczenia Praktyczne\n\nZbierz dane o popularności różnych gatunków muzycznych wśród Twoich znajomych. Stwórz wykres słupkowy przedstawiający te dane.\nZmierz czas reakcji 30 osób na bodziec dźwiękowy (w milisekundach). Utwórz histogram tych danych.\nZbierz dane o wzroście 50 osób w Twojej społeczności. Stwórz wykres pudełkowy dla tych danych, osobno dla mężczyzn i kobiet.\nZnajdź zestaw danych online (np. na Kaggle) i stwórz trzy różne wizualizacje dla tych danych. Opisz, jakie wnioski można wyciągnąć z każdej wizualizacji.\nStwórz wykres skrzypcowy dla danych o cenach domów w różnych dzielnicach miasta. Porównaj go z wykresem pudełkowym tych samych danych. Jakie dodatkowe informacje dostarcza wykres skrzypcowy?\n\nPamiętaj, że praktyka jest kluczem do opanowania sztuki wizualizacji danych. Eksperymentuj z różnymi typami wykresów i parametrami, aby znaleźć najlepszy sposób przedstawienia swoich danych.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "correg_en.html",
    "href": "correg_en.html",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "",
    "text": "13.1 Introduction\nThe distinction between correlation and causation represents a fundamental challenge in statistical analysis. While correlation measures the statistical association between variables, causation implies a direct influence of one variable on another.\nStatistical relationships form the backbone of data-driven decision making across disciplines—from economics and public health to psychology and environmental science. Understanding when a relationship indicates mere association versus genuine causality is crucial for valid inference and effective policy recommendations.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#covariance",
    "href": "correg_en.html#covariance",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.2 Covariance",
    "text": "13.2 Covariance\nCovariance measures how two variables vary together, indicating both the direction and magnitude of their linear relationship.\nFormula: \\text{cov}(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\nWhere:\n\nx_i and y_i are individual data points\n\\bar{x} and \\bar{y} are the means of variables X and Y\nn is the number of observations\nWe divide by (n-1) for sample covariance (Bessel’s correction)\n\n\nStep-by-Step Manual Calculation Process\nExample 1: Student Study Hours vs. Test Scores\nData:\n\nStudy Hours (X): 2, 4, 6, 8, 10\nTest Scores (Y): 65, 70, 80, 85, 95\n\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate means\n\\bar{x} = \\frac{2+4+6+8+10}{5} = 6 hours\n\n\n\n\n\\bar{y} = \\frac{65+70+80+85+95}{5} = 79 points\n\n\n2\nCalculate deviations\n(x_i - \\bar{x}): -4, -2, 0, 2, 4\n\n\n\n\n(y_i - \\bar{y}): -14, -9, 1, 6, 16\n\n\n3\nCalculate products\n(x_i - \\bar{x})(y_i - \\bar{y}):\n\n\n\n\n(-4)(-14) = 56\n\n\n\n\n(-2)(-9) = 18\n\n\n\n\n(0)(1) = 0\n\n\n\n\n(2)(6) = 12\n\n\n\n\n(4)(16) = 64\n\n\n4\nSum the products\n\\sum = 56 + 18 + 0 + 12 + 64 = 150\n\n\n5\nDivide by (n-1)\n\\text{cov}(X,Y) = \\frac{150}{5-1} = \\frac{150}{4} = 37.5\n\n\n\nR Verification:\n\n# Define the data\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Calculate covariance\ncov_manual &lt;- cov(study_hours, test_scores)\nprint(paste(\"Covariance:\", round(cov_manual, 2)))\n\n[1] \"Covariance: 37.5\"\n\n# Verify step by step\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\nproducts &lt;- x_dev * y_dev\ncov_calculated &lt;- sum(products) / (length(study_hours) - 1)\n\n# Display calculation steps\ndata.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_deviation = x_dev,\n  Y_deviation = y_dev,\n  Product = products\n)\n\n   X  Y X_deviation Y_deviation Product\n1  2 65          -4         -14      56\n2  4 70          -2          -9      18\n3  6 80           0           1       0\n4  8 85           2           6      12\n5 10 95           4          16      64\n\n\nInterpretation: The positive covariance (37.5) indicates that study hours and test scores tend to increase together.\n\n\nPractice Problem with Solution\nCalculate covariance manually for:\n\nTemperature (°F): 32, 50, 68, 86, 95\nIce Cream Sales ($): 100, 200, 400, 600, 800\n\nSolution:\n\n\n\nStep\nCalculation\n\n\n\n\n1. Means\n\\bar{x} = \\frac{32+50+68+86+95}{5} = 66.2°F\n\n\n\n\\bar{y} = \\frac{100+200+400+600+800}{5} = 420\n\n\n2. Deviations\nX: -34.2, -16.2, 1.8, 19.8, 28.8\n\n\n\nY: -320, -220, -20, 180, 380\n\n\n3. Products\n10944, 3564, -36, 3564, 10944\n\n\n4. Sum\n28980\n\n\n5. Covariance\n\\frac{28980}{4} = 7245\n\n\n\n\n# Verify practice problem\ntemp &lt;- c(32, 50, 68, 86, 95)\nsales &lt;- c(100, 200, 400, 600, 800)\nprint(paste(\"Covariance for practice problem:\", round(cov(temp, sales), 2)))\n\n[1] \"Covariance for practice problem: 7245\"",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#correlation-coefficient",
    "href": "correg_en.html#correlation-coefficient",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.3 Correlation Coefficient",
    "text": "13.3 Correlation Coefficient\nThe correlation coefficient standardizes covariance to eliminate scale dependency, producing values between -1 and +1.\n\nInterpretation Guidelines\n\n\n\n\n\n\n\n\n\nCorrelation Value\nStrength\nInterpretation\nExample\n\n\n\n\n±0.90 to ±1.00\nVery Strong\nAlmost perfect relationship\nHeight of parents and children\n\n\n±0.70 to ±0.89\nStrong\nHighly related variables\nStudy time and grades\n\n\n±0.50 to ±0.69\nModerate\nModerately related\nExercise and weight loss\n\n\n±0.30 to ±0.49\nWeak\nWeakly related\nShoe size and reading ability\n\n\n±0.00 to ±0.29\nVery Weak/None\nLittle to no relationship\nBirth month and intelligence\n\n\n\n\n\nTypes of Correlations Visualization\n\n# Generate sample data with different correlation patterns\nn &lt;- 100\n\n# Positive linear correlation\nset.seed(123)\nyears_education &lt;- rnorm(n, 14, 3)\nannual_income &lt;- 15000 + 3500 * years_education + rnorm(n, 0, 10000)\npos_cor &lt;- round(cor(years_education, annual_income), 3)\n\n# Negative linear correlation\nscreen_time &lt;- runif(n, 1, 8)\nsleep_hours &lt;- 9 - 0.5 * screen_time + rnorm(n, 0, 0.5)\nneg_cor &lt;- round(cor(screen_time, sleep_hours), 3)\n\n# No correlation\nx_random &lt;- rnorm(n, 50, 10)\ny_random &lt;- rnorm(n, 50, 10)\nno_cor &lt;- round(cor(x_random, y_random), 3)\n\n# Non-linear correlation (quadratic)\nhours_studied &lt;- runif(n, 0, 12)\ntest_score &lt;- -2 * (hours_studied - 6)^2 + 90 + rnorm(n, 0, 5)\nnonlin_cor &lt;- round(cor(hours_studied, test_score), 3)\n\n# Create data frames with correlation values\npositive_data &lt;- data.frame(\n  x = years_education, \n  y = annual_income,\n  label = paste0(\"Positive Linear (r = \", pos_cor, \")\")\n)\n\nnegative_data &lt;- data.frame(\n  x = screen_time, \n  y = sleep_hours,\n  label = paste0(\"Negative Linear (r = \", neg_cor, \")\")\n)\n\nno_corr_data &lt;- data.frame(\n  x = x_random, \n  y = y_random,\n  label = paste0(\"No Correlation (r = \", no_cor, \")\")\n)\n\nnonlinear_data &lt;- data.frame(\n  x = hours_studied, \n  y = test_score,\n  label = paste0(\"Non-linear (Pearson r = \", nonlin_cor, \")\")\n)\n\n# Combine data\nall_data &lt;- rbind(positive_data, negative_data, no_corr_data, nonlinear_data)\n\n# Create faceted plot\nggplot(all_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE, alpha = 0.3) +\n  facet_wrap(~ label, scales = \"free\", ncol = 2) +\n  labs(\n    title = \"Different Types of Correlations\",\n    subtitle = \"Linear regression line shown in red with confidence band\",\n    x = \"\", \n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, color = \"gray50\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#pearson-correlation",
    "href": "correg_en.html#pearson-correlation",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.4 Pearson Correlation",
    "text": "13.4 Pearson Correlation\nFormula: r = \\frac{\\text{cov}(X,Y)}{s_X s_Y} = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}}\n\nComplete Manual Calculation Example\nUsing our study hours example:\n\nStudy Hours (X): 2, 4, 6, 8, 10\nTest Scores (Y): 65, 70, 80, 85, 95\n\nDetailed Calculation Steps:\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate covariance\nFrom above: \\text{cov}(X,Y) = 37.5\n\n\n2\nCalculate deviations squared\n\n\n\n\nFor X\n(x_i - \\bar{x})^2: 16, 4, 0, 4, 16\n\n\n\n\nSum = 40\n\n\n\nFor Y\n(y_i - \\bar{y})^2: 196, 81, 1, 36, 256\n\n\n\n\nSum = 570\n\n\n3\nCalculate standard deviations\n\n\n\n\ns_X\ns_X = \\sqrt{\\frac{40}{4}} = \\sqrt{10} = 3.162\n\n\n\ns_Y\ns_Y = \\sqrt{\\frac{570}{4}} = \\sqrt{142.5} = 11.937\n\n\n4\nCalculate correlation\nr = \\frac{37.5}{3.162 \\times 11.937}\n\n\n\n\nr = \\frac{37.5}{37.73} = 0.994\n\n\n\n\n# Manual calculation verification\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Calculate Pearson correlation\nr_value &lt;- cor(study_hours, test_scores, method = \"pearson\")\nprint(paste(\"Pearson correlation coefficient:\", round(r_value, 3)))\n\n[1] \"Pearson correlation coefficient: 0.993\"\n\n# Detailed calculation\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\n\n# Show calculation table\ncalc_table &lt;- data.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_dev = x_dev,\n  Y_dev = y_dev,\n  X_dev_sq = x_dev^2,\n  Y_dev_sq = y_dev^2,\n  Product = x_dev * y_dev\n)\nprint(calc_table)\n\n   X  Y X_dev Y_dev X_dev_sq Y_dev_sq Product\n1  2 65    -4   -14       16      196      56\n2  4 70    -2    -9        4       81      18\n3  6 80     0     1        0        1       0\n4  8 85     2     6        4       36      12\n5 10 95     4    16       16      256      64\n\n# Summary statistics\ncat(\"\\nSummary:\")\n\n\nSummary:\n\ncat(\"\\nSum of (X-mean)²:\", sum(x_dev^2))\n\n\nSum of (X-mean)²: 40\n\ncat(\"\\nSum of (Y-mean)²:\", sum(y_dev^2))\n\n\nSum of (Y-mean)²: 570\n\ncat(\"\\nSum of products:\", sum(x_dev * y_dev))\n\n\nSum of products: 150\n\ncat(\"\\nStandard deviation of X:\", sd(study_hours))\n\n\nStandard deviation of X: 3.162278\n\ncat(\"\\nStandard deviation of Y:\", sd(test_scores))\n\n\nStandard deviation of Y: 11.93734\n\n# Calculate confidence interval and p-value\ncor_test &lt;- cor.test(study_hours, test_scores, method = \"pearson\")\nprint(cor_test)\n\n\n    Pearson's product-moment correlation\n\ndata:  study_hours and test_scores\nt = 15, df = 3, p-value = 0.0006431\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8994446 0.9995859\nsample estimates:\n      cor \n0.9933993 \n\n\nInterpretation: r = 0.994 indicates an almost perfect positive linear relationship between study hours and test scores. The p-value &lt; 0.05 suggests this relationship is statistically significant.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#spearman-rank-correlation",
    "href": "correg_en.html#spearman-rank-correlation",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.5 Spearman Rank Correlation",
    "text": "13.5 Spearman Rank Correlation\nSpearman correlation measures monotonic relationships using ranks instead of raw values.\nFormula: \\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2 - 1)}\nWhere d_i is the difference between ranks for observation i.\n\nComplete Manual Example\nData: Math and English Scores\n\n\n\nStudent\nMath Score\nEnglish Score\n\n\n\n\nA\n78\n82\n\n\nB\n85\n90\n\n\nC\n88\n88\n\n\nD\n92\n94\n\n\nE\n95\n96\n\n\n\nRanking and Calculation:\n\n\n\n\n\n\n\n\n\n\n\n\nStudent\nMath Score\nMath Rank\nEnglish Score\nEnglish Rank\nd = (Math Rank - English Rank)\nd²\n\n\n\n\nA\n78\n1\n82\n1\n0\n0\n\n\nB\n85\n2\n90\n3\n-1\n1\n\n\nC\n88\n3\n88\n2\n1\n1\n\n\nD\n92\n4\n94\n4\n0\n0\n\n\nE\n95\n5\n96\n5\n0\n0\n\n\n\n\n\n\n\nSum:\n2\n\n\n\nCalculation: \\rho = 1 - \\frac{6 \\times 2}{5(25-1)} = 1 - \\frac{12}{120} = 1 - 0.1 = 0.9\n\n# Data\nmath_scores &lt;- c(78, 85, 88, 92, 95)\nenglish_scores &lt;- c(82, 90, 88, 94, 96)\n\n# Show ranks\nrank_table &lt;- data.frame(\n  Student = LETTERS[1:5],\n  Math_Score = math_scores,\n  Math_Rank = rank(math_scores),\n  English_Score = english_scores,\n  English_Rank = rank(english_scores)\n)\n\nrank_table$d &lt;- rank_table$Math_Rank - rank_table$English_Rank\nrank_table$d_squared &lt;- rank_table$d^2\n\nprint(rank_table)\n\n  Student Math_Score Math_Rank English_Score English_Rank  d d_squared\n1       A         78         1            82            1  0         0\n2       B         85         2            90            3 -1         1\n3       C         88         3            88            2  1         1\n4       D         92         4            94            4  0         0\n5       E         95         5            96            5  0         0\n\ncat(\"\\nSum of d²:\", sum(rank_table$d_squared))\n\n\nSum of d²: 2\n\n# Calculate Spearman correlation\nrho &lt;- cor(math_scores, english_scores, method = \"spearman\")\ncat(\"\\nSpearman correlation (R):\", round(rho, 3))\n\n\nSpearman correlation (R): 0.9\n\n# Manual calculation\nn &lt;- length(math_scores)\nrho_manual &lt;- 1 - (6 * sum(rank_table$d_squared)) / (n * (n^2 - 1))\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.9",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#cross-tabulation-and-categorical-data",
    "href": "correg_en.html#cross-tabulation-and-categorical-data",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.6 Cross-tabulation and Categorical Data",
    "text": "13.6 Cross-tabulation and Categorical Data\nCross-tabulation shows relationships between categorical variables.\n\n# Create more realistic sample data\nset.seed(123)\nn_total &lt;- 120\n\n# Create education and employment data with realistic relationship\neducation &lt;- factor(\n  c(rep(\"High School\", 40), \n    rep(\"College\", 50), \n    rep(\"Graduate\", 30)),\n  levels = c(\"High School\", \"College\", \"Graduate\")\n)\n\n# Employment status with education-related probabilities\nemployment &lt;- factor(\n  c(# High School: 75% employed\n    sample(c(rep(\"Employed\", 30), rep(\"Unemployed\", 10)), 40),\n    # College: 90% employed  \n    sample(c(rep(\"Employed\", 45), rep(\"Unemployed\", 5)), 50),\n    # Graduate: 95% employed\n    sample(c(rep(\"Employed\", 28), rep(\"Unemployed\", 2)), 30)),\n  levels = c(\"Employed\", \"Unemployed\")\n)\n\n# Create contingency table\ncontingency_table &lt;- table(education, employment)\nprint(\"Contingency Table:\")\n\n[1] \"Contingency Table:\"\n\nprint(contingency_table)\n\n             employment\neducation     Employed Unemployed\n  High School       30         10\n  College           45          5\n  Graduate          28          2\n\n# Calculate row percentages\nprint(\"\\nRow Percentages (% within each education level):\")\n\n[1] \"\\nRow Percentages (% within each education level):\"\n\nprint(round(prop.table(contingency_table, 1) * 100, 1))\n\n             employment\neducation     Employed Unemployed\n  High School     75.0       25.0\n  College         90.0       10.0\n  Graduate        93.3        6.7\n\n# Chi-square test for independence\nchi_test &lt;- chisq.test(contingency_table)\n\nWarning in chisq.test(contingency_table): Chi-squared approximation may be\nincorrect\n\nprint(chi_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 5.9623, df = 2, p-value = 0.05073",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#practical-exercises-with-solutions",
    "href": "correg_en.html#practical-exercises-with-solutions",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.7 Practical Exercises with Solutions",
    "text": "13.7 Practical Exercises with Solutions\n\nExercise 1: Calculate Pearson Correlation Manually\nData:\n\nHeight (inches): 66, 68, 70, 72, 74\nWeight (pounds): 140, 155, 170, 185, 200\n\nSolution:\n\n# Data\nheight &lt;- c(66, 68, 70, 72, 74)\nweight &lt;- c(140, 155, 170, 185, 200)\n\n# Step 1: Calculate means\nh_mean &lt;- mean(height)  # 70\nw_mean &lt;- mean(weight)  # 170\n\n# Step 2: Calculate deviations and products\ncalculation_df &lt;- data.frame(\n  Height = height,\n  Weight = weight,\n  H_dev = height - h_mean,\n  W_dev = weight - w_mean,\n  H_dev_sq = (height - h_mean)^2,\n  W_dev_sq = (weight - w_mean)^2,\n  Product = (height - h_mean) * (weight - w_mean)\n)\n\nprint(calculation_df)\n\n  Height Weight H_dev W_dev H_dev_sq W_dev_sq Product\n1     66    140    -4   -30       16      900     120\n2     68    155    -2   -15        4      225      30\n3     70    170     0     0        0        0       0\n4     72    185     2    15        4      225      30\n5     74    200     4    30       16      900     120\n\n# Step 3: Calculate correlation\ncov_hw &lt;- sum(calculation_df$Product) / (length(height) - 1)\nsd_h &lt;- sqrt(sum(calculation_df$H_dev_sq) / (length(height) - 1))\nsd_w &lt;- sqrt(sum(calculation_df$W_dev_sq) / (length(weight) - 1))\nr_manual &lt;- cov_hw / (sd_h * sd_w)\n\ncat(\"\\nManual calculation:\")\n\n\nManual calculation:\n\ncat(\"\\nCovariance:\", round(cov_hw, 2))\n\n\nCovariance: 75\n\ncat(\"\\nSD Height:\", round(sd_h, 2))\n\n\nSD Height: 3.16\n\ncat(\"\\nSD Weight:\", round(sd_w, 2))\n\n\nSD Weight: 23.72\n\ncat(\"\\nCorrelation:\", round(r_manual, 3))\n\n\nCorrelation: 1\n\n# Verify with R\ncat(\"\\n\\nR calculation:\", round(cor(height, weight), 3))\n\n\n\nR calculation: 1\n\n\n\n\nExercise 2: Calculate Spearman Correlation Manually\nData:\n\nStudent rankings in Math: 1, 3, 2, 5, 4\nStudent rankings in Science: 2, 4, 1, 5, 3\n\nSolution:\n\n# Rankings (already ranked)\nmath_rank &lt;- c(1, 3, 2, 5, 4)\nscience_rank &lt;- c(2, 4, 1, 5, 3)\n\n# Calculate differences\nd &lt;- math_rank - science_rank\nd_squared &lt;- d^2\n\n# Create table\nrank_df &lt;- data.frame(\n  Student = 1:5,\n  Math_Rank = math_rank,\n  Science_Rank = science_rank,\n  d = d,\n  d_squared = d_squared\n)\n\nprint(rank_df)\n\n  Student Math_Rank Science_Rank  d d_squared\n1       1         1            2 -1         1\n2       2         3            4 -1         1\n3       3         2            1  1         1\n4       4         5            5  0         0\n5       5         4            3  1         1\n\n# Calculate Spearman correlation\nn &lt;- length(math_rank)\nsum_d_sq &lt;- sum(d_squared)\nrho_manual &lt;- 1 - (6 * sum_d_sq) / (n * (n^2 - 1))\n\ncat(\"\\nSum of d²:\", sum_d_sq)\n\n\nSum of d²: 4\n\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.8\n\ncat(\"\\nSpearman correlation (R):\", round(cor(math_rank, science_rank, method = \"spearman\"), 3))\n\n\nSpearman correlation (R): 0.8\n\n\n\n\nExercise 3: Interpretation Practice\nInterpret these correlation values:\n\nr = 0.85 between hours of practice and performance score\n\nAnswer: Strong positive relationship. As practice hours increase, performance scores tend to increase substantially.\n\nr = -0.72 between outside temperature and heating costs\n\nAnswer: Strong negative relationship. As temperature increases, heating costs decrease substantially.\n\nr = 0.12 between shoe size and intelligence\n\nAnswer: Very weak/no meaningful relationship. Shoe size and intelligence are essentially unrelated.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#important-points-to-remember",
    "href": "correg_en.html#important-points-to-remember",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.8 Important Points to Remember",
    "text": "13.8 Important Points to Remember\n\nCorrelation measures relationship strength: Values range from -1 to +1\nCorrelation ≠ Causation: High correlation doesn’t prove one variable causes another\nChoose the right method:\n\nPearson: For linear relationships in continuous data\nSpearman: For monotonic relationships or ranked data\n\nCheck assumptions:\n\nPearson assumes linear relationship and normal distribution\nSpearman only assumes monotonic relationship\n\nWatch for outliers: Extreme values can greatly affect Pearson correlation\nVisualize your data: Always plot before calculating correlation",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-decision-tree-for-correlation-analysis",
    "href": "correg_en.html#summary-decision-tree-for-correlation-analysis",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.9 Summary: Decision Tree for Correlation Analysis",
    "text": "13.9 Summary: Decision Tree for Correlation Analysis\n\n\n\nCHOOSING THE RIGHT CORRELATION METHOD:\n\nIs your data numerical?\n├─ YES → Is the relationship linear?\n│   ├─ YES → Use PEARSON correlation\n│   └─ NO → Is it monotonic?\n│       ├─ YES → Use SPEARMAN correlation\n│       └─ NO → Consider non-linear methods\n└─ NO → Is it ordinal (ranked)?\n    ├─ YES → Use SPEARMAN correlation\n    └─ NO → Use CROSS-TABULATION for categorical data",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#additional-practice-problems",
    "href": "correg_en.html#additional-practice-problems",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.10 Additional Practice Problems",
    "text": "13.10 Additional Practice Problems\n\nProblem Set A: Manual Calculations\n\nCalculate covariance and Pearson correlation for:\n\nX: 10, 20, 30, 40, 50\nY: 15, 25, 35, 45, 55\n\nSolution: Cov(X,Y) = 250, r = 1.0 (perfect positive correlation)\nCalculate Spearman correlation for movie ratings:\n\nMovie A ratings: 8, 6, 9, 7, 5\nMovie B ratings: 7, 8, 9, 6, 4\n\nSolution: ρ = 0.3 (weak positive correlation)\n\n\n\nProblem Set B: Interpretation\n\nA study finds r = 0.91 between hours of sleep and test performance.\n\nInterpretation: Very strong positive relationship suggesting that more sleep is associated with better test performance. However, this doesn’t prove causation—other factors might be involved.\n\nAnother study finds r = -0.03 between birth month and IQ scores.\n\nInterpretation: No meaningful relationship. Birth month and IQ are essentially unrelated.\n\n\n\n\nQuick Reference Card\n\n\n\n\n\n\n\n\n\nMeasure\nUse When\nFormula\nRange\n\n\n\n\nCovariance\nInitial exploration of relationship\n\\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n-∞ to +∞\n\n\nPearson r\nLinear relationships, continuous data\n\\frac{\\text{cov}(X,Y)}{s_X s_Y}\n-1 to +1\n\n\nSpearman ρ\nMonotonic relationships, ranked data\n1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n-1 to +1\n\n\nCross-tabs\nCategorical variables\nFrequency counts\nN/A",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#understanding-ordinary-least-squares-ols-a-quick-start-guide",
    "href": "correg_en.html#understanding-ordinary-least-squares-ols-a-quick-start-guide",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.11 Understanding Ordinary Least Squares (OLS): A Quick-start Guide",
    "text": "13.11 Understanding Ordinary Least Squares (OLS): A Quick-start Guide\n\n\n\n\n\n\nUnderstanding Ordinary Least Squares (OLS): A Quick-start Guide\n\n\n\n\nIntroduction: What is Regression Analysis?\nRegression analysis helps us understand and measure relationships between things we can observe. It provides mathematical tools to identify patterns in data that help us make predictions.\nConsider these research questions:\n\nHow does study time affect test scores?\nHow does experience affect salary?\nHow does advertising spending influence sales?\n\nRegression gives us systematic methods to answer these questions with real data.\n\n\nThe Starting Point: A Simple Example\nLet’s begin with something concrete. You’ve collected data from 20 students in your class:\n\n\n\nStudent\nStudy Hours\nExam Score\n\n\n\n\nAlex\n2\n68\n\n\nBeth\n4\n74\n\n\nCarlos\n6\n85\n\n\nDiana\n8\n91\n\n\n…\n…\n…\n\n\n\nWhen you plot this data, you get a scatter plot with dots all over. Your goal: find the straight line that best describes the relationship between study hours and exam scores.\nBut what does “best” mean? That’s what we’ll discover.\n\n\nWhy Real Data Doesn’t Fall on a Perfect Line\nBefore diving into the math, let’s understand why data points don’t line up perfectly.\n\nDeterministic vs. Stochastic Models\nDeterministic Models describe relationships with no uncertainty. Think of physics equations: \\text{Distance} = \\text{Speed} × \\text{Time}\nIf you drive at exactly 60 mph for exactly 2 hours, you’ll always travel exactly 120 miles. No variation, no exceptions.\nStochastic Models acknowledge that real-world data contains natural variation. The fundamental structure is: Y = f(X) + \\epsilon\nWhere:\n\nY is what we’re trying to predict (exam scores)\nf(X) is the systematic pattern (how study hours typically affect scores)\n\\epsilon (epsilon) represents all the random stuff we can’t measure\n\nIn our example, two students might study for 5 hours but get different scores because:\n\nOne slept better the night before\nOne is naturally better at test-taking\nOne had a noisy roommate during the exam\nPure chance in which questions were asked\n\nThis randomness is natural and expected - that’s what \\epsilon captures.\n\n\n\nThe Simple Linear Regression Model\nWe express the relationship between study hours and exam scores as: Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\nLet’s decode this:\n\nY_i = exam score for student i\nX_i = study hours for student i\n\\beta_0 = the intercept (baseline score with zero study hours)\n\\beta_1 = the slope (points gained per study hour)\n\\epsilon_i = everything else affecting student i’s score\n\nKey insight: We never know the true values of \\beta_0 and \\beta_1. Instead, we use our data to estimate them, calling our estimates \\hat{\\beta}_0 and \\hat{\\beta}_1 (the “hats” mean “estimated”).\n\n\nUnderstanding Residuals: How Wrong Are Our Predictions?\nOnce we draw a line through our data, we can make predictions. For each student:\n\nActual score (y_i): What they really got\nPredicted score (\\hat{y}_i): What our line says they should have gotten\nResidual (e_i): The difference = Actual - Predicted\n\nVisual Example:\nDiana: Studied 8 hours, scored 91\nOur line predicts: 88 points\nResidual: 91 - 88 = +3 points (we underestimated)\n\nEric: Studied 5 hours, scored 70\nOur line predicts: 79 points  \nResidual: 70 - 79 = -9 points (we overestimated)\n\n\nThe Key Insight: Why Square the Residuals?\nHere’s a puzzle. Consider these residuals from four students:\n\nStudent A: +5 points\nStudent B: -5 points\nStudent C: +3 points\nStudent D: -3 points\n\nIf we just add them: (+5) + (-5) + (+3) + (-3) = 0\nThis suggests perfect predictions, but every prediction was wrong! The positive and negative errors canceled out.\nThe solution: Square each residual before adding:\n\nStudent A: (+5)^2 = 25\nStudent B: (-5)^2 = 25\nStudent C: (+3)^2 = 9\nStudent D: (-3)^2 = 9\nTotal squared error: 68\n\nWhy squaring works:\n\nNo more cancellation: All squared values are positive\nBigger errors matter more: A 10-point error counts 4× as much as a 5-point error\nMathematical convenience: Squared functions are smooth and differentiable\n\n\n\nThe Ordinary Least Squares Method\nOLS finds the line that minimizes the Sum of Squared Errors (SSE):\n\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\nExpanding this: \\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1x_i))^2\nIn plain English: “Find the intercept and slope that make the total squared prediction error as small as possible.”\n\n\nThe Mathematical Solution (Formal Derivation)\nTo minimize SSE, we use calculus. Taking partial derivatives and setting them to zero:\n\\frac{\\partial SSE}{\\partial \\beta_0} = -2\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i) = 0\n\\frac{\\partial SSE}{\\partial \\beta_1} = -2\\sum_{i=1}^n x_i(y_i - \\beta_0 - \\beta_1x_i) = 0\nSolving this system of equations yields:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\nWhere \\bar{x} and \\bar{y} are the sample means.\nWhat this tells us:\n\nThe slope depends on how X and Y vary together (covariance) relative to how much X varies alone (variance)\nThe line always passes through the center point (\\bar{x}, \\bar{y})\n\n\n\nMaking Sense of Variation: How Good Is Our Line?\nTo evaluate our model’s performance, we break down the variation in exam scores:\n\nTotal Sum of Squares (SST)\n“How much do exam scores vary overall?” SST = \\sum_{i=1}^n(y_i - \\bar{y})^2\nThis measures how spread out the scores are from the class average.\n\n\nRegression Sum of Squares (SSR)\n“How much variation does our line explain?” SSR = \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2\nThis measures how much better our predictions are than just guessing the average for everyone.\n\n\nError Sum of Squares (SSE)\n“How much variation is left unexplained?” SSE = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\nThis is the variation our model couldn’t capture (the squared residuals).\n\n\nThe Fundamental Equation\nSST = SSR + SSE \\text{Total Variation} = \\text{Explained} + \\text{Unexplained}\n\n\n\nR-Squared: The Model Report Card\nThe coefficient of determination (R²) tells us what percentage of variation our model explains:\nR^2 = \\frac{\\text{Explained Variation}}{\\text{Total Variation}} = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nHow to interpret R²:\n\nR² = 0.75: “Study hours explain 75% of the variation in exam scores”\nR² = 0.30: “Our model captures 30% of what makes scores different”\nR² = 1.00: Perfect prediction (never happens with real data)\nR² = 0.00: Our line is no better than guessing the average\n\nImportant reality check: In social sciences, R² = 0.30 might be excellent. In engineering, R² = 0.95 might be the minimum acceptable. Context matters.\n\n\nInterpreting Your Results\nWhen you run OLS and get \\hat{\\beta}_0 = 60 and \\hat{\\beta}_1 = 4:\nThe Slope (\\hat{\\beta}_1 = 4):\n\n“Each additional hour of study is associated with 4 more points on the exam”\nThis is an average effect across all students\nIt’s not a guarantee for any individual student\n\nThe Intercept (\\hat{\\beta}_0 = 60):\n\n“A student who studies 0 hours is predicted to score 60”\nOften this is just a mathematical anchor point\nMay not make practical sense (who studies 0 hours?)\n\nThe Prediction Equation: \\text{Predicted Score} = 60 + 4 \\times \\text{Study Hours}\nSo a student studying 5 hours: Predicted score = 60 + 4(5) = 80 points\n\n\nEffect Size and Practical Significance\nStatistical significance tells us whether an effect exists. Practical significance tells us whether it matters. Understanding both is crucial for proper interpretation.\n\nCalculating and Interpreting Raw Effect Sizes\nThe raw (unstandardized) effect size is simply your slope coefficient \\hat{\\beta}_1.\nExample: If \\hat{\\beta}_1 = 4 points per hour:\n\nThis is the raw effect size\nInterpretation: “One hour of additional study yields 4 exam points”\n\nTo assess practical significance, consider:\n\nScale of the outcome: 4 points on a 100-point exam (4%) vs. 4 points on a 500-point exam (0.8%)\nCost of the intervention: Is one hour of study time worth 4 points?\nContext-specific thresholds: Does 4 points change a letter grade?\n\n\n\nCalculating Standardized Effect Sizes\nStandardized effects allow comparison across different scales and studies.\nFormula for standardized coefficient (beta weight): \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y}\nWhere:\n\ns_X = standard deviation of X (study hours)\ns_Y = standard deviation of Y (exam scores)\n\nStep-by-step calculation:\n\nCalculate the standard deviation of X: s_X = \\sqrt{\\frac{\\sum(x_i - \\bar{x})^2}{n-1}}\nCalculate the standard deviation of Y: s_Y = \\sqrt{\\frac{\\sum(y_i - \\bar{y})^2}{n-1}}\nMultiply: \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y}\n\nExample calculation:\n\nSuppose s_X = 2.5 hours and s_Y = 12 points\nWith \\hat{\\beta}_1 = 4: \\beta_{std} = 4 \\times \\frac{2.5}{12} = 0.83\nInterpretation: “A one standard deviation increase in study hours (2.5 hours) is associated with 0.83 standard deviations increase in exam score”\n\n\n\nCohen’s Guidelines for Effect Sizes\nFor standardized regression coefficients:\n\nSmall effect: |β| ≈ 0.10 (explains ~1% of variance)\nMedium effect: |β| ≈ 0.30 (explains ~9% of variance)\nLarge effect: |β| ≈ 0.50 (explains ~25% of variance)\n\nFor R² (proportion of variance explained):\n\nSmall effect: R² ≈ 0.02\nMedium effect: R² ≈ 0.13\nLarge effect: R² ≈ 0.26\n\nImportant: These are general benchmarks. Field-specific standards often differ:\n\nPsychology/Education: R² = 0.10 might be meaningful\nPhysics/Engineering: R² &lt; 0.90 might be unacceptable\nEconomics: R² = 0.30 might be excellent\n\n\n\nCalculating Confidence Intervals for Effect Sizes\nTo quantify uncertainty in your effect size:\nFor the raw coefficient: CI = \\hat{\\beta}_1 \\pm t_{critical} \\times SE(\\hat{\\beta}_1)\nWhere:\n\nt_{critical} = critical value from t-distribution (usually ≈ 2 for 95% CI)\nSE(\\hat{\\beta}_1) = standard error of the slope\n\nPractical interpretation: If 95% CI = [3.2, 4.8], we can say: “We’re 95% confident that each study hour adds between 3.2 and 4.8 exam points.”\n\n\nMaking Decisions About Practical Significance\nTo determine if an effect is practically significant, consider:\n\nMinimum meaningful difference: What’s the smallest effect that would matter?\n\nIn education: Often 0.25 standard deviations\nIn medicine: Determined by clinical relevance\nIn business: Based on cost-benefit analysis\n\nNumber needed to treat (NNT) analog: How much X must change for meaningful Y change?\n\nIf passing requires 10 more points and \\hat{\\beta}_1 = 4\nStudents need 2.5 more study hours to pass\n\nCost-effectiveness ratio: \\text{Efficiency} = \\frac{\\text{Effect Size}}{\\text{Cost of Intervention}}\n\nExample practical significance assessment:\n\nEffect: 4 points per study hour\nPassing threshold: 70 points\nCurrent average: 68 points\nConclusion: 30 minutes of extra study could change fail to pass\nDecision: Practically significant for borderline students\n\n\n\n\nUnderstanding Uncertainty: Nothing Is Perfect\nYour estimates come from a sample, not the entire population. This creates uncertainty.\n\nWhy We Have Uncertainty\n\nYou studied 20 students, not all students ever\nYour sample might be slightly unusual by chance\nMeasurement isn’t perfect (did students report hours accurately?)\n\n\n\nConfidence Intervals: Being Honest About Uncertainty\nInstead of saying “the effect is exactly 4 points per hour,” we say:\n\n“We estimate 4 points per hour”\n“We’re 95% confident the true effect is between 3.2 and 4.8 points”\n\nThis range (3.2 to 4.8) is called a 95% confidence interval.\nWhat it means: If we repeated this study many times with different samples, 95% of the intervals we calculate would contain the true effect.\nWhat it doesn’t mean: There’s a 95% chance the true value is in this specific interval (it either is or isn’t).\n\n\nTesting If There’s Really a Relationship\nThe big question: “Is there actually a relationship, or did we just get lucky with our sample?”\nWe test this by asking: “If study hours truly had zero effect on scores, how likely would we be to see a pattern this strong just by chance?”\nThe process (simplified):\n\nAssume there’s no relationship (the “null hypothesis”)\nCalculate how unlikely our data would be if that were true\nIf it’s very unlikely (typically less than 5% chance), we conclude there probably is a relationship\n\nP-values in plain English:\n\np = 0.03: “If study hours didn’t matter at all, there’s only a 3% chance we’d see a pattern this strong by luck”\np = 0.40: “This pattern could easily happen by chance even if there’s no real relationship”\n\nRule of thumb: p &lt; 0.05 → “statistically significant” (probably a real relationship)\n\n\n\nWhen Things Go Wrong: Model Diagnostics\n\nQuick Visual Checks\n\nPlot your data first: Does it look roughly linear?\nPlot residuals vs. predicted values: Should look like a random cloud\nLook for outliers: Any points way off from the others?\n\n\n\nWarning Signs Your Model Might Be Misleading\nPattern in residuals: If residuals show a curve or trend, you’re missing something\nIncreasing spread: If residuals get more spread out as predictions increase, standard errors might be wrong\nInfluential outliers: One or two weird points can drag your whole line off\nMissing variables: If you forgot something important (like prior knowledge), your estimates might be biased\n\n\n\nKey Assumptions: When OLS Works Well\n\nLinearity: The true relationship is approximately straight\n\nCheck: Look at your scatter plot\n\nIndependence: Each observation is separate\n\nCheck: Make sure students didn’t work together or copy\n\nConstant variance: The spread of residuals is similar everywhere\n\nCheck: Residual plot shouldn’t fan out\n\nNo perfect multicollinearity: (For multiple regression) Predictors aren’t perfectly related\n\nCheck: Make sure you didn’t include the same variable twice\n\nRandom sampling: Your data represents the population you care about\n\nCheck: Did you sample fairly?\n\n\n\n\nSummary: Your OLS Toolkit\nWhat OLS Does:\n\nFinds the straight line that minimizes squared prediction errors\nEstimates how much Y changes when X changes by one unit\nTells you how much variation your model explains (R²)\nQuantifies uncertainty in your estimates\n\nYour Step-by-Step Process:\n\nPlot your data - does a line make sense?\nRun OLS to get \\hat{\\beta}_0 and \\hat{\\beta}_1\nCheck R² - how much variation do you explain?\nCalculate effect sizes (raw and standardized)\nAssess practical significance using context-specific criteria\nLook at confidence intervals - how uncertain are you?\nCheck residuals - any obvious problems?\nMake decisions based on both statistical and practical significance\n\n\n\nKluczowe interpretacje / Key interpretations\nDomyślny model: regresja OLS Y=\\beta_0+\\beta_1 X+\\varepsilon (lub wieloraka: Y=\\beta_0+\\beta_1 X_1+\\cdots+\\beta_p X_p+\\varepsilon).\n\nNachylenie / Slope (\\beta_1) PL: Przy wzroście X o 1 jednostkę (ceteris paribus), przeciętna wartość Y zmienia się o \\beta_1 jednostek. ENG: When X increases by 1 unit (ceteris paribus), the expected value of Y changes by \\beta_1 units.\nStandaryzowane nachylenie / Standardized slope \\big(\\beta_{1}^{(\\mathrm{std})}\\big) Definicja:\n\n\\beta_{1}^{(\\mathrm{std})} \\;=\\; \\beta_1 \\cdot \\frac{s_X}{s_Y},\n\ngdzie s_X i s_Y to odchylenia standardowe X i Y. PL: Przy wzroście X o 1 odchylenie standardowe (SD), przeciętna wartość Y zmienia się o \\beta_{1}^{(\\mathrm{std})} odchyleń standardowych Y. ENG: For a 1 standard deviation (SD) increase in X, the expected value of Y changes by \\beta_{1}^{(\\mathrm{std})} SDs of Y. Uwaga/Note: W regresji prostej \\beta_{1}^{(\\mathrm{std})} = r (Pearson). / In simple regression, \\beta_{1}^{(\\mathrm{std})} = r (Pearson).\nWspółczynnik determinacji / R^2 Definicja:\n\nR^2 \\;=\\; 1 - \\frac{SS_{\\mathrm{res}}}{SS_{\\mathrm{tot}}}.\n\nPL: Model wyjaśnia 100\\times R^2% zmienności Y względem modelu tylko z wyrazem wolnym (in-sample). ENG: The model explains 100\\times R^2% of the variance in Y relative to the intercept-only model (in-sample). W wielu zmiennych rozważ: \\text{adjusted } R^2. / With multiple predictors consider: adjusted R^2.\nWartość p / P-value Formalnie/Formally:\n\np \\;=\\; \\Pr\\!\\big(\\,|T|\\ge |t_{\\mathrm{obs}}| \\mid H_0\\,\\big),\n\ngdzie T ma rozkład t przy H_0. PL: Zakładając prawdziwość H_0 i spełnione założenia modelu, prawdopodobieństwo uzyskania co najmniej tak ekstremalnej statystyki jak obserwowana wynosi p. ENG: Assuming H_0 and the model assumptions hold, p is the probability of observing a test statistic at least as extreme as the one obtained.\nPrzedział ufności / Confidence interval (np. dla \\beta_1) Konstrukcja/Construction:\n\n\\hat{\\beta}_1 \\;\\pm\\; t_{1-\\alpha/2,\\ \\mathrm{df}} \\cdot \\mathrm{SE}\\!\\left(\\hat{\\beta}_1\\right).\n\nPL (ściśle): W długiej serii powtórzeń 95% tak skonstruowanych przedziałów zawiera prawdziwą wartość \\beta_1; dla naszych danych oszacowanie mieści się w [\\text{lower},\\ \\text{upper}]. ENG (strict): Over many repetitions, 95% of such intervals would contain the true \\beta_1; for our data, the estimate lies within [\\text{lower},\\ \\text{upper}]. PL (skrót dydaktyczny): „Jesteśmy 95% pewni, że \\beta_1 leży w [\\text{lower},\\ \\text{upper}].” ENG (teaching shorthand): “We are 95% confident that \\beta_1 lies in [\\text{lower},\\ \\text{upper}].”\n\n\nNajczęstsze nieporozumienia / Common pitfalls\n\nPL: p nie jest prawdopodobieństwem, że H_0 jest prawdziwa. ENG: p is not the probability that H_0 is true.\nPL: 95% CI nie zawiera 95% obserwacji (od tego jest przedział predykcji). ENG: A 95% CI does not contain 95% of observations (that’s a prediction interval).\nPL/ENG: Wysokie R^2 ≠ przyczynowość / High R^2 ≠ causality. Zawsze sprawdzaj/Always check diagnozy reszt, skalę efektu, i dopasowanie poza próbą.\n\nCritical Reminders:\n\nAssociation does not imply causation\nStatistical significance does not guarantee practical importance\nEvery model is wrong, but some are useful\nAlways visualize your data and residuals\nConsider both effect size and uncertainty when making decisions\n\nOLS provides a principled, mathematical approach to finding patterns in real-world data. While it cannot provide perfect predictions, it offers the best linear approximation possible along with honest assessments of that approximation’s quality and uncertainty.\n\n\n\n13.12 Complete Manual OLS Calculation: A Step-by-Step Example\nA professor wants to understand the relationship between hours spent studying and exam scores. She collects data from 6 students:\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nOur goal: Find the best-fitting line \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X using OLS.\n\n\n13.13 Step 1: Calculate the Means\nFirst, we need the mean of X and Y.\nFor X (study hours): \\bar{X} = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5\nFor Y (exam scores): \\bar{Y} = \\frac{65 + 70 + 75 + 85 + 88 + 95}{6} = \\frac{478}{6} = 79.67\n\n\n13.14 Step 2: Calculate Deviations from Means\nFor each observation, calculate (X_i - \\bar{X}) and (Y_i - \\bar{Y}):\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n1 - 3.5 = -2.5\n65 - 79.67 = -14.67\n\n\nB\n2\n70\n2 - 3.5 = -1.5\n70 - 79.67 = -9.67\n\n\nC\n3\n75\n3 - 3.5 = -0.5\n75 - 79.67 = -4.67\n\n\nD\n4\n85\n4 - 3.5 = 0.5\n85 - 79.67 = 5.33\n\n\nE\n5\n88\n5 - 3.5 = 1.5\n88 - 79.67 = 8.33\n\n\nF\n6\n95\n6 - 3.5 = 2.5\n95 - 79.67 = 15.33\n\n\n\n\n\n13.15 Step 3: Calculate Products and Squares\nNow calculate (X_i - \\bar{X})(Y_i - \\bar{Y}) and (X_i - \\bar{X})^2:\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n(-2.5)(-14.67) = 36.68\n(-2.5)² = 6.25\n\n\nB\n(-1.5)(-9.67) = 14.51\n(-1.5)² = 2.25\n\n\nC\n(-0.5)(-4.67) = 2.34\n(-0.5)² = 0.25\n\n\nD\n(0.5)(5.33) = 2.67\n(0.5)² = 0.25\n\n\nE\n(1.5)(8.33) = 12.50\n(1.5)² = 2.25\n\n\nF\n(2.5)(15.33) = 38.33\n(2.5)² = 6.25\n\n\nSum\n107.03\n17.50\n\n\n\n\n\n13.16 Step 4: Calculate the Slope (\\hat{\\beta}_1)\nUsing the OLS formula: \\hat{\\beta}_1 = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} = \\frac{107.03}{17.50} = 6.12\nInterpretation: Each additional hour of study is associated with a 6.12-point increase in exam score.\n\n\n13.17 Step 5: Calculate the Intercept (\\hat{\\beta}_0)\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X} = 79.67 - (6.12 \\times 3.5) = 79.67 - 21.42 = 58.25\nInterpretation: A student who studies 0 hours is predicted to score 58.25 points.\n\n\n13.18 Step 6: Write the Regression Equation\n\\hat{Y} = 58.25 + 6.12X\n\n\n13.19 Step 7: Calculate Predicted Values and Residuals\nUsing our equation to predict each student’s score:\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nResidual e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n58.25 + 6.12(1) = 64.37\n65 - 64.37 = 0.63\n\n\nB\n2\n70\n58.25 + 6.12(2) = 70.49\n70 - 70.49 = -0.49\n\n\nC\n3\n75\n58.25 + 6.12(3) = 76.61\n75 - 76.61 = -1.61\n\n\nD\n4\n85\n58.25 + 6.12(4) = 82.73\n85 - 82.73 = 2.27\n\n\nE\n5\n88\n58.25 + 6.12(5) = 88.85\n88 - 88.85 = -0.85\n\n\nF\n6\n95\n58.25 + 6.12(6) = 94.97\n95 - 94.97 = 0.03\n\n\n\nCheck: Sum of residuals = 0.63 - 0.49 - 1.61 + 2.27 - 0.85 + 0.03 ≈ 0 ✓\n\n\n13.20 Step 8: Calculate Sum of Squares\n\nTotal Sum of Squares (SST)\nHow much total variation exists in exam scores?\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n(65 - 79.67)² = (-14.67)² = 215.21\n\n\nB\n70\n(70 - 79.67)² = (-9.67)² = 93.51\n\n\nC\n75\n(75 - 79.67)² = (-4.67)² = 21.81\n\n\nD\n85\n(85 - 79.67)² = (5.33)² = 28.41\n\n\nE\n88\n(88 - 79.67)² = (8.33)² = 69.39\n\n\nF\n95\n(95 - 79.67)² = (15.33)² = 235.01\n\n\nSum\n\nSST = 663.34\n\n\n\n\n\nRegression Sum of Squares (SSR)\nHow much variation does our model explain?\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n(64.37 - 79.67)² = (-15.30)² = 234.09\n\n\nB\n70.49\n(70.49 - 79.67)² = (-9.18)² = 84.27\n\n\nC\n76.61\n(76.61 - 79.67)² = (-3.06)² = 9.36\n\n\nD\n82.73\n(82.73 - 79.67)² = (3.06)² = 9.36\n\n\nE\n88.85\n(88.85 - 79.67)² = (9.18)² = 84.27\n\n\nF\n94.97\n(94.97 - 79.67)² = (15.30)² = 234.09\n\n\nSum\n\nSSR = 655.44\n\n\n\n\n\nError Sum of Squares (SSE)\nHow much variation is unexplained?\n\n\n\nStudent\nResidual e_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n-0.49\n0.24\n\n\nC\n-1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n-0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSum\n\nSSE = 9.10\n\n\n\nVerification: SST = SSR + SSE 663.34 ≈ 655.44 + 9.10 = 664.54 ✓ (small rounding difference)\n\n\n\n13.21 Step 9: Calculate R-Squared\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternative formula: R^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 1 - 0.014 = 0.986\n(Small difference due to rounding)\nInterpretation: Study hours explain 98.8% of the variation in exam scores. This is an extremely strong relationship.\n\n\n13.22 Step 10: Calculate Effect Sizes\n\nRaw Effect Size\nThe raw effect size is simply the slope: 6.12 points per hour\n\n\nStandardized Effect Size\nFirst, calculate standard deviations:\nFor X: s_X = \\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{n-1}} = \\sqrt{\\frac{17.50}{5}} = \\sqrt{3.50} = 1.87\nFor Y: s_Y = \\sqrt{\\frac{\\sum(Y_i - \\bar{Y})^2}{n-1}} = \\sqrt{\\frac{663.34}{5}} = \\sqrt{132.67} = 11.52\nStandardized coefficient: \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y} = 6.12 \\times \\frac{1.87}{11.52} = 6.12 \\times 0.162 = 0.99\nInterpretation: A one standard deviation increase in study hours (1.87 hours) is associated with a 0.99 standard deviation increase in exam score.\nAccording to Cohen’s guidelines:\n\nSmall effect: |β| = 0.10\nMedium effect: |β| = 0.30\nLarge effect: |β| = 0.50\n\nOur standardized effect of 0.99 is nearly twice Cohen’s “large effect” threshold.\n\n\n\n13.23 Step 11: Practical Significance Assessment\n\nContext Analysis\n\nScale consideration:\n\nEffect: 6.12 points per hour\nExam scale: 0-100 points\nPercentage impact: 6.12% per hour\n\nPractical thresholds:\n\nLetter grade difference: Often 10 points\nTime to improve one letter grade: 10/6.12 = 1.63 hours\nConclusion: Less than 2 hours of extra study could change a letter grade\n\nCost-benefit analysis:\n\nBenefit: 6.12 points per hour\nCost: 1 hour of time\nDecision: Highly favorable return on investment\n\n\n\n\n\n13.24 Summary of Results\nRegression equation: \\hat{Y} = 58.25 + 6.12X\nKey statistics:\n\nSlope (\\hat{\\beta}_1): 6.12 points/hour\nIntercept (\\hat{\\beta}_0): 58.25 points\nR²: 0.988 (98.8% of variance explained)\nStandardized effect: 0.99 (very large effect)\n\nPractical interpretation:\n\nEach hour of study adds about 6 points to exam score\nThe model fits extremely well (R² near 1)\nThe effect is both statistically and practically significant\nStudents can meaningfully improve grades with modest increases in study time\n\n\n\n13.25 Verification Check\nTo verify our calculations, let’s check that the regression line passes through (\\bar{X}, \\bar{Y}):\n\\hat{Y} = 58.25 + 6.12(3.5) = 58.25 + 21.42 = 79.67 = \\bar{Y} ✓\nThe calculation confirms our regression line passes through the point of means, as it should.\n\n\n13.26 R Code to Verify Manual Calculations\nBelow is R code that checks all our manual calculations. You can run this code to confirm every step.\n\n# Step 1: Create the data\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)  # X variable\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95)  # Y variable\nn &lt;- length(study_hours)  # Sample size\n\n# Create a data frame for easier handling\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Step 2: Calculate means\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Step 3: Calculate deviations from means\ndata$x_dev &lt;- data$X - x_bar  # X deviations\ndata$y_dev &lt;- data$Y - y_bar  # Y deviations\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Step 4: Calculate products and squares for OLS formula\ndata$xy_product &lt;- data$x_dev * data$y_dev  # (Xi - X̄)(Yi - Ȳ)\ndata$x_dev_sq &lt;- data$x_dev^2  # (Xi - X̄)²\n\n# Sum of products and squares\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - X̄)(Yi - Ȳ):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - X̄)(Yi - Ȳ): 107 \n\ncat(\"Sum of (Xi - X̄)²:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - X̄)²: 17.5 \n\n# Step 5: Calculate slope (beta_1) manually\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (β₁) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (β₁) calculated manually: 6.11 \n\n# Step 6: Calculate intercept (beta_0) manually\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (β₀) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (β₀) calculated manually: 58.27 \n\n# Step 7: Compare with R's lm() function\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Step 8: Calculate predicted values and residuals\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X  # Predicted values\ndata$residual &lt;- data$Y - data$Y_hat  # Residuals\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ≈ 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ≈ 0): 0 \n\n# Step 9: Calculate Sum of Squares\n# Total Sum of Squares (SST)\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\n# Regression Sum of Squares (SSR)\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\n# Error Sum of Squares (SSE)\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\n# Verify that SST = SSR + SSE\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Step 10: Calculate R-squared\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R² (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR² (Method 1: SSR/SST): 0.9863 \n\ncat(\"R² (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR² (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R² (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR² (from lm function): 0.9863 \n\n# Step 11: Calculate Effect Sizes\n# Raw effect size (just the slope)\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\n# Standard deviations for standardized effect\nsd_x &lt;- sd(data$X)  # Standard deviation of X\nsd_y &lt;- sd(data$Y)  # Standard deviation of Y\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\n# Standardized effect size\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Correlation coefficient (should equal sqrt(R²) for simple regression)\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R²:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R²: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n# Step 12: Create visualization\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\n# Plot the data and regression line\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5)\n\n# Add the regression line\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Add the mean point\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Add vertical lines for residuals\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\n# Add legend\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# Add the equation to the plot\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R² = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\n\n\n\n# Final summary\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )\n\n\n\nRunning the Code\nTo run this R code:\n\nCopy the entire code block above\nPaste it into RStudio or any R console\nExecute the code\nCompare the output with our manual calculations\n\nThe code will:\n\nRecreate all our manual calculations step by step\nVerify results using R’s built-in lm() function\nGenerate a visualization of the data with the regression line\nDisplay all intermediate calculations with clear labels\n\n\n\nExpected Output Highlights\nWhen you run this code, you should see:\n\nSlope: 6.12 (matching our manual calculation)\nIntercept: 58.25 (matching our manual calculation)\nR²: 0.988 (matching our manual calculation)\nStandardized effect: 0.99 (matching our manual calculation)\nA plot showing the data points, regression line, and residuals\n\nThis verification confirms that our pen-and-paper calculations were correct!",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#complete-manual-ols-calculation-a-step-by-step-example",
    "href": "correg_en.html#complete-manual-ols-calculation-a-step-by-step-example",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.12 Complete Manual OLS Calculation: A Step-by-Step Example",
    "text": "13.12 Complete Manual OLS Calculation: A Step-by-Step Example\nA professor wants to understand the relationship between hours spent studying and exam scores. She collects data from 6 students:\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nOur goal: Find the best-fitting line \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X using OLS.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-1-calculate-the-means",
    "href": "correg_en.html#step-1-calculate-the-means",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.13 Step 1: Calculate the Means",
    "text": "13.13 Step 1: Calculate the Means\nFirst, we need the mean of X and Y.\nFor X (study hours): \\bar{X} = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5\nFor Y (exam scores): \\bar{Y} = \\frac{65 + 70 + 75 + 85 + 88 + 95}{6} = \\frac{478}{6} = 79.67",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-2-calculate-deviations-from-means",
    "href": "correg_en.html#step-2-calculate-deviations-from-means",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.14 Step 2: Calculate Deviations from Means",
    "text": "13.14 Step 2: Calculate Deviations from Means\nFor each observation, calculate (X_i - \\bar{X}) and (Y_i - \\bar{Y}):\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n1 - 3.5 = -2.5\n65 - 79.67 = -14.67\n\n\nB\n2\n70\n2 - 3.5 = -1.5\n70 - 79.67 = -9.67\n\n\nC\n3\n75\n3 - 3.5 = -0.5\n75 - 79.67 = -4.67\n\n\nD\n4\n85\n4 - 3.5 = 0.5\n85 - 79.67 = 5.33\n\n\nE\n5\n88\n5 - 3.5 = 1.5\n88 - 79.67 = 8.33\n\n\nF\n6\n95\n6 - 3.5 = 2.5\n95 - 79.67 = 15.33",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-3-calculate-products-and-squares",
    "href": "correg_en.html#step-3-calculate-products-and-squares",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.15 Step 3: Calculate Products and Squares",
    "text": "13.15 Step 3: Calculate Products and Squares\nNow calculate (X_i - \\bar{X})(Y_i - \\bar{Y}) and (X_i - \\bar{X})^2:\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n(-2.5)(-14.67) = 36.68\n(-2.5)² = 6.25\n\n\nB\n(-1.5)(-9.67) = 14.51\n(-1.5)² = 2.25\n\n\nC\n(-0.5)(-4.67) = 2.34\n(-0.5)² = 0.25\n\n\nD\n(0.5)(5.33) = 2.67\n(0.5)² = 0.25\n\n\nE\n(1.5)(8.33) = 12.50\n(1.5)² = 2.25\n\n\nF\n(2.5)(15.33) = 38.33\n(2.5)² = 6.25\n\n\nSum\n107.03\n17.50",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-4-calculate-the-slope-hatbeta_1",
    "href": "correg_en.html#step-4-calculate-the-slope-hatbeta_1",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.16 Step 4: Calculate the Slope (\\hat{\\beta}_1)",
    "text": "13.16 Step 4: Calculate the Slope (\\hat{\\beta}_1)\nUsing the OLS formula: \\hat{\\beta}_1 = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} = \\frac{107.03}{17.50} = 6.12\nInterpretation: Each additional hour of study is associated with a 6.12-point increase in exam score.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-5-calculate-the-intercept-hatbeta_0",
    "href": "correg_en.html#step-5-calculate-the-intercept-hatbeta_0",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.17 Step 5: Calculate the Intercept (\\hat{\\beta}_0)",
    "text": "13.17 Step 5: Calculate the Intercept (\\hat{\\beta}_0)\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X} = 79.67 - (6.12 \\times 3.5) = 79.67 - 21.42 = 58.25\nInterpretation: A student who studies 0 hours is predicted to score 58.25 points.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-6-write-the-regression-equation",
    "href": "correg_en.html#step-6-write-the-regression-equation",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.18 Step 6: Write the Regression Equation",
    "text": "13.18 Step 6: Write the Regression Equation\n\\hat{Y} = 58.25 + 6.12X",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-7-calculate-predicted-values-and-residuals",
    "href": "correg_en.html#step-7-calculate-predicted-values-and-residuals",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.19 Step 7: Calculate Predicted Values and Residuals",
    "text": "13.19 Step 7: Calculate Predicted Values and Residuals\nUsing our equation to predict each student’s score:\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nResidual e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n58.25 + 6.12(1) = 64.37\n65 - 64.37 = 0.63\n\n\nB\n2\n70\n58.25 + 6.12(2) = 70.49\n70 - 70.49 = -0.49\n\n\nC\n3\n75\n58.25 + 6.12(3) = 76.61\n75 - 76.61 = -1.61\n\n\nD\n4\n85\n58.25 + 6.12(4) = 82.73\n85 - 82.73 = 2.27\n\n\nE\n5\n88\n58.25 + 6.12(5) = 88.85\n88 - 88.85 = -0.85\n\n\nF\n6\n95\n58.25 + 6.12(6) = 94.97\n95 - 94.97 = 0.03\n\n\n\nCheck: Sum of residuals = 0.63 - 0.49 - 1.61 + 2.27 - 0.85 + 0.03 ≈ 0 ✓",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-8-calculate-sum-of-squares",
    "href": "correg_en.html#step-8-calculate-sum-of-squares",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.20 Step 8: Calculate Sum of Squares",
    "text": "13.20 Step 8: Calculate Sum of Squares\n\nTotal Sum of Squares (SST)\nHow much total variation exists in exam scores?\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n(65 - 79.67)² = (-14.67)² = 215.21\n\n\nB\n70\n(70 - 79.67)² = (-9.67)² = 93.51\n\n\nC\n75\n(75 - 79.67)² = (-4.67)² = 21.81\n\n\nD\n85\n(85 - 79.67)² = (5.33)² = 28.41\n\n\nE\n88\n(88 - 79.67)² = (8.33)² = 69.39\n\n\nF\n95\n(95 - 79.67)² = (15.33)² = 235.01\n\n\nSum\n\nSST = 663.34\n\n\n\n\n\nRegression Sum of Squares (SSR)\nHow much variation does our model explain?\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n(64.37 - 79.67)² = (-15.30)² = 234.09\n\n\nB\n70.49\n(70.49 - 79.67)² = (-9.18)² = 84.27\n\n\nC\n76.61\n(76.61 - 79.67)² = (-3.06)² = 9.36\n\n\nD\n82.73\n(82.73 - 79.67)² = (3.06)² = 9.36\n\n\nE\n88.85\n(88.85 - 79.67)² = (9.18)² = 84.27\n\n\nF\n94.97\n(94.97 - 79.67)² = (15.30)² = 234.09\n\n\nSum\n\nSSR = 655.44\n\n\n\n\n\nError Sum of Squares (SSE)\nHow much variation is unexplained?\n\n\n\nStudent\nResidual e_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n-0.49\n0.24\n\n\nC\n-1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n-0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSum\n\nSSE = 9.10\n\n\n\nVerification: SST = SSR + SSE 663.34 ≈ 655.44 + 9.10 = 664.54 ✓ (small rounding difference)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-9-calculate-r-squared",
    "href": "correg_en.html#step-9-calculate-r-squared",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.21 Step 9: Calculate R-Squared",
    "text": "13.21 Step 9: Calculate R-Squared\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternative formula: R^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 1 - 0.014 = 0.986\n(Small difference due to rounding)\nInterpretation: Study hours explain 98.8% of the variation in exam scores. This is an extremely strong relationship.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-10-calculate-effect-sizes",
    "href": "correg_en.html#step-10-calculate-effect-sizes",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.22 Step 10: Calculate Effect Sizes",
    "text": "13.22 Step 10: Calculate Effect Sizes\n\nRaw Effect Size\nThe raw effect size is simply the slope: 6.12 points per hour\n\n\nStandardized Effect Size\nFirst, calculate standard deviations:\nFor X: s_X = \\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{n-1}} = \\sqrt{\\frac{17.50}{5}} = \\sqrt{3.50} = 1.87\nFor Y: s_Y = \\sqrt{\\frac{\\sum(Y_i - \\bar{Y})^2}{n-1}} = \\sqrt{\\frac{663.34}{5}} = \\sqrt{132.67} = 11.52\nStandardized coefficient: \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y} = 6.12 \\times \\frac{1.87}{11.52} = 6.12 \\times 0.162 = 0.99\nInterpretation: A one standard deviation increase in study hours (1.87 hours) is associated with a 0.99 standard deviation increase in exam score.\nAccording to Cohen’s guidelines:\n\nSmall effect: |β| = 0.10\nMedium effect: |β| = 0.30\nLarge effect: |β| = 0.50\n\nOur standardized effect of 0.99 is nearly twice Cohen’s “large effect” threshold.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-11-practical-significance-assessment",
    "href": "correg_en.html#step-11-practical-significance-assessment",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.23 Step 11: Practical Significance Assessment",
    "text": "13.23 Step 11: Practical Significance Assessment\n\nContext Analysis\n\nScale consideration:\n\nEffect: 6.12 points per hour\nExam scale: 0-100 points\nPercentage impact: 6.12% per hour\n\nPractical thresholds:\n\nLetter grade difference: Often 10 points\nTime to improve one letter grade: 10/6.12 = 1.63 hours\nConclusion: Less than 2 hours of extra study could change a letter grade\n\nCost-benefit analysis:\n\nBenefit: 6.12 points per hour\nCost: 1 hour of time\nDecision: Highly favorable return on investment",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-of-results",
    "href": "correg_en.html#summary-of-results",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.24 Summary of Results",
    "text": "13.24 Summary of Results\nRegression equation: \\hat{Y} = 58.25 + 6.12X\nKey statistics:\n\nSlope (\\hat{\\beta}_1): 6.12 points/hour\nIntercept (\\hat{\\beta}_0): 58.25 points\nR²: 0.988 (98.8% of variance explained)\nStandardized effect: 0.99 (very large effect)\n\nPractical interpretation:\n\nEach hour of study adds about 6 points to exam score\nThe model fits extremely well (R² near 1)\nThe effect is both statistically and practically significant\nStudents can meaningfully improve grades with modest increases in study time",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#verification-check",
    "href": "correg_en.html#verification-check",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.25 Verification Check",
    "text": "13.25 Verification Check\nTo verify our calculations, let’s check that the regression line passes through (\\bar{X}, \\bar{Y}):\n\\hat{Y} = 58.25 + 6.12(3.5) = 58.25 + 21.42 = 79.67 = \\bar{Y} ✓\nThe calculation confirms our regression line passes through the point of means, as it should.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#r-code-to-verify-manual-calculations",
    "href": "correg_en.html#r-code-to-verify-manual-calculations",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.26 R Code to Verify Manual Calculations",
    "text": "13.26 R Code to Verify Manual Calculations\nBelow is R code that checks all our manual calculations. You can run this code to confirm every step.\n\n# Step 1: Create the data\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)  # X variable\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95)  # Y variable\nn &lt;- length(study_hours)  # Sample size\n\n# Create a data frame for easier handling\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Step 2: Calculate means\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Step 3: Calculate deviations from means\ndata$x_dev &lt;- data$X - x_bar  # X deviations\ndata$y_dev &lt;- data$Y - y_bar  # Y deviations\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Step 4: Calculate products and squares for OLS formula\ndata$xy_product &lt;- data$x_dev * data$y_dev  # (Xi - X̄)(Yi - Ȳ)\ndata$x_dev_sq &lt;- data$x_dev^2  # (Xi - X̄)²\n\n# Sum of products and squares\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - X̄)(Yi - Ȳ):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - X̄)(Yi - Ȳ): 107 \n\ncat(\"Sum of (Xi - X̄)²:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - X̄)²: 17.5 \n\n# Step 5: Calculate slope (beta_1) manually\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (β₁) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (β₁) calculated manually: 6.11 \n\n# Step 6: Calculate intercept (beta_0) manually\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (β₀) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (β₀) calculated manually: 58.27 \n\n# Step 7: Compare with R's lm() function\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Step 8: Calculate predicted values and residuals\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X  # Predicted values\ndata$residual &lt;- data$Y - data$Y_hat  # Residuals\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ≈ 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ≈ 0): 0 \n\n# Step 9: Calculate Sum of Squares\n# Total Sum of Squares (SST)\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\n# Regression Sum of Squares (SSR)\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\n# Error Sum of Squares (SSE)\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\n# Verify that SST = SSR + SSE\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Step 10: Calculate R-squared\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R² (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR² (Method 1: SSR/SST): 0.9863 \n\ncat(\"R² (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR² (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R² (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR² (from lm function): 0.9863 \n\n# Step 11: Calculate Effect Sizes\n# Raw effect size (just the slope)\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\n# Standard deviations for standardized effect\nsd_x &lt;- sd(data$X)  # Standard deviation of X\nsd_y &lt;- sd(data$Y)  # Standard deviation of Y\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\n# Standardized effect size\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Correlation coefficient (should equal sqrt(R²) for simple regression)\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R²:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R²: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n# Step 12: Create visualization\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\n# Plot the data and regression line\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5)\n\n# Add the regression line\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Add the mean point\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Add vertical lines for residuals\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\n# Add legend\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# Add the equation to the plot\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R² = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\n\n\n\n# Final summary\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )\n\n\n\nRunning the Code\nTo run this R code:\n\nCopy the entire code block above\nPaste it into RStudio or any R console\nExecute the code\nCompare the output with our manual calculations\n\nThe code will:\n\nRecreate all our manual calculations step by step\nVerify results using R’s built-in lm() function\nGenerate a visualization of the data with the regression line\nDisplay all intermediate calculations with clear labels\n\n\n\nExpected Output Highlights\nWhen you run this code, you should see:\n\nSlope: 6.12 (matching our manual calculation)\nIntercept: 58.25 (matching our manual calculation)\nR²: 0.988 (matching our manual calculation)\nStandardized effect: 0.99 (matching our manual calculation)\nA plot showing the data points, regression line, and residuals\n\nThis verification confirms that our pen-and-paper calculations were correct!",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-linear-regression-model",
    "href": "correg_en.html#the-linear-regression-model",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.27 The Linear Regression Model",
    "text": "13.27 The Linear Regression Model\nRegression analysis provides a statistical framework for modeling relationships between a dependent variable and one or more independent variables. This methodology enables researchers to quantify relationships, test hypotheses, and make predictions based on observed data.\n\nSimple Linear Regression\nThe simple linear regression model expresses the relationship between a dependent variable and a single independent variable:\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\nWhere: - Y_i represents the dependent variable for observation i - X_i represents the independent variable for observation i - \\beta_0 is the intercept parameter - \\beta_1 is the slope parameter - \\varepsilon_i is the error term for observation i\n\n\nMultiple Linear Regression\nThe multiple linear regression model extends this framework to incorporate k independent variables:\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + ... + \\beta_k X_{ki} + \\varepsilon_i\nThis formulation allows for the simultaneous analysis of multiple predictors and their respective contributions to the dependent variable.\n\n\nOrdinary Least Squares Estimation\n\nDefining the Optimization Criterion\nThe estimation of regression parameters requires a criterion for determining the “best” fit. Consider three potential approaches for defining the optimal line through a set of data points:\n\n\nApproach 1: Minimizing the Sum of Residuals\n\\min \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i) = \\min \\sum_{i=1}^{n} e_i\nThis approach is fundamentally flawed. For any line passing through the data, we can always find another line where positive and negative residuals sum to zero. In fact, infinitely many lines satisfy \\sum e_i = 0. This criterion fails to uniquely identify an optimal solution. Moreover, a horizontal line through the mean of Y would achieve zero sum of residuals while ignoring the relationship with X entirely.\n\n\nApproach 2: Minimizing the Sum of Absolute Residuals\n\\min \\sum_{i=1}^{n} |Y_i - \\hat{Y}_i| = \\min \\sum_{i=1}^{n} |e_i|\nThis criterion, known as Least Absolute Deviations (LAD), addresses the cancellation problem by taking absolute values. It produces estimates that are more robust to outliers than OLS. However, this approach presents significant challenges:\n\nThe absolute value function is not differentiable at zero, complicating analytical solutions\nMultiple solutions may exist (the objective function may have multiple minima)\nNo closed-form solution exists; iterative numerical methods are required\nStatistical inference is more complex, lacking the elegant properties of OLS estimators\n\n\n\nApproach 3: Minimizing the Sum of Squared Residuals\n\\min \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\min \\sum_{i=1}^{n} e_i^2\nThe Ordinary Least Squares (OLS) approach minimizes the sum of squared residuals. This criterion offers several advantages:\n\nPrevents cancellation of positive and negative errors\nProvides a unique solution (except in cases of perfect multicollinearity)\nYields closed-form analytical solutions through differentiation\nProduces estimators with optimal statistical properties under classical assumptions\nFacilitates straightforward statistical inference\n\n\n\n\nVisualizing the Sum of Squared Errors\nThe OLS method can be understood geometrically through the following conceptual framework:\n\nEach error appears as a vertical line from the data point to the regression line\nEach of these vertical lines represents a residual (e_i)\nWe square each residual, which can be visualized as creating a square area\nThe sum of all these squared areas is what OLS minimizes\n\n\n# Visualization of squared errors as geometric areas\nlibrary(ggplot2)\n\n# Generate sample data with clear pattern\nset.seed(42)\nn &lt;- 8  # Small number for clarity\nx &lt;- seq(2, 16, length.out = n)\ny &lt;- 10 + 1.5*x + rnorm(n, 0, 3)\ndf &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, df)\ndf$fitted &lt;- fitted(model)\ndf$residual &lt;- residuals(model)\n\n# Create visualization with actual squares\np &lt;- ggplot(df, aes(x = x, y = y)) +\n  # Add regression line first (bottom layer)\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\", linewidth = 1.2) +\n  \n  # Add squares for each residual\n  # For positive residuals\n  geom_rect(data = subset(df, residual &gt; 0),\n            aes(xmin = x - abs(residual)/2, \n                xmax = x + abs(residual)/2, \n                ymin = fitted, \n                ymax = fitted + abs(residual)),\n            fill = \"red\", alpha = 0.25, color = \"red\", linewidth = 0.5) +\n  \n  # For negative residuals  \n  geom_rect(data = subset(df, residual &lt; 0),\n            aes(xmin = x - abs(residual)/2, \n                xmax = x + abs(residual)/2, \n                ymin = fitted - abs(residual), \n                ymax = fitted),\n            fill = \"red\", alpha = 0.25, color = \"red\", linewidth = 0.5) +\n  \n  # Add residual lines\n  geom_segment(aes(xend = x, yend = fitted), \n               color = \"red\", linewidth = 0.8, linetype = \"solid\") +\n  \n  # Add data points\n  geom_point(size = 3.5, color = \"black\") +\n  \n  # Add text annotations for selected squared values\n  geom_text(data = subset(df, abs(residual) &gt; 2),\n            aes(x = x, \n                y = fitted + sign(residual) * abs(residual)/2,\n                label = paste0(\"e²=\", round(residual^2, 1))),\n            size = 3, color = \"darkred\", fontface = \"italic\") +\n  \n  # Styling\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(face = \"italic\")\n  ) +\n  coord_equal() +  # Ensures squares appear as squares\n  labs(\n    title = \"Geometric Visualization of Sum of Squared Errors\",\n    subtitle = paste(\"SSE =\", round(sum(df$residual^2), 1), \n                     \"- Red squares represent squared residuals\"),\n    x = \"Independent Variable (X)\",\n    y = \"Dependent Variable (Y)\"\n  ) +\n  # Add SSE annotation\n  annotate(\"rect\", \n           xmin = max(df$x) - 3, xmax = max(df$x) - 0.5,\n           ymin = min(df$y) - 2, ymax = min(df$y),\n           fill = \"lightyellow\", alpha = 0.8, color = \"gray40\") +\n  annotate(\"text\", \n           x = max(df$x) - 1.75, y = min(df$y) - 1,\n           label = paste(\"Σe² =\", round(sum(df$residual^2), 1)),\n           size = 4, fontface = \"bold\", color = \"darkred\")\n\nprint(p)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nMathematical Derivation of OLS Estimators\nFor simple linear regression, the OLS estimators are obtained by minimizing the sum of squared residuals:\nSSE = \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2\nTaking partial derivatives with respect to \\beta_0 and \\beta_1 and setting them equal to zero yields the normal equations. Solving this system produces:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} = \\frac{Cov(X,Y)}{Var(X)}\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\n\n\nProperties of OLS Estimators\nThe OLS procedure guarantees several important properties:\n\nZero sum of residuals: \\sum_{i=1}^{n} e_i = 0\nOrthogonality of residuals and predictors: \\sum_{i=1}^{n} X_i e_i = 0\nThe fitted regression line passes through the point (\\bar{X}, \\bar{Y})\nZero covariance between fitted values and residuals: \\sum_{i=1}^{n} \\hat{Y}_i e_i = 0\n\n\n\nClassical Linear Model Assumptions\n\nCore Assumptions\nFor OLS estimators to possess desirable statistical properties, the following assumptions must hold:\n\n\nAssumption 1: Linearity in Parameters\nThe relationship between the dependent and independent variables is linear in the parameters: Y_i = \\beta_0 + \\beta_1 X_{1i} + ... + \\beta_k X_{ki} + \\varepsilon_i\n\n\nAssumption 2: Strict Exogeneity\nThe error term has zero conditional expectation given all values of the independent variables: E[\\varepsilon_i | X] = 0\nThis assumption implies that the independent variables contain no information about the mean of the error term. It is stronger than contemporaneous exogeneity and rules out feedback from past errors to current regressors. This assumption is critical for unbiased estimation and is often violated in time series contexts with lagged dependent variables or in the presence of omitted variables.\nThis assumption is particularly important for our discussion of spurious correlations. Violations of the exogeneity assumption lead to endogeneity problems, which we will discuss later.\n\n\nAssumption 3: No Perfect Multicollinearity\nIn multiple regression, no independent variable can be expressed as a perfect linear combination of other independent variables. The matrix X'X must be invertible.\n\n\nAssumption 4: Homoscedasticity\nThe variance of the error term is constant across all observations: Var(\\varepsilon_i | X) = \\sigma^2\nThis assumption ensures that the precision of the regression does not vary systematically with the level of the independent variables.\n\n\nAssumption 5: No Autocorrelation\nThe error terms are uncorrelated with each other: Cov(\\varepsilon_i, \\varepsilon_j | X) = 0 \\text{ for } i \\neq j\n\n\nAssumption 6: Normality of Errors (for inference)\nThe error terms follow a normal distribution: \\varepsilon_i \\sim N(0, \\sigma^2)\nThis assumption is not required for the unbiasedness or consistency of OLS estimators but is necessary for exact finite-sample inference.\n\n\n\nGauss-Markov Theorem\nUnder Assumptions 1-5, the OLS estimators are BLUE (Best Linear Unbiased Estimators):\n\nBest: Minimum variance among the class of linear unbiased estimators\nLinear: The estimators are linear functions of the dependent variable\nUnbiased: E[\\hat{\\beta}] = \\beta\n\n\n\nVisualization of OLS Methodology\n\nGeometric Interpretation\n\n# Comprehensive visualization of OLS regression\nlibrary(ggplot2)\n\n# Generate sample data\nset.seed(42)\nn &lt;- 50\nx &lt;- runif(n, 0, 100)\nepsilon &lt;- rnorm(n, 0, 15)\ny &lt;- 20 + 0.8*x + epsilon\n\n# Create data frame\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, data = data)\ndata$fitted &lt;- fitted(model)\ndata$residuals &lt;- residuals(model)\n\n# Create comprehensive plot\nggplot(data, aes(x = x, y = y)) +\n  # Add confidence interval\n  geom_smooth(method = \"lm\", se = TRUE, alpha = 0.15, fill = \"blue\") +\n  # Add regression line\n  geom_line(aes(y = fitted), color = \"blue\", linewidth = 1.2) +\n  # Add residual segments\n  geom_segment(aes(xend = x, yend = fitted), \n               color = \"red\", alpha = 0.5, linewidth = 0.7) +\n  # Add observed points\n  geom_point(size = 2.5, alpha = 0.8) +\n  # Add fitted values\n  geom_point(aes(y = fitted), color = \"blue\", size = 1.5, alpha = 0.6) +\n  # Annotations\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(size = 11),\n    plot.title = element_text(size = 12, face = \"bold\")\n  ) +\n  labs(\n    title = \"Ordinary Least Squares Regression\",\n    subtitle = sprintf(\"Estimated equation: Y = %.2f + %.3f X  (R² = %.3f, RSE = %.2f)\",\n                      coef(model)[1], coef(model)[2], \n                      summary(model)$r.squared, \n                      summary(model)$sigma),\n    x = \"Independent Variable (X)\",\n    y = \"Dependent Variable (Y)\"\n  ) +\n  annotate(\"text\", x = min(x) + 5, y = max(y) - 5,\n           label = sprintf(\"SSE = %.1f\", sum(residuals(model)^2)),\n           hjust = 0, size = 3.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nVisualization of Squared Residuals\n\n# Demonstrate why squaring is necessary\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Generate example with clear pattern\nset.seed(123)\nn &lt;- 20\nx &lt;- seq(1, 10, length.out = n)\ny &lt;- 2 + 1.5*x + rnorm(n, 0, 2)\ndf &lt;- data.frame(x = x, y = y)\n\n# Fit model\nmodel &lt;- lm(y ~ x, df)\ndf$fitted &lt;- fitted(model)\ndf$residual &lt;- residuals(model)\n\n# Plot 1: Raw residuals\np1 &lt;- ggplot(df, aes(x = x)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(aes(y = 0, yend = residual, xend = x), \n               color = ifelse(df$residual &gt; 0, \"darkgreen\", \"darkred\"),\n               linewidth = 1) +\n  geom_point(aes(y = residual), size = 3,\n             color = ifelse(df$residual &gt; 0, \"darkgreen\", \"darkred\")) +\n  theme_minimal() +\n  labs(title = \"Residuals (ei)\",\n       subtitle = sprintf(\"Sum = %.2f (not meaningful)\", sum(df$residual)),\n       x = \"X\", y = \"Residual\") +\n  ylim(c(-6, 6))\n\n# Plot 2: Squared residuals\np2 &lt;- ggplot(df, aes(x = x)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(aes(y = 0, yend = residual^2, xend = x), \n               color = \"darkred\", linewidth = 1) +\n  geom_point(aes(y = residual^2), size = 3, color = \"darkred\") +\n  theme_minimal() +\n  labs(title = \"Squared Residuals (ei²)\",\n       subtitle = sprintf(\"Sum = %.2f (minimized by OLS)\", sum(df$residual^2)),\n       x = \"X\", y = \"Squared Residual\") +\n  ylim(c(0, 36))\n\n# Combine plots\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\nDiagnostic Analysis\n\nResidual Diagnostics\nAssessment of model assumptions requires careful examination of residual patterns:\n\n# Generate diagnostic plots\npar(mfrow = c(2, 2))\n\n# Residuals vs Fitted Values\nplot(model, which = 1)\n# Tests linearity and homoscedasticity assumptions\n\n# Normal Q-Q Plot\nplot(model, which = 2)\n# Tests normality assumption\n\n# Scale-Location Plot\nplot(model, which = 3)\n# Tests homoscedasticity assumption\n\n# Cook's Distance\nplot(model, which = 4)\n\n\n\n\n\n\n\n# Identifies influential observations\n\n\n\nTesting Assumptions Formally\n\n# Formal statistical tests\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(car)\n\n# Test for heteroscedasticity\n# Breusch-Pagan test\nbptest(model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model\nBP = 0.37876, df = 1, p-value = 0.5383\n\n# Test for autocorrelation\n# Durbin-Watson test\ndwtest(model)\n\n\n    Durbin-Watson test\n\ndata:  model\nDW = 2.1781, p-value = 0.5599\nalternative hypothesis: true autocorrelation is greater than 0\n\n# Test for normality\n# Shapiro-Wilk test on residuals\nshapiro.test(residuals(model))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(model)\nW = 0.98221, p-value = 0.9593\n\n# Test for linearity\n# Rainbow test\nraintest(model)\n\n\n    Rainbow test\n\ndata:  model\nRain = 1.2139, df1 = 10, df2 = 8, p-value = 0.3995\n\n\n\n\n\nExtensions and Alternatives\n\nWhen OLS Assumptions Fail\nWhen classical assumptions are violated, alternative approaches may be necessary:\n\nHeteroscedasticity: Weighted Least Squares (WLS) or robust standard errors\nAutocorrelation: Generalized Least Squares (GLS) or Newey-West standard errors\nNon-normality: Bootstrap inference or robust regression methods\nMulticollinearity: Ridge regression or LASSO\nEndogeneity: Instrumental Variables (IV) or Two-Stage Least Squares (2SLS)\n\n\n\nRobust Regression Methods\nWhen outliers are present, robust alternatives to OLS include:\n\nM-estimators (Huber regression)\nLeast Trimmed Squares (LTS)\nMM-estimators\n\n\n\n\nPractical Implementation\n\nComplete Analysis Example\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(car)\n\n# Generate dataset\nset.seed(2024)\nn &lt;- 200\ndata &lt;- data.frame(\n  x1 = rnorm(n, 50, 10),\n  x2 = rnorm(n, 30, 5),\n  x3 = rbinom(n, 1, 0.5)\n)\ndata$y &lt;- 10 + 2*data$x1 + 3*data$x2 + 15*data$x3 + rnorm(n, 0, 10)\n\n# Fit multiple regression model\nfull_model &lt;- lm(y ~ x1 + x2 + x3, data = data)\n\n# Model summary\nsummary(full_model)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25.6507  -6.9674  -0.7472   6.3670  30.4959 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 13.59158    5.97565   2.274               0.024 *  \nx1           2.05837    0.06876  29.934 &lt;0.0000000000000002 ***\nx2           2.76233    0.15259  18.103 &lt;0.0000000000000002 ***\nx3          14.80771    1.42654  10.380 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.897 on 196 degrees of freedom\nMultiple R-squared:  0.8744,    Adjusted R-squared:  0.8725 \nF-statistic: 454.9 on 3 and 196 DF,  p-value: &lt; 0.00000000000000022\n\n# ANOVA table\nanova(full_model)\n\nAnalysis of Variance Table\n\nResponse: y\n           Df Sum Sq Mean Sq F value                Pr(&gt;F)    \nx1          1  82189   82189  839.04 &lt; 0.00000000000000022 ***\nx2          1  40936   40936  417.90 &lt; 0.00000000000000022 ***\nx3          1  10555   10555  107.75 &lt; 0.00000000000000022 ***\nResiduals 196  19200      98                                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Confidence intervals for parameters\nconfint(full_model, level = 0.95)\n\n                2.5 %    97.5 %\n(Intercept)  1.806741 25.376410\nx1           1.922758  2.193977\nx2           2.461401  3.063262\nx3          11.994367 17.621053\n\n# Variance Inflation Factors (multicollinearity check)\nvif(full_model)\n\n      x1       x2       x3 \n1.009487 1.044192 1.038735 \n\n# Model diagnostics\npar(mfrow = c(2, 2))\nplot(full_model)\n\n\n\n\n\n\n\n# Tidy output\ntidy(full_model, conf.int = TRUE)\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    13.6     5.98        2.27 2.40e- 2     1.81     25.4 \n2 x1              2.06    0.0688     29.9  4.90e-75     1.92      2.19\n3 x2              2.76    0.153      18.1  1.06e-43     2.46      3.06\n4 x3             14.8     1.43       10.4  2.14e-20    12.0      17.6 \n\nglance(full_model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.874         0.872  9.90      455. 5.21e-88     3  -740. 1490. 1507.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\nConclusion\nOrdinary Least Squares regression remains a fundamental tool in statistical analysis. The method’s mathematical elegance, combined with its optimal properties under the classical assumptions, explains its widespread application. However, practitioners must carefully verify assumptions and consider alternatives when these conditions are not met. Understanding both the theoretical foundations and practical limitations of OLS is essential for proper statistical inference and prediction.\n\n\nVisualizing OLS Through Different Regression Lines\nA simple but effective way to visualize the concept of “best fit” is to compare multiple lines and their resulting SSE values:\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Create sample data\nset.seed(123)\nx &lt;- 1:20\ny &lt;- 2 + 3*x + rnorm(20, 0, 5)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, data = data)\ncoef &lt;- coefficients(model)\n\n# Define different lines: optimal and sub-optimal with clearer differences\nlines &lt;- data.frame(\n  label = c(\"Best Fit (OLS)\", \"Line A\", \"Line B\", \"Line C\"),\n  intercept = c(coef[1], coef[1] - 8, coef[1] + 8, coef[1] - 4),\n  slope = c(coef[2], coef[2] - 1.2, coef[2] + 0.8, coef[2] - 0.7)\n)\n\n# Calculate SSE for each line\nlines$sse &lt;- sapply(1:nrow(lines), function(i) {\n  predicted &lt;- lines$intercept[i] + lines$slope[i] * x\n  sum((y - predicted)^2)\n})\n\n# Add percentage increase over optimal SSE\nlines$pct_increase &lt;- round((lines$sse / lines$sse[1] - 1) * 100, 1)\nlines$pct_text &lt;- ifelse(lines$label == \"Best Fit (OLS)\", \n                         \"Optimal\", \n                         paste0(\"+\", lines$pct_increase, \"%\"))\n\n# Assign distinct colors for better visibility\nline_colors &lt;- c(\"Best Fit (OLS)\" = \"blue\", \n                \"Line A\" = \"red\", \n                \"Line B\" = \"darkgreen\", \n                \"Line C\" = \"purple\")\n\n# Create data for mini residual plots\nmini_data &lt;- data.frame()\nfor(i in 1:nrow(lines)) {\n  line_data &lt;- data.frame(\n    x = x,\n    y = y,\n    predicted = lines$intercept[i] + lines$slope[i] * x,\n    residuals = y - (lines$intercept[i] + lines$slope[i] * x),\n    line = lines$label[i]\n  )\n  mini_data &lt;- rbind(mini_data, line_data)\n}\n\n# Create main comparison plot with improved visibility\np1 &lt;- ggplot(data, aes(x = x, y = y)) +\n  # Add background grid for reference\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_line(color = \"gray90\"),\n    panel.grid.major = element_line(color = \"gray85\"),\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 13),\n    axis.title = element_text(size = 13, face = \"bold\"),\n    axis.text = element_text(size = 12)\n  ) +\n  # Add data points\n  geom_point(size = 2.5, alpha = 0.8) +\n  # Add lines with improved visibility\n  geom_abline(data = lines, \n              aes(intercept = intercept, slope = slope, \n                  color = label, linetype = label == \"Best Fit (OLS)\"),\n              size = 1.2) +\n  # Use custom colors\n  scale_color_manual(values = line_colors) +\n  scale_linetype_manual(values = c(\"TRUE\" = \"solid\", \"FALSE\" = \"dashed\"), guide = \"none\") +\n  # Better legends\n  labs(title = \"Comparing Different Regression Lines\",\n       subtitle = \"The OLS line minimizes the sum of squared errors\",\n       x = \"X\", y = \"Y\",\n       color = \"Regression Line\") +\n  guides(color = guide_legend(override.aes = list(size = 2)))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# Create mini residual plots with improved visibility\np_mini &lt;- list()\n\nfor(i in 1:nrow(lines)) {\n  line_data &lt;- subset(mini_data, line == lines$label[i])\n  \n  p_mini[[i]] &lt;- ggplot(line_data, aes(x = x, y = residuals)) +\n    # Add reference line\n    geom_hline(yintercept = 0, linetype = \"dashed\", size = 0.8, color = \"gray50\") +\n    # Add residual points with line color\n    geom_point(color = line_colors[lines$label[i]], size = 2.5) +\n    # Add squares to represent squared errors\n    geom_rect(aes(xmin = x - 0.3, xmax = x + 0.3,\n                  ymin = 0, ymax = residuals),\n              fill = line_colors[lines$label[i]], alpha = 0.2) +\n    # Improved titles\n    labs(title = lines$label[i],\n         subtitle = paste(\"SSE =\", round(lines$sse[i], 1), \n                          ifelse(i == 1, \" (Optimal)\", \n                                 paste0(\" (+\", lines$pct_increase[i], \"%)\"))),\n         x = NULL, y = NULL) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(size = 12, face = \"bold\", color = line_colors[lines$label[i]]),\n      plot.subtitle = element_text(size = 10),\n      panel.grid.minor = element_blank()\n    )\n}\n\n# Create SSE comparison table with better visibility\nsse_df &lt;- data.frame(\n  x = rep(1, nrow(lines)),\n  y = nrow(lines):1,\n  label = paste0(lines$label, \": SSE = \", round(lines$sse, 1), \" (\", lines$pct_text, \")\"),\n  color = line_colors[lines$label]\n)\n\nsse_table &lt;- ggplot(sse_df, aes(x = x, y = y, label = label, color = color)) +\n  geom_text(hjust = 0, size = 5, fontface = \"bold\") +\n  scale_color_identity() +\n  theme_void() +\n  xlim(1, 10) +\n  ylim(0.5, nrow(lines) + 0.5) +\n  labs(title = \"Sum of Squared Errors (SSE) Comparison\") +\n  theme(plot.title = element_text(hjust = 0, face = \"bold\", size = 14))\n\n# Arrange the plots with better spacing\ngrid.arrange(\n  p1, \n  arrangeGrob(p_mini[[1]], p_mini[[2]], p_mini[[3]], p_mini[[4]], \n              ncol = 2, padding = unit(1, \"cm\")),\n  sse_table, \n  ncol = 1, \n  heights = c(4, 3, 1)\n)\n\n\n\n\nComparing different regression lines\n\n\n\n\n\n\nKey Learning Points\n\nThe Sum of Squared Errors (SSE) is what Ordinary Least Squares (OLS) regression minimizes\nEach residual contributes its squared value to the total SSE\nThe OLS line has a lower SSE than any other possible line\nLarge residuals contribute disproportionately to the SSE due to the squaring operation\nThis is why outliers can have such a strong influence on regression lines\n\n\nStep-by-Step SSE Minimization\nTo illustrate the process of finding the minimum SSE, we can create a sequence that passes through the optimal point, showing how the SSE first decreases to a minimum and then increases again:\n\n# Create sample data\nset.seed(123)\nx &lt;- 1:20\ny &lt;- 2 + 3*x + rnorm(20, 0, 5)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, data = data)\ncoef &lt;- coefficients(model)\n\n# Create a sequence of steps that passes through the optimal OLS line\nsteps &lt;- 9  # Use odd number to have a middle point at the optimum\nstep_seq &lt;- data.frame(\n  step = 1:steps,\n  intercept = seq(coef[1] - 8, coef[1] + 8, length.out = steps),\n  slope = seq(coef[2] - 1.5, coef[2] + 1.5, length.out = steps)\n)\n\n# Mark the middle step (optimal OLS solution)\noptimal_step &lt;- ceiling(steps/2)\n\n# Calculate SSE for each step\nstep_seq$sse &lt;- sapply(1:nrow(step_seq), function(i) {\n  predicted &lt;- step_seq$intercept[i] + step_seq$slope[i] * x\n  sum((y - predicted)^2)\n})\n\n# Create a \"journey through the SSE valley\" plot\np2 &lt;- ggplot(data, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_abline(data = step_seq, \n              aes(intercept = intercept, slope = slope, \n                  color = sse, group = step),\n              size = 1) +\n  # Highlight the optimal line\n  geom_abline(intercept = step_seq$intercept[optimal_step], \n              slope = step_seq$slope[optimal_step],\n              color = \"green\", size = 1.5) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  labs(title = \"Journey Through the SSE Valley\",\n       subtitle = \"The green line represents the OLS solution with minimum SSE\",\n       color = \"SSE Value\") +\n  theme_minimal()\n\n# Create an SSE valley plot\np3 &lt;- ggplot(step_seq, aes(x = step, y = sse)) +\n  geom_line(size = 1) +\n  geom_point(size = 3, aes(color = sse)) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  # Highlight the optimal point\n  geom_point(data = step_seq[optimal_step, ], aes(x = step, y = sse), \n             size = 5, color = \"green\") +\n  # Add annotation\n  annotate(\"text\", x = optimal_step, y = step_seq$sse[optimal_step] * 1.1, \n           label = \"Minimum SSE\", color = \"darkgreen\", fontface = \"bold\") +\n  labs(title = \"The SSE Valley: Decreasing Then Increasing\",\n       subtitle = \"The SSE reaches its minimum at the OLS solution\",\n       x = \"Step\",\n       y = \"Sum of Squared Errors\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Display both plots\ngrid.arrange(p2, p3, ncol = 1, heights = c(3, 2))\n\n\n\n\nSSE minimization visualization\n\n\n\n\nIn R, the lm() function fits linear regression models:\n\nmodel &lt;- lm(y ~ x, data = data_frame)\n\n\n\n\nModel Interpretation: A Beginner’s Guide\nLet’s create a simple dataset to understand regression output better. Imagine we’re studying how years of education affect annual income:\n\n# Create a simple dataset - this is our Data Generating Process (DGP)\nset.seed(123) # For reproducibility\neducation_years &lt;- 10:20  # Education from 10 to 20 years\nn &lt;- length(education_years)\n\n# True parameters in our model - using more realistic values for Poland\ntrue_intercept &lt;- 3000   # Base monthly income with no education (in PLN)\ntrue_slope &lt;- 250        # Each year of education increases monthly income by 250 PLN\n\n# Generate monthly incomes with some random noise\nincome &lt;- true_intercept + true_slope * education_years + rnorm(n, mean=0, sd=300)\n\n# Create our dataset\neducation_income &lt;- data.frame(\n  education = education_years,\n  income = income\n)\n\n# Let's visualize our data\nlibrary(ggplot2)\nggplot(education_income, aes(x = education, y = income)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  scale_y_continuous(limits = c(5000, 8500), \n                     breaks = seq(5000, 8500, by = 500),\n                     labels = scales::comma) +\n  scale_x_continuous(breaks = 10:20) +\n  labs(\n    title = \"Relationship between Education and Income in Poland\",\n    x = \"Years of Education\",\n    y = \"Monthly Income (PLN)\",\n    subtitle = \"Red line shows the estimated linear relationship\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\")\n  ) +\n  annotate(\"text\", x = 11, y = 8000, \n           label = \"Each point represents\\none person's data\", \n           hjust = 0, size = 4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nFitting the Model\nNow let’s fit a linear regression model to this data:\n\n# Fit a simple regression model\nedu_income_model &lt;- lm(income ~ education, data = education_income)\n\n# Display the results\nmodel_summary &lt;- summary(edu_income_model)\nmodel_summary\n\n\nCall:\nlm(formula = income ~ education, data = education_income)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-427.72 -206.04  -38.12  207.32  460.78 \n\nCoefficients:\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)   3095.3      447.6   6.915 0.0000695 ***\neducation      247.2       29.2   8.467 0.0000140 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 306.3 on 9 degrees of freedom\nMultiple R-squared:  0.8885,    Adjusted R-squared:  0.8761 \nF-statistic: 71.69 on 1 and 9 DF,  p-value: 0.00001403\n\n\n\n\nUnderstanding the Regression Output Step by Step\nLet’s break down what each part of this output means in simple terms:\n\n1. The Formula\nAt the top, you see income ~ education, which means we’re predicting income based on education.\n\n\n2. Residuals\nThese show how far our predictions are from the actual values. Ideally, they should be centered around zero.\n\n\n3. Coefficients Table\n\n\n\nCoefficient Estimates\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n3095.27\n447.63\n6.91\n0\n\n\neducation\n247.23\n29.20\n8.47\n0\n\n\n\n\n\nIntercept (\\beta_0):\n\nValue: Approximately 3095\nInterpretation: This is the predicted monthly income for someone with 0 years of education\nNote: Sometimes the intercept isn’t meaningful in real-world terms, especially if x=0 is outside your data range\n\nEducation (\\beta_1):\n\nValue: Approximately 247\nInterpretation: For each additional year of education, we expect monthly income to increase by this amount in PLN\nThis is our main coefficient of interest!\n\nStandard Error:\n\nMeasures how precise our estimates are\nSmaller standard errors mean more precise estimates\nThink of it as “give or take how much” for our coefficients\n\nt value:\n\nThis is the coefficient divided by its standard error\nIt tells us how many standard errors away from zero our coefficient is\nLarger absolute t values (above 2) suggest the effect is statistically significant\n\np-value:\n\nThe probability of seeing our result (or something more extreme) if there was actually no relationship\nTypically, p &lt; 0.05 is considered statistically significant\nFor education, p = 0.000014, which is significant!\n\n\n\n4. Model Fit Statistics\n\n\n\nModel Fit Statistics\n\n\nStatistic\nValue\n\n\n\n\nR-squared\n0.888\n\n\nAdjusted R-squared\n0.876\n\n\nF-statistic\n71.686\n\n\np-value\n0.000\n\n\n\n\n\nR-squared:\n\nValue: 0.888\nInterpretation: 89% of the variation in income is explained by education\nHigher is better, but be cautious of very high values (could indicate overfitting)\n\nF-statistic:\n\nTests whether the model as a whole is statistically significant\nA high F-statistic with a low p-value indicates a significant model\n\n\n\n\nVisualizing the Model Results\nLet’s visualize what our model actually tells us:\n\n# Predicted values\neducation_income$predicted &lt;- predict(edu_income_model)\neducation_income$residuals &lt;- residuals(edu_income_model)\n\n# Create a more informative plot\nggplot(education_income, aes(x = education, y = income)) +\n  # Actual data points\n  geom_point(size = 3, color = \"blue\") +\n  \n  # Regression line\n  geom_line(aes(y = predicted), color = \"red\", size = 1.2) +\n  \n  # Residual lines\n  geom_segment(aes(xend = education, yend = predicted), \n               color = \"darkgray\", linetype = \"dashed\") +\n  \n  # Set proper scales\n  scale_y_continuous(limits = c(5000, 8500), \n                     breaks = seq(5000, 8500, by = 500),\n                     labels = scales::comma) +\n  scale_x_continuous(breaks = 10:20) +\n  \n  # Annotations\n  annotate(\"text\", x = 19, y = 7850, \n           label = paste(\"Slope =\", round(coef(edu_income_model)[2]), \"PLN per year\"),\n           color = \"red\", hjust = 1, fontface = \"bold\") +\n  \n  annotate(\"text\", x = 10.5, y = 5500, \n           label = paste(\"Intercept =\", round(coef(edu_income_model)[1]), \"PLN\"),\n           color = \"red\", hjust = 0, fontface = \"bold\") +\n  \n  annotate(\"text\", x = 14, y = 8200, \n           label = paste(\"R² =\", round(model_summary$r.squared, 2)),\n           color = \"black\", fontface = \"bold\") +\n  \n  # Labels\n  labs(\n    title = \"Interpreting the Education-Income Regression Model\",\n    subtitle = \"Red line shows predicted income for each education level\",\n    x = \"Years of Education\",\n    y = \"Monthly Income (PLN)\",\n    caption = \"Gray dashed lines represent residuals (prediction errors)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\nReal-World Interpretation\n\nA person with 16 years of education (college graduate) would be predicted to earn about: \\hat{Y} = 3095 + 247 \\times 16 = 7051 \\text{ PLN monthly}\nThe model suggests that each additional year of education is associated with a 247 PLN increase in monthly income.\nOur model explains approximately 89% of the variation in income in our sample.\nThe relationship is statistically significant (p &lt; 0.001), meaning it’s very unlikely to observe this relationship if education truly had no effect on income.\n\n\n\nImportant Cautions for Beginners\n\nCorrelation ≠ Causation: Our model shows association, not necessarily causation\nOmitted Variables: Other factors might influence both education and income\nExtrapolation: Be careful predicting outside the range of your data\nLinear Relationship: We’ve assumed the relationship is linear, which may not always be true",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#regression-analysis-and-ordinary-least-squares",
    "href": "correg_en.html#regression-analysis-and-ordinary-least-squares",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.28 Regression Analysis and Ordinary Least Squares (*)",
    "text": "13.28 Regression Analysis and Ordinary Least Squares (*)\n\nFoundations of Regression Analysis\nRegression analysis constitutes a fundamental statistical methodology for examining relationships between variables. At its core, regression provides a systematic framework for understanding how changes in one or more independent variables influence a dependent variable.\nThe primary objectives of regression analysis include: - Quantifying relationships between variables - Making predictions based on observed patterns - Testing hypotheses about variable associations - Understanding the proportion of variation explained by predictors\n\n\nDeterministic versus Stochastic Models\nStatistical modeling encompasses two fundamental approaches:\nDeterministic models assume precise, invariant relationships between variables. Given specific inputs, these models yield identical outputs without variation. Consider the physics equation:\n\\text{Distance} = \\text{Speed} \\times \\text{Time}\nThis relationship exhibits no randomness; identical inputs always produce identical outputs.\nStochastic models, in contrast, acknowledge inherent variability in real-world phenomena. Regression analysis employs stochastic modeling through the fundamental equation:\nY = f(X) + \\epsilon\nWhere: - Y represents the outcome variable - f(X) captures the systematic relationship between predictors and outcome - \\epsilon represents random variation inherent in the data\nThis formulation recognizes that real-world relationships contain both systematic patterns and random variation.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#simple-linear-regression-model",
    "href": "correg_en.html#simple-linear-regression-model",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.29 Simple Linear Regression Model",
    "text": "13.29 Simple Linear Regression Model\n\nModel Specification\nSimple linear regression models the relationship between a single predictor variable and an outcome variable through a linear equation:\nY_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\nThe model components represent: - Y_i: The dependent variable for observation i - X_i: The independent variable for observation i - \\beta_0: The population intercept parameter - \\beta_1: The population slope parameter - \\epsilon_i: The random error term for observation i\n\n\nInterpretation of Parameters\nThe parameters possess specific interpretations:\n\nIntercept (\\beta_0): The expected value of Y when X = 0. This represents the baseline level of the outcome variable.\nSlope (\\beta_1): The expected change in Y for a one-unit increase in X. This quantifies the strength and direction of the linear relationship.\nError term (\\epsilon_i): Captures all factors affecting Y not explained by X, including measurement error, omitted variables, and inherent randomness.\n\n\n\nEstimation versus True Parameters\nThe distinction between population parameters and sample estimates proves crucial:\n\nPopulation parameters (\\beta_0, \\beta_1) represent true, unknown values\nSample estimates (\\hat{\\beta}_0, \\hat{\\beta}_1) represent our best approximations based on available data\nThe hat notation (^) consistently denotes estimated values\n\nThe fitted regression equation becomes:\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1X_i",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-ordinary-least-squares-method-1",
    "href": "correg_en.html#the-ordinary-least-squares-method-1",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.30 The Ordinary Least Squares Method",
    "text": "13.30 The Ordinary Least Squares Method\n\nThe Fundamental Challenge\nGiven a dataset with observations (X_i, Y_i), we need a systematic method to determine the “best” values for \\hat{\\beta}_0 and \\hat{\\beta}_1. The challenge lies in defining what constitutes “best” and developing a practical method to find these values.\nConsider that for any given line through the data, each observation will have a prediction error or residual:\ne_i = Y_i - \\hat{Y}_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1X_i)\nThese residuals represent how far our predictions deviate from actual values. A good fitting line should make these residuals as small as possible overall.\n\n\nWhy Minimize the Sum of Squared Residuals?\nThe Ordinary Least Squares method determines optimal parameter estimates by minimizing the sum of squared residuals. This choice requires justification, as we could conceivably minimize other quantities. The rationale for squaring residuals includes:\nMathematical tractability: Squaring creates a smooth, differentiable function that yields closed-form solutions through calculus. The derivatives of squared terms lead to linear equations that can be solved analytically.\nEqual treatment of positive and negative errors: Simply summing raw residuals would allow positive and negative errors to cancel, potentially yielding a sum of zero even when predictions are poor. Squaring ensures all deviations contribute positively to the total error measure.\nPenalization of large errors: Squaring gives progressively greater weight to larger errors. An error of 4 units contributes 16 to the sum, while an error of 2 units contributes only 4. This property encourages finding a line that avoids extreme prediction errors.\nStatistical optimality: Under certain assumptions (including normally distributed errors), OLS estimators possess desirable statistical properties, including being the Best Linear Unbiased Estimators (BLUE) according to the Gauss-Markov theorem.\nConnection to variance: The sum of squared deviations directly relates to variance, a fundamental measure of spread in statistics. Minimizing squared residuals thus minimizes the variance of prediction errors.\n\n\nThe OLS Optimization Problem\nThe OLS method formally seeks values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that minimize:\n\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1X_i))^2\nThis optimization problem can be solved using calculus by: 1. Taking partial derivatives with respect to \\hat{\\beta}_0 and \\hat{\\beta}_1 2. Setting these derivatives equal to zero 3. Solving the resulting system of equations\n\n\nDerivation of OLS Estimators\nThe minimization yields closed-form solutions:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n(X_i - \\bar{X})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\nThese formulas reveal that: - The slope estimate depends on the covariance between variables relative to the predictor’s variance - The intercept ensures the regression line passes through the point of means (\\bar{X}, \\bar{Y})\n\n\nProperties of OLS Estimators\nOLS estimators possess several desirable properties:\n\nUnbiasedness: Under appropriate conditions, E[\\hat{\\beta}_j] = \\beta_j\nEfficiency: OLS provides minimum variance among linear unbiased estimators\nConsistency: As sample size increases, estimates converge to true values\nThe regression line passes through the centroid: The point (\\bar{X}, \\bar{Y}) always lies on the fitted line\n\n\n\nExtension to Multiple Regression\nWhile this guide focuses on simple linear regression with one predictor, the OLS framework extends naturally to multiple regression with several predictors:\nY_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + ... + \\beta_kX_{ki} + \\epsilon_i\nThe same principle applies: we minimize the sum of squared residuals, though the mathematics involves matrix algebra rather than simple formulas. The fundamental logic—finding parameter values that minimize prediction errors—remains unchanged.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#understanding-variance-decomposition",
    "href": "correg_en.html#understanding-variance-decomposition",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.31 Understanding Variance Decomposition",
    "text": "13.31 Understanding Variance Decomposition\n\nThe Baseline Model Concept\nBefore introducing predictors, consider the simplest possible model: predicting every observation using the overall mean \\bar{Y}. This baseline model represents our best prediction in the absence of additional information.\nThe baseline model’s predictions: \\hat{Y}_i^{\\text{baseline}} = \\bar{Y} \\text{ for all } i\nThis model serves as a reference point for evaluating improvement gained through incorporating predictors. The baseline model essentially asks: “If we knew nothing about the relationship between X and Y, what would be our best constant prediction?”\n\n\nComponents of Total Variation\nThe total variation in the outcome variable decomposes into three fundamental components:\n\nTotal Sum of Squares (SST)\nSST quantifies the total variation in the outcome variable relative to its mean:\n\\text{SST} = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\nInterpretation: SST represents the total variance that requires explanation. It measures the prediction error when using only the mean as our model—essentially the variance explained by the baseline (zero) model. This is the starting point: the total amount of variation we hope to explain by introducing predictors.\n\n\nRegression Sum of Squares (SSR)\nSSR measures the variation explained by the regression model:\n\\text{SSR} = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\nInterpretation: SSR quantifies the improvement in prediction achieved by incorporating the predictor variable. It represents the reduction in prediction error relative to the baseline model—the portion of total variation that our regression line successfully captures.\n\n\nError Sum of Squares (SSE)\nSSE captures the unexplained variation remaining after regression:\n\\text{SSE} = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\nInterpretation: SSE represents the residual variation that the model cannot explain, reflecting the inherent randomness and effects of omitted variables. This is the variation that remains even after using our best-fitting line.\n\n\n\nThe Fundamental Decomposition Identity\nThese components relate through the fundamental equation:\n\\text{SST} = \\text{SSR} + \\text{SSE}\nThis identity demonstrates that: - Total variation equals the sum of explained and unexplained components - The regression model partitions total variation into systematic and random parts - Model improvement can be assessed by comparing SSR to SST\n\n\nConceptual Framework for Variance Decomposition\n# Demonstration of Variance Decomposition\nlibrary(ggplot2)\n\n# Generate sample data\nset.seed(42)\nn &lt;- 50\nx &lt;- runif(n, 1, 10)\ny &lt;- 3 + 2*x + rnorm(n, 0, 2)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit model\nmodel &lt;- lm(y ~ x, data = data)\ny_mean &lt;- mean(y)\ny_pred &lt;- predict(model)\n\n# Calculate components\nSST &lt;- sum((y - y_mean)^2)\nSSR &lt;- sum((y_pred - y_mean)^2)\nSSE &lt;- sum((y - y_pred)^2)\n\n# Display decomposition\ncat(\"Variance Decomposition\\n\")\ncat(\"======================\\n\")\ncat(\"Total SS (SST):\", round(SST, 2), \n    \"- Total variation from mean\\n\")\ncat(\"Regression SS (SSR):\", round(SSR, 2), \n    \"- Variation explained by model\\n\")\ncat(\"Error SS (SSE):\", round(SSE, 2), \n    \"- Unexplained variation\\n\")\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\ncat(round(SST, 2), \"=\", round(SSR, 2), \"+\", \n    round(SSE, 2), \"\\n\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-coefficient-of-determination-r²",
    "href": "correg_en.html#the-coefficient-of-determination-r²",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.32 The Coefficient of Determination (R²)",
    "text": "13.32 The Coefficient of Determination (R²)\n\nDefinition and Calculation\nThe coefficient of determination, denoted R², quantifies the proportion of total variation explained by the regression model:\nR^2 = \\frac{\\text{SSR}}{\\text{SST}} = \\frac{\\text{Explained Variation}}{\\text{Total Variation}}\nAlternatively expressed as:\nR^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}} = 1 - \\frac{\\text{Unexplained Variation}}{\\text{Total Variation}}\nR² directly answers the question: “What proportion of the total variation in Y (relative to the baseline mean model) does our regression model explain?”\n\n\nInterpretation Guidelines\nR² values range from 0 to 1, with specific interpretations:\n\nR² = 0: The model explains no variation beyond the baseline mean model. The regression line provides no improvement over simply using \\bar{Y}.\nR² = 0.25: The model explains 25% of total variation. Three-quarters of the variation remains unexplained.\nR² = 0.75: The model explains 75% of total variation. This represents substantial explanatory power.\nR² = 1.00: The model explains all variation (perfect fit). All data points fall exactly on the regression line.\n\n\n\nContextual Considerations\nThe interpretation of R² requires careful consideration of context:\nField-specific standards: Acceptable R² values vary dramatically across disciplines - Physical sciences often expect R² &gt; 0.90 due to controlled conditions - Social sciences may consider R² = 0.30 meaningful given human complexity - Biological systems typically show intermediate values due to natural variation\nSample size effects: Small samples can artificially inflate R², leading to overly optimistic assessments of model fit.\nModel complexity: In multiple regression, additional predictors mechanically increase R², even if they lack true explanatory power.\nPractical significance: Statistical fit should align with substantive importance. A model with R² = 0.95 may be less useful than one with R² = 0.60 if the latter addresses more relevant questions.\n\n\nAdjusted R² for Multiple Regression\nWhen extending to multiple regression, adjusted R² accounts for the number of predictors:\nR^2_{\\text{adj}} = 1 - \\frac{\\text{SSE}/(n-p)}{\\text{SST}/(n-1)}\nWhere: - n = sample size - p = number of parameters (including intercept)\nAdjusted R² penalizes model complexity, providing a more conservative measure when comparing models with different numbers of predictors.\n\n\nMeasures of Fit\n\nR-squared (R^2): R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nRoot Mean Square Error (RMSE): RMSE = \\sqrt{\\frac{SSE}{n}}\nMean Absolute Error (MAE): MAE = \\frac{1}{n}\\sum_{i=1}^n |Y_i - \\hat{Y}_i|",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#practical-implementation-1",
    "href": "correg_en.html#practical-implementation-1",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.33 Practical Implementation",
    "text": "13.33 Practical Implementation\n\nComplete Regression Analysis Example\n\n# Comprehensive regression analysis\nlibrary(ggplot2)\n\n# Generate educational data\nset.seed(123)\nn &lt;- 100\nstudy_hours &lt;- runif(n, 0, 10)\nexam_scores &lt;- 50 + 4*study_hours + rnorm(n, 0, 5)\neducation_data &lt;- data.frame(\n  study_hours = study_hours,\n  exam_scores = exam_scores\n)\n\n# Fit regression model\nmodel &lt;- lm(exam_scores ~ study_hours, data = education_data)\n\n# Extract key statistics\ny_mean &lt;- mean(exam_scores)\ny_pred &lt;- predict(model)\n\n# Variance decomposition\nSST &lt;- sum((exam_scores - y_mean)^2)\nSSR &lt;- sum((y_pred - y_mean)^2)\nSSE &lt;- sum((exam_scores - y_pred)^2)\nr_squared &lt;- SSR/SST\n\n# Display results\ncat(\"OLS Regression Results\\n\")\n\nOLS Regression Results\n\ncat(\"======================\\n\")\n\n======================\n\ncat(\"\\nParameter Estimates:\\n\")\n\n\nParameter Estimates:\n\ncat(\"Intercept (β₀):\", round(coef(model)[1], 2), \"\\n\")\n\nIntercept (β₀): 49.96 \n\ncat(\"Slope (β₁):\", round(coef(model)[2], 2), \"\\n\")\n\nSlope (β₁): 3.96 \n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Expected score with 0 study hours:\", \n    round(coef(model)[1], 2), \"\\n\")\n\n- Expected score with 0 study hours: 49.96 \n\ncat(\"- Score increase per study hour:\", \n    round(coef(model)[2], 2), \"\\n\")\n\n- Score increase per study hour: 3.96 \n\ncat(\"\\nVariance Decomposition:\\n\")\n\n\nVariance Decomposition:\n\ncat(\"Total variation (SST):\", round(SST, 2), \"\\n\")\n\nTotal variation (SST): 14880.14 \n\ncat(\"Explained by model (SSR):\", round(SSR, 2), \"\\n\")\n\nExplained by model (SSR): 12578.3 \n\ncat(\"Unexplained (SSE):\", round(SSE, 2), \"\\n\")\n\nUnexplained (SSE): 2301.84 \n\ncat(\"\\nModel Performance:\\n\")\n\n\nModel Performance:\n\ncat(\"R-squared:\", round(r_squared, 4), \"\\n\")\n\nR-squared: 0.8453 \n\ncat(\"Interpretation: The model explains\", \n    round(r_squared * 100, 1), \n    \"% of the variation in exam scores\\n\")\n\nInterpretation: The model explains 84.5 % of the variation in exam scores\n\ncat(\"beyond what the mean alone could explain.\\n\")\n\nbeyond what the mean alone could explain.\n\n\n\n\nVisualization of Key Concepts\n\n# Create comprehensive visualization\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Plot 1: Baseline model (mean only)\np1 &lt;- ggplot(education_data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = y_mean, color = \"blue\", size = 1) +\n  geom_segment(aes(xend = study_hours, yend = y_mean), \n               alpha = 0.3, color = \"red\") +\n  labs(title = \"Baseline Model: Predicting with Mean\",\n       subtitle = paste(\"Total variation (SST) =\", round(SST, 1)),\n       x = \"Study Hours\", y = \"Exam Scores\") +\n  theme_minimal() +\n  annotate(\"text\", x = 8, y = y_mean + 1, \n           label = \"Mean of Y\", color = \"blue\")\n\n# Plot 2: Regression model\np2 &lt;- ggplot(education_data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_segment(aes(xend = study_hours, yend = y_pred), \n               alpha = 0.3, color = \"red\") +\n  labs(title = \"Regression Model: Improved Predictions\",\n       subtitle = paste(\"Unexplained variation (SSE) =\", \n                       round(SSE, 1), \n                       \"| R² =\", round(r_squared, 3)),\n       x = \"Study Hours\", y = \"Exam Scores\") +\n  theme_minimal() +\n  annotate(\"text\", x = 8, y = max(y_pred) + 1, \n           label = \"Regression Line\", color = \"blue\")\n\n# Plot 3: Variance components\nvariance_data &lt;- data.frame(\n  Component = c(\"Total (SST)\", \"Explained (SSR)\", \"Unexplained (SSE)\"),\n  Value = c(SST, SSR, SSE),\n  Type = c(\"Total\", \"Explained\", \"Unexplained\")\n)\n\np3 &lt;- ggplot(variance_data, aes(x = Component, y = Value, fill = Type)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"Total\" = \"gray50\", \n                               \"Explained\" = \"green4\", \n                               \"Unexplained\" = \"red3\")) +\n  labs(title = \"Variance Decomposition\",\n       y = \"Sum of Squares\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Combine plots\ngrid.arrange(p1, p2, p3, layout_matrix = rbind(c(1, 2), c(3, 3)))\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-and-key-insights",
    "href": "correg_en.html#summary-and-key-insights",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.34 Summary and Key Insights",
    "text": "13.34 Summary and Key Insights\n\nCore Concepts Review\nRegression analysis models relationships between variables using stochastic frameworks that acknowledge inherent variation. The simple linear regression model expresses this relationship as Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i.\nOrdinary Least Squares provides optimal parameter estimates by minimizing the sum of squared residuals. This choice of minimizing squared errors stems from mathematical tractability, equal treatment of positive and negative errors, appropriate penalization of large errors, and desirable statistical properties.\nVariance decomposition partitions total variation into: - Total variation from the baseline mean model (SST) - Variation explained by the regression model (SSR)\n- Unexplained residual variation (SSE)\nThe fundamental identity SST = SSR + SSE shows how regression improves upon the baseline model.\nR² quantifies model performance as the proportion of total variation explained, providing a standardized measure of how much better the regression model performs compared to simply using the mean.\n\n\nCritical Considerations\nThe baseline model (predicting with the mean) serves as the fundamental reference point. All regression improvement is measured relative to this simple model. SST represents the total variance requiring explanation when we start with no predictors.\nParameter estimates (\\hat{\\beta}_0, \\hat{\\beta}_1) represent sample-based approximations of unknown population values (\\beta_0, \\beta_1). The distinction between population parameters and sample estimates remains crucial for proper inference.\nModel assessment requires considering both statistical fit (R²) and practical significance. A model with modest R² may still provide valuable insights, while high R² does not guarantee causation or practical utility.\n\n\nExtensions and Applications\nWhile this guide focuses on simple linear regression, the framework extends naturally to: - Multiple regression with several predictors - Polynomial regression for nonlinear relationships - Interaction terms to capture conditional effects - Categorical predictors through appropriate coding\nThe fundamental principles—minimizing prediction errors, decomposing variation, and assessing model fit—remain consistent across these extensions. The mathematical complexity increases, but the conceptual foundation established here continues to apply.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#key-assumptions-of-linear-regression",
    "href": "correg_en.html#key-assumptions-of-linear-regression",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.35 Key Assumptions of Linear Regression",
    "text": "13.35 Key Assumptions of Linear Regression\n\nStrict Exogeneity: The Fundamental Assumption\nThe most crucial assumption in regression is strict exogeneity:\nE[\\varepsilon|X] = 0\nThis means:\n\nThe error term has zero mean conditional on X\nX contains no information about the average error\nThere are no systematic patterns in how our predictions are wrong\n\nLet’s visualize when this assumption holds and when it doesn’t:\n\n# Generate data\nset.seed(789)\nx &lt;- seq(1, 10, by = 0.2)\n\n# Case 1: Exogenous errors\ny_exog &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\n\n# Case 2: Non-exogenous errors (error variance increases with x)\ny_nonexog &lt;- 2 + 3*x + 0.5*x*rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_exog &lt;- data.frame(\n  x = x,\n  y = y_exog,\n  type = \"Exogenous Errors\\n(Assumption Satisfied)\"\n)\n\ndata_nonexog &lt;- data.frame(\n  x = x,\n  y = y_nonexog,\n  type = \"Non-Exogenous Errors\\n(Assumption Violated)\"\n)\n\ndata_combined &lt;- rbind(data_exog, data_nonexog)\n\n# Create plots with residuals\nplot_residuals &lt;- function(data, title) {\n  model &lt;- lm(y ~ x, data = data)\n  data$predicted &lt;- predict(model)\n  data$residuals &lt;- residuals(model)\n  \n  p1 &lt;- ggplot(data, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    theme_minimal() +\n    labs(title = title)\n  \n  p2 &lt;- ggplot(data, aes(x = x, y = residuals)) +\n    geom_point() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n    theme_minimal() +\n    labs(y = \"Residuals\")\n  \n  list(p1, p2)\n}\n\n# Generate plots\nplots_exog &lt;- plot_residuals(data_exog, \"Exogenous Errors\")\nplots_nonexog &lt;- plot_residuals(data_nonexog, \"Non-Exogenous Errors\")\n\n# Arrange plots\ngridExtra::grid.arrange(\n  plots_exog[[1]], plots_exog[[2]],\n  plots_nonexog[[1]], plots_nonexog[[2]],\n  ncol = 2\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 13.1: Exogeneity vs. Non-Exogeneity Examples\n\n\n\n\n\n\n\nLinearity: The Form Assumption\nThe relationship between X and Y should be linear in parameters:\nE[Y|X] = \\beta_0 + \\beta_1X\nNote that this doesn’t mean X and Y must have a straight-line relationship - we can transform variables. Let’s see different types of relationships:\n\n# Generate data\nset.seed(101)\nx &lt;- seq(1, 10, by = 0.1)\n\n# Different relationships\ndata_relationships &lt;- data.frame(\n  x = rep(x, 3),\n  y = c(\n    # Linear\n    2 + 3*x + rnorm(length(x), 0, 2),\n    # Quadratic\n    2 + 0.5*x^2 + rnorm(length(x), 0, 2),\n    # Exponential\n    exp(0.3*x) + rnorm(length(x), 0, 2)\n  ),\n  type = rep(c(\"Linear\", \"Quadratic\", \"Exponential\"), each = length(x))\n)\n\n# Plot\nggplot(data_relationships, aes(x = x, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  facet_wrap(~type, scales = \"free_y\") +\n  theme_minimal() +\n  labs(subtitle = \"Red: linear fit, Blue: true relationship\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 13.2: Linear and Nonlinear Relationships\n\n\n\n\n\n\n\nUnderstanding Violations and Solutions\nWhen linearity is violated:\n\nTransform variables:\n\nLog transformation: for exponential relationships\nSquare root: for moderate nonlinearity\nPower transformations: for more complex relationships\n\n\n\n# Generate exponential data\nset.seed(102)\nx &lt;- seq(1, 10, by = 0.2)\ny &lt;- exp(0.3*x) + rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_trans &lt;- data.frame(\n  x = x,\n  y = y,\n  log_y = log(y)\n)\n\nWarning in log(y): NaNs produced\n\n# Original scale plot\np1 &lt;- ggplot(data_trans, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Original Scale\")\n\n# Log scale plot\np2 &lt;- ggplot(data_trans, aes(x = x, y = log_y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Log-Transformed Y\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFigure 13.3: Effect of Variable Transformations",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#spurious-correlation-causes-and-examples",
    "href": "correg_en.html#spurious-correlation-causes-and-examples",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.36 Spurious Correlation: Causes and Examples",
    "text": "13.36 Spurious Correlation: Causes and Examples\nSpurious correlation occurs when variables appear related but the relationship is not causal. These misleading correlations arise from several sources:\n\nRandom coincidence (chance)\nConfounding variables (hidden third factors)\nSelection biases\nImproper statistical analysis\nReverse causality\nEndogeneity problems (including simultaneity)\n\n\nRandom Coincidence (Chance)\nWith sufficient data mining or small sample sizes, seemingly meaningful correlations can emerge purely by chance. This is especially problematic when researchers conduct multiple analyses without appropriate corrections for multiple comparisons, a practice known as “p-hacking.”\n\n# Create a realistic example of spurious correlation based on actual country data\n# Using country data on chocolate consumption and Nobel prize winners\n# This example is inspired by a published correlation (Messerli, 2012)\nset.seed(123)\ncountries &lt;- c(\"Switzerland\", \"Sweden\", \"Denmark\", \"Belgium\", \"Austria\", \n               \"Norway\", \"Germany\", \"Netherlands\", \"United Kingdom\", \"Finland\", \n               \"France\", \"Italy\", \"Spain\", \"Poland\", \"Greece\", \"Portugal\")\n\n# Create realistic data: Chocolate consumption correlates with GDP per capita\n# Higher GDP countries tend to consume more chocolate and have better research funding\ngdp_per_capita &lt;- c(87097, 58977, 67218, 51096, 53879, 89154, 51860, 57534, \n                    46510, 53982, 43659, 35551, 30416, 17841, 20192, 24567)\n\n# Normalize GDP values to make them more manageable\ngdp_normalized &lt;- (gdp_per_capita - min(gdp_per_capita)) / \n                 (max(gdp_per_capita) - min(gdp_per_capita))\n\n# More realistic chocolate consumption - loosely based on real consumption patterns\n# plus some randomness, but influenced by GDP\nchocolate_consumption &lt;- 4 + 8 * gdp_normalized + rnorm(16, 0, 0.8)\n\n# Nobel prizes - also influenced by GDP (research funding) with noise\n# The relationship is non-linear, but will show up as correlated\nnobel_prizes &lt;- 2 + 12 * gdp_normalized^1.2 + rnorm(16, 0, 1.5)\n\n# Create dataframe\ncountry_data &lt;- data.frame(\n  country = countries,\n  chocolate = round(chocolate_consumption, 1),\n  nobel = round(nobel_prizes, 1),\n  gdp = gdp_per_capita\n)\n\n# Fit regression model - chocolate vs nobel without controlling for GDP\nchocolate_nobel_model &lt;- lm(nobel ~ chocolate, data = country_data)\n\n# Better model that reveals the confounding\nfull_model &lt;- lm(nobel ~ chocolate + gdp, data = country_data)\n\n# Plot the apparent relationship\nggplot(country_data, aes(x = chocolate, y = nobel)) +\n  geom_point(color = \"darkblue\", size = 3, alpha = 0.7) +\n  geom_text(aes(label = country), hjust = -0.2, vjust = 0, size = 3) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  labs(\n    title = \"Apparent Correlation: Chocolate Consumption vs. Nobel Prizes\",\n    subtitle = \"Demonstrates how confounding variables create spurious correlations\",\n    x = \"Chocolate Consumption (kg per capita)\",\n    y = \"Nobel Prizes per 10M Population\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Show regression results\nsummary(chocolate_nobel_model)\n\n\nCall:\nlm(formula = nobel ~ chocolate, data = country_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9080 -1.4228  0.0294  0.5962  3.2977 \n\nCoefficients:\n            Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept)  -4.0518     1.3633  -2.972     0.0101 *  \nchocolate     1.3322     0.1682   7.921 0.00000154 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.626 on 14 degrees of freedom\nMultiple R-squared:  0.8176,    Adjusted R-squared:  0.8045 \nF-statistic: 62.75 on 1 and 14 DF,  p-value: 0.000001536\n\n# Demonstrate multiple testing problem\np_values &lt;- numeric(100)\nfor(i in 1:100) {\n  # Generate two completely random variables with n=20\n  x &lt;- rnorm(20)\n  y &lt;- rnorm(20)\n  # Test for correlation and store p-value\n  p_values[i] &lt;- cor.test(x, y)$p.value\n}\n\n# How many \"significant\" results at alpha = 0.05?\nsum(p_values &lt; 0.05)\n\n[1] 3\n\n# Visualize the multiple testing phenomenon\nhist(p_values, breaks = 20, main = \"P-values from 100 Tests of Random Data\",\n     xlab = \"P-value\", col = \"lightblue\", border = \"white\")\nabline(v = 0.05, col = \"red\", lwd = 2, lty = 2)\ntext(0.15, 20, paste(\"Approximately\", sum(p_values &lt; 0.05),\n                     \"tests are 'significant'\\nby random chance alone!\"), \n     col = \"darkred\")\n\n\n\n\n\n\n\n\nThis example demonstrates how seemingly compelling correlations can emerge between unrelated variables due to confounding factors and chance. The correlation between chocolate consumption and Nobel prizes appears significant (p &lt; 0.05) when analyzed directly, even though it’s explained by a third variable - national wealth (GDP per capita).\nWealthier countries typically consume more chocolate and simultaneously invest more in education and research, leading to more Nobel prizes. Without controlling for this confounding factor, we would mistakenly conclude a direct relationship between chocolate and Nobel prizes.\nThe multiple testing demonstration further illustrates why spurious correlations appear so frequently in research. When conducting 100 statistical tests on completely random data, we expect approximately 5 “significant” results at α = 0.05 purely by chance. In real research settings where hundreds of variables might be analyzed, the probability of finding false positive correlations increases dramatically.\nThis example underscores three critical points:\n\nSmall sample sizes (16 countries) are particularly vulnerable to chance correlations\nConfounding variables can create strong apparent associations between unrelated factors\nMultiple testing without appropriate corrections virtually guarantees finding “significant” but meaningless patterns\n\nSuch findings explain why replication is essential in research and why most initial “discoveries” fail to hold up in subsequent studies.\n\n\nConfounding Variables (Hidden Third Factors)\nConfounding occurs when an external variable influences both the predictor and outcome variables, creating an apparent relationship that may disappear when the confounder is accounted for.\n\n# Create sample data\nn &lt;- 200\nability &lt;- rnorm(n, 100, 15)                       # Natural ability \neducation &lt;- 10 + 0.05 * ability + rnorm(n, 0, 2)  # Education affected by ability\nincome &lt;- 10000 + 2000 * education + 100 * ability + rnorm(n, 0, 5000)  # Income affected by both\n\nomitted_var_data &lt;- data.frame(\n  ability = ability,\n  education = education,\n  income = income\n)\n\n# Model without accounting for ability\nmodel_naive &lt;- lm(income ~ education, data = omitted_var_data)\n\n# Model accounting for ability\nmodel_full &lt;- lm(income ~ education + ability, data = omitted_var_data)\n\n# Show results\nsummary(model_naive)\n\n\nCall:\nlm(formula = income ~ education, data = omitted_var_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14422.9  -3362.1    142.7   3647.7  14229.6 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  18982.0     2410.5   7.875    0.000000000000221 ***\neducation     2050.9      158.7  12.926 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5066 on 198 degrees of freedom\nMultiple R-squared:  0.4576,    Adjusted R-squared:  0.4549 \nF-statistic: 167.1 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_full)\n\n\nCall:\nlm(formula = income ~ education + ability, data = omitted_var_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12739.9  -3388.7    -41.1   3572.1  14976.8 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 13203.84    3018.85   4.374            0.0000198 ***\neducation    1871.43     166.03  11.272 &lt; 0.0000000000000002 ***\nability        85.60      27.87   3.071              0.00243 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4961 on 197 degrees of freedom\nMultiple R-squared:  0.4824,    Adjusted R-squared:  0.4772 \nF-statistic: 91.81 on 2 and 197 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Create visualization with ability shown through color\nggplot(omitted_var_data, aes(x = education, y = income, color = ability)) +\n  geom_point(alpha = 0.8) +\n  scale_color_viridis_c(name = \"Ability Score\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) + \n  labs(\n    title = \"Income vs. Education, Colored by Ability\",\n    subtitle = \"Visualizing the confounding variable\",\n    x = \"Years of Education\",\n    y = \"Annual Income (PLN)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis example illustrates omitted variable bias: without accounting for ability, the estimated effect of education on income is exaggerated (2,423 PLN per year vs. 1,962 PLN per year). The confounding occurs because ability influences both education and income, creating a spurious component in the observed correlation.\n\nClassic Example: Ice Cream and Drownings\nA classic example of confounding involves the correlation between ice cream sales and drowning incidents, both influenced by temperature:\n\n# Create sample data\nn &lt;- 100\ntemperature &lt;- runif(n, 5, 35)  # Temperature in Celsius\n\n# Both ice cream sales and drownings are influenced by temperature\nice_cream_sales &lt;- 100 + 10 * temperature + rnorm(n, 0, 20)\ndrownings &lt;- 1 + 0.3 * temperature + rnorm(n, 0, 1)\n\nconfounding_data &lt;- data.frame(\n  temperature = temperature,\n  ice_cream_sales = ice_cream_sales,\n  drownings = drownings\n)\n\n# Model without controlling for temperature\nmodel_naive &lt;- lm(drownings ~ ice_cream_sales, data = confounding_data)\n\n# Model controlling for temperature\nmodel_full &lt;- lm(drownings ~ ice_cream_sales + temperature, data = confounding_data)\n\n# Show results\nsummary(model_naive)\n\n\nCall:\nlm(formula = drownings ~ ice_cream_sales, data = confounding_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8163 -0.7597  0.0118  0.7846  2.5797 \n\nCoefficients:\n                 Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)     -1.503063   0.370590  -4.056              0.0001 ***\nice_cream_sales  0.028074   0.001205  23.305 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.088 on 98 degrees of freedom\nMultiple R-squared:  0.8471,    Adjusted R-squared:  0.8456 \nF-statistic: 543.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_full)\n\n\nCall:\nlm(formula = drownings ~ ice_cream_sales + temperature, data = confounding_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.85074 -0.61169  0.01186  0.60556  2.01776 \n\nCoefficients:\n                 Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)      1.243785   0.530123   2.346         0.021 *  \nice_cream_sales -0.002262   0.004839  -0.467         0.641    \ntemperature      0.317442   0.049515   6.411 0.00000000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9169 on 97 degrees of freedom\nMultiple R-squared:  0.8926,    Adjusted R-squared:  0.8904 \nF-statistic: 403.2 on 2 and 97 DF,  p-value: &lt; 0.00000000000000022\n\n# Create visualization\nggplot(confounding_data, aes(x = ice_cream_sales, y = drownings, color = temperature)) +\n  geom_point(alpha = 0.8) +\n  scale_color_viridis_c(name = \"Temperature (°C)\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"The Ice Cream and Drownings Correlation\",\n    subtitle = \"Temperature as a confounding variable\",\n    x = \"Ice Cream Sales\",\n    y = \"Drowning Incidents\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe naive model shows a statistically significant relationship between ice cream sales and drownings. However, once temperature is included in the model, the coefficient for ice cream sales decreases substantially and becomes statistically insignificant. This demonstrates how failing to account for confounding variables can lead to spurious correlations.\n\n\n\nReverse Causality\nReverse causality occurs when the assumed direction of causation is incorrect. Consider this example of anxiety and relaxation techniques:\n\n# Create sample data\nn &lt;- 200\nanxiety_level &lt;- runif(n, 1, 10)  # Anxiety level (1-10)\n\n# People with higher anxiety tend to use more relaxation techniques\nrelaxation_techniques &lt;- 1 + 0.7 * anxiety_level + rnorm(n, 0, 1)\n\nreverse_data &lt;- data.frame(\n  anxiety = anxiety_level,\n  relaxation = relaxation_techniques\n)\n\n# Fit models in both directions\nmodel_incorrect &lt;- lm(anxiety ~ relaxation, data = reverse_data)\nmodel_correct &lt;- lm(relaxation ~ anxiety, data = reverse_data)\n\n# Show regression results\nsummary(model_incorrect)\n\n\nCall:\nlm(formula = anxiety ~ relaxation, data = reverse_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9651 -0.7285 -0.0923  0.7247  3.7996 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) -0.09482    0.21973  -0.432               0.667    \nrelaxation   1.15419    0.04105  28.114 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.182 on 198 degrees of freedom\nMultiple R-squared:  0.7997,    Adjusted R-squared:  0.7987 \nF-statistic: 790.4 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_correct)\n\n\nCall:\nlm(formula = relaxation ~ anxiety, data = reverse_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.15178 -0.51571 -0.00222  0.55513  2.04334 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  1.05726    0.15286   6.917      0.0000000000624 ***\nanxiety      0.69284    0.02464  28.114 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9161 on 198 degrees of freedom\nMultiple R-squared:  0.7997,    Adjusted R-squared:  0.7987 \nF-statistic: 790.4 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(reverse_data, aes(x = relaxation, y = anxiety)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Anxiety and Relaxation Techniques\",\n    subtitle = \"Example of reverse causality\",\n    x = \"Use of Relaxation Techniques (frequency/week)\",\n    y = \"Anxiety Level (1-10 scale)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nBoth regression models show statistically significant relationships, but they imply different causal mechanisms. The incorrect model suggests that relaxation techniques increase anxiety, while the correct model reflects the true data generating process: anxiety drives the use of relaxation techniques.\n\n\nCollider Bias (Selection Bias)\nCollider bias occurs when conditioning on a variable that is affected by both the independent and dependent variables of interest, creating an artificial relationship between variables that are actually independent.\n\n# Create sample data\nn &lt;- 1000\n\n# Generate two independent variables (no relationship between them)\nintelligence &lt;- rnorm(n, 100, 15)  # IQ score\nfamily_wealth &lt;- rnorm(n, 50, 15)  # Wealth score (independent from intelligence)\n  \n# True data-generating process: admission depends on both intelligence and wealth\nadmission_score &lt;- 0.4 * intelligence + 0.4 * family_wealth + rnorm(n, 0, 10)\nadmitted &lt;- admission_score &gt; median(admission_score)  # Binary admission variable\n\n# Create full dataset\nfull_data &lt;- data.frame(\n  intelligence = intelligence,\n  wealth = family_wealth,\n  admission_score = admission_score,\n  admitted = admitted\n)\n\n# Regression in full population (true model)\nfull_model &lt;- lm(intelligence ~ wealth, data = full_data)\nsummary(full_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth, data = full_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-49.608 -10.115   0.119  10.832  55.581 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 101.42330    1.73139   58.58 &lt;0.0000000000000002 ***\nwealth       -0.02701    0.03334   -0.81               0.418    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.41 on 998 degrees of freedom\nMultiple R-squared:  0.0006569, Adjusted R-squared:  -0.0003444 \nF-statistic: 0.656 on 1 and 998 DF,  p-value: 0.4182\n\n# Get just the admitted students\nadmitted_only &lt;- full_data[full_data$admitted, ]\n\n# Regression in admitted students (conditioning on the collider)\nadmitted_model &lt;- lm(intelligence ~ wealth, data = admitted_only)\nsummary(admitted_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth, data = admitted_only)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.511  -9.064   0.721   8.965  48.267 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 115.4750     2.6165  44.133 &lt; 0.0000000000000002 ***\nwealth       -0.1704     0.0462  -3.689              0.00025 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.91 on 498 degrees of freedom\nMultiple R-squared:  0.0266,    Adjusted R-squared:  0.02464 \nF-statistic: 13.61 on 1 and 498 DF,  p-value: 0.0002501\n\n# Additional analysis - regression with the collider as a control variable\n# This demonstrates how controlling for a collider introduces bias\ncollider_control_model &lt;- lm(intelligence ~ wealth + admitted, data = full_data)\nsummary(collider_control_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth + admitted, data = full_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.729  -8.871   0.700   8.974  48.044 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  102.90069    1.56858  65.601 &lt; 0.0000000000000002 ***\nwealth        -0.19813    0.03224  -6.145        0.00000000116 ***\nadmittedTRUE  14.09944    0.94256  14.959 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.93 on 997 degrees of freedom\nMultiple R-squared:  0.1838,    Adjusted R-squared:  0.1822 \nF-statistic: 112.3 on 2 and 997 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Plot for full population\np1 &lt;- ggplot(full_data, aes(x = wealth, y = intelligence)) +\n  geom_point(alpha = 0.3, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Full Population\",\n    subtitle = paste(\"Correlation:\", round(cor(full_data$intelligence, full_data$wealth), 3)),\n    x = \"Family Wealth Score\",\n    y = \"Intelligence Score\"\n  ) +\n  theme_minimal()\n\n# Plot for admitted students\np2 &lt;- ggplot(admitted_only, aes(x = wealth, y = intelligence)) +\n  geom_point(alpha = 0.3, color = \"darkgreen\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Admitted Students Only\",\n    subtitle = paste(\"Correlation:\", round(cor(admitted_only$intelligence, admitted_only$wealth), 3)),\n    x = \"Family Wealth Score\",\n    y = \"Intelligence Score\"\n  ) +\n  theme_minimal()\n\n# Display plots side by side\nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis example demonstrates collider bias in three ways:\n\nIn the full population, intelligence and wealth have no relationship (coefficient near zero, p-value = 0.87)\nAmong admitted students (conditioning on the collider), a significant negative relationship appears (coefficient = -0.39, p-value &lt; 0.001)\nWhen controlling for admission status in a regression, a spurious relationship is introduced (coefficient = -0.16, p-value &lt; 0.001)\n\nThe collider bias creates relationships between variables that are truly independent. This can be represented in a directed acyclic graph (DAG):\n\\text{Intelligence} \\rightarrow \\text{Admission} \\leftarrow \\text{Wealth}\nWhen we condition on admission (the collider), we create a spurious association between intelligence and wealth.\n\n\nImproper Analysis\nInappropriate statistical methods can produce spurious correlations. Common issues include using linear models for non-linear relationships, ignoring data clustering, or mishandling time series data.\n\n# Generate data with a true non-linear relationship\nn &lt;- 100\nx &lt;- seq(-3, 3, length.out = n)\ny &lt;- x^2 + rnorm(n, 0, 1)  # Quadratic relationship\n\nimproper_data &lt;- data.frame(x = x, y = y)\n\n# Fit incorrect linear model\nwrong_model &lt;- lm(y ~ x, data = improper_data)\n\n# Fit correct quadratic model\ncorrect_model &lt;- lm(y ~ x + I(x^2), data = improper_data)\n\n# Show results\nsummary(wrong_model)\n\n\nCall:\nlm(formula = y ~ x, data = improper_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2176 -2.1477 -0.6468  2.4365  7.3457 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  3.14689    0.28951  10.870 &lt;0.0000000000000002 ***\nx            0.08123    0.16548   0.491               0.625    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.895 on 98 degrees of freedom\nMultiple R-squared:  0.002453,  Adjusted R-squared:  -0.007726 \nF-statistic: 0.2409 on 1 and 98 DF,  p-value: 0.6246\n\nsummary(correct_model)\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = improper_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81022 -0.65587  0.01935  0.61168  2.68894 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  0.12407    0.14498   0.856               0.394    \nx            0.08123    0.05524   1.470               0.145    \nI(x^2)       0.98766    0.03531  27.972 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9664 on 97 degrees of freedom\nMultiple R-squared:   0.89, Adjusted R-squared:  0.8877 \nF-statistic: 392.3 on 2 and 97 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(improper_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), color = \"green\", se = FALSE) +\n  labs(\n    title = \"Improper Analysis Example\",\n    subtitle = \"Linear model (red) vs. Quadratic model (green)\",\n    x = \"Variable X\",\n    y = \"Variable Y\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe linear model incorrectly suggests no relationship between x and y (coefficient near zero, p-value = 0.847), while the quadratic model reveals the true relationship (R^2 = 0.90). This demonstrates how model misspecification can create spurious non-correlations, masking real relationships that exist in different forms.\n\n\nEndogeneity and Its Sources\nEndogeneity occurs when an explanatory variable is correlated with the error term in a regression model. This violates the exogeneity assumption of OLS regression and leads to biased estimates. There are several sources of endogeneity:\n\nOmitted Variable Bias\nAs shown in the education-income example, when important variables are omitted from the model, their effects are absorbed into the error term, which becomes correlated with included variables.\n\n\nMeasurement Error\nWhen variables are measured with error, the observed values differ from true values, creating correlation between the error term and the predictors.\n\n\nSimultaneity (Bidirectional Causality)\nWhen the dependent variable also affects the independent variable, creating a feedback loop. Let’s demonstrate this:\n\n# Create sample data with mutual influence\nn &lt;- 100\n\n# Initialize variables\neconomic_growth &lt;- rnorm(n, 2, 1)\nemployment_rate &lt;- rnorm(n, 60, 5)\n\n# Create mutual influence through iterations\nfor(i in 1:3) {\n  economic_growth &lt;- 2 + 0.05 * employment_rate + rnorm(n, 0, 0.5)\n  employment_rate &lt;- 50 + 5 * economic_growth + rnorm(n, 0, 2)\n}\n\nsimultaneity_data &lt;- data.frame(\n  growth = economic_growth,\n  employment = employment_rate\n)\n\n# Model estimating effect of growth on employment\nmodel_growth_on_emp &lt;- lm(employment ~ growth, data = simultaneity_data)\n\n# Model estimating effect of employment on growth\nmodel_emp_on_growth &lt;- lm(growth ~ employment, data = simultaneity_data)\n\n# Show results\nsummary(model_growth_on_emp)\n\n\nCall:\nlm(formula = employment ~ growth, data = simultaneity_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.603 -1.500 -0.099  1.387  5.673 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  49.9665     2.0717   24.12 &lt;0.0000000000000002 ***\ngrowth        5.0151     0.3528   14.22 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.045 on 98 degrees of freedom\nMultiple R-squared:  0.6734,    Adjusted R-squared:  0.6701 \nF-statistic: 202.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_emp_on_growth)\n\n\nCall:\nlm(formula = growth ~ employment, data = simultaneity_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11417 -0.20626 -0.02185  0.22646  0.72941 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) -4.801257   0.749557  -6.405        0.00000000523 ***\nemployment   0.134283   0.009446  14.216 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3346 on 98 degrees of freedom\nMultiple R-squared:  0.6734,    Adjusted R-squared:  0.6701 \nF-statistic: 202.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(simultaneity_data, aes(x = growth, y = employment)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Simultaneity Between Economic Growth and Employment\",\n    x = \"Economic Growth (%)\",\n    y = \"Employment Rate (%)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe true data generating process is a system of simultaneous equations:\n\\text{Growth}_i = \\alpha_0 + \\alpha_1 \\text{Employment}_i + u_i \\text{Employment}_i = \\beta_0 + \\beta_1 \\text{Growth}_i + v_i\nStandard OLS regression cannot consistently estimate either equation because each explanatory variable is correlated with the error term in its respective equation.\n\n\nSelection Bias\nWhen the sample is not randomly selected from the population, the selection process can introduce correlation between the error term and the predictors. The collider bias example demonstrates a form of selection bias.\nThe consequences of endogeneity include: - Biased coefficient estimates - Incorrect standard errors - Invalid hypothesis tests - Misleading causal interpretations\nAddressing endogeneity requires specialized methods such as instrumental variables, system estimation, panel data methods, or experimental designs.\n\n\n\n\n\n\nUnderstanding Endogeneity in Regression\n\n\n\nEndogeneity is a critical concept in statistical analysis that occurs when an explanatory variable in a regression model is correlated with the error term. This creates challenges for accurately understanding cause-and-effect relationships in research. Let’s examine the three main types of endogeneity and how they affect research outcomes.\n\nOmitted Variable Bias (OVB)\nOmitted Variable Bias occurs when an important variable that affects both the dependent and independent variables is left out of the analysis. This omission leads to incorrect conclusions about the relationship between the variables we’re studying.\nConsider a study examining the relationship between education and income:\nExample: Education and Income The observed relationship shows that more education correlates with higher income. However, an individual’s inherent abilities affect both their educational attainment and their earning potential. Without accounting for ability, we may overestimate education’s direct effect on income.\nThe statistical representation shows why this matters:\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2z_i + \\epsilon_i (Complete model)\ny_i = \\beta_0 + \\beta_1x_i + u_i (Incomplete model)\nWhen we omit an important variable, our estimates of the remaining relationships become biased and unreliable.\n\n\nSimultaneity\nSimultaneity occurs when two variables simultaneously influence each other, making it difficult to determine the direction of causation. This creates a feedback loop that complicates statistical analysis.\nCommon Examples of Simultaneity:\nAcademic Performance and Study Habits represent a clear case of simultaneity. Academic performance influences how much time students dedicate to studying, while study time affects academic performance. This two-way relationship makes it challenging to measure the isolated effect of either variable.\nMarket Dynamics provide another example. Prices influence demand, while demand influences prices. This concurrent relationship requires special analytical approaches to understand the true relationships.\n\n\nMeasurement Error\nMeasurement error occurs when we cannot accurately measure our variables of interest. This imprecision can significantly impact our analysis and conclusions.\nCommon Sources of Measurement Error:\nSelf-Reported Data presents a significant challenge. When participants report their own behaviors or characteristics, such as study time, the reported values often differ from actual values. This discrepancy affects our ability to measure true relationships.\nTechnical Limitations also contribute to measurement error through imprecise measuring tools, inconsistent measurement conditions, and recording or data entry errors.\n\n\nAddressing Endogeneity in Research\n\nIdentification Strategies\n\n# Example of controlling for omitted variables\nmodel_simple &lt;- lm(income ~ education, data = df)\nmodel_full &lt;- lm(income ~ education + ability + experience + region, data = df)\n\n# Compare coefficients\nsummary(model_simple)\nsummary(model_full)\n\n\nInclude Additional Variables: Collect data on potentially important omitted variables and include relevant control variables in your analysis. For example, including measures of ability when studying education’s effect on income.\nUse Panel Data: Collect data across multiple time periods to control for unobserved fixed characteristics and analyze changes over time.\nInstrumental Variables: Find variables that affect your independent variable but not your dependent variable to isolate the relationship of interest.\n\n\n\nImproving Measurement\n\nMultiple Measurements: Take several measurements of key variables, use averaging to reduce random error, and compare different measurement methods.\nBetter Data Collection: Use validated measurement instruments, implement quality control procedures, and document potential sources of error.\n\n\n\n\nBest Practices for Researchers\nResearch Design fundamentally shapes your ability to address endogeneity. Plan for potential endogeneity issues before collecting data, include measures for potentially important control variables, and consider using multiple measurement approaches.\nAnalysis should include testing for endogeneity when possible, using appropriate statistical methods for your specific situation, and documenting assumptions and limitations.\nReporting must clearly describe potential endogeneity concerns, explain how you addressed these issues, and discuss implications for your conclusions.\n\n\n\n\n\n\n\n\n\nFormal Derivation of OLS Estimators: A Complete Mathematical Treatment\n\n\n\n\nObjective and Setup\nWe seek to find the values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that minimize the sum of squared residuals:\nSSE = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nThis is an unconstrained optimization problem where we treat SSE as a function of two variables: SSE(\\hat{\\beta}_0, \\hat{\\beta}_1).\n\n\nMathematical Prerequisites\nChain Rule for Composite Functions: For f(g(x)), the derivative is: \\frac{d}{dx}[f(g(x))] = f'(g(x)) \\cdot g'(x)\nIn our context:\n\nOuter function: f(u) = u^2 with derivative f'(u) = 2u\nInner function: u = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\n\nFirst-Order Conditions: At a minimum, both partial derivatives equal zero: \\frac{\\partial SSE}{\\partial \\hat{\\beta}_0} = 0 \\quad \\text{and} \\quad \\frac{\\partial SSE}{\\partial \\hat{\\beta}_1} = 0\n\n\nDerivation of \\hat{\\beta}_0\nStep 1: Take the partial derivative with respect to \\hat{\\beta}_0:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_0} = \\frac{\\partial}{\\partial \\hat{\\beta}_0} \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2\nStep 2: Apply the chain rule to each term: = \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot \\frac{\\partial}{\\partial \\hat{\\beta}_0}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\n= \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot (-1)\n= -2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\nStep 3: Set equal to zero and solve: -2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\nDividing by -2 and expanding: \\sum_{i=1}^n y_i - n\\hat{\\beta}_0 - \\hat{\\beta}_1\\sum_{i=1}^n x_i = 0\nStep 4: Isolate \\hat{\\beta}_0: n\\hat{\\beta}_0 = \\sum_{i=1}^n y_i - \\hat{\\beta}_1\\sum_{i=1}^n x_i\n\\boxed{\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}}\nInterpretation: The intercept adjusts to ensure the regression line passes through the point of means (\\bar{x}, \\bar{y}).\n\n\nDerivation of \\hat{\\beta}_1\nStep 1: Take the partial derivative with respect to \\hat{\\beta}_1:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_1} = \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot (-x_i)\n= -2\\sum_{i=1}^n x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\nStep 2: Substitute the expression for \\hat{\\beta}_0: = -2\\sum_{i=1}^n x_i(y_i - (\\bar{y} - \\hat{\\beta}_1\\bar{x}) - \\hat{\\beta}_1x_i)\n= -2\\sum_{i=1}^n x_i((y_i - \\bar{y}) - \\hat{\\beta}_1(x_i - \\bar{x}))\nStep 3: Set equal to zero and expand: \\sum_{i=1}^n x_i(y_i - \\bar{y}) - \\hat{\\beta}_1\\sum_{i=1}^n x_i(x_i - \\bar{x}) = 0\nStep 4: Use the algebraic identity \\sum_{i=1}^n x_i(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}):\nThis identity holds because: \\sum_{i=1}^n x_i(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x} + \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) + \\bar{x}\\sum_{i=1}^n(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) + 0\nSimilarly, \\sum_{i=1}^n x_i(x_i - \\bar{x}) = \\sum_{i=1}^n (x_i - \\bar{x})^2.\nStep 5: Solve for \\hat{\\beta}_1: \\hat{\\beta}_1\\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\n\\boxed{\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}}\n\n\nVerification of Minimum (Second-Order Conditions)\nTo confirm we have found a minimum (not a maximum or saddle point), we examine the Hessian matrix of second partial derivatives:\nSecond partial derivatives:\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_0^2} = 2n &gt; 0\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_1^2} = 2\\sum_{i=1}^n x_i^2 &gt; 0\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_0 \\partial \\hat{\\beta}_1} = 2\\sum_{i=1}^n x_i\nHessian matrix: \\mathbf{H} = 2\\begin{bmatrix} n & \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i & \\sum_{i=1}^n x_i^2 \\end{bmatrix}\nPositive definiteness check:\n\nFirst leading principal minor: 2n &gt; 0 ✓\nSecond leading principal minor (determinant): \\det(\\mathbf{H}) = 4\\left(n\\sum_{i=1}^n x_i^2 - \\left(\\sum_{i=1}^n x_i\\right)^2\\right) = 4n\\sum_{i=1}^n(x_i - \\bar{x})^2 &gt; 0 ✓\n\nSince the Hessian is positive definite, we have confirmed a minimum.\n\n\nGeometric Interpretation\n\n# Visualizing the optimization surface\nlibrary(tidyverse)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Generate sample data\nset.seed(42)\nx &lt;- runif(20, 1, 8)\ny &lt;- 2 + 3*x + rnorm(20, 0, 1)\n\n# Create grid of beta values\nbeta0_seq &lt;- seq(0, 4, length.out = 50)\nbeta1_seq &lt;- seq(2, 4, length.out = 50)\ngrid &lt;- expand.grid(beta0 = beta0_seq, beta1 = beta1_seq)\n\n# Calculate SSE for each combination\ngrid$SSE &lt;- apply(grid, 1, function(params) {\n  sum((y - (params[1] + params[2]*x))^2)\n})\n\n# Create contour plot\nggplot(grid, aes(x = beta0, y = beta1, z = SSE)) +\n  geom_contour_filled(aes(fill = after_stat(level))) +\n  geom_point(x = coef(lm(y ~ x))[1], \n             y = coef(lm(y ~ x))[2], \n             color = \"red\", size = 3) +\n  labs(title = \"SSE Surface in Parameter Space\",\n       subtitle = \"Red point shows the OLS minimum\",\n       x = expression(hat(beta)[0]),\n       y = expression(hat(beta)[1])) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.1-understanding-correlation-measures-a-self-study-tutorial-using-stress-level-and-cognitive-performance-data",
    "href": "correg_en.html#appendix-a.1-understanding-correlation-measures-a-self-study-tutorial-using-stress-level-and-cognitive-performance-data",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.37 Appendix A.1: Understanding Correlation Measures: A Self-Study Tutorial (Using Stress Level and Cognitive Performance Data)",
    "text": "13.37 Appendix A.1: Understanding Correlation Measures: A Self-Study Tutorial (Using Stress Level and Cognitive Performance Data)\n\nDataset Overview\n\ndata &lt;- data.frame(\n  anxiety_level = c(8, 5, 11, 14, 7, 10),\n  cognitive_performance = c(85, 90, 62, 55, 80, 65)\n)\n\n\n\n1. Covariance Calculation\n\nStep 1: Calculate Means\n\n\n\n\n\n\n\n\nVariable\nCalculation\nResult\n\n\n\n\nMean Anxiety (\\bar{x})\n(8 + 5 + 11 + 14 + 7 + 10) ÷ 6\n9.17\n\n\nMean Cognitive (\\bar{y})\n(85 + 90 + 62 + 55 + 80 + 65) ÷ 6\n72.83\n\n\n\n\n\nStep 2: Calculate Deviations and Products\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\n(x_i - \\bar{x})\n(y_i - \\bar{y})\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n1\n8\n85\n-1.17\n12.17\n-14.24\n\n\n2\n5\n90\n-4.17\n17.17\n-71.60\n\n\n3\n11\n62\n1.83\n-10.83\n-19.82\n\n\n4\n14\n55\n4.83\n-17.83\n-86.12\n\n\n5\n7\n80\n-2.17\n7.17\n-15.56\n\n\n6\n10\n65\n0.83\n-7.83\n-6.50\n\n\nSum\n55\n437\n0.00\n0.00\n-213.84\n\n\n\n\n\nStep 3: Calculate Covariance\n \\text{Cov}(X,Y) = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n-1} = \\frac{-213.84}{5} = -42.77 \n\n\n\n2. Pearson Correlation Coefficient\n\nStep 1: Calculate Squared Deviations\n\n\n\n\n\n\n\n\n\n\ni\n(x_i - \\bar{x})\n(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n-1.17\n12.17\n1.37\n148.11\n\n\n2\n-4.17\n17.17\n17.39\n294.81\n\n\n3\n1.83\n-10.83\n3.35\n117.29\n\n\n4\n4.83\n-17.83\n23.33\n317.91\n\n\n5\n-2.17\n7.17\n4.71\n51.41\n\n\n6\n0.83\n-7.83\n0.69\n61.31\n\n\nSum\n0.00\n0.00\n50.84\n990.84\n\n\n\n\n\nStep 2: Calculate Standard Deviations\n\n\n\n\n\n\n\n\n\nMeasure\nFormula\nCalculation\nResult\n\n\n\n\ns_x\n\\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n-1}}\n\\sqrt{\\frac{50.84}{5}}\n3.19\n\n\ns_y\n\\sqrt{\\frac{\\sum (y_i - \\bar{y})^2}{n-1}}\n\\sqrt{\\frac{990.84}{5}}\n14.08\n\n\n\n\n\nStep 3: Calculate Pearson Correlation\n r = \\frac{\\text{Cov}(X,Y)}{s_x s_y} = \\frac{-42.77}{3.19 \\times 14.08} = -0.95 \n\n\n\n3. Spearman Rank Correlation\n\nStep 1: Assign Ranks\n\n\n\ni\nx_i\ny_i\nRank x_i\nRank y_i\nd_i\nd_i^2\n\n\n\n\n1\n8\n85\n3\n2\n1\n1\n\n\n2\n5\n90\n1\n1\n0\n0\n\n\n3\n11\n62\n5\n5\n0\n0\n\n\n4\n14\n55\n6\n6\n0\n0\n\n\n5\n7\n80\n2\n3\n-1\n1\n\n\n6\n10\n65\n4\n4\n0\n0\n\n\nSum\n\n\n\n\n\n2\n\n\n\n\n\nStep 2: Calculate Spearman Correlation\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} = 1 - \\frac{6(2)}{6(36-1)} = 1 - \\frac{12}{210} = 0.94 \n\n\n\nVerification using R\n\n# Calculate correlations using R\ncor(data$anxiety_level, data$cognitive_performance, method = \"pearson\")\n\n[1] -0.9527979\n\ncor(data$anxiety_level, data$cognitive_performance, method = \"spearman\")\n\n[1] -0.9428571\n\n\n\n\nInterpretation\n\nThe strong negative Pearson correlation (r = -0.95) indicates a very strong negative linear relationship between anxiety level and cognitive performance.\nThe strong positive Spearman correlation (ρ = 0.94) shows that the relationship is also strongly monotonic.\nThe difference between Pearson and Spearman correlations suggests that while there is a strong relationship, it might not be perfectly linear.\n\n\n\nExercise\n\nVerify each calculation step in the tables above.\nTry calculating these measures with a modified dataset:\n\nAdd one outlier and observe how it affects both correlation coefficients\nChange one pair of values and recalculate",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.2-calculating-covariance-pearson-spearman-correlation-and-ols---worked-example",
    "href": "correg_en.html#appendix-a.2-calculating-covariance-pearson-spearman-correlation-and-ols---worked-example",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.38 Appendix A.2: Calculating Covariance, Pearson, Spearman Correlation, and OLS - worked example",
    "text": "13.38 Appendix A.2: Calculating Covariance, Pearson, Spearman Correlation, and OLS - worked example\nA political science student is investigating the relationship between district magnitude (DM) and Gallagher’s disproportionality index (GH) in parliamentary elections across 10 randomly selected democracies.\nData on electoral district magnitudes (\\text{DM}) and Gallagher index:\n\n\n\n\\text{DM} (X)\nGallagher (Y)\n\n\n\n\n2\n18.2\n\n\n3\n16.7\n\n\n4\n15.8\n\n\n5\n15.3\n\n\n6\n15.0\n\n\n7\n14.8\n\n\n8\n14.7\n\n\n9\n14.6\n\n\n10\n14.55\n\n\n11\n14.52\n\n\n\n\nStep 1: Calculate Basic Statistics\nCalculation of means:\nFor \\text{DM} (X): \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\nDetailed calculation:\n2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 65 \\bar{x} = \\frac{65}{10} = 6.5\nFor Gallagher index (Y): \\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\nDetailed calculation:\n18.2 + 16.7 + 15.8 + 15.3 + 15.0 + 14.8 + 14.7 + 14.6 + 14.55 + 14.52 = 154.17 \\bar{y} = \\frac{154.17}{10} = 15.417\n\n\nStep 2: Detailed Covariance Calculations\nComplete working table showing all calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18.2\n-4.5\n2.783\n-12.5235\n20.25\n7.7451\n\n\n2\n3\n16.7\n-3.5\n1.283\n-4.4905\n12.25\n1.6461\n\n\n3\n4\n15.8\n-2.5\n0.383\n-0.9575\n6.25\n0.1467\n\n\n4\n5\n15.3\n-1.5\n-0.117\n0.1755\n2.25\n0.0137\n\n\n5\n6\n15.0\n-0.5\n-0.417\n0.2085\n0.25\n0.1739\n\n\n6\n7\n14.8\n0.5\n-0.617\n-0.3085\n0.25\n0.3807\n\n\n7\n8\n14.7\n1.5\n-0.717\n-1.0755\n2.25\n0.5141\n\n\n8\n9\n14.6\n2.5\n-0.817\n-2.0425\n6.25\n0.6675\n\n\n9\n10\n14.55\n3.5\n-0.867\n-3.0345\n12.25\n0.7517\n\n\n10\n11\n14.52\n4.5\n-0.897\n-4.0365\n20.25\n0.8047\n\n\nSum\n65\n154.17\n0\n0\n-28.085\n82.5\n12.8442\n\n\n\nCovariance calculation: \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\text{Cov}(X,Y) = \\frac{-28.085}{9} = -3.120556\n\n\nStep 3: Standard Deviation Calculations\nFor \\text{DM} (X): \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}}\n\\sigma_x = \\sqrt{\\frac{82.5}{9}} = \\sqrt{9.1667} = 3.026582\nFor Gallagher (Y): \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}}\n\\sigma_y = \\sqrt{\\frac{12.8442}{9}} = \\sqrt{1.4271} = 1.194612\n\n\nStep 4: Pearson Correlation Calculation\nr = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\nr = \\frac{-3.120556}{3.026582 \\times 1.194612} = \\frac{-3.120556}{3.615752} = -0.863044\n\n\nStep 5: Spearman Rank Correlation Calculation\nComplete ranking table with all calculations:\n\n\n\ni\nX_i\nY_i\nRank X_i\nRank Y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n18.2\n1\n10\n-9\n81\n\n\n2\n3\n16.7\n2\n9\n-7\n49\n\n\n3\n4\n15.8\n3\n8\n-5\n25\n\n\n4\n5\n15.3\n4\n7\n-3\n9\n\n\n5\n6\n15.0\n5\n6\n-1\n1\n\n\n6\n7\n14.8\n6\n5\n1\n1\n\n\n7\n8\n14.7\n7\n4\n3\n9\n\n\n8\n9\n14.6\n8\n3\n5\n25\n\n\n9\n10\n14.55\n9\n2\n7\n49\n\n\n10\n11\n14.52\n10\n1\n9\n81\n\n\nSum\n\n\n\n\n\n330\n\n\n\nSpearman correlation calculation: \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n\\rho = 1 - \\frac{6 \\times 330}{10(100 - 1)} = 1 - \\frac{1980}{990} = 1 - 2 = -1\n\n\nStep 6: R Verification\n\n# Create vectors\nDM &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nGH &lt;- c(18.2, 16.7, 15.8, 15.3, 15.0, 14.8, 14.7, 14.6, 14.55, 14.52)\n\n# Calculate covariance\ncov(DM, GH)\n\n[1] -3.120556\n\n# Calculate correlations\ncor(DM, GH, method = \"pearson\")\n\n[1] -0.8627742\n\ncor(DM, GH, method = \"spearman\")\n\n[1] -1\n\n\n\n\nStep 7: Basic Visualization\n\nlibrary(ggplot2)\n\n# Create data frame\ndata &lt;- data.frame(DM = DM, GH = GH)\n\n# Create scatter plot\nggplot(data, aes(x = DM, y = GH)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"District Magnitude vs Gallagher Index\",\n    x = \"District Magnitude (DM)\",\n    y = \"Gallagher Index (GH)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nOLS Estimation and Goodness-of-Fit Measures\n\n\nStep 1: Calculate OLS Estimates\nUsing previously calculated values:\n\n\\sum(X_i - \\bar{X})(Y_i - \\bar{Y}) = -28.085\n\\sum(X_i - \\bar{X})^2 = 82.5\n\\bar{X} = 6.5\n\\bar{Y} = 15.417\n\nCalculate slope (\\hat{\\beta_1}): \\hat{\\beta_1} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\hat{\\beta_1} = -28,085 ÷ 82,5 = -0,3404\nCalculate intercept (\\hat{\\beta_0}): \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 15,417 - (-0,3404 × 6,5)\n   = 15,417 + 2,2126\n   = 17,6296\nTherefore, the OLS regression equation is: \\hat{Y} = 17.6296 - 0.3404X\n\n\nStep 2: Calculate Fitted Values and Residuals\nComplete table showing all calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i\ne_i = Y_i - \\hat{Y}_i\ne_i^2\n(Y_i - \\bar{Y})^2\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18.2\n16.9488\n1.2512\n1.5655\n7.7451\n2.3404\n\n\n2\n3\n16.7\n16.6084\n0.0916\n0.0084\n1.6461\n1.4241\n\n\n3\n4\n15.8\n16.2680\n-0.4680\n0.2190\n0.1467\n0.7225\n\n\n4\n5\n15.3\n15.9276\n-0.6276\n0.3939\n0.0137\n0.2601\n\n\n5\n6\n15.0\n15.5872\n-0.5872\n0.3448\n0.1739\n0.0289\n\n\n6\n7\n14.8\n15.2468\n-0.4468\n0.1996\n0.3807\n0.0290\n\n\n7\n8\n14.7\n14.9064\n-0.2064\n0.0426\n0.5141\n0.2610\n\n\n8\n9\n14.6\n14.5660\n0.0340\n0.0012\n0.6675\n0.7241\n\n\n9\n10\n14.55\n14.2256\n0.3244\n0.1052\n0.7517\n1.4184\n\n\n10\n11\n14.52\n13.8852\n0.6348\n0.4030\n0.8047\n2.3439\n\n\nSum\n65\n154.17\n154.17\n0\n3.2832\n12.8442\n9.5524\n\n\n\nCalculations for fitted values:\nFor X = 2:\nŶ = 17.6296 + (-0.3404 × 2) = 16.9488\n\nFor X = 3:\nŶ = 17.6296 + (-0.3404 × 3) = 16.6084\n\n[... continue for all values]\n\n\nStep 3: Calculate Goodness-of-Fit Measures\nSum of Squared Errors (SSE): SSE = \\sum e_i^2\nSSE = 3.2832\nSum of Squared Total (SST): SST = \\sum(Y_i - \\bar{Y})^2\nSST = 12.8442\nSum of Squared Regression (SSR): SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\nSSR = 9.5524\nVerify decomposition: SST = SSR + SSE\n12.8442 = 9.5524 + 3.2832 (within rounding error)\nR-squared calculation: R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nR² = 9.5524 ÷ 12.8442\n   = 0.7438\n\n\nStep 4: R Verification\n\n# Fit linear model\nmodel &lt;- lm(GH ~ DM, data = data)\n\n# View summary statistics\nsummary(model)\n\n\nCall:\nlm(formula = GH ~ DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62764 -0.46274 -0.08615  0.26624  1.25109 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept) 17.62976    0.50121  35.174 0.000000000467 ***\nDM          -0.34042    0.07053  -4.827        0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6406 on 8 degrees of freedom\nMultiple R-squared:  0.7444,    Adjusted R-squared:  0.7124 \nF-statistic:  23.3 on 1 and 8 DF,  p-value: 0.00131\n\n# Calculate R-squared manually\nSST &lt;- sum((GH - mean(GH))^2)\nSSE &lt;- sum(residuals(model)^2)\nSSR &lt;- SST - SSE\nR2_manual &lt;- SSR/SST\nR2_manual\n\n[1] 0.7443793\n\n\n\n\nStep 5: Residual Analysis\n\n# Create residual plots\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\nStep 6: Predicted vs Actual Values Plot\n\n# Create predicted vs actual plot\nggplot(data.frame(\n  Actual = GH,\n  Predicted = fitted(model)\n), aes(x = Predicted, y = Actual)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Predicted vs Actual Values\",\n    x = \"Predicted Gallagher Index\",\n    y = \"Actual Gallagher Index\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nLog-Transformed Models\n\n\nStep 1: Data Transformation\nFirst, calculate natural logarithms of variables:\n\n\n\ni\nX_i\nY_i\n\\ln(X_i)\n\\ln(Y_i)\n\n\n\n\n1\n2\n18.2\n0.6931\n2.9014\n\n\n2\n3\n16.7\n1.0986\n2.8154\n\n\n3\n4\n15.8\n1.3863\n2.7600\n\n\n4\n5\n15.3\n1.6094\n2.7278\n\n\n5\n6\n15.0\n1.7918\n2.7081\n\n\n6\n7\n14.8\n1.9459\n2.6946\n\n\n7\n8\n14.7\n2.0794\n2.6878\n\n\n8\n9\n14.6\n2.1972\n2.6810\n\n\n9\n10\n14.55\n2.3026\n2.6777\n\n\n10\n11\n14.52\n2.3979\n2.6757\n\n\n\n\n\nStep 2: Compare Different Model Specifications\nWe estimate three alternative specifications:\n\nLog-linear model: \\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nLinear-log model: Y_i = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\nLog-log model: \\ln(Y_i) = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\n\n\n# Create transformed variables\ndata$log_DM &lt;- log(data$DM)\ndata$log_GH &lt;- log(data$GH)\n\n# Fit models\nmodel_linear &lt;- lm(GH ~ DM, data = data)\nmodel_loglinear &lt;- lm(log_GH ~ DM, data = data)\nmodel_linearlog &lt;- lm(GH ~ log_DM, data = data)\nmodel_loglog &lt;- lm(log_GH ~ log_DM, data = data)\n\n# Compare R-squared values\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Linear\", \"Log-linear\", \"Linear-log\", \"Log-log\"),\n  R_squared = c(\n    summary(model_linear)$r.squared,\n    summary(model_loglinear)$r.squared,\n    summary(model_linearlog)$r.squared,\n    summary(model_loglog)$r.squared\n  )\n)\n\n# Display comparison\nmodels_comparison\n\n       Model R_squared\n1     Linear 0.7443793\n2 Log-linear 0.7670346\n3 Linear-log 0.9141560\n4    Log-log 0.9288088\n\n\n\n\nStep 3: Visual Comparison\n\n# Create plots for each model\np1 &lt;- ggplot(data, aes(x = DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Linear Model\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Log-linear Model\") +\n  theme_minimal()\n\np3 &lt;- ggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Linear-log Model\") +\n  theme_minimal()\n\np4 &lt;- ggplot(data, aes(x = log_DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Log-log Model\") +\n  theme_minimal()\n\n# Arrange plots in a grid\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Residual Analysis for Best Model\nBased on R-squared values, analyze residuals for the best-fitting model:\n\n# Residual plots for best model\npar(mfrow = c(2, 2))\nplot(model_linearlog)\n\n\n\n\n\n\n\n\n\n\nStep 5: Interpretation of Best Model\nThe linear-log model coefficients:\n\nsummary(model_linearlog)\n\n\nCall:\nlm(formula = GH ~ log_DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40702 -0.30207 -0.04907  0.22905  0.60549 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)  19.0223     0.4079   46.64 0.0000000000494 ***\nlog_DM       -2.0599     0.2232   -9.23 0.0000153880425 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3712 on 8 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9034 \nF-statistic: 85.19 on 1 and 8 DF,  p-value: 0.00001539\n\n\nInterpretation: - \\hat{\\beta_0} represents the expected Gallagher Index when ln(DM) = 0 (i.e., when DM = 1) - \\hat{\\beta_1} represents the change in Gallagher Index associated with a one-unit increase in ln(DM)\n\n\nStep 6: Model Predictions\n\n# Create prediction plot for best model\nggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Linear-log Model: Gallagher Index vs ln(District Magnitude)\",\n    x = \"ln(District Magnitude)\",\n    y = \"Gallagher Index\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nStep 7: Elasticity Analysis\nFor the log-log model, coefficients represent elasticities directly. Calculate average elasticity for the linear-log model:\n\n# Calculate elasticity at means\nmean_DM &lt;- mean(data$DM)\nmean_GH &lt;- mean(data$GH)\nbeta1 &lt;- coef(model_linearlog)[2]\nelasticity &lt;- beta1 * (1/mean_GH)\nelasticity\n\n    log_DM \n-0.1336136 \n\n\nThis represents the percentage change in the Gallagher Index for a 1% change in District Magnitude.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.3-understanding-pearson-spearman-and-kendall",
    "href": "correg_en.html#appendix-a.3-understanding-pearson-spearman-and-kendall",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.39 Appendix A.3: Understanding Pearson, Spearman, and Kendall",
    "text": "13.39 Appendix A.3: Understanding Pearson, Spearman, and Kendall\n\nDataset\n\ndata &lt;- data.frame(\n  x = c(2, 4, 5, 3, 8),\n  y = c(3, 5, 4, 4, 7)\n)\n\n\n\nPearson Correlation\n r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}} \n\nStep-by-Step Calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n2\n3\n-2.4\n-1.6\n3.84\n5.76\n2.56\n\n\n2\n4\n5\n-0.4\n0.4\n-0.16\n0.16\n0.16\n\n\n3\n5\n4\n0.6\n-0.6\n-0.36\n0.36\n0.36\n\n\n4\n3\n4\n-1.4\n-0.6\n0.84\n1.96\n0.36\n\n\n5\n8\n7\n3.6\n2.4\n8.64\n12.96\n5.76\n\n\nSum\n22\n23\n0\n0\n12.8\n21.2\n9.2\n\n\n\n\\bar{x} = 4.4 \\bar{y} = 4.6\n r = \\frac{12.8}{\\sqrt{21.2 \\times 9.2}} = \\frac{12.8}{\\sqrt{195.04}} = \\frac{12.8}{13.97} = 0.92 \n\n\n\nSpearman Correlation\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} \n\nStep-by-Step Calculations:\n\n\n\ni\nx_i\ny_i\nRank x_i\nRank y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n3\n1\n1\n0\n0\n\n\n2\n4\n5\n3\n5\n-2\n4\n\n\n3\n5\n4\n4\n2.5\n1.5\n2.25\n\n\n4\n3\n4\n2\n2.5\n-0.5\n0.25\n\n\n5\n8\n7\n5\n4\n1\n1\n\n\nSum\n\n\n\n\n\n7.5\n\n\n\n \\rho = 1 - \\frac{6(7.5)}{5(25-1)} = 1 - \\frac{45}{120} = 0.82 \n\n\n\nKendall’s Tau\n \\tau = \\frac{\\text{number of concordant pairs} - \\text{number of discordant pairs}}{\\frac{1}{2}n(n-1)} \n\nStep-by-Step Calculations:\n\n\n\nPair (i,j)\nx_i,x_j\ny_i,y_j\nx_j-x_i\ny_j-y_i\nResult\n\n\n\n\n(1,2)\n2,4\n3,5\n+2\n+2\nC\n\n\n(1,3)\n2,5\n3,4\n+3\n+1\nC\n\n\n(1,4)\n2,3\n3,4\n+1\n+1\nC\n\n\n(1,5)\n2,8\n3,7\n+6\n+4\nC\n\n\n(2,3)\n4,5\n5,4\n+1\n-1\nD\n\n\n(2,4)\n4,3\n5,4\n-1\n-1\nC\n\n\n(2,5)\n4,8\n5,7\n+4\n+2\nC\n\n\n(3,4)\n5,3\n4,4\n-2\n0\nD\n\n\n(3,5)\n5,8\n4,7\n+3\n+3\nC\n\n\n(4,5)\n3,8\n4,7\n+5\n+3\nC\n\n\n\nNumber of concordant pairs = 8 Number of discordant pairs = 2  \\tau = \\frac{8-2}{10} = 0.74 \n\n\n\nVerification in R\n\ncat(\"Pearson:\", round(cor(data$x, data$y, method=\"pearson\"), 2), \"\\n\")\n\nPearson: 0.92 \n\ncat(\"Spearman:\", round(cor(data$x, data$y, method=\"spearman\"), 2), \"\\n\")\n\nSpearman: 0.82 \n\ncat(\"Kendall:\", round(cor(data$x, data$y, method=\"kendall\"), 2), \"\\n\")\n\nKendall: 0.74 \n\n\n\n\nInterpretation of Results\n\nPearson Correlation (r = 0.92)\n\nStrong positive linear correlation\nIndicates a very strong linear relationship between variables\n\nSpearman Correlation (ρ = 0.82)\n\nAlso strong positive correlation\nSlightly lower than Pearson’s, suggesting some deviations from monotonicity\n\nKendall’s Tau (τ = 0.74)\n\nLowest of the three values, but still indicates strong association\nMore robust to outliers\n\n\n\n\nComparison of Measures\n\nDifferences in Values:\n\nPearson (0.92) - highest value, strong linearity\nSpearman (0.82) - considers only ranking\nKendall (0.74) - most conservative measure\n\nPractical Application:\n\nAll measures confirm strong positive association\nDifferences between measures indicate slight deviations from perfect linearity\nKendall provides the most conservative estimate of relationship strength\n\n\n\n\nExercises\n\nChange y[3] from 4 to 6 and recalculate all three correlations\nAdd an outlier (x=10, y=2) and recalculate correlations\nCompare which measure is most sensitive to changes in the data\n\n\n\nKey Points to Remember\n\nPearson Correlation:\n\nMeasures linear relationship\nMost sensitive to outliers\nRequires interval or ratio data\n\nSpearman Correlation:\n\nMeasures monotonic relationship\nLess sensitive to outliers\nWorks with ordinal data\n\nKendall’s Tau:\n\nMeasures ordinal association\nMost robust to outliers\nBest for small samples and tied ranks",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-b-bias-in-ols-estimation-with-endogenous-regressors",
    "href": "correg_en.html#appendix-b-bias-in-ols-estimation-with-endogenous-regressors",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.40 Appendix B: Bias in OLS Estimation with Endogenous Regressors",
    "text": "13.40 Appendix B: Bias in OLS Estimation with Endogenous Regressors\nIn this tutorial, we will explore the bias in Ordinary Least Squares (OLS) estimation when the error term is correlated with the explanatory variable, a situation known as endogeneity. We will first derive the bias mathematically and then illustrate it using a simulated dataset in R.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#theoretical-derivation",
    "href": "correg_en.html#theoretical-derivation",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.41 Theoretical Derivation",
    "text": "13.41 Theoretical Derivation\nConsider a data generating process (DGP) where the true relationship between x and y is:\n y = 2x + e \nHowever, there is an endogeneity problem because the error term e is correlated with x in the following way:\n e = 1x + u \nwhere u is an independent error term.\nIf we estimate the simple linear model y = \\hat{\\beta_0} + \\hat{\\beta_1}x + \\varepsilon using OLS, the OLS estimator of \\hat{\\beta_1} will be biased due to the endogeneity issue.\nTo understand the bias, let’s derive the expected value of the OLS estimator \\hat{\\beta}_1:\n\\begin{align*}\nE[\\hat{\\beta}_1] &= E[(X'X)^{-1}X'y] \\\\\n                 &= E[(X'X)^{-1}X'(2x + 1x + u)] \\\\\n                 &= E[(X'X)^{-1}X'(3x + u)] \\\\\n                 &= 3 + E[(X'X)^{-1}X'u]\n\\end{align*}\nIf the error term u is uncorrelated with x, then E[(X'X)^{-1}X'u] = 0, and the OLS estimator would be unbiased: E[\\hat{\\beta}_1] = 3. However, in this case, the original error term e is correlated with x, so u is also likely to be correlated with x.\nAssuming E[(X'X)^{-1}X'u] \\neq 0, the OLS estimator will be biased:\n\\begin{align*}\n\\text{Bias}(\\hat{\\beta}_1) &= E[\\hat{\\beta}_1] - \\beta_{1,\\text{true}} \\\\\n                           &= 3 + E[(X'X)^{-1}X'u] - 2 \\\\\n                           &= 1 + E[(X'X)^{-1}X'u]\n\\end{align*}\nThe direction and magnitude of the bias will depend on the correlation between x and u. If x and u are positively correlated, the bias will be positive, and the OLS estimator will overestimate the true coefficient. Conversely, if x and u are negatively correlated, the bias will be negative, and the OLS estimator will underestimate the true coefficient.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#simulation-in-r",
    "href": "correg_en.html#simulation-in-r",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.42 Simulation in R",
    "text": "13.42 Simulation in R\nLet’s create a simple dataset with 10 observations where x is in the interval 1:10, and generate y values based on the given DGP: y = 2x + e, where e = 1x + u, and u is a random error term.\n\nset.seed(123)  # for reproducibility\nx &lt;- 1:10\nu &lt;- rnorm(10, mean = 0, sd = 1)\ne &lt;- 1*x + u\n# e &lt;- 1*x\ny &lt;- 2*x + e\n\n# Generate the data frame\ndata &lt;- data.frame(x = x, y = y)\n\n# Estimate the OLS model\nmodel &lt;- lm(y ~ x, data = data)\n\n# Print the model summary\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1348 -0.5624 -0.1393  0.3854  1.6814 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)   0.5255     0.6673   0.787         0.454    \nx             2.9180     0.1075  27.134 0.00000000367 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9768 on 8 degrees of freedom\nMultiple R-squared:  0.9893,    Adjusted R-squared:  0.9879 \nF-statistic: 736.3 on 1 and 8 DF,  p-value: 0.000000003666\n\n\nIn this example, the true relationship is y = 2x + e, where e = 1x + u. However, when we estimate the OLS model, we get:\n \\hat{y} = 0.18376 + 3.05874x \nThe estimated coefficient for x is 3.05874, which is biased upward from the true value of 2. This bias is due to the correlation between the error term e and the explanatory variable x.\nTo visualize the bias using ggplot2, we can plot the true relationship (y = 2x) and the estimated OLS relationship:\n\nlibrary(ggplot2)\n\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 2, color = \"blue\", linewidth = 1, linetype = \"dashed\") +\n  geom_abline(intercept = coef(model)[1], slope = coef(model)[2], color = \"red\", linewidth = 1) +\n  labs(title = \"True vs. Estimated Relationship\", x = \"x\", y = \"y\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  scale_color_manual(name = \"Lines\", values = c(\"blue\", \"red\"), \n                     labels = c(\"True\", \"OLS\"))\n\n\n\n\n\n\n\n\nThe plot will show that the estimated OLS line (red) is steeper than the true relationship line (blue), illustrating the upward bias in the estimated coefficient.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#conclusion-1",
    "href": "correg_en.html#conclusion-1",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.43 Conclusion",
    "text": "13.43 Conclusion\nIn summary, when the error term is correlated with the explanatory variable (endogeneity), the OLS estimator will be biased. The direction and magnitude of the bias depend on the nature of the correlation between the error term and the explanatory variable. This tutorial demonstrated the bias both mathematically and through a simulated example in R, using ggplot2 for visualization.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-c.-worked-examples",
    "href": "correg_en.html#appendix-c.-worked-examples",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.44 Appendix C. Worked Examples",
    "text": "13.44 Appendix C. Worked Examples",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "href": "correg_en.html#descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.45 Descriptive Statistics and OLS Example - Income and Voter Turnout",
    "text": "13.45 Descriptive Statistics and OLS Example - Income and Voter Turnout\nBackground\nIn preparation for the 2024 municipal elections, the Amsterdam Electoral Commission conducted research on voter participation patterns across different city neighborhoods. A key question emerged:\nDoes economic prosperity of a neighborhood correlate with civic engagement, specifically voter turnout?\nData Collection\nSample: 5 representative neighborhoods in Amsterdam\nTime Period: Data from the 2022 municipal elections\nVariables:\n\nIncome: Average annual household income per capita (thousands €)\nTurnout: Percentage of registered voters who voted in the election\n\n\nInitial R Output for Reference\n\n# Data\nincome &lt;- c(50, 45, 56, 40, 60)  # thousands €\nturnout &lt;- c(60, 56, 70, 50, 75) # %\n\n# Full model check\nmodel &lt;- lm(turnout ~ income)\nsummary(model)\n\n\nCall:\nlm(formula = turnout ~ income)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \nincome       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n\n\n\nDispersion Measures\nMeans:\n\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n} = \\frac{50 + 45 + 56 + 40 + 60}{5} = \\frac{251}{5} = 50.2\n\\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n} = \\frac{60 + 56 + 70 + 50 + 75}{5} = \\frac{311}{5} = 62.2\n\n# Verification\nmean(income)  # 50.2\n\n[1] 50.2\n\nmean(turnout) # 62.2\n\n[1] 62.2\n\n\nVariances:\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X: (-0.2, -5.2, 5.8, -10.2, 9.8)\ns^2_X = \\frac{0.04 + 27.04 + 33.64 + 104.04 + 96.04}{4} = \\frac{260.8}{4} = 65.2\nDeviations for Y: (-2.2, -6.2, 7.8, -12.2, 12.8)\ns^2_Y = \\frac{4.84 + 38.44 + 60.84 + 148.84 + 163.84}{4} = \\frac{416.8}{4} = 104.2\n\n# Verification\nvar(income)  # 65.2\n\n[1] 65.2\n\nvar(turnout) # 104.2\n\n[1] 104.2\n\n\n\n\nCovariance and Correlation\nCovariance:\ns_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-0.2 \\times -2.2) = 0.44 (-5.2 \\times -6.2) = 32.24 (5.8 \\times 7.8) = 45.24 (-10.2 \\times -12.2) = 124.44 (9.8 \\times 12.8) = 125.44\ns_{XY} = \\frac{327.8}{4} = 81.95\n\n# Verification\ncov(income, turnout) # 81.95\n\n[1] 81.95\n\n\nCorrelation:\nr_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{81.95}{\\sqrt{65.2}\\sqrt{104.2}} = 0.994\n\n# Verification\ncor(income, turnout) # 0.994\n\n[1] 0.9942402\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{81.95}{65.2} = 1.2571429\nIntercept:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\nStep by step:\n\n1.2571429 \\times 50.2 = 63.1085714\n\\hat{\\beta_0} = 62.2 - 63.1085714 = -0.9085714\n\n\n# Verification\ncoef(model)  # Exact coefficients from R\n\n(Intercept)      income \n -0.8964724   1.2569018 \n\n\n\n\nDetailed Decomposition of Variance and R-squared\nStep 1: Calculate predicted values (\\hat{Y}):\n \\hat{Y} = -0.9085714 + 1.2571429X\nThe predicted values \\hat{Y} for each X value:\nFor X = 50:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (50)\n \\hat{Y} = -0.9085714 + 62.857145 \\hat{Y} = 61.9485736\nFor X = 45:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (45)\n \\hat{Y} = -0.9085714 + 56.5714305 \\hat{Y} = 55.6535591\nFor X = 56:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (56)\n \\hat{Y} = -0.9085714 + 70.4200024 \\hat{Y} = 69.5114310\nFor X = 40:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (40)\n \\hat{Y} = -0.9085714 + 50.2657160 \\hat{Y} = 49.3571446\nFor X = 60:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (60)\n \\hat{Y} = -0.9085714 + 75.4285740 \\hat{Y} = 74.5200026\n\n# Verification of predicted values\ny_hat &lt;- -0.9085714 + 1.2571429 * income\ndata.frame(\n  X = income,\n  Y = turnout,\n  Y_hat = y_hat,\n  row.names = 1:5\n)\n\n   X  Y    Y_hat\n1 50 60 61.94857\n2 45 56 55.66286\n3 56 70 69.49143\n4 40 50 49.37714\n5 60 75 74.52000\n\n\nStep 2: Calculate SST (Total Sum of Squares)\nSST = \\sum(Y_i - \\bar{Y})^2 \\text{ where } \\bar{Y} = 62.2\n(60 - 62.2)^2 = (-2.2)^2 = 4.84 (56 - 62.2)^2 = (-6.2)^2 = 38.44 (70 - 62.2)^2 = (7.8)^2 = 60.84 (50 - 62.2)^2 = (-12.2)^2 = 148.84 (75 - 62.2)^2 = (12.8)^2 = 163.84\nSST = 4.84 + 38.44 + 60.84 + 148.84 + 163.84 = 416.8\nStep 3: Calculate SSR (Regression Sum of Squares)\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\n(61.9485736 - 62.2)^2 = (-0.2514264)^2 = 0.0632151 (55.6535591 - 62.2)^2 = (-6.5464409)^2 = 42.8558689 (69.5114310 - 62.2)^2 = (7.3114310)^2 = 53.4570178 (49.3571446 - 62.2)^2 = (-12.8428554)^2 = 164.9389370 (74.5200026 - 62.2)^2 = (12.3200026)^2 = 151.7824640\nSSR = 413.0975028\nStep 4: Calculate SSE (Error Sum of Squares)\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n(60 - 61.9485736)^2 = (-1.9485736)^2 = 3.7969384 (56 - 55.6535591)^2 = (0.3464409)^2 = 0.1200212 (70 - 69.5114310)^2 = (0.4885690)^2 = 0.2387198 (50 - 49.3571446)^2 = (0.6428554)^2 = 0.4132631 (75 - 74.5200026)^2 = (0.4799974)^2 = 0.2303975\nSSE = 4.7024972\nStep 5: Verify decomposition\nSST = SSR + SSE 416.8 = 413.0975028 + 4.7024972\nStep 6: Calculate R-squared\nR^2 = \\frac{SSR}{SST} = \\frac{413.0975028}{416.8} = 0.9916\n\n# Verification\nsummary(model)$r.squared  # Should match our calculation\n\n[1] 0.9885135\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\ndf &lt;- data.frame(income = income, turnout = turnout)\n\nggplot(df, aes(x = income, y = turnout)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Voter Turnout vs Income per Capita\",\n    x = \"Income per Capita (thousands €)\",\n    y = \"Voter Turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    text = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\nInterpretation\nThe analysis shows:\n\nA very strong positive correlation (r = 0.994) between income and voter turnout\nThe regression equation \\hat{Y} = -0.9085714 + 1.2571429X indicates that:\n\nFor each €1,000 increase in income, turnout increases by about 1.26 percentage points\nThe intercept (-0.9086) has little practical meaning as income is never zero\n\nThe R-squared of 0.9916 indicates that 99.16% of the variance in turnout is explained by income",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "href": "correg_en.html#anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.46 Anxiety Levels and Cognitive Performance: A Laboratory Study",
    "text": "13.46 Anxiety Levels and Cognitive Performance: A Laboratory Study\n\nData and Context\nIn a psychology experiment, researchers measured the relationship between anxiety levels (measured by galvanic skin response, GSR) and cognitive performance (score on a working memory task).\n\n# Data\nanxiety &lt;- c(2.1, 3.4, 4.2, 5.1, 5.8, 6.4, 7.2, 8.0)  # GSR readings\nperformance &lt;- c(92, 88, 84, 78, 74, 70, 65, 62)      # Working memory scores\n\n# Initial model check\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8993 -0.6660  0.2162  0.6106  1.5262 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 105.3248     1.3189   79.86 0.00000000026 ***\nanxiety      -5.4407     0.2359  -23.06 0.00000043549 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 6 degrees of freedom\nMultiple R-squared:  0.9888,    Adjusted R-squared:  0.987 \nF-statistic: 531.9 on 1 and 6 DF,  p-value: 0.0000004355\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{2.1 + 3.4 + 4.2 + 5.1 + 5.8 + 6.4 + 7.2 + 8.0}{8} = \\frac{42.2}{8} = 5.275\n\\bar{Y} = \\frac{92 + 88 + 84 + 78 + 74 + 70 + 65 + 62}{8} = \\frac{613}{8} = 76.625\n\n# Verification\nmean(anxiety)\n\n[1] 5.275\n\nmean(performance)\n\n[1] 76.625\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(2.1 - 5.275) = -3.175\n(3.4 - 5.275) = -1.875\n(4.2 - 5.275) = -1.075\n(5.1 - 5.275) = -0.175\n(5.8 - 5.275) = 0.525\n(6.4 - 5.275) = 1.125\n(7.2 - 5.275) = 1.925\n(8.0 - 5.275) = 2.725\n\nSquared deviations:\n10.08063 + 3.51563 + 1.15563 + 0.03063 + 0.27563 + 1.26563 + 3.70563 +\n7.42563 = 27.45500\ns^2_X = \\frac{27.45500}{7} = 3.922143\nSimilarly for Y: Deviations:\n15.375, 11.375, 7.375, 1.375, -2.625, -6.625, -11.625, -14.625\ns^2_Y = \\frac{236.875 + 129.391 + 54.391 + 1.891 + 6.891 + 43.891 + 135.141 + 213.891}{7} = \\frac{822.362}{7} = 117.4803\n\n# Verification\nvar(anxiety)\n\n[1] 3.922143\n\nvar(performance)\n\n[1] 117.4107\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-3.175 × 15.375) = -48.815625\n(-1.875 × 11.375) = -21.328125\n(-1.075 × 7.375) = -7.928125\n(-0.175 × 1.375) = -0.240625\n(0.525 × -2.625) = -1.378125\n(1.125 × -6.625) = -7.453125\n(1.925 × -11.625) = -22.378125\n(2.725 × -14.625) = -39.853125\nSum = -149.375\ns_{XY} = \\frac{-149.375}{7} = -21.33929\n\n# Verification\ncov(anxiety, performance)\n\n[1] -21.33929\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-21.33929}{\\sqrt{3.922143}\\sqrt{117.4803}} = -0.9932\n\n# Verification\ncor(anxiety, performance)\n\n[1] -0.9944073\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-21.33929}{3.922143} = -5.4407\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-5.4407 × 5.275 = -28.6997\n\\hat{\\beta_0} = 76.625 - (-28.6997) = 105.3247\n\n\n# Verification\ncoef(model)\n\n(Intercept)     anxiety \n 105.324804   -5.440721 \n\n\n\n\n4. R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}): \\hat{Y} = 105.3247 - 5.4407X\n\n# Predicted values\ny_hat &lt;- 105.3247 - 5.4407 * anxiety\ndata.frame(\n  Anxiety = anxiety,\n  Performance = performance,\n  Predicted = y_hat,\n  row.names = 1:8\n)\n\n  Anxiety Performance Predicted\n1     2.1          92  93.89923\n2     3.4          88  86.82632\n3     4.2          84  82.47376\n4     5.1          78  77.57713\n5     5.8          74  73.76864\n6     6.4          70  70.50422\n7     7.2          65  66.15166\n8     8.0          62  61.79910\n\n\nStep 2: Sum of Squares\nSST = \\sum(Y_i - \\bar{Y})^2 = 822.362\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 816.3094\nSSE = \\sum(Y_i - \\hat{Y}_i)^2 = 6.0526\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{816.3094}{822.362} = 0.9926\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.9888459\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\n\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Cognitive Performance vs. Anxiety Levels\",\n    x = \"Anxiety (GSR)\",\n    y = \"Performance Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.993) between anxiety and cognitive performance\nFor each unit increase in GSR (anxiety), performance decreases by approximately 5.44 points\nThe model explains 99.26% of the variance in performance scores\nThe relationship appears to be strongly linear, suggesting a reliable anxiety-performance relationship\nThe high intercept (105.32) represents the theoretical maximum performance at zero anxiety\n\n\n\nStudy Limitations\n\nSmall sample size (n=8)\nPossible other confounding variables\nLimited range of anxiety levels\nCross-sectional rather than longitudinal data",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "href": "correg_en.html#district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "title": "13  Introduction to Correlation and Regression Analysis",
    "section": "13.47 District Magnitude and Electoral Disproportionality: A Comparative Analysis",
    "text": "13.47 District Magnitude and Electoral Disproportionality: A Comparative Analysis\n\nData Generating Process\nLet’s set up a DGP where:\n\\begin{aligned}\n& Y_{\\text{Gallagher}} = 12 - 0.8X_{\\text{DM}} + \\varepsilon \\\\\n& \\varepsilon \\sim \\mathcal{N}(0, 1) \\\\\n& X_{\\text{DM}} \\in \\{3, 5, 7, 10, 12, 15\\}\n\\end{aligned}\n\n# DGP\nmagnitude &lt;- c(3, 5, 7, 10, 12, 15)\nepsilon &lt;- rnorm(6, mean = 0, sd = 1)\ngallagher &lt;- 12 - 0.8 * magnitude + epsilon\n\n# Round (sampled from the DGP) Gallagher indices to one decimal place\ngallagher &lt;- round(c(9.0, 7.8, 9.2, 4.1, 2.5, 1.7), 1)\n\n# Show data\ndata.frame(\n  District_Magnitude = magnitude,\n  Gallagher_Index = gallagher\n)\n\n  District_Magnitude Gallagher_Index\n1                  3             9.0\n2                  5             7.8\n3                  7             9.2\n4                 10             4.1\n5                 12             2.5\n6                 15             1.7\n\n# Initial model check\nmodel &lt;- lm(gallagher ~ magnitude)\nsummary(model)\n\n\nCall:\nlm(formula = gallagher ~ magnitude)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.6516 -0.4628  2.3260 -0.6908 -0.9020  0.3813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7349     1.3034   9.003 0.000843 ***\nmagnitude    -0.6944     0.1359  -5.110 0.006934 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.368 on 4 degrees of freedom\nMultiple R-squared:  0.8672,    Adjusted R-squared:  0.834 \nF-statistic: 26.11 on 1 and 4 DF,  p-value: 0.006934\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{3 + 5 + 7 + 10 + 12 + 15}{6} = \\frac{52}{6} = 8.6667\n\\bar{Y} = \\frac{9.0 + 7.8 + 9.2 + 4.1 + 2.5 + 1.7}{6} = \\frac{34.3}{6} = 5.7167\n\n# Verification\nmean(magnitude)\n\n[1] 8.666667\n\nmean(gallagher)\n\n[1] 5.716667\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(3 - 8.6667) = -5.6667\n(5 - 8.6667) = -3.6667\n(7 - 8.6667) = -1.6667\n(10 - 8.6667) = 1.3333\n(12 - 8.6667) = 3.3333\n(15 - 8.6667) = 6.3333\n\nSquared deviations:\n\n32.1115 + 13.4445 + 2.7779 + 1.7777 + 11.1109 + 40.1107 = 101.3332\n\ns^2_X = \\frac{101.3332}{5} = 20.2666\nFor Y: Deviations: 3.2833, 2.0833, 3.4833, -1.6167, -3.2167, -4.0167\ns^2_Y = \\frac{56.3483}{5} = 11.2697\n\n# Verification\nvar(magnitude)\n\n[1] 20.26667\n\nvar(gallagher)\n\n[1] 11.26967\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n\n(-5.6667 × 3.2833) = -18.6057\n(-3.6667 × 2.0833) = -7.6387\n(-1.6667 × 3.4833) = -5.8056\n(1.3333 × -1.6167) = -2.1556\n(3.3333 × -3.2167) = -10.7223\n(6.3333 × -4.0167) = -25.4391\n\nSum = -70.3670\ns_{XY} = \\frac{-70.3670}{5} = -14.0734\n\n# Verification\ncov(magnitude, gallagher)\n\n[1] -14.07333\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-14.0734}{\\sqrt{20.2666}\\sqrt{11.2697}} = -0.9279\n\n# Verification\ncor(magnitude, gallagher)\n\n[1] -0.9312157\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-14.0734}{20.2666} = -0.6944\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-0.6944 × 8.6667 = -6.0181\n\\hat{\\beta_0} = 5.7167 - (-6.0181) = 11.7348\n\n\n# Verification\ncoef(model)\n\n(Intercept)   magnitude \n 11.7348684  -0.6944079 \n\n\n\n\nR-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = 11.7348 - 0.6944X\n\n# Predicted values\ny_hat &lt;- 11.7348 - 0.6944 * magnitude\ndata.frame(\n  Magnitude = magnitude,\n  Gallagher = gallagher,\n  Predicted = y_hat,\n  row.names = 1:6\n)\n\n  Magnitude Gallagher Predicted\n1         3       9.0    9.6516\n2         5       7.8    8.2628\n3         7       9.2    6.8740\n4        10       4.1    4.7908\n5        12       2.5    3.4020\n6        15       1.7    1.3188\n\n\nStep 2: Sum of Squares SST = \\sum(Y_i - \\bar{Y})^2 = 56.3483 SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 48.5271 SSE = \\sum(Y_i - \\hat{Y}_i)^2 = 7.8212\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{48.5271}{56.3483} = 0.8612\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.8671626\n\n\n\n\nVisualization - True vs. Estimated Parameters\n\nTrue DGP: Y = 12 - 0.8X + ε\nEstimated Model: Y = 11.7348 - 0.6944X\n\n\nlibrary(ggplot2)\n\n# Create data frame with original data\ndf &lt;- data.frame(\n  magnitude = magnitude,\n  gallagher = gallagher\n)\n\n# Create sequence for smooth lines\nx_seq &lt;- seq(min(magnitude), max(magnitude), length.out = 100)\n\n# Calculate predicted values for both lines\ntrue_dgp &lt;- 12 - 0.8 * x_seq\nestimated &lt;- 11.7348 - 0.6944 * x_seq\n\n# Combine into a data frame for plotting\nlines_df &lt;- data.frame(\n  magnitude = rep(x_seq, 2),\n  value = c(true_dgp, estimated),\n  Model = rep(c(\"True DGP\", \"Estimated\"), each = length(x_seq))\n)\n\n# Create plot\nggplot() +\n  geom_line(data = lines_df, \n            aes(x = magnitude, y = value, color = Model, linetype = Model),\n            size = 1) +\n  geom_point(data = df, \n             aes(x = magnitude, y = gallagher),\n             color = \"black\", \n             size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  labs(\n    title = \"True DGP vs. Estimated Regression Line\",\n    subtitle = \"Black points show observed data with random noise\",\n    x = \"District Magnitude\",\n    y = \"Gallagher Index\",\n    caption = \"True DGP: Y = 12 - 0.8X + ε\\nEstimated: Y = 11.73 - 0.69X\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.caption = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\n\nObservations about Model Fit\n\nSlope Comparison\n\nTrue slope: -0.8\nEstimated slope: -0.69\nThe estimated slope is reasonably close to the true parameter\n\nIntercept Comparison\n\nTrue intercept: 12\nEstimated intercept: 11.73\nThe estimated intercept very closely approximates the true value\n\nVisual Patterns\n\nThe lines are nearly parallel, showing good slope recovery\nPoints scatter around both lines due to the random error term (ε)\nThe small sample size (n=6) leads to some imprecision in estimation\nThe estimated line (blue) provides a good approximation of the true DGP (red dashed)\n\nImpact of Random Error\n\nThe scatter of points around the true DGP line reflects the N(0,1) error term\nThis noise leads to the slight differences in estimated parameters\nWith a larger sample, we would expect even closer convergence to true parameters\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.93) between district magnitude and electoral disproportionality\nFor each unit increase in district magnitude, the Gallagher index decreases by approximately 0.69 points\nThe model explains 86.12% of the variance in disproportionality\nThe relationship appears strongly linear with moderate scatter\nThe intercept (11.73) represents the expected disproportionality in a hypothetical single-member district system\n\n\n\nStudy Context\n\nData represents simulated observations from a DGP with moderate noise\nSample shows how increasing district magnitude tends to reduce disproportionality\nRandom component reflects other institutional and political factors affecting disproportionality\n\n\n\nLimitations\n\nSmall sample size (n=6)\nSimulated rather than real-world data\nAssumes linear relationship\nDoes not account for other institutional features",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_pl.html",
    "href": "correg_pl.html",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "",
    "text": "14.1 Wprowadzenie (Introduction)\nRóżnica między korelacją (correlation) a przyczynowością/kausalnością (causation) to jedno z podstawowych wyzwań w analizie statystycznej. Korelacja mierzy statystyczny związek między zmiennymi, natomiast przyczynowość oznacza bezpośredni wpływ jednej zmiennej na drugą.\nZależności statystyczne stanowią fundament podejmowania decyzji opartych na danych w wielu dyscyplinach — od ekonomii i zdrowia publicznego po psychologię i nauki o środowisku. Zrozumienie, kiedy związek wskazuje jedynie na asocjację (association), a kiedy na prawdziwą kausalność (genuine causality), jest kluczowe dla poprawnych wniosków i skutecznych rekomendacji politycznych.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kowariancja-covariance",
    "href": "correg_pl.html#kowariancja-covariance",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.2 Kowariancja (Covariance)",
    "text": "14.2 Kowariancja (Covariance)\nKowariancja (covariance) mierzy, w jaki sposób dwie zmienne współzmieniają się, wskazując zarówno kierunek, jak i siłę ich liniowego związku.\nWzór (z próby):\n\n\\operatorname{cov}(X,Y)\n= \\frac{\\sum_{i=1}^{n} (x_i - \\bar x)(y_i - \\bar y)}{n - 1}.\n\nGdzie:\n\nx_i i y_i to poszczególne obserwacje,\n\\bar{x} i \\bar{y} to średnie odpowiednio zmiennych X i Y,\nn to liczba obserwacji,\ndzielimy przez (n-1), ponieważ liczymy kowariancję z próby (tzw. poprawka Bessela; Bessel’s correction).\n\n\nObliczenia ręczne krok po kroku (Step-by-Step Manual Calculation Process)\nPrzykład 1: Godziny nauki a wyniki testu\nDane:\n\nGodziny nauki (X): 2, 4, 6, 8, 10\nWyniki testu (Y): 65, 70, 80, 85, 95\n\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nOblicz średnie\n\\bar{x}=\\frac{2+4+6+8+10}{5}=6 godz.\n\n\n\n\n\\bar{y}=\\frac{65+70+80+85+95}{5}=79 pkt\n\n\n2\nOdchylenia od średnich\n(x_i-\\bar{x}): -4, -2, 0, 2, 4\n\n\n\n\n(y_i-\\bar{y}): -14, -9, 1, 6, 16\n\n\n3\nIloczyny odchyleń\n(x_i-\\bar{x})(y_i-\\bar{y}): 56, 18, 0, 12, 64\n\n\n4\nSuma iloczynów\n\\sum = 56+18+0+12+64=150\n\n\n5\nPodziel przez (n-1)\n\\operatorname{cov}(X,Y)=\\frac{150}{5-1}=\\frac{150}{4}=37.5\n\n\n\nWeryfikacja w R (R Verification):\n\n# Definicja danych\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Kowariancja\ncov_manual &lt;- cov(study_hours, test_scores)\nprint(paste(\"Covariance:\", round(cov_manual, 2)))\n\n[1] \"Covariance: 37.5\"\n\n# Weryfikacja krok po kroku\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\nproducts &lt;- x_dev * y_dev\ncov_calculated &lt;- sum(products) / (length(study_hours) - 1)\n\n# Tabela obliczeń\ndata.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_deviation = x_dev,\n  Y_deviation = y_dev,\n  Product = products\n)\n\n   X  Y X_deviation Y_deviation Product\n1  2 65          -4         -14      56\n2  4 70          -2          -9      18\n3  6 80           0           1       0\n4  8 85           2           6      12\n5 10 95           4          16      64\n\n\nInterpretacja: Dodatnia kowariancja (37.5) wskazuje, że wraz ze wzrostem liczby godzin nauki rosną także wyniki testu — zmienne mają tendencję do wspólnego wzrostu.\n\n\nZadanie ćwiczeniowe z rozwiązaniem (Practice Problem with Solution)\nPolicz ręcznie kowariancję dla:\n\nTemperatura (°F): 32, 50, 68, 86, 95\nSprzedaż lodów ($): 100, 200, 400, 600, 800\n\nRozwiązanie:\n\n\n\n\n\n\n\nKrok\nObliczenie\n\n\n\n\n1. Średnie\n\\bar{x}=\\frac{32+50+68+86+95}{5}=66.2^{\\circ}\\mathrm{F}\n\n\n\n\\bar{y}=\\frac{100+200+400+600+800}{5}=420\n\n\n2. Odchylenia\nX: -34.2, -16.2, 1.8, 19.8, 28.8\n\n\n\nY: -320, -220, -20, 180, 380\n\n\n3. Iloczyny\n10944, 3564, -36, 3564, 10944\n\n\n4. Suma\n28980\n\n\n5. Kowariancja\n\\frac{28980}{4}=7245\n\n\n\n\n# Weryfikacja zadania\ntemp &lt;- c(32, 50, 68, 86, 95)\nsales &lt;- c(100, 200, 400, 600, 800)\nprint(paste(\"Covariance for practice problem:\", round(cov(temp, sales), 2)))\n\n[1] \"Covariance for practice problem: 7245\"",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#współczynnik-korelacji-correlation-coefficient",
    "href": "correg_pl.html#współczynnik-korelacji-correlation-coefficient",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.3 Współczynnik korelacji (Correlation Coefficient)",
    "text": "14.3 Współczynnik korelacji (Correlation Coefficient)\nWspółczynnik korelacji (correlation coefficient) standaryzuje kowariancję, usuwając zależność od skali i przyjmując wartości od -1 do +1.\n\nWskazówki interpretacyjne (Interpretation Guidelines)\n\n\n\n\n\n\n\n\n\nWartość korelacji\nSiła\nInterpretacja\nPrzykład\n\n\n\n\n±0.90 do ±1.00\nBardzo silna\nNiemal doskonały związek\nWzrost rodziców i dzieci\n\n\n±0.70 do ±0.89\nSilna\nZmienne silnie powiązane\nCzas nauki i oceny\n\n\n±0.50 do ±0.69\nUmiarkowana\nUmiarkowany związek\nĆwiczenia a spadek masy\n\n\n±0.30 do ±0.49\nSłaba\nSłaby związek\nRozmiar buta a umiejętność czytania\n\n\n±0.00 do ±0.29\nBardzo słaba/brak\nZnikomy lub brak związku\nMiesiąc urodzenia a inteligencja\n\n\n\n\n\nWizualizacja typów zależności korelacyjnych (Types of Correlations Visualization)\n\n# Generowanie przykładowych danych dla różnych wzorców korelacji\nn &lt;- 100\n\n# Dodatnia zależność liniowa\nset.seed(123)\nyears_education &lt;- rnorm(n, 14, 3)\nannual_income &lt;- 15000 + 3500 * years_education + rnorm(n, 0, 10000)\npos_cor &lt;- round(cor(years_education, annual_income), 3)\n\n# Ujemna zależność liniowa\nscreen_time &lt;- runif(n, 1, 8)\nsleep_hours &lt;- 9 - 0.5 * screen_time + rnorm(n, 0, 0.5)\nneg_cor &lt;- round(cor(screen_time, sleep_hours), 3)\n\n# Brak korelacji\nx_random &lt;- rnorm(n, 50, 10)\ny_random &lt;- rnorm(n, 50, 10)\nno_cor &lt;- round(cor(x_random, y_random), 3)\n\n# Nieliniowa zależność (kwadratowa)\nhours_studied &lt;- runif(n, 0, 12)\ntest_score &lt;- -2 * (hours_studied - 6)^2 + 90 + rnorm(n, 0, 5)\nnonlin_cor &lt;- round(cor(hours_studied, test_score), 3)\n\n# Ramki danych z wartościami korelacji\npositive_data &lt;- data.frame(\n  x = years_education, \n  y = annual_income,\n  label = paste0(\"Dodatnia liniowa (r = \", pos_cor, \")\")\n)\n\nnegative_data &lt;- data.frame(\n  x = screen_time, \n  y = sleep_hours,\n  label = paste0(\"Ujemna liniowa (r = \", neg_cor, \")\")\n)\n\nno_corr_data &lt;- data.frame(\n  x = x_random, \n  y = y_random,\n  label = paste0(\"Brak korelacji (r = \", no_cor, \")\")\n)\n\nnonlinear_data &lt;- data.frame(\n  x = hours_studied, \n  y = test_score,\n  label = paste0(\"Nieliniowa (Pearson r = \", nonlin_cor, \")\")\n)\n\n# Połączenie danych\nall_data &lt;- rbind(positive_data, negative_data, no_corr_data, nonlinear_data)\n\n# Wykres fasetowy\nggplot(all_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE, alpha = 0.3) +\n  facet_wrap(~ label, scales = \"free\", ncol = 2) +\n  labs(\n    title = \"Różne typy korelacji\",\n    subtitle = \"Linia regresji liniowej (na czerwono) z pasmem ufności\",\n    x = \"\", \n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, color = \"gray50\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#korelacja-pearsona-pearson-correlation",
    "href": "correg_pl.html#korelacja-pearsona-pearson-correlation",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.4 Korelacja Pearsona (Pearson Correlation)",
    "text": "14.4 Korelacja Pearsona (Pearson Correlation)\nWzór:\n\nr\n= \\frac{\\operatorname{cov}(X,Y)}{s_X\\, s_Y}\n= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}\n{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2}\\,\\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}}.\n\n\nPełny przykład obliczeń ręcznych (Complete Manual Calculation Example)\nNa danych o godzinach nauki:\n\nGodziny nauki (X): 2, 4, 6, 8, 10\nWyniki testu (Y): 65, 70, 80, 85, 95\n\nSzczegółowe kroki:\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nKowariancja\nZ powyżej: \\operatorname{cov}(X,Y) = 37.5\n\n\n2\nKwadraty odchyleń\n\n\n\n\nDla X\n(x_i-\\bar{x})^2: 16, 4, 0, 4, 16\n\n\n\n\nSuma = 40\n\n\n\nDla Y\n(y_i-\\bar{y})^2: 196, 81, 1, 36, 256\n\n\n\n\nSuma = 570\n\n\n3\nOdchylenia standardowe (standard deviations)\n\n\n\n\ns_X\ns_X=\\sqrt{\\frac{40}{4}}=\\sqrt{10}=3.162\n\n\n\ns_Y\ns_Y=\\sqrt{\\frac{570}{4}}=\\sqrt{142.5}=11.937\n\n\n4\nKorelacja\nr=\\frac{37.5}{3.162 \\times 11.937}\n\n\n\n\nr=\\frac{37.5}{37.73}\\approx 0.994\n\n\n\n\n# Weryfikacja obliczeń\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Współczynnik korelacji Pearsona\nr_value &lt;- cor(study_hours, test_scores, method = \"pearson\")\nprint(paste(\"Pearson correlation coefficient:\", round(r_value, 3)))\n\n[1] \"Pearson correlation coefficient: 0.993\"\n\n# Obliczenia szczegółowe\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\n\n# Tabela obliczeń\ncalc_table &lt;- data.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_dev = x_dev,\n  Y_dev = y_dev,\n  X_dev_sq = x_dev^2,\n  Y_dev_sq = y_dev^2,\n  Product = x_dev * y_dev\n)\nprint(calc_table)\n\n   X  Y X_dev Y_dev X_dev_sq Y_dev_sq Product\n1  2 65    -4   -14       16      196      56\n2  4 70    -2    -9        4       81      18\n3  6 80     0     1        0        1       0\n4  8 85     2     6        4       36      12\n5 10 95     4    16       16      256      64\n\n# Statystyki podsumowujące\ncat(\"\\nSummary:\")\n\n\nSummary:\n\ncat(\"\\nSum of (X-mean)^2:\", sum(x_dev^2))\n\n\nSum of (X-mean)^2: 40\n\ncat(\"\\nSum of (Y-mean)^2:\", sum(y_dev^2))\n\n\nSum of (Y-mean)^2: 570\n\ncat(\"\\nSum of products:\", sum(x_dev * y_dev))\n\n\nSum of products: 150\n\ncat(\"\\nStandard deviation of X:\", sd(study_hours))\n\n\nStandard deviation of X: 3.162278\n\ncat(\"\\nStandard deviation of Y:\", sd(test_scores))\n\n\nStandard deviation of Y: 11.93734\n\n# Przedział ufności i p-value\ncor_test &lt;- cor.test(study_hours, test_scores, method = \"pearson\")\nprint(cor_test)\n\n\n    Pearson's product-moment correlation\n\ndata:  study_hours and test_scores\nt = 15, df = 3, p-value = 0.0006431\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8994446 0.9995859\nsample estimates:\n      cor \n0.9933993 \n\n\nInterpretacja: r \\approx 0.994 wskazuje na niemal doskonały dodatni liniowy związek między godzinami nauki a wynikiem testu. Wartość p &lt; 0.05 sugeruje statystyczną istotność tej zależności.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#korelacja-rang-spearmana-spearman-rank-correlation",
    "href": "correg_pl.html#korelacja-rang-spearmana-spearman-rank-correlation",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.5 Korelacja rang Spearmana (Spearman Rank Correlation)",
    "text": "14.5 Korelacja rang Spearmana (Spearman Rank Correlation)\nKorelacja Spearmana mierzy monotoniczne zależności, używając rang zamiast surowych wartości.\nWzór:\n\n\\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2 - 1)},\n\ngdzie d_i to różnica rang dla obserwacji i.\n\nPełny przykład obliczeń ręcznych (Complete Manual Example)\nDane: Wyniki z matematyki i angielskiego\n\n\n\nUczeń\nMatematyka\nAngielski\n\n\n\n\nA\n78\n82\n\n\nB\n85\n90\n\n\nC\n88\n88\n\n\nD\n92\n94\n\n\nE\n95\n96\n\n\n\nRangowanie i obliczenia:\n\n\n\n\n\n\n\n\n\n\n\n\nUczeń\nWynik z mat.\nRanga mat.\nWynik z ang.\nRanga ang.\nd = \\text{ranga mat.} - \\text{ranga ang.}\nd^2\n\n\n\n\nA\n78\n1\n82\n1\n0\n0\n\n\nB\n85\n2\n90\n3\n-1\n1\n\n\nC\n88\n3\n88\n2\n1\n1\n\n\nD\n92\n4\n94\n4\n0\n0\n\n\nE\n95\n5\n96\n5\n0\n0\n\n\n\n\n\n\n\nSuma:\n2\n\n\n\nObliczenie:\n\n\\rho = 1 - \\frac{6 \\times 2}{5(25-1)} = 1 - \\frac{12}{120} = 0.9.\n\n\n# Dane\nmath_scores &lt;- c(78, 85, 88, 92, 95)\nenglish_scores &lt;- c(82, 90, 88, 94, 96)\n\n# Rangi\nrank_table &lt;- data.frame(\n  Student = LETTERS[1:5],\n  Math_Score = math_scores,\n  Math_Rank = rank(math_scores),\n  English_Score = english_scores,\n  English_Rank = rank(english_scores)\n)\n\nrank_table$d &lt;- rank_table$Math_Rank - rank_table$English_Rank\nrank_table$d_squared &lt;- rank_table$d^2\n\nprint(rank_table)\n\n  Student Math_Score Math_Rank English_Score English_Rank  d d_squared\n1       A         78         1            82            1  0         0\n2       B         85         2            90            3 -1         1\n3       C         88         3            88            2  1         1\n4       D         92         4            94            4  0         0\n5       E         95         5            96            5  0         0\n\ncat(\"\\nSum of d^2:\", sum(rank_table$d_squared))\n\n\nSum of d^2: 2\n\n# Korelacja Spearmana\nrho &lt;- cor(math_scores, english_scores, method = \"spearman\")\ncat(\"\\nSpearman correlation (R):\", round(rho, 3))\n\n\nSpearman correlation (R): 0.9\n\n# Obliczenie ręczne\nn &lt;- length(math_scores)\nrho_manual &lt;- 1 - (6 * sum(rank_table$d_squared)) / (n * (n^2 - 1))\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.9",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#tabele-krzyżowe-cross-tabulation-i-dane-kategoryczne",
    "href": "correg_pl.html#tabele-krzyżowe-cross-tabulation-i-dane-kategoryczne",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.6 Tabele krzyżowe (Cross-tabulation) i dane kategoryczne",
    "text": "14.6 Tabele krzyżowe (Cross-tabulation) i dane kategoryczne\nTabela krzyżowa (cross-tabulation, contingency table) pokazuje zależności między zmiennymi kategorycznymi.\n\n# Bardziej realistyczne dane przykładowe\nset.seed(123)\nn_total &lt;- 120\n\n# Poziom edukacji a zatrudnienie\neducation &lt;- factor(\n  c(rep(\"High School\", 40), \n    rep(\"College\", 50), \n    rep(\"Graduate\", 30)),\n  levels = c(\"High School\", \"College\", \"Graduate\")\n)\n\n# Status zatrudnienia z prawdopodobieństwami zależnymi od edukacji\nemployment &lt;- factor(\n  c(\n    # High School: 75% employed\n    sample(c(rep(\"Employed\", 30), rep(\"Unemployed\", 10)), 40),\n    # College: 90% employed  \n    sample(c(rep(\"Employed\", 45), rep(\"Unemployed\", 5)), 50),\n    # Graduate: 95% employed\n    sample(c(rep(\"Employed\", 28), rep(\"Unemployed\", 2)), 30)\n  ),\n  levels = c(\"Employed\", \"Unemployed\")\n)\n\n# Tabela kontyngencji\ncontingency_table &lt;- table(education, employment)\nprint(\"Contingency Table:\")\n\n[1] \"Contingency Table:\"\n\nprint(contingency_table)\n\n             employment\neducation     Employed Unemployed\n  High School       30         10\n  College           45          5\n  Graduate          28          2\n\n# Procenty w wierszach\nprint(\"\\nRow Percentages (% within each education level):\")\n\n[1] \"\\nRow Percentages (% within each education level):\"\n\nprint(round(prop.table(contingency_table, 1) * 100, 1))\n\n             employment\neducation     Employed Unemployed\n  High School     75.0       25.0\n  College         90.0       10.0\n  Graduate        93.3        6.7\n\n# Test niezależności chi-kwadrat (Chi-square test)\nchi_test &lt;- chisq.test(contingency_table)\n\nWarning in chisq.test(contingency_table): Chi-squared approximation may be\nincorrect\n\nprint(chi_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 5.9623, df = 2, p-value = 0.05073",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#ćwiczenia-praktyczne-z-rozwiązaniami-practical-exercises-with-solutions",
    "href": "correg_pl.html#ćwiczenia-praktyczne-z-rozwiązaniami-practical-exercises-with-solutions",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.7 Ćwiczenia praktyczne z rozwiązaniami (Practical Exercises with Solutions)",
    "text": "14.7 Ćwiczenia praktyczne z rozwiązaniami (Practical Exercises with Solutions)\n\nĆwiczenie 1: Ręczne obliczenie korelacji Pearsona (Calculate Pearson Correlation Manually)\nDane:\n\nWzrost (cale): 66, 68, 70, 72, 74\nWaga (funty): 140, 155, 170, 185, 200\n\nRozwiązanie:\n\n# Dane\nheight &lt;- c(66, 68, 70, 72, 74)\nweight &lt;- c(140, 155, 170, 185, 200)\n\n# Krok 1: Średnie\nh_mean &lt;- mean(height)  # 70\nw_mean &lt;- mean(weight)  # 170\n\n# Krok 2: Odchylenia i iloczyny\ncalculation_df &lt;- data.frame(\n  Height = height,\n  Weight = weight,\n  H_dev = height - h_mean,\n  W_dev = weight - w_mean,\n  H_dev_sq = (height - h_mean)^2,\n  W_dev_sq = (weight - w_mean)^2,\n  Product = (height - h_mean) * (weight - w_mean)\n)\n\nprint(calculation_df)\n\n  Height Weight H_dev W_dev H_dev_sq W_dev_sq Product\n1     66    140    -4   -30       16      900     120\n2     68    155    -2   -15        4      225      30\n3     70    170     0     0        0        0       0\n4     72    185     2    15        4      225      30\n5     74    200     4    30       16      900     120\n\n# Krok 3: Korelacja\ncov_hw &lt;- sum(calculation_df$Product) / (length(height) - 1)\nsd_h &lt;- sqrt(sum(calculation_df$H_dev_sq) / (length(height) - 1))\nsd_w &lt;- sqrt(sum(calculation_df$W_dev_sq) / (length(weight) - 1))\nr_manual &lt;- cov_hw / (sd_h * sd_w)\n\ncat(\"\\nManual calculation:\")\n\n\nManual calculation:\n\ncat(\"\\nCovariance:\", round(cov_hw, 2))\n\n\nCovariance: 75\n\ncat(\"\\nSD Height:\", round(sd_h, 2))\n\n\nSD Height: 3.16\n\ncat(\"\\nSD Weight:\", round(sd_w, 2))\n\n\nSD Weight: 23.72\n\ncat(\"\\nCorrelation:\", round(r_manual, 3))\n\n\nCorrelation: 1\n\n# Weryfikacja w R\ncat(\"\\n\\nR calculation:\", round(cor(height, weight), 3))\n\n\n\nR calculation: 1\n\n\n\n\nĆwiczenie 2: Ręczne obliczenie korelacji Spearmana (Calculate Spearman Correlation Manually)\nDane:\n\nRangi uczniów z matematyki: 1, 3, 2, 5, 4\nRangi uczniów z nauk ścisłych (science): 2, 4, 1, 5, 3\n\nRozwiązanie:\n\n# Rangi (już zrankowane)\nmath_rank &lt;- c(1, 3, 2, 5, 4)\nscience_rank &lt;- c(2, 4, 1, 5, 3)\n\n# Różnice\nd &lt;- math_rank - science_rank\nd_squared &lt;- d^2\n\n# Tabela\nrank_df &lt;- data.frame(\n  Student = 1:5,\n  Math_Rank = math_rank,\n  Science_Rank = science_rank,\n  d = d,\n  d_squared = d_squared\n)\n\nprint(rank_df)\n\n  Student Math_Rank Science_Rank  d d_squared\n1       1         1            2 -1         1\n2       2         3            4 -1         1\n3       3         2            1  1         1\n4       4         5            5  0         0\n5       5         4            3  1         1\n\n# Korelacja Spearmana\nn &lt;- length(math_rank)\nsum_d_sq &lt;- sum(d_squared)\nrho_manual &lt;- 1 - (6 * sum_d_sq) / (n * (n^2 - 1))\n\ncat(\"\\nSum of d^2:\", sum_d_sq)\n\n\nSum of d^2: 4\n\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.8\n\ncat(\"\\nSpearman correlation (R):\", round(cor(math_rank, science_rank, method = \"spearman\"), 3))\n\n\nSpearman correlation (R): 0.8\n\n\n\n\nĆwiczenie 3: Interpretacja wyników (Interpretation Practice)\nZinterpretuj następujące wartości korelacji:\n\nr = 0.85 między godzinami treningu a wynikiem sprawdzianu sprawności Odpowiedź: Silny dodatni związek. Wraz ze wzrostem liczby godzin treningu wyniki istotnie rosną.\nr = -0.72 między temperaturą na zewnątrz a kosztami ogrzewania Odpowiedź: Silny ujemny związek. Wraz ze wzrostem temperatury koszty ogrzewania wyraźnie maleją.\nr = 0.12 między rozmiarem buta a inteligencją Odpowiedź: Bardzo słaby/brak istotnego związku. Zmienne są praktycznie niezależne.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#najważniejsze-rzeczy-do-zapamiętania-important-points-to-remember",
    "href": "correg_pl.html#najważniejsze-rzeczy-do-zapamiętania-important-points-to-remember",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.8 Najważniejsze rzeczy do zapamiętania (Important Points to Remember)",
    "text": "14.8 Najważniejsze rzeczy do zapamiętania (Important Points to Remember)\n\nKorelacja mierzy siłę związku: Wartości od -1 do +1.\nKorelacja ≠ przyczynowość (Correlation ≠ Causation): Wysoka korelacja nie dowodzi wpływu jednej zmiennej na drugą.\nDobierz właściwą metodę:\n\nPearson: Związki liniowe dla danych ciągłych.\nSpearman: Związki monotoniczne lub dane rangowe.\n\nSprawdź założenia:\n\nPearson: liniowość i (w praktyce) rozkład zbliżony do normalnego.\nSpearman: wymagana jedynie monotoniczność.\n\nUwaga na obserwacje odstające (outliers): Mogą silnie wpływać na korelację Pearsona.\nZawsze wizualizuj dane: Wykresy pomagają ocenić kształt zależności.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#podsumowanie-drzewko-decyzyjne-do-analizy-korelacji-summary-decision-tree-for-correlation-analysis",
    "href": "correg_pl.html#podsumowanie-drzewko-decyzyjne-do-analizy-korelacji-summary-decision-tree-for-correlation-analysis",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.9 Podsumowanie: drzewko decyzyjne do analizy korelacji (Summary: Decision Tree for Correlation Analysis)",
    "text": "14.9 Podsumowanie: drzewko decyzyjne do analizy korelacji (Summary: Decision Tree for Correlation Analysis)\n\n\n\nWYBÓR WŁAŚCIWEJ MIARY KORELACJI:\n\nCzy dane są liczbowe (numeryczne)?\n├─ TAK → Czy związek jest liniowy?\n│   ├─ TAK → Użyj korelacji PEARSONA\n│   └─ NIE → Czy związek jest monotoniczny?\n│       ├─ TAK → Użyj korelacji SPEARMANA\n│       └─ NIE → Rozważ metody nieliniowe\n└─ NIE → Czy dane są porządkowe (rangi)?\n    ├─ TAK → Użyj korelacji SPEARMANA\n    └─ NIE → Użyj TABEL KRZYŻOWYCH dla danych kategorycznych",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#dodatkowe-zadania-additional-practice-problems",
    "href": "correg_pl.html#dodatkowe-zadania-additional-practice-problems",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.10 Dodatkowe zadania (Additional Practice Problems)",
    "text": "14.10 Dodatkowe zadania (Additional Practice Problems)\n\nZestaw zadań A: obliczenia ręczne (Problem Set A: Manual Calculations)\n\nPolicz kowariancję i korelację Pearsona dla:\n\nX: 10, 20, 30, 40, 50\nY: 15, 25, 35, 45, 55 Rozwiązanie: \\operatorname{cov}(X,Y)=250, r=1.0 (idealna dodatnia korelacja).\n\nPolicz korelację Spearmana dla ocen filmów:\n\nOceny filmu A: 8, 6, 9, 7, 5\nOceny filmu B: 7, 8, 9, 6, 4 Rozwiązanie: \\rho \\approx 0.3 (słaba dodatnia korelacja).\n\n\n\n\nZestaw zadań B: interpretacja (Problem Set B: Interpretation)\n\nBadanie wykazuje r = 0.91 między godzinami snu a wynikami testów. Interpretacja: Bardzo silny dodatni związek — więcej snu wiąże się z lepszymi wynikami. Nie dowodzi to jednak kausalności; możliwe czynniki uboczne.\nInne badanie: r = -0.03 między miesiącem urodzenia a IQ. Interpretacja: Brak istotnego związku. Miesiąc urodzenia i IQ są w praktyce niezależne.\n\n\n\nŚciąga (Quick Reference Card)\n\n\n\n\n\n\n\n\n\nMiara\nKiedy używać (Use When)\nWzór (Formula)\nZakres (Range)\n\n\n\n\nKowariancja (Covariance)\nWstępne badanie związku\n\\displaystyle \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n-\\infty do +\\infty\n\n\nPearson r\nZwiązki liniowe, dane ciągłe\n\\displaystyle \\frac{\\operatorname{cov}(X,Y)}{s_X s_Y}\n-1 do +1\n\n\nSpearman \\rho\nZwiązki monotoniczne, rangi\n\\displaystyle 1-\\frac{6\\sum d_i^2}{n(n^2-1)}\n-1 do +1\n\n\nTabele krzyżowe (Cross-tabs)\nZmienne kategoryczne\nZliczenia częstości\nn/d",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#analiza-regresji-ols-ordinary-least-squares-przewodnik-na-start-a-quick-start-guide",
    "href": "correg_pl.html#analiza-regresji-ols-ordinary-least-squares-przewodnik-na-start-a-quick-start-guide",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.11 Analiza regresji OLS (Ordinary Least Squares): przewodnik na start (A Quick-start Guide)",
    "text": "14.11 Analiza regresji OLS (Ordinary Least Squares): przewodnik na start (A Quick-start Guide)\n\n\n\n\n\n\nAnaliza regresji OLS: przewodnik na start\n\n\n\n\nWprowadzenie: czym jest analiza regresji?\nAnaliza regresji (regression analysis) pomaga zrozumieć i mierzyć zależności między obserwowalnymi wielkościami. To zestaw narzędzi matematycznych do identyfikowania wzorców w danych, które umożliwiają prognozowanie (prediction).\nRozważ pytania badawcze:\n\nJak czas nauki wpływa na wynik testu?\nJak doświadczenie wpływa na wynagrodzenie?\nJak wydatki na reklamę oddziałują na sprzedaż?\n\nRegresja dostarcza systematycznych metod, by na te pytania odpowiadać na podstawie realnych danych.\n\n\nPunkt wyjścia: prosty przykład\nZacznijmy od konkretu. Zebrano dane o 20 studentach z Twojej klasy:\n\n\n\nStudent\nStudy Hours\nExam Score\n\n\n\n\nAlex\n2\n68\n\n\nBeth\n4\n74\n\n\nCarlos\n6\n85\n\n\nDiana\n8\n91\n\n\n…\n…\n…\n\n\n\nPo narysowaniu wykresu punktowego (scatter plot) chcesz znaleźć prostą, która najlepiej opisuje związek między godzinami nauki a wynikiem.\nAle co znaczy „najlepiej”? Właśnie to odkryjemy.\n\n\nDlaczego prawdziwe dane nie układają się w idealną linię\nZanim przejdziemy do rachunków, zrozummy, dlaczego punkty zwykle nie leżą na jednej prostej.\n\nModele deterministyczne vs. stochastyczne\nModele deterministyczne (deterministic models) opisują związki bez niepewności. Przykład z fizyki:\n\\text{Distance} = \\text{Speed} \\times \\text{Time}\nJedziesz dokładnie 60 mph przez 2 godziny → zawsze 120 mil. Zero odchyleń.\nModele stochastyczne (stochastic models) uznają, że w danych naturalnie występuje losowość. Ogólna postać to:\nY = f(X) + \\epsilon\nGdzie:\n\nY — wielkość, którą prognozujemy (np. wynik testu),\nf(X) — wzorzec systematyczny (jak godziny nauki typowo wpływają na wyniki),\n\\epsilon — „reszta”/szum: wszystko, czego nie mierzymy.\n\nW naszym przykładzie dwoje studentów może uczyć się po 5 godzin, a jednak dostać różne oceny, bo:\n\njedno lepiej spało,\njedno ma talent do testów,\njedno miało hałas na sali,\npytania trafiły bardziej/mniej pod ich przygotowanie.\n\nTa losowość jest naturalna — tym zajmuje się \\epsilon.\n\n\n\nProsty model regresji liniowej\nZależność między godzinami nauki a wynikiem zapisujemy jako:\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nRozszyfrujmy:\n\nY_i — wynik testu studenta i,\nX_i — godziny nauki studenta i,\n\\beta_0 — wyraz wolny (intercept, „poziom bazowy” przy 0 godzin),\n\\beta_1 — nachylenie (slope, przyrost punktów na godzinę),\n\\epsilon_i — „wszystko inne” wpływające na wynik i.\n\nWażne: Prawdziwych wartości \\beta_0, \\beta_1 nie znamy. Szacujemy je z danych i oznaczamy „z daszkiem”: \\hat{\\beta}_0, \\hat{\\beta}_1.\n\n\nReszty: jak bardzo mylimy się w przewidywaniach?\nPo dopasowaniu prostej możemy przewidzieć wyniki. Dla każdej obserwacji:\n\nWartość rzeczywista (y_i): faktyczny wynik,\nWartość przewidziana (\\hat{y}_i): co „mówi” nasza prosta,\nReszta (e_i): różnica = Rzeczywista − Przewidziana.\n\nPrzykład:\nDiana: 8 h nauki, wynik 91\nLinia przewiduje: 88\nReszta: 91 − 88 = +3 (zaniżyliśmy)\n\nEric: 5 h nauki, wynik 70\nLinia przewiduje: 79\nReszta: 70 − 79 = −9 (zawyżyliśmy)\n\n\nKluczowy pomysł: dlaczego kwadratujemy reszty?\nZałóżmy reszty czterech studentów:\n\nA: +5\nB: −5\nC: +3\nD: −3\n\nSuma: (+5) + (-5) + (+3) + (-3) = 0.\nTo nie znaczy, że przewidywania są idealne — błędy się znoszą.\nRozwiązanie: sumujemy kwadraty reszt:\n\n(+5)^2 = 25\n(-5)^2 = 25\n(+3)^2 = 9\n(-3)^2 = 9\nSuma kwadratów błędów = 68\n\nDlaczego to działa:\n\nBrak znoszenia znaków, bo kwadraty są dodatnie,\nDuże błędy ważą mocniej (10 punktów to 4× więcej niż 5 punktów),\nWygoda matematyczna: funkcje kwadratowe są gładkie i różniczkowalne.\n\n\n\nMetoda zwykłych najmniejszych kwadratów (OLS)\nOLS wybiera taką prostą, która minimalizuje sumę kwadratów reszt (SSE — Sum of Squared Errors):\n\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\nCzyli:\n\\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n \\big(y_i - (\\beta_0 + \\beta_1 x_i)\\big)^2\n„Po ludzku”: Znajdź takie \\beta_0 i \\beta_1, by łączny błąd (w kwadracie) przewidywań był jak najmniejszy.\n\n\nRozwiązanie matematyczne\nMinimalizujemy SSE rachunkiem różniczkowym. Warunki pierwszego rzędu:\n\\frac{\\partial SSE}{\\partial \\beta_0} = -2\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i) = 0\n\\frac{\\partial SSE}{\\partial \\beta_1} = -2\\sum_{i=1}^n x_i(y_i - \\beta_0 - \\beta_1x_i) = 0\nRozwiązanie układu:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\nWnioski:\n\nNachylenie zależy od tego, jak współzmieniają się X i Y (kowariancja) względem zmienności samego X (wariancja),\nLinia przechodzi przez punkt średnich (\\bar{x}, \\bar{y}).\n\n\n\nSkąd wiemy, że linia jest „dobra”? Rozkład zmienności\nRozbijamy całkowitą zmienność wyników:\nCałkowita suma kwadratów (SST — Total Sum of Squares)\n„Jak bardzo ogólnie różnią się wyniki?”\nSST = \\sum_{i=1}^n (y_i - \\bar{y})^2\nSuma kwadratów regresji (SSR — Regression Sum of Squares)\n„Ile zmienności wyjaśnia nasza linia?”\nSSR = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\nSuma kwadratów błędów (SSE — Error Sum of Squares)\n„Ile nie wyjaśniamy?”\nSSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\nTożsamość wariancyjna:\nSST = SSR + SSE \\quad\\Rightarrow\\quad \\text{Całkowita} = \\text{Wyjaśniona} + \\text{Niewyjaśniona}\n\n\nR^2: ocena dopasowania\nWspółczynnik determinacji (R^2) mówi, jaki odsetek zmienności wyjaśnia model:\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nInterpretacja:\n\nR^2 = 0.75: „Godziny nauki wyjaśniają 75% zróżnicowania wyników”,\nR^2 = 0.30: „Model wyjaśnia 30% tego, czym różnią się wyniki”,\nR^2 = 1.00: perfekcyjne dopasowanie (w realu prawie nigdy),\nR^2 = 0.00: nie lepiej niż zgadywanie średniej.\n\nUwaga kontekstowa: w naukach społecznych 0.30 bywa świetne; w inżynierii oczekuje się bardzo wysokich R^2.\n\n\nInterpretacja wyników\nZałóżmy, że oszacowaliśmy \\hat{\\beta}_0 = 60 i \\hat{\\beta}_1 = 4.\nNachylenie (\\hat{\\beta}_1 = 4):\n\n„Każda dodatkowa godzina nauki wiąże się średnio z +4 punktami”,\nTo efekt przeciętny, nie obietnica dla konkretnej osoby.\n\nWyraz wolny (\\hat{\\beta}_0 = 60):\n\n„Przy 0 godzinach nauki przewidujemy 60 punktów”,\nCzęsto to tylko kotwica matematyczna — może nie mieć sensu praktycznego.\n\nRównanie predykcji:\n\\widehat{\\text{Wynik}} = 60 + 4 \\times \\text{Godziny nauki}\n5 godzin → 60 + 4 \\cdot 5 = 80 punktów.\n\n\nWielkość efektu i istotność praktyczna\nIstotność statystyczna mówi, czy efekt istnieje; istotność praktyczna — czy ma znaczenie. Potrzebujemy obu.\n\nSurowa wielkość efektu\nTo po prostu nachylenie \\hat{\\beta}_1.\nPrzykład: \\hat{\\beta}_1 = 4 pkt/godz.\nCzy to „dużo”? Zależy od:\n\nSkali wyniku (4/100 = 4% vs 4/500 = 0,8%),\nKosztu interwencji (czy 1h nauki warta 4 pkt?),\nProgów decyzyjnych (czy 4 pkt zmienia ocenę?).\n\n\n\nStandaryzowana wielkość efektu\nUłatwia porównania między badaniami:\n\\beta_{\\text{std}} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y}\ngdzie s_X i s_Y to odchylenia standardowe X i Y.\nPrzykład: jeśli s_X = 2.5 h, s_Y = 12 pkt i \\hat{\\beta}_1 = 4:\n\\beta_{\\text{std}} = 4 \\cdot \\frac{2.5}{12} = 0.83\n„Przy wzroście X o 1 SD, Y rośnie o 0.83 SD”.\n\n\nWskazówki Cohena\nDla standaryzowanych współczynników:\n\nmały: |\\beta| \\approx 0.10 (~1% wariancji),\nśredni: |\\beta| \\approx 0.30 (~9%),\nduży: |\\beta| \\approx 0.50 (~25%).\n\nDla R^2:\n\nmały: R^2 \\approx 0.02,\nśredni: R^2 \\approx 0.13,\nduży: R^2 \\approx 0.26.\n\nUwaga: to ogólne progi — normy różnią się między dziedzinami.\n\n\nPrzedziały ufności dla wielkości efektu\nDla surowego współczynnika:\nCI = \\hat{\\beta}_1 \\pm t_{\\text{critical}} \\cdot SE(\\hat{\\beta}_1)\nJeśli 95% CI = [3.2, 4.8], mówimy: „Z 95% pewnością prawdziwy efekt mieści się między 3.2 a 4.8 pkt/godz.”\n\n\nOcena istotności praktycznej\nWeź pod uwagę:\n\nMinimalnie istotną różnicę (MID),\nIle trzeba zmienić X, by osiągnąć sensowną zmianę Y,\nKoszt-efektywność:\n\n\\text{Efektywność} = \\frac{\\text{Wielkość efektu}}{\\text{Koszt interwencji}}\nPrzykład:\nEfekt: 4 pkt/godz.\nPróg zaliczenia: 70; średnia: 68 → 30 min nauki może zmienić niezal na zal → istotne praktycznie.\n\n\n\nNiepewność\nSzacunki pochodzą z próby, nie z całej populacji → niepewność.\n\nSkąd niepewność?\n\nMasz 20 studentów, nie wszystkich,\nPróba może być nietypowa,\nPomiary nie są doskonałe (raportowanie godzin nauki).\n\n\n\nPrzedziały ufności\nZamiast „efekt to dokładnie 4”, mówimy:\n\n„Szacujemy 4”,\n„95% CI: [3.2, 4.8]“.\n\nZnaczenie: w wielu powtórzeniach 95% takich przedziałów zawiera prawdziwą wartość.\n\n\nTestowanie istnienia zależności\nPytamy: „Gdyby prawdziwie nie było związku, jak mało prawdopodobny byłby obserwowany wzorzec?”\n\np = 0.03: przy braku efektu tylko 3% szans na tak silny wzorzec „z przypadku”,\np = 0.40: wzorzec „mógłby się łatwo zdarzyć” bez efektu.\n\nReguła kciuka: p &lt; 0.05 → „statystycznie istotne”.\n\n\n\nGdy coś idzie źle: diagnostyka modelu\nSzybkie wizualizacje:\n\nWykres punktowy: czy związek jest mniej więcej liniowy?\nReszty vs. przewidywania: powinien być losowy obłok,\nOutliery: punkty bardzo odległe?\n\nSygnały ostrzegawcze:\n\nWzorzec w resztach → brak liniowości lub zmienna pominięta,\nRozszerzający się wachlarz reszt → heteroskedastyczność,\nWpływowe obserwacje (influential) ciągną prostą,\nPominięte zmienne → obciążenia (bias).\n\n\n\nZałożenia: kiedy OLS działa dobrze\n\nLiniowość (linearity) — zależność w przybliżeniu prosta,\nNiezależność (independence) — obserwacje od siebie niezależne,\nStała wariancja (homoskedastyczność) — rozrzut reszt podobny w całym zakresie,\nBrak doskonałej współliniowości (no perfect multicollinearity) — w regresji wielorakiej predyktory nie są liniowo zależne „na 100%“,\nLosowy dobór próby (random sampling) — dane reprezentatywne.\n\n\n\nPodsumowanie\nCo robi OLS:\n\nDopasowuje prostą minimalizującą SSE,\nSzacuje, o ile średnio zmienia się Y przy zmianie X o 1,\nPodaje R^2 — ile zmienności wyjaśniamy,\nKwantyfikuje niepewność (SE, CI, p-values).\n\nKroki praktyczne:\n\nWykres danych — czy linia ma sens?\nUruchom OLS → \\hat{\\beta}_0, \\hat{\\beta}_1,\nSprawdź R^2,\nOblicz wielkości efektu (surową i standaryzowaną),\nOceń istotność praktyczną,\nSprawdź przedziały ufności,\nObejrzyj reszty,\nDecyduj, łącząc istotność statystyczną i praktyczną.\n\n\n\nKluczowe interpretacje / Key interpretations\nDomyślny model: regresja OLS Y=\\beta_0+\\beta_1 X+\\varepsilon (lub wieloraka: Y=\\beta_0+\\beta_1 X_1+\\cdots+\\beta_p X_p+\\varepsilon).\n\nNachylenie / Slope (\\beta_1) PL: Przy wzroście X o 1 jednostkę (ceteris paribus), przeciętna wartość Y zmienia się o \\beta_1 jednostek. ENG: When X increases by 1 unit (ceteris paribus), the expected value of Y changes by \\beta_1 units.\nStandaryzowane nachylenie / Standardized slope \\big(\\beta_{1}^{(\\mathrm{std})}\\big) Definicja:\n\n\\beta_{1}^{(\\mathrm{std})} \\;=\\; \\beta_1 \\cdot \\frac{s_X}{s_Y},\n\ngdzie s_X i s_Y to odchylenia standardowe X i Y. PL: Przy wzroście X o 1 odchylenie standardowe (SD), przeciętna wartość Y zmienia się o \\beta_{1}^{(\\mathrm{std})} odchyleń standardowych Y. ENG: For a 1 standard deviation (SD) increase in X, the expected value of Y changes by \\beta_{1}^{(\\mathrm{std})} SDs of Y. Uwaga/Note: W regresji prostej \\beta_{1}^{(\\mathrm{std})} = r (Pearson). / In simple regression, \\beta_{1}^{(\\mathrm{std})} = r (Pearson).\nWspółczynnik determinacji / R^2 Definicja:\n\nR^2 \\;=\\; 1 - \\frac{SS_{\\mathrm{res}}}{SS_{\\mathrm{tot}}}.\n\nPL: Model wyjaśnia 100\\times R^2% zmienności Y względem modelu tylko z wyrazem wolnym (in-sample). ENG: The model explains 100\\times R^2% of the variance in Y relative to the intercept-only model (in-sample). W wielu zmiennych rozważ: \\text{adjusted } R^2. / With multiple predictors consider: adjusted R^2.\nWartość p / P-value Formalnie/Formally:\n\np \\;=\\; \\Pr\\!\\big(\\,|T|\\ge |t_{\\mathrm{obs}}| \\mid H_0\\,\\big),\n\ngdzie T ma rozkład t przy H_0. PL: Zakładając prawdziwość H_0 i spełnione założenia modelu, prawdopodobieństwo uzyskania co najmniej tak ekstremalnej statystyki jak obserwowana wynosi p. ENG: Assuming H_0 and the model assumptions hold, p is the probability of observing a test statistic at least as extreme as the one obtained.\nPrzedział ufności / Confidence interval (np. dla \\beta_1) Konstrukcja/Construction:\n\n\\hat{\\beta}_1 \\;\\pm\\; t_{1-\\alpha/2,\\ \\mathrm{df}} \\cdot \\mathrm{SE}\\!\\left(\\hat{\\beta}_1\\right).\n\nPL (ściśle): W długiej serii powtórzeń 95% tak skonstruowanych przedziałów zawiera prawdziwą wartość \\beta_1; dla naszych danych oszacowanie mieści się w [\\text{lower},\\ \\text{upper}]. ENG (strict): Over many repetitions, 95% of such intervals would contain the true \\beta_1; for our data, the estimate lies within [\\text{lower},\\ \\text{upper}]. PL (skrót dydaktyczny): „Jesteśmy 95% pewni, że \\beta_1 leży w [\\text{lower},\\ \\text{upper}].” ENG (teaching shorthand): “We are 95% confident that \\beta_1 lies in [\\text{lower},\\ \\text{upper}].”\n\n\nNajczęstsze nieporozumienia / Common pitfalls\n\nPL: p nie jest prawdopodobieństwem, że H_0 jest prawdziwa. ENG: p is not the probability that H_0 is true.\nPL: 95% CI nie zawiera 95% obserwacji (od tego jest przedział predykcji). ENG: A 95% CI does not contain 95% of observations (that’s a prediction interval).\nPL/ENG: Wysokie R^2 ≠ przyczynowość / High R^2 ≠ causality. Zawsze sprawdzaj/Always check diagnozy reszt, skalę efektu, i dopasowanie poza próbą.\n\nPamiętaj:\n\nAsocjacja ≠ przyczynowość,\nIstotność statystyczna ≠ istotność praktyczna,\n„Każdy model jest błędny, niektóre są użyteczne”,\nZawsze wizualizuj dane i reszty,\nDecyzje opieraj na wielkości efektu i niepewności.\n\nOLS dostarcza uporządkowany, matematyczny sposób znajdowania wzorców w danych. Nie daje doskonałych prognoz, ale zapewnia najlepszą liniową aproksymację wraz z uczciwą oceną jej jakości i niepewności.\n\n\n\n14.12 Ręczne obliczenia OLS krok po kroku\nBadaczka chce zbadać zależność między godzinami nauki a wynikiem testu (6 studentów):\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nCelem jest wyznaczyć \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X metodą OLS.\n\nKrok 1: Średnie\nDla X:\n\\bar{X} = \\frac{1+2+3+4+5+6}{6} = \\frac{21}{6} = 3.5\nDla Y:\n\\bar{Y} = \\frac{65+70+75+85+88+95}{6} = \\frac{478}{6} = 79.67\n\n\nKrok 2: Odchylenia od średnich\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n−2.5\n−14.67\n\n\nB\n2\n70\n−1.5\n−9.67\n\n\nC\n3\n75\n−0.5\n−4.67\n\n\nD\n4\n85\n0.5\n5.33\n\n\nE\n5\n88\n1.5\n8.33\n\n\nF\n6\n95\n2.5\n15.33\n\n\n\n\n\nKrok 3: Iloczyny i kwadraty\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n36.68\n6.25\n\n\nB\n14.51\n2.25\n\n\nC\n2.34\n0.25\n\n\nD\n2.67\n0.25\n\n\nE\n12.50\n2.25\n\n\nF\n38.33\n6.25\n\n\nSuma\n107.03\n17.50\n\n\n\n\n\nKrok 4: Nachylenie \\hat{\\beta}_1\n\\hat{\\beta}_1 = \\frac{107.03}{17.50} = 6.12\nInterpretacja: +6.12 punktu za każdą dodatkową godzinę nauki.\n\n\nKrok 5: Wyraz wolny \\hat{\\beta}_0\n\\hat{\\beta}_0 = 79.67 - 6.12 \\cdot 3.5 = 58.25\nInterpretacja: przy 0 godzinach przewidujemy 58.25.\n\n\nKrok 6: Równanie regresji\n\\hat{Y} = 58.25 + 6.12 X\n\n\nKrok 7: Predykcje i reszty\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nReszta e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n64.37\n0.63\n\n\nB\n2\n70\n70.49\n−0.49\n\n\nC\n3\n75\n76.61\n−1.61\n\n\nD\n4\n85\n82.73\n2.27\n\n\nE\n5\n88\n88.85\n−0.85\n\n\nF\n6\n95\n94.97\n0.03\n\n\n\nKontrola: suma reszt ≈ 0 ✓\n\n\nKrok 8: Sumy kwadratów\nSST — całkowita:\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n215.21\n\n\nB\n70\n93.51\n\n\nC\n75\n21.81\n\n\nD\n85\n28.41\n\n\nE\n88\n69.39\n\n\nF\n95\n235.01\n\n\nSuma\n\nSST = 663.34\n\n\n\nSSR — wyjaśniona:\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n234.09\n\n\nB\n70.49\n84.27\n\n\nC\n76.61\n9.36\n\n\nD\n82.73\n9.36\n\n\nE\n88.85\n84.27\n\n\nF\n94.97\n234.09\n\n\nSuma\n\nSSR = 655.44\n\n\n\nSSE — błąd:\n\n\n\nStudent\ne_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n−0.49\n0.24\n\n\nC\n−1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n−0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSuma\n\nSSE = 9.10\n\n\n\nWeryfikacja: SST \\approx SSR + SSE\n663.34 \\approx 655.44 + 9.10 = 664.54 (drobne różnice zaokrągleń).\n\n\nKrok 9: R^2\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternatywnie:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 0.986\nInterpretacja: ~98.8% zmienności wyników wyjaśniają godziny nauki — bardzo silny związek.\n\n\nKrok 10: Wielkości efektu\nSurowa: 6.12 pkt/godz.\nStandaryzowana:\n\ns_X = \\sqrt{17.50/5} = 1.87,\ns_Y = \\sqrt{663.34/5} = 11.52,\n\\beta_{\\text{std}} = 6.12 \\cdot (1.87/11.52) = 0.99 → bardzo duży efekt (wg Cohena).\n\n\n\nKrok 11: Istotność praktyczna\n\nSkala: 6.12% na 100-punktowej skali / godz.,\nProgi: zmiana oceny (10 pkt) ≈ 1.63 h,\nKoszt-efekt: korzystny — sensowna inwestycja czasu.\n\n\n\nPodsumowanie wyników\n\nRównanie: \\hat{Y} = 58.25 + 6.12 X\nNachylenie: 6.12 pkt/godz.\nWyraz wolny: 58.25 pkt\nR^2: 0.988\n\\beta_{\\text{std}}: 0.99\n\nW praktyce: każda godzina nauki to ≈ +6 pkt; dopasowanie znakomite; efekt istotny statystycznie i praktycznie.\n\n\nKontrola wyniku\nSprawdź, że linia przechodzi przez (\\bar{X}, \\bar{Y}):\n58.25 + 6.12 \\cdot 3.5 = 79.67 = \\bar{Y} ✓\n\n\n\n14.13 Kod R do weryfikacji obliczeń\n\n# Krok 1: Dane\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)      # X\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95) # Y\nn &lt;- length(study_hours)\n\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Krok 2: Średnie\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Krok 3: Odchylenia od średnich\ndata$x_dev &lt;- data$X - x_bar\ndata$y_dev &lt;- data$Y - y_bar\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Krok 4: Iloczyny i kwadraty\ndata$xy_product &lt;- data$x_dev * data$y_dev\ndata$x_dev_sq &lt;- data$x_dev^2\n\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - X̄)(Yi - Ȳ):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - X̄)(Yi - Ȳ): 107 \n\ncat(\"Sum of (Xi - X̄)²:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - X̄)²: 17.5 \n\n# Krok 5: Nachylenie (beta_1)\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (β₁) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (β₁) calculated manually: 6.11 \n\n# Krok 6: Wyraz wolny (beta_0)\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (β₀) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (β₀) calculated manually: 58.27 \n\n# Krok 7: Porównanie z lm()\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Krok 8: Predykcje i reszty\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X\ndata$residual &lt;- data$Y - data$Y_hat\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ≈ 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ≈ 0): 0 \n\n# Krok 9: Sumy kwadratów\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Krok 10: R-kwadrat\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R² (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR² (Method 1: SSR/SST): 0.9863 \n\ncat(\"R² (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR² (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R² (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR² (from lm function): 0.9863 \n\n# Krok 11: Wielkości efektu\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\nsd_x &lt;- sd(data$X)\nsd_y &lt;- sd(data$Y)\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Korelacja (dla regresji prostej |r| = sqrt(R^2))\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R²:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R²: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n\n\n# Krok 12: Wizualizacja\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5,\n     xlim = c(0, 7), ylim = c(60, 100))\n\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Punkt średnich\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Reszty jako odcinki pionowe\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# Równanie na wykresie\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R² = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\nAnaliza regresji OLS - godziny nauki vs. wynik testu\n\n\n\n# Podsumowanie końcowe\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )\n\n\n\n\n14.14 Jak uruchomić kod\n\nSkopiuj cały blok kodu,\nWklej do RStudio,\nUruchom chunk po chunk lub cały dokument,\nPorównaj wyniki z obliczeniami ręcznymi.\n\nCo zobaczysz:\n\nNachylenie: 6.12,\nWyraz wolny: 58.25,\nR^2: ≈ 0.988,\nEfekt standaryzowany: ≈ 0.99,\nWykres z punktami, linią regresji i resztami.\n\nTo potwierdza poprawność obliczeń manualnych.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#ręczne-obliczenia-ols-krok-po-kroku",
    "href": "correg_pl.html#ręczne-obliczenia-ols-krok-po-kroku",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.12 Ręczne obliczenia OLS krok po kroku",
    "text": "14.12 Ręczne obliczenia OLS krok po kroku\nBadaczka chce zbadać zależność między godzinami nauki a wynikiem testu (6 studentów):\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nCelem jest wyznaczyć \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X metodą OLS.\n\nKrok 1: Średnie\nDla X:\n\\bar{X} = \\frac{1+2+3+4+5+6}{6} = \\frac{21}{6} = 3.5\nDla Y:\n\\bar{Y} = \\frac{65+70+75+85+88+95}{6} = \\frac{478}{6} = 79.67\n\n\nKrok 2: Odchylenia od średnich\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n−2.5\n−14.67\n\n\nB\n2\n70\n−1.5\n−9.67\n\n\nC\n3\n75\n−0.5\n−4.67\n\n\nD\n4\n85\n0.5\n5.33\n\n\nE\n5\n88\n1.5\n8.33\n\n\nF\n6\n95\n2.5\n15.33\n\n\n\n\n\nKrok 3: Iloczyny i kwadraty\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n36.68\n6.25\n\n\nB\n14.51\n2.25\n\n\nC\n2.34\n0.25\n\n\nD\n2.67\n0.25\n\n\nE\n12.50\n2.25\n\n\nF\n38.33\n6.25\n\n\nSuma\n107.03\n17.50\n\n\n\n\n\nKrok 4: Nachylenie \\hat{\\beta}_1\n\\hat{\\beta}_1 = \\frac{107.03}{17.50} = 6.12\nInterpretacja: +6.12 punktu za każdą dodatkową godzinę nauki.\n\n\nKrok 5: Wyraz wolny \\hat{\\beta}_0\n\\hat{\\beta}_0 = 79.67 - 6.12 \\cdot 3.5 = 58.25\nInterpretacja: przy 0 godzinach przewidujemy 58.25.\n\n\nKrok 6: Równanie regresji\n\\hat{Y} = 58.25 + 6.12 X\n\n\nKrok 7: Predykcje i reszty\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nReszta e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n64.37\n0.63\n\n\nB\n2\n70\n70.49\n−0.49\n\n\nC\n3\n75\n76.61\n−1.61\n\n\nD\n4\n85\n82.73\n2.27\n\n\nE\n5\n88\n88.85\n−0.85\n\n\nF\n6\n95\n94.97\n0.03\n\n\n\nKontrola: suma reszt ≈ 0 ✓\n\n\nKrok 8: Sumy kwadratów\nSST — całkowita:\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n215.21\n\n\nB\n70\n93.51\n\n\nC\n75\n21.81\n\n\nD\n85\n28.41\n\n\nE\n88\n69.39\n\n\nF\n95\n235.01\n\n\nSuma\n\nSST = 663.34\n\n\n\nSSR — wyjaśniona:\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n234.09\n\n\nB\n70.49\n84.27\n\n\nC\n76.61\n9.36\n\n\nD\n82.73\n9.36\n\n\nE\n88.85\n84.27\n\n\nF\n94.97\n234.09\n\n\nSuma\n\nSSR = 655.44\n\n\n\nSSE — błąd:\n\n\n\nStudent\ne_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n−0.49\n0.24\n\n\nC\n−1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n−0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSuma\n\nSSE = 9.10\n\n\n\nWeryfikacja: SST \\approx SSR + SSE\n663.34 \\approx 655.44 + 9.10 = 664.54 (drobne różnice zaokrągleń).\n\n\nKrok 9: R^2\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternatywnie:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 0.986\nInterpretacja: ~98.8% zmienności wyników wyjaśniają godziny nauki — bardzo silny związek.\n\n\nKrok 10: Wielkości efektu\nSurowa: 6.12 pkt/godz.\nStandaryzowana:\n\ns_X = \\sqrt{17.50/5} = 1.87,\ns_Y = \\sqrt{663.34/5} = 11.52,\n\\beta_{\\text{std}} = 6.12 \\cdot (1.87/11.52) = 0.99 → bardzo duży efekt (wg Cohena).\n\n\n\nKrok 11: Istotność praktyczna\n\nSkala: 6.12% na 100-punktowej skali / godz.,\nProgi: zmiana oceny (10 pkt) ≈ 1.63 h,\nKoszt-efekt: korzystny — sensowna inwestycja czasu.\n\n\n\nPodsumowanie wyników\n\nRównanie: \\hat{Y} = 58.25 + 6.12 X\nNachylenie: 6.12 pkt/godz.\nWyraz wolny: 58.25 pkt\nR^2: 0.988\n\\beta_{\\text{std}}: 0.99\n\nW praktyce: każda godzina nauki to ≈ +6 pkt; dopasowanie znakomite; efekt istotny statystycznie i praktycznie.\n\n\nKontrola wyniku\nSprawdź, że linia przechodzi przez (\\bar{X}, \\bar{Y}):\n58.25 + 6.12 \\cdot 3.5 = 79.67 = \\bar{Y} ✓",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kod-r-do-weryfikacji-obliczeń",
    "href": "correg_pl.html#kod-r-do-weryfikacji-obliczeń",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.13 Kod R do weryfikacji obliczeń",
    "text": "14.13 Kod R do weryfikacji obliczeń\n\n# Krok 1: Dane\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)      # X\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95) # Y\nn &lt;- length(study_hours)\n\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Krok 2: Średnie\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Krok 3: Odchylenia od średnich\ndata$x_dev &lt;- data$X - x_bar\ndata$y_dev &lt;- data$Y - y_bar\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Krok 4: Iloczyny i kwadraty\ndata$xy_product &lt;- data$x_dev * data$y_dev\ndata$x_dev_sq &lt;- data$x_dev^2\n\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - X̄)(Yi - Ȳ):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - X̄)(Yi - Ȳ): 107 \n\ncat(\"Sum of (Xi - X̄)²:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - X̄)²: 17.5 \n\n# Krok 5: Nachylenie (beta_1)\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (β₁) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (β₁) calculated manually: 6.11 \n\n# Krok 6: Wyraz wolny (beta_0)\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (β₀) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (β₀) calculated manually: 58.27 \n\n# Krok 7: Porównanie z lm()\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Krok 8: Predykcje i reszty\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X\ndata$residual &lt;- data$Y - data$Y_hat\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ≈ 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ≈ 0): 0 \n\n# Krok 9: Sumy kwadratów\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Krok 10: R-kwadrat\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R² (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR² (Method 1: SSR/SST): 0.9863 \n\ncat(\"R² (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR² (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R² (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR² (from lm function): 0.9863 \n\n# Krok 11: Wielkości efektu\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\nsd_x &lt;- sd(data$X)\nsd_y &lt;- sd(data$Y)\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Korelacja (dla regresji prostej |r| = sqrt(R^2))\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R²:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R²: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n\n\n# Krok 12: Wizualizacja\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5,\n     xlim = c(0, 7), ylim = c(60, 100))\n\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Punkt średnich\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Reszty jako odcinki pionowe\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# Równanie na wykresie\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R² = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\nAnaliza regresji OLS - godziny nauki vs. wynik testu\n\n\n\n# Podsumowanie końcowe\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#jak-uruchomić-kod",
    "href": "correg_pl.html#jak-uruchomić-kod",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.14 Jak uruchomić kod",
    "text": "14.14 Jak uruchomić kod\n\nSkopiuj cały blok kodu,\nWklej do RStudio,\nUruchom chunk po chunk lub cały dokument,\nPorównaj wyniki z obliczeniami ręcznymi.\n\nCo zobaczysz:\n\nNachylenie: 6.12,\nWyraz wolny: 58.25,\nR^2: ≈ 0.988,\nEfekt standaryzowany: ≈ 0.99,\nWykres z punktami, linią regresji i resztami.\n\nTo potwierdza poprawność obliczeń manualnych.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#appendix-a-obliczanie-kowariancji-korelacji-pearsona-i-spearmana-oraz-modelowanie-ols---przykład-wprowadzający",
    "href": "correg_pl.html#appendix-a-obliczanie-kowariancji-korelacji-pearsona-i-spearmana-oraz-modelowanie-ols---przykład-wprowadzający",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.15 Appendix A: Obliczanie Kowariancji, Korelacji Pearsona i Spearmana, oraz modelowanie OLS - przykład wprowadzający",
    "text": "14.15 Appendix A: Obliczanie Kowariancji, Korelacji Pearsona i Spearmana, oraz modelowanie OLS - przykład wprowadzający\nStudentka politologii bada związek między wielkością okręgu wyborczego (DM) a wskaźnikiem dysproporcjonalności Gallaghera (GH) w wyborach parlamentarnych w 10 losowo wybranych demokracjach.\nDane dotyczące wielkości okręgu wyborczego (\\text{DM}) i indeksu Gallaghera:\n\n\n\n\\text{DM} (X)\nGallagher (Y)\n\n\n\n\n2\n18,2\n\n\n3\n16,7\n\n\n4\n15,8\n\n\n5\n15,3\n\n\n6\n15,0\n\n\n7\n14,8\n\n\n8\n14,7\n\n\n9\n14,6\n\n\n10\n14,55\n\n\n11\n14,52\n\n\n\n\nKrok 1: Obliczanie Podstawowych Statystyk\nObliczanie średnich:\nDla \\text{DM} (X): \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\nSzczegółowe obliczenia:\n2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 65 \\bar{x} = \\frac{65}{10} = 6,5\nDla indeksu Gallaghera (Y): \\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\nSzczegółowe obliczenia:\n18,2 + 16,7 + 15,8 + 15,3 + 15,0 + 14,8 + 14,7 + 14,6 + 14,55 + 14,52 = 154,17 \\bar{y} = \\frac{154,17}{10} = 15,417\n\n\nKrok 2: Szczegółowe Obliczenia Kowariancji\nPełna tabela robocza ze wszystkimi obliczeniami:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18,2\n-4,5\n2,783\n-12,5235\n20,25\n7,7451\n\n\n2\n3\n16,7\n-3,5\n1,283\n-4,4905\n12,25\n1,6461\n\n\n3\n4\n15,8\n-2,5\n0,383\n-0,9575\n6,25\n0,1467\n\n\n4\n5\n15,3\n-1,5\n-0,117\n0,1755\n2,25\n0,0137\n\n\n5\n6\n15,0\n-0,5\n-0,417\n0,2085\n0,25\n0,1739\n\n\n6\n7\n14,8\n0,5\n-0,617\n-0,3085\n0,25\n0,3807\n\n\n7\n8\n14,7\n1,5\n-0,717\n-1,0755\n2,25\n0,5141\n\n\n8\n9\n14,6\n2,5\n-0,817\n-2,0425\n6,25\n0,6675\n\n\n9\n10\n14,55\n3,5\n-0,867\n-3,0345\n12,25\n0,7517\n\n\n10\n11\n14,52\n4,5\n-0,897\n-4,0365\n20,25\n0,8047\n\n\nSuma\n65\n154,17\n0\n0\n-28,085\n82,5\n12,8442\n\n\n\nObliczanie kowariancji: \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\text{Cov}(X,Y) = \\frac{-28,085}{9} = -3,120556\n\n\nKrok 3: Obliczanie Odchylenia Standardowego\nDla \\text{DM} (X): \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}}\n\\sigma_x = \\sqrt{\\frac{82,5}{9}} = \\sqrt{9,1667} = 3,026582\nDla Gallaghera (Y): \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}}\n\\sigma_y = \\sqrt{\\frac{12,8442}{9}} = \\sqrt{1,4271} = 1,194612\n\n\nKrok 4: Obliczanie Korelacji Pearsona\nr = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\nr = \\frac{-3,120556}{3,026582 \\times 1,194612} = \\frac{-3,120556}{3,615752} = -0,863044\n\n\nKrok 5: Obliczanie Korelacji Rangowej Spearmana\nPełna tabela rangowa ze wszystkimi obliczeniami:\n\n\n\ni\nX_i\nY_i\nRanga X_i\nRanga Y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n18,2\n1\n10\n-9\n81\n\n\n2\n3\n16,7\n2\n9\n-7\n49\n\n\n3\n4\n15,8\n3\n8\n-5\n25\n\n\n4\n5\n15,3\n4\n7\n-3\n9\n\n\n5\n6\n15,0\n5\n6\n-1\n1\n\n\n6\n7\n14,8\n6\n5\n1\n1\n\n\n7\n8\n14,7\n7\n4\n3\n9\n\n\n8\n9\n14,6\n8\n3\n5\n25\n\n\n9\n10\n14,55\n9\n2\n7\n49\n\n\n10\n11\n14,52\n10\n1\n9\n81\n\n\nSuma\n\n\n\n\n\n330\n\n\n\nObliczanie korelacji Spearmana: \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n\\rho = 1 - \\frac{6 \\times 330}{10(100 - 1)} = 1 - \\frac{1980}{990} = 1 - 2 = -1\n\n\nKrok 6: Weryfikacja w R\n\n# Tworzenie wektorów\nDM &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nGH &lt;- c(18.2, 16.7, 15.8, 15.3, 15.0, 14.8, 14.7, 14.6, 14.55, 14.52)\n\n# Obliczanie kowariancji\ncov(DM, GH)\n\n[1] -3.120556\n\n# Obliczanie korelacji\ncor(DM, GH, method = \"pearson\")\n\n[1] -0.8627742\n\ncor(DM, GH, method = \"spearman\")\n\n[1] -1\n\n\n\n\nKrok 7: Podstawowa Wizualizacja\n\nlibrary(ggplot2)\n\n# Tworzenie ramki danych\ndata &lt;- data.frame(DM = DM, GH = GH)\n\n# Tworzenie wykresu rozrzutu\nggplot(data, aes(x = DM, y = GH)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Wielkość Okręgu vs Indeks Gallaghera\",\n    x = \"Wielkość Okręgu (DM)\",\n    y = \"Indeks Gallaghera (GH)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nEstymacja OLS i Miary Dopasowania Modelu\n\n\nKrok 1: Obliczanie Estymatorów OLS\nKorzystając z wcześniej obliczonych wartości:\n\n\\sum(X_i - \\bar{X})(Y_i - \\bar{Y}) = -28,085\n\\sum(X_i - \\bar{X})^2 = 82,5\n\\bar{X} = 6,5\n\\bar{Y} = 15,417\n\nObliczanie nachylenia (\\hat{\\beta_1}):\n\\hat{\\beta_1} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\hat{\\beta_1} = -28,085 ÷ 82,5 = -0,3404\nObliczanie wyrazu wolnego (\\hat{\\beta_0}): \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 15,417 - (-0,3404 × 6,5)\n   = 15,417 + 2,2126\n   = 17,6296\nZatem równanie regresji OLS ma postać: \\hat{Y} = 17,6296 - 0,3404X\n\n\nKrok 2: Obliczanie Wartości Dopasowanych i Reszt\nPełna tabela ze wszystkimi obliczeniami:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i\ne_i = Y_i - \\hat{Y}_i\ne_i^2\n(Y_i - \\bar{Y})^2\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18,2\n16,9488\n1,2512\n1,5655\n7,7451\n2,3404\n\n\n2\n3\n16,7\n16,6084\n0,0916\n0,0084\n1,6461\n1,4241\n\n\n3\n4\n15,8\n16,2680\n-0,4680\n0,2190\n0,1467\n0,7225\n\n\n4\n5\n15,3\n15,9276\n-0,6276\n0,3939\n0,0137\n0,2601\n\n\n5\n6\n15,0\n15,5872\n-0,5872\n0,3448\n0,1739\n0,0289\n\n\n6\n7\n14,8\n15,2468\n-0,4468\n0,1996\n0,3807\n0,0290\n\n\n7\n8\n14,7\n14,9064\n-0,2064\n0,0426\n0,5141\n0,2610\n\n\n8\n9\n14,6\n14,5660\n0,0340\n0,0012\n0,6675\n0,7241\n\n\n9\n10\n14,55\n14,2256\n0,3244\n0,1052\n0,7517\n1,4184\n\n\n10\n11\n14,52\n13,8852\n0,6348\n0,4030\n0,8047\n2,3439\n\n\nSuma\n65\n154,17\n154,17\n0\n3,2832\n12,8442\n9,5524\n\n\n\nObliczenia dla wartości dopasowanych:\nDla X = 2:\nŶ = 17,6296 + (-0,3404 × 2) = 16,9488\n\nDla X = 3:\nŶ = 17,6296 + (-0,3404 × 3) = 16,6084\n\n[... kontynuacja dla wszystkich wartości]\n\n\nKrok 3: Obliczanie Miar Dopasowania\nSuma kwadratów reszt (SSE): SSE = \\sum e_i^2\nSSE = 3,2832\nCałkowita suma kwadratów (SST): SST = \\sum(Y_i - \\bar{Y})^2\nSST = 12,8442\nSuma kwadratów regresji (SSR): SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\nSSR = 9,5524\nWeryfikacja dekompozycji: SST = SSR + SSE\n12,8442 = 9,5524 + 3,2832 (w granicach błędu zaokrąglenia)\nObliczanie współczynnika determinacji R-kwadrat: R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nR² = 9,5524 ÷ 12,8442\n   = 0,7438\n\n\nKrok 4: Weryfikacja w R\n\n# Dopasowanie modelu liniowego\nmodel &lt;- lm(GH ~ DM, data = data)\n\n# Podsumowanie statystyk\nsummary(model)\n\n\nCall:\nlm(formula = GH ~ DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62764 -0.46274 -0.08615  0.26624  1.25109 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept) 17.62976    0.50121  35.174 0.000000000467 ***\nDM          -0.34042    0.07053  -4.827        0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6406 on 8 degrees of freedom\nMultiple R-squared:  0.7444,    Adjusted R-squared:  0.7124 \nF-statistic:  23.3 on 1 and 8 DF,  p-value: 0.00131\n\n# Ręczne obliczenie R-kwadrat\nSST &lt;- sum((GH - mean(GH))^2)\nSSE &lt;- sum(residuals(model)^2)\nSSR &lt;- SST - SSE\nR2_manual &lt;- SSR/SST\nR2_manual\n\n[1] 0.7443793\n\n\n\n\nKrok 5: Analiza Reszt\n\n# Tworzenie wykresów reszt\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\nKrok 6: Wykres Wartości Przewidywanych vs Rzeczywistych\n\n# Tworzenie wykresu wartości przewidywanych vs rzeczywistych\nggplot(data.frame(\n  Rzeczywiste = GH,\n  Przewidywane = fitted(model)\n), aes(x = Przewidywane, y = Rzeczywiste)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Wartości Przewidywane vs Rzeczywiste\",\n    x = \"Przewidywany Indeks Gallaghera\",\n    y = \"Rzeczywisty Indeks Gallaghera\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nModele z Transformacją Logarytmiczną\n\n\nKrok 1: Transformacja Danych\nNajpierw obliczamy logarytmy naturalne zmiennych:\n\n\n\ni\nX_i\nY_i\n\\ln(X_i)\n\\ln(Y_i)\n\n\n\n\n1\n2\n18,2\n0,6931\n2,9014\n\n\n2\n3\n16,7\n1,0986\n2,8154\n\n\n3\n4\n15,8\n1,3863\n2,7600\n\n\n4\n5\n15,3\n1,6094\n2,7278\n\n\n5\n6\n15,0\n1,7918\n2,7081\n\n\n6\n7\n14,8\n1,9459\n2,6946\n\n\n7\n8\n14,7\n2,0794\n2,6878\n\n\n8\n9\n14,6\n2,1972\n2,6810\n\n\n9\n10\n14,55\n2,3026\n2,6777\n\n\n10\n11\n14,52\n2,3979\n2,6757\n\n\n\n\n\nKrok 2: Porównanie Różnych Specyfikacji Modelu\nSzacujemy trzy alternatywne specyfikacje:\n\nModel log-liniowy: \\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nModel liniowo-logarytmiczny: Y_i = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\nModel log-log: \\ln(Y_i) = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\n\n\n# Tworzenie zmiennych transformowanych\ndata$log_DM &lt;- log(data$DM)\ndata$log_GH &lt;- log(data$GH)\n\n# Dopasowanie modeli\nmodel_linear &lt;- lm(GH ~ DM, data = data)\nmodel_loglinear &lt;- lm(log_GH ~ DM, data = data)\nmodel_linearlog &lt;- lm(GH ~ log_DM, data = data)\nmodel_loglog &lt;- lm(log_GH ~ log_DM, data = data)\n\n# Porównanie wartości R-kwadrat\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Liniowy\", \"Log-liniowy\", \"Liniowo-logarytmiczny\", \"Log-log\"),\n  R_kwadrat = c(\n    summary(model_linear)$r.squared,\n    summary(model_loglinear)$r.squared,\n    summary(model_linearlog)$r.squared,\n    summary(model_loglog)$r.squared\n  )\n)\n\n# Wyświetlenie porównania\nmodels_comparison\n\n                  Model R_kwadrat\n1               Liniowy 0.7443793\n2           Log-liniowy 0.7670346\n3 Liniowo-logarytmiczny 0.9141560\n4               Log-log 0.9288088\n\n\n\n\nKrok 3: Porównanie Wizualne\n\n# Tworzenie wykresów dla każdego modelu\np1 &lt;- ggplot(data, aes(x = DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Liniowy\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Log-liniowy\") +\n  theme_minimal()\n\np3 &lt;- ggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Liniowo-logarytmiczny\") +\n  theme_minimal()\n\np4 &lt;- ggplot(data, aes(x = log_DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Log-log\") +\n  theme_minimal()\n\n# Układanie wykresów w siatkę\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nKrok 4: Analiza Reszt dla Najlepszego Modelu\nNa podstawie wartości R-kwadrat, analiza reszt dla najlepiej dopasowanego modelu:\n\n# Wykresy reszt dla najlepszego modelu\npar(mfrow = c(2, 2))\nplot(model_linearlog)\n\n\n\n\n\n\n\n\n\n\nKrok 5: Interpretacja Najlepszego Modelu\nWspółczynniki modelu liniowo-logarytmicznego:\n\nsummary(model_linearlog)\n\n\nCall:\nlm(formula = GH ~ log_DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40702 -0.30207 -0.04907  0.22905  0.60549 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)  19.0223     0.4079   46.64 0.0000000000494 ***\nlog_DM       -2.0599     0.2232   -9.23 0.0000153880425 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3712 on 8 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9034 \nF-statistic: 85.19 on 1 and 8 DF,  p-value: 0.00001539\n\n\nInterpretacja:\n\n\\hat{\\beta_0} reprezentuje oczekiwany Indeks Gallaghera, gdy ln(DM) = 0 (czyli gdy DM = 1)\n\\hat{\\beta_1} reprezentuje zmianę Indeksu Gallaghera związaną z jednostkowym wzrostem ln(DM)\n\n\n\nKrok 6: Predykcje Modelu\n\n# Tworzenie wykresu predykcji dla najlepszego modelu\nggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Model Liniowo-logarytmiczny: Indeks Gallaghera vs ln(Wielkość Okręgu)\",\n    x = \"ln(Wielkość Okręgu)\",\n    y = \"Indeks Gallaghera\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nKrok 7: Analiza Elastyczności\nDla modelu log-log współczynniki bezpośrednio reprezentują elastyczności. Obliczenie średniej elastyczności dla modelu liniowo-logarytmicznego:\n\n# Obliczenie elastyczności przy wartościach średnich\nmean_DM &lt;- mean(data$DM)\nmean_GH &lt;- mean(data$GH)\nbeta1 &lt;- coef(model_linearlog)[2]\nelastycznosc &lt;- beta1 * (1/mean_GH)\nelastycznosc\n\n    log_DM \n-0.1336136 \n\n\nWartość ta reprezentuje procentową zmianę Indeksu Gallaghera przy jednoprocentowej zmianie Wielkości Okręgu.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#appendix-b.-prosty-model-regresji-liniowej-mnkols---przykłady-różne",
    "href": "correg_pl.html#appendix-b.-prosty-model-regresji-liniowej-mnkols---przykłady-różne",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.16 Appendix B. Prosty Model Regresji Liniowej (MNK/OLS) - przykłady różne",
    "text": "14.16 Appendix B. Prosty Model Regresji Liniowej (MNK/OLS) - przykłady różne",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przykład-1.-związek-między-wielkością-okręgu-a-dysproporcjonalnością-wyborczą-1",
    "href": "correg_pl.html#przykład-1.-związek-między-wielkością-okręgu-a-dysproporcjonalnością-wyborczą-1",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.17 Przykład 1. Związek Między Wielkością Okręgu a Dysproporcjonalnością Wyborczą (1)",
    "text": "14.17 Przykład 1. Związek Między Wielkością Okręgu a Dysproporcjonalnością Wyborczą (1)\nTa analiza bada związek między wielkością okręgu wyborczego (DM) a wskaźnikiem dysproporcjonalności Loosemore-Hanby (LH) w wyborach parlamentarnych w 6 krajach. Indeks Loosemore-Hanby mierzy dysproporcjonalność wyborczą, gdzie wyższe wartości wskazują na większą dysproporcjonalność między głosami a mandatami.\n\nDane\n\n\nWarning: package 'knitr' was built under R version 4.4.3\n\n\n\nWielkość Okręgu i Indeks LH według Kraju\n\n\nCountry\nDM\nLH\n\n\n\n\nA\n3\n15.50\n\n\nB\n4\n14.25\n\n\nC\n5\n13.50\n\n\nD\n6\n13.50\n\n\nE\n7\n13.00\n\n\nF\n8\n12.75\n\n\n\n\n\n\n\nKrok 1: Obliczenie wariancji i odchylenia standardowego dla DM i LH\n\nObliczenia dla Wielkości Okręgu (DM)\nNajpierw obliczam średnią wartości DM:\n\\bar{x}_{DM} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\frac{3 + 4 + 5 + 6 + 7 + 8}{6} = \\frac{33}{6} = 5.5\nNastępnie obliczam wariancję używając formuły z korektą Bessela:\n\\sigma^2_{DM} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\n\n\nKraj\nDM (x_i)\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\n\n\n\n\nA\n3\n3 - 5.5 = -2.5\n(-2.5)^2 = 6.25\n\n\nB\n4\n4 - 5.5 = -1.5\n(-1.5)^2 = 2.25\n\n\nC\n5\n5 - 5.5 = -0.5\n(-0.5)^2 = 0.25\n\n\nD\n6\n6 - 5.5 = 0.5\n(0.5)^2 = 0.25\n\n\nE\n7\n7 - 5.5 = 1.5\n(1.5)^2 = 2.25\n\n\nF\n8\n8 - 5.5 = 2.5\n(2.5)^2 = 6.25\n\n\nSuma\n\n\n17.5\n\n\n\n\\sigma^2_{DM} = \\frac{17.5}{6-1} = \\frac{17.5}{5} = 3.5\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji:\n\\sigma_{DM} = \\sqrt{\\sigma^2_{DM}} = \\sqrt{3.5} = 1.871\n\n\nObliczenia dla Indeksu LH\nNajpierw obliczam średnią wartości LH:\n\\bar{x}_{LH} = \\frac{15.5 + 14.25 + 13.5 + 13.5 + 13 + 12.75}{6} = \\frac{82.5}{6} = 13.75\nNastępnie obliczam wariancję:\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n15.5\n15.5 - 13.75 = 1.75\n(1.75)^2 = 3.0625\n\n\nB\n14.25\n14.25 - 13.75 = 0.5\n(0.5)^2 = 0.25\n\n\nC\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nD\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nE\n13\n13 - 13.75 = -0.75\n(-0.75)^2 = 0.5625\n\n\nF\n12.75\n12.75 - 13.75 = -1\n(-1)^2 = 1\n\n\nSuma\n\n\n5\n\n\n\n\\sigma^2_{LH} = \\frac{5}{6-1} = \\frac{5}{5} = 1\nOdchylenie standardowe wynosi:\n\\sigma_{LH} = \\sqrt{\\sigma^2_{LH}} = \\sqrt{1} = 1\nPodsumowanie Zadania 1:\n\nWariancja DM (z korektą Bessela): 3.5\nOdchylenie Standardowe DM: 1.871\nWariancja LH (z korektą Bessela): 1\nOdchylenie Standardowe LH: 1\n\n\n\n\nKrok 2: Obliczenie kowariancji między DM i LH dla tej próby danych\nKowariancja jest obliczana przy użyciu formuły z korektą Bessela:\nCov(DM, LH) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\nA\n3\n15.5\n-2.5\n1.75\n(-2.5)(1.75) = -4.375\n\n\nB\n4\n14.25\n-1.5\n0.5\n(-1.5)(0.5) = -0.75\n\n\nC\n5\n13.5\n-0.5\n-0.25\n(-0.5)(-0.25) = 0.125\n\n\nD\n6\n13.5\n0.5\n-0.25\n(0.5)(-0.25) = -0.125\n\n\nE\n7\n13\n1.5\n-0.75\n(1.5)(-0.75) = -1.125\n\n\nF\n8\n12.75\n2.5\n-1\n(2.5)(-1) = -2.5\n\n\nSuma\n\n\n\n\n-8.75\n\n\n\nCov(DM, LH) = \\frac{-8.75}{5} = -1.75\nKowariancja między DM i LH: -1.75\nUjemna kowariancja wskazuje na odwrotną zależność: gdy wielkość okręgu wzrasta, indeks dysproporcjonalności LH ma tendencję do spadku.\n\n\nKrok 3: Obliczenie współczynnika korelacji liniowej Pearsona między DM i LH\nWspółczynnik korelacji Pearsona obliczany jest przy użyciu formuły:\nr = \\frac{Cov(DM, LH)}{\\sigma_{DM} \\cdot \\sigma_{LH}}\nMamy już obliczone:\n\nCov(DM, LH) = -1.75\n\\sigma_{DM} = 1.871\n\\sigma_{LH} = 1\n\nr = \\frac{-1.75}{1.871 \\cdot 1} = \\frac{-1.75}{1.871} = -0.935\nWspółczynnik korelacji Pearsona: -0.935\n\nInterpretacja:\nWspółczynnik korelacji -0.935 wskazuje:\n\nKierunek: Znak ujemny pokazuje odwrotną zależność między wielkością okręgu a indeksem LH.\nSiła: Wartość bezwzględna 0.935 wskazuje na bardzo silną korelację (blisko -1).\nInterpretacja praktyczna: Ponieważ wyższe wartości indeksu LH wskazują na większą dysproporcjonalność, ta silna ujemna korelacja sugeruje, że gdy wielkość okręgu wzrasta, dysproporcjonalność wyborcza ma tendencję do znacznego spadku. Innymi słowy, systemy wyborcze z większymi okręgami (więcej przedstawicieli wybieranych z jednego okręgu) zwykle dają bardziej proporcjonalne wyniki (niższa dysproporcjonalność).\n\nOdkrycie to jest zgodne z teorią nauk politycznych, która sugeruje, że większe okręgi zapewniają więcej możliwości mniejszym partiom, aby uzyskać reprezentację, co prowadzi do wyników wyborczych, które lepiej odzwierciedlają rozkład głosów między partiami.\n\n\n\nKrok 4: Skonstruowanie modelu regresji liniowej prostej i obliczenie R-kwadrat\nFormuła dla regresji liniowej prostej:\nLH = \\beta_0 + \\beta_1 \\cdot DM + \\varepsilon\nGdzie:\n\n\\beta_0 to wyraz wolny (przecięcie)\n\\beta_1 to współczynnik nachylenia dla DM\n\nFormuła do obliczenia \\beta_1 to:\n\\beta_1 = \\frac{Cov(DM, LH)}{\\sigma^2_{DM}} = \\frac{-1.75}{3.5} = -0.5\nAby obliczyć \\beta_0, używam:\n\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x} = 13.75 - (-0.5) \\cdot 5.5 = 13.75 + 2.75 = 16.5\nZatem równanie regresji to:\nLH = 16.5 - 0.5 \\cdot DM\n\nObliczanie wartości przewidywanych i błędów\nUżywając naszego równania regresji, obliczam przewidywane wartości LH:\n\\hat{LH} = 16.5 - 0.5 \\cdot DM\n\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i = 16.5 - 0.5x_i)\nBłąd (y_i - \\hat{y}_i)\nBłąd bezwzględny (|y_i - \\hat{y}_i|)\nBłąd kwadratowy ([y_i - \\hat{y}_i]^2)\n\n\n\n\nA\n3\n15.5\n16.5 - 0.5(3) = 16.5 - 1.5 = 15\n15.5 - 15 = 0.5\n|0.5| = 0.5\n(0.5)^2 = 0.25\n\n\nB\n4\n14.25\n16.5 - 0.5(4) = 16.5 - 2 = 14.5\n14.25 - 14.5 = -0.25\n|-0.25| = 0.25\n(-0.25)^2 = 0.0625\n\n\nC\n5\n13.5\n16.5 - 0.5(5) = 16.5 - 2.5 = 14\n13.5 - 14 = -0.5\n|-0.5| = 0.5\n(-0.5)^2 = 0.25\n\n\nD\n6\n13.5\n16.5 - 0.5(6) = 16.5 - 3 = 13.5\n13.5 - 13.5 = 0\n|0| = 0\n(0)^2 = 0\n\n\nE\n7\n13\n16.5 - 0.5(7) = 16.5 - 3.5 = 13\n13 - 13 = 0\n|0| = 0\n(0)^2 = 0\n\n\nF\n8\n12.75\n16.5 - 0.5(8) = 16.5 - 4 = 12.5\n12.75 - 12.5 = 0.25\n|0.25| = 0.25\n(0.25)^2 = 0.0625\n\n\nSuma\n\n\n\n\n1.5\n0.625\n\n\n\n\n\nObliczanie R-kwadrat\n\nSST (Całkowita suma kwadratów)\n\nSST = \\sum_{i=1}^{n}(y_i - \\bar{y})^2\nObliczyliśmy już te odchylenia w Kroku 1:\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n15.5\n1.75\n3.0625\n\n\nB\n14.25\n0.5\n0.25\n\n\nC\n13.5\n-0.25\n0.0625\n\n\nD\n13.5\n-0.25\n0.0625\n\n\nE\n13\n-0.75\n0.5625\n\n\nF\n12.75\n-1\n1\n\n\nSuma\n\n\n5\n\n\n\nSST = 5\n\nSSR (Suma kwadratów regresji)\n\nSSR = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2\n\n\n\n\n\n\n\n\n\nKraj\nPrzewidywane LH (\\hat{y}_i)\n\\hat{y}_i - \\bar{y}\n(\\hat{y}_i - \\bar{y})^2\n\n\n\n\nA\n15\n15 - 13.75 = 1.25\n(1.25)^2 = 1.5625\n\n\nB\n14.5\n14.5 - 13.75 = 0.75\n(0.75)^2 = 0.5625\n\n\nC\n14\n14 - 13.75 = 0.25\n(0.25)^2 = 0.0625\n\n\nD\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nE\n13\n13 - 13.75 = -0.75\n(-0.75)^2 = 0.5625\n\n\nF\n12.5\n12.5 - 13.75 = -1.25\n(-1.25)^2 = 1.5625\n\n\nSuma\n\n\n4.375\n\n\n\nSSR = 4.375\n\nSSE (Suma kwadratów błędów)\n\nSSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\nZ tabeli powyżej, suma kwadratów błędów wynosi:\nSSE = 0.625\n\nWeryfikacja\n\nMożemy zweryfikować nasze obliczenia sprawdzając, czy SST = SSR + SSE:\n5 = 4.375 + 0.625 = 5 \\checkmark\n\nObliczanie R-kwadrat\n\nR^2 = \\frac{SSR}{SST} = \\frac{4.375}{5} = 0.875\n\n\nObliczanie RMSE (Root Mean Square Error)\nRMSE jest obliczane przy użyciu formuły:\nRMSE = \\sqrt{\\frac{SSE}{n}} = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}}\nUżywając naszego obliczonego SSE:\nRMSE = \\sqrt{\\frac{0.625}{6}} = \\sqrt{0.104} \\approx 0.323\n\n\nObliczanie MAE (Mean Absolute Error)\nMAE jest obliczane przy użyciu formuły:\nMAE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}\nUżywając sum z tabeli:\nMAE = \\frac{1.5}{6} = 0.25\nModel regresji: LH = 16.5 - 0.5 \\cdot DM\nR-kwadrat: 0.875\nRMSE: 0.323\nMAE: 0.25\n\n\nInterpretacja:\n\nRównanie regresji: Dla każdego wzrostu jednostkowego wielkości okręgu, indeks dysproporcjonalności LH jest oczekiwany spadek o 0.5 jednostki. Wyraz wolny (16.5) reprezentuje oczekiwany indeks LH, gdy wielkość okręgu wynosi zero (choć nie ma to praktycznego znaczenia, ponieważ wielkość okręgu nie może wynosić zero).\nR-kwadrat: 0.875 wskazuje, że około 87.5% wariancji w dysproporcjonalności wyborczej (indeks LH) może być wyjaśnione przez wielkość okręgu. Jest to wysoka wartość, sugerująca, że wielkość okręgu jest rzeczywiście silnym predyktorem dysproporcjonalności wyborczej.\nRMSE i MAE: Niskie wartości RMSE (0.323) i MAE (0.25) wskazują, że model dobrze dopasowuje się do danych, z małymi błędami predykcji.\nImplikacje polityczne: Odkrycia sugerują, że zwiększanie wielkości okręgu mogłoby być skuteczną strategią reformy wyborczej dla krajów dążących do zmniejszenia dysproporcjonalności między głosami a mandatami. Jednak korzyści marginalne wydają się zmniejszać wraz ze wzrostem wielkości okręgu, jak widać w wzorcu danych.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#example-2.-descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "href": "correg_pl.html#example-2.-descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.18 Example 2. Descriptive Statistics and OLS Example - Income and Voter Turnout",
    "text": "14.18 Example 2. Descriptive Statistics and OLS Example - Income and Voter Turnout\nBackground\nIn preparation for the 2024 municipal elections, the Amsterdam Electoral Commission conducted research on voter participation patterns across different city neighborhoods. A key question emerged:\nDoes economic prosperity of a neighborhood correlate with civic engagement, specifically voter turnout?\nData Collection\nSample: 5 representative neighborhoods in Amsterdam\nTime Period: Data from the 2022 municipal elections\nVariables:\n\nIncome: Average annual household income per capita (thousands €)\nTurnout: Percentage of registered voters who voted in the election\n\n\nInitial R Output for Reference\n\n# Data\nincome &lt;- c(50, 45, 56, 40, 60)  # thousands €\nturnout &lt;- c(60, 56, 70, 50, 75) # %\n\n# Full model check\nmodel &lt;- lm(turnout ~ income)\nsummary(model)\n\n\nCall:\nlm(formula = turnout ~ income)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \nincome       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n\n\n\nDispersion Measures\nMeans:\n\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n} = \\frac{50 + 45 + 56 + 40 + 60}{5} = \\frac{251}{5} = 50.2\n\\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n} = \\frac{60 + 56 + 70 + 50 + 75}{5} = \\frac{311}{5} = 62.2\n\n# Verification\nmean(income)  # 50.2\n\n[1] 50.2\n\nmean(turnout) # 62.2\n\n[1] 62.2\n\n\nVariances:\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X: (-0.2, -5.2, 5.8, -10.2, 9.8)\ns^2_X = \\frac{0.04 + 27.04 + 33.64 + 104.04 + 96.04}{4} = \\frac{260.8}{4} = 65.2\nDeviations for Y: (-2.2, -6.2, 7.8, -12.2, 12.8)\ns^2_Y = \\frac{4.84 + 38.44 + 60.84 + 148.84 + 163.84}{4} = \\frac{416.8}{4} = 104.2\n\n# Verification\nvar(income)  # 65.2\n\n[1] 65.2\n\nvar(turnout) # 104.2\n\n[1] 104.2\n\n\n\n\nCovariance and Correlation\nCovariance:\ns_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-0.2 \\times -2.2) = 0.44 (-5.2 \\times -6.2) = 32.24 (5.8 \\times 7.8) = 45.24 (-10.2 \\times -12.2) = 124.44 (9.8 \\times 12.8) = 125.44\ns_{XY} = \\frac{327.8}{4} = 81.95\n\n# Verification\ncov(income, turnout) # 81.95\n\n[1] 81.95\n\n\nCorrelation:\nr_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{81.95}{\\sqrt{65.2}\\sqrt{104.2}} = 0.994\n\n# Verification\ncor(income, turnout) # 0.994\n\n[1] 0.9942402\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{81.95}{65.2} = 1.2571429\nIntercept:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\nStep by step:\n\n1.2571429 \\times 50.2 = 63.1085714\n\\hat{\\beta_0} = 62.2 - 63.1085714 = -0.9085714\n\n\n# Verification\ncoef(model)  # Exact coefficients from R\n\n(Intercept)      income \n -0.8964724   1.2569018 \n\n\n\n\nDetailed Decomposition of Variance and R-squared\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = -0.9085714 + 1.2571429X\nThe predicted values \\hat{Y} for each X value:\nFor X = 50:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (50)\n \\hat{Y} = -0.9085714 + 62.857145 \\hat{Y} = 61.9485736\nFor X = 45:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (45)\n \\hat{Y} = -0.9085714 + 56.5714305 \\hat{Y} = 55.6535591\nFor X = 56:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (56)\n \\hat{Y} = -0.9085714 + 70.4200024 \\hat{Y} = 69.5114310\nFor X = 40:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (40)\n \\hat{Y} = -0.9085714 + 50.2657160 \\hat{Y} = 49.3571446\nFor X = 60:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (60)\n \\hat{Y} = -0.9085714 + 75.4285740 \\hat{Y} = 74.5200026\n\n# Verification of predicted values\ny_hat &lt;- -0.9085714 + 1.2571429 * income\ndata.frame(\n  X = income,\n  Y = turnout,\n  Y_hat = y_hat,\n  row.names = 1:5\n)\n\n   X  Y    Y_hat\n1 50 60 61.94857\n2 45 56 55.66286\n3 56 70 69.49143\n4 40 50 49.37714\n5 60 75 74.52000\n\n\nStep 2: Calculate SST (Total Sum of Squares)\nSST = \\sum(Y_i - \\bar{Y})^2 \\text{ where } \\bar{Y} = 62.2\n(60 - 62.2)^2 = (-2.2)^2 = 4.84 (56 - 62.2)^2 = (-6.2)^2 = 38.44 (70 - 62.2)^2 = (7.8)^2 = 60.84 (50 - 62.2)^2 = (-12.2)^2 = 148.84 (75 - 62.2)^2 = (12.8)^2 = 163.84\nSST = 4.84 + 38.44 + 60.84 + 148.84 + 163.84 = 416.8\nStep 3: Calculate SSR (Regression Sum of Squares)\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\n(61.9485736 - 62.2)^2 = (-0.2514264)^2 = 0.0632151 (55.6535591 - 62.2)^2 = (-6.5464409)^2 = 42.8558689 (69.5114310 - 62.2)^2 = (7.3114310)^2 = 53.4570178 (49.3571446 - 62.2)^2 = (-12.8428554)^2 = 164.9389370 (74.5200026 - 62.2)^2 = (12.3200026)^2 = 151.7824640\nSSR = 413.0975028\nStep 4: Calculate SSE (Error Sum of Squares)\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n(60 - 61.9485736)^2 = (-1.9485736)^2 = 3.7969384 (56 - 55.6535591)^2 = (0.3464409)^2 = 0.1200212 (70 - 69.5114310)^2 = (0.4885690)^2 = 0.2387198 (50 - 49.3571446)^2 = (0.6428554)^2 = 0.4132631 (75 - 74.5200026)^2 = (0.4799974)^2 = 0.2303975\nSSE = 4.7024972\nStep 5: Verify decomposition\nSST = SSR + SSE 416.8 = 413.0975028 + 4.7024972\nStep 6: Calculate R-squared\nR^2 = \\frac{SSR}{SST} = \\frac{413.0975028}{416.8} = 0.9916\n\n# Verification\nsummary(model)$r.squared  # Should match our calculation\n\n[1] 0.9885135\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\ndf &lt;- data.frame(income = income, turnout = turnout)\n\nggplot(df, aes(x = income, y = turnout)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Voter Turnout vs Income per Capita\",\n    x = \"Income per Capita (thousands €)\",\n    y = \"Voter Turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    text = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\nInterpretation\nThe analysis shows:\n\nA very strong positive correlation (r = 0.994) between income and voter turnout\nThe regression equation \\hat{Y} = -0.9085714 + 1.2571429X indicates that:\n\nFor each €1,000 increase in income, turnout increases by about 1.26 percentage points\nThe intercept (-0.9086) has little practical meaning as income is never zero\n\nThe R-squared of 0.9916 indicates that 99.16% of the variance in turnout is explained by income",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#example-3.-anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "href": "correg_pl.html#example-3.-anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.19 Example 3. Anxiety Levels and Cognitive Performance: A Laboratory Study",
    "text": "14.19 Example 3. Anxiety Levels and Cognitive Performance: A Laboratory Study\n\nData and Context\nIn a psychology experiment, researchers measured the relationship between anxiety levels (measured by galvanic skin response, GSR) and cognitive performance (score on a working memory task).\n\n# Data\nanxiety &lt;- c(2.1, 3.4, 4.2, 5.1, 5.8, 6.4, 7.2, 8.0)  # GSR readings\nperformance &lt;- c(92, 88, 84, 78, 74, 70, 65, 62)      # Working memory scores\n\n# Initial model check\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8993 -0.6660  0.2162  0.6106  1.5262 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 105.3248     1.3189   79.86 0.00000000026 ***\nanxiety      -5.4407     0.2359  -23.06 0.00000043549 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 6 degrees of freedom\nMultiple R-squared:  0.9888,    Adjusted R-squared:  0.987 \nF-statistic: 531.9 on 1 and 6 DF,  p-value: 0.0000004355\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{2.1 + 3.4 + 4.2 + 5.1 + 5.8 + 6.4 + 7.2 + 8.0}{8} = \\frac{42.2}{8} = 5.275\n\\bar{Y} = \\frac{92 + 88 + 84 + 78 + 74 + 70 + 65 + 62}{8} = \\frac{613}{8} = 76.625\n\n# Verification\nmean(anxiety)\n\n[1] 5.275\n\nmean(performance)\n\n[1] 76.625\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(2.1 - 5.275) = -3.175\n(3.4 - 5.275) = -1.875\n(4.2 - 5.275) = -1.075\n(5.1 - 5.275) = -0.175\n(5.8 - 5.275) = 0.525\n(6.4 - 5.275) = 1.125\n(7.2 - 5.275) = 1.925\n(8.0 - 5.275) = 2.725\n\nSquared deviations:\n10.08063 + 3.51563 + 1.15563 + 0.03063 + 0.27563 + 1.26563 + 3.70563 +\n7.42563 = 27.45500\ns^2_X = \\frac{27.45500}{7} = 3.922143\nSimilarly for Y: Deviations:\n15.375, 11.375, 7.375, 1.375, -2.625, -6.625, -11.625, -14.625\ns^2_Y = \\frac{236.875 + 129.391 + 54.391 + 1.891 + 6.891 + 43.891 + 135.141 + 213.891}{7} = \\frac{822.362}{7} = 117.4803\n\n# Verification\nvar(anxiety)\n\n[1] 3.922143\n\nvar(performance)\n\n[1] 117.4107\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-3.175 × 15.375) = -48.815625\n(-1.875 × 11.375) = -21.328125\n(-1.075 × 7.375) = -7.928125\n(-0.175 × 1.375) = -0.240625\n(0.525 × -2.625) = -1.378125\n(1.125 × -6.625) = -7.453125\n(1.925 × -11.625) = -22.378125\n(2.725 × -14.625) = -39.853125\nSum = -149.375\ns_{XY} = \\frac{-149.375}{7} = -21.33929\n\n# Verification\ncov(anxiety, performance)\n\n[1] -21.33929\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-21.33929}{\\sqrt{3.922143}\\sqrt{117.4803}} = -0.9932\n\n# Verification\ncor(anxiety, performance)\n\n[1] -0.9944073\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-21.33929}{3.922143} = -5.4407\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-5.4407 × 5.275 = -28.6997\n\\hat{\\beta_0} = 76.625 - (-28.6997) = 105.3247\n\n\n# Verification\ncoef(model)\n\n(Intercept)     anxiety \n 105.324804   -5.440721 \n\n\n\n\n4. R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}): \\hat{Y} = 105.3247 - 5.4407X\n\n# Predicted values\ny_hat &lt;- 105.3247 - 5.4407 * anxiety\ndata.frame(\n  Anxiety = anxiety,\n  Performance = performance,\n  Predicted = y_hat,\n  row.names = 1:8\n)\n\n  Anxiety Performance Predicted\n1     2.1          92  93.89923\n2     3.4          88  86.82632\n3     4.2          84  82.47376\n4     5.1          78  77.57713\n5     5.8          74  73.76864\n6     6.4          70  70.50422\n7     7.2          65  66.15166\n8     8.0          62  61.79910\n\n\nStep 2: Sum of Squares\nSST = \\sum(Y_i - \\bar{Y})^2 = 822.362\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 816.3094\nSSE = \\sum(Y_i - \\hat{Y}_i)^2 = 6.0526\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{816.3094}{822.362} = 0.9926\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.9888459\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\n\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Cognitive Performance vs. Anxiety Levels\",\n    x = \"Anxiety (GSR)\",\n    y = \"Performance Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.993) between anxiety and cognitive performance\nFor each unit increase in GSR (anxiety), performance decreases by approximately 5.44 points\nThe model explains 99.26% of the variance in performance scores\nThe relationship appears to be strongly linear, suggesting a reliable anxiety-performance relationship\nThe high intercept (105.32) represents the theoretical maximum performance at zero anxiety\n\n\n\nStudy Limitations\n\nSmall sample size (n=8)\nPossible other confounding variables\nLimited range of anxiety levels\nCross-sectional rather than longitudinal data",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przykład-4.-analiza-związku-między-wielkością-okręgu-a-wskaźnikiem-dysproporcjonalności-wyborczej-2",
    "href": "correg_pl.html#przykład-4.-analiza-związku-między-wielkością-okręgu-a-wskaźnikiem-dysproporcjonalności-wyborczej-2",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.20 Przykład 4. Analiza związku między wielkością okręgu a wskaźnikiem dysproporcjonalności wyborczej (2)",
    "text": "14.20 Przykład 4. Analiza związku między wielkością okręgu a wskaźnikiem dysproporcjonalności wyborczej (2)\nTa analiza bada związek między wielkością okręgu (DM) a wskaźnikiem dysproporcjonalności Loosemore-Hanby (LH) w wyborach parlamentarnych w 6 krajach. Wskaźnik Loosemore-Hanby mierzy dysproporcjonalność wyborczą, przy czym wyższe wartości wskazują na większą dysproporcjonalność między głosami a mandatami.\n\nDane\n\n\n\nWielkość okręgu i wskaźnik LH według kraju\n\n\nKraj\nDM\nLH\n\n\n\n\nA\n4\n12\n\n\nB\n10\n8\n\n\nC\n3\n15\n\n\nD\n8\n10\n\n\nE\n7\n6\n\n\nF\n4\n13\n\n\n\n\n\n\n\nKrok 1: Obliczenie wariancji i odchylenia standardowego dla DM i LH\n\nObliczenia dla wielkości okręgu (DM)\nNajpierw obliczę średnią wartości DM:\n\\bar{x}_{DM} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\frac{4 + 10 + 3 + 8 + 7 + 4}{6} = \\frac{36}{6} = 6\nNastępnie obliczę wariancję z korektą Bessela, korzystając z wzoru:\ns^2_{DM} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\n\n\nKraj\nDM (x_i)\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\n\n\n\n\nA\n4\n4 - 6 = -2\n(-2)^2 = 4\n\n\nB\n10\n10 - 6 = 4\n(4)^2 = 16\n\n\nC\n3\n3 - 6 = -3\n(-3)^2 = 9\n\n\nD\n8\n8 - 6 = 2\n(2)^2 = 4\n\n\nE\n7\n7 - 6 = 1\n(1)^2 = 1\n\n\nF\n4\n4 - 6 = -2\n(-2)^2 = 4\n\n\nSuma\n\n\n38\n\n\n\ns^2_{DM} = \\frac{38}{6-1} = \\frac{38}{5} = 7.6\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji:\ns_{DM} = \\sqrt{s^2_{DM}} = \\sqrt{7.6} = 2.757\n\n\nObliczenia dla wskaźnika LH\nNajpierw obliczę średnią wartości LH:\n\\bar{y}_{LH} = \\frac{12 + 8 + 15 + 10 + 6 + 13}{6} = \\frac{64}{6} = 10.667\nNastępnie obliczę wariancję z korektą Bessela:\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n12\n12 - 10.667 = 1.333\n(1.333)^2 = 1.777\n\n\nB\n8\n8 - 10.667 = -2.667\n(-2.667)^2 = 7.113\n\n\nC\n15\n15 - 10.667 = 4.333\n(4.333)^2 = 18.775\n\n\nD\n10\n10 - 10.667 = -0.667\n(-0.667)^2 = 0.445\n\n\nE\n6\n6 - 10.667 = -4.667\n(-4.667)^2 = 21.781\n\n\nF\n13\n13 - 10.667 = 2.333\n(2.333)^2 = 5.443\n\n\nSuma\n\n\n55.334\n\n\n\ns^2_{LH} = \\frac{55.334}{6-1} = \\frac{55.334}{5} = 11.067\nOdchylenie standardowe to:\ns_{LH} = \\sqrt{s^2_{LH}} = \\sqrt{11.067} = 3.327\nPodsumowanie kroku 1:\n\nWariancja DM (z korektą Bessela): 7.6\nOdchylenie standardowe DM: 2.757\nWariancja LH (z korektą Bessela): 11.067\nOdchylenie standardowe LH: 3.327\n\n\n\n\nKrok 2: Obliczenie kowariancji między DM a LH dla tej próby danych\nKowariancja jest obliczana przy użyciu wzoru z korektą Bessela:\nCov(DM, LH) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\nA\n4\n12\n-2\n1.333\n(-2)(1.333) = -2.666\n\n\nB\n10\n8\n4\n-2.667\n(4)(-2.667) = -10.668\n\n\nC\n3\n15\n-3\n4.333\n(-3)(4.333) = -12.999\n\n\nD\n8\n10\n2\n-0.667\n(2)(-0.667) = -1.334\n\n\nE\n7\n6\n1\n-4.667\n(1)(-4.667) = -4.667\n\n\nF\n4\n13\n-2\n2.333\n(-2)(2.333) = -4.666\n\n\nSuma\n\n\n\n\n-37\n\n\n\nCov(DM, LH) = \\frac{-37}{6-1} = \\frac{-37}{5} = -7.4\nKowariancja między DM a LH: -7.4\nUjemna kowariancja wskazuje na odwrotną zależność: wraz ze wzrostem wielkości okręgu wskaźnik dysproporcjonalności LH ma tendencję do spadku.\n\n\nKrok 3: Obliczenie współczynnika korelacji liniowej Pearsona między DM a LH\nWspółczynnik korelacji Pearsona oblicza się przy użyciu wzoru:\nr = \\frac{Cov(DM, LH)}{s_{DM} \\cdot s_{LH}}\nMamy już obliczone:\n\nCov(DM, LH) = -7.4\ns_{DM} = 2.757\ns_{LH} = 3.327\n\nr = \\frac{-7.4}{2.757 \\cdot 3.327} = \\frac{-7.4}{9.172} = -0.807\nWspółczynnik korelacji Pearsona: -0.807\n\nInterpretacja:\nWspółczynnik korelacji -0.807 wskazuje:\n\nKierunek: Ujemny znak pokazuje odwrotną zależność między wielkością okręgu a wskaźnikiem LH.\nSiła: Wartość bezwzględna 0.807 wskazuje na silną korelację (blisko -1).\nInterpretacja praktyczna: Ponieważ wyższe wartości wskaźnika LH wskazują na większą dysproporcjonalność, ta silna ujemna korelacja sugeruje, że wraz ze wzrostem wielkości okręgu, dysproporcjonalność wyborcza ma tendencję do znacznego spadku. Innymi słowy, systemy wyborcze z większymi okręgami wyborczymi (więcej przedstawicieli wybieranych w okręgu) mają tendencję do generowania bardziej proporcjonalnych wyników (mniejsza dysproporcjonalność).\n\nUstalenie to jest zgodne z teorią nauk politycznych, która sugeruje, że większe okręgi wyborcze zapewniają mniejszym partiom więcej możliwości uzyskania reprezentacji, co prowadzi do wyników wyborczych, które lepiej odzwierciedlają rozkład głosów między partiami.\n\n\n\nKrok 4: Skonstruowanie prostego modelu regresji liniowej i obliczenie R-kwadrat\nUżyję wzoru na prostą regresję liniową:\nLH = \\beta_0 + \\beta_1 \\cdot DM + \\varepsilon\nGdzie:\n\n\\beta_0 to wyraz wolny\n\\beta_1 to współczynnik nachylenia dla DM\n\nWzór na obliczenie \\beta_1 z korektą Bessela to:\n\\beta_1 = \\frac{Cov(DM, LH)}{s^2_{DM}} = \\frac{-7.4}{7.6} = -0.974\nAby obliczyć \\beta_0, użyję:\n\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x} = 10.667 - (-0.974) \\cdot 6 = 10.667 + 5.844 = 16.511\nZatem równanie regresji to:\nLH = 16.511 - 0.974 \\cdot DM\n\nObliczanie R-kwadrat\nAby właściwie obliczyć R-kwadrat, muszę obliczyć następujące sumy kwadratów:\n\nSST (Całkowita suma kwadratów): Mierzy całkowitą zmienność zmiennej zależnej (LH)\nSSR (Suma kwadratów regresji): Mierzy zmienność wyjaśnioną przez model regresji\nSSE (Suma kwadratów błędów): Mierzy niewyjaśnioną zmienność modelu\n\nNajpierw obliczę przewidywane wartości LH, używając naszego równania regresji:\n\\hat{LH} = 16.511 - 0.974 \\cdot DM\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i = 16.511 - 0.974x_i)\n\n\n\n\nA\n4\n12\n16.511 - 0.974(4) = 16.511 - 3.896 = 12.615\n\n\nB\n10\n8\n16.511 - 0.974(10) = 16.511 - 9.74 = 6.771\n\n\nC\n3\n15\n16.511 - 0.974(3) = 16.511 - 2.922 = 13.589\n\n\nD\n8\n10\n16.511 - 0.974(8) = 16.511 - 7.792 = 8.719\n\n\nE\n7\n6\n16.511 - 0.974(7) = 16.511 - 6.818 = 9.693\n\n\nF\n4\n13\n16.511 - 0.974(4) = 16.511 - 3.896 = 12.615\n\n\n\nTeraz, obliczę każdą sumę kwadratów:\n\nSST (Całkowita suma kwadratów)\n\nSST = \\sum_{i=1}^{n}(y_i - \\bar{y})^2\nJuż obliczyliśmy te odchylenia w Kroku 1:\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n12\n1.333\n1.777\n\n\nB\n8\n-2.667\n7.113\n\n\nC\n15\n4.333\n18.775\n\n\nD\n10\n-0.667\n0.445\n\n\nE\n6\n-4.667\n21.781\n\n\nF\n13\n2.333\n5.443\n\n\nSuma\n\n\n55.334\n\n\n\nSST = 55.334\n\nSSR (Suma kwadratów regresji)\n\nSSR = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2\n\n\n\n\n\n\n\n\n\nKraj\nPrzewidywane LH (\\hat{y}_i)\n\\hat{y}_i - \\bar{y}\n(\\hat{y}_i - \\bar{y})^2\n\n\n\n\nA\n12.615\n12.615 - 10.667 = 1.948\n(1.948)^2 = 3.795\n\n\nB\n6.771\n6.771 - 10.667 = -3.896\n(-3.896)^2 = 15.178\n\n\nC\n13.589\n13.589 - 10.667 = 2.922\n(2.922)^2 = 8.538\n\n\nD\n8.719\n8.719 - 10.667 = -1.948\n(-1.948)^2 = 3.795\n\n\nE\n9.693\n9.693 - 10.667 = -0.974\n(-0.974)^2 = 0.949\n\n\nF\n12.615\n12.615 - 10.667 = 1.948\n(1.948)^2 = 3.795\n\n\nSuma\n\n\n36.05\n\n\n\nSSR = 36.05\n\nSSE (Suma kwadratów błędów)\n\nSSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i)\ny_i - \\hat{y}_i\n(y_i - \\hat{y}_i)^2\n\n\n\n\nA\n12\n12.615\n12 - 12.615 = -0.615\n(-0.615)^2 = 0.378\n\n\nB\n8\n6.771\n8 - 6.771 = 1.229\n(1.229)^2 = 1.510\n\n\nC\n15\n13.589\n15 - 13.589 = 1.411\n(1.411)^2 = 1.991\n\n\nD\n10\n8.719\n10 - 8.719 = 1.281\n(1.281)^2 = 1.641\n\n\nE\n6\n9.693\n6 - 9.693 = -3.693\n(-3.693)^2 = 13.638\n\n\nF\n13\n12.615\n13 - 12.615 = 0.385\n(0.385)^2 = 0.148\n\n\nSuma\n\n\n\n19.306\n\n\n\nSSE = 19.306\n\nWeryfikacja\n\nMożemy zweryfikować nasze obliczenia, sprawdzając czy SST = SSR + SSE:\n55.334 \\approx 36.05 + 19.306 = 55.356\nNiewielka różnica (0.022) wynika z zaokrągleń w obliczeniach.\n\nObliczanie R-kwadrat\n\nR^2 = \\frac{SSR}{SST} = \\frac{36.05}{55.334} = 0.652\nAlternatywnie, możemy też obliczyć:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{19.306}{55.334} = 1 - 0.349 = 0.651\nDrobna różnica wynika z zaokrągleń.\n\n\nObliczanie RMSE (Pierwiastek średniego błędu kwadratowego)\nObliczanie RMSE (Pierwiastek średniego błędu kwadratowego)\nRMSE oblicza się przy użyciu wzoru:\nRMSE = \\sqrt{\\frac{SSE}{n}} = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}}\nKorzystając z naszej obliczonej SSE:\nRMSE = \\sqrt{\\frac{19.306}{6}} = \\sqrt{3.218} = 1.794\nKorekta Bessela (dzielenie przez n-1 zamiast n) stosuje się do estymacji wariancji próby, ale nie jest standardowo stosowana przy obliczaniu RMSE, gdyż RMSE jest miarą błędu predykcji, a nie estymatorem parametru populacji.\n\n\nObliczanie MAE (Średni błąd bezwzględny)\nMAE oblicza się przy użyciu wzoru:\nMAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i)\n|y_i - \\hat{y}_i|\n\n\n\n\nA\n12\n12.615\n|12 - 12.615| = 0.615\n\n\nB\n8\n6.771\n|8 - 6.771| = 1.229\n\n\nC\n15\n13.589\n|15 - 13.589| = 1.411\n\n\nD\n10\n8.719\n|10 - 8.719| = 1.281\n\n\nE\n6\n9.693\n|6 - 9.693| = 3.693\n\n\nF\n13\n12.615\n|13 - 12.615| = 0.385\n\n\nSuma\n\n\n8.614\n\n\n\nMAE = \\frac{8.614}{6} = 1.436\nModel regresji: LH = 16.511 - 0.974 \\cdot DM\nR-kwadrat: 0.651\nRMSE: 1.794\nMAE: 1.436\n\n\nInterpretacja:\n\nRównanie regresji: Dla każdego jednostkowego wzrostu wielkości okręgu, wskaźnik dysproporcjonalności LH zmniejsza się o 0.974 jednostki. Wyraz wolny (16.511) reprezentuje oczekiwany wskaźnik LH, gdy wielkość okręgu wynosi zero (choć nie ma to praktycznego znaczenia, ponieważ wielkość okręgu nie może wynosić zero).\nR-kwadrat: 0.651 wskazuje, że około 65.1% wariancji dysproporcjonalności wyborczej (wskaźnik LH) może być wyjaśnione przez wielkość okręgu. Jest to dość wysoka wartość, sugerująca, że wielkość okręgu jest rzeczywiście silnym predyktorem dysproporcjonalności wyborczej, choć mniejszym niż w poprzednim zestawie danych.\nRMSE: Wartość 1.794 informuje nas o przeciętnym błędzie prognozy modelu. Jest to miara dokładności przewidywań modelu wyrażona w jednostkach zmiennej zależnej (LH).\nMAE: Wartość 1.436 informuje nas o przeciętnym bezwzględnym błędzie prognozy modelu. W porównaniu z RMSE, MAE jest mniej czuły na wartości odstające, co potwierdza, że niektóre obserwacje (np. dla kraju E) mają stosunkowo duży błąd predykcji.\nImplikacje polityczne: Wyniki sugerują, że zwiększenie wielkości okręgu mogłoby być skuteczną strategią reform wyborczych dla krajów starających się zmniejszyć dysproporcjonalność między głosami a mandatami. Jednakże, korzyści marginalne wydają się zmniejszać wraz ze wzrostem wielkości okręgu, jak widać we wzorcu danych.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#example-5.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "href": "correg_pl.html#example-5.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "title": "14  Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "14.21 Example 5. District Magnitude and Electoral Disproportionality: A Comparative Analysis (*)",
    "text": "14.21 Example 5. District Magnitude and Electoral Disproportionality: A Comparative Analysis (*)\n\nData Generating Process\nLet’s set up a DGP where:\n\\begin{aligned}\n& Y_{\\text{Gallagher}} = 12 - 0.8X_{\\text{DM}} + \\varepsilon \\\\\n& \\varepsilon \\sim \\mathcal{N}(0, 1) \\\\\n& X_{\\text{DM}} \\in \\{3, 5, 7, 10, 12, 15\\}\n\\end{aligned}\n\n# DGP\nmagnitude &lt;- c(3, 5, 7, 10, 12, 15)\nepsilon &lt;- rnorm(6, mean = 0, sd = 1)\ngallagher &lt;- 12 - 0.8 * magnitude + epsilon\n\n# Round (sampled from the DGP) Gallagher indices to one decimal place\ngallagher &lt;- round(c(9.0, 7.8, 9.2, 4.1, 2.5, 1.7), 1)\n\n# Show data\ndata.frame(\n  District_Magnitude = magnitude,\n  Gallagher_Index = gallagher\n)\n\n  District_Magnitude Gallagher_Index\n1                  3             9.0\n2                  5             7.8\n3                  7             9.2\n4                 10             4.1\n5                 12             2.5\n6                 15             1.7\n\n# Initial model check\nmodel &lt;- lm(gallagher ~ magnitude)\nsummary(model)\n\n\nCall:\nlm(formula = gallagher ~ magnitude)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.6516 -0.4628  2.3260 -0.6908 -0.9020  0.3813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7349     1.3034   9.003 0.000843 ***\nmagnitude    -0.6944     0.1359  -5.110 0.006934 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.368 on 4 degrees of freedom\nMultiple R-squared:  0.8672,    Adjusted R-squared:  0.834 \nF-statistic: 26.11 on 1 and 4 DF,  p-value: 0.006934\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{3 + 5 + 7 + 10 + 12 + 15}{6} = \\frac{52}{6} = 8.6667\n\\bar{Y} = \\frac{9.0 + 7.8 + 9.2 + 4.1 + 2.5 + 1.7}{6} = \\frac{34.3}{6} = 5.7167\n\n# Verification\nmean(magnitude)\n\n[1] 8.666667\n\nmean(gallagher)\n\n[1] 5.716667\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(3 - 8.6667) = -5.6667\n(5 - 8.6667) = -3.6667\n(7 - 8.6667) = -1.6667\n(10 - 8.6667) = 1.3333\n(12 - 8.6667) = 3.3333\n(15 - 8.6667) = 6.3333\n\nSquared deviations:\n\n32.1115 + 13.4445 + 2.7779 + 1.7777 + 11.1109 + 40.1107 = 101.3332\n\ns^2_X = \\frac{101.3332}{5} = 20.2666\nFor Y: Deviations: 3.2833, 2.0833, 3.4833, -1.6167, -3.2167, -4.0167\ns^2_Y = \\frac{56.3483}{5} = 11.2697\n\n# Verification\nvar(magnitude)\n\n[1] 20.26667\n\nvar(gallagher)\n\n[1] 11.26967\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n\n(-5.6667 × 3.2833) = -18.6057\n(-3.6667 × 2.0833) = -7.6387\n(-1.6667 × 3.4833) = -5.8056\n(1.3333 × -1.6167) = -2.1556\n(3.3333 × -3.2167) = -10.7223\n(6.3333 × -4.0167) = -25.4391\n\nSum = -70.3670\ns_{XY} = \\frac{-70.3670}{5} = -14.0734\n\n# Verification\ncov(magnitude, gallagher)\n\n[1] -14.07333\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-14.0734}{\\sqrt{20.2666}\\sqrt{11.2697}} = -0.9279\n\n# Verification\ncor(magnitude, gallagher)\n\n[1] -0.9312157\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-14.0734}{20.2666} = -0.6944\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-0.6944 × 8.6667 = -6.0181\n\\hat{\\beta_0} = 5.7167 - (-6.0181) = 11.7348\n\n\n# Verification\ncoef(model)\n\n(Intercept)   magnitude \n 11.7348684  -0.6944079 \n\n\n\n\nR-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = 11.7348 - 0.6944X\n\n# Predicted values\ny_hat &lt;- 11.7348 - 0.6944 * magnitude\ndata.frame(\n  Magnitude = magnitude,\n  Gallagher = gallagher,\n  Predicted = y_hat,\n  row.names = 1:6\n)\n\n  Magnitude Gallagher Predicted\n1         3       9.0    9.6516\n2         5       7.8    8.2628\n3         7       9.2    6.8740\n4        10       4.1    4.7908\n5        12       2.5    3.4020\n6        15       1.7    1.3188\n\n\nStep 2: Sum of Squares SST = \\sum(Y_i - \\bar{Y})^2 = 56.3483 SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 48.5271 SSE = \\sum(Y_i - \\hat{Y}_i)^2 = 7.8212\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{48.5271}{56.3483} = 0.8612\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.8671626\n\n\n\n\nVisualization - True vs. Estimated Parameters\n\nTrue DGP: Y = 12 - 0.8X + ε\nEstimated Model: Y = 11.7348 - 0.6944X\n\n\nlibrary(ggplot2)\n\n# Create data frame with original data\ndf &lt;- data.frame(\n  magnitude = magnitude,\n  gallagher = gallagher\n)\n\n# Create sequence for smooth lines\nx_seq &lt;- seq(min(magnitude), max(magnitude), length.out = 100)\n\n# Calculate predicted values for both lines\ntrue_dgp &lt;- 12 - 0.8 * x_seq\nestimated &lt;- 11.7348 - 0.6944 * x_seq\n\n# Combine into a data frame for plotting\nlines_df &lt;- data.frame(\n  magnitude = rep(x_seq, 2),\n  value = c(true_dgp, estimated),\n  Model = rep(c(\"True DGP\", \"Estimated\"), each = length(x_seq))\n)\n\n# Create plot\nggplot() +\n  geom_line(data = lines_df, \n            aes(x = magnitude, y = value, color = Model, linetype = Model),\n            size = 1) +\n  geom_point(data = df, \n             aes(x = magnitude, y = gallagher),\n             color = \"black\", \n             size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  labs(\n    title = \"True DGP vs. Estimated Regression Line\",\n    subtitle = \"Black points show observed data with random noise\",\n    x = \"District Magnitude\",\n    y = \"Gallagher Index\",\n    caption = \"True DGP: Y = 12 - 0.8X + ε\\nEstimated: Y = 11.73 - 0.69X\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.caption = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\n\nObservations about Model Fit\n\nSlope Comparison\n\nTrue slope: -0.8\nEstimated slope: -0.69\nThe estimated slope is reasonably close to the true parameter\n\nIntercept Comparison\n\nTrue intercept: 12\nEstimated intercept: 11.73\nThe estimated intercept very closely approximates the true value\n\nVisual Patterns\n\nThe lines are nearly parallel, showing good slope recovery\nPoints scatter around both lines due to the random error term (ε)\nThe small sample size (n=6) leads to some imprecision in estimation\nThe estimated line (blue) provides a good approximation of the true DGP (red dashed)\n\nImpact of Random Error\n\nThe scatter of points around the true DGP line reflects the N(0,1) error term\nThis noise leads to the slight differences in estimated parameters\nWith a larger sample, we would expect even closer convergence to true parameters\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.93) between district magnitude and electoral disproportionality\nFor each unit increase in district magnitude, the Gallagher index decreases by approximately 0.69 points\nThe model explains 86.12% of the variance in disproportionality\nThe relationship appears strongly linear with moderate scatter\nThe intercept (11.73) represents the expected disproportionality in a hypothetical single-member district system\n\n\n\nStudy Context\n\nData represents simulated observations from a DGP with moderate noise\nSample shows how increasing district magnitude tends to reduce disproportionality\nRandom component reflects other institutional and political factors affecting disproportionality\n\n\n\nLimitations\n\nSmall sample size (n=6)\nSimulated rather than real-world data\nAssumes linear relationship\nDoes not account for other institutional features",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "probability_en.html",
    "href": "probability_en.html",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "",
    "text": "15.1 Probability: Preliminary Concepts\nImagine you’re trying to decide whether to bring an umbrella to class tomorrow. You check the weather forecast, which says there’s a 30% chance of rain. But what does this number really mean?\nThis is where probability comes in - it’s a mathematical way to measure how likely something is to happen.\nA probability represents the likelihood or chance of an event occurring, expressed as a number between 0 and 1 (or as a percentage between 0% and 100%).\nBefore we dive into probability theory, let’s establish some foundational concepts that we’ll use throughout this course.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#probability-preliminary-concepts",
    "href": "probability_en.html#probability-preliminary-concepts",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "",
    "text": "Basic Set Concepts\nBefore we can understand probability, we need to grasp some fundamental concepts from set theory. A set is simply a collection of distinct objects.\nA set can be defined by:\n\nListing all elements: A = \\{1, 2, 3\\}\nDescribing a property: B = \\{\\text{x | x is a positive integer less than 4}\\}\n\nThe empty set \\emptyset contains no elements.\nIf A and B are sets:\n\nIf A is a subset of B, we write A \\subseteq B\nIf x is an element of A, we write x \\in A\n\nFor example, if B = \\{1, 2, 3\\}:\n\n\\{1, 2\\} is a subset of B (written \\{1, 2\\} \\subseteq B)\n1 is an element of B (written 1 \\in B)\n\n\n\n\n\n\n\nNote\n\n\n\nThe proper subset notation uses a strict subset symbol. If A is a proper subset of B, we write:\nA \\subset B\nThis means that A is a subset of B AND A \\neq B (A is not equal to B).\nIn contrast, A \\subseteq B allows for the possibility that A = B.\nA set is a fundamental mathematical concept - it’s a collection of distinct objects where order doesn’t matter and duplicates are not allowed. In other words, each element either belongs to the set or it doesn’t, with no concept of “how many times” it belongs.\nFormally, if x \\in A (meaning x is an element of set A), then adding another copy of x has no effect on A. This gives us identities like:\n\\{1, 2, 2, 3\\} = \\{1, 2, 3\\} = \\{3, 1, 2\\}\nThis distinguishes sets from other mathematical collections:\n\nLists/Sequences: Order matters and duplicates are allowed\n\n[1, 2, 2, 3] ≠ [1, 2, 3]\n[1, 2, 3] ≠ [3, 2, 1]\n\nMultisets: Order doesn’t matter but duplicates are allowed\n\n{1, 2, 2, 3}ₘ ≠ {1, 2, 3}ₘ\n{1, 2, 2, 3}ₘ = {3, 2, 1, 2}ₘ\n\n\nThis unique property of sets - that membership is binary (an element either belongs or doesn’t) - makes them particularly useful in mathematics for describing collections where we only care about whether something is present, not how many times it appears or in what order.\n\n\n\n\nSet Operations\nBasic set (events) operations (given two sets A and B):\n\nUnion (A \\cup B): Elements in either A OR B (or both)\nIntersection (A \\cap B): Elements in BOTH A AND B\nComplement (A^c or A^{'}): Elements NOT in A\nDifference (A \\setminus B): Elements in A but NOT in B\n\nThese operations follow important laws like:\n(A \\cup B)^c = A^c \\cap B^c (DeMorgan’s Law)\n(A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C) (Distributive Law)\nSets and the associated operations are easy to visualize in terms of Venn diagrams, as illustrated in the figure below:\n\n\n\nhttp://athenasc.com/probbook.html\n\n\nExamples of Venn diagrams:\n\nThe shaded region is S \\cap T.\nThe shaded region is S \\cup T.\nThe shaded region is S \\cap T^c.\nHere, T \\subset S. The shaded region is the complement of S.\nThe sets S, T, and U are disjoint.\nThe sets S, T, and U form a partition of the universal set \\Omega.\n\nThe Universal Set (often denoted as \\Omega, U, or S):\n\nIn Set Theory:\n\n\nSet containing all elements in a given context\nAll other sets are its subsets\nComplement of set A is A' = \\Omega - A\n\n\nIn Probability:\n\n\nCalled the sample space S or \\Omega\nContains all possible outcomes\nHas probability P(\\Omega) = 1\n\nKey Properties:\n\nA \\subseteq \\Omega\nA \\cup A' = \\Omega\nA \\cap A' = \\emptyset\n\\Omega' = \\emptyset\n\\emptyset' = \\Omega\n\nExamples:\n\nDie roll: \\Omega = \\{1,2,3,4,5,6\\}\nCoin flip: \\Omega = \\{H,T\\}\n\n\n\n\n\n\n\nSet Theory as a Language for Probability\n\n\n\nSet theory provides the mathematical framework for probability theory. Here are the key parallels:\n\n\n\n\n\n\n\n\nSet Theory\nProbability Theory\nDescription\n\n\n\n\n\\Omega (Universal set)\nSample space (S)\nAll possible outcomes\n\n\nx \\in A (Element)\nOutcome\nSingle result\n\n\nA \\subseteq \\Omega (Subset)\nEvent\nCollection of outcomes\n\n\n\\emptyset (Empty set)\nImpossible event\nCannot occur (P(\\emptyset) = 0)\n\n\n\\Omega (Universal set)\nCertain event\nMust occur (P(\\Omega) = 1)\n\n\nA \\cup B (Union)\nEither A OR B\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\n\nA \\cap B (Intersection)\nBoth A AND B\nP(A \\cap B) = P(A)P(B) (if independent)\n\n\nA' (Complement)\nNot A\nP(A') = 1 - P(A)\n\n\nA \\cap B = \\emptyset\nMutually exclusive\nP(A \\cap B) = 0\n\n\n\n\n\n\n\n\n\n\n\nCardinality of Sets\n\n\n\nIn set theory, we denote cardinality (the number of elements in a set) using vertical bars: |A|\nKey points:\n\n|A| means “number of elements in set A”\nFor a finite set like A = \\{1, 2, 3\\}, we have |A| = 3\nEmpty set has cardinality zero: |\\emptyset| = 0\nFor two sets A and B:\n\nUnion (no overlap): |A \\cup B| = |A| + |B|\nUnion (with overlap): |A \\cup B| = |A| + |B| - |A \\cap B|\nCartesian product: |A \\times B| = |A| \\times |B|\n\n\nExample:\nIf A = \\{\\spadesuit, \\clubsuit, \\heartsuit, \\diamondsuit\\} and B = \\{K, Q, J\\}, then:\n\n|A| = 4\n|B| = 3\n|A \\times B| = 12 (all possible combinations)\n\nThe cardinality of a set is denoted by |A| or #A. Here are the calculations:\n\n|\\{apple, orange, watermelon\\}| = 3 (Each element is distinct)\n|\\{1, 1, 1, 1, 1\\}| = 1 (In a set, duplicates are counted only once)\n|[0, 1]| = \\aleph_1 (This is an uncountably infinite interval of real numbers)\n|\\{1, 2, 3, \\cdots\\}| = \\aleph_0 (This is countably infinite)\n|\\{\\emptyset, \\{1\\}, \\{2\\}, \\{1, 2\\}\\}| = 4 (Each element is a distinct set)\n|\\{\\emptyset, \\{1\\}, \\{1, 1\\}, \\{1, 1, 1\\}, \\cdots\\}| = 2 (After removing duplicates: \\{\\emptyset, \\{1\\}\\} since \\{1\\} = \\{1, 1\\} = \\{1, 1, 1\\} = \\cdots)\n\n\n\n\n\nUnderstanding Set Relations: Elements vs Subsets (*)\nThe difference between an element belonging to a set and one set being a subset of another.\n\nThe “Belongs To” Relationship (\\in)\nWhen we say an element belongs to a set (written as x \\in A), we’re describing membership of a single item in a collection. Think of a classroom: each individual student belongs to (is a member of) the class. They are elements of the set “class.”\nConsider a deck of cards and let H be the set of all hearts:\nH = \\{2♥, 3♥, 4♥, 5♥, 6♥, 7♥, 8♥, 9♥, 10♥, J♥, Q♥, K♥, A♥\\}\nWe can say:\n\nA♥ \\in H (true, because the ace of hearts is one of the hearts)\nK♠ \\notin H (false, because the king of spades is not a heart)\n\\{A♥\\} \\notin H (false, this is a set containing the ace of hearts, not the card itself)\n\n\n\nThe “Is Contained In” Relationship (\\subseteq)\nA subset relationship (written as A \\subseteq B) describes when one set is entirely contained within another set. Every element of the smaller set must appear in the larger set. This is different from set membership (\\in), which describes when a single element belongs to a set.\nTo understand the distinction, let’s look at some examples:\nConsider the following sets:\n\nA = \\{1, 2\\}\nB = \\{1, 2, 3, 4\\}\nC = \\{1\\}\n\nFor set membership (\\in):\n\n1 \\in A (the number 1 is an element of set A)\n\\{1\\} \\notin A (the set containing 1 is not an element of A)\n2 \\in B (the number 2 is an element of B)\n\nFor subset relationships (\\subseteq):\n\nA \\subseteq B (all elements of A are in B)\nC \\subseteq A (all elements of C are in A)\n\\{1\\} \\subseteq A (the set containing 1 is a subset of A)\n\nA key insight is that while 1 \\in A is true (1 is an element of A), \\{1\\} \\in A is false (the set containing 1 is not an element of A). However, \\{1\\} \\subseteq A is true (the set containing 1 is a subset of A).\nThink of it this way: membership (\\in) asks “Is this single thing in the set?” while subset (\\subseteq) asks “Is every element of this smaller set found in the larger set?”\nAnother helpful example is with the empty set \\emptyset:\n\n\\emptyset \\subseteq A for any set A (the empty set is a subset of every set)\nBut \\emptyset \\notin A unless A specifically contains the empty set as an element\n\nExercise (https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf). Using the given sets:\n\nA = \\{1, 3\\}\nB = \\{3, 1\\}\nC = \\{1, 2\\}\nD = \\{\\emptyset, \\{1\\}, \\{2\\}, \\{1, 2\\}, 1, 2\\}\n\nDetermine whether the following are true or false:\n\n1 \\in A : TRUE (1 is an element of A)\n1 \\subseteq A : FALSE (1 is not a set, so subset relation doesn’t apply)\n\\{1\\} \\subseteq A : TRUE (every element of the set {1} is an element of A)\n\\{1\\} \\in A : FALSE (A doesn’t contain any sets as elements)\n3 \\notin C : TRUE (3 is not an element of C)\nA \\in B : FALSE (B doesn’t contain any sets as elements)\nA \\subseteq B : TRUE (A and B contain the same elements)\nC \\in D : TRUE (the set {1,2} appears in D, but not the set C itself)\nC \\subseteq D : TRUE (all elements of C (1 and 2) are also elements of D)\n\\emptyset \\in D : TRUE (empty set is listed as an element of D)\n\\emptyset \\subseteq D : TRUE (True, by definition, the empty set is a subset of any set. This is because if this were not the case, there would have to be an element of \\emptyset which was not in D. But there are no elements in \\emptyset, so the statement is true.)\nA = B : TRUE (they contain the same elements)\n\\emptyset \\subseteq \\emptyset : TRUE (empty set is a subset of itself; the empty set is a subset of any set)\n\\emptyset \\in \\emptyset : FALSE (empty set contains no elements)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#set-theory-and-power-sets-event-space",
    "href": "probability_en.html#set-theory-and-power-sets-event-space",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "15.2 Set Theory and Power Sets (Event Space)",
    "text": "15.2 Set Theory and Power Sets (Event Space)\nThe power set of a set, denoted as \\mathcal{F}(S) or 2^{|S|}, is the set of all possible subsets of S, including the empty set and S itself.\nThis concept is crucial in probability theory because it helps us understand the relationship between the sample space S (all possible outcomes) and the event space (all possible events (i.e. all possible subsets of S) we might want to consider).\nLet’s explore this with a simple example. Consider flipping a single coin where: S = \\{H, T\\} (our sample space)\nThe power set would be:\n\\mathcal{F}(S) = \\{\\emptyset, \\{H\\}, \\{T\\}, \\{H,T\\}\\}\nEach element in the power set represents a possible event. For instance:\n\n\\emptyset: The impossible event (e.g., the coin landing neither heads nor tails)\n\\{H\\}: The event of getting heads\n\\{T\\}: The event of getting tails\n\\{H,T\\}: The certain event (the coin must land either heads or tails)\n\nFor a set with n elements, its power set will have 2^n elements. This is because for each element, we have two choices: include it or not include it in a subset.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#counting-rules-in-probability-the-power-of-and-or",
    "href": "probability_en.html#counting-rules-in-probability-the-power-of-and-or",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "15.3 Counting Rules in Probability: The Power of AND & OR",
    "text": "15.3 Counting Rules in Probability: The Power of AND & OR\nA fundamental challenge in probability is counting possible outcomes. Two key rules help us solve these problems:\n\nThe Multiplication Rule for Independent Events (“AND” Situations)\nWhen we need a sequence of independent choices where we must make ALL choices, we multiply the number of possibilities for each choice. This principle applies when we need option A AND option B AND option C, etc.\nFor example, consider creating a password with exactly three characters in this order:\n\nFirst character must be a letter (26 choices)\nSecond character must be a digit (10 choices)\nThird character must be a symbol (@, #, $, or % - so 4 choices)\n\nTotal possible passwords = 26 × 10 × 4 = 1,040\nThis is like filling three slots where each slot has its own set of valid options. Each new requirement multiplies our total possibilities.\n\n\nThe Addition Rule for Mutually Exclusive Events (“OR” Situations)\nWhen there are multiple valid ways to achieve a goal, and we can use ANY ONE of these ways, we add the number of possibilities. This applies when we accept option A OR option B OR option C, etc.\nFor example, if a password must be EITHER:\n\nA 3-letter word (26³ possibilities) OR\nA 4-digit number (10⁴ possibilities)\n\nTotal possibilities = 26³ + 10⁴ = 17,576 + 10,000 = 27,576\nThink of this as having separate paths to success - we count how many ways each path offers and sum them up.\n\n\nCombining the Rules\nMany real problems require both multiplication and addition.\nExample 1. For instance, if a password must be EITHER:\n\nA letter followed by two digits (26 × 10 × 10 possibilities) OR\nThree symbols (4 × 4 × 4 possibilities)\n\nTotal = (26 × 10 × 10) + (4 × 4 × 4) = 2,600 + 64 = 2,664\nUnderstanding when to multiply (AND situations) versus when to add (OR situations) is key to solving counting problems correctly.\nExample 2. Calvin wants to reach Milwaukee and has these options:\n\nFirst leg (home → Chicago): 3 bus services OR 2 train services\nSecond leg (Chicago → Milwaukee): 2 bus services OR 3 train services\n\nTo solve this:\n\nFirst leg options = 3 + 2 = 5 ways\nSecond leg options = 2 + 3 = 5 ways\nTotal routes = 5 × 5 = 25 possibilities\n\nWhy multiply at the end? Because for EACH way of reaching Chicago, Calvin can use ANY of the ways to reach Milwaukee. This creates 25 unique combinations like:\n\nBus 1 → Bus 1\nBus 1 → Train 1\nBus 2 → Bus 2 …and so on.\n\nThe key is recognizing whether you’re dealing with sequential choices (multiply) or alternative options (add) at each step. Master this distinction, and you’ll solve complex counting problems with ease.\n\n\n\n\n\nflowchart TD\n    Start[Problem: Count Possible Outcomes] --&gt; Q1{\"Are we counting outcomes\\nthat happen in sequence?\"}\n    \n    Q1 --&gt;|Yes| M1[Multiplication Rule:\\nMultiply choices for each step]\n    Q1 --&gt;|No| Q2{\"Are we counting different\\nways to achieve same result?\"}\n    \n    M1 --&gt; ME1[Examples of Sequential Choices]\n    ME1 --&gt; MC1[\"Password: letter then number\\n26 letters × 10 numbers\\n= 260 possibilities\"]\n    MC1 --&gt; MC2[\"Travel: bus then train\\n3 bus routes × 2 train routes\\n= 6 possible journeys\"]\n    \n    Q2 --&gt;|Yes| Q3{\"Do options overlap?\"}\n    \n    Q3 --&gt;|No| A1[Simple Addition Rule:\\nAdd all possibilities]\n    Q3 --&gt;|Yes| A2[\"Extended Addition Rule:\\nAdd - Overlap\"]\n    \n    A1 --&gt; AE1[Examples of Non-Overlapping Options]\n    AE1 --&gt; AC1[\"Coin toss: H or T\\n1 + 1 = 2 outcomes\"]\n    AC1 --&gt; AC2[\"License type: Car or Motorcycle\\n100 + 50 = 150 types\"]\n    \n    A2 --&gt; AE2[Examples of Overlapping Options]\n    AE2 --&gt; AC3[\"Students in Sports or Music:\\n45 + 35 - 15 in both\\n= 65 students\"]\n    \n    classDef start fill:#2d5a8c,stroke:#333,color:#fff,stroke-width:2px\n    classDef question fill:#d4426e,stroke:#333,color:#fff,stroke-width:2px\n    classDef rule fill:#156b45,stroke:#333,color:#fff,stroke-width:2px\n    classDef example fill:#4a4a4a,stroke:#333,color:#fff\n    \n    class Start start\n    class Q1,Q2,Q3 question\n    class M1,A1,A2 rule\n    class ME1,AE1,AE2,MC1,MC2,AC1,AC2,AC3 example\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n    subgraph Addition_Rules[Addition Rule Examples]\n        direction TB\n        subgraph Exclusive[Mutually Exclusive Example]\n            direction TB\n            A1[\"Coin Flip\"] --&gt; AH((Heads))\n            A1 --&gt; AT((Tails))\n            AT --&gt; AR[\"Total = 1 + 1 = 2\\nNo overlap possible\"]\n            AH --&gt; AR\n        end\n        \n        subgraph Overlapping[Overlapping Sets Example]\n            direction TB\n            O1[\"Students in\\nClubs\"] --&gt; OS[\"Science Club\\n25 students\"] & OA[\"Art Club\\n20 students\"]\n            OS & OA --&gt; OI[\"Both Clubs\\n8 students\"]\n            OI --&gt; OT[\"Total = 25 + 20 - 8\\n= 37 students\"]\n        end\n    end\n    \n    classDef default fill:#f5f5f5,stroke:#333,color:#000\n    classDef set fill:#e6e6e6,stroke:#333,color:#000\n    classDef result fill:#d9d9d9,stroke:#333,color:#000\n    classDef option fill:#ffffff,stroke:#333,color:#000\n    classDef example fill:#f0f0f0,stroke:#333,color:#000\n    \n    class A1,O1 set\n    class AR,OT result\n    class AH,AT option\n    class Exclusive,Overlapping example\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n    subgraph Multiplication_Rule[Multiplication Rule Examples]\n        direction TB\n        M1[\"Choose Breakfast\"] --&gt; MA[\"Drink\\n(3 options)\"] & MB[\"Food\\n(2 options)\"]\n        MA --&gt; MA1((Coffee)) & MA2((Tea)) & MA3((Juice))\n        MB --&gt; MB1((Toast)) & MB2((Cereal))\n        MA1 & MA2 & MA3 --&gt; MR[\"Total Combinations:\\n3 drinks × 2 foods\\n= 6 possible breakfasts\"]\n        MB1 & MB2 --&gt; MR\n        \n        subgraph Tree_Example[Tree Diagram]\n            direction TB\n            T1[\"PIN First Digit\\n(0-9)\"] --&gt; T2[\"Second Digit\\n(0-9)\"]\n            T2 --&gt; T3[\"10 × 10 = 100\\ntotal combinations\"]\n        end\n    end\n    \n    classDef default fill:#f5f5f5,stroke:#333,color:#000\n    classDef set fill:#e6e6e6,stroke:#333,color:#000\n    classDef result fill:#d9d9d9,stroke:#333,color:#000\n    classDef option fill:#ffffff,stroke:#333,color:#000\n    classDef example fill:#f0f0f0,stroke:#333,color:#000\n    \n    class MA,MB set\n    class MR,T3 result\n    class MA1,MA2,MA3,MB1,MB2 option\n    class Tree_Example example\n\n\n\n\n\n\n\n\n\n\n\n\nThe Inclusion-Exclusion Principle (*)\n\n\n\nThe Inclusion-Exclusion Principle states that for two sets A and B:\n|A \\cup B| = |A| + |B| - |A \\cap B|\nThis means: The size of their union equals the sum of their individual sizes, minus their intersection (to avoid double counting shared elements).\nFor three sets A, B, and C, the principle extends to:\n|A \\cup B \\cup C| = |A| + |B| + |C| - |A \\cap B| - |B \\cap C| - |A \\cap C| + |A \\cap B \\cap C|\nThis pattern continues for more sets, alternating between adding and subtracting intersections of increasing size.\nA simple example:\n\nSet A: Students who play soccer (20 students)\nSet B: Students who play basketball (15 students)\n8 students play both sports\nTotal students in either sport = 20 + 15 - 8 = 27 students\n\nThe principle is essential in probability theory, combinatorics, and set theory. It helps us correctly count elements when sets overlap, avoiding the common error of double-counting shared elements.\n\n\n\nhttps://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#probability-theory-basic-concepts-and-rules",
    "href": "probability_en.html#probability-theory-basic-concepts-and-rules",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "15.4 Probability Theory: Basic Concepts and Rules",
    "text": "15.4 Probability Theory: Basic Concepts and Rules\n\n\n\n\n\n\nCore Concepts\n\n\n\nProbability theory provides a rigorous foundation for quantifying uncertainty and analyzing random phenomena.\n\n\n\nRandom Experiments\nA random experiment is any procedure that has a well-defined set of possible outcomes but whose specific result cannot be predicted with certainty.\nProperties:\n\nRepeatable under identical conditions\nKnown possible outcomes\nUnpredictable specific results\n\n\n\nSample Space (S)\n\nComplete set of all possible outcomes of a random experiment\nDenoted by S (or \\Omega)\nProperties:\n\nMutually exclusive outcomes\nCollectively exhaustive\n\n\nExamples:\n\nCoin flip: S = \\{H, T\\}\nDie roll: S = \\{1, 2, 3, 4, 5, 6\\}\n\n\n\nThe Event Space: What Can Happen in an Experiment\nEvents are subsets of the sample space S. This means we can use standard set operations to work with them in a precise, mathematical way.\nThe event space \\mathcal{F} is a collection of all events (outcomes or sets of outcomes) that we can assign probabilities to in an experiment. It must follow three fundamental rules:\n\nComplete Space Rule\n\nThe entire sample space S must be in \\mathcal{F}\nThis means all possible outcomes together form a valid event\n\nComplement Rule\n\nIf event A is in \\mathcal{F}, then “not A” (written as A^c) must also be in \\mathcal{F}\nExample: If “getting heads” is an event, “not getting heads” must also be an event\n\nUnion Rule\n\nIf we have any sequence of events A_1, A_2, ... in \\mathcal{F}, their union must also be in \\mathcal{F}\nThis means we can combine valid events to form new valid events\n\n\n\n\n\n\n\n\nDiscrete vs. Continuous Probability: Understanding the Two Types of Random Events\n\n\n\nIn probability, we encounter two fundamentally different types of random events: those we can count (discrete) and those we can measure (continuous). This distinction shapes how we calculate and interpret probabilities.\n\nDiscrete Probability\nWhat: Events that can be counted with whole numbers - Like counting marbles, rolling dice, or flipping coins - Has “gaps” between possible values\nKey Examples:\n\nRolling a die\n\nPossible outcomes: 1, 2, 3, 4, 5, or 6\nNothing in between (can’t roll a 2.5)\nCan say: P(\\text{rolling a 6}) = \\frac{1}{6}\n\nNumber of customers per hour\n\nCould be 0, 1, 2, 3, …\nCan’t have 2.7 customers\nCan say: P(\\text{exactly 5 customers}) = 0.1\n\n\n\n\nContinuous Probability\nWhat: Events measured on a continuous scale - Like measuring height, time, or temperature - Values flow smoothly with no gaps\nKey Examples:\n\nPerson’s height\n\nCould be 170cm, 170.1cm, 170.11cm, …\nCan measure with increasing precision\nMust use ranges: P(170 \\leq \\text{height} \\leq 171)\n\nTime until next bus arrives\n\nCould be 5 mins, 5.1 mins, 5.01 mins, …\nInfinitely divisible\nMust use ranges: P(\\text{waiting time} \\leq 10 \\text{ mins})\n\n\n\n\nCritical Differences\n\nIndividual Values\n\nDiscrete: Can have positive probability\n\nP(\\text{rolling a 6}) = \\frac{1}{6} &gt; 0\n\nContinuous: Always have zero probability\n\nP(\\text{height} = 170.000...) = 0\n\n\nHow We Calculate\n\nDiscrete: Can sum individual probabilities\nContinuous: Must use ranges and integrals\n\n\n\n\nReal-World Application\nThink about a pizza delivery:\n\nDiscrete: Number of toppings (1, 2, 3, …)\nContinuous: Delivery time (15.7 minutes, 15.73 minutes, …)\n\n\n\nWhy Understanding This Matters\n\nHelps choose appropriate probability tools\nGuides how we collect and analyze data\nDetermines how we express uncertainty\nShapes how we make predictions\n\nThis foundation helps us tackle real-world probability problems with the right approach!\n\n\n\nLet’s examine a simple coin flip:\n\nSample space: S = \\{H, T\\} (Heads or Tails)\nThe complete event space: \\mathcal{F} = \\{\\emptyset, \\{H\\}, \\{T\\}, \\{H,T\\}\\}\n\n\\emptyset : impossible event (no outcomes)\n\\{H\\} : getting Heads\n\\{T\\} : getting Tails\n\\{H,T\\} : getting either Heads or Tails\n\n\nVerifying the rules:\n\nRule 1: \\{H,T\\} (the sample space) is included\nRule 2: For event \\{H\\}, its complement \\{T\\} is included\nRule 3: The union of any events (like \\{H\\} \\cup \\{T\\} = \\{H,T\\}) is included\n\n\n\n\n\n\n\nUnderstanding Outcomes vs Events\n\n\n\nThere’s an important distinction between outcomes (also called simple events) and events:\n\nAn outcome or simple event is a single, indivisible result of an experiment. For example, getting heads on a single coin flip is an outcome.\nAn event is a set of outcomes - it can contain one outcome, multiple outcomes, or even no outcomes (the empty set). For example, “getting at least one head when flipping two coins” is an event containing multiple outcomes.\n\nLet’s illustrate this with two coin flips where:\nS = \\{HH, HT, TH, TT\\} (our sample space)\nThe power set (all possible events) would contain 2^4 = 16 events:\n\n\\emptyset (impossible event)\nSingle outcomes: \\{HH\\}, \\{HT\\}, \\{TH\\}, \\{TT\\}\nPairs of outcomes: \\{HH,HT\\}, \\{HH,TH\\}, \\{HH,TT\\}, \\{HT,TH\\}, \\{HT,TT\\}, \\{TH,TT\\}\nTriples: \\{HH,HT,TH\\}, \\{HH,HT,TT\\}, \\{HH,TH,TT\\}, \\{HT,TH,TT\\}\nComplete sample space: \\{HH,HT,TH,TT\\}\n\n\n\n\n\n\n\n\n\nEvent Types\n\n\n\n\nSimple Events: Single outcomes\nCompound Events: Multiple outcomes\nSure (or Certain) Event: Sample space S\nImpossible Event: Empty set \\emptyset\n\n\n\n\n\nProbability Measure and Probability Axioms: Assigning Numbers to Events\nA probability measure P is a way to quantify how likely events are to occur. It takes any event from our event space \\mathcal{F} and assigns it a number between 0 and 1.\nThis assignment follows three essential rules/axioms. These axioms, introduced by Andrey Kolmogorov in 1933, serve as the foundation for all probability calculations:\n\nNon-Negativity Rule\n\nFor any event A, its probability must be at least 0: P(A) \\geq 0\nWe can never have a negative probability\nExample: If we roll a die, the probability of getting a 6 is \\frac{1}{6} (it cannot be negative)\n\nTotal Probability Rule\n\nThe probability of all possible outcomes must equal 1: P(S) = 1\nSomething must happen - the probabilities of all possibilities add up to 100%\nExample: For a fair coin, P(\\text{heads}) + P(\\text{tails}) = 0.5 + 0.5 = 1\n\nAddition Rule for Non-Overlapping Events\n\nIf events cannot happen together (they’re “disjoint”), the probability of their union equals the sum of their individual probabilities\nWritten formally: P(A_1 \\cup A_2 \\cup ...) = P(A_1) + P(A_2) + ...\nExample: In drawing a card, P(\\text{getting ace}) = P(\\text{ace of hearts}) + P(\\text{ace of diamonds}) + P(\\text{ace of clubs}) + P(\\text{ace of spades})\n\n\nThese rules/axioms ensure that our probability assignments make logical sense and match our intuitive understanding of chance and likelihood.\n\n\n\n\n\n\nImportant Terms in Probability Theory\n\n\n\nLet’s clarify some closely related but distinct concepts:\n\nProbabilistic Model consists of two fundamental elements:\n\nA sample space \\Omega (or S): the set of all possible outcomes\nA probability law that assigns probabilities to events (subsets of \\Omega)\n\nProbability Measure (P):\n\nThe formal mathematical function that maps events to numbers in [0,1]\nMust satisfy the three axioms (non-negativity, normalization, additivity)\nExample: P(A) gives the probability of event A occurring\n\nProbability Distribution:\n\nThe specific assignment of probabilities to all possible outcomes\nDescribes how probability is distributed across the sample space\nExample: For a fair die, {1: 1/6, 2: 1/6, 3: 1/6, 4: 1/6, 5: 1/6, 6: 1/6}\n\nProbability Law:\n\nOften used as a synonym for probability distribution\nCan also refer to the underlying rule generating the probabilities\nExample: “Each face of a fair die has equal probability”\n\n\nIn practice, these terms are interrelated: The probability measure implements the probability law, which determines the probability distribution, all within the context of a probabilistic model.\n\n\n\n\n\n\n\n\nVisualizing Sample Spaces\n\n\n\nIn probability theory and statistics, being able to visualize sample spaces is crucial for understanding possible outcomes and their relationships. We’ll explore three main approaches to visualizing sample spaces:\n\nVenn Diagrams\nTree Diagrams\nGrid/Matrix Diagrams\n\n\nVenn Diagrams\nVenn diagrams provide a powerful visual tool for understanding sample spaces.\n\nA Venn diagram is a graphical representation of sets and their relationships using (overlapping or disjoint) circles or other shapes.\nThink of each shape in a Venn diagram as a container that holds items with specific characteristics. Where these shapes overlap, we find items that share characteristics of multiple groups.\n\nIn probability theory, our sample space (usually denoted by Ω or S) represents all possible outcomes of an experiment. When we draw a Venn diagram, the rectangular frame represents this entire sample space (a universal set), with a probability of 1. Any event is then a subset of this space.\n\n\n\nTree Diagrams\nTree diagrams are particularly useful for visualizing sequential events and their outcomes. Here’s a tree diagram showing a simple probability experiment: We toss a fair coin twice.\n\n\n\n\n\ngraph LR\n    Start[Start] --&gt; H1[H]\n    Start --&gt; T1[T]\n    H1 --&gt; H2[H]\n    H1 --&gt; T2[T]\n    T1 --&gt; H3[H]\n    T1 --&gt; T3[T]\n    \n    H2 --&gt; HH([HH: 1/4])\n    T2 --&gt; HT([HT: 1/4])\n    H3 --&gt; TH([TH: 1/4])\n    T3 --&gt; TT([TT: 1/4])\n    \n    linkStyle 0,2,3 stroke:#1e88e5,stroke-width:2px\n    linkStyle 4,5 stroke:#ff5252,stroke-width:2px\n    linkStyle 1 stroke:#ff5252,stroke-width:2px\n    \n    style Start fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style H1 fill:#bbdefb,stroke:#1e88e5,stroke-width:2px\n    style T1 fill:#ffcdd2,stroke:#ff5252,stroke-width:2px\n    style H2 fill:#bbdefb,stroke:#1e88e5,stroke-width:2px\n    style T2 fill:#ffcdd2,stroke:#ff5252,stroke-width:2px\n    style H3 fill:#bbdefb,stroke:#1e88e5,stroke-width:2px\n    style T3 fill:#ffcdd2,stroke:#ff5252,stroke-width:2px\n    \n    style HH fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style HT fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style TH fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style TT fill:#f5f5f5,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\n\nGrid/Matrix Diagrams\nGrid diagrams are excellent for showing combinations of events.\nScenario: We have 7 balls in the bag:\n\n4 red balls (R₁, R₂, R₃, R₄)\n3 black balls (B₁, B₂, B₃)\nWe’ll draw 2 balls without replacement\n\nLet’s visualize the entire sample space using a grid where each cell represents selecting two balls in order (first draw → columns, second draw ↓ rows):\n\n\n\nFirst Draw →\nR₁\nR₂\nR₃\nR₄\nB₁\nB₂\nB₃\n\n\n\n\nSecond Draw ↓\n\n\n\n\n\n\n\n\n\nR₁\nX\n⚫\n⚫\n⚫\n⚪\n⚪\n⚪\n\n\nR₂\n⚫\nX\n⚫\n⚫\n⚪\n⚪\n⚪\n\n\nR₃\n⚫\n⚫\nX\n⚫\n⚪\n⚪\n⚪\n\n\nR₄\n⚫\n⚫\n⚫\nX\n⚪\n⚪\n⚪\n\n\nB₁\n⚪\n⚪\n⚪\n⚪\nX\n⚫\n⚫\n\n\nB₂\n⚪\n⚪\n⚪\n⚪\n⚫\nX\n⚫\n\n\nB₃\n⚪\n⚪\n⚪\n⚪\n⚫\n⚫\nX\n\n\n\nWhere:\n\nX: Impossible (same ball twice)\n⚫: Both same color (both red in upper-left, both black in lower-right)\n⚪: Different colors (red-black or black-red)\n\nFrom this grid:\n\nBoth red = 12 outcomes (⚫ in upper-left quadrant)\nBoth black = 6 outcomes (⚫ in lower-right quadrant)\nRed then black = 12 outcomes (⚪ in lower-left quadrant)\nBlack then red = 12 outcomes (⚪ in upper-right quadrant)\nTotal possible outcomes = 42 (remove seven diagonal X’s from 7 × 7 grid)\n\nTotal count verification: 12 + 6 + 12 + 12 = 42 outcomes\nNote: Each outcome is determined by reading first draw (column) then second draw (row).\n\nGrid/Matrix Diagrams with Unordered Pairs\nFor this modified scenario where order doesn’t matter, we need to adjust our counting since (R₁,B₁) and (B₁,R₁) would be considered the same outcome.\nScenario: We have 7 balls in the bag:\n\n4 red balls (R₁, R₂, R₃, R₄)\n3 black balls (B₁, B₂, B₃)\nWe’ll draw 2 balls without replacement\nOrder does NOT matter\n\nBecause order doesn’t matter, we only need to look at half of the grid, excluding the diagonal.\n\n\n\nFirst Draw →\nR₁\nR₂\nR₃\nR₄\nB₁\nB₂\nB₃\n\n\n\n\nSecond Draw ↓\n\n\n\n\n\n\n\n\n\nR₁\nX\n⚫\n⚫\n⚫\n⚪\n⚪\n⚪\n\n\nR₂\n–\nX\n⚫\n⚫\n⚪\n⚪\n⚪\n\n\nR₃\n–\n–\nX\n⚫\n⚪\n⚪\n⚪\n\n\nR₄\n–\n–\n–\nX\n⚪\n⚪\n⚪\n\n\nB₁\n–\n–\n–\n–\nX\n⚫\n⚫\n\n\nB₂\n–\n–\n–\n–\n–\nX\n⚫\n\n\nB₃\n–\n–\n–\n–\n–\n–\nX\n\n\n\nWhere:\n\nX: Impossible (same ball twice)\n⚫: Both same color\n⚪: Different colors\n–: Redundant (already counted in upper half)\n\nFrom this grid:\n\nBoth red = 6 outcomes (⚫ in upper-left quadrant)\nBoth black = 3 outcomes (⚫ in lower-right quadrant)\nOne red and one black = 12 outcomes (⚪ only counted once)\nTotal possible outcomes = 21 (half of the ordered outcomes: 42 ÷ 2)\n\nTotal count verification: 6 + 3 + 12 = 21 unordered outcomes\nNote: Each unordered pair {R₁,B₁} is counted only once, whereas in the ordered scenario we counted both (R₁,B₁) and (B₁,R₁).\n\n\n\n\n\n\nDiscrete Sample Spaces & Probability\nWhen we analyze random events like coin flips or dice rolls, we need a way to list all possible outcomes. This is where discrete sample spaces come in. Let’s break this down step by step:\n\nDiscrete Sample Spaces (S or \\Omega)\nThink of a sample space as a container holding all possible outcomes of a random event/experiment. In discrete sample spaces, we can count these outcomes one by one, like counting marbles in a bag. We write it as:\n\nFor finite events: S = \\{s_1, s_2, ..., s_n\\} [classical (‘naive’) probability]\nFor infinite but countable events: S = \\{s_1, s_2, ...\\}\n\n\n\nThree Essential Rules\n\nMust include everything possible (no missing outcomes)\nNo overlap between outcomes (each is unique)\nMust be countable (you can list them out)\n\n\n\nExamples\n1. Equally Likely Outcomes (uniform probability distribution)\nThese are scenarios where each basic outcome has the same probability:\n\nFair die: S = \\{1, 2, 3, 4, 5, 6\\}, each with probability \\frac{1}{6}\nFair deck: S = \\{\\text{52 cards}\\}, each with probability \\frac{1}{52}\nFair coin: S = \\{\\text{H}, \\text{T}\\}, each with probability \\frac{1}{2}\n\n2. Events with Different Probabilities\nSome (discrete) cases:\nBinomial Scenarios (Counting Successes)\n\nExample: Number of successes in n trials, each with probability p\nSample Space: S = \\{0, 1, 2, ..., n\\}\nOutcomes have different probabilities based on:\n\nNumber of ways to get k successes (\\binom{n}{k})\nProbability of each arrangement (p^k(1-p)^{n-k})\n\n\nGeometric Scenarios (Waiting for Success)\n\nExample: Number of trials until first success, probability p per trial\nSample Space: S = \\{1, 2, 3, ...\\}\nProbability decreases with number of trials\nP(\\text{first success on trial }n) = (1-p)^{n-1}p\n\nNote: The term “success” in probability simply means the outcome we’re tracking - it could be any event of interest. Each trial has two possible outcomes: success (probability p) or failure (probability 1-p).\n\n\nHow to Assign Probabilities\n1. Equal Chances (Classical Probability) When all outcomes are equally likely:\nP(\\text{one outcome}) = \\frac{1}{\\text{total outcomes}}\nExample: Rolling a fair die\n\nProbability of rolling a 4 = \\frac{1}{6}\n\n2. Mathematical Models (Probability Distribution Functions) For more complex situations, we use specific formulas (functions):\n\nMultiple trials (Binomial): P(X=k) = \\binom{n}{k}p^k(1-p)^{n-k}\nFirst success (Geometric): P(X=k) = p(1-p)^{k-1}\n\n3. Using Data (Empirical/Experimental Probability) When we have actual observations:\nP(\\text{outcome}) = \\frac{\\text{times outcome occurred}}{\\text{total observations}}\nExample: If you flip a coin 100 times and get 53 heads: P(\\text{heads}) = \\frac{53}{100} = 0.53\n\n\n\n\n\n\nTip\n\n\n\nClassical (“Naive”) Probability: Why Single Outcomes Have Equal Probabilities\nLet’s prove this step by step:\n\nStart with n equally likely outcomes: s_1, s_2, ..., s_n\nWe know the total probability must be 1 (rule 2 above)\nCall the probability of each outcome p\nSince outcomes are equally likely, each has the same probability p\nAdding up all outcomes: p + p + ... + p (n times) = 1\nTherefore: np = 1\nSolving for p: p = \\frac{1}{n}\n\nThis gives us the following rule under the assumption of equally likely outcomes:\nP(\\text{single outcome}) = \\frac{1}{\\text{number of possible outcomes}}\n\n\nImportant Considerations\n\nClassical probability (equal likelihood) is a special case\nMany real-world phenomena follow specific probability distributions (a probability distribution is the mathematical function that gives the probabilities of occurrence of possible outcomes for an experiment.)\nThe type of probability assignment depends on the context:\n\nPhysical symmetry → Classical probability\nRepeated independent trials → Binomial distribution\nRare events → Poisson distribution\nWaiting times → Geometric distribution\n(…)\n\nAll these cases work within discrete sample spaces\nThe probabilities must always sum to 1 over the entire sample space\n\nThis framework helps us systematically study discrete random variables and their probabilities, whether they follow uniform or non-uniform distributions.\n\n\n\n\n\n\nWarning\n\n\n\nThere is no single, universal formula for calculating probabilities across all probability spaces and situations.\nDifferent Types of Probability Spaces\n\nClassical (Finite, Equally Likely)\n\nOnly here we can use: P(A) = \\frac{\\text{favorable outcomes}}{\\text{total outcomes}}\nLimited to finite, equally likely cases\n\nGeneral Discrete\n\nMust specify individual probabilities\nExample: Loaded die needs experimental/empirical determination\nSum of probabilities must equal 1\n\nContinuous\n\nUses calculus and density functions\nProbabilities found by integration\nExample: P(a \\leq X \\leq b) = \\int_a^b f(x)dx\nNo universal formula for density function\n\nMixed/Hybrid\n\nCombines discrete and continuous elements\nDifferent methods needed for different parts\n\n\nWhy No Universal Formula?\n\nDifferent types of randomness need different mathematical tools\nNature of outcomes (discrete/continuous) affects calculation method\nPrior knowledge or assumptions shape probability calculation\nSome probabilities must be found empirically (frequentist/statistical probability) rather than calculated\n\n\n\n\n\n\n\n\n\nFrequentist/Statistical/Empirical Probability & the Law of Large Numbers\n\n\n\nFrequentist probability defines probability as the long-term relative frequency of an event’s occurrence in repeated trials under identical conditions:\nP(A) = \\lim_{n \\to \\infty} \\frac{\\text{number of times A occurs}}{n}\nThe Law of Large Numbers states that as we increase the number of trials, the observed frequency converges to the true probability:\n\nIf true probability of heads is 0.5\nIn 10 flips: might get 7 heads (frequency = 0.7)\nIn 1000 flips: might get 495 heads (frequency ≈ 0.495)\nAs trials → ∞, frequency → 0.5\n\nKey Characteristics:\n\nRequires repeatable experiments under identical conditions\nObjective approach - probability viewed as physical property\nCannot handle one-time events\nFoundation for classical statistical inference\n\nLimitation: We can never perform infinite trials, so we estimate probabilities from large but finite samples.\n\n\n\n\n\n\n\n\nClassical Probability: Equal-Likelihood in Discrete Sample Spaces\n\n\n\nClassical probability, also known as Laplace probability, applies when all outcomes in a finite sample space are equally likely to occur. This framework, developed by Pierre-Simon Laplace, provides a simple yet powerful way to calculate probabilities when symmetry exists.\n\nMathematical Foundation\nFor any event A in sample space S, the classical probability is calculated as:\nP(A) = \\frac{\\text{number of favorable outcomes}}{\\text{number of possible outcomes}} = \\frac{|A|}{|S|}\nThis definition automatically satisfies Kolmogorov’s probability axioms:\n\nNon-negativity: P(A) \\geq 0 for all A \\subseteq S\nNormalization: P(S) = \\frac{|S|}{|S|} = 1\nAdditivity: For disjoint events A and B, P(A \\cup B) = P(A) + P(B)\n\n\n\nKey Requirements\nTwo essential conditions must be met:\n\nThe sample space S must be finite\nEach elementary outcome s \\in S must be equally likely, with probability P(\\{s\\}) = \\frac{1}{|S|}\n\n\n\nCommon Applications\n\nFair Dice\n\nRolling a six: P(6) = \\frac{1}{6}\nRolling an even number: P(2,4,6) = \\frac{3}{6} = \\frac{1}{2}\nRolling a number greater than 4: P(5,6) = \\frac{2}{6} = \\frac{1}{3}\n\nPlaying Cards\n\nDrawing a heart: P(♥) = \\frac{13}{52} = \\frac{1}{4}\nDrawing a face card: P(J,Q,K) = \\frac{12}{52} = \\frac{3}{13}\nDrawing a red ace: P(\\text{A♥,A♦}) = \\frac{2}{52} = \\frac{1}{26}\n\nMultiple Coin Flips\n\nTwo fair coins: S = \\{HH, HT, TH, TT\\}\nP(\\text{exactly one head}) = \\frac{|\\{HT,TH\\}|}{|\\{HH,HT,TH,TT\\}|} = \\frac{2}{4} = \\frac{1}{2}\nP(\\text{at least one head}) = \\frac{|\\{HH,HT,TH\\}|}{|\\{HH,HT,TH,TT\\}|} = \\frac{3}{4}\n\n\n\n\nLimitations and Considerations\n\nEqually Likely Assumption\n\nThis framework fails for biased coins, loaded dice, or any scenario where outcomes aren’t equally likely\nIn such cases, we need empirical probability or other probability measures\n\nFinite Space Requirement\n\nCannot directly apply to infinite sample spaces\nRequires modification for continuous probability spaces\n\nSymmetry Assessment\n\nPhysical symmetry (as in fair dice) often suggests equal likelihood\nBut physical symmetry alone doesn’t guarantee equal probabilities in practice\n\n\n\n\nConnection to Other Probability Concepts\nClassical probability serves as a foundation for understanding more complex probability concepts:\n\nForms the basis for combinatorial probability\nProvides intuition for uniform distributions\nHelps in understanding probability density functions in continuous spaces\n\nNote: While classical probability is intuitive and mathematically elegant, real-world applications often require more general probability frameworks to handle non-uniform probabilities and infinite sample spaces.\n\n\n\n\n\n\nClassical (or ‘Naive’) Probability: The Equal-Likelihood Special Case in Discrete Sample Spaces\nClassical probability applies to finite sample spaces where all outcomes are equally likely to occur. The probability formula is:\nP(\\text{event}) = \\frac{\\text{number of favorable outcomes}}{\\text{total number of possible outcomes}}\nRequirements:\n\nFinite sample space (finite number of outcomes)\nAll outcomes equally likely\nTotal probability sums to 1\n\nExamples:\n\nFair die roll:\n\nP(\\text{rolling a 3}) = \\frac{1}{6}\nP(\\text{rolling an even number}) = \\frac{3}{6} = \\frac{1}{2}\n\nDrawing from a standard deck:\n\nP(\\text{drawing an ace}) = \\frac{4}{52} = \\frac{1}{13}\nP(\\text{drawing a heart}) = \\frac{13}{52} = \\frac{1}{4}\n\n\nKey Limitation:\nClassical probability fails when outcomes are not equally likely (e.g., loaded die) or when the sample space is infinite.\nThis is why it’s called “naive” probability - it assumes a simple, idealized situation where simple counting is sufficient.\n\nStarting Assumptions\nWhen developing classical probability theory, we begin with a probability experiment that has two key properties:\n\nThe sample space S is finite, with cardinality |S| = n\nAll elementary outcomes are equally likely\n\nWe can write our sample space explicitly as: S = \\{s_1, s_2, ..., s_n\\}\nThis leads us to classical (or Laplace) probability, which we’ll derive rigorously in the following section. This special case provides a foundation for understanding more complex probability scenarios and helps build crucial probabilistic intuition.\n\n\nCore Axioms of Probability Theory\nTo derive classical probability, we start with the aforementioned key probability axioms. These form the mathematical foundation for all probability theory, including the classical or ‘naive’ probability:\n\nFor any event A, P(A) \\geq 0 (Non-negativity)\nP(S) = 1 (Total probability)\nFor disjoint events A and B, P(A \\cup B) = P(A) + P(B) (Additivity)\n\n\n\nDeriving the Classical (‘Naive’) Probability Formula (Equal-Likelihood Case)\n\nStarting Point\nIn a fair game or unbiased experiment where all outcomes are equally likely, we can derive the famous “number of favorable outcomes divided by total outcomes” formula.\n\n\nThe Setup\nConsider a finite sample space S with n outcomes: \\{s_1, s_2, ..., s_n\\}\nEqual-Likelihood Assumption:\n\nEach outcome has the same probability p\nMathematically: P(\\{s_1\\}) = P(\\{s_2\\}) = ... = P(\\{s_n\\}) = p\n\n\n\nStep-by-Step Derivation\n\nUse Total Probability Axiom\n\nAll probabilities must sum to 1\nFor our n equally likely outcomes:\n\nP(S) = P(\\{s_1\\}) + P(\\{s_2\\}) + ... + P(\\{s_n\\}) = 1 \\underbrace{p + p + ... + p}_{n \\text{ terms}} = 1 np = 1 p = \\frac{1}{n} = \\frac{1}{|S|}\nCalculate Probability of Any Event A\n\nLet event A contain k outcomes\nBy the addition rule for disjoint events: P(A) = \\underbrace{p + p + ... + p}_{k \\text{ terms}} P(A) = k \\cdot \\frac{1}{|S|} = \\frac{k}{|S|} = \\frac{|A|}{|S|}\n\nThe Classical Probability Formula P(A) = \\frac{|A|}{|S|} = \\frac{\\text{number of favorable outcomes}}{\\text{total number of possible outcomes}}\n\n\n\n\nExamples to Illustrate\n\nFair Die Roll\n\nS = \\{1,2,3,4,5,6\\}, so |S| = 6\nFor getting an even number, A = \\{2,4,6\\}, so |A| = 3\nP(A) = \\frac{3}{6} = \\frac{1}{2}\n\nDrawing a Card\n\n|S| = 52 (total cards)\nFor drawing a king, |A| = 4\nP(\\text{king}) = \\frac{4}{52} = \\frac{1}{13}\n\n\n\n\nImportant Limitations\nThis formula only works when:\n\nSample space is finite (we can count outcomes)\nAll outcomes are equally likely\nEach outcome is distinct and well-defined\n\nIf any of these conditions fail (like with a loaded die), we need different methods to calculate probabilities.\n\n\nUnderstanding the Result\nThis derivation reveals several profound insights about classical probability:\nThe familiar “counting formula” (‘Naive’ probability) isn’t just an intuitive rule - it follows necessarily from our axioms combined with the equal-likelihood assumption. When we say outcomes are equally likely, we’re forced mathematically to assign each elementary outcome a probability of \\frac{1}{|S|}. This isn’t a choice but a requirement of the axioms.\nFor any event A, its probability is determined entirely by comparing two cardinalities: the size of the event (|A|) relative to the size of the sample space (|S|).\n\n\n\n\n\n\nFair Coin Toss Probability Space\n\n\n\nA fair coin toss experiment is defined by:\nSample Space:\n\\Omega = \\{H, T\\}\nEvent Space (collection of all possible events):\n\\mathcal{F} = \\{\\{H\\}, \\{T\\}, \\{H, T\\}, \\emptyset\\}\nProbability Measure:\n\nP(\\{H\\}) = P(\\{T\\}) = \\frac{1}{2} (probability of heads or tails)\nP(\\Omega) = P(\\{H, T\\}) = 1 (certainty)\nP(\\emptyset) = 0 (impossible event)\n\nKey Points:\n\nThis is a simple probability space that satisfies the axioms:\n\nP(A) \\geq 0 for all events A\nP(\\Omega) = 1\nP(A \\cup B) = P(A) + P(B) for disjoint events\n\nThe event space \\mathcal{F} includes:\n\nIndividual outcomes: \\{H\\} and \\{T\\}\nThe entire sample space: \\{H, T\\}\nThe empty set: \\emptyset\n\nThe probabilities are equal (\\frac{1}{2}) because it’s a fair coin\n\nThis example illustrates a complete probability space with its three components: sample space (\\Omega), event space (\\mathcal{F}), and probability measure (P).\n\n\n\n\nExample Application\nLet’s solidify this understanding by working through a concrete example. Consider rolling a fair six-sided die and finding P(\\text{even number}):\n\nFirst, identify the sample space: S = \\{1, 2, 3, 4, 5, 6\\}, giving us |S| = 6\nThen, identify the event: A = \\{2, 4, 6\\}, giving us |A| = 3\nApply the formula: P(\\text{even number}) = \\frac{|A|}{|S|} = \\frac{3}{6} = \\frac{1}{2}\n\n\n\n\n\n\n\nKey Questions Before Calculating Probabilities\n\n\n\nBefore we can correctly calculate probabilities in any discrete scenario, we must answer two fundamental questions:\n\nDoes Order Matter?\n\nThe importance of order fundamentally changes how we count outcomes. Consider selecting two cards from a deck:\n\nIf we’re playing poker, order doesn’t matter - getting an ace and then a king is the same hand as getting a king and then an ace.\nIf we’re performing a magic trick where we need specific cards in sequence, order matters - getting an ace then a king is different from getting a king then an ace.\n\nWhen order matters, we’re dealing with permutations. When order doesn’t matter, we’re dealing with combinations. This distinction dramatically affects the number of possible outcomes and, consequently, our probability calculations.\n\nIs Sampling With or Without Replacement?\n\nAfter selecting an item, do we put it back before the next selection? This question fundamentally changes the probability structure:\n\nWith replacement: Each selection has the same probability distribution as the first selection. Drawing a red ball and replacing it means the probability of drawing red on the next try remains unchanged.\nWithout replacement: Each selection changes the probability distribution for subsequent selections. Drawing a red ball and not replacing it means there are fewer red balls available for the next draw.\n\nThese sampling schemes lead to different probability models:\n\nWith replacement leads to independent events and often simpler calculations\nWithout replacement leads to dependent events and requires conditional probability\n\n\n\n\n\n\n\n\n\nUnderstanding When to ADD vs MULTIPLY Probabilities\n\n\n\n\nThe Key Principle\n\nADD when events represent different ways (paths) to achieve the same outcome\nMULTIPLY when events must occur in sequence (one after another)\n\nExample 1: Single Die Roll Consider events:\n\nA: “rolling an even number” = {2, 4, 6}\nB: “rolling a number &gt; 4” = {5, 6}\n\nP(A or B) requires ADDITION because we want any outcome satisfying either condition:\n\nP(A or B) = P(A) + P(B) - P(A and B)\n= 3/6 + 2/6 - 1/6 = 4/6\n\nExample 2: Two Coin Flips\nFor P(at least one heads):\n\nADD different successful paths: P(HT or TH or HH)\n= 1/4 + 1/4 + 1/4 = 3/4\n\nFor P(two heads):\n\nMULTIPLY along path: P(H) × P(H)\n= 1/2 × 1/2 = 1/4\n\n\n\nWhy This Works\n\nAddition combines different ways to succeed\nMultiplication reflects narrowing down possibilities with each sequential requirement\n\n\n\n\n\n\ngraph LR\n    Start[Start] --&gt; H1[H]\n    Start --&gt; T1[T]\n    H1 --&gt; H2[H]\n    H1 --&gt; T2[T]\n    T1 --&gt; H3[H]\n    T1 --&gt; T3[T]\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Classical Probability Through the Urn Example\nIn our urn with 3 green and 2 red balls:\n\nP(\\text{green}) = \\frac{3}{5}\nP(\\text{red}) = \\frac{2}{5}\nP(\\text{green}) + P(\\text{red}) = 1\n\nThe classical definition of probability assumes:\n\nA finite sample space \\Omega with equally likely outcomes (‘fair’ experiment)\nFor an event A, probability is defined as: P(A) = \\frac{\\text{favorable outcomes}}{\\text{total outcomes}}\n\nIn our urn with 3 green and 2 red balls, these assumptions manifest as:\n\nSample space \\Omega = \\{b_1, b_2, b_3, b_4, b_5\\} where each ball is equally likely\nFor green: P(\\text{green}) = \\frac{|\\text{green balls}|}{|\\Omega|} = \\frac{3}{5}\nFor red: P(\\text{red}) = \\frac{|\\text{red balls}|}{|\\Omega|} = \\frac{2}{5}\n\nKey probability axioms are demonstrated:\n\nNon-negativity: P(\\text{green}), P(\\text{red}) \\geq 0\nNormalization: P(\\Omega) = P(\\text{green}) + P(\\text{red}) = 1\nAdditivity: Since green and red are disjoint events, P(\\text{green or red}) = P(\\text{green}) + P(\\text{red})\n\nREMARK: Many probabilistic situations have the property that they involve a number of different possible outcomes, all of which are equally likely. For example, Heads and Tails on a coin are equally likely to be tossed, the numbers 1 through 6 on a die are equally likely to be rolled, and the ten balls in the above box are all equally likely to be picked.\n‘Naive’ (classical) probability definition assumes uniform probability measure (all outcomes equally likely), and finite uniform sample space.\nWhen considering shapes or elements of the same color in an urn or box, treating them as distinguishable allows you to assume a uniform sample space — equally likely outcomes.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#how-to-calculate-basic-probabilities",
    "href": "probability_en.html#how-to-calculate-basic-probabilities",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "15.5 How to Calculate Basic Probabilities",
    "text": "15.5 How to Calculate Basic Probabilities\nLet’s explore some fundamental probability concepts using a simple example with colored balls in an urn/bag. This example will help us understand:\n\nHow to calculate basic probabilities using the tree diagrams\nHow replacement affects probability\nHow the importance of order affects our calculations\nHow to break down probability problems into steps\n\nTree diagrams are powerful tools for visualizing sequential events. Each branch represents a possible outcome, and probabilities multiply along paths.\n\n\n\n\n\n\nUnderstanding Sampling Methods: Sequences vs Sets\n\n\n\nWhen we count possibilities (sample space size = |S|) in probability problems, we need to think about two important questions:\n\nDoes the order of our selections matter? (Like picking a phone PIN where 1234 is different from 4321)\nCan we reuse items we’ve already selected? (Like picking letters where we can reuse them, versus picking students where we can’t pick the same person twice)\n\nConsider sampling from elements \\{A, B, C\\}. The key distinction is whether we care about:\n\nSequences (ordered lists): where position matters\nSets: where only membership matters\n\n\n15.6 Sampling Scenarios\n\n\n\n\n\n\n\n\nSampling Method\nWith Replacement\nWithout Replacement\n\n\n\n\nSequences (Order Matters)\nOrdered lists with repetition:(A,A), (A,B), (A,C)(B,A), (B,B), (B,C)(C,A), (C,B), (C,C)\nOrdered lists without repetition:(A,B), (A,C)(B,A), (B,C)(C,A), (C,B)\n\n\nSets (Order Doesn’t Matter)\nMultisets (sets with repetition):\\{A,A\\}, \\{A,B\\}, \\{A,C\\}\\{B,B\\}, \\{B,C\\}\\{C,C\\}\nSets (no repetition):\\{A,B\\}, \\{A,C\\}\\{B,C\\}\n\n\n\n\n\n15.7 Mathematical Properties\n\nSequences (Order Matters)\n\nElements have positions: a_1, a_2, \\ldots, a_n\nTwo sequences \\mathbf{x}, \\mathbf{y} are equal iff x_i = y_i for all i\nDenoted as ordered tuples: (a_1, a_2, \\ldots, a_n)\n\nSets (Order Doesn’t Matter)\n\nElements have no position, only membership matters\nTwo sets are equal if they contain the same elements\nDenoted with curly braces: \\{a_1, a_2, \\ldots, a_n\\}\n\n\n\n\n15.8 Factorial Notation\nFor any non-negative integer n, the factorial of n (denoted as n!) is defined as:\nn! = n \\cdot (n-1) \\cdot (n-2) \\cdot ... \\cdot 2 \\cdot 1\nSpecial cases:\n\n0! = 1 (by definition)\n1! = 1\n2! = 2 \\cdot 1 = 2\n3! = 3 \\cdot 2 \\cdot 1 = 6\n4! = 4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\n\nThis can be written recursively as:\n\nn! = n \\cdot (n-1)! for n &gt; 0\n0! = 1\n\nHere’s why 0! equals 1:\n\nBy definition, for any positive integer n, n! = n × (n-1)!\nThis means 1! = 1 × 0!\nWe know 1! = 1\nTherefore: 1 = 1 × 0!\nSolving for 0!: 0! = 1\n\nThis definition is also consistent with the combinatorial interpretation - there is exactly one way to arrange zero elements.\n\n\n15.9 Number of Outcomes\nFor n distinct elements, selecting k items:\n\nSequences with Replacement\n\nEach position has n choices\nTotal: n^k outcomes\n\nSequences without Replacement\n\nPermutations: P(n,k) = \\frac{n!}{(n-k)!}\nEach next position has one fewer choice\n\nSets with Replacement\n\nCombinations with repetition allowed\nTotal: \\binom{n+k-1}{k} outcomes\n\nSets without Replacement\n\nCombinations: \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\nEach subset of size k counted once\n\n\n\n\n15.10 Key Relationships\n\nFor sequences vs sets without replacement:\nP(n,k) = k! \\cdot \\binom{n}{k}\nFor any sampling scheme:\n\\text{sequences} \\geq \\text{sets} \\text{with replacement} \\geq \\text{without replacement}\n\n\n\n\n\nExample 1: Drawing Two Balls from an Urn/Bag\nConsider drawing two balls from an urn containing 3 green and 2 red balls.\nFind the probabilities of the following random events:\n\nThe first ball is red and the second one is green (order matters, drawing without replacement)\nThe first ball is red and the second one is green (order matters, drawing with replacement)\nThe balls are of different colors (order doesn’t matter, drawing without replacement)\nThe balls are of different colors (order doesn’t matter, drawing with replacement)\n\nUnderstanding Event Types in Probability:\n\nSimple events represent a single outcome from a single random action, such as drawing one ball from an urn. The probability of a simple event is calculated directly from the number of favorable outcomes divided by the total possible outcomes.\nCompound events involve multiple outcomes or conditions that must occur together. These can occur simultaneously (like rolling two dice at once) or sequentially (like drawing two balls one after another). The key difference lies in whether the events happen at the same time or in sequence.\n\n\nSequential events are a specific type of compound events where outcomes occur in a particular order over time. Our urn example is particularly instructive here because it demonstrates sequential events through the process of drawing balls one after another. This allows us to explore how the probability of the second draw depends on what happened in the first draw (when sampling without replacement).\n\n\n\n\nSample spaces (S) visualized using the grid diagrams\n\n\nTo better understand how the sample space changes based on our sampling method, let’s examine two scenarios:\n\nWith Replacement\n\nWhen we sample with replacement, we return the ball to the urn after the first draw. This means:\n\nThe probability remains constant for each draw\nTotal possible outcomes: 25 (5×5 grid)\nEach outcome has equal probability\nP(\\text{both green}) = \\frac{3}{5} \\times \\frac{3}{5} = \\frac{9}{25}\nP(\\text{both red}) = \\frac{2}{5} \\times \\frac{2}{5} = \\frac{4}{25}\nP(\\text{mixed}) = \\frac{12}{25}\n\n\nWithout Replacement\n\nWhen we sample without replacement, the first draw affects the probability of the second draw:\n\nTotal possible outcomes: 20 (removing diagonal cells where same ball is drawn twice)\nSecond draw probabilities change based on first draw\nP(\\text{both green}) = \\frac{3}{5} \\times \\frac{2}{4} = \\frac{6}{20}\nP(\\text{both red}) = \\frac{2}{5} \\times \\frac{1}{4} = \\frac{2}{20}\nP(\\text{mixed}) = \\frac{12}{20}\n\nThe grid diagram above visualizes both scenarios, where:\n\nGreen cells represent both balls drawn being green\nRed cells represent both balls drawn being red\nOrange cells represent mixed outcomes (one green, one red)\nCrossed-out cells in the “Without Replacement” grid show impossible outcomes\n\nThis visualization helps demonstrate how the sample space and probabilities change between the two sampling methods, while maintaining the fundamental principle that probabilities must sum to 1 in both cases.\n\nDrawing Two Balls Without Replacement\n\nConsider drawing two balls from an urn containing 3 green and 2 red balls. Let’s analyze all scenarios systematically.\n\n\n\n\n\nflowchart TD\n    A([\"Initial State\\n3G, 2R\"]) --&gt; B[\"First: Green\\n3/5\"]\n    A --&gt; C[\"First: Red\\n2/5\"]\n    B --&gt; D[\"Second: Green\\n2/4\"]\n    B --&gt; E[\"Second: Red\\n2/4\"]\n    C --&gt; F[\"Second: Green\\n3/4\"]\n    C --&gt; G[\"Second: Red\\n1/4\"]\n    \n    D --&gt; H[\"GG: 3/5 × 2/4 = 6/20\"]\n    E --&gt; I[\"GR: 3/5 × 2/4 = 6/20\"]\n    F --&gt; J[\"RG: 2/5 × 3/4 = 6/20\"]\n    G --&gt; K[\"RR: 2/5 × 1/4 = 2/20\"]\n\n\n\n\n\n\nLet’s solve for different scenarios:\n\nFirst red, then green (order matters):\nP(R \\text{ then } G) = \\frac{2}{5} \\cdot \\frac{3}{4} = \\frac{6}{20} = 0.3\nDifferent colors (order doesn’t matter):\nP(\\text{different colors}) = P(R \\text{ then } G) + P(G \\text{ then } R)\n= \\frac{2}{5} \\cdot \\frac{3}{4} + \\frac{3}{5} \\cdot \\frac{2}{4} = \\frac{6}{20} + \\frac{6}{20} = \\frac{12}{20} = 0.6\n\n\n\nDrawing With Replacement\n\nWhen we replace the first ball before drawing the second, the probabilities for the second draw remain unchanged:\n\n\n\n\n\nflowchart TD\n    A([\"Initial State\\n3G, 2R\"]) --&gt; B[\"First: Green\\n3/5\"]\n    A --&gt; C[\"First: Red\\n2/5\"]\n    B --&gt; D[\"Second: Green\\n3/5\"]\n    B --&gt; E[\"Second: Red\\n2/5\"]\n    C --&gt; F[\"Second: Green\\n3/5\"]\n    C --&gt; G[\"Second: Red\\n2/5\"]\n    \n    D --&gt; H[\"GG: 3/5 × 3/5 = 9/25\"]\n    E --&gt; I[\"GR: 3/5 × 2/5 = 6/25\"]\n    F --&gt; J[\"RG: 2/5 × 3/5 = 6/25\"]\n    G --&gt; K[\"RR: 2/5 × 2/5 = 4/25\"]\n\n\n\n\n\n\nNow:\n\nFirst red, then green (order matters):\nP(R \\text{ then } G) = \\frac{2}{5} \\cdot \\frac{3}{5} = \\frac{6}{25} = 0.24\nDifferent colors (order doesn’t matter):\nP(\\text{different colors}) = \\frac{2}{5} \\cdot \\frac{3}{5} + \\frac{3}{5} \\cdot \\frac{2}{5} = \\frac{12}{25} = 0.48\n\nKey observations:\n\nWithout replacement:\n\nDifferent orders of the same colors have different probabilities\nThe second draw’s probability depends on the first outcome\n\nWith replacement:\n\nEach draw is independent\nProbabilities multiply directly because sample space remains unchanged\n\n\n\n\nExample 2: The 4 Red and 3 Black Balls Problem\nLet’s solve a real problem using what we learned. We have:\n\n4 red balls (let’s call them R₁, R₂, R₃, R₄)\n3 black balls (B₁, B₂, B₃)\nWe’ll draw 2 balls without replacement\nOrder doesn’t matter (like picking team members)\n\nWe want to find three probabilities:\n\nGetting two red balls\nGetting two black balls\nGetting one of each color\n\n\nMethod 1: Using Counting Rules\nFirst, let’s count the total possible outcomes:\n\nWe’re picking 2 balls from 7 total balls, order doesn’t matter\nTotal outcomes = \\binom{7}{2} = \\frac{7!}{2!(7-2)!} = \\frac{7 \\times 6}{2 \\times 1} = 21\n\nNow let’s find each probability:\n\nTwo Red Balls\n\n\nWe need to pick 2 red balls from 4 red balls\nThis is like picking 2 team members from 4 people\nNumber of ways = \\binom{4}{2} = \\frac{4 \\times 3}{2 \\times 1} = 6\nProbability = \\frac{6}{21}\n\n\nTwo Black Balls\n\n\nSimilarly, we need to pick 2 black balls from 3 black balls\nNumber of ways = \\binom{3}{2} = \\frac{3 \\times 2}{2 \\times 1} = 3\nProbability = \\frac{3}{21}\n\n\nOne Red and One Black\n\n\nWe need:\n\nOne red ball (we have 4 to choose from)\nOne black ball (we have 3 to choose from)\n\nNumber of ways = 4 \\times 3 = 12\nProbability = \\frac{12}{21}\n\nLet’s verify our work:\n\nAll probabilities should add to 1\n\\frac{6}{21} + \\frac{3}{21} + \\frac{12}{21} = \\frac{21}{21} = 1 ✓\n\nThis matches what we expect - every time we draw two balls, we must get either:\n\nTwo red balls\nTwo black balls\nOne of each color\n\nUnderstanding how to count correctly helps us solve these probability problems systematically and avoid common mistakes like counting the same outcome multiple times.\n\n\nMethod 2: Tree Diagram Approach\nThe tree diagram helps us visualize the sequential nature of the draws:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[First: Red 4/7]\n    A --&gt; C[First: Black 3/7]\n    B --&gt; D[Second: Red 3/6]\n    B --&gt; E[Second: Black 3/6]\n    C --&gt; F[Second: Red 4/6]\n    C --&gt; G[Second: Black 2/6]\n\n\n\n\n\n\nUsing the tree diagram:\n\nP(both red) = \\frac{4}{7} \\cdot \\frac{3}{6} = \\frac{12}{42} = \\frac{6}{21}\nP(red then black) = \\frac{4}{7} \\cdot \\frac{3}{6} = \\frac{12}{42}\nP(multi-colored) = P(red then black) + P(black then red)\n= \\frac{4}{7} \\cdot \\frac{3}{6} + \\frac{3}{7} \\cdot \\frac{4}{6} = \\frac{24}{42}\n\n\n\nMethod 3: Grid Diagram Analysis of Two-Ball Draws\nLet’s visualize the ordered sample space using a grid where rows represent the second draw and columns represent the first draw:\n\n\n\nFirst Draw →\nR₁\nR₂\nR₃\nR₄\nB₁\nB₂\nB₃\n\n\n\n\nSecond Draw ↓\n\n\n\n\n\n\n\n\n\nR₁\nX\n⚫\n⚫\n⚫\n⚪\n⚪\n⚪\n\n\nR₂\n⚫\nX\n⚫\n⚫\n⚪\n⚪\n⚪\n\n\nR₃\n⚫\n⚫\nX\n⚫\n⚪\n⚪\n⚪\n\n\nR₄\n⚫\n⚫\n⚫\nX\n⚪\n⚪\n⚪\n\n\nB₁\n⚪\n⚪\n⚪\n⚪\nX\n⚫\n⚫\n\n\nB₂\n⚪\n⚪\n⚪\n⚪\n⚫\nX\n⚫\n\n\nB₃\n⚪\n⚪\n⚪\n⚪\n⚫\n⚫\nX\n\n\n\nWhere:\n\nX: Impossible (same ball drawn twice)\n⚫: Both same color (both red in upper-left, both black in lower-right)\n⚪: Different colors (red-black or black-red)\n\nFrom this grid:\n\nBoth red = 12 outcomes (⚫ in upper-left quadrant)\nBoth black = 6 outcomes (⚫ in lower-right quadrant)\nRed then black = 12 outcomes (⚪ in lower-left quadrant)\nBlack then red = 12 outcomes (⚪ in upper-right quadrant)\nTotal possible outcomes = 42 (all cells minus 7 diagonal X’s)\n\n\n\nAnalysis of Two-Ball Draws\nFrom the grid, we can count the following outcomes:\n\nBoth red = 12 outcomes (⚫ in upper-left quadrant)\nRed then black = 12 outcomes (⚪ in lower-left quadrant)\nBlack then red = 12 outcomes (⚪ in upper-right quadrant)\nTotal possible outcomes = 42 (all cells minus 7 diagonal X’s)\n\nTherefore:\n\nP(both red) = \\frac{12}{42} = \\frac{2}{7}\nP(red then black) = \\frac{12}{42} = \\frac{2}{7}\nP(multi-colored) = \\frac{24}{42} = \\frac{4}{7} (includes both red-then-black and black-then-red)\n\n\n\nComparing the Methods\nEach method highlights different aspects of the problem:\n\nCounting Rules:\n\nMost efficient for calculation\nHelps understand combinations and arrangements\nMay obscure the actual outcomes\n\nTree Diagram:\n\nShows sequential nature of draws\nMakes conditional probability clear\nVisualizes how probabilities combine\nGood for checking intuition\n\nGrid Diagram:\n\nShows entire sample space explicitly\nMakes it clear why diagonal is impossible\nHelps visualize groups of outcomes\nDemonstrates why we divide by total possibilities\nShows symmetry in the problem",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#sampling-scenarios",
    "href": "probability_en.html#sampling-scenarios",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "15.6 Sampling Scenarios",
    "text": "15.6 Sampling Scenarios\n\n\n\n\n\n\n\n\nSampling Method\nWith Replacement\nWithout Replacement\n\n\n\n\nSequences (Order Matters)\nOrdered lists with repetition:(A,A), (A,B), (A,C)(B,A), (B,B), (B,C)(C,A), (C,B), (C,C)\nOrdered lists without repetition:(A,B), (A,C)(B,A), (B,C)(C,A), (C,B)\n\n\nSets (Order Doesn’t Matter)\nMultisets (sets with repetition):\\{A,A\\}, \\{A,B\\}, \\{A,C\\}\\{B,B\\}, \\{B,C\\}\\{C,C\\}\nSets (no repetition):\\{A,B\\}, \\{A,C\\}\\{B,C\\}",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#mathematical-properties",
    "href": "probability_en.html#mathematical-properties",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "15.7 Mathematical Properties",
    "text": "15.7 Mathematical Properties\n\nSequences (Order Matters)\n\nElements have positions: a_1, a_2, \\ldots, a_n\nTwo sequences \\mathbf{x}, \\mathbf{y} are equal iff x_i = y_i for all i\nDenoted as ordered tuples: (a_1, a_2, \\ldots, a_n)\n\nSets (Order Doesn’t Matter)\n\nElements have no position, only membership matters\nTwo sets are equal if they contain the same elements\nDenoted with curly braces: \\{a_1, a_2, \\ldots, a_n\\}",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#factorial-notation",
    "href": "probability_en.html#factorial-notation",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "15.8 Factorial Notation",
    "text": "15.8 Factorial Notation\nFor any non-negative integer n, the factorial of n (denoted as n!) is defined as:\nn! = n \\cdot (n-1) \\cdot (n-2) \\cdot ... \\cdot 2 \\cdot 1\nSpecial cases:\n\n0! = 1 (by definition)\n1! = 1\n2! = 2 \\cdot 1 = 2\n3! = 3 \\cdot 2 \\cdot 1 = 6\n4! = 4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\n\nThis can be written recursively as:\n\nn! = n \\cdot (n-1)! for n &gt; 0\n0! = 1\n\nHere’s why 0! equals 1:\n\nBy definition, for any positive integer n, n! = n × (n-1)!\nThis means 1! = 1 × 0!\nWe know 1! = 1\nTherefore: 1 = 1 × 0!\nSolving for 0!: 0! = 1\n\nThis definition is also consistent with the combinatorial interpretation - there is exactly one way to arrange zero elements.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#number-of-outcomes",
    "href": "probability_en.html#number-of-outcomes",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "15.9 Number of Outcomes",
    "text": "15.9 Number of Outcomes\nFor n distinct elements, selecting k items:\n\nSequences with Replacement\n\nEach position has n choices\nTotal: n^k outcomes\n\nSequences without Replacement\n\nPermutations: P(n,k) = \\frac{n!}{(n-k)!}\nEach next position has one fewer choice\n\nSets with Replacement\n\nCombinations with repetition allowed\nTotal: \\binom{n+k-1}{k} outcomes\n\nSets without Replacement\n\nCombinations: \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\nEach subset of size k counted once",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#key-relationships",
    "href": "probability_en.html#key-relationships",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "15.10 Key Relationships",
    "text": "15.10 Key Relationships\n\nFor sequences vs sets without replacement:\nP(n,k) = k! \\cdot \\binom{n}{k}\nFor any sampling scheme:\n\\text{sequences} \\geq \\text{sets} \\text{with replacement} \\geq \\text{without replacement}",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#problem-solutions-1",
    "href": "probability_en.html#problem-solutions-1",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "15.11 Problem Solutions (1)",
    "text": "15.11 Problem Solutions (1)\n\nProblem 1: Two-Ball Drawing from an Urn\nAn urn contains 3 red, 2 blue, and 1 yellow balls. Two balls are drawn sequentially without replacement. We need to find the probability that the balls drawn are different colors.\n\nInitial Conditions\nLet’s first state our starting conditions:\n\nTotal number of balls: n = 3 + 2 + 1 = 6\nDistribution of balls:\n\nRed: n_R = 3\nBlue: n_B = 2\nYellow: n_Y = 1\n\n\n\n\nVisual Representation\nLet’s visualize all possible outcomes using a tree diagram:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[\"R (3/6)\"]\n    A --&gt; C[\"B (2/6)\"]\n    A --&gt; D[\"Y (1/6)\"]\n    \n    B --&gt; E[\"B (2/5)\"]\n    B --&gt; F[\"Y (1/5)\"]\n    B --&gt; G[\"R (2/5)\"]\n    \n    C --&gt; H[\"R (3/5)\"]\n    C --&gt; I[\"Y (1/5)\"]\n    C --&gt; J[\"B (1/5)\"]\n    \n    D --&gt; K[\"R (3/5)\"]\n    D --&gt; L[\"B (2/5)\"]\n    D --&gt; M[\"Y (0/5)\"]\n    \n    E --&gt; N[\"RB (Success)\"]\n    F --&gt; O[\"RY (Success)\"]\n    G --&gt; P[\"RR (Fail)\"]\n    H --&gt; Q[\"BR (Success)\"]\n    I --&gt; R[\"BY (Success)\"]\n    J --&gt; S[\"BB (Fail)\"]\n    K --&gt; T[\"YR (Success)\"]\n    L --&gt; U[\"YB (Success)\"]\n    M --&gt; V[\"YY (Fail)\"]\n\n\n\n\n\n\n\n\nProbability Calculation\nLet’s calculate the probability of drawing different colors systematically:\n\nStarting with Red (probability \\frac{3}{6}):\n\nRed → Blue: P(R,B) = \\frac{3}{6} \\cdot \\frac{2}{5} = \\frac{6}{30}\nRed → Yellow: P(R,Y) = \\frac{3}{6} \\cdot \\frac{1}{5} = \\frac{3}{30}\n\nStarting with Blue (probability \\frac{2}{6}):\n\nBlue → Red: P(B,R) = \\frac{2}{6} \\cdot \\frac{3}{5} = \\frac{6}{30}\nBlue → Yellow: P(B,Y) = \\frac{2}{6} \\cdot \\frac{1}{5} = \\frac{2}{30}\n\nStarting with Yellow (probability \\frac{1}{6}):\n\nYellow → Red: P(Y,R) = \\frac{1}{6} \\cdot \\frac{3}{5} = \\frac{3}{30}\nYellow → Blue: P(Y,B) = \\frac{1}{6} \\cdot \\frac{2}{5} = \\frac{2}{30}\n\n\n\n\nFinal Solution\nThe total probability of drawing two different colored balls is the sum of all favorable outcomes:\n\n\\begin{align*}\nP(\\text{different colors}) &= P(R,B) + P(R,Y) + P(B,R) + P(B,Y) + P(Y,R) + P(Y,B) \\\\\n&= \\frac{6}{30} + \\frac{3}{30} + \\frac{6}{30} + \\frac{2}{30} + \\frac{3}{30} + \\frac{2}{30} \\\\\n&= \\frac{22}{30} \\\\\n&= \\frac{11}{15} \\\\\n&\\approx 0.733 \\text{ or } 73.3\\%\n\\end{align*}\n\n\n\nVerification\nThis result aligns with our intuition because:\n\nThe sample space contains more ways to draw different colors than same colors\nThe complementary probability (drawing same colors) would be \\frac{4}{15} or about 26.7%\nSince same-color draws are limited to RR, BB, and YY combinations, it makes sense that different-color draws are more likely\n\n\n\n\nProblem 2: Die and Coin Probability Exercise\nLet’s analyze the probability of getting heads OR tails OR three dots when flipping both a coin and a die. This problem offers an excellent opportunity to explore probability unions and the importance of careful counting.\n\nUnderstanding the Problem Space\nIn our experiment:\n\nWe flip a coin (possible outcomes: heads, tails)\nWe roll a die (possible outcomes: 1, 2, 3, 4, 5, 6 dots)\nThese events occur simultaneously\n\nLet’s start with a visualization:\n\n\n\n\n\ngraph TD\n    A[Experiment] --&gt; B[Coin]\n    A --&gt; C[Die]\n    B --&gt; D[Heads]\n    B --&gt; E[Tails]\n    C --&gt; F[1 dot]\n    C --&gt; G[2 dots]\n    C --&gt; H[3 dots]\n    C --&gt; I[4 dots]\n    C --&gt; J[5 dots]\n    C --&gt; K[6 dots]\n\n\n\n\n\n\n\n\nCommon Mistakes and Overcounting Analysis\nA common first instinct might be to simply add the individual probabilities:\nP(heads) + P(tails) + P(three dots) = \\frac{1}{2} + \\frac{1}{2} + \\frac{1}{6} = \\frac{7}{6}\nThis incorrect approach reveals several important issues:\n\nThe result exceeds 1, which is impossible for a probability\nWe’ve counted many outcomes multiple times\nWe’ve failed to recognize event overlaps\n\nLet’s analyze the overcounting:\n\n\n\n\n\ngraph TD\n    A[Overcounting Analysis] --&gt; B[Heads counted: 6/12]\n    A --&gt; C[Tails counted: 6/12]\n    A --&gt; D[Three dots counted: 2/12]\n    B --&gt; E[Including three with heads: 1/12]\n    C --&gt; F[Including three with tails: 1/12]\n    E --&gt; G[Double counted!]\n    F --&gt; G\n\n\n\n\n\n\n\n\nCorrect Solution Using Set Theory\nLet’s solve this properly using set theory:\n\nSet H: All outcomes with heads\nSet T: All outcomes with tails\nSet 3: All outcomes with three dots\n\nKey insights:\n\nSets H and T are mutually exclusive\nSet 3 is entirely contained within H ∪ T\nTherefore, P(H ∪ T ∪ 3) = P(H ∪ T) = 1\n\nWe can write this formally:\nP(H ∪ T ∪ 3) = P(H) + P(T) - P(H ∩ T) + P(3) - P(3 ∩ (H ∪ T)) = \\frac{1}{2} + \\frac{1}{2} - 0 + \\frac{1}{6} - \\frac{1}{6} = 1\n\n\nSample Space Analysis\n\n\n\n\n\ngraph TD\n    A[Total Outcomes: 12] --&gt; B[Heads: 6]\n    A --&gt; C[Tails: 6]\n    B --&gt; D[With three: 1]\n    C --&gt; E[With three: 1]\n    D --&gt; F[Already counted in heads]\n    E --&gt; G[Already counted in tails]\n\n\n\n\n\n\nThis visual representation helps us understand why:\n\nThe sample space has 12 total outcomes (2 × 6)\nThe three-dot outcomes are already included in heads and tails counts\nAdding P(three dots) would lead to double counting\n\n\n\nKey Learning Points\nThis problem illustrates several fundamental probability concepts:\n\nExhaustive Events: Heads and tails together cover all possible coin outcomes, making additional events redundant unless they introduce new dimensions.\nDouble Counting Protection: The inclusion-exclusion principle helps us avoid counting outcomes multiple times.\nSample Space Structure: Understanding your sample space structure (12 total outcomes) helps verify solution logic.\n\n\n\n\nProblem 3: Laplace’s Two-Draw Probability Problem\nSuppose there are two urns of coloured marbles:\n\nUrn X contains 3 black marbles, 1 white.\nUrn Y contains 1 black marble, 3 white.\n\nI flip a fair coin to decide which urn to draw from, heads for Urn X and tails for Urn Y. Then I draw marbles at random.\nLaplace asked what happens if we do two draws, with replacement. What’s the probability both draws will come up black?\nLet’s solve this fascinating probability problem involving two draws with replacement. This is a particularly interesting case because the replacement aspect affects how we think about sequential probabilities.\n\nUnderstanding the Initial Setup\nFirst, let’s clarify our starting conditions:\nUrn X (selected with heads):\n\n3 black marbles, 1 white marble\nTotal: 4 marbles\nP(black|X) = \\frac{3}{4}\n\nUrn Y (selected with tails):\n\n1 black marble, 3 white marbles\nTotal: 4 marbles\nP(black|Y) = \\frac{1}{4}\n\nLet’s visualize this with a tree diagram showing all possible paths:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[Urn X 1/2]\n    A --&gt; C[Urn Y 1/2]\n    \n    B --&gt; D[Draw 1 Black 3/4]\n    B --&gt; E[Draw 1 White 1/4]\n    \n    C --&gt; F[Draw 1 Black 1/4]\n    C --&gt; G[Draw 1 White 3/4]\n    \n    D --&gt; H[Draw 2 Black 3/4]\n    D --&gt; I[Draw 2 White 1/4]\n    \n    E --&gt; J[Draw 2 Black 3/4]\n    E --&gt; K[Draw 2 White 1/4]\n    \n    F --&gt; L[Draw 2 Black 1/4]\n    F --&gt; M[Draw 2 White 3/4]\n    \n    G --&gt; N[Draw 2 Black 1/4]\n    G --&gt; O[Draw 2 White 3/4]\n\n\n\n\n\n\n\n\nStep-by-Step Solution\nLet’s break this down into manageable steps:\n\nFirst, consider the urn selection:\n\nP(Urn X) = P(heads) = \\frac{1}{2}\nP(Urn Y) = P(tails) = \\frac{1}{2}\n\nFor two black draws from Urn X:\n\nP(black and black|X) = \\frac{3}{4} \\times \\frac{3}{4} = \\frac{9}{16}\nP(X and both black) = \\frac{1}{2} \\times \\frac{9}{16} = \\frac{9}{32}\n\nFor two black draws from Urn Y:\n\nP(black and black|Y) = \\frac{1}{4} \\times \\frac{1}{4} = \\frac{1}{16}\nP(Y and both black) = \\frac{1}{2} \\times \\frac{1}{16} = \\frac{1}{32}\n\nTotal probability (using the law of total probability): P(both black) = P(X and both black) + P(Y and both black) = \\frac{9}{32} + \\frac{1}{32} = \\frac{10}{32} = \\frac{5}{16} ≈ 0.3125 or about 31.25%\n\n\n\nKey Insights from This Problem\n\nReplacement Matters:\n\nBecause we replace after the first draw, the probabilities remain constant for the second draw\nThis is different from drawing without replacement, where probabilities would change\n\nConditional Independence:\n\nOnce we know which urn we’re using, the draws are independent\nHowever, the draws are not unconditionally independent\n\nLaw of Total Probability:\n\nWe needed to consider both paths (Urn X and Urn Y) to find the total probability\nEach path’s contribution is weighted by the probability of selecting that urn",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#core-probability-rules",
    "href": "probability_en.html#core-probability-rules",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "15.12 Core Probability Rules",
    "text": "15.12 Core Probability Rules\n\nThe Complement Rule\nThe complement rule is one of the most fundamental concepts in probability theory. For any event A, there’s always the possibility that A doesn’t occur. We call this the complement of A, written as A' or A^c.\nThe complement rule states:\nP(A') = 1 - P(A)\nThis makes intuitive sense because any outcome must either be in A or in A’ (but not both), and something must happen (the total probability must be 1).\nReal-World Example: Consider a weather forecast that predicts a 70% chance of rain tomorrow. Using the complement rule, we can immediately calculate that there’s a 30% chance it won’t rain:\nP(\\text{no rain}) = 1 - P(\\text{rain}) = 1 - 0.70 = 0.30\nAnother Example: In a game of roulette, what’s the probability of not landing on red? There are 18 red numbers, 18 black numbers, and 2 green numbers (0 and 00) on a roulette wheel. Therefore:\nP(\\text{red}) = \\frac{18}{38} P(\\text{not red}) = 1 - \\frac{18}{38} = \\frac{20}{38}\n\n\nThe Addition/Sum Rule\nWhen we want to find the probability of either one event OR another occurring, we use the addition rule. However, we need to be careful about double-counting outcomes that are in both events.\nFor any two events A and B:\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\nThe term P(A \\cap B) represents the probability of both events occurring simultaneously. We subtract it to avoid counting these outcomes twice.\nReal-World Example: In a college class, 65% of students play sports, 45% are in clubs, and 25% do both. What percentage of students are involved in either sports or clubs?\nP(\\text{sports or clubs}) = 65\\% + 45\\% - 25\\% = 85\\%\nFor mutually exclusive events (events that cannot occur simultaneously), P(A \\cap B) = 0, so the formula simplifies to:\nP(A \\cup B) = P(A) + P(B)\nExample: When rolling a die, what’s the probability of rolling either a 1 or a 6? Since these outcomes can’t happen simultaneously:\nP(1 \\text{ or } 6) = P(1) + P(6) = \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{3}\n\n\nConditional Probability, the Multiplication Rule, and Bayes’ Theorem\n\nUnderstanding Conditional Probability\nConditional probability represents how the probability of one event changes when we have information about another event. Let’s start with a simple example:\nImagine you have a deck of 52 playing cards. What’s the probability of drawing a King given that you’ve drawn a face card (Jack, Queen, or King)?\nTo solve this:\n\nTotal face cards = 12 (4 each of Jack, Queen, King)\nNumber of Kings = 4\nP(\\text{King}|\\text{Face Card}) = \\frac{P(\\text{King} \\cap \\text{Face Card})}{P(\\text{Face Card})} = \\frac{4/52}{12/52} = \\frac{1}{3}\n\nThis illustrates the fundamental formula for conditional probability:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\nIn the context of classical probability, where all outcomes are equally likely, we can express these probabilities in terms of the number of favorable outcomes:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{|A \\cap B|/|S|}{|B|/|S|} = \\frac{|A \\cap B|}{|B|}\nwhere:\n\n|A \\cap B| represents the number of outcomes in both events A and B\n|B| represents the number of outcomes in event B\n|S| represents the total number of outcomes in the sample space\n\nThis is why in our card example: P(\\text{King}|\\text{Face Card}) = \\frac{|\\text{Kings}|}{|\\text{Face Cards}|} = \\frac{4}{12} = \\frac{1}{3}\nThe formula shows that in classical probability, conditional probability is simply the ratio of the number of outcomes favorable to both events to the number of outcomes in the conditioning event.\n\n\nMedical Testing Example\nLet’s explore a more practical example involving medical testing. Consider a disease that affects 1% of the population and a test T with the following characteristics:\n\nSensitivity (true positive rate): P(T^+|D^+) = 0.95\nSpecificity (true negative rate): P(T^-|D^-) = 0.98\n\nWe can organize this information in a cross table for a population of 10,000:\n\n\n\nTest/Disease\nD^+ Present\nD^- Absent\nTotal\n\n\n\n\nT^+\n95\n198\n293\n\n\nT^-\n5\n9,702\n9,707\n\n\nTotal\n100\n9,900\n10,000\n\n\n\nUsing this, we can calculate important probabilities like:\n\nPositive Predictive Value: P(D^+|T^+) = \\frac{95}{293} \\approx 0.32\nNegative Predictive Value: P(D^-|T^-) = \\frac{9,702}{9,707} \\approx 0.999\n\nNote that:\n\nP(D^+) = 0.01 (disease prevalence)\nP(T^+|D^-) = 0.02 (false positive rate = 1 - specificity)\nP(T^-|D^+) = 0.05 (false negative rate = 1 - sensitivity)\n\nThese probabilities show how a test with seemingly good characteristics (95% sensitivity and 98% specificity) can still lead to many false positives when the condition being tested for is rare in the population.\n\n\n\n\n\n\nUrn Example with Probability Tree\n\n\n\nConsider an urn containing:\n\n3 blue marbles (B)\n\n2 marked with star (S^+)\n1 unmarked (S^-)\n\n2 red marbles (R)\n\n1 marked with star (S^+)\n1 unmarked (S^-)\n\n\nLet’s calculate the probability of drawing a starred marble given that we drew a blue marble.\nUsing the tree diagram:\n\n\n\n\n\ngraph LR\n    Ω{Ω} --&gt; B[B: 3/5]\n    Ω --&gt; R[R: 2/5]\n    B --&gt; BS+[\"S⁺|B: 2/3\"]\n    B --&gt; BS-[\"S⁻|B: 1/3\"]\n    R --&gt; RS+[\"S⁺|R: 1/2\"]\n    R --&gt; RS-[\"S⁻|R: 1/2\"]\n    \n    style Ω fill:#e6e6ff,stroke:#333,stroke-width:2px,color:#000\n    style B fill:#ccf2ff,stroke:#333,stroke-width:2px,color:#000\n    style R fill:#ffe6e6,stroke:#333,stroke-width:2px,color:#000\n    style BS+ fill:#f2f2f2,stroke:#333,stroke-width:1px,color:#000\n    style BS- fill:#f2f2f2,stroke:#333,stroke-width:1px,color:#000\n    style RS+ fill:#f2f2f2,stroke:#333,stroke-width:1px,color:#000\n    style RS- fill:#f2f2f2,stroke:#333,stroke-width:1px,color:#000\n    \n    linkStyle default stroke:#333,stroke-width:1px\n\n\n\n\n\n\nFrom this tree, we can calculate:\n\nP(B) = \\frac{3}{5}\nP(S^+|B) = \\frac{2}{3} (probability of star given blue)\nP(B \\cap S^+) = \\frac{3}{5} \\cdot \\frac{2}{3} = \\frac{2}{5}\n\nWe can verify the conditional probability formula:\nP(S^+|B) = \\frac{P(B \\cap S^+)}{P(B)} = \\frac{2/5}{3/5} = \\frac{2}{3}\nOther probabilities from this scenario:\n\nP(R) = \\frac{2}{5}\nP(S^+|R) = \\frac{1}{2}\nP(S^+) = P(B)P(S^+|B) + P(R)P(S^+|R) = \\frac{3}{5} \\cdot \\frac{2}{3} + \\frac{2}{5} \\cdot \\frac{1}{2} = \\frac{3}{5}\n\nThis example illustrates how:\n\nThe probability tree helps visualize sequential events\nBranch probabilities multiply along paths\nThe conditional probability formula naturally emerges from the tree structure\nThe law of total probability can be visualized as summing across different paths\n\n\n\n\n\n\n\n\n\nSequential Drawing Example\n\n\n\nConsider drawing two balls from an urn containing 3 blue (B) and 2 red (R) balls without replacement. Let’s find the probability of drawing two blue balls.\n\n\n\n\n\ngraph LR\n    Ω{Start} --&gt; B1[\"B₁: 3/5\"]\n    Ω --&gt; R1[\"R₁: 2/5\"]\n    B1 --&gt; B2[\"B₂|B₁: 2/4\"]\n    B1 --&gt; R2[\"R₂|B₁: 2/4\"]\n    R1 --&gt; B3[\"B₂|R₁: 3/4\"]\n    R1 --&gt; R3[\"R₂|R₁: 1/4\"]\n    \n    style Ω fill:#e6e6ff,stroke:#333,stroke-width:2px,color:#000\n    style B1 fill:#ccf2ff,stroke:#333,stroke-width:2px,color:#000\n    style R1 fill:#ffe6e6,stroke:#333,stroke-width:2px,color:#000\n    style B2 fill:#ccf2ff,stroke:#333,stroke-width:1px,color:#000\n    style R2 fill:#ffe6e6,stroke:#333,stroke-width:1px,color:#000\n    style B3 fill:#ccf2ff,stroke:#333,stroke-width:1px,color:#000\n    style R3 fill:#ffe6e6,stroke:#333,stroke-width:1px,color:#000\n    \n    linkStyle default stroke:#333,stroke-width:1px\n\n\n\n\n\n\nLet’s calculate various probabilities:\n\nTwo blue balls (B₁ and B₂):\n\nP(B_1) = \\frac{3}{5} (first draw)\nP(B_2|B_1) = \\frac{2}{4} (second draw given first was blue)\nP(B_1 \\cap B_2) = \\frac{3}{5} \\cdot \\frac{2}{4} = \\frac{3}{10}\n\nBlue then Red:\n\nP(R_2|B_1) = \\frac{2}{4}\nP(B_1 \\cap R_2) = \\frac{3}{5} \\cdot \\frac{2}{4} = \\frac{3}{10}\n\nRed then Blue:\n\nP(R_1) = \\frac{2}{5}\nP(B_2|R_1) = \\frac{3}{4}\nP(R_1 \\cap B_2) = \\frac{2}{5} \\cdot \\frac{3}{4} = \\frac{3}{10}\n\nTwo red balls:\n\nP(R_2|R_1) = \\frac{1}{4}\nP(R_1 \\cap R_2) = \\frac{2}{5} \\cdot \\frac{1}{4} = \\frac{1}{10}\n\n\nKey observations:\n\nThe probability of second draw depends on first outcome (conditional probability)\nTotal probability = 1: \\frac{3}{10} + \\frac{3}{10} + \\frac{3}{10} + \\frac{1}{10} = 1\nNotice that P(B_1 \\cap R_2) = P(R_1 \\cap B_2) due to symmetry\nThe denominator changes after first draw (4 balls remain)\n\nThis example illustrates how:\n\nProbabilities update based on previous outcomes\nThe multiplication rule applies to sequential events\nSample space reduces after each draw\nOrder can matter in sequential probability calculations\n\n\n\n\n\n\n\n\n\nConditional Probability: A Geometric Perspective\n\n\n\nConditional probability answers the question: “Given that we know event B has occurred, what is the probability that event A will occur?” We write this as P(A|B), read as “the probability of A given B.”\nThe formal definition is:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\nGeometrically, we can visualize this as:\n\nThe original sample space \\Omega represented as a rectangle\nEvent B as a region within \\Omega\nThe intersection A \\cap B as the overlap between regions A and B\nConditional probability as the ratio of the overlap area to the area of B\n\nThis visualization helps understand why we divide by P(B) - we’re essentially creating a new probability space where B is our universe.\n\n\n\n\n\n\n\n\nWhy P(A|B) ≠ P(B|A)?\n\n\n\nImagine these two questions:\n\nWhat’s the probability it’s raining (A) given there are clouds (B)?\nWhat’s the probability there are clouds (B) given it’s raining (A)?\n\nClearly, these are different:\n\nP(A|B): Among all cloudy days, how many are rainy?\nP(B|A): Among all rainy days, how many are cloudy?\n\nP(B|A) would be close to 1 (almost all rainy days have clouds) While P(A|B) might be around 0.3 (not all cloudy days bring rain)\n\nWhen are they equal?\n\nWhen events are independent:\n\nP(A|B) = P(A) and P(B|A) = P(B)\n\nWhen events have symmetric relationship:\n\nDrawing cards: P(\\text{red}|\\text{face}) = P(\\text{face}|\\text{red})\nBoth equal \\frac{6}{26} = \\frac{3}{13}\n\nWhen applying Bayes: if P(A|B) = P(B|A), then P(A) = P(B)\n\nFrom P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\nIf P(A|B) = P(B|A), then P(A) = P(B)\n\n\n\n\n\n\n\nThe Law of Total Probability\n\n\n\n\n\n\nSample Space Partitions\n\n\n\nImagine a sample space \\Omega as a complete population where every individual must be classified into exactly one category. This is what a partition does - it divides our universe of possibilities into distinct, non-overlapping groups that together include all possibilities.\nConsider how we might partition a population:\n\nLet A_1 = “Category 1”\nLet A_2 = “Category 2”\nLet A_3 = “Category 3”\n\nThese classifications form a partition because:\n\nComplete Coverage (\\Omega = A_1 \\cup A_2 \\cup ... \\cup A_n):\n\nEvery outcome in the sample space must belong to exactly one category\nNothing can be left unclassified\nThe categories together capture all possibilities\n\nMutual Exclusivity (A_i \\cap A_j = \\emptyset for i \\neq j):\n\nEach outcome belongs to exactly one category\nCategories cannot overlap\nBeing in one category excludes being in any other\n\n\nThis framework becomes powerful when:\n\nCalculating total probability (sum across all categories)\nUpdating beliefs with new information\nBreaking complex problems into manageable pieces\n\nThink of it like organizing a filing system: each document must go into exactly one folder (mutual exclusivity), and every document must be filed somewhere (complete coverage). When we get new information, we might need to update our filing system, but we always maintain these two key properties.\nThe power of partitioning lies in its ability to help us systematically organize possibilities and update probabilities as new information becomes available. This forms the foundation for understanding more complex concepts like the law of total probability and Bayes’ theorem.\n\n\nThe law of total probability is a fundamental bridge between conditional probabilities and overall probabilities. Given a partition \\{A_1, A_2, ..., A_n\\} of the sample space, for any event B:\nP(B) = \\sum_{i=1}^n P(B|A_i)P(A_i)\nVisually, this represents:\n\nBreaking the sample space into disjoint “slices” (the partition)\nFinding the probability of B within each slice (P(B|A_i))\nWeighting each slice by its probability (P(A_i))\nSumming all contributions\n\nExample: In a tech company:\n\n40% of employees are developers (A_1)\n35% are managers (A_2)\n25% are other roles (A_3)\n\nTo find the probability of an employee working remotely (B):\n\n80% of developers work remotely: P(B|A_1) = 0.80\n60% of managers work remotely: P(B|A_2) = 0.60\n40% of other roles work remotely: P(B|A_3) = 0.40\n\nUsing the law of total probability:\nP(B) = (0.80)(0.40) + (0.60)(0.35) + (0.40)(0.25) = 0.64\nSo 64% of all employees work remotely.\n\n\nThe Multiplication Rule\nThe conditional probability formula can be rearranged to give us the multiplication rule:\nP(A \\cap B) = P(A|B) \\cdot P(B) = P(B|A) \\cdot P(A)\nThis symmetry is crucial because it shows:\n\nWe can compute joint probabilities in two ways\nThe order of conditioning doesn’t matter\nBoth perspectives must yield the same result\n\n\n\nBayes’ Theorem: From Prior to Posterior Beliefs\nLet’s derive Bayes’ theorem starting from the fundamental definitions of conditional probability and using the multiplication rule.\n\nStarting Point: Conditional Probability\nThe conditional probability formula for two events A and B is:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\nSimilarly, we can write:\nP(B|A) = \\frac{P(A \\cap B)}{P(A)}\n\n\nMultiplication Rule\nFrom either of these formulas, we can derive the multiplication rule:\nP(A \\cap B) = P(B|A) \\cdot P(A) or equivalently P(A \\cap B) = P(A|B) \\cdot P(B)\n\n\nDeriving Bayes’ Theorem\n\nStart with the conditional probability formula:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\nUse the multiplication rule to express the intersection:\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\nThis gives us Bayes’ theorem in its basic form. The denominator P(B) can be expanded using the law of total probability:\nP(B) = P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)\nLeading to the full form:\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)}\nThis derivation shows how Bayes’ theorem emerges naturally from the basic rules of probability, allowing us to “reverse” conditional probabilities and update prior beliefs with new evidence.\nUsing the law of total probability for the denominator:\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)}\nThe formula has three key components:\n\nPrior probability: P(A) - initial belief about event A\nLikelihood: P(B|A) - probability of evidence B given A is true\nNormalizing constant: P(B) - ensures probabilities sum to 1\n\n\n\nExample 1: Playing Cards\nLet’s start with a simple example using cards:\nSuppose we draw a card and are told it’s red. What’s the probability it’s a face card?\nGiven:\n\nP(\\text{Face}) = 12/52 = 3/13 (prior)\nP(\\text{Red}|\\text{Face}) = 6/12 = 1/2 (likelihood)\nP(\\text{Red}) = 26/52 = 1/2 (normalizing constant)\n\nUsing Bayes’ theorem: P(\\text{Face}|\\text{Red}) = \\frac{(1/2)(3/13)}{1/2} = \\frac{3}{13}\n\n\nExample 2: Medical Testing\nA more practical application involves medical diagnostics. Let’s formalize the terminology:\nTesting Framework:\n\nConditions:\n\nD^+: Disease present\nD^-: Disease absent\n\nTest Results:\n\nT^+: Positive test\nT^-: Negative test\n\n\nKey Metrics:\n\nSensitivity: P(T^+|D^+) - True Positive Rate\nSpecificity: P(T^-|D^-) - True Negative Rate\nPPV: P(D^+|T^+) - Positive Predictive Value\nNPV: P(D^-|T^-) - Negative Predictive Value\n\nThese relationships can be visualized in a confusion matrix:\n\n\n\n\nD^+\nD^-\n\n\n\n\nT^+\nTP\nFP\n\n\nT^-\nFN\nTN\n\n\n\nwhere:\n\nTP: True Positives\nTN: True Negatives\nFP: False Positives (Type I error)\nFN: False Negatives (Type II error)\n\nExample Calculation: Consider a test for a rare disease where:\n\nPrevalence: P(D^+) = 0.01 (1%)\nSensitivity: P(T^+|D^+) = 0.95 (95%)\nSpecificity: P(T^-|D^-) = 0.98 (98%)\n\nWhat’s the probability of having the disease given a positive test?\nUsing Bayes’ theorem:\nP(D^+|T^+) = \\frac{P(T^+|D^+) \\cdot P(D^+)}{P(T^+|D^+) \\cdot P(D^+) + P(T^+|D^-) \\cdot P(D^-)}\nP(D^+|T^+) = \\frac{(0.95)(0.01)}{(0.95)(0.01) + (0.02)(0.99)} \\approx 0.32\nThis counterintuitive result (only 32% chance of disease despite a positive test) illustrates the base rate fallacy - when the condition is rare, even a highly accurate test can have a low positive predictive value.\nThis example shows why Bayes’ theorem is crucial in medical decision-making, as it properly accounts for both the test’s accuracy and the disease’s prevalence in the population.\n\n\nSpam Filtering Example\nEmail spam filters are a perfect real-world application of Bayes’ theorem. Let’s see how it works:\nConsider a single word “lottery” in an email. We want to know: given that an email contains this word, what’s the probability it’s spam?\nLet’s define our events:\n\nS: Email is spam\nW: Email contains the word “lottery”\n\nWe need:\n\nPrior probability: P(S) = 0.30 (30% of all emails are spam)\nLikelihood: P(W|S) = 0.20 (20% of spam emails contain “lottery”)\nFalse positive rate: P(W|S') = 0.001 (0.1% of legitimate emails contain “lottery”)\n\nUsing Bayes’ theorem:\nP(S|W) = \\frac{(0.20)(0.30)}{(0.20)(0.30) + (0.001)(0.70)} \\approx 0.989\nSo if an email contains “lottery,” there’s a 98.9% chance it’s spam!\nReal spam filters:\n\nLook at multiple words and features\nUpdate probabilities continuously based on user feedback\nCombine evidence using the multiplication rule for independent events\nUse logarithms to avoid numerical underflow with many multiplications\n\n\n\nWeather Forecasting Example\nAnother practical application is weather forecasting. Suppose we want to know if it will rain tomorrow given certain atmospheric conditions:\nLet’s define:\n\nR: It rains tomorrow\nC: Current atmospheric conditions (high pressure system)\n\nGiven:\n\nP(R) = 0.25 (25% chance of rain on any day)\nP(C|R) = 0.10 (10% of rainy days have high pressure)\nP(C|R') = 0.70 (70% of non-rainy days have high pressure)\n\nUsing Bayes’ theorem:\nP(R|C) = \\frac{(0.10)(0.25)}{(0.10)(0.25) + (0.70)(0.75)} \\approx 0.045\nSo given high pressure, there’s only about a 4.5% chance of rain tomorrow.\n\n\n\nKey Interconnections and Applications\nThese concepts form a unified framework with wide-ranging applications:\n\nConditional probability provides the foundation for understanding dependent events\nThe multiplication rule enables complex probability calculations\nTotal probability helps break down complex scenarios into manageable pieces\nBayes’ theorem combines these tools to update probabilities with new evidence\n\nModern Applications:\n\nMachine Learning: Naive Bayes classifiers for text categorization\nMedical Diagnosis: Interpreting test results and screening procedures\nQuality Control: Identifying defective products based on test results\nRisk Assessment: Updating risk probabilities with new information\nNatural Language Processing: Sentiment analysis and language modeling\nForensics: Evaluating evidence in legal cases\nRecommender Systems: Predicting user preferences\n\nUnderstanding these relationships helps in:\n\nChoosing the right probabilistic tool for a given problem\nBreaking complex problems into manageable pieces\nAvoiding common probability misconceptions\nMaking better decisions under uncertainty\nBuilding intuition for machine learning algorithms\n\n\n\n\nIndependent and Disjoint Events\nUnderstanding the difference between independent and disjoint events is crucial for correctly applying probability rules. The key insight is that these concepts are fundamentally different - in fact, disjoint events are always dependent (except in trivial cases).\n\nIndependent Events\nEvents A and B are independent if knowing that one occurred doesn’t affect the probability of the other occurring. Mathematically, this means any of these equivalent conditions:\n\nP(A|B) = P(A)\nP(B|A) = P(B)\nP(A \\cap B) = P(A) \\cdot P(B)\n\nExample 1: Flipping a fair coin twice\n\nLet A = “heads on first flip” and B = “heads on second flip”\nP(A) = \\frac{1}{2} and P(B) = \\frac{1}{2}\nP(A \\cap B) = \\frac{1}{4} = P(A) \\cdot P(B)\nTherefore, the flips are independent\n\n\n\nDisjoint (Mutually Exclusive) Events\nEvents A and B are disjoint if they cannot occur simultaneously:\nP(A \\cap B) = 0\nExample 2: Rolling a die\n\nLet A = “rolling a 6” and B = “rolling an odd number”\nP(A \\cap B) = 0 (can’t be both 6 and odd)\nThese events are disjoint\n\n\n\nWhy Disjoint Events are Always Dependent\nLet’s prove that disjoint events (with non-zero probabilities) must be dependent:\n\nFor disjoint events: P(A \\cap B) = 0\nFor events to be independent, we need: P(A \\cap B) = P(A) \\cdot P(B)\nTherefore, for disjoint events to be independent:\n\n0 = P(A) \\cdot P(B) (from 1 and 2)\nThis equation is only true if either P(A) = 0 or P(B) = 0\nBut if either probability is 0, the event is impossible and trivial\n\nFor any non-trivial disjoint events:\n\nAssume P(A) &gt; 0 and P(B) &gt; 0 (considering non-trivial cases)\nThen P(B|A) = \\frac{P(A \\cap B)}{P(A)} = \\frac{0}{P(A)} = 0\nSince P(B) &gt; 0 (by our assumption of non-trivial cases)\nWe have P(B|A) = 0 \\neq P(B)\nThis inequality proves the events are dependent\n\n\nThe assumption P(B) \\neq 0 is crucial because:\n\nIf we allowed P(B) = 0, then P(B|A) = P(B) would be true (both equal to 0)\nThis would mean the events are technically independent\nBut this is a trivial case where B is an impossible event\n\nExample 3: Rolling a die illustrates dependence\n\nLet A = “rolling a 1” and B = “rolling a 2”\nP(A) = \\frac{1}{6} and P(B) = \\frac{1}{6} (both non-zero)\nP(B|A) = 0 (if we rolled a 1, we definitely didn’t roll a 2)\nBut P(B) = \\frac{1}{6}\nTherefore P(B|A) \\neq P(B), showing dependence\n\n\n\nKey Insights\n\nIndependence means events don’t affect each other’s probabilities\nDisjoint means events can’t occur together\nThese concepts are almost opposites:\n\nIndependent events can occur together\nDisjoint events must affect each other’s probabilities\n\nThe only case where events can be both independent and disjoint is when at least one event has probability 0 (impossible event)\n\nThis understanding is crucial for correctly applying probability rules and avoiding common misconceptions in probability calculations.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#problem-solutions-2",
    "href": "probability_en.html#problem-solutions-2",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "15.13 Problem Solutions (2)",
    "text": "15.13 Problem Solutions (2)\n\nProblem 1: Colored Balls - At Least One Red\nQuestion: A bag contains 5 red and 3 blue marbles. Two marbles are drawn simultaneously from the bag. What is the probability that at least one marble is red?\nDetailed Solution:\n\nApproach 1: Using Tree Diagram and Complement Rule\nThe key idea here is that finding the probability of “at least one red” directly can be complex, but finding its complement - “no red balls” (all blue) - is simpler.\nThen we can use P(\\text{at least one red}) = 1 - P(\\text{no red}).\nLet’s visualize this with a diagram:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[First Draw]\n    B --&gt; C[\"Blue (3/8)\"]\n    B --&gt; D[\"Red (5/8)\"]\n    C --&gt; E[\"Blue (2/7)\"]\n    C --&gt; F[\"Red (5/7)\"]\n    D --&gt; G[\"Blue (3/7)\"]\n    D --&gt; H[\"Red (4/7)\"]\n    \n    E --&gt; I[\"P = 3/8 * 2/7&lt;br/&gt;All Blue\"]\n    F --&gt; J[\"P = 3/8 * 5/7&lt;br/&gt;At least one Red\"]\n    G --&gt; K[\"P = 5/8 * 3/7&lt;br/&gt;At least one Red\"]\n    H --&gt; L[\"P = 5/8 * 4/7&lt;br/&gt;At least one Red\"]\n\n    style I fill:#f9f9f9,stroke:#333\n    style J fill:#f9f9f9,stroke:#333\n    style K fill:#f9f9f9,stroke:#333\n    style L fill:#f9f9f9,stroke:#333\n\n\n\n\n\n\nNow let’s solve using the complement rule:\nP(\\text{at least one red}) = 1 - P(\\text{no red})\nTo get no red marbles, we need to draw both blue marbles:\n\nTotal marbles: 8\nBlue marbles: 3\nP(\\text{first blue}) = \\frac{3}{8}\nP(\\text{second blue}|\\text{first blue}) = \\frac{2}{7}\n\nP(\\text{no red}) = P(\\text{both blue}) = \\frac{3}{8} \\times \\frac{2}{7} = \\frac{6}{56} = \\frac{3}{28}\nTherefore:\nP(\\text{at least one red}) = 1 - \\frac{3}{28} = \\frac{25}{28} \\approx 0.893 or about 89.3%\n\n\nApproach 2: Using Combinations\nThe combinations approach involves finding all possible ways to select 2 marbles out of 8, then subtracting the unfavorable outcomes (selecting 2 blue marbles).\nLet’s understand combinations first:\n\nA combination represents the number of ways to select r items from n items where order doesn’t matter\nNotation: C(n,r) or \\binom{n}{r}\nFormula: C(n,r) = \\frac{n!}{r!(n-r)!}\n\nFor this problem:\n\nTotal possible outcomes = C(8,2) = 28 ways to select 2 marbles from 8\nUnfavorable outcomes = C(3,2) = 3 ways to select 2 blue marbles from 3 blue marbles\nFavorable outcomes = C(8,2) - C(3,2) = 28 - 3 = 25\n\nTherefore:\nP(\\text{at least one red}) = \\frac{25}{28} \\approx 0.893 or about 89.3%\nBoth methods give us the same result! The combinations approach is often more elegant for problems involving simultaneous selection, while the tree diagram approach helps visualize the problem better and is particularly useful when events happen in sequence.\n\n\n\nProblem 2: Probability of Drawing Diamonds or Tens\nFrom a standard deck of 52 cards, find the probability of drawing either a diamond or a ten.\n\nSetup\nLet’s define our events:\n\nLet D = “drawing a diamond”\nLet T = “drawing a ten”\n\nWe need to find P(D \\cup T)\n\n\nSolution Using the Addition Rule\n\nIndividual Probabilities\nFor diamonds:\n\nNumber of diamonds = 13\nP(D) = \\frac{13}{52} = \\frac{1}{4}\n\nFor tens:\n\nNumber of tens = 4\nP(T) = \\frac{4}{52} = \\frac{1}{13}\n\nIntersection\n\nThe ten of diamonds is counted in both events\nP(D \\cap T) = \\frac{1}{52}\n\nAddition Rule\nP(D \\cup T) = P(D) + P(T) - P(D \\cap T)\n= \\frac{13}{52} + \\frac{4}{52} - \\frac{1}{52}\n= \\frac{16}{52} - \\frac{1}{52}\n= \\frac{15}{52}\n\\approx 0.288 or about 28.8%\n\n\n\nVerification\nWe can verify this result is reasonable because:\n\nUpper Bound Check\n\nIf we simply added P(D) and P(T): \\frac{13}{52} + \\frac{4}{52} = \\frac{17}{52}\nOur answer must be less than this due to double counting\n\nLower Bound Check\n\nOur answer must be greater than the larger individual probability (\\frac{13}{52})\n\\frac{15}{52} &gt; \\frac{13}{52} ✓\n\n\n\n\nTeaching Notes\nThis problem illustrates several important concepts:\n\nAddition Rule Application\n\nWhy we can’t simply add probabilities\nThe role of intersection in avoiding double counting\n\nFraction Arithmetic\n\nWorking with common denominators\nSimplifying fractions (if desired)\n\nSet Theory Visualization\n\nThe problem can be illustrated with a Venn diagram\nShows why subtraction of intersection is necessary\n\nReasonableness Checks\n\nUsing bounds to verify answers\nUnderstanding why certain values are impossible\n\n\n\n\n\nProblem 3a: Introduction to Conditional Probability\nA tech company has 100 employees who work on various projects. The company records show that:\n\n60 employees work on Project A\n45 employees work on Project B\n25 employees work on both projects\n\nThe HR manager randomly selects one employee. Given that this employee works on Project A, what is the probability they also work on Project B?\n\nStep-by-Step Solution\n\nDefine Events\n\nLet A = “employee works on Project A”\nLet B = “employee works on Project B”\nWe need to find P(B|A)\n\nReview the Conditional Probability Formula\nP(B|A) = \\frac{P(A \\cap B)}{P(A)}\nIdentify Known Values\n\nTotal employees: n = 100\nNumber working on A: n_A = 60\nNumber working on B: n_B = 45\nNumber working on both: n_{A \\cap B} = 25\n\nCalculate Probabilities\n\nP(A) = \\frac{60}{100} = 0.6\nP(A \\cap B) = \\frac{25}{100} = 0.25\n\nApply the Formula\nP(B|A) = \\frac{0.25}{0.6} = \\frac{25}{60} \\approx 0.417\n\n\n\nInterpretation\nThere is about a 41.7% chance that an employee works on Project B, given that they work on Project A. In other words, among the 60 employees who work on Project A, 25 of them (41.7%) also work on Project B.\n\n\nTeaching Notes\nThis problem helps students understand:\n\nBasic Concepts\n\nThe difference between joint probability P(A \\cap B) and conditional probability P(B|A)\nWhy P(B|A) is not the same as P(A \\cap B)\nThe role of the denominator P(A) in “restricting the sample space”\n\nVisual Representation\n\nThe problem can be illustrated with a Venn diagram:\n\nOne circle for Project A (60)\nOne circle for Project B (45)\nOverlap shows both projects (25)\nTotal space represents all employees (100)\n\n\nCommon Misconceptions\n\nStudents often confuse P(B|A) with P(A \\cap B)\nThey might think P(B|A) = P(B)\nThey might mix up P(B|A) and P(A|B)\n\nExtensions\n\nCalculate P(A|B) for comparison\nFind the probability of working on exactly one project\nConsider what happens if projects were independent\n\n\n\n\n\nProblem 3b: Colored Balls with Replacement and Addition\nQuestion: A box contains 5 red and 3 green balls. One ball is drawn at random, its color is noted, and it is replaced back. Then one more ball of the same color is added. Then a second ball is drawn. What is the probability that both balls drawn are green?\nDetailed Solution:\nThis is a sequential probability problem where the probability of the second event depends on the outcome of the first. Let’s solve it step by step:\n\nDefine our events:\n\nG₁ = first ball is green\nG₂ = second ball is green\nWe want P(G₁ ∩ G₂)\n\nCalculate P(G₁):\n\nInitially: 3 green balls out of 8 total\nP(G_1) = \\frac{3}{8}\n\nCalculate P(G₂|G₁):\n\nIf first ball was green:\n\nAfter replacement and adding another green: 4 green balls out of 9 total\n\nP(G_2|G_1) = \\frac{4}{9}\n\nApply the multiplication rule:\nP(G_1 \\cap G_2) = P(G_1) \\cdot P(G_2|G_1) = \\frac{3}{8} \\cdot \\frac{4}{9} = \\frac{12}{72} = \\frac{1}{6} \\approx 0.167 or about 16.7%\n\nUnderstanding the Solution:\n\nThe probability is relatively low because we need two specific events to occur in sequence\nThe addition of a ball of the same color as the first draw creates a dependency between the draws\nIf we had simply replaced the first ball without adding another, the draws would have been independent\n\n\n\nProblem 4a: Bayesian Analysis of Medical Test Results\nA medical test for disease D has the following characteristics:\n\nSensitivity (true positive rate): P(T=1|D=1) = 0.95\nSpecificity (true negative rate): P(T=0|D=0) = 0.95\nPrior probability (disease prevalence): P(D=1) = 0.001 (1/1000)\nTest result for Alicia: Positive (T=1)\n\nWe need to find the posterior probability that Alicia has the disease given a positive test result: P(D=1|T=1)\n\nDerivation Using Bayes’ Theorem\nStarting with the conditional probability formula:\nP(D=1|T=1) = \\frac{P(T=1|D=1)P(D=1)}{P(T=1)}\nThe denominator P(T=1) can be expanded using the law of total probability:\nP(T=1) = P(T=1|D=1)P(D=1) + P(T=1|D=0)P(D=0)\n\n\nComponents Analysis\n\nPrior: P(D=1) = 0.001\n\nComplement: P(D=0) = 0.999\n\nLikelihood:\n\nP(T=1|D=1) = 0.95 (sensitivity)\nP(T=0|D=0) = 0.95 (specificity)\nP(T=1|D=0) = 1 - P(T=0|D=0) = 0.05 (false positive rate)\n\nTotal Probability (denominator): P(T=1) = (0.95)(0.001) + (0.05)(0.999) = 0.00095 + 0.04995 = 0.0509\nPosterior Calculation:\n\nP(D=1|T=1) = \\frac{(0.95)(0.001)}{0.0509} = \\frac{0.00095}{0.0509} \\approx 0.0187\n\n\nInterpretation\nDespite receiving a positive test result, the probability that Alicia has disease D is only about 1.87%. This counterintuitive result is known as the “Bayesian flip” or “base rate fallacy.”\n\n\nWhy is the Probability So Low?\n\nBase Rate Consideration:\n\nThe very low prevalence (1/1000) means that in a population of 1000 women:\n\n1 woman has the disease\n999 women don’t have the disease\n\n\nTest Results in Population:\n\nOf the 1 woman with disease:\n\n0.95 will test positive (true positive)\n\nOf the 999 women without disease:\n\nAbout 50 will test positive (false positives)\n\n\nRatio Analysis:\n\nAmong all positive tests (≈51), only about 1 is a true positive\nThis explains why P(D=1|T=1) is so low\n\n\n\n\nTeaching Notes\nThis problem illustrates several important concepts:\n\nThe distinction between conditional probabilities:\n\nP(T=1|D=1) (sensitivity)\nP(D=1|T=1) (positive predictive value)\n\nThe crucial role of base rates in Bayesian reasoning\nWhy medical professionals should:\n\nConsider prevalence when interpreting test results\nBe cautious about testing asymptomatic patients\nConsider confirmatory testing for positive results\n\nThe importance of communicating probabilistic information effectively to patients\nThe mathematical relationship between:\n\nPrior probabilities\nTest characteristics (sensitivity/specificity)\nPosterior probabilities\n\n\n\n\n\nProblem 4b: COVID-19 Test Analysis\nQuestion: Given a COVID-19 test with:\n\nSensitivity (P(T=1|D=1)) = 87.5%\nSpecificity (P(T=0|D=0)) = 97.5%\nDisease prevalence (P(D=1)) = 10% Find P(D=1|T=1), the probability that a person with a positive test actually has the disease.\n\nDetailed Solution:\nThis is a perfect application of Bayes’ Theorem. Let’s break it down:\n\nDefine our variables:\n\nD=1: Person has COVID-19\nD=0: Person doesn’t have COVID-19\nT=1: Test is positive\nT=0: Test is negative\n\nGiven information:\n\nP(T=1|D=1) = 0.875 (sensitivity)\nP(T=0|D=0) = 0.975 (specificity)\nP(D=1) = 0.1 (prevalence)\n\nCalculate additional probabilities:\n\nP(D=0) = 1 - P(D=1) = 0.9\nP(T=1|D=0) = 1 - P(T=0|D=0) = 0.025 (false positive rate)\n\nApply Bayes’ Theorem: P(D=1|T=1) = \\frac{P(T=1|D=1) \\cdot P(D=1)}{P(T=1)}\nCalculate P(T=1) using the law of total probability: P(T=1) = P(T=1|D=1)P(D=1) + P(T=1|D=0)P(D=0) = (0.875)(0.1) + (0.025)(0.9) = 0.0875 + 0.0225 = 0.11\nNow we can complete Bayes’ Theorem: P(D=1|T=1) = \\frac{(0.875)(0.1)}{0.11} = \\frac{0.0875}{0.11} \\approx 0.795 or about 79.5%\n\nUnderstanding the Result:\nThis result tells us that even with a positive test, there’s still about a 20.5% chance that the person doesn’t have COVID-19. This might seem surprising, but it’s due to the relatively low prevalence of the disease (10%) in the population. This is known as the base rate fallacy - even a test with good sensitivity and specificity can have a significant false positive rate when the condition being tested for is rare.\n\n\nProblem 5: Conditional Probability: Marble Drawing with Coin Flip\nWe have a probability experiment involving two boxes of marbles and a fair coin:\nBox X1:\n\n2 black marbles\n3 red marbles\nTotal: 5 marbles\n\nBox X2:\n\n1 black marble\n1 red marble\nTotal: 2 marbles\n\nA fair coin is flipped to select the box (heads for X1, tails for X2), then one marble is drawn.\nVisual Representation\nLet’s create a tree diagram to visualize all possible outcomes and their probabilities:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[X1 1/2]\n    A --&gt; C[X2 1/2]\n    \n    B --&gt; D[Black 2/5]\n    B --&gt; E[Red 3/5]\n    \n    C --&gt; F[Black 1/2]\n    C --&gt; G[Red 1/2]\n    \n    D --&gt; H[Black & X1]\n    E --&gt; I[Red & X1]\n    F --&gt; J[Black & X2]\n    G --&gt; K[Red & X2]\n\n\n\n\n\n\n\nSolution\nLet’s solve each part step by step:\n\n\nP(Black | X1)\nThis is the probability of drawing a black marble given that we selected Box X1.\nP(Black | X1) = \\frac{\\text{Number of black marbles in X1}}{\\text{Total marbles in X1}} = \\frac{2}{5}\nThis is a direct probability from the contents of Box X1. We only consider Box X1’s marbles since we’re given that Box X1 was selected.\n\n\nP(Black and X1)\nThis is the probability of both selecting Box X1 and drawing a black marble.\nP(Black and X1) = P(X1) × P(Black | X1) = \\frac{1}{2} \\times \\frac{2}{5} = \\frac{1}{5}\nWe multiply these probabilities because both events must occur (intersection).\n\n\nP(Black)\nThis is the total probability of drawing a black marble from either box. We use the law of total probability:\nP(Black) = P(X1) × P(Black | X1) + P(X2) × P(Black | X2) = \\frac{1}{2} \\times \\frac{2}{5} + \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{5} + \\frac{1}{4} = \\frac{4}{20} + \\frac{5}{20} = \\frac{9}{20}\n\n\nP(X1 | Black)\nThis is the probability that we selected Box X1 given that we drew a black marble. We use Bayes’ Theorem:\nP(X1 | Black) = \\frac{P(Black | X1) \\times P(X1)}{P(Black)} = \\frac{\\frac{2}{5} \\times \\frac{1}{2}}{\\frac{9}{20}} = \\frac{\\frac{1}{5}}{\\frac{9}{20}} = \\frac{4}{9}\n\n\nKey Concepts Demonstrated\n\nConditional Probability: Shown in P(Black | X1), where we consider probability within a subset of outcomes\nMultiplication Rule: Used in finding P(Black and X1), where we multiply probabilities of sequential events\nLaw of Total Probability: Applied in finding P(Black), where we consider all possible ways an event can occur\nBayes’ Theorem: Used to find P(X1 | Black), reversing the direction of conditioning\n\n\n\n\nProblem 6: Probability of Intersecting Events and Independence Analysis\nYou roll a fair die. What is the probability of getting an even number (A) and the number greater or equal to 4 (B)? Are events A and B independent?\nLet’s explore this problem by first understanding what each event means, then calculating their probabilities both separately and together, and finally examining their independence.\n\nUnderstanding the Events\nLet’s first identify what numbers satisfy each condition on a standard six-sided die:\nEvent A (Even numbers): {2, 4, 6} Event B (Numbers ≥ 4): {4, 5, 6}\nWe can visualize this using a Venn diagram:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[\"R (3/6)\"]\n    A --&gt; C[\"B (2/6)\"]\n    A --&gt; D[\"Y (1/6)\"]\n    \n    B --&gt; E[\"B (2/5)\"]\n    B --&gt; F[\"Y (1/5)\"]\n    B --&gt; G[\"R (2/5)\"]\n    \n    C --&gt; H[\"R (3/5)\"]\n    C --&gt; I[\"Y (1/5)\"]\n    C --&gt; J[\"B (1/5)\"]\n    \n    D --&gt; K[\"R (3/5)\"]\n    D --&gt; L[\"B (2/5)\"]\n    D --&gt; M[\"Y (0/5)\"]\n    \n    E --&gt; N[\"RB (Success)\"]\n    F --&gt; O[\"RY (Success)\"]\n    G --&gt; P[\"RR (Fail)\"]\n    H --&gt; Q[\"BR (Success)\"]\n    I --&gt; R[\"BY (Success)\"]\n    J --&gt; S[\"BB (Fail)\"]\n    K --&gt; T[\"YR (Success)\"]\n    L --&gt; U[\"YB (Success)\"]\n    M --&gt; V[\"YY (Fail)\"]\n\n\n\n\n\n\n\n\nCalculating P(A ∩ B)\nTo find the probability of getting both an even number AND a number greater than or equal to 4:\n\nFirst, let’s identify the numbers that satisfy both conditions:\n\nMust be even AND ≥ 4\nNumbers that satisfy both: {4, 6}\n\nTherefore: P(A ∩ B) = \\frac{\\text{number of favorable outcomes}}{\\text{total number of possible outcomes}} = \\frac{2}{6} = \\frac{1}{3}\n\n\n\nTesting for Independence\nTo determine if events A and B are independent, we need to check if: P(A ∩ B) = P(A) × P(B)\nLet’s calculate each probability:\n\nP(A) = P(even number) = \\frac{3}{6} = \\frac{1}{2}\n\nFavorable outcomes: {2, 4, 6}\n\nP(B) = P(number ≥ 4) = \\frac{3}{6} = \\frac{1}{2}\n\nFavorable outcomes: {4, 5, 6}\n\nP(A) × P(B) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}\nCompare:\n\nP(A ∩ B) = \\frac{1}{3}\nP(A) × P(B) = \\frac{1}{4}\n\n\nSince \\frac{1}{3} \\neq \\frac{1}{4}, events A and B are NOT independent.\n\n\nUnderstanding the Meaning of Dependence\nThis dependence makes intuitive sense because:\n\nKnowing a number is even affects the probability it’s ≥ 4\nIf we know we rolled an even number, there are three possibilities (2, 4, 6)\nWithin these possibilities, the probability of getting ≥ 4 is \\frac{2}{3}, not \\frac{1}{2}\n\nThis illustrates an important principle: events can be dependent even when they don’t seem directly related. The overlap in their outcome spaces creates a subtle but measurable dependence.\n\n\nTeaching Extension\nTo deepen understanding, consider this question: How would the independence calculation change if we used “numbers less than 4” instead of “numbers greater than or equal to 4”? This variation helps illustrate how the structure of event spaces influences their independence.\n\n\n\nProblem 7: The Monty Hall Problem - Two Solution Approaches\nLet’s analyze this fascinating probability problem that has puzzled many people, including mathematicians. We’ll solve it using both a tree diagram and conditional probability to build a complete understanding.\n\nProblem Statement\nThe Monty Hall problem:\n\nThere are three doors: behind one is a car, behind the others are goats\nYou pick a door\nMonty Hall (who knows what’s behind each door) opens another door, always showing a goat\nYou’re offered the chance to switch to the remaining door\nQuestion: Should you switch? What’s the probability of winning if you switch vs. if you stay?\n\n\n\nApproach 1: Tree Diagram Solution\nLet’s visualize all possible scenarios:\n\n\n\n\n\ngraph TD\n    A[Initial Choice] --&gt; B[Car 1/3]\n    A --&gt; C[Goat1 1/3]\n    A --&gt; D[Goat2 1/3]\n    \n    B --&gt; E[Monty Shows Goat2]\n    B --&gt; F[Monty Shows Goat1]\n    \n    C --&gt; G[Monty Must Show Goat2]\n    D --&gt; H[Monty Must Show Goat1]\n    \n    E --&gt; I[Switch loses]\n    F --&gt; J[Switch loses]\n    G --&gt; K[Switch wins]\n    H --&gt; L[Switch wins]\n\n    style I fill:#ffcccc\n    style J fill:#ffcccc\n    style K fill:#ccffcc\n    style L fill:#ccffcc\n\n\n\n\n\n\nAnalyzing the outcomes: 1. If you initially picked the car (1/3 chance): - Monty can show either goat - Switching loses\n\nIf you initially picked a goat (2/3 chance):\n\nMonty must show the other goat\nSwitching wins\n\n\nTherefore:\n\nP(win if stay) = \\frac{1}{3}\nP(win if switch) = \\frac{2}{3}\n\n\n\nApproach 2: Conditional Probability Solution\nLet’s use Bayes’ Theorem to solve this. Define events:\n\nC₁: Car is behind Door 1 (your initial choice)\nM₂: Monty opens Door 2 showing a goat\n\nP(Car behind Door 3 | Monty opens Door 2) = ?\nWe can write: P(Car in 3 | M₂) = \\frac{P(M₂|Car in 3) \\times P(Car in 3)}{P(M₂)}\nLet’s calculate each term:\n\nP(Car in 3) = \\frac{1}{3} (prior probability)\nP(M₂|Car in 3) = 1 (Monty must open Door 2)\nP(M₂) = P(M₂|Car in 1) × P(Car in 1) + P(M₂|Car in 2) × P(Car in 2) + P(M₂|Car in 3) × P(Car in 3) = \\frac{1}{2} \\times \\frac{1}{3} + 0 \\times \\frac{1}{3} + 1 \\times \\frac{1}{3} = \\frac{1}{6} + \\frac{1}{3} = \\frac{1}{2}\n\nTherefore:\nP(Car in 3 | M₂) = \\frac{1 \\times \\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}\n\n\nKey Insights\n\nWhy Intuition Fails:\n\nPeople often think it’s 50-50 after Monty opens a door\nThis ignores the crucial fact that Monty’s choice is informed, not random\nHis action provides information that should update our probabilities\n\nInformation Value:\n\nMonty’s choice is constrained (must show a goat)\nThis constraint carries information\nThe probability shifts from the initial \\frac{1}{3} to \\frac{2}{3} for switching\n\nSimulation Verification: We could write a simple program to simulate this game thousands of times, and it would confirm these probabilities. The most convincing evidence is often seeing the results empirically.\n\n\n\n\nProblem 8: The Bertrand Box Paradox - A Teaching Analysis\n\nUnderstanding the Problem Setup\nFirst, let’s clearly state what we’re dealing with:\n\nWe have three boxes:\n\nBox 1: Contains two gold coins (GG)\nBox 2: Contains two silver coins (SS)\nBox 3: Contains one gold and one silver coin (GS)\n\nThe process:\n\nWe randomly select a box\nWe randomly draw one coin from the chosen box\nIf we see a gold coin, what’s the probability it came from the gold-only box?\n\n\nMost people intuitively answer \\frac{1}{2}, but let’s discover why this isn’t correct.\n\n\nApproach 1: Tree Diagram Analysis\nLet’s visualize all possible paths and outcomes:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[Box GG 1/3]\n    A --&gt; C[Box SS 1/3]\n    A --&gt; D[Box GS 1/3]\n    \n    B --&gt; E[Draw G 1]\n    C --&gt; F[Draw S 1]\n    D --&gt; G[Draw G 1/2]\n    D --&gt; H[Draw S 1/2]\n    \n    E --&gt; I[Saw Gold]\n    G --&gt; I[Saw Gold]\n    F --&gt; J[Saw Silver]\n    H --&gt; J[Saw Silver]\n    \n    style I fill:#FFD700\n    style J fill:#C0C0C0\n\n\n\n\n\n\nFollowing the paths where we see gold:\n\nFrom Box GG (probability = \\frac{1}{3} \\times 1 = \\frac{1}{3})\nFrom Box GS (probability = \\frac{1}{3} \\times \\frac{1}{2} = \\frac{1}{6})\n\nTherefore:\n\nTotal probability of seeing gold = \\frac{1}{3} + \\frac{1}{6} = \\frac{1}{2}\nGiven we saw gold, probability it came from Box GG = \\frac{\\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}\n\n\n\nApproach 2: Bayes’ Theorem Solution\nLet’s solve this formally using Bayes’ Theorem:\nP(Box GG | Gold) = \\frac{P(Gold|Box GG) \\times P(Box GG)}{P(Gold)}\nLet’s calculate each component:\n\nP(Gold|Box GG) = 1 (certainty of drawing gold)\nP(Box GG) = \\frac{1}{3} (equal box probabilities)\nP(Gold) = \\frac{1}{3} \\times 1 + \\frac{1}{3} \\times 0 + \\frac{1}{3} \\times \\frac{1}{2} = \\frac{1}{2}\n\nPutting it together:\nP(Box GG | Gold) = \\frac{1 \\times \\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}\n\n\nWhy This Is Counterintuitive\nThe reason many people get this wrong reveals interesting aspects of how we think about probability:\n\nThe Setup Trick: People often think, “If I see gold, it must be from either Box GG or Box GS, so it’s 50-50.” This ignores the fact that Box GG has twice the opportunity to show gold.\nPrior vs Posterior: The problem shows how observing evidence (seeing gold) updates our prior probability (\\frac{1}{3}) to a posterior probability (\\frac{2}{3}).\nSample Space Structure: Box GG contributes more gold coins to the total sample space of possible draws than Box GS does.\n\n\n\nA Teaching Analogy\nThink of it this way: Imagine three people named GG, SS, and GS.\n\nGG always raises both hands when asked\nSS never raises hands\nGS raises one hand\n\nIf you see a raised hand randomly, it’s more likely to belong to GG (who contributes two hands) than GS (who contributes only one).\n\n\nExtension for Deeper Understanding\nTo reinforce this concept, consider: How would the probabilities change if we had:\n\nThree coins in each box?\nDifferent prior probabilities for selecting each box?\nThe ability to see both coins but only after selecting a box?",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#appendix-1.-advanced-counting-in-probability-a-student-guide",
    "href": "probability_en.html#appendix-1.-advanced-counting-in-probability-a-student-guide",
    "title": "15  Introduction to (Discrete) Probability",
    "section": "15.14 Appendix 1. Advanced Counting in Probability: A Student Guide (*)",
    "text": "15.14 Appendix 1. Advanced Counting in Probability: A Student Guide (*)\n\nPoker Hands: A Window into Complex Counting\nPoker hands provide some of the most interesting examples for understanding counting in probability. They’re perfect for learning because they combine multiple counting principles and help us understand common pitfalls. Let’s explore these concepts step by step.\n\n\nUnderstanding Our Sample Space\nBefore we dive into specific hands, let’s understand what we’re working with. A poker hand consists of 5 cards drawn from a standard 52-card deck. Understanding the sample space is crucial because it forms the foundation of all our probability calculations.\nThe total number of possible poker hands represents how many different ways we can select 5 cards from 52 cards, where the order doesn’t matter (getting ace-king-queen is the same hand as getting king-queen-ace), we can’t reuse cards (we can’t have the ace of spades twice in our hand), and we must take exactly 5 cards (not more, not less).\nThis means we’re dealing with combinations. Let’s calculate this step by step:\n\\binom{52}{5} = \\frac{52!}{5!(52-5)!} = \\frac{52!}{5!(47)!} = \\frac{52 \\cdot 51 \\cdot 50 \\cdot 49 \\cdot 48}{5 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1} = 2,598,960\nThis number, 2,598,960, will be our denominator for calculating the probability of any specific poker hand.\n\n\nUnderstanding Two Pairs: A Careful Counting Approach\nTwo pairs is one of the most interesting hands for understanding counting principles. To get two pairs, we need:\n\nTwo cards of one rank\nTwo cards of another rank\nOne card of a third rank (the kicker)\n\nLet’s build this hand step by step, being careful to understand each choice we make:\nFirst, let’s select our ranks. We might think we should just choose two ranks from 13 for our pairs using \\binom{13}{2}, but this approach hides some important subtleties. Instead, let’s think about the actual process of constructing the hand:\n\nWe have 13 possible ranks for our first pair\nAfter choosing the first pair’s rank, we have 12 ranks left for our second pair\nAfter choosing both pair ranks, we have 11 ranks left for our kicker\n\nFor each rank we’ve chosen, we need to select specific cards:\n\nFor our first pair: we choose 2 cards from the 4 available cards of that rank: \\binom{4}{2} = 6 ways\nFor our second pair: again \\binom{4}{2} = 6 ways\nFor our kicker: we choose 1 card from 4: \\binom{4}{1} = 4 ways\n\nNow, here’s where many students get confused: Does it matter which pair we count “first” and which we count “second”? The answer reveals a deep truth about counting in probability.\nLet’s use a concrete example. Suppose we want two pairs with Aces and Kings, and a Two as our kicker. We could:\n\nChoose Aces as our first pair, then Kings as our second pair\nChoose Kings as our first pair, then Aces as our second pair\n\nThese lead to the exact same hand type, but we need to count both paths to this hand because they represent different ways of constructing it. It’s similar to how we can make a sandwich by putting either cheese slice on first - the order of construction matters for counting all possibilities, even though the final sandwich is the same.\nThis is why our final formula multiplies all these independent choices:\n13 (first pair rank) × 12 (second pair rank) × 11 (kicker rank) × \\binom{4}{2} (first pair cards) × \\binom{4}{2} (second pair cards) × \\binom{4}{1} (kicker card)\nEach term represents a separate decision we make in constructing the hand. While the order of these decisions doesn’t affect the final hand we get, we need to account for all possible ways to arrive at each hand to get the correct total.\nLet’s calculate the total probability:\nP(\\text{two pairs}) = \\frac{13 \\cdot 12 \\cdot 11 \\cdot \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}}{2,598,960} = \\frac{123,552}{2,598,960} \\approx 0.0475\nThis means about 4.75% of all possible poker hands are two pairs.\n\n\nUnderstanding Full House: A Different Counting Challenge\nA full house gives us a perfect contrast to two pairs. While both hands involve multiple cards of the same rank, the counting process reveals important differences in how we approach probability problems.\nIn a full house, we need: - Three cards of one rank (called “three of a kind”) - Two cards of another rank (a pair)\nLet’s think about why counting a full house is different from counting two pairs. With two pairs, we had to be careful about the order of selecting our pairs. With a full house, we have a natural order: we must choose our three of a kind first (because it’s distinct from the pair), then choose our pair.\nLet’s count step by step:\n\nFor the three of a kind:\n\nChoose the rank: 13 possible ranks\nChoose which three cards of that rank: \\binom{4}{3} = 4 ways\n\nFor the pair:\n\nChoose the rank: 12 remaining ranks\nChoose which two cards of that rank: \\binom{4}{2} = 6 ways\n\n\nMultiplying these together:\n13 (three of a kind rank) × \\binom{4}{3} (specific three cards) × 12 (pair rank) × \\binom{4}{2} (specific pair cards)\n= 13 \\cdot 4 \\cdot 12 \\cdot 6 = 3,744\nTherefore:\nP(\\text{full house}) = \\frac{3,744}{2,598,960} \\approx 0.0014\nAbout 0.14% of all poker hands are full houses, making them significantly rarer than two pairs (4.75%). This makes intuitive sense - it’s harder to get three of the same rank plus a pair than to get two pairs plus a kicker.\n\n\nThe Birthday Problem: A Beautiful Probability Surprise\nThe birthday problem provides a fascinating connection to our poker probability work, while teaching us something profound about the nature of counting. The classic question is: “How many people need to be in a room for there to be a 50% chance that at least two share a birthday?”\nMost people guess around 183 (half of 365), but the actual answer is just 23 people! Let’s understand why this connects to our previous counting work and why the answer is so surprising.\nFirst, let’s think about what makes this problem different from our poker calculations:\n\nIn poker, we were looking for specific combinations (like two pairs)\nIn the birthday problem, we’re looking for any match at all\n\nThis is similar to the difference between asking: - “What’s the probability of drawing the ace of spades and king of hearts specifically?” - “What’s the probability of drawing any two cards of different ranks?”\nThe second question has many more ways to succeed.\nLet’s solve the birthday problem step by step:\n\nFirst, it’s easier to calculate the probability of no matches\nThen we can subtract from 1 to get the probability of at least one match\n\nFor 23 people, we calculate no matches like this: - First person can have any birthday: \\frac{365}{365} - Second person needs a different birthday: \\frac{364}{365} - Third person needs a different birthday: \\frac{363}{365} And so on until person 23.\nThis gives us:\nP(\\text{no matches}) = \\frac{365}{365} \\cdot \\frac{364}{365} \\cdot \\frac{363}{365} \\cdot ... \\cdot \\frac{343}{365}\n= \\frac{365!}{(365-23)! \\cdot 365^{23}} \\approx 0.492\nTherefore:\nP(\\text{at least one match}) = 1 - 0.492 \\approx 0.508\nThis teaches us something profound about probability: when we’re looking for any match among many possibilities (like in the birthday problem), we often get much higher probabilities than when we’re looking for specific matches (like in poker hands).\n\n\nLottery Mathematics\nLet’s apply everything we’ve learned to understand lottery probabilities. Consider a typical “6/49” lottery where players choose 6 numbers from 1-49. This gives us a perfect opportunity to apply our counting principles in a real-world context.\nThe fundamental question is: What’s the probability of winning the jackpot (matching all 6 numbers)?\nThis is a combination problem because: - Order doesn’t matter (matching 1-2-3-4-5-6 is the same as matching 6-5-4-3-2-1) - We can’t use the same number twice - We need exactly 6 numbers\nTherefore:\nP(\\text{jackpot}) = \\frac{1}{\\binom{49}{6}} = \\frac{1}{13,983,816}\nThis tiny probability (about 0.0000000715) shows why lottery wins are so rare. But modern lotteries have multiple prize tiers, which gives us a chance to explore more interesting probability calculations.\nConsider matching 5 numbers plus a bonus number. For this, we need to: 1. Match 5 of the 6 winning numbers: \\binom{6}{5} ways to choose which 5 2. Match 1 of the remaining 43 numbers with the bonus: \\binom{43}{1} ways\nTherefore:\nP(\\text{5 + bonus}) = \\frac{\\binom{6}{5} \\cdot \\binom{43}{1}}{\\binom{49}{6}} = \\frac{6 \\cdot 43}{13,983,816} \\approx 0.0000184\nThis shows us how breaking down complex probability problems into simpler parts helps us solve them systematically.\n\n\nAppendix 2. Alternative Approaches to Poker Hand Probabilities (*)\nUnderstanding different ways to calculate the same probability deepens our insight into counting principles. Let’s explore several methods for finding the probabilities of two pairs and full house, seeing how each approach highlights different aspects of the problem.\n\nMultiple Paths to Two Pairs Probability\nLet’s start with two pairs. We’ve seen one method, but there are several valid approaches:\nMethod 1: Sequential Selection (Our Original Approach) We build the hand step by step: 1. Choose first pair’s rank: 13 ways 2. Choose second pair’s rank: 12 ways 3. Choose kicker’s rank: 11 ways 4. Choose specific cards for first pair: \\binom{4}{2} ways 5. Choose specific cards for second pair: \\binom{4}{2} ways 6. Choose specific card for kicker: \\binom{4}{1} ways\nThis gives us: P(\\text{two pairs}) = \\frac{13 \\cdot 12 \\cdot 11 \\cdot \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}}{2,598,960}\nMethod 2: Complementary Counting We can find two pairs probability by subtracting the probability of all other possible hands from 1. However, this is more complex than direct counting because we need to know the probabilities of all other poker hands. Still, it serves as a good verification:\nP(\\text{two pairs}) = 1 - P(\\text{high card}) - P(\\text{one pair}) - P(\\text{three of a kind}) - P(\\text{straight}) - P(\\text{flush}) - P(\\text{full house}) - P(\\text{four of a kind}) - P(\\text{straight flush})\nMethod 3: Using Permutations with Adjustment We can use permutations and then adjust for overcounting:\n\nChoose an ordered arrangement of two ranks for pairs: P(13,2) = 13 \\cdot 12\nChoose kicker rank: 11 ways\nChoose specific cards for pairs and kicker: \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}\nDivide by 2 to account for the fact that the order of pairs doesn’t matter\n\nThis gives: P(\\text{two pairs}) = \\frac{P(13,2) \\cdot 11 \\cdot \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}}{2 \\cdot 2,598,960}\nMethod 4: Combination-Based Approach with Multiplication Principle We can separate rank selection from card selection:\n\nFirst, select three ranks: \\binom{13}{3} ways\nFrom these three ranks, designate two for pairs and one for kicker: \\binom{3}{2} ways\nFor each pair rank, select two cards: \\binom{4}{2} \\cdot \\binom{4}{2} ways\nFor the kicker rank, select one card: \\binom{4}{1} ways\n\nThis gives us: P(\\text{two pairs}) = \\frac{\\binom{13}{3} \\cdot \\binom{3}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}}{2,598,960}\n\n\nAlternative Approaches to Full House Probability\nThe full house probability can also be calculated in several ways:\nMethod 1: Direct Sequential Selection (Our Original Approach) 1. Choose rank for three of a kind: 13 ways 2. Choose specific three cards: \\binom{4}{3} ways 3. Choose rank for pair: 12 ways 4. Choose specific two cards: \\binom{4}{2} ways\nLeading to: P(\\text{full house}) = \\frac{13 \\cdot \\binom{4}{3} \\cdot 12 \\cdot \\binom{4}{2}}{2,598,960}\nMethod 2: Using Combinations with Distribution We can think about it as: 1. Choose two ranks from 13: \\binom{13}{2} ways 2. Designate which rank gets three cards: 2 ways (since either rank could be the three of a kind) 3. Choose specific cards: \\binom{4}{3} \\cdot \\binom{4}{2} ways\nThis gives: P(\\text{full house}) = \\frac{\\binom{13}{2} \\cdot 2 \\cdot \\binom{4}{3} \\cdot \\binom{4}{2}}{2,598,960}\nMethod 3: Using the Multiplication Principle with Sets Think about constructing the hand as selecting two sets of cards: 1. First set: three cards of the same rank from 13 ranks - Choose rank: 13 ways - Choose three cards: \\binom{4}{3} ways 2. Second set: two cards of the same rank from 12 remaining ranks - Choose rank: 12 ways - Choose two cards: \\binom{4}{2} ways\nThis yields the same result: P(\\text{full house}) = \\frac{13 \\cdot \\binom{4}{3} \\cdot 12 \\cdot \\binom{4}{2}}{2,598,960}\nEach method illuminates different aspects of the counting process: - Sequential selection helps us understand the step-by-step construction of hands - Combination-based approaches highlight the underlying structure of the selections - Permutation-based methods with adjustment show how overcounting can be handled systematically\nThe fact that all these methods yield the same result serves as a powerful verification tool. When solving complex probability problems, being able to approach the solution in multiple ways not only confirms our answer but also deepens our understanding of the underlying counting principles.\n\n\n\nAppendix 3: Occupancy Problems and Statistical Physics (*)\nUnderstanding how objects can be distributed into containers forms the foundation for both probability theory and statistical mechanics. Let’s explore this connection, starting with basic counting principles and building up to physical applications.\n\nThe Basic Occupancy Problem\nImagine we have n identical balls and k distinct boxes. How many ways can we distribute the balls? This simple question leads us to three fundamentally different scenarios that mirror important physical systems:\n\nUnrestricted occupancy (Bose-Einstein statistics)\n\nEach box can hold any number of balls\nThe balls are indistinguishable\nLike photons in quantum states\n\nMaximum one per box (Fermi-Dirac statistics)\n\nEach box can hold at most one ball\nThe balls are indistinguishable\nLike electrons in atomic orbitals\n\nAll arrangements count separately (Maxwell-Boltzmann statistics)\n\nEach box can hold any number of balls\nThe balls are distinguishable\nLike classical gas molecules\n\n\n\n\nStars and Bars: Understanding Unrestricted Occupancy\nLet’s start with the Bose-Einstein case. The “stars and bars” method provides a beautiful way to visualize and count these arrangements.\nImagine n=5 balls and k=3 boxes. We can represent any arrangement as a sequence of stars and bars:\n\nStars (*) represent balls\nBars (|) separate different boxes\n\nFor example:\n\n** | ** | * represents 2 balls in first box, 2 in second, 1 in third\n***** | | represents all 5 balls in first box, none in others\n| ***** | represents all 5 balls in middle box\n\nThe key insight is that we need: - n stars (one for each ball) - k-1 bars (to create k sections)\nTherefore, we’re really just choosing positions for the k-1 bars among n+(k-1) total positions. This gives us:\n\\text{Number of arrangements} = \\binom{n+k-1}{k-1} = \\binom{n+k-1}{n}\n\n\nFrom Counting to Physics\nNow let’s see how these counting principles reveal deep physical truths:\n\nBose-Einstein Statistics (Unrestricted, Indistinguishable)\n\nThink of photons in a laser\nMany particles can occupy same energy state\nTotal arrangements: \\binom{n+k-1}{k-1}\nExample: Light in a cavity\n\nFermi-Dirac Statistics (Restricted, Indistinguishable)\n\nThink of electrons in atoms\nMaximum one particle per state\nTotal arrangements: \\binom{k}{n} if n \\leq k, 0 otherwise\nExample: Electron configuration in atoms\n\nMaxwell-Boltzmann Statistics (Classical, Distinguishable)\n\nThink of gas molecules\nParticles are distinct\nTotal arrangements: k^n\nExample: Air molecules in a room\n\n\n\n\nAn Intuitive Bridge to Physics\nTo understand why these statistics matter, consider three real scenarios:\n\nPhotons in a Laser (Bose-Einstein) Imagine shining a laser into a mirror cavity. Photons are happy to bunch together in the same quantum state - they’re “social particles.” This is why lasers can produce intense, coherent light.\nElectrons in an Atom (Fermi-Dirac) Electrons are “antisocial” - they refuse to share quantum states (Pauli exclusion principle). This explains atomic structure and why matter is mostly empty space.\nGas Molecules in a Room (Maxwell-Boltzmann) Air molecules bounce around randomly, and we can tell them apart (in principle). This gives us the familiar gas laws and diffusion.\n\n\n\nThe Power of the Star and Bars Method\nThe stars and bars visualization helps us understand more complex problems. For instance, if we have restrictions on box occupancy:\n\nAt least one ball per box:\n\nFirst put one ball in each box\nThen distribute remaining balls freely\nFormula: \\binom{n-k+k-1}{k-1} = \\binom{n-1}{k-1}\n\nMaximum capacity per box:\n\nUse inclusion-exclusion principle\nSubtract arrangements that violate constraints\nMore complex but same underlying principle\n\n\n\n\nConnection to Partition Problems\nThis same framework helps us solve other important problems:\n\nInteger Partitions How many ways can we write n as a sum of positive integers?\n\nLike distributing n balls into unlimited boxes\nEach box represents a different term in the sum\n\nCompositions How many ways can we write n as an ordered sum?\n\nLike distinguishable boxes\nOrder matters here\n\n\nThis connection between simple counting and profound physical phenomena shows the deep unity of mathematics and physics. The same principles that help us count poker hands and lottery combinations govern the behavior of the universe at its most fundamental level.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "rv_pdf_en.html",
    "href": "rv_pdf_en.html",
    "title": "17  Random Variables and Probability Distributions",
    "section": "",
    "text": "17.2 Random Variables: Making Outcomes Measurable\nA random variable is a way to assign numbers to the outcomes of a random experiment. Think of it as a function that converts outcomes into numbers, making them measurable and easier to analyze.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "rv_pdf_en.html#models-for-different-data-generating-processes-dgps",
    "href": "rv_pdf_en.html#models-for-different-data-generating-processes-dgps",
    "title": "17  Random Variables and Probability Distributions",
    "section": "17.1 Models for Different Data Generating Processes (DGPs)",
    "text": "17.1 Models for Different Data Generating Processes (DGPs)\nDifferent random experiments follow different patterns. We model these using specific probability distributions:\n\nBernoulli: For single yes/no experiments\n\nExample: Single coin flip\nDGP Assumption: Two outcomes with fixed success probability\n\nBinomial: For counting successes in fixed trials\n\nExample: Number of heads in 10 coin flips\nDGP Assumption: Independent trials with same success probability\n\nPoisson: For counting rare events in an interval\n\nExample: Number of customer arrivals per hour\nDGP Assumption: Events occur independently at constant rate\n\n\n💡 Important Note: These are mathematical models based on assumptions about how data is generated. Their usefulness depends on how well these assumptions match reality.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "rv_pdf_en.html#random-variables-making-outcomes-measurable",
    "href": "rv_pdf_en.html#random-variables-making-outcomes-measurable",
    "title": "17  Random Variables and Probability Distributions",
    "section": "",
    "text": "Example: Rolling a Die\n\nOutcome space: {⚀, ⚁, ⚂, ⚃, ⚄, ⚅}\nRandom variable X: “number of dots showing”\nX converts outcomes to numbers: X(⚀) = 1, X(⚁) = 2, …, X(⚅) = 6\n\n\n\nExample: Flipping a Coin\n\nOutcome space: {Heads, Tails}\nRandom variable Y: “1 if heads, 0 if tails”\nY converts outcomes to numbers: Y(Heads) = 1, Y(Tails) = 0\n\n\n\nProperties of Discrete Random Variables\n\nEach possible value has a probability\nAll probabilities must be ≥ 0\nSum of all probabilities must equal 1",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "rv_pdf_en.html#understanding-probability-distributions",
    "href": "rv_pdf_en.html#understanding-probability-distributions",
    "title": "17  Random Variables and Probability Distributions",
    "section": "17.3 Understanding Probability Distributions",
    "text": "17.3 Understanding Probability Distributions\nA probability distribution is a mathematical description of the probabilities of different possible outcomes in a random experiment. It tells us:\n\nWhat values can occur (the support of the distribution)\nHow likely each value is to occur (the probability of each outcome)\nHow the probabilities are spread across the possible values\n\nFor discrete random variables, we can represent the distribution as:\n\nA probability mass function (PMF) that gives P(X = x) for each possible value x\nA cumulative distribution function (CDF) that gives P(X ≤ x) for each possible value x\n\nKey Properties of Any Probability Distribution:\n\nAll probabilities must be between 0 and 1: 0 \\leq P(X = x) \\leq 1 for all x\nThe sum of all probabilities must equal 1: \\sum P(X = x) = 1\n\nLet’s explore three fundamental discrete distributions:\n\nUniform Distribution\nThe uniform distribution represents complete randomness - all outcomes are equally likely.\n\nProperties\n\nEach outcome has equal probability\nFor n possible outcomes, P(X = x) = 1/n for each x\nMean: E(X) = \\frac{a + b}{2} where a is minimum and b is maximum\nVariance: Var(X) = \\frac{(b-a+1)^2 - 1}{12} for discrete uniform\n\n\n\nExample: Fair Die\n\n# Visualization of uniform distribution (die roll)\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\ndie_data &lt;- data.frame(\n  outcome = 1:6,\n  probability = rep(1/6, 6)\n)\n\nggplot(die_data, aes(x = factor(outcome), y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  geom_text(aes(label = sprintf(\"1/6\")), vjust = -0.5) +\n  labs(title = \"Probability Distribution of a Fair Die Roll\",\n       x = \"Outcome\",\n       y = \"Probability\") +\n  theme_minimal() +\n  ylim(0, 0.5)\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Distribution\nThe Bernoulli distribution is the simplest probability distribution, modeling a single “yes/no” trial.\n\nProperties\n\nOnly two possible outcomes: 0 (failure) or 1 (success)\nControlled by single parameter p (probability of success)\nMean: E(X) = p\nVariance: Var(X) = p(1-p)\n\n\n\nExample: Biased Coin\n\n# Visualization of Bernoulli distribution (p = 0.7)\nbernoulli_data &lt;- data.frame(\n  outcome = c(\"Failure (0)\", \"Success (1)\"),\n  probability = c(0.3, 0.7)\n)\n\nggplot(bernoulli_data, aes(x = outcome, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"lightgreen\") +\n  geom_text(aes(label = scales::percent(probability)), vjust = -0.5) +\n  labs(title = \"Bernoulli Distribution (p = 0.7)\",\n       x = \"Outcome\",\n       y = \"Probability\") +\n  theme_minimal() +\n  ylim(0, 1)\n\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\nThe binomial distribution models the number of successes in n independent Bernoulli trials.\n\nProperties\n\nParameters: n (number of trials) and p (probability of success)\nPossible values: 0 to n successes\nMean: E(X) = np\nVariance: Var(X) = np(1-p)\nProbability mass function: P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\n\n\nUnderstanding the Formula\nThe binomial probability formula has three parts:\n\n\\binom{n}{k} - Number of ways to get k successes in n trials\np^k - Probability of k successes\n(1-p)^{n-k} - Probability of (n-k) failures\n\n\n\nVisualizing Binomial Distributions\n\nlibrary(ggplot2)\n# Function to calculate binomial probabilities\nbinomial_probs &lt;- function(n, p) {\n  k &lt;- 0:n\n  probs &lt;- dbinom(k, n, p)\n  data.frame(k = k, probability = probs)\n}\n\n# Create plots for different parameters\nggplot(binomial_probs(10, 0.5), aes(x = k, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"salmon\") +\n  labs(title = \"Binomial(n=10, p=0.5)\",\n       x = \"Number of Successes\",\n       y = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(binomial_probs(10, 0.2), aes(x = k, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  labs(title = \"Binomial(n=10, p=0.2)\",\n       x = \"Number of Successes\",\n       y = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nEffect of Parameters\nThe shape of the binomial distribution changes with n and p:\n\nEffect of n (number of trials):\n\nLarger n → more possible outcomes\nLarger n → distribution becomes more “bell-shaped”\n\nEffect of p (probability of success):\n\np = 0.5 → symmetric distribution\np &lt; 0.5 → right-skewed distribution\np &gt; 0.5 → left-skewed distribution\n\n\n\n\nReal-World Applications\n\nQuality Control\n\nn = number of items inspected\np = probability of defect\nX = number of defective items found\n\nClinical Trials\n\nn = number of patients\np = probability of recovery\nX = number of patients who recover\n\n\n\n\n\nComparing the Distributions\nKey differences between our three distributions:\n\nUniform Distribution\n\nEqual probability for all outcomes\nUsed when all outcomes are equally likely\nExample: rolling a fair die\n\nBernoulli Distribution\n\nSpecial case of binomial with n = 1\nOnly two possible outcomes\nExample: single coin flip\n\nBinomial Distribution\n\nCounts successes in multiple trials\nCombines multiple Bernoulli trials\nExample: number of heads in 10 coin flips\n\n\n\n\nInteractive R Code for Exploration\n\n# Function to compare distributions\ncompare_distributions &lt;- function(n = 10, p = 0.5) {\n  # Binomial probabilities\n  x &lt;- 0:n\n  binom_probs &lt;- dbinom(x, n, p)\n  \n  # Create data frame\n  df &lt;- data.frame(\n    x = x,\n    probability = binom_probs\n  )\n  \n  # Create plot\n  ggplot(df, aes(x = x, y = probability)) +\n    geom_bar(stat = \"identity\", fill = \"steelblue\", alpha = 0.7) +\n    geom_text(aes(label = round(probability, 3)), vjust = -0.5) +\n    labs(title = sprintf(\"Binomial Distribution (n=%d, p=%.2f)\", n, p),\n         x = \"Number of Successes\",\n         y = \"Probability\") +\n    theme_minimal()\n}\n\n# Try different parameters\ncompare_distributions(n = 5, p = 0.5)\n\n\n\n\n\n\n\ncompare_distributions(n = 10, p = 0.3)\n\n\n\n\n\n\n\ncompare_distributions(n = 20, p = 0.7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplying Binomial Distribution: ESP Probability Analysis\n\n\n\nExtrasensory perception (ESP), also known as a sixth sense, is a claimed ability to perceive information through mental means rather than the five physical senses.\nProblem: A man claims to have extrasensory perception (ESP). To test this claim, a fair coin is flipped 10 times and the man is asked to predict each outcome in advance. He gets 7 out of 10 predictions correct. Calculate the probability of obtaining 7 or more correct predictions out of 10 trials by chance alone (assuming no ESP).\nRemark: Under the null hypothesis of no ESP, the probability of each correct prediction is \\frac{1}{2}.\n\nPreliminary Concepts\n\nBernoulli Variable\nA Bernoulli random variable represents a trial with exactly two possible outcomes: success (1) or failure (0). Each trial has probability p of success and 1-p of failure.\nIn our case, each coin flip prediction is a Bernoulli trial where success means a correct prediction.\n\n\nBinomial Distribution\nThe binomial distribution extends the Bernoulli concept to model the sum of n independent Bernoulli trials. It describes the probability of obtaining exactly k successes in these n trials. The probability mass function is:\nP(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\nwhere:\n\nn is the number of trials\nk is the number of successes\np is the probability of success on each trial\n\\binom{n}{k} is the binomial coefficient\n\n\n\nBinomial Coefficient\nThe binomial coefficient \\binom{n}{k} represents the number of ways to choose k items from n items, regardless of order. It is calculated as:\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\nFor example, \\binom{10}{7} = \\frac{10!}{7!(10-7)!} = \\frac{10!}{7!3!} = 120\n\n\n\nProblem Solution\nGiven: - n = 10 coin flips - p = \\frac{1}{2} (probability of correct guess without ESP) - Need P(X \\geq 7) = P(X = 7) + P(X = 8) + P(X = 9) + P(X = 10)\nStep 1: Calculate P(X = 7) P(X = 7) = \\binom{10}{7}(\\frac{1}{2})^7(\\frac{1}{2})^3 = 120 \\cdot (\\frac{1}{128}) \\cdot (\\frac{1}{8}) = 120 \\cdot \\frac{1}{1024} = \\frac{120}{1024} \\approx 0.117\nStep 2: Calculate P(X = 8) P(X = 8) = \\binom{10}{8}(\\frac{1}{2})^8(\\frac{1}{2})^2 = 45 \\cdot (\\frac{1}{256}) \\cdot (\\frac{1}{4}) = 45 \\cdot \\frac{1}{1024} = \\frac{45}{1024} \\approx 0.044\nStep 3: Calculate P(X = 9) P(X = 9) = \\binom{10}{9}(\\frac{1}{2})^9(\\frac{1}{2})^1 = 10 \\cdot (\\frac{1}{512}) \\cdot (\\frac{1}{2}) = 10 \\cdot \\frac{1}{1024} = \\frac{10}{1024} \\approx 0.010\nStep 4: Calculate P(X = 10) P(X = 10) = \\binom{10}{10}(\\frac{1}{2})^{10}(\\frac{1}{2})^0 = 1 \\cdot (\\frac{1}{1024}) \\cdot 1 = \\frac{1}{1024} \\approx 0.001\nStep 5: Sum all probabilities P(X \\geq 7) = \\frac{120 + 45 + 10 + 1}{1024} = \\frac{176}{1024} \\approx 0.172\n\n\nInterpretation\nThere is approximately a 17.2% chance that someone without ESP would correctly guess 7 or more coin flips out of 10 by pure chance. This is a relatively high probability, suggesting that getting 7 out of 10 correct predictions is not strong evidence for ESP. Generally, we would want a much smaller probability (e.g., &lt; 5% or &lt; 1%) before considering the results statistically significant evidence for ESP.\n\n\n\n\n\n\n\n\n\nInductive Derivation of the Binomial Distribution Formula\n\n\n\n\nRandom Variable (X): A function that maps outcomes from a sample space to real numbers. In our case:\n\nX = number of successes in n trials\nX maps “Success” to 1 and “Failure” to 0 for each trial\nFor n trials, X takes values in {0, 1, 2, …, n}\n\nProbability Distribution: A function P(X = k) that assigns probabilities to each possible value k of the random variable X, where:\n\nP(X = k) ≥ 0 for all k\n\\sum_{k=0}^n P(X = k) = 1\n\n\nLet’s denote success as S and failure as F, with:\n\nProbability of success p = P(\\text{Success}) = P(X = 1) for a single trial\nProbability of failure q = 1-p = P(\\text{Failure}) = P(X = 0) for a single trial\n\n\n1. One Trial\n\n\n\nOutcome\nWays\nProbability\n\n\n\n\nS\n1\np\n\n\nF\n1\nq\n\n\n\nTotal outcomes: p + q = 1\n\n\n2. Two Trials\n\n\n\n# Successes\nWays\nCombinations\nProbability\n\n\n\n\n2\nSS\n1\np^2\n\n\n1\nSF, FS\n2\n2pq\n\n\n0\nFF\n1\nq^2\n\n\n\nNotice: \\binom{2}{k} gives us the number of ways (k = 0,1,2)\nTotal probability: p^2 + 2pq + q^2 = 1\n\n\n3. Three Trials\n\n\n\n# Successes\nWays\nCombinations\nProbability\n\n\n\n\n3\nSSS\n1\np^3\n\n\n2\nSSF, SFS, FSS\n3\n3p^2q\n\n\n1\nSFF, FSF, FFS\n3\n3pq^2\n\n\n0\nFFF\n1\nq^3\n\n\n\nHere, \\binom{3}{k} gives us the combinations (k = 0,1,2,3)\nTotal probability: p^3 + 3p^2q + 3pq^2 + q^3 = 1\n\n\nIllustrating Coordinates for 3 Choose 2\nFor n = 3 and k = 2 successes, \\binom{3}{2} = 3 gives us these success position coordinates:\n\n\n\nSuccess Positions\nSequence\nCoordinate Interpretation\n\n\n\n\n(1,2)\nSSF\nSuccesses in positions 1,2\n\n\n(2,3)\nFSS\nSuccesses in positions 2,3\n\n\n(1,3)\nSFS\nSuccesses in positions 1,3\n\n\n\nThis demonstrates why \\binom{3}{2} = 3 - it counts all possible ways to choose 2 positions from 3 available positions.\n\n\n4. Four Trials\n\n\n\n\n\n\n\n\n\n\n# Successes\nWays\nCombinations\nCoordinate Positions\nProbability\n\n\n\n\n4\nSSSS\n1\n(1,2,3,4)\np^4\n\n\n3\nSSSF, SSFS, SFSS, FSSS\n4\n(1,2,3), (1,2,4), (1,3,4), (2,3,4)\n4p^3q\n\n\n2\nSSFF, SFSF, SFFS, FSSF, FSFS, FFSS\n6\n(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)\n6p^2q^2\n\n\n1\nSFFF, FSFF, FFSF, FFFS\n4\n(1), (2), (3), (4)\n4pq^3\n\n\n0\nFFFF\n1\n()\nq^4\n\n\n\nNote how \\binom{4}{k} gives us the combinations:\n\n\\binom{4}{4} = 1 (one way to choose all positions)\n\\binom{4}{3} = 4 (four ways to choose 3 positions)\n\\binom{4}{2} = 6 (six ways to choose 2 positions)\n\\binom{4}{1} = 4 (four ways to choose 1 position)\n\\binom{4}{0} = 1 (one way to choose no positions)\n\nTotal probability: p^4 + 4p^3q + 6p^2q^2 + 4pq^3 + q^4 = 1\n\n\nPattern Recognition and Generalization\n\nFor n trials:\n\nEach outcome has exactly n positions\nFor k successes, we need k positions filled with S and (n-k) positions with F\n\\binom{n}{k} gives us the number of ways to choose k positions for successes\nEach success contributes p, each failure contributes q\n\nTherefore, for k successes in n trials:\n\nWays = \\binom{n}{k} (binomial coefficient)\nProbability of each way = p^k q^{n-k}\n\n\n\n\nBinomial Distribution Formula\nThe probability of exactly k successes in n trials is:\n\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\nwhere:\n\n\\binom{n}{k} counts the ways to arrange k successes in n positions\np^k accounts for k successes\n(1-p)^{n-k} accounts for (n-k) failures\n\n\n\nCoordinate System Interpretation\nThe binomial coefficient provides a systematic way to count all possible position combinations for successes. For example:\n\nIn the 3-trial case with 2 successes (\\binom{3}{2} = 3):\n\nPosition 1 means success in first trial\nPosition 2 means success in second trial\nPosition 3 means success in third trial\nThe coordinates (1,2), (2,3), (1,3) represent all possible ways to place 2 successes in 3 positions\n\nIn the 4-trial case with 2 successes (\\binom{4}{2} = 6):\n\nWe get six distinct pairs of positions: (1,2), (1,3), (1,4), (2,3), (2,4), (3,4)\nEach pair represents a unique way to distribute 2 successes across 4 trials\nThe remaining positions automatically become failures\n\n\nThis coordinate system demonstrates why the binomial coefficient is the correct counting tool: it systematically accounts for all possible ways to distribute k successes across n positions, where each position must be either a success or failure.\n\n\n\n\nUnderstanding the Binomial Coefficient\nThe binomial coefficient \\binom{n}{k} counts the number of ways to choose k items from n items, where order doesn’t matter. It relates to the multiplication rule through this key insight:\n\nFirst, count all possible ordered sequences (using multiplication rule)\nThen, divide by number of ways to arrange the k items (to remove order)\n\nFor example, to choose 2 items from 4:\n\nFirst position can be filled in 4 ways\nSecond position can be filled in 3 ways\nTotal ordered sequences = 4 × 3 = 12\nNumber of ways to arrange 2 items = 2 × 1 = 2\nTherefore \\binom{4}{2} = \\frac{4 × 3}{2 × 1} = 6",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "inference_en.html",
    "href": "inference_en.html",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "",
    "text": "18.1 Statistical Hypothesis Testing (introduction)\nStatistical inference is how we draw conclusions about a population from a sample. It’s like being a detective: we never have all the information, but we can make educated guesses based on the evidence we have.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_en.html#statistical-hypothesis-testing-introduction",
    "href": "inference_en.html#statistical-hypothesis-testing-introduction",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "",
    "text": "Key Steps in Statistical Hypothesis Testing (general framework)\n\n\n\n\nInitial Suspicion/Research Question\n\nWe suspect some effect/relationship/difference\nThis guides our research design and analysis\n\nData Collection\n\nWe collect appropriate amount of data\nSample size depends on expected effect and required precision\n\nResult Observation\n\nWe observe and summarize our data\nLook at relevant patterns in the data\n\nHypothesis System\n\nH₀: no effect/no difference (“status quo”)\nH₁: effect exists (one or two-sided)\nChoice of direction based on research question\n\nP-value Approach\n\nConsider: how likely are our results (or more extreme) if H₀ is true?\nChoose appropriate probability model based on data type\nCalculate this probability (p-value)\n\nDecision Making\n\nCompare p-value to significance level (typically α = 0.05)\nSmall p-value suggests results unlikely under H₀\n\nConclusion\n\nIf p ≤ α, reject H₀\nConclude evidence against null hypothesis\nConsider practical significance\n\n\n\n\n\n\n\n\n\n\nThe Logic of Statistical Hypothesis Testing: A Probabilistic Proof by Contradiction\n\n\n\n\nResearch Context\nA person claims to possess ESP (extrasensory perception) abilities that enable them to predict coin flips. This scenario illustrates the fundamental logic of statistical hypothesis testing.\n\nTask: Predict 100 coin flips before each flip occurs\nObserved Result: 70 correct predictions out of 100 attempts\nKey Consideration: High success could indicate either ESP ability OR a biased coin\n\n\n\nThe Core Logic\nStatistical hypothesis testing follows a logic similar to proof by contradiction in mathematics:\n\nWe start by assuming what we want to disprove (the null hypothesis)\nCalculate the probability of our observed data under this assumption\nIf this probability is extremely small, we reject the initial assumption\n\n\n\nStatistical Framework\n\n1. The Null Hypothesis Mechanism\nThe null hypothesis (H₀) serves as our “assumption to be disproven” and typically represents:\n\nNo effect\nNo difference\nPure chance\nThe status quo\n\nIn our ESP case: Random guessing (p = 0.5)\n\n\n2. The Alternative Hypothesis\nThe alternative hypothesis (H₁) represents what we suspect might be true:\n\nAn effect exists\nA difference exists\nNot due to chance\nA deviation from status quo\n\nIn our ESP case: Better than random guessing (p &gt; 0.5)\n\n\n3. The Decision Rule\nWe establish a conventional cutoff point (α) that defines “extremely unlikely”:\n\nTypically set at α = 0.05 (5%)\nRepresents the threshold for “rare enough” to reject H₀\nA conventional threshold, not a mathematical necessity\n\n\n\n4. The P-value Mechanism\nThe p-value quantifies the logical argument:\n\nAssuming H₀ is true\nWhat’s the probability of our observed result or more extreme?\nSmall p-value means either:\n\nH₀ is false (our desired conclusion)\nA rare event occurred under H₀\n\n\n\nP-value (statistical significance): In statistical hypothesis significance testing, the p-value is the probability of obtaining test results (outcomes) at least as extreme as the result actually observed, under the assumption that the null hypothesis (H0) is correct. A very small p-value means that, if the null hypothesis were true, the probability of observing data as extreme as or more extreme than what we actually observed would be very small (the empirical outcome “contradicts” H0). The smaller the p-value, the stronger the statistical evidence against the null hypothesis, leading us to reject H0 at predetermined significance levels (cut-off or threshold probability) such as 0.05 or 0.01, while recognizing that these thresholds are conventions rather than mathematically derived boundaries.\n\n\n\n\nApplication to ESP Testing\nStatistical Hypotheses: \n\\begin{align*}\nH_0&: p = 0.5 \\text{ (random guessing)} \\\\\nH_1&: p &gt; 0.5 \\text{ (better than guessing)}\n\\end{align*}\n\nProbability Calculation:\nFor 70 successes out of 100 trials:\n\\text{P-value} = P(X \\geq 70) = \\sum_{k=70}^{100} \\binom{100}{k}(0.5)^k(0.5)^{100-k} \\approx 0.0000393\nDecision Framework: \n\\text{Decision Rule} = \\begin{cases}\n\\text{Reject H}_0 & \\text{if p-value} &lt; 0.05 \\\\\n\\text{Fail to reject H}_0 & \\text{if p-value} \\geq 0.05\n\\end{cases}\n\n0.0000393 &lt; 0.05 (significance level)\nThis means that under the null hypothesis (pure guessing):\n\nThe probability of getting 70 or more correct predictions by chance is about 0.00393%\nSuch extreme results would occur by chance only about 4 times in 100,000 trials\nThis is far below our conventional significance level of 0.05 (5%)\n\n\n\nThe General Pattern\n\nAssume the null (like assuming not-A in proof by contradiction)\nCalculate probability of data under null\nIf probability &lt; α:\n\nReject null\nAccept alternative\nConclude evidence supports our suspicion\n\n\n\n\nKey Distinctions from Mathematical Proof\n\nProbabilistic rather than deterministic\nConclusions are “supported” rather than “proven”\nUses conventional thresholds (α)\nAlways includes uncertainty\n\n\n\n\n\nThe binomial test is a hypothesis test used when you have binary (two-outcome) trials, where each trial is independent and has the same probability of success. It tests whether the observed proportion of successes differs significantly from an expected probability under the null hypothesis. For example: Testing whether a coin is fair by checking if the proportion of heads in 100 flips differs significantly from the expected probability of 0.5 under the null hypothesis.\n\n\n\n\n\n\n\nWhat is a P-value?\n\n\n\nA p-value is a probability that captures how extreme our observed data is relative to a null hypothesis:\n\nThe p-value is the probability of obtaining the observed outcome, or a more extreme one in the direction of the alternative hypothesis, assuming the null hypothesis (H₀) is true.\n\n\nKey Components\n\nObserved outcome: The actual value or statistic computed from our sample data.\nMore extreme outcomes: Additional outcomes that provide even stronger evidence against H₀: \n\\begin{cases}\n\\text{Values } \\leq \\text{ observed} & \\text{for } H_1\\text{: parameter &lt; value} \\\\\n\\text{Values } \\geq \\text{ observed} & \\text{for } H_1\\text{: parameter &gt; value} \\\\\n\\text{Values in both tails} & \\text{for } H_1\\text{: parameter } \\neq \\text{ value}\n\\end{cases}\n\nNull hypothesis assumption: All probabilities are calculated using the parameter value specified in H₀.\n\n\n\nOne-Tailed vs Two-Tailed Tests\nThe choice between one-tailed and two-tailed tests depends on your alternative hypothesis and the context of your research question:\nOne-Tailed Tests:\n\nUsed when H₁ specifies a direction (&lt; or &gt;)\nP-value calculated from one tail of the distribution\nHigher power but requires strong directional justification\nExample hypotheses: \n\\begin{align*}\nH_0&: \\mu = \\mu_0 \\\\\nH_1&: \\mu &gt; \\mu_0 \\text{ (right-tailed) or } \\mu &lt; \\mu_0 \\text{ (left-tailed)}\n\\end{align*}\n\n\nTwo-Tailed Tests:\n\nUsed when H₁ is non-directional (≠)\nP-value includes both tails of the distribution\nMore conservative, standard choice when direction uncertain\nParticularly suitable for symmetric distributions like the normal distribution\nExample hypotheses: \n\\begin{align*}\nH_0&: \\mu = \\mu_0 \\\\\nH_1&: \\mu \\neq \\mu_0\n\\end{align*}\n\n\n\n\nExample: Binomial Test\nTesting if a politician is overestimating 98% support (p = 0.98) when observing 13 supporters in n = 15 people. In this context, a one-tailed test is most appropriate because the research question is inherently directional (overestimating implies p &lt; 0.98).\n\n\\begin{align*}\nH_0&: p = 0.98 \\\\\nH_1&: p &lt; 0.98\n\\end{align*}\n\nP-value calculation: \n\\begin{align*}\n\\text{p-value} &= P(X \\leq 13 \\mid H_0) \\\\\n&= 1 - P(X \\geq 14 \\mid p = 0.98) \\\\\n&= 0.0353\n\\end{align*}\n\n\n\nCommon Misconceptions to Avoid\n\nP-value ≠ Probability H₀ is true\n1 - p-value ≠ Probability H₁ is true\nLarge p-value ≠ H₀ is true\nOne-tailed tests aren’t automatically “better” despite higher power\nThe choice between one-tailed and two-tailed tests should be based on the research context, not just statistical convenience\n\nRemember: The p-value quantifies evidence against H₀ but should be considered alongside practical significance and effect size.\n\n\n\n\nThe Method of Proof by Contradiction\n\nIn Mathematics\nProof that \\sqrt{2} is Irrational\nInitial Assumption\nIf \\sqrt{2} is rational, then \\sqrt{2} = \\frac{p}{q} where:\n\np and q are integers\nq \\neq 0\np and q have no common factors\n\nAlgebraic Steps\n\nStarting with \\sqrt{2} = \\frac{p}{q}\nSquare both sides: 2 = \\frac{p^2}{q^2}\nMultiply both sides by q^2: 2q^2 = p^2\n\nProperties of p and q\n\nSince 2q^2 = p^2, p^2 is even\nIf p^2 is even, then p is even\nTherefore p = 2k for some integer k\nSubstituting p = 2k into 2q^2 = p^2: 2q^2 = (2k)^2 = 4k^2\nTherefore q^2 = 2k^2\nThus q^2 is even, so q is even\n\nContradiction\n\nWe proved both p and q are even\nThis contradicts our assumption that p and q have no common factors\nTherefore, \\sqrt{2} cannot be rational\n\nThus, \\sqrt{2} is irrational.\n\n\nIn Statistics\nWe use a similar but probabilistic approach:\n\nMake assumption (null hypothesis)\nSee if data contradicts this assumption\nIf contradiction is strong enough, reject assumption\n\nKey Difference: We deal with probability, not certainty.\n\n\n\nThe Null Hypothesis Framework\n\nStep 1: State the Hypotheses\nFor a coin example:\n\nH₀ (Null): Coin is fair (p = 0.5)\nH₁ (Alternative): Coin is not fair (p ≠ 0.5)\n\nThink of H₀ as the “innocent until proven guilty” assumption.\n\n\nStep 2: Collect Evidence\nSuppose we flip coin 100 times:\n\nExpected under H₀: About 50 heads\nActually observe: 70 heads\n\n\n\nStep 3: Assess Evidence\nAsk: “If coin were truly fair (H₀ true), how likely is this result?”\nThis is like asking in a legal case:\n\n“If defendant were innocent, how do we explain the evidence?”\nVery improbable evidence suggests innocence might be false\n\n\n\n\n\n\n\nUnderstanding Error Types in Hypothesis Testing\n\n\n\nIn hypothesis testing, we can make two types of errors:\n\nType I Error (False Positive)\n\nDefinition: Rejecting H₀ when it is actually true\nProbability: α (significance level)\nExample in Justice System: Convicting an innocent person\nExample in Medicine: Diagnosing healthy patient as sick\n\n\n\nType II Error (False Negative)\n\nDefinition: Failing to reject H₀ when it is actually false\nProbability: β\nPower = 1 - β (probability of correctly rejecting false H₀)\nExample in Justice System: Letting guilty person go free\nExample in Medicine: Missing an actual disease\n\n\n\nTrade-off Between Errors\n\nDecreasing α (being more conservative) increases β\nDecreasing β (increasing power) requires either:\n\nLarger sample size\nLarger effect size\nHigher α\n\n\n\n\nPractical Implications\n\n\n\n\n\n\n\n\n\nContext\nType I Concern\nType II Concern\nTypical α\n\n\n\n\nCriminal Justice\nConvict innocent\nFree guilty\n0.001\n\n\nMedical Testing\nUnnecessary treatment\nMiss disease\n0.01\n\n\n\n\n\n\n\n\n\n\n\n\nHistorical Note: Jerzy Neyman (1894-1981)\n\n\n\nThe framework of statistical hypothesis testing as we know it today was largely developed by Jerzy Neyman, a Polish mathematician and statistician, in collaboration with Egon Pearson. Born in Bendery, Imperial Russia (now Moldova), Neyman made fundamental contributions to statistics that transformed both theoretical foundations and practical applications.\nHis most significant contributions include:\n\nDevelopment of the formal hypothesis testing framework, introducing the concepts of Type I and Type II errors\nCreation of confidence intervals as a way to express uncertainty in estimation\nPioneering the potential outcomes framework in causal inference\nAdvancement of sampling theory\n\nThe potential outcomes framework, first introduced by Neyman in his 1923 master’s thesis on agricultural experiments, revolutionized how we think about causality in statistics. This framework, later rediscovered and expanded by Donald Rubin (hence sometimes called the Neyman-Rubin causal model), introduced the concept of comparing potential outcomes that would occur under different treatments. For each unit, Neyman conceived of multiple potential outcomes, only one of which could be observed - a fundamental concept now known as the “fundamental problem of causal inference.”\nHis approach to statistical inference differed notably from R.A. Fisher’s significance testing, leading to important debates that helped shape modern statistical theory and practice.\nThe potential outcomes framework he introduced has become particularly influential in modern causal inference, epidemiology, and social sciences research. The impact of his contributions continues to be felt in how we approach statistical inference, experimental design, and causal analysis today.\n\n\n\n\n\nThe Logic of P-values\n\nWhat is a p-value?\np-value = P(seeing this evidence or more extreme | H₀ is true)\nLike asking:\n\n“How surprising is this evidence if H₀ is true?”\n“Could this easily happen by chance?”\n\n\n\nExample: Step by Step\nObserve 8 heads in 10 flips:\n\nAssume H₀: p = 0.5 (fair coin)\nCalculate: P(X ≥ 8) = P(8 heads) + P(9 heads) + P(10 heads) = \\binom{10}{8}(0.5)^8(0.5)^2 + \\binom{10}{9}(0.5)^9(0.5)^1 + \\binom{10}{10}(0.5)^{10} ≈ 0.055\nInterpret:\n\nAbout 5.5% chance of seeing this under H₀\nModerately unusual, but not extremely so\n\n\n\n\nCommon Misunderstandings\n\n“p-value is probability H₀ is true”\n\nNo: It’s probability of data, assuming H₀\nLike P(evidence|innocent), not P(innocent|evidence)\n\n“Small p-value proves H₀ false”\n\nNo: Only suggests H₀ unlikely\nLike strong evidence, but not proof\n\n“Large p-value proves H₀ true”\n\nNo: Just fails to provide evidence against H₀\nLike “not guilty” vs “proven innocent”\n\n\n\n\n\n\n\n\nBeyond Simple Significance Testing\n\n\n\n\nWhy Not Just Use 5%?\nHistorical reasons:\n\nR.A. Fisher suggested as benchmark\nPre-computed tables used 5%\nBecame convention, not logical necessity\n\n\n\nBetter Approach:\nConsider p-value on continuous scale:\n\np = 0.049 vs p = 0.051 are virtually same\nContext matters:\n\nMedical trials might need p &lt; 0.001\nMarket research might accept p &lt; 0.1\n\n\n\n\nPractical Significance\nAlways consider:\n\nEffect size (how big is the difference?)\nPractical importance\nCosts and benefits of decisions\nSample size and power",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_en.html#problem-solutions-part-1-the-binomial-tests-one-tailedsided-tests",
    "href": "inference_en.html#problem-solutions-part-1-the-binomial-tests-one-tailedsided-tests",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "18.2 Problem Solutions – part 1: the binomial tests (one-tailed/sided tests)",
    "text": "18.2 Problem Solutions – part 1: the binomial tests (one-tailed/sided tests)\n\nThe binomial test is a statistical hypothesis test used when you have binary (two-outcome) trials, where each trial is independent and has the same probability of success. It tests whether the observed proportion of successes differs significantly from an expected probability under the null hypothesis. For example: Testing whether a coin is fair by checking if the proportion of heads in 100 flips differs significantly from the expected probability of 0.5 under the null hypothesis.\n\n\nProblem 1: Binomial Test for Candidate Support\n\nProblem Statement\nAn election candidate believes she has the support of 50% (p = 0.5) of the residents in a particular town. A researcher suspects this might be an underestimation and conducts a survey. The researcher asks 10 people whether they support the candidate or not; 7 people say that they do (70% in a sample).\nCalculate the p-value and decide whether there is enough evidence to reject H0 using data from the sample (assuming the critical probability = 5%).\n\n\nSetup\nHypotheses:\n\nH_0: p = 0.5 (null hypothesis: true proportion (support) is 50%)\nH_1: p &gt; 0.5 (alternative hypothesis: true proportion is greater than 50%)\n\nData:\n\nSample size: n = 10\nObserved successes: x = 7 (70% support)\nHypothesized proportion: p_0 = 0.5\nSignificance level: \\alpha = 0.05 (5%)\n\n\n\nFinding the P-value\nFor a one-sided test, the p-value is the probability of observing 7 or more successes out of 10 trials, assuming H_0 is true. Using the binomial distribution:\nP(X \\geq 7) = P(X = 7) + P(X = 8) + P(X = 9) + P(X = 10)\n\n\n\n\n\n\nWhy One-Tailed Test?\n\n\n\nThis is a one-tailed/sided test because we’re specifically interested in whether the candidate is under-estimating her support. In statistical terms:\n\nIf she believes support is 50% but it’s actually 40%, she’s over-estimating her support\nIf she believes support is 50% but it’s actually 60%, she’s under-estimating her support\n\nOur research question only concerns under-estimation, so we only need to consider evidence in that direction (values greater than 50%). This is reflected in our alternative hypothesis H_1: p &gt; 0.5.\n\nWhy Not Just P(X = 7)?\nWe can’t just calculate P(X = 7) because:\n\nThe p-value represents the probability of observing results as extreme or more extreme than what we saw, assuming H_0 is true\n“More extreme” means results that provide even stronger evidence against H_0 in the direction of H_1\nSince H_1 suggests higher proportions than 50%, outcomes of 8, 9, or 10 supporters out of 10 would be even stronger evidence against H_0\n\nTherefore, we must sum:\nP(X \\geq 7) = P(X = 7) + P(X = 8) + P(X = 9) + P(X = 10)\nIf we only calculated P(X = 7) = 0.1172, we would ignore these other possible outcomes that also support H_1, leading to an incorrect p-value.\n\n\n\nFor each value k, we use the binomial probability formula:\nP(X = k) = \\binom{n}{k} p_0^k (1-p_0)^{n-k}\nLet’s calculate each term:\n\nP(X = 7) = \\binom{10}{7} (0.5)^7 (0.5)^3 = 120 \\cdot (0.5)^{10} = 0.1172\nP(X = 8) = \\binom{10}{8} (0.5)^8 (0.5)^2 = 45 \\cdot (0.5)^{10} = 0.0439\nP(X = 9) = \\binom{10}{9} (0.5)^9 (0.5)^1 = 10 \\cdot (0.5)^{10} = 0.0098\nP(X = 10) = \\binom{10}{10} (0.5)^{10} (0.5)^0 = 1 \\cdot (0.5)^{10} = 0.0010\n\nP-value = 0.1172 + 0.0439 + 0.0098 + 0.0010 = 0.1719 (17.19%)\n\n\nDecision\nSince the p-value (0.1719) is greater than the significance level (0.05), we fail to reject the null hypothesis.\n\n\nInterpretation\nThere is not enough evidence to conclude that the candidate is under-estimating her support. While the sample shows 70% support (higher than 50%), this difference could reasonably occur by chance even if the true support was only 50%. The relatively small sample size (n = 10) makes it harder to detect real differences.\n\n\n\nProblem 2: Binomial Test for Candidate Support (2)\n\nProblem Statement\nAn election candidate believes she has the support of 40% (p = 0.4) of the residents in a particular town. A researcher suspects this might be an overestimation and conducts a survey. The researcher asks 20 people whether they support the candidate or not; 3 people say that they do (15% in a sample). Calculate the p-value and decide whether there is enough evidence to reject H0 using data from the sample (assuming the critical probability = 5%).\n\n\nSetup\nHypotheses:\n\nH_0: p = 0.4 (null hypothesis: true proportion is 40%)\nH_1: p &lt; 0.4 (alternative hypothesis: true proportion is less than 40%)\n\nData:\n\nSample size: n = 20\nObserved successes: x = 3 (15% support)\nHypothesized proportion: p_0 = 0.4\nSignificance level: \\alpha = 0.05 (5%)\n\n\n\nWhy One-Tailed Test?\nThis is a one-tailed test because we’re specifically interested in whether the candidate is over-estimating her support. We only care about evidence suggesting the true proportion is less than 40%, leading to a left-tailed test.\n\n\nFinding the P-value\nFor this left-tailed test, the p-value is the probability of observing 3 or fewer successes out of 20 trials, assuming H_0 is true. Using the binomial distribution:\nP(X \\leq 3) = P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3)\nFor each value k, we use the binomial probability formula: P(X = k) = \\binom{n}{k} p_0^k (1-p_0)^{n-k}\n\nWe want to find P(X ≤ 3) when X follows B(20, 0.4) Using the binomial formula:\n\nP(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\n\nCalculate the combinations \\binom{20}{k} for k = 0, 1, 2, 3:\n\n\\binom{20}{0} = 1\n\\binom{20}{1} = 20\n\\binom{20}{2} = \\frac{20 \\times 19}{2 \\times 1} = 190\n\\binom{20}{3} = \\frac{20 \\times 19 \\times 18}{3 \\times 2 \\times 1} = 1,140\n\nCalculate each probability:\n\nFor k = 0: P(X = 0) = \\binom{20}{0}(0.4)^0(0.6)^{20} = 1 \\times 1 \\times 0.6^{20} \\approx 0.0000366\nFor k = 1: P(X = 1) = \\binom{20}{1}(0.4)^1(0.6)^{19} = 20 \\times 0.4 \\times 0.6^{19} \\approx 0.0004875\nFor k = 2: P(X = 2) = \\binom{20}{2}(0.4)^2(0.6)^{18} = 190 \\times 0.16 \\times 0.6^{18} \\approx 0.0030874\nFor k = 3: P(X = 3) = \\binom{20}{3}(0.4)^3(0.6)^{17} = 1,140 \\times 0.064 \\times 0.6^{17} \\approx 0.0123497\n\nSum all probabilities: P(X \\leq 3) = \\sum_{k=0}^3 P(X = k) = 0.0000366 + 0.0004875 + 0.0030874 + 0.0123497 = 0.0159612\nDecision rule:\n\n\nReject H₀ if p-value &lt; α\nSince 0.0159612 &lt; 0.05, we reject H₀.\n\n\n\nDecision\nSince the p-value is less than the significance level (0.05), we reject the null hypothesis.\n\n\nInterpretation\nThere is sufficient evidence at the 5% significance level to conclude that the candidate is overestimating her support. The sample shows only 15% support, which is significantly lower than the candidate’s belief of 40%. The probability of observing such low support (3 or fewer out of 20) would be only about 1.6% if the true support were actually 40%.\n\n\n\nProblem 3: Binomial Test for Candidate Support (3)\n\nProblem Statement\nA political candidate claims that 40% of residents in a town support her campaign (p = 0.4). A researcher suspects this might be an overestimation and conducts a survey. In a random sample of 12 residents, 1 person expresses support for the candidate. Test whether there is sufficient evidence to conclude that the candidate is overestimating her support level, using a significance level of 5%.\n\n\nHypotheses\n\n\\begin{align*}\nH_0&: p = 0.4 \\text{ (The candidate's claim is correct)} \\\\\nH_1&: p &lt; 0.4 \\text{ (The candidate is overestimating support)}\n\\end{align*}\n\n\n\nGiven Information\n\nSample size: n = 12\nNumber of successes: x = 1\nHypothesized proportion: p_0 = 0.4\nSignificance level: \\alpha = 0.05\nObserved proportion: \\hat{p} = \\frac{1}{12} \\approx 0.083\n\n\n\nSolution\n\nFor a left-tailed test, we calculate the probability of observing 1 or fewer successes under H_0.\nUsing the binomial probability formula: P(X \\leq 1) = \\sum_{k=0}^{1} \\binom{12}{k}(0.4)^k(0.6)^{12-k}\nWe find: P(X \\leq 1) = 0.0196\nDecision Rule:\n\nReject H_0 if p-value &lt; \\alpha\nSince 0.0196 &lt; 0.05, we reject H_0\n\n\n\n\nConclusion\nAt a 5% significance level, there is sufficient evidence to conclude that the candidate is overestimating her support. The sample proportion (8.3%) is substantially lower than the claimed 40% support, and this difference is statistically significant (p = 0.0196).\n\n\nStatistical Power Consideration\nWhile the sample size (n = 12) is relatively small, we were still able to detect a significant difference. This is because the observed difference between the claimed proportion (40%) and sample proportion (8.3%) was quite large. However, a larger sample size would provide more reliable results and better estimation of the true support proportion.\n\n\n\nProblem 4: Binomial Test for Candidate Support (4)\nAn election candidate claims that 20% of residents in a town support her campaign. A researcher believes the candidate might be over-estimating her support and wants to test this claim. In a random sample of 12 residents, 4 people express support for the candidate. Test whether there is sufficient evidence to conclude that the candidate is over-estimating her support level, using a significance level of 5%.\nGiven:\n\nClaimed support: p = 0.2\nSample size: n = 12\nNumber of supporters in sample: x = 4\nSignificance level: α = 0.05\n\n\nSolution\nStep 1: State the Hypotheses\nSince we want to test if the candidate is over-estimating (true proportion is less than claimed):\n\n\\begin{align*}\nH_0&: p = 0.2 \\text{ (The candidate's claim is correct)} \\\\\nH_1&: p &lt; 0.2 \\text{ (The candidate is overestimating support)}\n\\end{align*}\n\nStep 2: Choose the Test Statistic\nWe use the number of successes (X) in the sample, where X follows a binomial distribution with n = 12 and p = 0.2 under H₀.\nObserved value: x = 4\nStep 3: Calculate the Test Statistic and P-value\nFor a left-tailed test, we calculate:\nP(X \\leq 4) = \\sum_{k=0}^{4} \\binom{12}{k}(0.2)^k(0.8)^{12-k}\n\n# Calculate p-value\np_value &lt;- pbinom(4, size = 12, prob = 0.2)\nprint(paste(\"P-value =\", round(p_value, 4)))\n\n[1] \"P-value = 0.9274\"\n\n\nThe p-value is 0.9274\nStep 4: Decision Rule\n\nReject H₀ if p-value &lt; α\nSince 0.9274 &gt; 0.05, we fail to reject H₀\n\nStep 5: Interpretation\nAt a 5% significance level, there is not enough evidence to conclude that the candidate is over-estimating her support. In fact, the sample data shows 4/12 ≈ 33.3% support, which is higher than her claimed 20%, going in the opposite direction of our alternative hypothesis.\n\n\nAdditional Notes\n\nSample Proportion: \\hat{p} = \\frac{x}{n} = \\frac{4}{12} = 0.333\nThe high p-value reflects that the sample proportion (33.3%) is actually higher than the hypothesized value (20%), not lower as we were testing for.\nIf we had suspected under-estimation rather than over-estimation, we should have set up the test with H₁: p &gt; 0.2.\nGiven the small sample size (n = 12), the power of this test to detect true differences is limited.\n\n\n\nR Code\nHere’s the complete R code for this analysis:\n\n# Given values\nn &lt;- 12          # sample size\nx &lt;- 4           # number of successes\np0 &lt;- 0.2        # hypothesized proportion\nalpha &lt;- 0.05    # significance level\n\n# Calculate p-value for left-tailed test\np_value &lt;- pbinom(x, size = n, prob = p0)\n\n# Calculate sample proportion\np_hat &lt;- x/n\n\n# Print results\ncat(\"Sample proportion =\", round(p_hat, 3), \"\\n\")\n\nSample proportion = 0.333 \n\ncat(\"P-value =\", round(p_value, 4), \"\\n\")\n\nP-value = 0.9274 \n\ncat(\"Decision: \", ifelse(p_value &lt; alpha, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\nDecision:  Fail to reject H0 \n\n\n\n\n\nProblem 5: Binomial Test for EU Support\n\nProblem Statement\nA politician believes that support for his country’s EU membership is about 98% (p = 0.98). A researcher wants to test whether the politician is overestimating this level of support.\nIn a sample of 15 people (n = 15), the researcher observes that 13 people support membership. Let’s define the random variable X as the number of people in the sample who support EU membership. We observed X = 13 “successes” in 15 Bernoulli trials.\nIs there enough evidence to reject the claim that the support is 98%?\n\n\nSetup\nHypotheses:\n\nH_0: p = 0.98 (null hypothesis: true proportion is 98%)\nH_1: p &lt; 0.98 (alternative hypothesis: true proportion is less than 98%)\n\nData:\n\nSample size: n = 15\nObserved successes: x = 13 (≈86.7% support)\nHypothesized proportion: p_0 = 0.98\nSignificance level: \\alpha = 0.05 (5%)\n\n\n\nFinding the P-value\nFor this left-tailed test, we need P(X \\leq 13). Given the high value of p_0, it’s more efficient to use the complement rule:\nP(X \\leq 13) = 1 - P(X \\geq 14)\n\nThe complement rule in probability states that P(A) = 1 - P(not A), because P(A) + P(not A) = 1. For a left-tailed test, we need P(X ≤ 13). Instead of summing P(X = 0) + P(X = 1) + … + P(X = 13), it’s easier to: calculate P(X ≤ 13) = 1 - P(X &gt; 13), or P(X ≤ 13) = 1 - P(X ≥ 14).\n\nFor this left-tailed test: P(X \\leq 13) = 1 - P(X &gt; 13) = 1 - P(X = 14) - P(X = 15)\nLet’s calculate step by step:\n\nFor X = 14:\n\n\nCalculate combination:\n\n\\binom{15}{14} = \\frac{15!}{14!(15-14)!} = \\frac{15}{1} = 15\n\nCalculate probability:\n\nP(X = 14) = \\binom{15}{14}(0.98)^{14}(0.02)^1 = 15 \\times (0.98)^{14} \\times 0.02 = 15 \\times 0.75051... \\times 0.02 \\approx 0.2252\n\nFor X = 15:\n\n\nCalculate combination:\n\n\\binom{15}{15} = 1\n\nCalculate probability:\n\nP(X = 15) = \\binom{15}{15}(0.98)^{15}(0.02)^0 = 1 \\times (0.98)^{15} \\times 1 = (0.98)^{15} \\approx 0.7395\n\nTherefore:\n\nP(X \\leq 13) = 1 - P(X = 14) - P(X = 15) = 1 - 0.2252 - 0.7395 \\approx 0.0353\n\n\nDecision\nSince the p-value (0.0353) is less than the significance level (0.05), we reject the null hypothesis.\n\n\nInterpretation\nThere is sufficient evidence to conclude that the politician is overestimating the support for EU membership. While 86.7% support in the sample is still very high, it’s significantly lower than the politician’s claim of 98%. Under the assumption that true support is 98%, the probability of observing 13 or fewer supporters in a sample of 15 people would be only about 3.53%.\nNote on Complement Rule\nThis problem demonstrates the utility of the complement rule in probability calculations. Instead of calculating probabilities for outcomes 0 through 13 (14 calculations), we only needed to calculate probabilities for outcomes 14 and 15 (2 calculations). This is particularly efficient when:\n\nWe have a large number of outcomes in the “tail” we’re interested in\nThe probability is concentrated in the other tail (here, high values due to p_0 = 0.98)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_en.html#problem-solutions-part-2-the-binomial-tests-two-tailedsided-tests",
    "href": "inference_en.html#problem-solutions-part-2-the-binomial-tests-two-tailedsided-tests",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "18.3 Problem Solutions – part 2: the binomial tests (two-tailed/sided tests (*))",
    "text": "18.3 Problem Solutions – part 2: the binomial tests (two-tailed/sided tests (*))\n(…)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_en.html#testing-ols-regression-parameter-significance-using-r",
    "href": "inference_en.html#testing-ols-regression-parameter-significance-using-r",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "18.4 Testing OLS Regression Parameter Significance Using R (*)",
    "text": "18.4 Testing OLS Regression Parameter Significance Using R (*)\n(…)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_pl.html",
    "href": "inference_pl.html",
    "title": "19  Wprowadzenie do Wnioskowania Statystycznego: Logika Testowania Hipotez Statystycznych",
    "section": "",
    "text": "Wnioskowanie statystyczne to sposób, w jaki wyciągamy wnioski o populacji na podstawie próby. To jak bycie detektywem: nigdy nie mamy wszystkich informacji, ale możemy wyciągać uzasadnione wnioski na podstawie dostępnych dowodów.\n\nPodstawowa Logika\nWyobraź sobie, że podejrzewasz, iż moneta może być nieuczciwa. Jak sprawdzić takie przypuszczenie?\n\nZbierz Dowody:\n\nWykonaj wiele rzutów monetą\nZapisz wyniki\nSprawdź, czy są zgodne z tym, czego oczekiwałbyś od uczciwej monety\n\nPodejmij Decyzję:\n\nJeśli wyniki wyglądają normalnie → kontynuuj założenie, że moneta jest uczciwa\nJeśli wyniki wyglądają bardzo nietypowo → podejrzewaj, że moneta jest nieuczciwa\n\n\n\n\n\n\n\n\nKluczowe Kroki Testowania Hipotez Statystycznych (ogólny schemat)\n\n\n\n\nWstępne Podejrzenie/Pytanie Badawcze\n\nPodejrzewamy istnienie pewnego efektu/związku/różnicy\nTo ukierunkowuje nasze badanie i analizę\n\nZbieranie Danych\n\nGromadzimy odpowiednią ilość danych\nWielkość próby zależy od oczekiwanego efektu i wymaganej precyzji\n\nObserwacja Wyników\n\nObserwujemy i podsumowujemy nasze dane\nSzukamy istotnych wzorców w danych\n\nSystem Hipotez\n\nH₀: brak efektu/brak różnicy (“status quo”)\nH₁: efekt istnieje (jedno- lub dwustronny)\nWybór kierunku zależy od pytania badawczego\n\nPodejście Wartości p\n\nRozważamy: jak prawdopodobne są nasze wyniki (lub bardziej skrajne) jeśli H₀ jest prawdziwa?\nWybieramy odpowiedni model probabilistyczny w zależności od typu danych\nObliczamy to prawdopodobieństwo (wartość p)\n\nPodejmowanie Decyzji\n\nPorównujemy wartość p z poziomem istotności (zazwyczaj α = 0.05)\nMała wartość p sugeruje, że wyniki są mało prawdopodobne przy H₀\n\nWniosek\n\nJeśli p ≤ α, odrzucamy H₀\nWnioskujemy o dowodach przeciwko hipotezie zerowej\nRozważamy znaczenie praktyczne\n\n\n\n\n\n\n\n\n\n\nPodstawowa Logika Testowania Hipotez Statystycznych: Analiza Zdolności ESP\n\n\n\nProblem Badawczy: Testowanie Deklaracji o Posiadaniu Zdolności Pozazmysłowych\nOsoba twierdzi, że posiada zdolności ESP (percepcję pozazmysłową, tzw. szósty zmysł), które pozwalają jej przewidywać wyniki rzutów monetą. Aby naukowo przetestować to twierdzenie, projektujemy eksperyment, w którym moneta jest rzucana 100 razy, a osoba musi przewidzieć każdy wynik przed rzutem.\nOsoba osiąga sukces w 70 na 100 przewidywań. Jednak istnieje subtelne, ale kluczowe zastrzeżenie: wysoki współczynnik sukcesu może wskazywać zarówno na zdolności ESP, JAK I na nieuczciwą monetę.\nDefiniowanie Prawdopodobieństwa Bazowego i Oczekiwanej Skuteczności\nJeśli osoba jedynie zgaduje (brak ESP), każde przewidywanie jest równoważne losowemu zgadywaniu z prawdopodobieństwem sukcesu 0.5. Jeśli rzeczywiście posiada zdolności ESP, spodziewalibyśmy się współczynnika sukcesu przekraczającego 0.5. To stanowi podstawę naszego badania statystycznego.\nUstanawianie Systemu Hipotez Statystycznych\nUstalamy dwie konkurencyjne hipotezy:\n\nHipoteza Zerowa (H₀): Przewidywania opierają się na losowym zgadywaniu przy użyciu uczciwej monety (p = 0.5)\nHipoteza Alternatywna (H₁): Albo osoba posiada zdolności ESP, ALBO moneta jest nieuczciwa (p &gt; 0.5)\n\nWybór Między Testami Jedno- i Dwustronnymi\nW testowaniu hipotez musimy zdecydować, czy testujemy efekt w jednym czy obu kierunkach:\nTest Jednostronny (Nasz Obecny Przypadek):\n\nTestuje efekt tylko w jednym kierunku (tutaj: lepszy niż przypadek)\nWiększa moc wykrywania określonego efektu kierunkowego\nOdpowiedni, gdy interesuje nas tylko jeden kierunek\nPrzykład: Interesuje nas tylko wynik lepszy niż przypadkowy, nie gorszy\n\nTest Dwustronny (Alternatywne Podejście):\n\nTestuje efekt w obu kierunkach (zarówno lepszy jak i gorszy niż przypadek)\nMniejsza moc, ale bardziej kompleksowy\nOdpowiedni, gdy każde odchylenie od hipotezy zerowej jest interesujące\nPrzykład: Testowanie, czy moneta jest nieuczciwa w kierunku orła LUB reszki\n\nWybór Modelu Probabilistycznego: Rozkład Dwumianowy\nNasz test ESP pasuje do modelu prawdopodobieństwa dwumianowego, ponieważ:\n\nKażde przewidywanie jest niezależne\nKażde przewidywanie ma dokładnie dwa możliwe wyniki (poprawne/niepoprawne)\nPrawdopodobieństwo sukcesu pozostaje stałe (0.5 przy H₀)\nZliczamy całkowitą liczbę sukcesów w ustalonej liczbie prób\n\nDla naszego przykładu obliczamy: P(X \\geq 70) = \\sum_{k=70}^{100} \\binom{100}{k}(0.5)^k(0.5)^{100-k}\nObliczanie i Interpretacja Wartości p\nWartość p pomaga nam ocenić, jak zaskakujące byłyby nasze wyniki, gdyby H₀ była prawdziwa:\n\nMierzy prawdopodobieństwo uzyskania 70 lub więcej poprawnych przewidywań na 100 prób przez czysty przypadek\nBardzo mała wartość p sugeruje, że taki sukces byłby rzadki przy losowym zgadywaniu\nKonwencjonalny próg 0.05 oznacza, że wymagamy wyników, które wystąpiłyby przypadkowo rzadziej niż w 5% przypadków\n\nReguły Decyzyjne i Potencjalne Błędy w Testowaniu ESP\n\nBłąd Typu I (Fałszywie Pozytywny):\n\nStwierdzenie, że ktoś ma ESP, gdy miał po prostu szczęście\nOgraniczamy to ryzyko do 5% poprzez poziom istotności\nTo jak błędne potwierdzenie zdolności ESP\n\nBłąd Typu II (Fałszywie Negatywny):\n\nNiewychwycenie rzeczywistych zdolności ESP, lub faktu, że moneta jest obciążona\nBardziej prawdopodobny przy:\n\nWspółczynniku “sukcesu” (prawdopodobieństwo “sukcesu”) niewiele powyżej 0.5\nMałej liczbie prób\nRygorystycznych poziomach istotności\n\n\n\n\n\n\n\n\n\n\n\nObliczenie Wartości p dla Przykładu ESP\n\n\n\nSzczegóły Obliczeniowe\nDla naszego testu ESP z 70 sukcesami na 100 prób, obliczamy:\nP(X \\geq 70) = \\sum_{k=70}^{100} \\binom{100}{k}(0.5)^k(0.5)^{100-k} \\approx 0.0000393\nOznacza to, że przy hipotezie zerowej (czyste zgadywanie):\n\nPrawdopodobieństwo uzyskania 70 lub więcej poprawnych przewidywań przez przypadek wynosi około 0.00393%\nTak skrajne wyniki wystąpiłyby przypadkowo tylko około 4 razy na 100,000 prób\nJest to znacznie poniżej konwencjonalnego poziomu istotności 0.05 (5%)\n\nDecyzja Statystyczna\nPonieważ nasza wartość p (0.0000393) jest znacznie mniejsza niż α = 0.05:\n\nOdrzucamy hipotezę zerową\nWnioskujemy, że istnieją silne dowody statystyczne przeciwko “losowemu zgadywaniu”\nWynik jest uznawany za “wysoce istotny statystycznie”\n\nOstrożna Interpretacja\nMimo że nasz wynik jest istotny statystycznie, powinniśmy rozważyć:\n\nIstotność statystyczna nie dowodzi istnienia ESP (moneta może być nieuczciwa?)\nEksperyment powinien być powtarzalny w kontrolowanych warunkach",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Wprowadzenie do Wnioskowania Statystycznego: Logika Testowania Hipotez Statystycznych</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\n\n\n\nBlair, G., Coppock, A., & Humphreys, M. (2023). Research design in the social sciences: declaration, diagnosis, and redesign. Princeton University Press. https://book.declaredesign.org/\nBryman, A., 2016. Social research methods. Oxford University Press.\nBueno de Mesquita, Ethan and Anthony Fowler. 2021. Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis. Princeton University Press.\nCausality for Machine Learning. https://ff13.fastforwardlabs.com/\nCetinkaya-Rundel, M., Diez, D.M. and Barr, C.D., 2019 (4th ed.). OpenIntro Statistics: an Open-source Textbook: https://www.openintro.org/book/os/\nClaude [Large language model], 2024. https://www.anthropic.com\nConcepts and Computation: An Introduction to Political Methodology. https://pos3713.github.io/notes/\nHannay, K. (2019). Introduction to statistics and data science. http://khannay.com/StatsBook/\nIsmay, C. and Kim, A.Y., 2019. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. https://moderndive.com/index.html\nNavarro, D.J. and Foxcroft, D.R. (2019). Learning statistics with Jamovi: a tutorial for psychology students and other beginners. (Version 0.70). DOI: 10.24384/hgc3-7p15\nRemler, D.K. and Van Ryzin, G.G., 2014. Research methods in practice: Strategies for description and causation. Sage Publications.\nSanchez, G., Marzban, E. (2020) All Models Are Wrong: Concepts of Statistical Learning. https://allmodelsarewrong.github.io\nSchneider, W. J. (2023). Psycheval: A psychological evaluation toolkit. https://github.com/wjschne/psycheval\nTimbers, T., Campbell, T., & Lee, M. (2022). Data science: A first introduction. Chapman and Hall/CRC. https://datasciencebook.ca/",
    "crumbs": [
      "References"
    ]
  }
]