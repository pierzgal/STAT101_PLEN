[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics: An Introduction (PL: Wprowadzenie do Statystyki)",
    "section": "",
    "text": "Preface\n\n\n\n\n\n\nImportant\n\n\n\nThis is a preliminary (unfinished) draft of a Quarto class notes on Statistics. Please do not cite or reproduce its contents, as it may contain errors!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Foundations of Statistics and Demography",
    "section": "",
    "text": "1.1 Introduction\nStatistics is a way to learn about the world from data. It teaches how to collect data wisely, spot patterns, estimate population parameters, and make predictions—stating how wrong we might be.\nStatistics and demography are interconnected disciplines that provide powerful tools for understanding populations, their characteristics, and the patterns that emerge from data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#introduction",
    "href": "chapter1.html#introduction",
    "title": "1  Foundations of Statistics and Demography",
    "section": "",
    "text": "Statistics is the science of learning from data under uncertainty.\n\n\n\n\n\n\n\n\nNote\n\n\n\nStatistics is the science of collecting, organizing, analyzing, interpreting, and presenting data. It encompasses both the methods for working with data and the theoretical foundations that justify these methods.\nBut statistics is more than just numbers and formulas—it’s a way of thinking about uncertainty and variation in the world around us.\n\n\n\n\nDemography is the scientific study of human populations, focusing on their size, structure, distribution, and changes over time. It’s essentially the statistical analysis of people - who they are, where they live, how many there are, and how these characteristics evolve.\n\n\n\n\n\n\n\n\nRounding and Scientific Notation\n\n\n\nMain Rule: Unless otherwise specified, round the decimal parts of decimal numbers to at least 2 significant figures. In statistics, we often work with long decimal parts and very small numbers — don’t round excessively in intermediate steps, round at the end of calculations.\n\nRounding in Statistical Context\nThe decimal part consists of digits after the decimal point. In statistics, it’s particularly important to maintain appropriate precision:\nDescriptive statistics:\n\nMean: \\bar{x} = 15.847693... \\rightarrow 15.85\nStandard deviation: s = 2.7488... \\rightarrow 2.75\nCorrelation coefficient: r = 0.78432... \\rightarrow 0.78\n\nVery small numbers (p-values, probabilities):\n\np = 0.000347... \\rightarrow 0.00035 or 3.5 \\times 10^{-4}\nP(X &gt; 2) = 0.0000891... \\rightarrow 0.000089 or 8.9 \\times 10^{-5}\n\n\n\nSignificant Figures in Decimal Parts\nIn the decimal part, significant figures are all digits except leading zeros:\n\n.78432 has 5 significant figures → round to .78 (2 s.f.)\n.000347 has 3 significant figures → round to .00035 (2 s.f.)\n.050600 has 4 significant figures → round to .051 (2 s.f.)\n\n\n\nRounding Rules in Statistics\n\nRound only the decimal part to at least 2 significant figures\nThe integer part remains unchanged\nIn long calculations keep 3-4 digits in the decimal part until the final step\nNEVER round to zero - small values have interpretive significance\nFor very small numbers use scientific notation when it improves readability\nP-values often require greater precision — keep 2-3 significant figures\n\n\n\nScientific Notation in Statistics\nIn statistics, we often encounter very small numbers. Use scientific notation when it improves readability:\nP-values and probabilities:\n\np = 0.000347 = 3.47 \\times 10^{-4} (better: 3.5 \\times 10^{-4})\nP(Z &gt; 3.5) = 0.000233 = 2.33 \\times 10^{-4}\n\nLarge numbers (rare in basic statistics):\n\nN = 1\\,234\\,567 = 1.23 \\times 10^6\n\nWhen in doubt: Better to keep an extra digit than to round too aggressively\n\n\n\n\n\n\n\n\n\n\nWhat is Statistics For in Social and Political Science?\n\n\n\nStatistics is essential in social and political science for several key purposes:\nUnderstanding Social Phenomena: Measuring inequality, poverty, unemployment, political participation; describing demographic patterns and social trends; quantifying attitudes, beliefs, and behaviors in populations.\nTesting Theories: Political scientists theorize about democracy, voting behavior, conflict, and institutions. Sociologists develop theories about social mobility, inequality, and group dynamics. Statistics allows us to test whether these theories match reality.\nCausal Inference: Social scientists want to answer “why” questions—Does education increase income? Do democracies go to war less often? Does social media affect political polarization? Statistics helps separate causation from mere correlation.\nPolicy Evaluation: Assessing whether interventions work—Does a job training program reduce unemployment? Did election reform increase voter turnout? Are anti-poverty programs effective? Statistics provides tools to evaluate what works and what doesn’t.\nPublic Opinion Research: Election polls and forecasting; measuring public support for policies; understanding how opinions vary across demographic groups; tracking attitude changes over time.\nMaking Generalizations: We can’t survey everyone, so we sample and use statistics to make inferences about entire populations. A poll of 1,000 people can tell us about a nation of millions (with known uncertainty).\nDealing with Complexity: Human societies are messy—many factors influence outcomes simultaneously. Statistics helps us control for confounding variables, isolate specific effects, and make sense of multivariate relationships.\nThe Uniqueness of Social Sciences: Unlike natural sciences, social sciences study human behavior, which is highly variable and context-dependent. Statistics provides the tools to find patterns and draw conclusions despite this inherent uncertainty.\n\n\n\n\nWhen working with data, statisticians use two different approaches: exploration and confirmation/verification (inferential statistics). First, we examine the data to understand its characteristics and identify patterns. Then, we use formal methods to test specific hypotheses and draw conclusions.\n\n\n\n\n\n\n\nPercent vs Percentage Points (pp)\n\n\n\nWhen news reports say “unemployment decreased by 2,” do they mean 2 percentage points (pp) or 2 percent?\nThese are not the same:\n\n2 pp (absolute change): e.g., 10% → 8% (−2 pp).\n2% (relative change): multiply the old rate by 0.98; e.g., 10% → 9.8% (−0.2 pp).\n\nAlways ask:\n\nWhat is the baseline (earlier rate)?\nIs the change absolute (pp) or relative (%)?\nCould this be sampling error / random variation?\nHow was unemployment measured (survey vs. administrative), when, and who’s included?\n\nRule of thumb\n\nUse percentage points (pp) when comparing rates directly (unemployment, turnout).\nUse percent (%) for relative changes (proportional to the starting value).\n\nTiny lookup table\n\n\n\n\n\n\n\n\nStarting rate\n“Down 2%” (relative)\n“Down 2 pp” (absolute)\n\n\n\n\n6%\n6% × 0.98 = 5.88% (−0.12 pp)\n4%\n\n\n8%\n8% × 0.98 = 7.84% (−0.16 pp)\n6%\n\n\n10%\n10% × 0.98 = 9.8% (−0.2 pp)\n8%\n\n\n\nUwaga (PL): 2% ≠ 2 punkty procentowe (pp).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#exploratory-data-analysis-eda",
    "href": "chapter1.html#exploratory-data-analysis-eda",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.2 Exploratory Data Analysis (EDA)",
    "text": "1.2 Exploratory Data Analysis (EDA)\nWhat is EDA? Exploratory Data Analysis is the initial step where we examine data systematically to understand its structure and characteristics. This phase does not involve formal hypothesis testing—it focuses on discovering what the data contains.\nWhy do we do EDA?\n\nFind interesting patterns you didn’t expect\nSpot mistakes or unusual values in your data\nGet ideas about what questions to ask\nUnderstand what your data looks like before doing formal tests (many statistical methods have specific requirements about the data to work properly. EDA helps check whether our data meets these requirements - e.g. 1) some tests require data to have a normal distribution (bell-shaped), 2) we need to verify that the relationship between variables is actually linear, or 3) check homogeneity of variance and find outliers)\n\n\n\n\n\n\n\nThe EDA Approach\n\n\n\nWhen conducting EDA, we begin without predetermined hypotheses. Instead, we examine data from multiple perspectives to discover patterns and generate questions for further investigation.\n\n\n\nSimple Tools for Exploring Data\n1. Summary Numbers (Descriptive Statistics)\nThese are basic calculations that describe your data:\nFinding the “Typical” Value:\n\nArithmetic Mean (Average): Add up all values and divide by how many you have. Example: If 5 students scored 70, 80, 85, 90, and 100 on a test, the average is 85.\nMedian (Middle): The value in the middle when you line up all numbers from smallest to largest. In our test example, the median is also 85.\nMode (Most Common): The value that appears most often. If ten families have 1, 2, 2, 2, 2, 3, 3, 3, 4, and 5 children, the mode is 2 children.\n\nUnderstanding Spread:\n\nRange: Just subtract the smallest number from the biggest. If students’ ages go from 18 to 24, the range is 6 years.\nStandard Deviation: Shows how spread out your data is from the average. A small standard deviation means most values are close to the average; a large one means they’re more spread out.\n\n2. Visual Exploration\nGraphical methods help reveal patterns that numerical summaries alone might not show:\n\nPopulation Pyramids: Show how many people are in each age group, split by males and females. Helps you see if a population is young or old.\nBox Plots: Show the middle of your data and help spot unusual values\nScatter Plots: Display relationships between two variables (such as hours studied versus test scores)\nTime (Series) Graphs: Show how something changes over time (like temperature throughout the year)\nHistograms: A histogram is a graphical representation of data that shows the frequency distribution of a dataset. It consists of adjacent bars (with no gaps between them) where each bar represents a range of values (called a bin or class interval), and the height of the bar shows how many data points (what proportion of data points) fall within that range. Histograms are used to visualize the shape, spread, and central tendency of numerical data.\n\n\n\n\nhttps://commons.wikimedia.org/wiki/File:%C5%81%C3%B3d%C5%BA_population_pyramid.svg\n\n\n3. Looking for Connections/Associations:\n\nDo two variables move together? (When one goes up, does the other go up too?)\nCan you draw a line (regression line) that roughly fits your data points?\nDo you see any clear patterns or trends?\n\n\n\n\n\n\n\nUsing the Same Techniques for Different Purposes\n\n\n\nMany statistical techniques serve both exploratory and confirmatory functions:\nExploring: We calculate correlations or fit regression lines to understand what relationships exist in the data. The focus is on discovering patterns.\nConfirming: We apply statistical tests to determine whether observed patterns are statistically significant or could have occurred by chance. The focus is on formal hypothesis testing.\nThe same technique can serve different purposes depending on the research phase.\n\n\n4. Good Questions to Ask While Exploring:\n\nWhat does the shape of my data look like?\nAre there any weird or unusual values?\nDo I see any patterns?\nIs any data missing?\nDo different groups show different patterns?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#inferential-statistics",
    "href": "chapter1.html#inferential-statistics",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.3 Inferential Statistics",
    "text": "1.3 Inferential Statistics\nAfter exploring, you might want to make formal conclusions. Inferential statistics helps you do this.\nThe Basic Idea: You have data from some people (a sample), but you want to know about everyone (the population). Inferential statistics helps you make educated guesses about the bigger group based on your smaller group.\n\n\n\n\n\n\nNote\n\n\n\nA random sample requires that each member has a known, non-zero chance of being selected, not necessarily an equal chance.\nWhen every member has an equal chance of selection, that’s specifically called a simple random sample - which is the most basic type.\n\n\n\n\n\n\n\n\nA Soup-Tasting Analogy\n\n\n\n\nConsider a chef preparing soup for 100 people who needs to assess its flavor without consuming the entire batch:\nPopulation: The entire pot of soup (100 servings)\nSample: A single spoonful for tasting\nPopulation Parameter: The true average saltiness of the complete pot (unknown)\nSample Statistic: The saltiness level detected in the spoonful (observable, a point estimate)\nStatistical Inference: Using the spoonful’s characteristics to draw conclusions about the entire pot\n\nKey Principles\n1. Random sampling is essential: The chef must thoroughly stir the soup before sampling. Consistently sampling from the surface might miss seasoning that has settled, introducing systematic bias.\n2. Sample size affects precision: A larger spoonful provides more reliable information about overall flavor than a small sip, though practical constraints (costs, time) limit sample size.\n3. Uncertainty is inherent: Even with proper sampling technique, the spoonful might not perfectly represent the entire pot’s characteristics.\n4. Systematic bias undermines inference: If someone secretly adds salt only to the sampling area, conclusions about the whole pot become invalid—illustrating how sampling bias distorts statistical inference.\n5. Inference has scope limitations: The sample can estimate average saltiness but cannot reveal whether some portions are saltier than others, highlighting the limits of what samples can tell us about population variability.\nThis analogy captures the essence of statistical reasoning: using carefully selected samples to learn about larger populations while explicitly acknowledging and quantifying the inherent uncertainty in this process.\n\n\n\n\n\nStatistical Thinking\n\nThe Scenario\nYour university is considering keeping the library open 24/7. The administration needs to know: What proportion of students support this change?\n\n\n\n\n\n\nThe Fundamental Challenge\n\n\n\nIdeal world: Ask all 20,000 students → Get the exact answer\nReal world: Survey 100 students → Get an estimate with uncertainty\n\n\n\n\n\nTwo Approaches to the Same Data\nImagine you survey 100 random students and find that 60 support the 24/7 library hours.\n\n\n❌ Without Statistical Thinking\n“60 out of 100 students said yes.”\nConclusion: “Exactly 60% of all students support it.”\nDecision: “Since it’s over 50%, we have clear majority support.”\nProblem: Ignores that a different sample might give 55% or 65%\n\n✅ With Statistical Thinking\n“60 out of 100 students said yes.”\nConclusion: “We estimate 60% support, with a margin of error of ±10 pp.”\nDecision: “True support is likely between 50% and 70%—we need more data to be certain of majority support.”\nAdvantage: Acknowledges uncertainty and informs better decisions\n\n\n\n\n\nThe Sample Size and Uncertainty (Random Error)\nSuppose we take a random sample of n=1000 voters and observe \\hat p = 0.55 (i.e., 55% support for a candidate in upcoming elections, 550 in 1000 sample). Then:\n\nOur best single-number estimate of the population share is \\hat p = 0.55.\nA typical “95\\% range of plausible values” around \\hat p can be approximated by \\hat p \\pm \\text{Margin of (Random) Error}, i.e., \n\\hat p \\;\\pm\\; 2\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\n\\;=\\;\n0.55 \\;\\pm\\; 2\\sqrt{\\frac{0.55\\cdot 0.45}{1000}}\n\\approx\n0.55 \\pm 0.031,\n i.e., roughly 52\\%\\text{–}58\\% (about \\pm 3.1 percentage points).\nThe width of this range shrinks predictably with sample size: \n\\text{MoE (width)} \\;\\propto\\; \\frac{1}{\\sqrt{n}}.\n For example, increasing n from 1000 to 4000 cuts the range by about half (0.0158).\n\n\nHere’s how sample size affects our confidence:\n\n\n\n\n\n\n\n\n\n\nSample Size\nYour Result\nMargin of (Random) Error\nConfidence Interval\nInterpretation\n\n\n\n\nn = 100\n60%\n±10 pp\n50% to 70%\nUncertain about majority\n\n\nn = 400\n60%\n±5 pp\n55% to 65%\nLikely majority support\n\n\nn = 1,000\n60%\n±3 pp\n57% to 63%\nClear majority support\n\n\nn = 1,600\n60%\n±2.5 pp\n57.5% to 62.5%\nStrong majority support\n\n\nn = 10,000\n60%\n±1 pp\n59% to 61%\nVery precise estimate\n\n\n\nThe Diminishing Returns Principle: Notice that going from 100 to 400 samples (4× increase) cuts the margin of error in half, but going from 1,600 to 10,000 samples (6× increase) only reduces it by 1.5 percentage points. To halve your margin of error, you must quadruple your sample size.\nThis is why most polls stop around 1,000-1,500 respondents—the gains in precision beyond that point rarely justify the additional cost and effort.\n\n\n\n\n\n\nRemember\n\n\n\nStatistical thinking transforms “60 students said yes” from a precise-sounding but misleading statement into an honest assessment: “We’re reasonably confident that between 50% and 70% of all students support this.”\nThis humility leads to better decisions.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFundamental Principle: Statistics does not eliminate uncertainty—it helps us measure, manage, and communicate it effectively.\n\n\n\n\n\n\n\n\n\nHistorical Example: The 1936 Literary Digest Poll\n\n\n\nThe Literary Digest conducted one of the largest polls in history with 2.4 million responses, predicting Alf Landon would defeat Franklin D. Roosevelt in the 1936 presidential election.\nDespite the massive sample size:\nPrediction: Landon 57%, Roosevelt 43%\nActual Result: Roosevelt 62%, Landon 38%\nError: 25 percentage points!\nWhat went wrong? The poll suffered from systematic bias:\nSelection bias in sampling frame:\n\nSources: telephone directories, automobile registrations, club memberships\nProblem: In 1936, these sources overrepresented wealthy Americans who favored Landon\nResult: The sample systematically excluded Roosevelt supporters\n\nNon-response bias:\n\nOnly 24% of those contacted responded (low response rate)\nLikely respondents: those with strong anti-Roosevelt opinions\nNon-respondents: many Roosevelt supporters didn’t feel compelled to participate\n\nKey Lessons:\n\nA large biased sample is worse than a small representative sample\nStandard statistical errors only measure random error, not bias\n\nSample size cannot fix fundamental sampling problems\nRepresentative sampling matters more than sample size\n\nThis disaster led to major improvements in polling methodology, including the development of probability sampling and response rate tracking.\n\nModern Polling\nToday’s polls, while much smaller than the Literary Digest’s 2.4 million responses, are far more accurate because they focus on:\nRepresentative sampling: Using probability-based methods to ensure all groups have known chances of selection\nBias detection and correction: Monitoring response rates across demographics and adjusting for known biases\nUncertainty quantification: Reporting margins of statistical error that honestly communicate the limits of what we know\nExample: A modern poll of 1,000 randomly selected voters with a 3 pp margin of error is far more reliable than the Literary Digest’s massive but biased survey.\n\n\n\n\n\n\n\n\n\n\nUnderstanding Randomness\n\n\n\nA random experiment is any process whose result cannot be predicted with certainty, such as tossing a coin or rolling a die. An outcome is a single possible result of that experiment—for example, getting “heads” or rolling a “5”. An event is a set of one or more outcomes that we’re interested in; it could be a simple event (like rolling exactly a 3) or a compound event (like rolling an even number, which includes the outcomes 2, 4, and 6).\n\nProbability is a way of measuring how likely something is to happen. It’s a number between 0 and 1 (or 0% and 100%) that represents the chance of an event occurring.\n\nIf something has a probability of 0, it’s impossible - it will never happen. If something has a probability of 1, it’s certain - it will definitely happen. Most things fall somewhere in between.\nFor example, when you flip a fair coin, there’s a 0.5 (or 50%) probability it will land on heads, because there are two equally likely outcomes and heads is one of them.\nProbability helps us make sense of uncertainty and randomness in the world.\n\n\nIn statistics, randomness is an orderly way to describe uncertainty. While each individual outcome is unpredictable, stable patterns (more formally, empirical distributions of outcomes converge to probability distributions) emerge over many repetitions.\n\nExample: Flip a fair coin:\n\nSingle flip: Completely unpredictable—you can’t know if it’ll be heads or tails\n100 flips: You’ll get close to 50% heads (maybe 48 or 53)\n10,000 flips: Almost certainly very close to 50% heads (perhaps 49.8%)\n\nThe same applies to dice: you can’t predict your next roll, but roll 600 times and each number (1-6) will appear close to 100 times. This predictable long-run behavior from unpredictable individual events is the essence of statistical randomness.\n\nTypes of Randomness\nEpistemic vs. Ontological Randomness:\n\nEpistemic (due to incomplete knowledge): We treat an outcome as random because not all determinants are observed or conditions are not controlled.\n\nExamples: The decision of an individual respondent in a poll (we don’t know the full set of motivations); measurement error in a survey (limited precision, item nonresponse); a coin toss modeled as random because minute, unobserved differences in initial conditions determine the outcome.\n\nOntological (intrinsic indeterminacy): Even complete knowledge does not remove outcome uncertainty.\n\nExamples: The time to radioactive decay of an atom; quantum mechanical phenomena.\n\n\n\n\nRelated Concepts\nRandomness vs. Haphazardness: Statistical randomness has mathematical structure and follows probability laws—it’s orderly uncertainty. Haphazardness suggests complete disorder without underlying patterns or rules.\nChaos: A deterministic process (governed by fixed rules) that appears random due to extreme sensitivity to initial conditions. Chaotic systems are predictable in theory but unpredictable in practice (e.g., weather).\nEntropy: A measure of disorder or uncertainty in a system. High entropy means high unpredictability; low entropy means high order. In statistics, entropy quantifies the uncertainty in a probability distribution.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#populations-and-samples",
    "href": "chapter1.html#populations-and-samples",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.4 Populations and Samples",
    "text": "1.4 Populations and Samples\nUnderstanding the distinction between populations and samples is crucial for proper statistical analysis. This distinction affects every aspect of our analysis, from planning data collection to interpreting results.\n\nPopulation\nA population is the complete set of individuals, objects, or measurements about which we wish to draw conclusions. The key word here is “complete”—a population includes every single member of the group we’re studying.\nExamples of Populations in Demography:\n\nAll residents of India as of January 1, 2024: This includes every person living in India on that specific date—approximately 1.4 billion people.\nAll births in Sweden during 2023: Every baby born within Swedish borders during that calendar year—roughly 100,000 births.\nAll households in Tokyo: Every residential unit where people live, cook, and sleep separately from others—about 7 million households.\nAll deaths from COVID-19 worldwide in 2020: Every death where COVID-19 was listed as a cause—several million deaths.\n\nPopulations can be:\nFinite: Having a countable number of members (all current U.S. citizens, all Polish municipalities in 2024)\nInfinite: Theoretical or uncountably large (all possible future births)\nFixed: Defined at a specific point in time (all residents on census day)\nDynamic: Changing over time (the population of a city that experiences births, deaths, and migration daily)\n\n\nSample\nA sample is a subset of the population that is actually observed or measured. We study samples because examining entire populations is often impossible, impractical, or unnecessary.\nWhy We Use Samples:\nPractical Impossibility: Imagine testing every person in China for a disease. By the time you finished testing 1.4 billion people, the disease situation would have changed completely, and some people tested early would need retesting.\nCost Considerations: The 2020 U.S. Census cost approximately $16 billion. Conducting such complete enumerations frequently would be prohibitively expensive. A well-designed sample survey can provide accurate estimates at a fraction of the cost.\nTime Constraints: Policy makers often need information quickly. A sample survey of 10,000 people can be completed in weeks, while a census takes years to plan, execute, and process.\nDestructive Measurement: Some measurements destroy what’s being measured. Testing the lifespan of light bulbs or the breaking point of materials requires using samples.\nGreater Accuracy: Surprisingly, samples can sometimes be more accurate than complete enumerations. With a sample, you can afford better training for interviewers, more careful data collection, and more thorough quality checks.\nExample of Sample vs. Population:\nLet’s say we want to know the average household size in New York City:\n\nPopulation: All 3.2 million households in NYC\nCensus approach: Attempt to contact every household (expensive, time-consuming, some will be missed)\nSample approach: Randomly select 5,000 households, carefully measure their sizes, and use this to estimate the average for all households\nResult: The sample might find an average of 2.43 people per household with a margin of error of ±0.05, meaning we’re confident the true population average is between 2.38 and 2.48\n\n\n\n\n\n\n\n\nSampling Methods Overview\n\n\n\nSampling selects a subset from a population to estimate characteristics. The sampling frame (the list from which we sample) must ideally include every member exactly once. Frame problems include undercoverage, overcoverage, duplication, and clustering.\n\nProbability Sampling (Statistical Inference Possible)\n\nSimple Random Sampling (SRS): Every sample of size n has equal probability. Gold standard but often impractical. Each unit has probability n/N of selection.\nSystematic Sampling: Select every kth element where k = N/n. Simple to implement but watch for hidden periodicity in the frame.\nStratified Sampling: Divide population into homogeneous strata, sample independently within each. Ensures subgroup representation and can increase precision substantially. Allocation types: proportional, optimal (Neyman), or equal.\nCluster Sampling: Select groups (clusters) rather than individuals. Cost-effective for geographically dispersed populations but less efficient than SRS (design effect: DEFF = Variance(cluster)/Variance(SRS)). Can be single-stage or multi-stage.\n\n\n\nNon-Probability Sampling (Limited Statistical Inference)\n\nConvenience: Selection by ease of access. Useful for pilots/exploratory work but severe selection bias likely.\nPurposive/Judgmental: Deliberate selection of typical, extreme, or information-rich cases. Valuable for qualitative research and rare populations.\nQuota: Match population proportions but without random selection. Fast and cheap but hidden selection bias and no measure of sampling error. (See: 1948 Dewey-Truman polling failure)\nSnowball: Participants recruit others from their networks. Essential for hidden populations (drug users, undocumented immigrants) but biased toward well-connected individuals.\n\nKey Principle: Probability sampling enables valid statistical inference; non-probability methods may be necessary for practical or ethical reasons but limit generalizability.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#superpopulation-and-data-generating-process-dgp",
    "href": "chapter1.html#superpopulation-and-data-generating-process-dgp",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.5 Superpopulation and Data Generating Process (DGP) (*)",
    "text": "1.5 Superpopulation and Data Generating Process (DGP) (*)\n\n\nSuperpopulation\nA superpopulation is a theoretical infinite population from which your finite population is considered to be one random sample.\nThink of it in three levels:\n\nSuperpopulation: An infinite collection of possible values (theoretical)\nFinite population: The actual population you could theoretically census (e.g., all 50 US states, all 10,000 firms in an industry)\nSample: The subset you actually observe (e.g., 30 states, 500 firms)\n\nWhy do we need this concept?\nConsider the 50 US states. You might measure unemployment rate for all 50 states—a complete census, no sampling needed. But you still want to:\n\nTest if unemployment is related to education levels\nPredict next year’s unemployment rates\nDetermine if differences between states are “statistically significant”\n\nWithout the superpopulation concept, you’re stuck—you have all the data, so what’s left to infer? The answer: treat this year’s 50 values as one draw from an infinite superpopulation of possible values that could occur under similar conditions.\nMathematical representation:\n\nFinite population value: Y_i (state i’s unemployment rate)\nSuperpopulation model: Y_i = \\mu + \\epsilon_i where \\epsilon_i \\sim (0, \\sigma^2)\nThe 50 observed values are one realization of this process\n\n\n\n\nData Generating Process: The True Recipe\nThe Data Generating Process (DGP) is the actual mechanism that creates your data—including all factors, relationships, and random elements.\nAn intuitive example: Suppose student test scores are truly generated by:\n\\text{Score}_i = 50 + 2(\\text{StudyHours}_i) + 3(\\text{SleepHours}_i) - 5(\\text{Stress}_i) + 1.5(\\text{Breakfast}_i) + \\epsilon_i\nThis is the TRUE DGP. But you don’t know this! You might estimate:\n\\text{Score}_i = \\alpha + \\beta(\\text{StudyHours}_i) + u_i\nYour model is simpler than reality. You’re missing variables (sleep, stress, breakfast), so your estimates might be biased. The u_i term captures everything you missed.\nKey insight: We never know the true DGP. Our statistical models are always approximations, trying to capture the most important parts of the unknown, complex truth.\n\n\n\nTwo Approaches to Statistical Inference\nWhen analyzing data, especially from surveys or samples, we can take two philosophical approaches:\n\n1. Design-Based Inference\n\nPhilosophy: The population values are fixed numbers. Randomness comes ONLY from which units we happened to sample.\nFocus: How we selected the sample (simple random, stratified, cluster sampling, etc.)\nExample: The mean income of California counties is a fixed number. We sample 10 counties. Our uncertainty comes from which 10 we randomly selected.\nNo models needed: We don’t assume anything about the population values’ distribution\n\n\n\n2. Model-Based Inference\n\nPhilosophy: The population values themselves are realizations from some probability model (superpopulation)\nFocus: The statistical model generating the population values\nExample: Each California county’s income is drawn from: Y_i = \\mu + \\epsilon_i where \\epsilon_i \\sim N(0, \\sigma^2)\nModels required: We make assumptions about how the data were generated\n\nWhich is better?\n\nLarge populations, good random samples: Design-based works well\nSmall populations (like 50 states): Model-based often necessary\nComplete enumeration: Only model-based allows inference\nModern practice: Often combines both approaches\n\n\n\n\n\nPractical Example: Analyzing State Education Spending\nSuppose you collect education spending per pupil for all 50 US states.\nWithout superpopulation thinking:\n\nYou have all 50 values—that’s it\nThe mean is the mean, no uncertainty\nYou can’t test hypotheses or make predictions\n\nWith superpopulation thinking:\n\nThis year’s 50 values are one realization from a superpopulation\nModel: \\text{Spending}_i = \\mu + \\beta(\\text{StateIncome}_i) + \\epsilon_i\nNow you can:\n\nTest if spending relates to state income (\\beta \\neq 0?)\nPredict next year’s values\nCalculate confidence intervals\n\n\nThe key insight: Even with complete data, the superpopulation framework enables statistical inference by treating observed values as one possible outcome from an underlying stochastic process.\n\n\n\nSummary\n\nSuperpopulation: Treats your finite population as one draw from an infinite possibility space—essential when your finite population is small or completely observed\nDGP: The true (unknown) process creating your data—your models try to approximate it",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#data-and-distributions",
    "href": "chapter1.html#data-and-distributions",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.6 Data and Distributions",
    "text": "1.6 Data and Distributions\nData: Information collected during research – this includes survey responses, experimental results, economic indicators, social media content, or any other measurable observations.\nUnderstanding data types and distributions is fundamental to choosing appropriate analyses and interpreting results correctly.\n\n\n\n\n\n\nUnderstanding Different Types of Data Sets and Their Formats\n\n\n\n\nCross-sectional Data\nObservations for variables (columns in a database) collected at a single point in time across multiple entities/individuals:\n\n\n\nIndividual\nAge\nIncome\nEducation\n\n\n\n\n1\n25\n50000\nBachelor’s\n\n\n2\n35\n75000\nMaster’s\n\n\n3\n45\n90000\nPhD\n\n\n\n\n\nTime Series Data\nObservations of a single entity tracked over multiple time points:\n\n\n\nYear\nGDP (in billions)\nUnemployment Rate\n\n\n\n\n2018\n20,580\n3.9%\n\n\n2019\n21,433\n3.7%\n\n\n2020\n20,933\n8.1%\n\n\n\n\n\nPanel Data (Longitudinal Data)\nObservations of multiple entities tracked over time:\n\n\n\nCountry\nYear\nGDP per capita\nLife Expectancy\n\n\n\n\nUSA\n2018\n62,794\n78.7\n\n\nUSA\n2019\n65,118\n78.8\n\n\nCanada\n2018\n46,194\n81.9\n\n\nCanada\n2019\n46,194\n82.0\n\n\n\n\n\nTime-series Cross-sectional (TSCS) Data\nA special case of panel data where:\n\nNumber of time points &gt; Number of entities\nSimilar structure to panel data but with emphasis on temporal depth\nCommon in political science and economics research\n\n\n\n\nData Formats\n\nWide Format\nEach row represents an entity; columns represent variables/time points:\n\n\n\nCountry\nGDP_2018\nGDP_2019\nLE_2018\nLE_2019\n\n\n\n\nUSA\n62,794\n65,118\n78.7\n78.8\n\n\nCanada\n46,194\n46,194\n81.9\n82.0\n\n\n\n\n\nLong Format\nEach row represents a unique entity-time-variable combination:\n\n\n\nCountry\nYear\nVariable\nValue\n\n\n\n\nUSA\n2018\nGDP per capita\n62,794\n\n\nUSA\n2019\nGDP per capita\n65,118\n\n\nUSA\n2018\nLife Expectancy\n78.7\n\n\nUSA\n2019\nLife Expectancy\n78.8\n\n\nCanada\n2018\nGDP per capita\n46,194\n\n\nCanada\n2019\nGDP per capita\n46,194\n\n\nCanada\n2018\nLife Expectancy\n81.9\n\n\nCanada\n2019\nLife Expectancy\n82.0\n\n\n\nNote: Long format is generally preferred for:\n\nData manipulation in R and Python\nStatistical analysis\nData visualization\n\n\n\n\n\n\n\nTypes of Data\nData consists of collected observations or measurements. The type of data determines what mathematical operations (e.g. multiplication) are meaningful and what statistical methods apply.\n\nQuantitative Data\nContinuous Data can take any value within a range:\nExamples with Demographic Relevance:\n\nAge: Can be 25.5 years, 25.51 years, 25.514 years (precision limited only by measurement)\nBody Mass Index: 23.7 kg/m²\nFertility Rate: 1.73 children per woman\nPopulation Density: 4,521.3 people per km²\n\nProperties:\n\nCan perform all arithmetic operations\nCan calculate means, standard deviations\nOften follow known probability distributions (e.g. weight and Normal distribution)\n\nDiscrete Data can only take specific values:\nExamples:\n\nNumber of Children: 0, 1, 2, 3… (can’t have 2.5 children)\nNumber of Marriages: 0, 1, 2, 3…\nHousehold Size: 1, 2, 3, 4… people\nNumber of Doctor Visits: 0, 1, 2, 3… per year\n\n\n\nQualitative/Categorical Data\nNominal Data represents categories with no inherent order:\nExamples:\n\nCountry of Birth: USA, China, India, Brazil…\nReligion: Christian, Muslim, Hindu, Buddhist, None…\nMarital Status: Single, Married, Divorced, Widowed\nCause of Death: Heart disease, Cancer, Accident, Stroke…\nBlood Type: A, B, AB, O\n\nWhat We Can Do:\n\nCount frequencies\nCalculate proportions\nFind mode\nTest for independence\n\nWhat We Cannot Do:\n\nCalculate mean (average religion makes no sense)\nOrder categories meaningfully\nCompute distances between categories\n\nOrdinal Data represents ordered categories:\nExamples:\n\nEducation Level: None &lt; Primary &lt; Secondary &lt; Tertiary\nSocioeconomic Status: Low &lt; Middle &lt; High\nSelf-Rated Health: Poor &lt; Fair &lt; Good &lt; Excellent\nAgreement Scale: Strongly Disagree &lt; Disagree &lt; Neutral &lt; Agree &lt; Strongly Agree\n\nThe Challenge: Intervals between categories aren’t necessarily equal. The “distance” from Poor to Fair health may not equal the distance from Good to Excellent.\n\n\n\nData Distribution\nA data distribution describes how values spread across possible outcomes (what values and how often a variable takes). Distributions tell us what values are common, what values are rare, and what patterns exist in our data.\nUnderstanding distributions is fundamental to statistics because it helps us summarize large datasets, identify patterns, and make informed decisions.\nFor example, knowing that most students score between 60-80 on an exam tells us more than just knowing the average score.\n\nFrequency, Relative Frequency, and Density\nWhen we analyze data, we’re often interested in how many times each value (or range of values) appears. This leads us to three related concepts:\n(Absolute) Frequency is simply the count of how many times a particular value or category occurs in your data. If 15 students scored between 70-80 points on an exam, the frequency for that range is 15.\nRelative frequency expresses frequency as a proportion or percentage of the total. It answers the question: “What fraction of all observations fall into this category?” Relative frequency is calculated as:\n\\text{Relative Frequency} = \\frac{\\text{Frequency}}{\\text{Total Number of Observations}}\nIf 15 out of 100 students scored 70-80 points, the relative frequency is 15/100 = 0.15 or 15%. Relative frequencies always sum to 1 (or 100%), making them useful for comparing distributions with different sample sizes.\nDensity (the degree of compactness of something, the probability per unit length) is similar to relative frequency but accounts for the width of intervals. When we group continuous data (like time or unemployment rate) into bins of different widths, density ensures fair comparison. Density is calculated as:\n\\text{Density} = \\frac{\\text{Relative Frequency}}{\\text{Interval Width}}\nDensity is particularly important for continuous variables because it ensures that the total area under the distribution equals 1, which allows us to interpret areas as probabilities.\n\n\n\n\n\n\nTip\n\n\n\nThe probability of an event is a number between 0 and 1; the larger the probability, the more likely an event is to occur.\n\n\nCumulative frequency tells us how many observations fall at or below a certain value. Instead of asking “how many observations are in this category?”, cumulative frequency answers “how many observations are in this category or any category below it?” It’s calculated by adding up all frequencies from the lowest value up to and including the current value.\nSimilarly, cumulative relative frequency expresses this as a proportion of the total, answering “what percentage of observations fall at or below this value?” For example, if the cumulative relative frequency at score 70 is 0.40, this means 40% of students scored 70 or below.\n\n\nDistribution Tables\nA frequency distribution table organizes data by showing how observations are distributed across different values or intervals. Here’s an example with exam scores:\n\n\n\n\n\n\n\n\n\n\n\nScore Range\nFrequency\nRelative Frequency\nCumulative Frequency\nCumulative Relative Frequency\nDensity\n\n\n\n\n0-50\n10\n0.10\n10\n0.10\n0.002\n\n\n50-70\n30\n0.30\n40\n0.40\n0.015\n\n\n70-90\n45\n0.45\n85\n0.85\n0.0225\n\n\n90-100\n15\n0.15\n100\n1.00\n0.015\n\n\nTotal\n100\n1.00\n-\n-\n-\n\n\n\nThis table reveals that most students scored in the 70-90 range, while very few scored below 50 or above 90. The cumulative columns show us that 40% of students scored below 70, and 85% scored below 90. Such tables are invaluable for getting a quick overview of your data before conducting more complex analyses.\n\n\nVisualizing Distributions: Histograms\nA histogram is a graphical representation of a frequency distribution. It displays data using bars where:\n\nThe x-axis shows the values or intervals (bins)\nThe y-axis can show frequency, relative frequency, or density\nThe height of each bar represents the count, proportion, or density for that interval\nBars touch each other (no gaps) for continuous variables\n\nChoosing bin widths: The number and width of bins significantly affects how your histogram looks. Too few bins hide important patterns, while too many bins create “noise” and make patterns hard to see.\n\nIn statistics, noise is unwanted random variation that obscures the pattern we’re trying to find. Think of it like static on a radio—it makes the music (the “signal”) harder to hear. In data, noise comes from measurement errors, random fluctuations, or the inherent variability in what we’re studying. Noise is random variation in data that hides the real patterns we want to see, similar to how background noise makes conversation difficult to hear.\n\nSeveral approaches help determine appropriate bin widths:\n\nSturges’ rule: Use k = 1 + \\log_2(n) bins, where n is the sample size. This works well for roughly symmetric distributions.\nSquare root rule: Use k = \\sqrt{n} bins. A simple, reasonable default for many situations.\n\nIn R, you can specify bins in several ways:\n\n# Specify number of bins\nhist(exam_scores, breaks = 10)\n\n\n\n\n\n\n\n# Specify exact break points\nhist(exam_scores, breaks = seq(0, 100, by = 10))\n\n\n\n\n\n\n\n# Let R choose automatically (uses Sturges' rule by default)\nhist(exam_scores)\n\n\n\n\n\n\n\n\nThe best approach is often to experiment with different bin widths to find what best reveals your data’s pattern. Start with a default, then try fewer and more bins to see how the story changes.\nDefining bin boundaries: When creating bins for a frequency table, you must decide how to handle values that fall exactly on the boundaries. For example, if you have bins 0-10 and 10-20, which bin does the value 10 belong to?\nThe solution is to use interval notation to specify whether each boundary is included or excluded:\n\nClosed interval [a, b] includes both endpoints: a \\leq x \\leq b\nOpen interval (a, b) excludes both endpoints: a &lt; x &lt; b\nHalf-open interval [a, b) includes the left endpoint but excludes the right: a \\leq x &lt; b\nHalf-open interval (a, b] excludes the left endpoint but includes the right: a &lt; x \\leq b\n\nStandard convention: Most statistical software, including R, uses left-closed, right-open intervals [a, b) for all bins except the last one, which is fully closed [a, b]. This means:\n\nThe value at the lower boundary is included in the bin\nThe value at the upper boundary belongs to the next bin\nThe very last bin includes both boundaries to capture the maximum value\n\nFor example, with bins 0-20, 20-40, 40-60, 60-80, 80-100:\n\n\n\nScore Range\nInterval Notation\nValues Included\n\n\n\n\n0-20\n[0, 20)\n0 ≤ score &lt; 20\n\n\n20-40\n[20, 40)\n20 ≤ score &lt; 40\n\n\n40-60\n[40, 60)\n40 ≤ score &lt; 60\n\n\n60-80\n[60, 80)\n60 ≤ score &lt; 80\n\n\n80-100\n[80, 100]\n80 ≤ score ≤ 100\n\n\n\nThis convention ensures that:\n\nEvery value is counted exactly once (no double-counting)\nNo values fall through the cracks\nThe bins partition the entire range completely\n\nWhen presenting frequency tables in reports, you can simply write “0-20, 20-40, …” and note that bins are left-closed, right-open, or explicitly show the interval notation if precision is important.\nFrequency histogram shows the raw counts:\n\n# R code example\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     main = \"Distribution of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Frequency\",\n     col = \"lightblue\")\n\n\n\n\n\n\n\n\nRelative frequency histogram shows proportions (useful when comparing groups of different sizes):\n\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     freq = FALSE,  # This creates relative frequency/density\n     main = \"Distribution of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Relative Frequency\",\n     col = \"lightgreen\")\n\n\n\n\n\n\n\n\nDensity histogram adjusts for interval width and is used with density curves:\n\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     freq = FALSE,  # Creates density scale\n     main = \"Distribution of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Density\",\n     col = \"lightcoral\")\n\n\n\n\n\n\n\n\n\n\nDensity Curves\nA density curve is a smooth line that approximates/models the shape of a distribution. Unlike histograms that show actual data in discrete bins, density curves show the overall pattern as a continuous function. The area under the entire curve always equals 1, and the area under any portion of the curve represents the proportion of observations in that range.\n\n# Adding a density curve to a histogram\nhist(exam_scores, \n     freq = FALSE,\n     main = \"Exam Scores with Density Curve\",\n     xlab = \"Score\",\n     ylab = \"Density\",\n     col = \"lightblue\",\n     border = \"white\")\nlines(density(exam_scores), \n      col = \"darkred\", \n      lwd = 2)\n\n\n\n\n\n\n\n\nDensity curves are particularly useful for:\n\nIdentifying the shape of the distribution (symmetric, skewed, bimodal)\nComparing multiple distributions on the same plot\nUnderstanding the theoretical (true) distribution underlying your data\n\n\n\n\n\n\n\nTip\n\n\n\nIn statistics, a percentile indicates the relative position of a data point within a dataset by showing the percentage of observations that fall at or below that value. For example, if a student scores at the 90th percentile on a test, their score is equal to or higher than 90% of all other scores.\nQuartiles are special percentiles that divide data into four equal parts: the first quartile (Q1, 25th percentile), second quartile (Q2, 50th percentile, also the median), and third quartile (Q3, 75th percentile). If Q1 = 65 points, then 25% of students scored 65 or below.\nMore generally, quantiles are values that divide data into equal-sized groups—percentiles divide into 100 parts, quartiles into 4 parts, deciles into 10 parts, and so on.\n\n\n\n\nVisualizing Cumulative Frequency (*)\nCumulative frequency plots, also called ogives (pronounced “oh-jive”), display how frequencies accumulate across values. These plots use lines rather than bars and always increase from left to right, eventually reaching the total number of observations (for cumulative frequency) or 1.0 (for cumulative relative frequency).\nCumulative frequency plots are excellent for:\n\nFinding percentiles and quartiles visually\nDetermining what proportion of data falls below or above a certain value\nComparing distributions of different groups\n\n\n# Creating cumulative frequency data\nscore_breaks &lt;- seq(0, 100, by = 10)\nfreq_counts &lt;- hist(exam_scores, breaks = score_breaks, plot = FALSE)$counts\ncumulative_freq &lt;- cumsum(freq_counts)\n\n# Plotting cumulative frequency\nplot(score_breaks[-1], cumulative_freq,\n     type = \"b\",  # both points and lines\n     main = \"Cumulative Frequency of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Cumulative Frequency\",\n     col = \"darkblue\",\n     lwd = 2,\n     pch = 19)\ngrid()\n\n\n\n\n\n\n\n\nFor cumulative relative frequency (which is more commonly used):\n\n# Cumulative relative frequency\ncumulative_rel_freq &lt;- cumulative_freq / length(exam_scores)\n\nplot(score_breaks[-1], cumulative_rel_freq,\n     type = \"b\",\n     main = \"Cumulative Relative Frequency of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Cumulative Relative Frequency\",\n     col = \"darkred\",\n     lwd = 2,\n     pch = 19,\n     ylim = c(0, 1))\ngrid()\nabline(h = c(0.25, 0.5, 0.75), lty = 2, col = \"gray\")  # Quartile lines\n\n\n\n\n\n\n\n\nThe cumulative relative frequency curve makes it easy to read percentiles. For example, if you draw a horizontal line at 0.75 and see where it intersects the curve, the corresponding x-value is the 75th percentile—the score below which 75% of students fall.\n\n\n\nDiscrete vs. Continuous Distributions\nThe type of variable you’re analyzing determines how you visualize its distribution:\nDiscrete distributions apply to variables that can only take specific, countable values. Examples include number of children in a family (0, 1, 2, 3…), number of customer complaints per day, or responses on a 5-point Likert scale.\nFor discrete data, we typically use:\n\nBar charts (with gaps between bars) rather than histograms\nFrequency or relative frequency on the y-axis\nEach distinct value gets its own bar\n\n\n# Example: Number of children per family\nchildren &lt;- c(0, 1, 2, 2, 1, 3, 0, 2, 1, 4, 2, 1, 0, 2, 3)\nbarplot(table(children),\n        main = \"Distribution of Number of Children\",\n        xlab = \"Number of Children\",\n        ylab = \"Frequency\",\n        col = \"skyblue\")\n\n\n\n\n\n\n\n\nContinuous distributions apply to variables that can take any value within a range. Examples include temperature, response time, height, or turnout percentage.\nFor continuous data, we use:\n\nHistograms (with touching bars) that group data into intervals\nDensity curves to show the smooth pattern\nDensity on the y-axis when using density curves\n\n\n# Example: Response time distribution\nhist(response_time, \n     breaks = 15,\n     freq = FALSE,\n     main = \"Distribution of Response Time\",\n     xlab = \"Response Time (seconds)\",\n     ylab = \"Density\",\n     col = \"lightgreen\",\n     border = \"white\")\nlines(density(response_time), \n      col = \"darkgreen\", \n      lwd = 2)\n\n\n\n\n\n\n\n\nThe key difference is that discrete distributions show probability at specific points, while continuous distributions show probability density across ranges. For continuous variables, the probability of any exact value is essentially zero—instead, we talk about the probability of falling within an interval.\nUnderstanding whether your variable is discrete or continuous guides your choice of visualization and statistical methods, ensuring your analysis accurately represents the nature of your data.\n\n\nDescribing Distributions\nShape Characteristics:\nSymmetry vs. Skewness:\n\nSymmetric: Mirror image around center (example: heights in homogeneous population)\nRight-skewed (positive skew): Long tail to right (example: income, wealth)\nLeft-skewed (negative skew): Long tail to left (example: age at death in developed countries)\n\nExample of Skewness Impact:\nIncome distribution in the U.S.:\n\nMedian household income: ~$70,000\nMean household income: ~$100,000\nMean &gt; Median indicates right skew\nA few very high incomes pull the mean up\n\n\nModality:\n\nUnimodal: One peak (example: test scores)\nBimodal: Two peaks (example: height when mixing males and females)\nMultimodal: Multiple peaks (example: age distribution in a college town—peaks at college age and middle age)\n\n\n\n\n\n\n\n\n\n\nImportant Probability Distributions:\nNormal (Gaussian) Distribution:\n\nBell-shaped, symmetric\nCharacterized by mean (\\mu) and standard deviation (\\sigma)\nAbout 68% of values within \\mu \\pm \\sigma\nAbout 95% within \\mu \\pm 2\\sigma\nAbout 99.7% within \\mu \\pm 3\\sigma\n\nDemographic Applications:\n\nHeights within homogeneous populations\nMeasurement errors\nSampling distributions of means (Central Limit Theorem)\n\nBinomial Distribution:\n\nNumber of successes in n independent trials\nEach trial has probability p of success\nMean = np, Variance = np(1-p)\n\nExample: Number of male births out of 100 births (p \\approx 0.512)\nPoisson Distribution:\n\nCount of events in fixed time/space\nMean = Variance = \\lambda\nGood for rare events\n\nDemographic Applications:\n\nNumber of deaths per day in small town\nNumber of births per hour in hospital\nNumber of accidents at intersection per month\n\n\n\n\nVisualizing Frequency Distributions (*)\nHistogram: For continuous data, shows frequency with bar heights.\n\nX-axis: Value ranges (bins)\nY-axis: Frequency or density\nNo gaps between bars (continuous data)\nBin width affects appearance\n\nBar Chart: For categorical data, shows frequency with separated bars.\n\nX-axis: Categories\nY-axis: Frequency\nGaps between bars (discrete categories)\nOrder may or may not matter\n\nCumulative Distribution Function (CDF): Shows proportion of values ≤ each point of data.\n\nAlways increases (or stays flat)\nStarts at 0, ends at 1\nSteep slopes indicate common values\nFlat areas indicate rare values\n\nBox Plot (Box-and-Whisker Plot): A visual summary that displays the distribution’s key statistics using five key values.\nThe Five-Number Summary:\n\nMinimum: Leftmost whisker end (excluding outliers)\nQ1 (First Quartile): Left edge of the box (25th percentile)\nMedian (Q2): Line inside the box (50th percentile)\n\nQ3 (Third Quartile): Right edge of the box (75th percentile)\nMaximum: Rightmost whisker end (excluding outliers)\n\nWhat It Reveals:\n\nSkewness: If median line is off-center in the box, or whiskers are unequal\nSpread: Wider boxes and longer whiskers indicate more variability\nOutliers: Immediately visible as separate points\nSymmetry: Equal whisker lengths and centered median suggest normal distribution\n\nQuick Interpretation:\n\nNarrow box = consistent data\nLong whiskers = wide range of values\n\nMany outliers = potential data quality issues or interesting extreme cases\nMedian closer to Q1 = right-skewed data (tail extends right)\nMedian closer to Q3 = left-skewed data (tail extends left)\n\nBox plots are especially useful for comparing multiple groups side-by-side!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#variables-and-measurement-scales",
    "href": "chapter1.html#variables-and-measurement-scales",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.7 Variables and Measurement Scales",
    "text": "1.7 Variables and Measurement Scales\n\nA variable is any characteristic that can take different values across units of observation.\n\n\nMeasurement: Transforming Concepts into Numbers\n\nThe Political World is Full of Data\nPolitical science has evolved from a primarily theoretical discipline to one that increasingly relies on empirical evidence. Whether we’re studying:\n\nElection outcomes: Why do people vote the way they do?\nPublic opinion: What shapes attitudes toward immigration or climate policy?\nInternational relations: What factors predict conflict between nations?\nPolicy effectiveness: Did a new education policy actually improve outcomes?\n\nWe need systematic ways to analyze data and draw conclusions that go beyond anecdotes and personal impressions.\n\nConsider this question: “Does democracy lead to economic growth?”\n\nYour intuition might suggest yes—democratic countries tend to be wealthier. But is this causation or correlation? Are there exceptions? How confident can we be in our conclusions?\nStatistics provides the tools to move from hunches to evidence-based answers, helping us distinguish between what seems true and what actually is true.\n\n\nThe Challenge of Measurement in Social Sciences\nIn social sciences, we often struggle with the fact that key concepts do not translate directly into numbers:\n\nHow do we measure “democracy”?\nWhat number captures “political ideology”?\nHow do we quantify “institutional strength”?\nHow do we measure “political participation”?\n\n\n\n\n\n\n\n\n🔍 Correlation ≠ Causation: Understanding Spurious Relationships\n\n\n\n\nThe Fundamental Distinction\nCorrelation measures how two variables move together:\n\nPositive: Both increase together (study hours ↑, grades ↑)\nNegative: One increases while other decreases (TV hours ↑, grades ↓)\nMeasured by correlation coefficient: r \\in [-1, 1]\n\nCausation means one variable directly influences another:\n\nX \\rightarrow Y: Changes in X directly cause changes in Y\nRequires: (1) correlation, (2) temporal precedence, (3) no alternative explanations\n\n\n\nThe Danger: Spurious Correlation\nA spurious correlation occurs when two variables appear related but are actually both influenced by a third variable (a confounder).\nClassic Example:\n\nObserved: Ice cream sales correlate with drowning deaths\nSpurious conclusion: Ice cream causes drowning (❌)\nReality: Summer weather (confounder) causes both:\nSummer → More ice cream sales\nSummer → More swimming → More drownings\n\nMathematical representation:\n\nObserved correlation: \\text{Cor}(X,Y) \\neq 0\nBut the true model: X = \\alpha Z + \\epsilon_1 and Y = \\beta Z + \\epsilon_2\nWhere Z is the confounding variable causing both\n\n\n\nConfounding: The Hidden Influence\nA confounding variable (confounder):\n\nAffects both the presumed cause and effect\nCreates an illusion of direct causation 3. Must be controlled for valid causal inference\n\nResearch Example:\n\nObserved: Coffee consumption correlates with heart disease\nPotential confounder: Smoking (coffee drinkers more likely to smoke)\nTrue relationships:\nSmoking → Heart disease (causal)\nSmoking → Coffee consumption (association)\nCoffee → Heart disease (spurious without controlling for smoking)\n\n\n\nHow to Identify Causal Relationships\n\nRandomized Controlled Trials (RCTs): Random assignment breaks confounding\nNatural Experiments: External events create “as-if” random variation\nStatistical Control: Include confounders in regression models\nInstrumental Variables: Find variables affecting X but not Y directly\n\n\n\nKey Takeaway\nFinding correlation is easy. Establishing causation is hard. Always ask: “What else could explain this relationship?”\nRemember: The most dangerous phrase in empirical research is “our data shows that X causes Y” when all you’ve measured is correlation.\n\n\n\n\n\n\n\n\n\n\n📊 Quick Test: Correlation or Causation?\n\n\n\n\n\nFor each scenario, identify whether the relationship is likely causal or spurious:\n\nCities with more churches have more crime\n\nAnswer: Spurious (confounder: population size)\n\nSmoking leads to lung cancer\n\nAnswer: Causal (established through multiple study designs)\n\nStudents with more books at home get better grades\n\nAnswer: Likely spurious (confounders: parental education, income)\n\nCountries with higher chocolate consumption have more Nobel laureates\n\nAnswer: Spurious (confounder: wealth/development level)\n\n\n\n\n\n\n\n\n\nTypes of Variables\nQuantitative Variables represent amounts or quantities and can be:\nContinuous Variables: Can take any value within a range, limited only by measurement precision.\n\nAge (22.5 years, 22.51 years, 22.514 years…)\nIncome ($45,234.67)\nHeight (175.3 cm)\nPopulation density (432.7 people per square kilometer)\n\nDiscrete Variables: Can only take specific values, usually counts.\n\nNumber of children in a family (0, 1, 2, 3…)\nNumber of marriages (0, 1, 2…)\nNumber of rooms in a dwelling (1, 2, 3…)\nNumber of migrants entering a country per year\n\nQualitative Variables represent categories or qualities and can be:\nNominal Variables: Categories with no inherent order.\n\nCountry of birth (USA, Mexico, Canada…)\nReligion (Christian, Muslim, Hindu, Buddhist…)\nBlood type (A, B, AB, O)\nCause of death (heart disease, cancer, accident…)\n\nOrdinal Variables: Categories with a meaningful order but unequal intervals.\n\nEducation level (no schooling, primary, secondary, tertiary)\nSatisfaction with healthcare (very dissatisfied, dissatisfied, neutral, satisfied, very satisfied)\nSocioeconomic status (low, middle, high)\nSelf-rated health (poor, fair, good, excellent)\n\n\n\nMeasurement Scales\nUnderstanding measurement scales is crucial because they determine which statistical methods are appropriate:\nNominal Scale: Categories only—we can count frequencies but cannot order or perform arithmetic. Example: We can say 45% of residents were born locally, but we cannot calculate an “average birthplace.”\nOrdinal Scale: Order matters but differences between values are not necessarily equal. Example: The difference between “poor” and “fair” health may not equal the difference between “good” and “excellent” health.\nInterval Scale: Equal intervals between values but no true zero point. Example: Temperature in Celsius—the difference between 20°C and 30°C equals the difference between 30°C and 40°C, but 0°C doesn’t mean “no temperature.”\nRatio Scale: Equal intervals with a true zero point, allowing all mathematical operations. Example: Income—$40,000 is twice as much as $20,000, and $0 means no income.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#parameters-statistics-and-estimation",
    "href": "chapter1.html#parameters-statistics-and-estimation",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.8 Parameters, Statistics, and Estimation",
    "text": "1.8 Parameters, Statistics, and Estimation\nThese concepts form the core of statistical inference—how we learn about populations from samples. Understanding the relationships between these terms is essential for proper statistical reasoning.\n\nParameter\nA parameter is a numerical characteristic of a population. Parameters are typically unknown because we cannot measure the entire population. They are fixed values (not random) but unknown to us. We denote parameters with Greek letters.\nCommon Demographic Parameters:\n\n\\mu (mu): Population mean age. For example, the true average age of all Europeans.\n\\sigma^2 (sigma squared): Population variance in income across all households in Brazil.\np: Population proportion. For example, the true proportion of all adults in Japan who are married.\n\\beta (beta): Regression coefficient. The true relationship between education and fertility in a population.\n\\lambda (lambda): Rate parameter. The true rate of migration from rural to urban areas.\n\nExample: The true mean age at first birth for all women in France who gave birth in 2023 is a parameter. Let’s call it \\mu = 31.2 years. We don’t know this value without measuring every single birth.\n\n\nStatistic\nA statistic is a numerical characteristic calculated from sample data. Statistics are random variables—their values vary from sample to sample. We use Roman letters for statistics.\nCommon Sample Statistics:\n\n\\bar{x} (x-bar): Sample mean age from a survey of 1,000 people\ns^2: Sample variance in income from 500 surveyed households\n\\hat{p} (p-hat): Sample proportion married from a survey\nr: Sample correlation between education and income\nb: Sample regression coefficient\n\nExample: From a sample of 500 births in France, we calculate a sample mean age at first birth of \\bar{x} = 30.9 years. This is our statistic. A different sample might yield \\bar{x} = 31.4 years.\n\n\nThe Relationship Between Parameters and Statistics\nThink of this relationship like trying to understand the depth of a lake:\n\nParameter: The true average depth of the lake (unknown, fixed)\nStatistic: The average depth from several measurement points (known, varies with different samples)\nEstimation: Using our measurements to guess the true average depth\n\n\n\nEstimator\nAn estimator is a rule or formula for calculating an estimate of a population parameter from sample data. An estimator is a function that maps sample data to parameter estimates. It’s the recipe, not the cake.\nProperties of Good Estimators:\nUnbiasedness: On average, the estimator equals the true parameter value. If we repeated sampling many times, the average of all our estimates would equal the true parameter.\nExample: The sample mean \\bar{x} is an unbiased estimator of population mean \\mu. If we took 1,000 different samples and calculated 1,000 sample means, their average would be very close to \\mu.\nConsistency: As sample size increases, the estimator converges to the true parameter value.\nExample: With n=10, our estimate of average income might be off by $5,000. With n=1,000, we might be off by only $500. With n=100,000, we might be off by only $50.\nEfficiency: Among unbiased estimators, the one with the smallest variance. The sample mean is more efficient than the sample median for estimating the population mean of a normal distribution.\nCommon Estimators:\n\nSample mean as estimator of population mean: \\bar{x} = \\frac{\\sum x_i}{n}\nSample proportion as estimator of population proportion: \\hat{p} = \\frac{x}{n} (where x is the count of successes)\nSample variance as estimator of population variance: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\n\nNote: We divide by (n-1) not n for sample variance to make it unbiased—this is called Bessel’s correction.\n\n\nEstimand\nThe estimand is the specific population parameter we aim to estimate. It’s the target of our estimation procedure. Clear specification of the estimand is crucial for proper statistical inference and avoiding misinterpretation.\nExamples of Clearly Defined Estimands:\n\n“The median household income for all households in California as of January 1, 2024”\n“The difference in life expectancy between males and females born in Sweden in 2023”\n“The proportion of all adults aged 25-34 in urban areas who completed tertiary education”\n\nWhy Precise Estimand Definition Matters:\nConsider studying “unemployment rate.” The estimand must specify:\n\nWho counts as unemployed? (Actively seeking work? Discouraged workers?)\nWhat age range? (15+? 16-64?)\nWhat geographic area?\nWhat time period?\n\nDifferent definitions lead to different numbers. The U.S. Bureau of Labor Statistics publishes six different unemployment rates (U-1 through U-6) based on different definitions.\n\n\nEstimate\nAn estimate is the specific numerical value calculated by applying an estimator to observed data. It’s our best guess at the true parameter value based on available information.\nExample of the Complete Process:\n\nEstimand (target): The proportion of all U.S. adults who approve of the president’s performance\nParameter (true unknown value): p = 0.42 (42%, but we don’t know this)\nEstimator (method): Sample proportion \\hat{p} = \\frac{x}{n} where x is approvals and n is sample size\nSample: Survey 1,500 randomly selected adults, 650 approve\nEstimate (calculated value): \\hat{p} = \\frac{650}{1,500} = 0.433 (43.3%)\n\n\n\n\n\n\n\nEstimands: What Exactly Are We Trying to Estimate?\n\n\n\nAn estimand is the specific quantity we aim to estimate—what we’re targeting with our statistical analysis. While this is often a population parameter, estimands can be more complex.\nExamples of different estimands:\nSimple parameter estimand: The population mean income (\\mu)\nComparative estimand: The difference in mean income between two groups (\\mu_1 - \\mu_2)\nCausal estimand: The average treatment effect of a job training program on earnings\nConditional estimand: Expected voter turnout given specific weather conditions\n\nThe Complete Framework\nUnderstanding statistical inference requires distinguishing between these related but distinct concepts:\n\nPopulation Parameter: The true characteristic of the population (e.g., \\mu)\nEstimand: The specific quantity we want to estimate (often, but not always, a parameter)\nEstimator: The method for computing our estimate (e.g., sample mean)\n\nEstimate: The actual number we calculate from our data\n\nExample in context:\n\nParameter: True mean voter turnout in all elections (\\mu)\nEstimand: Expected turnout difference between rainy vs. sunny election days (\\mu_{\\text{rainy}} - \\mu_{\\text{sunny}})\nEstimator: Difference between sample means from rainy and sunny elections\nEstimate: 3.2 percentage points lower turnout on rainy days\n\nThis framework helps clarify exactly what question we’re answering and ensures our methods align with our research goals.\n\n\n\n\n\n\n\n\n\n\nUnderstanding Different Types of Unpredictability\n\n\n\nNot all uncertainty is the same. Understanding different sources of unpredictability helps us choose appropriate statistical methods and interpret results correctly.\n\n\n\n\n\n\n\n\n\nConcept\nWhat is it?\nSource of unpredictability\nExample\n\n\n\n\nRandomness\nIndividual outcomes are uncertain, but the probability distribution is known or modeled.\nFluctuations across realizations; lack of information about a specific outcome.\nDice roll, coin toss, polling sample\n\n\nChaos\nDeterministic dynamics highly sensitive to initial conditions (butterfly effect).\nTiny initial differences grow rapidly → large trajectory divergences.\nWeather forecasting, double pendulum, population dynamics\n\n\nEntropy\nA measure of uncertainty/dispersion (information-theoretic or thermodynamic).\nLarger when outcomes are more evenly distributed (less predictive information).\nShannon entropy in data compression\n\n\n“Haphazardness” (colloquial)\nA felt lack of order without an explicit model; a mixture of mechanisms.\nNo structured description or stable rules; overlapping processes.\nTraffic patterns, social media trends\n\n\nQuantum randomness\nA single outcome is not determined; only the distribution is specified (Born rule).\nFundamental (ontological) indeterminacy of individual measurements.\nElectron spin measurement, photon polarization\n\n\n\n\nKey Distinctions for Statistical Practice\nDeterministic chaos ≠ statistical randomness: A chaotic system is fully deterministic yet practically unpredictable due to extreme sensitivity to initial conditions. Statistical randomness, by contrast, models uncertainty via probability distributions where individual outcomes are genuinely uncertain.\nWhy this matters: In statistics, we typically model phenomena as random processes, assuming we can specify probability distributions even when individual outcomes are unpredictable. This assumption underlies most statistical inference.\n\n\nQuantum Mechanics and Fundamental Randomness\nIn the Copenhagen interpretation, randomness is fundamental (ontological): a single outcome cannot be predicted, but the probability distribution is given by the Born rule.\nThis represents true randomness at the most basic level of nature, not just our ignorance of determining factors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#statistical-error-and-uncertainty",
    "href": "chapter1.html#statistical-error-and-uncertainty",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.9 Statistical Error and Uncertainty",
    "text": "1.9 Statistical Error and Uncertainty\n\nIntroduction: Why Uncertainty Matters\nNo measurement or estimate is perfect. Understanding different types of error is crucial for interpreting results and improving study design.\n\n\n\n\n\n\nThe Central Challenge\n\n\n\nEvery time we use a sample to learn about a population, we introduce uncertainty. The key is to:\n\nQuantify this uncertainty honestly\nDistinguish between different sources of error\nCommunicate results transparently",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#statistical-error-and-uncertainty-1",
    "href": "chapter1.html#statistical-error-and-uncertainty-1",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.10 Statistical Error and Uncertainty",
    "text": "1.10 Statistical Error and Uncertainty\n\nIntroduction: Why Uncertainty Matters\nNo measurement or estimate is perfect. Understanding different types of error is crucial for interpreting results and improving study design.\n\n\n\n\n\n\nThe Central Challenge\n\n\n\nEvery time we use a sample to learn about a population, we introduce uncertainty. The key is to:\n\nQuantify this uncertainty honestly\nDistinguish between different sources of error\nCommunicate results transparently\n\n\n\n\n\n\nTypes of Error\n\nRandom Error\nRandom error represents unpredictable fluctuations that vary from observation to observation without a consistent pattern. These errors arise from various sources of natural variability in the data collection and measurement process.\n\n\n\n\n\n\nKey Characteristics\n\n\n\n\nUnpredictable Direction: Sometimes too high, sometimes too low\nNo Consistent Pattern: Varies randomly across observations\nAverages to Zero: Over many measurements, positive and negative errors cancel out\nQuantifiable: Can be estimated and reduced through appropriate methods\n\n\n\nRandom error encompasses several subtypes:\n\nSampling Error\nSampling error is the most common type of random error—it arises because we observe a sample rather than the entire population. Different random samples from the same population will yield different estimates purely by chance.\nKey properties:\n\nDecreases with sample size: \\propto 1/\\sqrt{n}\nQuantifiable using probability theory\nInevitable when working with samples\n\nExample: Internet Access Survey\nImagine surveying 100 random households about internet access:\n\n\n\n\n\n\n\n\n\nThe variation around the true value (red line) represents sampling error. With larger samples, estimates would cluster more tightly.\n\n\nMeasurement Error\nMeasurement error is random variation in the measurement process itself—even when measuring the same thing repeatedly.\nExamples:\n\nSlight variations when reading a thermometer due to parallax\nRandom fluctuations in electronic instruments\nInconsistencies in human judgment when coding qualitative data\n\nUnlike sampling error (which comes from who/what we observe), measurement error comes from how we observe.\n\n\nOther Sources of Random Error\n\nProcessing error: Random mistakes in data entry, coding, or computation\nModel specification error: When the true relationship is more complex than assumed\nTemporal variation: Natural day-to-day fluctuations in the phenomenon being measured\n\n\n\n\nSystematic Error (Bias)\nSystematic error represents consistent deviation in a particular direction. Unlike random error, it doesn’t average out with repeated sampling or measurement—it persists and pushes results consistently away from the truth.\n\nSelection BiasMeasurement BiasResponse BiasNon-response BiasSurvivorship BiasObserver/Interviewer Bias\n\n\nSampling method systematically excludes certain groups.\nExample: Phone surveys during business hours underrepresent employed people.\n\n\nMeasurement instrument consistently over/under-measures.\nExample: Scales that always read 2 pounds heavy; survey questions that lead respondents toward particular answers.\n\n\nRespondents systematically misreport.\nExample: People underreport alcohol consumption, overreport voting, or give socially desirable answers.\n\n\nNon-responders differ systematically from responders.\nExample: Very sick and very healthy people less likely to respond to health surveys, leaving only those with moderate health.\n\n\nOnly observing “survivors” of some process.\nExample: During WWII, the military analyzed returning bombers to determine where to add armor. Planes showed the most damage on wings and tail sections. Abraham Wald realized the flaw: they should armor where there weren’t bullet holes—the engine and cockpit. Planes hit in those areas never made it back to be analyzed. They were only studying the survivors.\n\n\nObservers or interviewers systematically influence results.\nExample: Interviewers unconsciously prompting certain responses or recording observations that confirm their expectations.\n\n\n\n\n\nThe Bias-Variance Decomposition\nMathematically, total error (Mean Squared Error) decomposes into:\n\\mathrm{MSE}(\\hat\\theta) = \\underbrace{\\mathrm{Var}(\\hat\\theta)}_{\\text{random error}} + \\underbrace{\\big(\\mathrm{Bias}(\\hat\\theta)\\big)^2}_{\\text{systematic error}}\n\n\n\n\n\n\nCritical Insight\n\n\n\n\nA large biased sample gives a precisely wrong answer.\n\n\nIncrease n → reduces random error (specifically sampling error)\nImprove study design → reduces systematic error\nBetter instruments → reduces measurement error\n\n\n\n\n\n\nDifferent combinations of bias and variance in estimation\n\n\nIntuitive analogy: Think of trying to hit a bullseye:\n\nRandom error = scattered shots around a target (sometimes left, sometimes right, sometimes high, sometimes low)\nSystematic error = consistently hitting the same wrong spot (all shots clustered, but away from the bullseye)\nIdeal = shots tightly clustered at the bullseye center\n\n\n\n\n\nQuantifying Uncertainty\n\nStandard Error\nThe standard error (SE) quantifies how much an estimate varies across different possible samples. It measures sampling error specifically.\n\n\nFor a Proportion:\nSE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\nFor a Mean:\nSE(\\bar{x}) = \\frac{s}{\\sqrt{n}}\n\nFor a Difference:\nSE(\\bar{x}_1 - \\bar{x}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\n\n\n\n\n\n\n\nWhat SE Tells Us\n\n\n\nStandard error quantifies sampling error only. It does not account for systematic errors (bias), measurement error, or other sources of uncertainty.\n\n\n\n\nMargin of Error\nThe margin of error (MOE) represents the expected maximum difference between sample estimate and true parameter.\n\\text{MOE} = \\text{Critical Value} \\times \\text{Standard Error}\n\n\n\n\n\n\nUnderstanding the Critical Value\n\n\n\n\n\nFor 95% confidence, we use 1.96 (often simplified to 2). This ensures that ~95% of intervals constructed this way will contain the true parameter.\n\n90% confidence: z = 1.645\n95% confidence: z = 1.96\n99% confidence: z = 2.576\n\n\n\n\n\n\nConfidence Intervals\nA confidence interval provides a range of plausible values:\n\\text{CI} = \\text{Estimate} \\pm (\\text{Critical Value} \\times \\text{Standard Error})\n\n\n\n\n\n\nImportant Limitation\n\n\n\nConfidence intervals quantify sampling uncertainty but assume no systematic error. A perfectly precise estimate (narrow CI) can still be biased if the study design is flawed.\n\n\n\n\n\n\nPractical Application: Opinion Polling\n\n\n\n\n\n\nCase Study: Political Polls\n\n\n\nWhen a poll reports “Candidate A: 52%, Candidate B: 48%”, this is incomplete without uncertainty quantification.\n\n\n\nThe Golden Rule of Polling\nWith ~1,000 randomly selected respondents:\n\nMargin of error: ±3 percentage points (95% confidence)\nInterpretation: A reported 52% means true support likely between 49% and 55%\nWhat this covers: Only random sampling error—assumes no systematic bias\n\n\n\n\n\n\n\nCritical Distinction\n\n\n\nThe ±3% margin of error quantifies sampling uncertainty only. It does not account for:\n\nCoverage bias (who’s excluded from the sampling frame)\nNon-response bias (who refuses to participate)\nResponse bias (people misreporting their true views)\nTiming effects (opinions changing between poll and election)\n\n\n\n\n\nSample Size and Precision\n\n\n\nSample Size\nMargin of Error (95%)\nUse Case\n\n\n\n\nn = 100\n± 10 pp\nBroad direction only\n\n\nn = 400\n± 5 pp\nGeneral trends\n\n\nn = 1,000\n± 3 pp\nStandard polls\n\n\nn = 2,500\n± 2 pp\nHigh precision\n\n\nn = 10,000\n± 1 pp\nVery high precision\n\n\n\n\n\n\n\n\n\nLaw of Diminishing Returns\n\n\n\nTo halve the margin of error, you need four times the sample size because \\text{MOE} \\propto 1/\\sqrt{n}\nThis applies only to sampling error. Doubling your sample size from 1,000 to 2,000 won’t fix systematic problems like biased question wording or unrepresentative sampling methods.\n\n\n\n\nWhat Quality Polls Should Report\nA transparent poll discloses:\n\nField dates: When was data collected?\nPopulation and sampling method: Who was surveyed and how were they selected?\nSample size: How many people responded?\nResponse rate: What proportion of contacted people participated?\nWeighting procedures: How was the sample adjusted to match population characteristics?\nMargin of sampling error: Quantification of sampling uncertainty\nQuestion wording: Exact text of questions asked\n\n\n\n\n\n\n\nThe Reporting Gap\n\n\n\nMost news reports mention only the topline numbers and occasionally the margin of error. They rarely discuss potential systematic biases, which can be much larger than sampling error.\n\n\n\n\n\n\nVisualization: Sampling Variability\nThe following simulation demonstrates how confidence intervals behave across repeated sampling:\n\n\nShow simulation code\nlibrary(ggplot2)\nset.seed(42)\n\n# Parameters\nn_polls      &lt;- 20\nn_people     &lt;- 100\ntrue_support &lt;- 0.50\n\n# Simulate independent polls\nsupport &lt;- rbinom(n_polls, n_people, true_support) / n_people\n\n# Calculate standard errors and margins of error\nse   &lt;- sqrt(support * (1 - support) / n_people)\nmoe  &lt;- 2 * se  # Simplified multiplier for clarity\n\n# Create confidence intervals\nlower &lt;- pmax(0, support - moe)\nupper &lt;- pmin(1, support + moe)\n\n# Check coverage\ncovers &lt;- (lower &lt;= true_support) & (upper &gt;= true_support)\nn_cover &lt;- sum(covers)\n\nresults &lt;- data.frame(\n  poll = seq_len(n_polls),\n  support, se, moe, lower, upper, covers\n)\n\n# Create visualization\nggplot(results, aes(x = poll, y = support, color = covers)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                width = 0.3, alpha = 0.8, size = 1) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = true_support, \n             linetype = \"dashed\", \n             color = \"black\",\n             alpha = 0.7) +\n  scale_color_manual(\n    values = c(\"TRUE\" = \"forestgreen\", \"FALSE\" = \"darkorange\"),\n    labels = c(\"TRUE\" = \"Covers truth\", \"FALSE\" = \"Misses truth\"),\n    name   = NULL\n  ) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0, 1)) +\n  labs(\n    title    = \"Sampling Variability in 20 Independent Polls\",\n    subtitle = paste0(\n      \"Each poll: n = \", n_people, \" | True value = \",\n      scales::percent(true_support),\n      \" | Coverage: \", n_cover, \"/\", n_polls,\n      \" (\", round(100 * n_cover / n_polls), \"%)\"\n    ),\n    x = \"Poll Number\",\n    y = \"Estimated Support\",\n    caption = \"Error bars show approximate 95% confidence intervals\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Observation\n\n\n\nMost intervals capture the true value, but some “miss” purely due to sampling randomness. This is expected and quantifiable—it’s the nature of random sampling error.\nImportant: This simulation assumes no systematic bias. In real polling, systematic errors (non-response bias, coverage problems, question wording effects) can shift all estimates in the same direction, making them consistently wrong even with large samples.\n\n\n\n\n\nCommon Misconceptions\n\n\n\n\n\n\nMisconception #1: Margin of Error Covers All Uncertainty\n\n\n\n❌ Myth: “The true value is definitely within the margin of error”\n✅ Reality:\n\nWith 95% confidence, there’s still a 5% chance the true value falls outside the interval due to sampling randomness alone\nMore importantly, margin of error only covers sampling error, not systematic biases\nReal polls often have larger errors from non-response bias, question wording, or coverage problems than from sampling error\n\n\n\n\n\n\n\n\n\nMisconception #2: Larger Samples Fix Everything\n\n\n\n❌ Myth: “If we just survey more people, we’ll eliminate all error”\n✅ Reality:\n\nLarger samples reduce random error (particularly sampling error): more precise estimates\nLarger samples do NOT reduce systematic error: bias remains unchanged\nA poll of 10,000 people with 70% response rate and biased sampling frame will give a precisely wrong answer\nBetter to have 1,000 well-selected respondents than 10,000 poorly selected ones\n\n\n\n\n\n\n\n\n\nMisconception #3: Random = Careless\n\n\n\n❌ Myth: “Random error means someone made mistakes”\n✅ Reality:\n\nRandom error is inherent in sampling and measurement—it’s not a mistake\nEven with perfect methodology, different random samples yield different results\nRandom errors are predictable in aggregate even though unpredictable individually\nThe term “random” refers to the pattern (no systematic direction), not to carelessness\n\n\n\n\n\n\n\n\n\nMisconception #4: Confidence Intervals are Guarantees\n\n\n\n❌ Myth: “95% confidence means there’s a 95% chance the true value is in this specific interval”\n✅ Reality:\n\nThe true value is fixed (but unknown)—it either is or isn’t in the interval\n“95% confidence” means: if we repeated this process many times, about 95% of the intervals we construct would contain the true value\nEach specific interval either captures the truth or doesn’t—we just don’t know which\n\n\n\n\n\n\n\n\n\nMisconception #5: Bias Can Be Calculated Like Random Error\n\n\n\n❌ Myth: “We can calculate the bias just like we calculate standard error”\n✅ Reality:\n\nRandom error is quantifiable using probability theory because we know the sampling process\nSystematic error is usually unknown and unknowable without external validation\nYou can’t use the sample itself to detect bias—you need independent information about the population\nThis is why comparing polls to election results is valuable: it reveals biases that weren’t quantifiable beforehand\n\n\n\n\n\n\nReal-World Example: Polling Failures\n\n\n\n\n\n\nCase Study: When Polls Mislead\n\n\n\nConsider a scenario where 20 polls all show Candidate A leading by 3-5 points, with margins of error around ±3%. The polls seem consistent, but Candidate B wins.\nWhat happened?\n\nNot sampling error: All polls agreed—unlikely if only random variation\nLikely systematic error:\n\nNon-response bias: Certain voters consistently refused to participate\nSocial desirability bias: Some voters misreported their true preference\nTurnout modeling error: Wrong assumptions about who would actually vote\nCoverage bias: Sampling frame (e.g., phone lists) systematically excluded certain groups\n\n\nThe lesson: Consistency among polls doesn’t guarantee accuracy. All polls can share the same systematic biases, giving false confidence in wrong estimates.\n\n\n\n\n\nKey Takeaways\n\n\n\n\n\n\nEssential Points\n\n\n\nUnderstanding Error Types:\n\nRandom error is unpredictable variation that averages to zero\n\nSampling error: From observing a sample, not the whole population\nMeasurement error: From imperfect measurement instruments or processes\nReduced by: larger samples, better instruments, more measurements\n\nSystematic error (bias) is consistent deviation in one direction\n\nSelection bias, measurement bias, response bias, non-response bias, etc.\nReduced by: better study design, not larger samples\n\n\nQuantifying Uncertainty:\n\nStandard error measures typical sampling variability (one type of random error)\nMargin of error ≈ 2 × SE gives a range for 95% confidence about sampling uncertainty\nSample size and sampling error precision follow: \\text{SE} \\propto 1/\\sqrt{n}\n\nQuadrupling sample size halves sampling error\nDiminishing returns as n increases\n\nConfidence intervals provide plausible ranges but assume no systematic bias\n\nCritical Insights:\n\nA precisely wrong answer (large biased sample) is often worse than an imprecisely right answer (small unbiased sample)\nAlways consider both sampling error AND potential systematic biases—published margins of error typically ignore the latter\nTransparency matters: Report methodology, response rates, and potential biases, not just point estimates and margins of error\nValidation is essential: Compare estimates to known values whenever possible to detect systematic errors\n\n\n\n\n\n\n\n\n\nThe Practitioner’s Priority\n\n\n\nWhen designing studies:\nFirst: Minimize systematic error through careful design\n\nRepresentative sampling methods\nHigh response rates\nUnbiased measurement tools\nProper question wording\n\nThen: Optimize sample size to achieve acceptable precision\n\nLarger samples help only after bias is addressed\nBalance cost vs. precision improvement\nRemember diminishing returns\n\nFinally: Report uncertainty honestly\n\nState assumptions clearly\nAcknowledge potential biases\nDon’t let precise estimates create false confidence",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#sampling-and-sampling-methods",
    "href": "chapter1.html#sampling-and-sampling-methods",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.11 Sampling and Sampling Methods (*)",
    "text": "1.11 Sampling and Sampling Methods (*)\nSampling is the process of selecting a subset of individuals from a population to estimate characteristics of the whole population. The way we sample profoundly affects what we can conclude from our data.\n\nThe Sampling Frame\nBefore discussing methods, we must understand the sampling frame—the list or device from which we draw our sample. The frame should ideally include every population member exactly once.\nCommon Sampling Frames:\n\nElectoral rolls (for adult citizens)\nTelephone directories (increasingly problematic due to mobile phones and unlisted numbers)\nAddress lists from postal services\nBirth registrations (for newborns)\nSchool enrollment lists (for children)\nTax records (for income earners)\nSatellite imagery (for dwellings in remote areas)\n\nFrame Problems:\n\nUndercoverage: Frame missing population members (homeless individuals not on address lists)\nOvercoverage: Frame includes non-population members (deceased people still on voter rolls)\nDuplication: Same unit appears multiple times (people with multiple phone numbers)\nClustering: Multiple population members per frame unit (multiple families at one address)\n\n\n\nProbability Sampling Methods\nProbability sampling gives every population member a known, non-zero probability of selection. This allows us to make statistical inferences about the population.\n\nSimple Random Sampling (SRS)\nEvery possible sample of size n has equal probability of selection. It’s the gold standard for statistical theory but often impractical for large populations.\nHow It Works:\n\nNumber every unit in the population from 1 to N\nUse random numbers to select n units\nEach unit has probability n/N of selection\n\nExample: To sample 50 students from a school of 1,000:\n\nAssign each student a number from 1 to 1,000\nGenerate 50 random numbers between 1 and 1,000\nSelect students with those numbers\n\nAdvantages:\n\nStatistically optimal\nEasy to analyze\nNo need for additional information about population\n\nDisadvantages:\n\nRequires complete sampling frame\nCan be expensive (selected units might be far apart)\nMay not represent important subgroups well by chance\n\n\n\nSystematic Sampling\nSelect every kth element from an ordered sampling frame, where k = N/n (the sampling interval).\nHow It Works:\n\nCalculate sampling interval k = N/n\nRandomly select starting point between 1 and k\nSelect every kth unit thereafter\n\nExample: To sample 100 houses from 5,000 on a street listing:\n\nk = 5,000/100 = 50\nRandom start: 23\nSample houses: 23, 73, 123, 173, 223…\n\nAdvantages:\n\nSimple to implement in field\nSpreads sample throughout population\n\nDisadvantages:\n\nCan introduce bias if there’s periodicity in the frame\n\nHidden Periodicity Example: Sampling every 10th apartment in buildings where corner apartments (numbers ending in 0) are all larger. This would bias our estimate of average apartment size.\n\n\nStratified Sampling\nDivide population into homogeneous subgroups (strata) before sampling. Sample independently within each stratum.\nHow It Works:\n\nDivide population into non-overlapping strata\nSample independently from each stratum\nCombine results with appropriate weights\n\nExample: Studying income in a city with distinct neighborhoods:\n\nStratum 1: High-income neighborhood (10% of population) - sample 100\nStratum 2: Middle-income neighborhood (60% of population) - sample 600\nStratum 3: Low-income neighborhood (30% of population) - sample 300\n\nTypes of Allocation:\nProportional: Sample size in each stratum proportional to stratum size\n\nIf stratum has 20% of population, it gets 20% of sample\n\nOptimal (Neyman): Larger samples from more variable strata\n\nIf income varies more in high-income areas, sample more there\n\nEqual: Same sample size per stratum regardless of population size\n\nUseful when comparing strata is primary goal\n\nAdvantages:\n\nEnsures representation of all subgroups\nCan increase precision substantially\nAllows different sampling methods per stratum\nProvides estimates for each stratum\n\nDisadvantages:\n\nRequires information to create strata\nCan be complex to analyze\n\n\n\nCluster Sampling\nSelect groups (clusters) rather than individuals. Often used when population is naturally grouped or when creating a complete frame is difficult.\nSingle-Stage Cluster Sampling:\n\nDivide population into clusters\nRandomly select some clusters\nInclude all units from selected clusters\n\nTwo-Stage Cluster Sampling:\n\nRandomly select clusters (Primary Sampling Units)\nWithin selected clusters, randomly select individuals (Secondary Sampling Units)\n\nExample: Surveying rural households in a large country:\n\nStage 1: Randomly select 50 villages from 1,000 villages\nStage 2: Within each selected village, randomly select 20 households\nTotal sample: 50 × 20 = 1,000 households\n\nMulti-Stage Example: National health survey:\n\nStage 1: Select states\nStage 2: Select counties within selected states\nStage 3: Select census blocks within selected counties\nStage 4: Select households within selected blocks\nStage 5: Select one adult within selected households\n\nAdvantages:\n\nDoesn’t require complete population list\nReduces travel costs (units clustered geographically)\nCan use different methods at different stages\nNatural for hierarchical populations\n\nDisadvantages:\n\nLess statistically efficient than SRS\nComplex variance estimation\nLarger samples needed for same precision\n\nDesign Effect: Cluster sampling typically requires larger samples than SRS. The design effect (DEFF) quantifies this:\n\\text{DEFF} = \\frac{\\text{Variance(cluster sample)}}{\\text{Variance(SRS)}}\nIf DEFF = 2, you need twice the sample size to achieve the same precision as SRS.\n\n\n\nNon-Probability Sampling Methods\nNon-probability sampling doesn’t guarantee known selection probabilities. While limiting statistical inference, these methods may be necessary or useful in certain situations.\n\nConvenience Sampling\nSelection based purely on ease of access. No attempt at representation.\nExamples:\n\nSurveying students in your class about study habits\nInterviewing people at a shopping mall about consumer preferences\nOnline polls where anyone can participate\nMedical studies using volunteers who respond to advertisements\n\nWhen It Might Be Acceptable:\n\nPilot studies to test survey instruments\nExploratory research to identify issues\nWhen studying processes believed to be universal\n\nMajor Problems:\n\nNo basis for inference to population\nSevere selection bias likely\nResults may be completely misleading\n\nReal Example: Literary Digest’s 1936 U.S. presidential poll surveyed 2.4 million people (huge sample!) but used telephone directories and club memberships as frames during the Depression, dramatically overrepresenting wealthy voters and incorrectly predicting Landon would defeat Roosevelt.\n\n\nPurposive (Judgmental) Sampling\nDeliberate selection of specific cases based on researcher judgment about what’s “typical” or “interesting.”\nExamples:\n\nSelecting “typical” villages to represent rural areas\nChoosing specific age groups for a developmental study\nSelecting extreme cases to understand range of variation\nPicking information-rich cases for in-depth study\n\nTypes of Purposive Sampling:\nTypical Case: Choose average or normal examples\n\nStudying “typical” American suburbs\n\nExtreme/Deviant Case: Choose unusual examples\n\nStudying villages with unusually low infant mortality to understand success factors\n\nMaximum Variation: Deliberately pick diverse cases\n\nSelecting diverse schools (urban/rural, rich/poor, large/small) for education research\n\nCritical Case: Choose cases that will be definitive\n\n“If it doesn’t work here, it won’t work anywhere”\n\nWhen It’s Useful:\n\nQualitative research focusing on depth over breadth\nWhen studying rare populations\nResource constraints limit sample size severely\nExploratory phases of research\n\nProblems:\n\nEntirely dependent on researcher judgment\nNo statistical inference possible\nDifferent researchers might select different “typical” cases\n\n\n\nQuota Sampling\nSelection to match population proportions on key characteristics. Like stratified sampling but without random selection within groups.\nHow Quota Sampling Works:\n\nIdentify key characteristics (age, sex, race, education)\nDetermine population proportions for these characteristics\nSet quotas for each combination\nInterviewers fill quotas using convenience methods\n\nDetailed Example: Political poll with quotas:\nPopulation proportions:\n\nMale 18-34: 15%\nMale 35-54: 20%\nMale 55+: 15%\nFemale 18-34: 16%\nFemale 35-54: 19%\nFemale 55+: 15%\n\nFor a sample of 1,000:\n\nInterview 150 males aged 18-34\nInterview 200 males aged 35-54\nAnd so on…\n\nInterviewers might stand on street corners approaching people who appear to fit needed categories until quotas are filled.\nWhy It’s Popular in Market Research:\n\nFaster than probability sampling\nCheaper (no callbacks for specific individuals)\nEnsures demographic representation\nNo sampling frame needed\n\nWhy It’s Problematic for Statistical Inference:\nHidden Selection Bias: Interviewers approach people who look approachable, speak the language well, aren’t in a hurry—systematically excluding certain types within each quota cell.\nExample of Bias: An interviewer filling a quota for “women 18-34” might approach women at a shopping mall on Tuesday afternoon, systematically missing:\n\nWomen who work during weekdays\nWomen who can’t afford to shop at malls\nWomen with young children who avoid malls\nWomen who shop online\n\nEven though the final sample has the “right” proportion of young women, they’re not representative of all young women.\nNo Measure of Sampling Error: Without selection probabilities, we can’t calculate standard errors or confidence intervals.\nHistorical Cautionary Tale: Quota sampling was standard in polling until the 1948 U.S. presidential election, when polls using quota sampling incorrectly predicted Dewey would defeat Truman. The failure led to adoption of probability sampling in polling.\n\n\nSnowball Sampling\nParticipants recruit additional subjects from their acquaintances. The sample grows like a rolling snowball.\nHow It Works:\n\nIdentify initial participants (seeds)\nAsk them to refer others with required characteristics\nAsk new participants for further referrals\nContinue until sample size reached or referrals exhausted\n\nExample: Studying undocumented immigrants:\n\nStart with 5 immigrants you can identify\nEach refers 3 others they know\nThose 15 each refer 2-3 others\nContinue until you have 100+ participants\n\nWhen It’s Valuable:\nHidden Populations: Groups without sampling frames\n\nDrug users\nHomeless individuals\nPeople with rare diseases\nMembers of underground movements\n\nSocially Connected Populations: When relationships matter\n\nStudying social network effects\nResearching community transmission of diseases\nUnderstanding information diffusion\n\nTrust-Dependent Research: When referrals increase participation\n\nSensitive topics where trust is essential\nClosed communities suspicious of outsiders\n\nMajor Limitations:\n\nSamples biased toward cooperative, well-connected individuals\nIsolated members of population missed entirely\nStatistical inference generally impossible\nCan reinforce social divisions (chains rarely cross social boundaries)\n\nAdvanced Version - Respondent-Driven Sampling (RDS):\nAttempts to make snowball sampling more rigorous by:\n\nTracking who recruited whom\nLimiting number of referrals per person\nWeighting based on network size\nUsing mathematical models to adjust for bias\n\nStill controversial whether RDS truly allows valid inference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#probability-concepts-for-statistical-analysis",
    "href": "chapter1.html#probability-concepts-for-statistical-analysis",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.12 Probability Concepts for Statistical Analysis",
    "text": "1.12 Probability Concepts for Statistical Analysis\nWhile this is primarily a statistics course, understanding basic probability is essential for statistical inference.\n\nBasic Probability\nProbability quantifies uncertainty on a scale from 0 (impossible) to 1 (certain).\nClassical Probability: P(\\text{event}) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total possible outcomes}}\nExample: Probability a randomly selected person is female \\approx 0.5\nEmpirical Probability: Based on observed frequencies\nExample: In a village, 423 of 1,000 residents are female, so P(\\text{female}) \\approx 0.423\n\n\nConditional Probability\nConditional Probability is the probability of event A given that event B has occurred: P(A|B)\nDemographic Example: Probability of dying within a year given current age:\n\nP(\\text{death within year} | \\text{age 30}) \\approx 0.001\nP(\\text{death within year} | \\text{age 80}) \\approx 0.05\n\nThese conditional probabilities form the basis of life tables.\n\n\nIndependence\nEvents A and B are independent if P(A|B) = P(A).\nTesting Independence in Demographic Data:\nAre education and fertility independent?\n\nP(\\text{3+ children}) = 0.3 overall\nP(\\text{3+ children} | \\text{college degree}) = 0.15\nDifferent probabilities indicate dependence\n\n\n\nLaw of Large Numbers\nAs sample size increases, sample statistics converge to population parameters.\nDemonstration: Estimating sex ratio at birth:\n\n10 births: 7 males (70% - very unstable)\n100 births: 53 males (53% - getting closer to ~51.2%)\n1,000 births: 515 males (51.5% - quite close)\n10,000 births: 5,118 males (51.18% - very close)\n\n\n\nVisualizing the Law of Large Numbers: Coin Flips\nLet’s see this in action with coin flips. A fair coin has a 50% chance of landing heads, but individual flips are unpredictable.\n\n# Simulate coin flips and show convergence\nset.seed(42)\nn_flips &lt;- 1000\nflips &lt;- rbinom(n_flips, 1, 0.5)  # 1 = heads, 0 = tails\n\n# Calculate cumulative proportion of heads\ncumulative_prop &lt;- cumsum(flips) / seq_along(flips)\n\n# Create data frame for plotting\nlln_data &lt;- data.frame(\n  flip_number = 1:n_flips,\n  cumulative_proportion = cumulative_prop\n)\n\n# Plot the convergence\nggplot(lln_data, aes(x = flip_number, y = cumulative_proportion)) +\n  geom_line(color = \"steelblue\", alpha = 0.7) +\n  geom_hline(yintercept = 0.5, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_hline(yintercept = c(0.45, 0.55), color = \"red\", linetype = \"dotted\", alpha = 0.7) +\n  labs(\n    title = \"Law of Large Numbers: Coin Flip Proportions Converge to 0.5\",\n    x = \"Number of coin flips\",\n    y = \"Cumulative proportion of heads\",\n    caption = \"Red dashed line = true probability (0.5)\\nDotted lines = ±5% range\"\n  ) +\n  scale_y_continuous(limits = c(0.3, 0.7), breaks = seq(0.3, 0.7, 0.1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhat this shows:\n\nEarly flips show wild variation (first 10 flips might be 70% or 30% heads)\nAs we add more flips, the proportion stabilizes around 50%\nThe “noise” of individual outcomes averages out over time\n\n\n\nThe Mathematical Statement\nLet A denote an event of interest (e.g., “heads on a coin flip”, “vote for party X”, “sum of dice equals 7”). If P(A) = p and we observe n independent trials with the same distribution (i.i.d.), then the sample frequency of A:\n\\hat{p}_n = \\frac{\\text{number of occurrences of } A}{n}\nconverges to p as n increases.\n\n\nExamples in Different Contexts\nDice example: The event “sum = 7” with two dice has probability 6/36 ≈ 16.7\\%, while “sum = 4” has 3/36 ≈ 8.3\\%. Over many throws, a sum of 7 appears about twice as often as a sum of 4.\nElection polling: If population support for a party equals p, then under random sampling of size n, the observed frequency \\hat{p}_n will approach p as n grows (assuming random sampling and independence).\nQuality control: If 2% of products are defective, then in large batches, approximately 2% will be found defective (assuming independent production).\n\n\nWhy This Matters for Statistics\nBottom line: Randomness underpins statistical inference by turning uncertainty in individual outcomes into predictable distributions for estimates. The Law of Large Numbers guarantees that the “noise” of individual outcomes averages out, allowing us to:\n\nPredict long-run frequencies\nQuantify uncertainty (margins of error)\n\nDraw reliable inferences from samples\nMake probabilistic statements about populations\n\nThis principle works in surveys, experiments, and even quantum phenomena (in the frequentist interpretation).\n\n\n\nCentral Limit Theorem (CLT)\nThe Central Limit Theorem states that the distribution of sample means approaches a normal distribution as sample size increases, regardless of the shape of the original population distribution. This holds true even for highly skewed or non-normal populations.\n\nKey Insights\n\nSample Size Threshold: Sample sizes of n ≥ 30 are typically sufficient for the CLT to apply\nStandard Error: The standard deviation of sample means equals σ/√n, where σ is the population standard deviation\nStatistical Foundation: We can make inferences about population parameters using normal distribution properties, even when the underlying data is non-normal\n\n\n\nWhy This Matters in Practice\nConsider income data, which is typically right-skewed with a long tail of high earners. While individual incomes don’t follow a normal distribution, something remarkable happens when we repeatedly take samples and calculate their means:\nWhat “normally distributed sample means” actually means:\n\nIf you take many different groups of 30+ people and calculate each group’s average income\nThese group averages will form a bell-shaped pattern when plotted\nMost group averages will cluster near the true population mean\nThe probability of getting a group average far from the population mean becomes predictable\n\nThis predictable pattern (normal distribution) allows us to:\n\nCalculate confidence intervals using normal distribution properties\nPerform statistical hypothesis tests\nMake predictions about sample means with known probability\n\nConcrete Example: Imagine a city where individual incomes range from $20,000 to $10,000,000, heavily skewed right. If you:\n\nRandomly select 100 people and calculate their mean income: maybe $75,000\nRepeat this 1000 times (1000 different groups of 100 people)\nPlot these 1000 group means: they’ll form a bell curve centered around the true population mean\nAbout 95% of these group means will fall within a predictable range\nThis happens even though individual incomes are extremely skewed!\n\n\n\nMathematical Foundation\nFor a population with mean μ and finite variance σ²:\n\nSampling distribution of the mean: \\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n}) as n \\to \\infty\nStandard error of the mean: SE_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\nStandardized sample mean: Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1) for large n\n\n\n\nKey Takeaways\n\nUniversal Application: The CLT applies to any distribution with finite variance\nConvergence to Normality: The approximation to normal distribution improves as sample size increases\nFoundation for Inference: Most parametric statistical tests rely on the CLT\nSample Size Considerations: While n ≥ 30 is a common guideline, highly skewed distributions may require larger samples for accurate approximation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#statistical-significance-a-quick-start-guide",
    "href": "chapter1.html#statistical-significance-a-quick-start-guide",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.13 Statistical Significance: A Quick Start Guide",
    "text": "1.13 Statistical Significance: A Quick Start Guide\nImagine you flip a coin 10 times and get 8 heads. Is the coin biased, or did you just get lucky? This is the core question statistical significance (statistical inference) helps us answer.\nStatistical significance tells us whether patterns in our data likely reflect something real or could have happened by pure chance.\nStatistical significance is a measure (p-value) of how confident we can be that patterns observed in our sample are not due to chance alone. When a result is statistically significant (typically p-value &lt; 0.05), it means the probability of obtaining such data in the absence of a real effect is very low.\n\nThe Courtroom Analogy\nStatistical hypothesis testing works like a criminal trial:\n\nNull Hypothesis (H_0): The defendant is innocent (no effect exists)\nAlternative Hypothesis (H_1): The defendant is guilty (an effect exists)\nThe Evidence: Your data and test results\nThe Verdict: “Guilty” (reject H_0) or “Not Guilty” (fail to reject H_0)\n\nCrucial distinction: “Not guilty” ≠ “Innocent”\n\nA “not guilty” verdict means insufficient evidence to convict\nSimilarly, “not statistically significant” means insufficient evidence for an effect, NOT proof of no effect\n\n\n\nStart with Skepticism (Presumption of Innocence)\nIn statistics, we always start by assuming nothing special is happening:\n\nNull Hypothesis (H_0): “There’s no effect”\n\nThe coin is fair\nThe new drug doesn’t work\nStudy time doesn’t affect grades\n\nAlternative Hypothesis (H_1): “There IS an effect”\n\nThe coin is biased\nThe drug works\nMore study time improves grades\n\n\nKey principle: We maintain the null hypothesis (innocence) unless our data provides strong evidence against it—“beyond a reasonable doubt” in legal terms, or “p &lt; 0.05” in statistical terms.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-p-value-your-surprise-meter",
    "href": "chapter1.html#the-p-value-your-surprise-meter",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.14 The p-value: Your “Surprise Meter”",
    "text": "1.14 The p-value: Your “Surprise Meter”\nThe p-value answers one specific question:\n\n“If nothing special were happening (null hypothesis is true), how surprising would our results be?”\nA p-value is the probability of observing the study’s results, or more extreme results, if the null hypothesis (a statement of no effect or no difference) is true.\n\n\nThree Ways to Think About p-values\n\n1. The Surprise Scale\n\np &lt; 0.01: Very surprising! (Strong evidence against H_0)\np &lt; 0.05: Pretty surprising (Moderate evidence against H_0)\np &gt; 0.05: Not that surprising (Insufficient evidence against H_0)\n\n\n\n2. Concrete Example: The Suspicious Coin\nYou flip a coin 10 times and get 8 heads. What’s the p-value?\nThe calculation: If the coin were fair, the probability of getting 8 or more heads is: p = P(≥8 \\text{ heads in 10 flips}) \\approx 0.055 \\approx 5.5\\%\nP(X \\geq 8) = \\sum_{k=8}^{10} \\binom{10}{k} 0,5^{10} = \\frac{56}{1024} \\approx 0,0547\nInterpretation: There’s a 5.5% chance of getting results this extreme with a fair coin. That’s somewhat unusual but not shocking.\n\n\n3. The Formal Definition\nA p-value is the probability of getting results at least as extreme as what you observed, assuming the null hypothesis is true.\n\n\n\n\n\n\nWarning\n\n\n\nCommon Mistake: The p-value is NOT the probability that the null hypothesis is true! It assumes the null is true and tells you how unusual your data would be in that world.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-prosecutor-fallacy-a-warning",
    "href": "chapter1.html#the-prosecutor-fallacy-a-warning",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.15 The Prosecutor Fallacy: A Warning",
    "text": "1.15 The Prosecutor Fallacy: A Warning\nI can see why the example might be challenging for beginners! Here’s a revised version that builds up the intuition more gradually without requiring knowledge of Bayes theorem or significance levels:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-prosecutor-fallacy-a-warning-1",
    "href": "chapter1.html#the-prosecutor-fallacy-a-warning-1",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.16 The Prosecutor Fallacy: A Warning",
    "text": "1.16 The Prosecutor Fallacy: A Warning\n\nThe Fallacy Explained\nImagine this courtroom scenario:\nProsecutor: “If the defendant were innocent, there’s only a 1% chance we’d find his DNA at the crime scene. We found his DNA. Therefore, there’s a 99% chance he’s guilty!”\nThis is WRONG! The prosecutor confused:\n\nP(Evidence | Innocent) = 0.01 ← What we know\nP(Innocent | Evidence) = ? ← What we want to know (but can’t get from the p-value alone!)\n\n\nWhen we get p = 0.01, it’s tempting to think:\n❌ WRONG: “There’s only a 1% chance the null hypothesis is true”\n❌ WRONG: “There’s a 99% chance our treatment works”\n✅ CORRECT: “If the null hypothesis were true, there’s only a 1% chance we’d see data this extreme”\n\n\nWhy This Matters: A Simple Medical Testing Example\nImagine a rare disease test that’s 99% accurate:\n\nIf you have the disease, the test is positive 99% of the time\nIf you don’t have the disease, the test is negative 99% of the time (so 1% false positive rate)\n\nHere’s the key: Suppose only 1 in 1000 people actually have this disease.\nNow let’s test 10,000 people:\n\n10 people have the disease → 10 test positive (rounded)\n9,990 people don’t have the disease → about 100 test positive by mistake (1% of 9,990)\nTotal positive tests: 110\n\nIf you test positive, what’s the chance you actually have the disease?\n\nOnly 10 out of 110 positive tests are real\nThat’s about 9%, not 99%!\n\n\n\nThe Research Analogy\nThe same thing happens in research:\n\nWhen we test many hypotheses (like testing many potential drugs)\nMost don’t work (like most people don’t have the rare disease)\nEven with “significant” results (like a positive test), most findings might be false positives\n\n\n\n\n\n\n\nImportant\n\n\n\nA p-value tells you how surprising your data would be IF the null hypothesis were true. It doesn’t tell you the probability that the null hypothesis IS true.\nThink of it like this: The probability of the ground being wet IF it rained is very different from the probability it rained IF the ground is wet—the ground could be wet from a sprinkler!\n\nRemember: A p-value tells you P(Data | Null is true), not P(Null is true | Data). These are as different as P(Wet ground | Rain) and P(Rain | Wet ground)—the ground could be wet from a sprinkler!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#introduction-to-regression-analysis-modeling-relationships-between-variables",
    "href": "chapter1.html#introduction-to-regression-analysis-modeling-relationships-between-variables",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.17 Introduction to Regression Analysis: Modeling Relationships Between Variables",
    "text": "1.17 Introduction to Regression Analysis: Modeling Relationships Between Variables\nOne of the most powerful tools in statistical analysis is regression analysis—a method for understanding and quantifying relationships between variables.\nThe core idea is simple: How does one thing relate to another, and can we use that relationship to make predictions?\n\nThe One-Sentence Summary: Regression helps us understand how things relate to each other in a messy, complicated world where everything affects everything else.\n\n\nWhat is Regression Analysis?\nImagine you’re curious about the relationship between education and income. You notice that people with more education tend to earn more money, but you want to understand this relationship more precisely:\n\nHow much does each additional year of education increase income, on average?\nHow strong is this relationship?\nAre there other factors we should consider?\nCan we predict someone’s likely income if we know their education level?\n\nRegression analysis provides systematic answers to these questions. It’s like finding the “best-fitting story” that describes how variables relate to each other.\n\n\nVariables and Variation\nA variable is any characteristic that can take different values across units of observation. In political science:\n\nUnits of analysis: Countries, individuals, elections, policies, years\nVariables: GDP, voting preference, democracy score, conflict occurrence\n\n\n💡 In Plain English: A variable is anything that changes. If everyone voted the same way, “voting preference” wouldn’t be a variable—it would be a constant. We study variables because we want to understand why things differ.\n\n\n\n\n\n\n\n\nNote\n\n\n\nConsider a typical pre-election news headline: “Candidate Smith’s approval rating reaches 68%.” Your immediate inference likely suggests favorable electoral prospects for Smith—not guaranteed victory, but a strong position. You naturally understand that higher approval ratings tend to predict better electoral performance, even though the relationship is not perfect.\nThis intuitive assessment exemplifies the core logic of regression analysis. You used one piece of information (approval rating) to make a prediction about another outcome (electoral success). Moreover, you recognized both the relationship between these variables and the uncertainty inherent in your prediction.\nWhile such informal reasoning serves us well in daily life, it has important limitations. How much better are Smith’s chances at 68% approval compared to 58%? What happens when we need to consider multiple factors simultaneously—approval ratings, economic conditions, and incumbency status? How confident should we be in our predictions?\nRegression analysis provides a systematic framework for addressing these questions. It transforms our intuitive understanding of relationships into precise mathematical models that can be tested and refined. Through regression analysis, researchers can:\n\nGenerate precise predictions: Move beyond general assessments to specific numerical estimates—for instance, predicting not just that Smith will “probably win,” but estimating the expected vote share and range of likely outcomes.\nIdentify which factors matter most: Determine the relative importance of different variables—perhaps discovering that economic conditions influence elections more strongly than approval ratings.\nQuantify uncertainty in predictions: Explicitly measure how confident we should be in our predictions, distinguishing between near-certain outcomes and educated guesses.\nTest theoretical propositions with empirical data: Evaluate whether our beliefs about cause-and-effect relationships hold up when examined systematically across many observations.\n\n\nIn essence, regression analysis systematizes the pattern recognition we perform intuitively, providing tools to make our predictions more accurate, our comparisons more meaningful, and our conclusions more reliable.\n\n\n\nThe Fundamental Model\nA model represents an object, person, or system in an informative way. Models divide into physical representations (such as architectural models) and abstract representations (such as mathematical equations describing atmospheric dynamics).\nThe core of statistical thinking can be expressed as:\nY = f(X) + \\text{error}\nThis equation states that our outcome (Y) equals some function of our predictors (X), plus unpredictable variation.\nComponents:\n\nY = Dependent variable (the phenomenon we seek to explain)\nX = Independent variable(s) (explanatory factors)\nf() = The functional relationship (often assumed linear)\nerror (\\epsilon) = Unexplained variation\n\n\n💡 What This Really Means: Think of it like a recipe. Your grade in a class (Y) depends on study hours (X), but not perfectly. Two students studying 10 hours might get different grades because of test anxiety, prior knowledge, or just luck (the error term). Regression finds the average relationship.\n\nThis model provides the foundation for all statistical analysis—from simple correlations to complex machine learning algorithms.\nRegression helps answer fundamental questions such as:\n\nHow much does education increase political participation?\nWhat factors predict electoral success?\nDo democratic institutions promote economic growth?\n\n\n\n\n\n\nThe Basic Idea: Drawing the Best Line Through Points\n\nSimple Linear Regression\nLet’s start with the simplest case: the relationship between two variables. Suppose we plot education (years of schooling) on the x-axis and annual income on the y-axis for 100 people. We’d see a cloud of points, and regression finds the straight line that best represents the pattern in these points.\nWhat makes a line “best”? The regression line minimizes the total squared vertical distances from all points to the line. Think of it as finding the line that makes the smallest total prediction error.\nThe equation of this line is: Y = a + bX + \\text{error}\nOr in our example: \\text{Income} = a + b \\times \\text{Education} + \\text{error}\nWhere:\n\na (intercept) = predicted income with zero education\nb (slope) = change in income per additional year of education\nerror (e) = difference between actual and predicted income\n\nInterpreting the Results:\nIf our analysis finds: \\text{Income} = 15,000 + 4,000 \\times \\text{Education}\nThis tells us:\n\nSomeone with 0 years of education is predicted to earn $15,000\nEach additional year of education is associated with $4,000 more income\nSomeone with 12 years of education is predicted to earn: $15,000 + (4,000 ) = $63,000\nSomeone with 16 years (bachelor’s degree) is predicted to earn: $15,000 + (4,000 ) = $79,000\n\n\n\n\nUnderstanding Relationships vs. Proving Causation\nA crucial distinction: regression shows association, not necessarily causation. Our education-income regression shows they’re related, but doesn’t prove education causes higher income. Other explanations are possible:\n\nReverse causation: Maybe wealthier families can afford more education for their children\nCommon cause: Perhaps intelligence or motivation affects both education and income\nCoincidence: In small samples, patterns can appear by chance\n\nExample of Spurious Correlation: A regression might show that ice cream sales strongly predict drowning deaths. Does ice cream cause drowning? No! Both increase in summer (the common cause, confounding variable).\n\n\n\nMultiple Regression: Controlling for Other Factors\nReal life is complicated—many factors influence outcomes simultaneously. Multiple regression lets us examine one relationship while “controlling for” or “holding constant” other variables.\n\nThe Power of Statistical Control\nReturning to education and income, we might wonder: Is the education effect just because educated people tend to be from wealthier families, or live in cities? Multiple regression can separate these effects:\n\\text{Income} = a + b_1 \\times \\text{Education} + b_2 \\times \\text{Age} + b_3 \\times \\text{Urban} + b_4 \\times \\text{Parent Income} + \\text{error}\nNow b_1 represents the education effect after accounting for age, location, and family background. If b_1 = 3,000, it means: “Comparing people of the same age, location, and family background, each additional year of education is associated with $3,000 more income.”\nDemographic Example: Fertility and Women’s Education\nResearchers studying fertility might find: \\text{Children} = 4.5 - 0.3 \\times \\text{Education}\nThis suggests each year of women’s education is associated with 0.3 fewer children. But is education the cause, or are educated women different in other ways? Adding controls:\n\\text{Children} = a - 0.15 \\times \\text{Education} - 0.2 \\times \\text{Urban} + 0.1 \\times \\text{Husband Education} - 0.4 \\times \\text{Contraceptive Access}\nNow we see education’s association is weaker (-0.15 instead of -0.3) after accounting for urban residence and contraceptive access. This suggests part of education’s apparent effect operates through these other pathways.\n\n\n\nTypes of Variables in Regression\n\nOutcome (Dependent) Variable\nThis is what we’re trying to understand or predict:\n\nIncome in our first example\nNumber of children in our fertility example\nLife expectancy in health studies\nMigration probability in population studies\n\n\n\nPredictor (Independent) Variables\nThese are factors we think might influence the outcome:\n\nQuantitative: Age, years of education, income, distance\nQualitative (categorical): Gender, race, marital status, region\nBinary (Dummy): Urban/rural, employed/unemployed, married/unmarried\n\nHandling Categorical Variables: We can’t directly put “religion” into an equation. Instead, we create binary variables:\n\nChristian = 1 if Christian, 0 otherwise\nMuslim = 1 if Muslim, 0 otherwise\nHindu = 1 if Hindu, 0 otherwise\n(One category becomes the reference group)\n\n\n\n\nDifferent Types of Regression for Different Outcomes\nThe basic regression idea adapts to many situations:\n\nLinear Regression\nFor continuous outcomes (income, height, blood pressure): Y = a + b_1X_1 + b_2X_2 + … + \\text{error}\n\n\nLogistic Regression\nFor binary outcomes (died/survived, migrated/stayed, married/unmarried):\nInstead of predicting the outcome directly, we predict the probability: \\log\\left(\\frac{p}{1-p}\\right) = a + b_1X_1 + b_2X_2 + …\nWhere p is the probability of the event occurring.\nExample: Predicting migration probability based on age, education, and marital status. The model might find young, educated, unmarried people have 40% probability of migrating, while older, less educated, married people have only 5% probability.\n\n\nPoisson Regression\nFor count outcomes (number of children, number of doctor visits): \\log(\\text{expected count}) = a + b_1X_1 + b_2X_2 + …\nExample: Modeling number of children based on women’s characteristics. Useful because it ensures predictions are never negative (can’t have -0.5 children!).\n\n\nSurvival (Cox model)/Hazard Regression\nWhat it’s for: Predicting when something will happen, not just if it will happen.\nThe challenge: Imagine you’re studying how long marriages last. You follow 1,000 couples for 10 years, but by the end of your study:\n\n400 couples divorced (you know exactly when)\n600 couples are still married (you don’t know if/when they’ll divorce)\n\nRegular regression can’t handle this “incomplete story” problem—those 600 ongoing marriages contain valuable information, but we don’t know their endpoints yet.\nHow Cox models help: Instead of trying to predict the exact timing, they focus on relative risk—who’s more likely to experience the event sooner. Think of it like asking “At any given moment, who’s at higher risk?” rather than “Exactly when will this happen?”\nReal-world applications:\n\nMedical research: Who responds to treatment faster?\nBusiness: Which customers cancel subscriptions sooner?\nSocial science: What factors make life events happen earlier/later?\n\n\n\n\n\nInterpreting Regression Results\n\nCoefficients\nThe coefficient tells us the expected change in outcome for a one-unit increase in the predictor, holding other variables constant.\nExamples of Interpretation:\nLinear regression for income:\n\n“Each additional year of education is associated with $3,500 higher annual income, controlling for age and experience”\n\nLogistic regression for infant mortality:\n\n“Each additional prenatal visit is associated with 15% lower odds of infant death, controlling for mother’s age and education”\n\nMultiple regression for life expectancy:\n\n“Each $1,000 increase in per-capita GDP is associated with 0.4 years longer life expectancy, after controlling for education and healthcare access”\n\n\n\nStatistical Significance\nThe regression also tests whether relationships could be due to chance:\n\np-value &lt; 0.05: Relationship unlikely due to chance (statistically significant)\np-value &gt; 0.05: Relationship could plausibly be random variation\n\n\nBut remember: Statistical significance ≠ practical importance. With large samples, tiny effects become “significant.”\n\n\n\nConfidence Intervals for Coefficients\nJust as we have confidence intervals for means or proportions, we have them for regression coefficients:\n“The effect of education on income is $3,500 per year, 95% CI: [$2,800, $4,200]”\nThis means we’re 95% confident the true effect is between $2,800 and $4,200.\n\n\nR-squared: How Well Does the Model Fit?\nR^2 (R-squared) measures the proportion of variation in the outcome explained by the predictors:\n\nR^2 = 0: Predictors explain nothing\nR^2 = 1: Predictors explain everything\nR^2 = 0.3: Predictors explain 30% of variation\n\nExample: A model of income with only education might have R^2 = 0.15 (education explains 15% of income variation). Adding age, experience, and location might increase R^2 to 0.35 (together they explain 35%).\n\n\n\n\n\n\nAssumptions and Limitations\n\n\n\nRegression makes assumptions that may not hold:\n\nExogeneity (No Hidden Relationships)\nThe most fundamental assumption: predictors must not be correlated with errors. In simple terms, there shouldn’t be hidden factors that affect both your predictors and outcome.\nExample: If studying education’s effect on income but omitting “ability,” your results are biased - ability affects both education level and income. This assumption is written as: E[\\varepsilon | X] = 0\nWhy it matters: Without it, all your coefficients are wrong, even with millions of observations!\n\n\nLinearity\nAssumes straight-line relationships. But what if education’s effect on income is stronger at higher levels? We can add polynomial terms: \\text{Income} = a + b_1 \\times \\text{Education} + b_2 \\times \\text{Education}^2\n\n\nIndependence\nAssumes observations are independent. But family members might be similar, repeated measures on the same person are related, and neighbors might influence each other. Special methods handle these dependencies.\n\n\nHomoscedasticity\nAssumes error variance is constant. But prediction errors might be larger for high-income people than low-income people. Diagnostic plots help detect this.\n\n\nNormality\nAssumes errors follow normal distribution. Important for small samples and hypothesis tests, less critical for large samples.\nNote: The first assumption (exogeneity) is about getting the right answer. The others are mostly about precision and statistical inference. Violating exogeneity means your model is fundamentally wrong; violating the others means your confidence intervals and p-values might be off.\n\n\n\n\n\n\n\n\n\nCommon Statistical Pitfalls\n\n\n\n\nEndogeneity (omitted variable bias): Forgetting about hidden factors that affect both X and Y, violating the fundamental exogeneity assumption. Example: Studying education→income without accounting for ability.\nSimultaneity/Reverse causality: When X and Y determine each other at the same time. Simple regression assumes one-way causation, but reality is often bidirectional. Example: Price affects demand AND demand affects price simultaneously.\nConfounding: Failing to account for variables that affect both predictor and outcome, leading to spurious relationships. Example: Ice cream sales correlate with drownings (both caused by summer).\nSelection bias: Non-random samples that systematically exclude certain groups, making results ungeneralizable. Example: Surveying only smartphone users about internet usage.\nEcological fallacy: Assuming group-level patterns apply to individuals. Example: Rich countries have lower birth rates ≠ rich people have fewer children.\nP-hacking (data dredging): Testing multiple hypotheses until finding significance, or tweaking analysis until p &lt; 0.05. With 20 tests, you expect 1 false positive by chance alone!\nOverfitting: Building a model too complex for your data - perfect on training data, useless for prediction. Remember: With enough parameters, you can fit an elephant.\nSurvivorship bias: Analyzing only “survivors” while ignoring failures. Example: Studying successful companies while ignoring those that went bankrupt.\nOvergeneralization: Extending findings beyond the studied population, time period, or context. Example: Results from US college students ≠ universal human behavior.\n\nRemember: The first three are forms of endogeneity - they violate E[\\varepsilon|X]=0 and make your coefficients fundamentally wrong. The others make results misleading or non-representative.\n\n\n\n\n\n\nApplications in Demography\n\nFertility Analysis\nUnderstanding what factors influence fertility decisions: \\text{Children} = f(\\text{Education, Income, Urban, Religion, Contraception, …})\nHelps identify policy levers for countries concerned about high or low fertility.\nPolicy levers are the tools and methods that governments and organizations use to influence events and achieve specific goals by affecting behavior and outcomes.\n\n\nMortality Modeling\nPredicting life expectancy or mortality risk: \\text{Mortality Risk} = f(\\text{Age, Sex, Smoking, Education, Healthcare Access, …})\nUsed by insurance companies, public health officials, and researchers.\n\n\nMigration Prediction\nUnderstanding who migrates and why: P(\\text{Migration}) = f(\\text{Age, Education, Employment, Family Ties, Distance, …})\nHelps predict population flows and plan for demographic change.\n\n\nMarriage and Divorce\nAnalyzing union formation and dissolution: P(\\text{Divorce}) = f(\\text{Age at Marriage, Education Match, Income, Children, Duration, …})\nInforms social policy and support services.\n\n\n\nCommon Pitfalls and How to Avoid Them\n\nOverfitting\nIncluding too many predictors can make the model fit perfectly in your sample but fail with new data. Like memorizing exam answers instead of understanding concepts.\nSolution: Use simpler models, cross-validation, or reserve some data for testing.\n\n\nMulticollinearity\nWhen predictors are highly correlated (e.g., years of education and degree level), the model can’t separate their effects.\nSolution: Choose one variable or combine them into an index.\n\n\nOmitted Variable Bias\nLeaving out important variables can make other effects appear stronger or weaker than they really are.\nExample: The relationship between ice cream sales and crime rates disappears when you control for temperature.\n\n\nExtrapolation\nUsing the model outside the range of observed data.\nExample: If your data includes education from 0-20 years, don’t predict income for someone with 30 years of education.\n\n\n\nMaking Regression Intuitive\nThink of regression as a sophisticated averaging technique:\n\nSimple average: “The average income is $50,000”\nConditional average: “The average income for college graduates is $70,000”\nRegression: “The average income for 35-year-old college graduates in urban areas is $78,000”\n\nEach added variable makes our prediction more specific and (hopefully) more accurate.\n\n\nRegression in Practice: A Complete Example\nResearch Question: What factors influence age at first birth?\nData: Survey of 1,000 women who have had at least one child\nVariables:\n\nOutcome: Age at first birth (years)\nPredictors: Education (years), Urban (0/1), Income (thousands), Religious (0/1)\n\nSimple Regression Result: \\text{Age at First Birth} = 18 + 0.8 \\times \\text{Education}\nInterpretation: Each year of education associated with 0.8 years later first birth.\nMultiple Regression Result: \\text{Age at First Birth} = 16 + 0.5 \\times \\text{Education} + 2 \\times \\text{Urban} + 0.03 \\times \\text{Income} - 1.5 \\times \\text{Religious}\nInterpretation:\n\nEducation effect reduced but still positive (0.5 years per education year)\nUrban women have first births 2 years later\nEach $1,000 income associated with 0.03 years (11 days) later\nReligious women have first births 1.5 years earlier\nR^2 = 0.42 (model explains 42% of variation)\n\nThis richer model helps us understand that education’s effect partly operates through urban residence and income.\n\n\n\n\n\n\nWarning\n\n\n\nRegression is a gateway to advanced statistical modeling. Once you understand the basic concept—using variables to predict outcomes and quantifying relationships—you can explore:\n\nInteraction effects: When one variable’s effect depends on another\nNon-linear relationships: Curves, thresholds, and complex patterns\nMultilevel models: Accounting for grouped data (students in schools, people in neighborhoods)\nTime series regression: Analyzing change over time\nMachine learning extensions: Random forests, neural networks, and more\n\nThe key insight remains: We’re trying to understand how things relate to each other in a systematic, quantifiable way.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#data-quality-and-sources",
    "href": "chapter1.html#data-quality-and-sources",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.18 Data Quality and Sources",
    "text": "1.18 Data Quality and Sources\nNo analysis is better than the data it’s based on. Understanding data quality issues is crucial for demographic and social research.\n\nDimensions of Data Quality\nAccuracy: How close are measurements to true values?\nExample: Age reporting often shows “heaping” at round numbers (30, 40, 50) because people round their ages.\nCompleteness: What proportion of the population is covered?\nExample: Birth registration completeness varies widely:\n\nDeveloped countries: &gt;99%\nSome developing countries: &lt;50%\n\nTimeliness: How current is the data?\nExample: Census conducted every 10 years becomes increasingly outdated, especially in rapidly changing areas.\nConsistency: Are definitions and methods stable over time and space?\nExample: Definition of “urban” varies by country, making international comparisons difficult.\nAccessibility: Can researchers and policy makers actually use the data?\n\n\nCommon Data Sources in Demography\nCensus: Complete enumeration of population\nAdvantages:\n\nComplete coverage (in theory)\nSmall area data available\nBaseline for other estimates\n\nDisadvantages:\n\nExpensive and infrequent\nSome populations hard to count\nLimited variables collected\n\nSample Surveys: Detailed data from population subset\nExamples:\n\nDemographic and Health Surveys (DHS)\nAmerican Community Survey (ACS)\nLabour Force Surveys\n\nAdvantages:\n\nCan collect detailed information\nMore frequent than census\nCan focus on specific topics\n\nDisadvantages:\n\nSampling error present\nSmall areas not represented\nResponse burden may reduce quality\n\nAdministrative Records: Data collected for non-statistical purposes\nExamples:\n\nTax records\nSchool enrollment\nHealth insurance claims\nMobile phone data\n\nAdvantages:\n\nAlready collected (no additional burden)\nOften complete for covered population\nContinuously updated\n\nDisadvantages:\n\nCoverage may be selective\nDefinitions may not match research needs\nAccess often restricted\n\n\n\nData Quality Issues Specific to Demography\nAge Heaping: Tendency to report ages ending in 0 or 5\nDetection: Calculate Whipple’s Index or Myers’ Index\nImpact: Affects age-specific rates and projections\nDigit Preference: Reporting certain final digits more than others\nExample: Birth weights often reported as 3,000g, 3,500g rather than precise values\nRecall Bias: Difficulty remembering past events accurately\nExample: “How many times did you visit a doctor last year?” Often underreported for frequent visitors, overreported for rare visitors.\nProxy Reporting: Information provided by someone else\nChallenge: Household head reporting for all members may not know everyone’s exact age or education",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#ethical-considerations-in-statistical-demographics",
    "href": "chapter1.html#ethical-considerations-in-statistical-demographics",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.19 Ethical Considerations in Statistical Demographics",
    "text": "1.19 Ethical Considerations in Statistical Demographics\nStatistics isn’t just about numbers—it involves real people and has real consequences.\n\nInformed Consent\nParticipants should understand:\n\nPurpose of data collection\nHow data will be used\nRisks and benefits\nTheir right to refuse or withdraw\n\nChallenge in Demographics: Census participation is often mandatory, raising ethical questions about consent.\n\n\nConfidentiality and Privacy\nStatistical Disclosure Control: Protecting individual identity in published data\nMethods include:\n\nSuppressing small cells (e.g., “&lt;5” instead of “2”)\nGeographic aggregation\n\nExample: In a table of occupation by age by sex for a small town, there might be only one female doctor aged 60-65, making her identifiable.\n\n\nRepresentation and Fairness\nWho’s Counted?: Decisions about who to include affect representation\n\nPrisoners: Where are they counted—prison location or home address?\nHomeless: How to ensure coverage?\nUndocumented immigrants: Include or exclude?\n\nDifferential Privacy: Mathematical framework for privacy protection while maintaining statistical utility\nTrade-off: More privacy protection = less accurate statistics\n\n\nMisuse of Statistics\nCherry-Picking: Selecting only favorable results\nExample: Reporting decline in teen pregnancy from peak year rather than showing full trend\nP-Hacking: Manipulating analysis to achieve statistical significance\nEcological Fallacy: Inferring individual relationships from group data\nExample: Counties with more immigrants have higher average incomes ≠ immigrants have higher incomes\n\n\nResponsible Reporting\nUncertainty Communication: Always report confidence intervals or margins of error\nContext Provision: Include relevant comparison groups and historical trends\nLimitation Acknowledgment: Clearly state what data can and cannot show",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#common-misconceptions-in-statistics",
    "href": "chapter1.html#common-misconceptions-in-statistics",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.20 Common Misconceptions in Statistics",
    "text": "1.20 Common Misconceptions in Statistics\nUnderstanding what statistics is NOT is as important as understanding what it is.\n\nMisconception 1: “Statistics Can Prove Anything”\nReality: Statistics can only provide evidence, never absolute proof. And proper statistics, honestly applied, constrains conclusions significantly.\nExample: A study finds correlation between ice cream sales and drowning deaths. Statistics doesn’t “prove” ice cream causes drowning—both are related to summer weather.\n\n\nMisconception 2: “Larger Samples Are Always Better”\nReality: Beyond a certain point, larger samples add little precision but may add bias.\nExample: Online survey with 1 million responses may be less accurate than probability sample of 1,000 due to self-selection bias.\nDiminishing Returns:\n\nn = 100: Margin of error \\approx 10 pp.\nn = 1,000: Margin of error \\approx 3.2 pp.\nn = 10,000: Margin of error \\approx 1 pp.\nn = 100,000: Margin of error \\approx 0.32 pp.\n\nThe jump from 10,000 to 100,000 barely improves precision but costs 10\\times more.\n\n\nMisconception 3: “Statistical Significance = Practical Importance”\nReality: With large samples, tiny differences become “statistically significant” even if meaningless.\nExample: Study of 100,000 people finds men are 0.1 cm taller on average (p &lt; 0.001). Statistically significant but practically irrelevant.\n\n\nMisconception 4: “Correlation Implies Causation”\nReality: Correlation is necessary but not sufficient for causation.\nClassic Examples:\n\nCities with more churches have more crime (both correlate with population size)\nCountries with more TV sets have longer life expectancy (both correlate with development)\n\n\n\nMisconception 5: “Random Means Haphazard”\nReality: Statistical randomness is carefully controlled and systematic.\nExample: Random sampling requires careful procedure, not just grabbing whoever is convenient.\n\n\nMisconception 6: “Average Represents Everyone”\nReality: Averages can be misleading when distributions are skewed or multimodal.\nExample: Average income of bar patrons is $50,000. Bill Gates walks in. Now average is $1 million. Nobody’s actual income changed.\n\n\nMisconception 7: “Past Patterns Guarantee Future Results”\nReality: Extrapolation assumes conditions remain constant.\nExample: Linear population growth projection from 1950-2000 would badly overestimate 2050 population because it misses fertility decline.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#applications-in-demography-1",
    "href": "chapter1.html#applications-in-demography-1",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.21 Applications in Demography",
    "text": "1.21 Applications in Demography\nThese statistical foundations enable sophisticated demographic analyses. Let’s explore key applications.\n\nPopulation Estimation and Projection\nIntercensal Estimates: Estimating population between censuses\nComponents Method: P(t+1) = P(t) + B - D + I - E\nWhere:\n\nP(t) = Population at time t\nB = Births\nD = Deaths\nI = Immigration\nE = Emigration\n\nEach component estimated from different sources with different error structures.\nPopulation Projections: Forecasting future population\nCohort Component Method:\n\nProject survival rates by age\nProject fertility rates\nProject migration rates\nApply to base population\nAggregate results\n\nUncertainty increases with projection horizon.\n\n\nDemographic Rate Calculation\nCrude Rates: Events per 1,000 population\n\\text{Crude Birth Rate} = \\frac{\\text{Births}}{\\text{Mid-year Population}} \\times 1,000\nAge-Specific Rates: Control for age structure\n\\text{Age-Specific Fertility Rate} = \\frac{\\text{Births to women aged } x}{\\text{Women aged } x} \\times 1,000\nStandardization: Compare populations with different structures\nDirect Standardization: Apply population’s rates to standard age structure Indirect Standardization: Apply standard rates to population’s age structure\n\n\nLife Table Analysis\nLife tables summarize mortality experience of a population.\nKey Columns:\n\nq_x: Probability of dying between age x and x+1\nl_x: Number surviving to age x (from 100,000 births)\nd_x: Deaths between age x and x+1\nL_x: Person-years lived between age x and x+1\ne_x: Life expectancy at age x\n\nExample Interpretation: If q_{65} = 0.015, then 1.5% of 65-year-olds die before reaching 66. If e_{65} = 18.5, then 65-year-olds average 18.5 more years of life.\n\n\nFertility Analysis\nTotal Fertility Rate (TFR): Average children per woman given current age-specific rates\n\\text{TFR} = \\sum (\\text{ASFR} \\times \\text{age interval width})\nExample: If each 5-year age group from 15-49 has ASFR = 20 per 1,000: \\text{TFR} = 7 \\text{ age groups} \\times \\frac{20}{1,000} \\times 5 \\text{ years} = 0.7 \\text{ children per woman}\nThis very low TFR indicates below-replacement fertility.\n\n\nMigration Analysis\nNet Migration Rate: \\text{NMR} = \\frac{\\text{Immigrants} - \\text{Emigrants}}{\\text{Population}} \\times 1,000\nMigration Effectiveness Index: \\text{MEI} = \\frac{|\\text{In} - \\text{Out}|}{\\text{In} + \\text{Out}}\n\nValues near 0: High turnover, little net change\nValues near 1: Mostly one-way flow\n\n\n\nPopulation Health Metrics\nDisability-Adjusted Life Years (DALYs): Years of healthy life lost\nDALY = Years of Life Lost (YLL) + Years Lived with Disability (YLD)\nHealthy Life Expectancy: Expected years in good health\nCombines mortality and morbidity information.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#software-and-tools",
    "href": "chapter1.html#software-and-tools",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.22 Software and Tools",
    "text": "1.22 Software and Tools\nModern demographic and social statistics relies heavily on computational tools.\n\nStatistical Software Packages\nR: Free, open-source, extensive demographic packages\n\nPackages: demography, popReconstruct, bayesPop\nAdvantages: Reproducible research, cutting-edge methods\nDisadvantages: Steep learning curve\n\nStata: Widely used in social sciences\n\nStrengths: Survey data analysis, panel data\nCommon in: Economics, epidemiology\n\nSPSS: User-friendly interface\n\nStrengths: Point-and-click interface\nCommon in: Social sciences, market research\n\nPython: General programming language with statistical libraries\n\nLibraries: pandas, numpy, scipy, statsmodels\nAdvantages: Integration with other applications",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#conclusion",
    "href": "chapter1.html#conclusion",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.23 Conclusion",
    "text": "1.23 Conclusion\n\nKey Terms Summary\nStatistics: The science of collecting, organizing, analyzing, interpreting, and presenting data to understand phenomena and support decision-making\nDescriptive Statistics: Methods for summarizing and presenting data in meaningful ways without extending conclusions beyond the observed data\nInferential Statistics: Techniques for drawing conclusions about populations from samples, including estimation and hypothesis testing\nPopulation: The complete set of individuals, objects, or measurements about which conclusions are to be drawn\nSample: A subset of the population that is actually observed or measured to make inferences about the population\nSuperpopulation: A theoretical infinite population from which observed finite populations are considered to be samples\nParameter: A numerical characteristic of a population (usually unknown and denoted by Greek letters)\nStatistic: A numerical characteristic calculated from sample data (known and denoted by Roman letters)\nEstimator: A rule or formula for calculating estimates of population parameters from sample data\nEstimand: The specific population parameter targeted for estimation\nEstimate: The numerical value produced by applying an estimator to observed data\nRandom Error (Sampling Error): Unpredictable variation arising from the sampling process that decreases with larger samples\nSystematic Error (Bias): Consistent deviation from true values that cannot be reduced by increasing sample size\nSampling: The process of selecting a subset of units from a population for measurement\nSampling Frame: The list or device from which a sample is drawn, ideally containing all population members\nProbability Sampling: Sampling methods where every population member has a known, non-zero probability of selection\nSimple Random Sampling: Every possible sample of size n has equal probability of selection\nSystematic Sampling: Selection of every kth element from an ordered sampling frame\nStratified Sampling: Division of population into homogeneous subgroups before sampling within each\nCluster Sampling: Selection of groups (clusters) rather than individuals\nNon-probability Sampling: Sampling methods without guaranteed known selection probabilities\nConvenience Sampling: Selection based purely on ease of access\nPurposive Sampling: Deliberate selection based on researcher judgment\nQuota Sampling: Selection to match population proportions on key characteristics without random selection\nSnowball Sampling: Participants recruit additional subjects from their acquaintances\nStandard Error: The standard deviation of the sampling distribution of a statistic\nMargin of Error: Maximum expected difference between estimate and parameter at specified confidence\nConfidence Interval: Range of plausible values for a parameter at specified confidence level\nConfidence Level: Probability that the confidence interval method produces intervals containing the parameter\nData: Collected observations or measurements\nQuantitative Data: Numerical measurements (continuous or discrete)\nQualitative Data: Categorical information (nominal or ordinal)\nData Distribution: Description of how values spread across possible outcomes\nFrequency Distribution: Summary showing how often each value occurs in data\nAbsolute Frequency: Count of observations for each value\nRelative Frequency: Proportion of observations in each category\nCumulative Frequency: Running total of frequencies up to each value",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-a-visualizations-for-statistics-demography",
    "href": "chapter1.html#appendix-a-visualizations-for-statistics-demography",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.24 Appendix A: Visualizations for Statistics & Demography",
    "text": "1.24 Appendix A: Visualizations for Statistics & Demography\n\n## ============================================\n## Visualizations for Statistics & Demography\n## Chapter 1: Foundations\n## ============================================\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(gridExtra)\nlibrary(scales)\nlibrary(patchwork)  # for combining plots\n\n# Set theme for all plots\ntheme_set(theme_minimal(base_size = 12))\n\n# Color palette for consistency\ncolors &lt;- c(\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#6A994E\")\n\n\n# ==================================================\n# 1. POPULATION vs SAMPLE VISUALIZATION\n# ==================================================\n\n# Create a population and sample visualization\nset.seed(123)\n\n# Generate population data (e.g., ages of 10,000 people)\npopulation &lt;- data.frame(\n  id = 1:10000,\n  age = round(rnorm(10000, mean = 40, sd = 15))\n)\npopulation$age[population$age &lt; 0] &lt;- 0\npopulation$age[population$age &gt; 100] &lt;- 100\n\n# Take a random sample\nsample_size &lt;- 500\nsample_data &lt;- population[sample(nrow(population), sample_size), ]\n\n# Create visualization\np1 &lt;- ggplot(population, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = colors[1], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = mean(population$age), \n             color = colors[2], linetype = \"dashed\", size = 1.2) +\n  labs(title = \"Population Distribution (N = 10,000)\",\n       subtitle = paste(\"Population mean (μ) =\", round(mean(population$age), 2), \"years\"),\n       x = \"Age (years)\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(sample_data, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = colors[3], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = mean(sample_data$age), \n             color = colors[4], linetype = \"dashed\", size = 1.2) +\n  labs(title = paste(\"Sample Distribution (n =\", sample_size, \")\"),\n       subtitle = paste(\"Sample mean (x̄) =\", round(mean(sample_data$age), 2), \"years\"),\n       x = \"Age (years)\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Combine plots\npopulation_sample_plot &lt;- p1 / p2\nprint(population_sample_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 2. TYPES OF DATA DISTRIBUTIONS\n# ==================================================\n\n# Generate different distribution types\nset.seed(456)\nn &lt;- 5000\n\n# Normal distribution\nnormal_data &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Right-skewed distribution (income-like)\nright_skewed &lt;- rgamma(n, shape = 2, scale = 15)\n\n# Left-skewed distribution (age at death in developed country)\nleft_skewed &lt;- 90 - rgamma(n, shape = 3, scale = 5)\nleft_skewed[left_skewed &lt; 0] &lt;- 0\n\n# Bimodal distribution (e.g., height of mixed male/female population)\nn2  &lt;- 20000\nnf &lt;- n2 %/% 2; nm &lt;- n2 - nf\nbimodal &lt;- c(rnorm(nf, mean = 164, sd = 5),\n             rnorm(nm, mean = 182, sd = 5))\n\n\n# Create data frame\ndistributions_df &lt;- data.frame(\n  Normal = normal_data,\n  `Right Skewed` = right_skewed,\n  `Left Skewed` = left_skewed,\n  Bimodal = bimodal\n) %&gt;%\n  pivot_longer(everything(), names_to = \"Distribution\", values_to = \"Value\")\n\n# Plot distributions\ndistributions_plot &lt;- ggplot(distributions_df, aes(x = Value, fill = Distribution)) +\n  geom_histogram(bins = 30, alpha = 0.7, color = \"white\") +\n  facet_wrap(~Distribution, scales = \"free\", nrow = 2) +\n  scale_fill_manual(values = colors[1:4]) +\n  labs(title = \"Types of Data Distributions\",\n       subtitle = \"Common patterns in demographic data\",\n       x = \"Value\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"none\")\n\nprint(distributions_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 3. NORMAL DISTRIBUTION WITH 68-95-99.7 RULE\n# ==================================================\n\n# Generate normal distribution data\nset.seed(789)\nmean_val &lt;- 100\nsd_val &lt;- 15\nx &lt;- seq(mean_val - 4*sd_val, mean_val + 4*sd_val, length.out = 1000)\ny &lt;- dnorm(x, mean = mean_val, sd = sd_val)\ndf_norm &lt;- data.frame(x = x, y = y)\n\n# Create the plot\nnormal_plot &lt;- ggplot(df_norm, aes(x = x, y = y)) +\n  # Fill areas under the curve\n  geom_area(data = subset(df_norm, x &gt;= mean_val - sd_val & x &lt;= mean_val + sd_val),\n            aes(x = x, y = y), fill = colors[1], alpha = 0.3) +\n  geom_area(data = subset(df_norm, x &gt;= mean_val - 2*sd_val & x &lt;= mean_val + 2*sd_val),\n            aes(x = x, y = y), fill = colors[2], alpha = 0.2) +\n  geom_area(data = subset(df_norm, x &gt;= mean_val - 3*sd_val & x &lt;= mean_val + 3*sd_val),\n            aes(x = x, y = y), fill = colors[3], alpha = 0.1) +\n  # Add the curve\n  geom_line(size = 1.5, color = \"black\") +\n  # Add vertical lines for standard deviations\n  geom_vline(xintercept = mean_val, linetype = \"solid\", size = 1, color = \"black\") +\n  geom_vline(xintercept = c(mean_val - sd_val, mean_val + sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[1]) +\n  geom_vline(xintercept = c(mean_val - 2*sd_val, mean_val + 2*sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[2]) +\n  geom_vline(xintercept = c(mean_val - 3*sd_val, mean_val + 3*sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[3]) +\n  # Add labels\n  annotate(\"text\", x = mean_val, y = max(y) * 0.5, label = \"68%\", \n           size = 5, fontface = \"bold\", color = colors[1]) +\n  annotate(\"text\", x = mean_val, y = max(y) * 0.3, label = \"95%\", \n           size = 5, fontface = \"bold\", color = colors[2]) +\n  annotate(\"text\", x = mean_val, y = max(y) * 0.1, label = \"99.7%\", \n           size = 5, fontface = \"bold\", color = colors[3]) +\n  # Labels\n  scale_x_continuous(breaks = c(mean_val - 3*sd_val, mean_val - 2*sd_val, \n                                mean_val - sd_val, mean_val, \n                                mean_val + sd_val, mean_val + 2*sd_val, \n                                mean_val + 3*sd_val),\n                     labels = c(\"μ-3σ\", \"μ-2σ\", \"μ-σ\", \"μ\", \"μ+σ\", \"μ+2σ\", \"μ+3σ\")) +\n  labs(title = \"Normal Distribution: The 68-95-99.7 Rule\",\n       subtitle = \"Proportion of data within standard deviations from the mean\",\n       x = \"Value\", y = \"Probability Density\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\nprint(normal_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 4. SIMPLE LINEAR REGRESSION\n# ==================================================\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(scales)\n\n# Define color palette (this was missing in original code)\ncolors &lt;- c(\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#592E83\")\n\n# Generate data for regression example (Education vs Income)\nset.seed(2024)\nn_reg &lt;- 200\neducation &lt;- round(rnorm(n_reg, mean = 14, sd = 3))\neducation[education &lt; 8] &lt;- 8\neducation[education &gt; 22] &lt;- 22\n\n# Create income with linear relationship plus noise\nincome &lt;- 15000 + 4000 * education + rnorm(n_reg, mean = 0, sd = 8000)\nincome[income &lt; 10000] &lt;- 10000\n\nreg_data &lt;- data.frame(education = education, income = income)\n\n# Fit linear model\nlm_model &lt;- lm(income ~ education, data = reg_data)\n\n# Create subset of data for residual lines\nsubset_indices &lt;- sample(nrow(reg_data), 20)\nsubset_data &lt;- reg_data[subset_indices, ]\nsubset_data$predicted &lt;- predict(lm_model, newdata = subset_data)\n\n# Create regression plot\nregression_plot &lt;- ggplot(reg_data, aes(x = education, y = income)) +\n  # Add points\n  geom_point(alpha = 0.6, size = 2, color = colors[1]) +\n  \n  # Add regression line with confidence interval\n  geom_smooth(method = \"lm\", se = TRUE, color = colors[2], fill = colors[2], alpha = 0.2) +\n  \n  # Add residual lines for a subset of points to show the concept\n  geom_segment(data = subset_data,\n               aes(x = education, xend = education, \n                   y = income, yend = predicted),\n               color = colors[4], alpha = 0.5, linetype = \"dotted\") +\n  \n  # Add equation to plot (adjusted position based on data range)\n  annotate(\"text\", x = min(reg_data$education) + 1, y = max(reg_data$income) * 0.9, \n           label = paste(\"Income = $\", format(round(coef(lm_model)[1]), big.mark = \",\"), \n                        \" + $\", format(round(coef(lm_model)[2]), big.mark = \",\"), \" × Education\",\n                        \"\\nR² = \", round(summary(lm_model)$r.squared, 3), sep = \"\"),\n           hjust = 0, size = 4, fontface = \"italic\") +\n  \n  # Labels and formatting\n  scale_y_continuous(labels = dollar_format()) +\n  labs(title = \"Simple Linear Regression: Education and Income\",\n       subtitle = \"Each year of education associated with higher income\",\n       x = \"Years of Education\", \n       y = \"Annual Income\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\nprint(regression_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 5. SAMPLING ERROR AND SAMPLE SIZE\n# ==================================================\n\n# Show how standard error decreases with sample size\nset.seed(111)\nsample_sizes &lt;- c(10, 25, 50, 100, 250, 500, 1000, 2500, 5000)\nn_simulations &lt;- 1000\n\n# True population parameters\ntrue_mean &lt;- 50\ntrue_sd &lt;- 10\n\n# Run simulations for each sample size\nse_results &lt;- data.frame()\nfor (n in sample_sizes) {\n  sample_means &lt;- replicate(n_simulations, mean(rnorm(n, true_mean, true_sd)))\n  se_results &lt;- rbind(se_results, \n                      data.frame(n = n, \n                                se_empirical = sd(sample_means),\n                                se_theoretical = true_sd / sqrt(n)))\n}\n\n# Create the plot\nse_plot &lt;- ggplot(se_results, aes(x = n)) +\n  geom_line(aes(y = se_empirical, color = \"Empirical SE\"), size = 1.5) +\n  geom_point(aes(y = se_empirical, color = \"Empirical SE\"), size = 3) +\n  geom_line(aes(y = se_theoretical, color = \"Theoretical SE\"), \n            size = 1.5, linetype = \"dashed\") +\n  scale_x_log10(breaks = sample_sizes) +\n  scale_color_manual(values = c(\"Empirical SE\" = colors[1], \n                               \"Theoretical SE\" = colors[2])) +\n  labs(title = \"Standard Error Decreases with Sample Size\",\n       subtitle = \"The precision of estimates improves with larger samples\",\n       x = \"Sample Size (log scale)\", \n       y = \"Standard Error\",\n       color = \"\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"top\")\n\nprint(se_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 6. CONFIDENCE INTERVALS VISUALIZATION\n# ==================================================\n\n# Simulate multiple samples and their confidence intervals\nset.seed(999)\nn_samples &lt;- 20\nsample_size_ci &lt;- 100\ntrue_mean_ci &lt;- 50\ntrue_sd_ci &lt;- 10\n\n# Generate samples and calculate CIs\nci_data &lt;- data.frame()\nfor (i in 1:n_samples) {\n  sample_i &lt;- rnorm(sample_size_ci, true_mean_ci, true_sd_ci)\n  mean_i &lt;- mean(sample_i)\n  se_i &lt;- sd(sample_i) / sqrt(sample_size_ci)\n  ci_lower &lt;- mean_i - 1.96 * se_i\n  ci_upper &lt;- mean_i + 1.96 * se_i\n  contains_true &lt;- (true_mean_ci &gt;= ci_lower) & (true_mean_ci &lt;= ci_upper)\n  \n  ci_data &lt;- rbind(ci_data,\n                   data.frame(sample = i, mean = mean_i, \n                             lower = ci_lower, upper = ci_upper,\n                             contains = contains_true))\n}\n\n# Create CI plot\nci_plot &lt;- ggplot(ci_data, aes(x = sample, y = mean)) +\n  geom_hline(yintercept = true_mean_ci, color = \"red\", \n             linetype = \"dashed\", size = 1) +\n  geom_errorbar(aes(ymin = lower, ymax = upper, color = contains), \n                width = 0.3, size = 0.8) +\n  geom_point(aes(color = contains), size = 2) +\n  scale_color_manual(values = c(\"TRUE\" = colors[1], \"FALSE\" = colors[4]),\n                    labels = c(\"Misses true value\", \"Contains true value\")) +\n  coord_flip() +\n  labs(title = \"95% Confidence Intervals from 20 Different Samples\",\n       subtitle = paste(\"True population mean = \", true_mean_ci, \n                       \" (red dashed line)\", sep = \"\"),\n       x = \"Sample Number\", \n       y = \"Sample Mean with 95% CI\",\n       color = \"\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"bottom\")\n\nprint(ci_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 7. SAMPLING DISTRIBUTIONS (CENTRAL LIMIT THEOREM)\n# ==================================================\n\n# ---- Setup ----\nlibrary(tidyverse)\nlibrary(ggplot2)\ntheme_set(theme_minimal(base_size = 13))\nset.seed(2025)\n\n# Skewed population (Gamma); change if you want another DGP\nNpop &lt;- 100000\npopulation &lt;- rgamma(Npop, shape = 2, scale = 10)  # skewed right\nmu    &lt;- mean(population)\nsigma &lt;- sd(population)\n\n# ---- CLT: sampling distribution of the mean ----\nsample_sizes &lt;- c(1, 5, 10, 30, 100)\nB &lt;- 2000  # resamples per n\n\nclt_df &lt;- purrr::map_dfr(sample_sizes, \\(n) {\n  tibble(n = n,\n         mean = replicate(B, mean(sample(population, n, replace = TRUE))))\n})\n\n# Normal overlays: N(mu, sigma/sqrt(n))\nclt_range &lt;- clt_df |&gt;\n  group_by(n) |&gt;\n  summarise(min_x = min(mean), max_x = max(mean), .groups = \"drop\")\n\nnormal_df &lt;- clt_range |&gt;\n  rowwise() |&gt;\n  mutate(x = list(seq(min_x, max_x, length.out = 200))) |&gt;\n  unnest(x) |&gt;\n  mutate(density = dnorm(x, mean = mu, sd = sigma / sqrt(n)))\n\nclt_plot &lt;- ggplot(clt_df, aes(mean)) +\n  geom_histogram(aes(y = after_stat(density), fill = factor(n)),\n                 bins = 30, alpha = 0.6, color = \"white\") +\n  geom_line(data = normal_df, aes(x, density), linewidth = 0.8) +\n  geom_vline(xintercept = mu, linetype = \"dashed\") +\n  facet_wrap(~ n, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"CLT: Sampling distribution of the mean → Normal(μ, σ/√n)\",\n    subtitle = sprintf(\"Skewed population: Gamma(shape=2, scale=10).  μ≈%.2f, σ≈%.2f; B=%d resamples each.\", mu, sigma, B),\n    x = \"Sample mean\", y = \"Density\"\n  ) +\n  guides(fill = \"none\")\n\nclt_plot\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 8. TYPES OF SAMPLING ERROR\n# ==================================================\n\n# Create data to show random vs systematic error\nset.seed(321)\nn_measurements &lt;- 100\ntrue_value &lt;- 50\n\n# Random error only\nrandom_error &lt;- rnorm(n_measurements, mean = true_value, sd = 5)\n\n# Systematic error (bias) only\nsystematic_error &lt;- rep(true_value + 10, n_measurements) + rnorm(n_measurements, 0, 0.5)\n\n# Both errors\nboth_errors &lt;- rnorm(n_measurements, mean = true_value + 10, sd = 5)\n\nerror_data &lt;- data.frame(\n  measurement = 1:n_measurements,\n  `Random Error Only` = random_error,\n  `Systematic Error Only` = systematic_error,\n  `Both Errors` = both_errors\n) %&gt;%\n  pivot_longer(-measurement, names_to = \"Error_Type\", values_to = \"Value\")\n\n# Create error visualization\nerror_plot &lt;- ggplot(error_data, aes(x = measurement, y = Value, color = Error_Type)) +\n  geom_hline(yintercept = true_value, linetype = \"dashed\", size = 1, color = \"black\") +\n  geom_point(alpha = 0.6, size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 1.2) +\n  facet_wrap(~Error_Type, nrow = 1) +\n  scale_color_manual(values = colors[1:3]) +\n  labs(title = \"Random Error vs Systematic Error (Bias)\",\n       subtitle = paste(\"True value = \", true_value, \" (black dashed line)\", sep = \"\"),\n       x = \"Measurement Number\", \n       y = \"Measured Value\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"none\")\n\nprint(error_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 9. DEMOGRAPHIC PYRAMID\n# ==================================================\n\n# Create age pyramid data\nset.seed(777)\nage_groups &lt;- c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \n               \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \n               \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80+\")\n\n# Create data for a developing country pattern\nmale_pop &lt;- c(12, 11.5, 11, 10.5, 10, 9.5, 9, 8.5, 8, 7.5, 7, \n             6, 5, 4, 3, 2, 1.5)\nfemale_pop &lt;- c(11.8, 11.3, 10.8, 10.3, 9.8, 9.3, 8.8, 8.3, 7.8, \n               7.3, 6.8, 5.8, 4.8, 3.8, 2.8, 2.2, 2)\n\npyramid_data &lt;- data.frame(\n  Age = factor(rep(age_groups, 2), levels = rev(age_groups)),\n  Population = c(-male_pop, female_pop),  # Negative for males\n  Sex = c(rep(\"Male\", length(male_pop)), rep(\"Female\", length(female_pop)))\n)\n\n# Create population pyramid\npyramid_plot &lt;- ggplot(pyramid_data, aes(x = Age, y = Population, fill = Sex)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  scale_y_continuous(labels = function(x) paste0(abs(x), \"%\")) +\n  scale_fill_manual(values = c(\"Male\" = colors[1], \"Female\" = colors[3])) +\n  coord_flip() +\n  labs(title = \"Population Pyramid\",\n       subtitle = \"Age and sex distribution (typical developing country pattern)\",\n       x = \"Age Group\", \n       y = \"Percentage of Population\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"top\")\n\nprint(pyramid_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 10. REGRESSION RESIDUALS AND DIAGNOSTICS\n# ==================================================\n\n# Use the previous regression model for diagnostics\nreg_diagnostics &lt;- data.frame(\n  fitted = fitted(lm_model),\n  residuals = residuals(lm_model),\n  standardized_residuals = rstandard(lm_model),\n  education = reg_data$education,\n  income = reg_data$income\n)\n\n# Create diagnostic plots\n# 1. Residuals vs Fitted\np_resid_fitted &lt;- ggplot(reg_diagnostics, aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.5, color = colors[1]) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = colors[2], size = 0.8) +\n  labs(title = \"Residuals vs Fitted Values\",\n       subtitle = \"Check for homoscedasticity\",\n       x = \"Fitted Values\", y = \"Residuals\")\n\n# 2. Q-Q plot\np_qq &lt;- ggplot(reg_diagnostics, aes(sample = standardized_residuals)) +\n  stat_qq(color = colors[1]) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Normal Q-Q Plot\",\n       subtitle = \"Check for normality of residuals\",\n       x = \"Theoretical Quantiles\", y = \"Standardized Residuals\")\n\n# 3. Histogram of residuals\np_hist_resid &lt;- ggplot(reg_diagnostics, aes(x = residuals)) +\n  geom_histogram(bins = 30, fill = colors[3], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Distribution of Residuals\",\n       subtitle = \"Should be approximately normal\",\n       x = \"Residuals\", y = \"Frequency\")\n\n# 4. Residuals vs Predictor\np_resid_x &lt;- ggplot(reg_diagnostics, aes(x = education, y = residuals)) +\n  geom_point(alpha = 0.5, color = colors[4]) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = colors[2], size = 0.8) +\n  labs(title = \"Residuals vs Predictor\",\n       subtitle = \"Check for patterns\",\n       x = \"Education (years)\", y = \"Residuals\")\n\n# Combine diagnostic plots\ndiagnostic_plots &lt;- (p_resid_fitted + p_qq) / (p_hist_resid + p_resid_x)\nprint(diagnostic_plots)\n\n\n\n\n\n\n\n# ==================================================\n# 11. SAVE ALL PLOTS (Optional)\n# ==================================================\n\n# Uncomment to save plots as high-resolution images\n# ggsave(\"population_sample.png\", population_sample_plot, width = 10, height = 8, dpi = 300)\n# ggsave(\"distributions.png\", distributions_plot, width = 12, height = 8, dpi = 300)\n# ggsave(\"normal_distribution.png\", normal_plot, width = 10, height = 6, dpi = 300)\n# ggsave(\"regression.png\", regression_plot, width = 10, height = 7, dpi = 300)\n# ggsave(\"standard_error.png\", se_plot, width = 10, height = 6, dpi = 300)\n# ggsave(\"confidence_intervals.png\", ci_plot, width = 10, height = 8, dpi = 300)\n# ggsave(\"central_limit_theorem.png\", clt_plot, width = 14, height = 5, dpi = 300)\n# ggsave(\"error_types.png\", error_plot, width = 14, height = 5, dpi = 300)\n# ggsave(\"population_pyramid.png\", pyramid_plot, width = 8, height = 8, dpi = 300)\n# ggsave(\"regression_diagnostics.png\", diagnostic_plots, width = 12, height = 10, dpi = 300)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-b-central-limit-theorem-clt",
    "href": "chapter1.html#appendix-b-central-limit-theorem-clt",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.25 Appendix B: Central Limit Theorem (CLT)",
    "text": "1.25 Appendix B: Central Limit Theorem (CLT)\nThe Central Limit Theorem states that the distribution of sample means approaches a normal distribution as sample size increases, regardless of the shape of the original population distribution.\n\nKey Insights\n\nSample Size Threshold: Sample sizes of n ≥ 30 are typically sufficient for the CLT to apply\nStandard Error: The standard deviation of sample means equals σ/√n, where σ is the population standard deviation\nStatistical Foundation: We can make inferences about population parameters using normal distribution properties",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#visual-demonstration-step-by-step-progression",
    "href": "chapter1.html#visual-demonstration-step-by-step-progression",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.26 Visual Demonstration: Step-by-Step Progression",
    "text": "1.26 Visual Demonstration: Step-by-Step Progression\nThe most effective approach to understanding CLT is to observe the systematic transformation of the distribution as the number of dice increases. Beginning with 1 die (uniform distribution), we can observe how increasing the sample size gradually transforms the distribution into a normal distribution.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n\nThe Progressive Transformation\n\n# Sample sizes to demonstrate\nsample_sizes &lt;- c(1, 2, 5, 10, 30, 50)\nnum_simulations &lt;- 10000\n\n# Simulate for each sample size\nall_data &lt;- data.frame()\n\nfor (n in sample_sizes) {\n  means &lt;- replicate(num_simulations, {\n    dice &lt;- sample(1:6, n, replace = TRUE)\n    mean(dice)\n  })\n  \n  temp_df &lt;- data.frame(\n    mean = means,\n    n = n,\n    label = paste(n, ifelse(n == 1, \"die\", \"dice\"))\n  )\n  all_data &lt;- rbind(all_data, temp_df)\n}\n\n# Create ordered factor\nall_data$label &lt;- factor(all_data$label, \n                         levels = paste(sample_sizes, \n                                       ifelse(sample_sizes == 1, \"die\", \"dice\")))\n\n# Plot the progression\nggplot(all_data, aes(x = mean)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 bins = 40, fill = \"#3b82f6\", color = \"white\", alpha = 0.7) +\n  facet_wrap(~label, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"Central Limit Theorem: Step-by-Step Progression\",\n    subtitle = sprintf(\"Each panel shows %s simulations demonstrating the convergence to normality\", \n                      format(num_simulations, big.mark = \",\")),\n    x = \"Mean Value\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    strip.text = element_text(face = \"bold\", size = 12),\n    strip.background = element_rect(fill = \"#f0f0f0\", color = NA)\n  )\n\n\n\n\n\n\n\n\n\nAnalysis of Progressive Stages:\n\n1 die: Uniform (discrete) distribution - all values 1 to 6 equally probable\n2 dice: Triangular tendency - central values more frequent\n5 dice: Emergent bell-shaped pattern - observable clustering around 3.5\n10 dice: Distinctly normal - narrow Gaussian curve forming\n30 dice: Normal distribution - practical demonstration of CLT\n50 dice: Near-ideal normal distribution - strong concentration around mean\n\nThe distribution exhibits decreasing variability and increasingly pronounced bell-shaped characteristics as n increases.\n\n\n\nComparative Analysis\nA cleaner comparison of key developmental stages:\n\nkey_sizes &lt;- all_data %&gt;%\n  filter(n %in% c(1, 2, 5, 10, 30))\n\nggplot(key_sizes, aes(x = mean)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 bins = 40, fill = \"#3b82f6\", color = \"white\", alpha = 0.7) +\n  facet_wrap(~label, scales = \"free_x\", nrow = 1) +\n  labs(\n    title = \"CLT Evolution: From Uniform to Normal\",\n    x = \"Mean Value\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    strip.text = element_text(face = \"bold\", size = 11),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\nSuperimposed Distributions\nAn alternative visualization method displaying all distributions simultaneously:\n\ncomparison_data &lt;- all_data %&gt;%\n  filter(n %in% c(1, 5, 10, 30))\n\nggplot(comparison_data, aes(x = mean, fill = label, color = label)) +\n  geom_density(alpha = 0.3, linewidth = 1.2) +\n  scale_fill_manual(values = c(\"#991b1b\", \"#ea580c\", \"#ca8a04\", \"#16a34a\")) +\n  scale_color_manual(values = c(\"#991b1b\", \"#ea580c\", \"#ca8a04\", \"#16a34a\")) +\n  labs(\n    title = \"CLT Progression: Superimposed Distributions\",\n    subtitle = \"Systematic narrowing and convergence to normal form\",\n    x = \"Mean Value\",\n    y = \"Density\",\n    fill = \"Sample Size\",\n    color = \"Sample Size\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nKey Observation: As sample size increases, the distribution exhibits:\n\nIncreased symmetry (bell-shaped form)\nGreater concentration around the population mean (3.5)\nImproved conformity to the normal distribution\n\n\n\nStandard Error Convergence\nThe dispersion (standard deviation) decreases according to the relationship SE = σ/√n:\n\nvariance_data &lt;- all_data %&gt;%\n  group_by(n, label) %&gt;%\n  summarise(\n    observed_sd = sd(mean),\n    theoretical_se = sqrt(35/12) / sqrt(n),\n    .groups = \"drop\"\n  )\n\nggplot(variance_data, aes(x = n)) +\n  geom_line(aes(y = observed_sd, color = \"Observed SD\"), \n            linewidth = 1.5) +\n  geom_point(aes(y = observed_sd, color = \"Observed SD\"), \n             size = 4) +\n  geom_line(aes(y = theoretical_se, color = \"Theoretical SE\"), \n            linewidth = 1.5, linetype = \"dashed\") +\n  geom_point(aes(y = theoretical_se, color = \"Theoretical SE\"), \n             size = 4) +\n  scale_color_manual(values = c(\"Observed SD\" = \"#3b82f6\", \n                                \"Theoretical SE\" = \"#ef4444\")) +\n  scale_x_continuous(breaks = sample_sizes) +\n  labs(\n    title = \"Standard Error Decreases as Sample Size Increases\",\n    subtitle = \"Following the SE = σ/√n relationship\",\n    x = \"Sample Size (n)\",\n    y = \"Standard Deviation / Standard Error\",\n    color = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    legend.position = \"top\",\n    legend.text = element_text(size = 11)\n  )\n\n\n\n\n\n\n\n\n\n\nNumerical Summary\n\nsummary_stats &lt;- all_data %&gt;%\n  group_by(label) %&gt;%\n  summarise(\n    n = first(n),\n    Observed_Mean = round(mean(mean), 3),\n    Observed_SD = round(sd(mean), 3),\n    Theoretical_Mean = 3.5,\n    Theoretical_SE = round(sqrt(35/12) / sqrt(first(n)), 3),\n    Range = paste0(\"[\", round(min(mean), 2), \", \", round(max(mean), 2), \"]\")\n  ) %&gt;%\n  select(-label)\n\nknitr::kable(summary_stats, \n             caption = \"Observed vs Theoretical Values Across Sample Sizes\")\n\n\nObserved vs Theoretical Values Across Sample Sizes\n\n\n\n\n\n\n\n\n\n\nn\nObserved_Mean\nObserved_SD\nTheoretical_Mean\nTheoretical_SE\nRange\n\n\n\n\n1\n3.470\n1.716\n3.5\n1.708\n[1, 6]\n\n\n2\n3.503\n1.213\n3.5\n1.208\n[1, 6]\n\n\n5\n3.494\n0.764\n3.5\n0.764\n[1, 6]\n\n\n10\n3.507\n0.537\n3.5\n0.540\n[1.7, 5.4]\n\n\n30\n3.500\n0.311\n3.5\n0.312\n[2.27, 4.63]\n\n\n50\n3.498\n0.239\n3.5\n0.242\n[2.68, 4.3]\n\n\n\n\n\nObservations:\n\nThe population mean remains constant at 3.5 (independent of sample size)\nThe standard error exhibits systematic decline as n increases (SE ∝ 1/√n)\nThe range narrows considerably with increasing sample size",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#mathematical-foundation-1",
    "href": "chapter1.html#mathematical-foundation-1",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.27 Mathematical Foundation",
    "text": "1.27 Mathematical Foundation\nFor a population with mean μ and finite variance σ²:\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\text{ as } n \\to \\infty\nStandard error of the mean:\nSE_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\nFor a fair die: μ = 3.5, σ² = 35/12 ≈ 2.917",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#key-takeaways-2",
    "href": "chapter1.html#key-takeaways-2",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.28 Key Takeaways",
    "text": "1.28 Key Takeaways\n\nInitial Condition: A single die exhibits a uniform (discrete) distribution\nProgressive Transformation: As the number of observations increases, the distribution shape systematically evolves\nConvergence to Normality: At n=30, a distinct normal distribution is observable\nVariance Reduction: The distribution demonstrates increasing concentration around the expected value\nUniversality: The theorem applies to any population distribution with finite variance",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#practical-significance",
    "href": "chapter1.html#practical-significance",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.29 Practical Significance",
    "text": "1.29 Practical Significance\nThis distributional transformation enables:\n\nApplication of normal distribution tables and properties for statistical inference\nConstruction of confidence intervals with specified confidence levels\nExecution of hypothesis tests (t-tests, z-tests)\nFormulation of predictions about sample means with known probability\n\nEssential Property of CLT: Although individual die rolls follow a uniform distribution, the distribution of means from multiple dice converges asymptotically to a normal distribution in a predictable manner consistent with mathematical theory, providing the foundation for classical statistical inference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html",
    "href": "rozdzial1.html",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "",
    "text": "2.1 Wprowadzenie\nStatystyka jest sposobem poznawania świata na podstawie danych. Uczy nas, jak mądrze zbierać dane, dostrzegać wzorce, szacować parametry (cechy) populacyjne i dokonywać prognoz — określając, jak bardzo możemy się mylić.\nStatystyka i demografia to powiązane ze sobą dyscypliny, które dostarczają narzędzi do zrozumienia populacji, ich charakterystyk i wzorców wyłaniających się z danych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wprowadzenie",
    "href": "rozdzial1.html#wprowadzenie",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "",
    "text": "Statystyka to nauka o uczeniu się z danych (the science of learning from data) w warunkach niepewności.\n\n\n\n\n\n\n\n\nNote\n\n\n\nStatystyka to nauka o zbieraniu, organizowaniu, analizowaniu, interpretowaniu i prezentowaniu danych. Obejmuje zarówno metody pracy z danymi, jak i teoretyczne podstawy uzasadniające te metody.\nAle statystyka to coś więcej niż tylko liczby i wzory — to sposób myślenia o niepewności i zmienności w otaczającym nas świecie.\n\n\n\n\nDemografia to nauka zajmująca się badaniem ludności, koncentrująca się na jej wielkości, strukturze, rozmieszczeniu i zmianach zachodzących w czasie. To zasadniczo analiza statystyczna populacji - kim są ludzie, gdzie mieszkają, ilu ich jest i jak te charakterystyki ewoluują.\n\n\n\n\n\n\n\n\nZaokrąglenia i notacja naukowa\n\n\n\nZasada główna: O ile nie podano inaczej, części ułamkowe liczb dziesiętnych zaokrąglaj do co najmniej 2 cyfr znaczących. W statystyce często pracujemy z długimi częściami ułamkowymi i bardzo małymi liczbami — w obliczeniach, nie zaokrąglaj nadmiernie w krokach pośrednich, zaokrąglaj na końcu obliczeń.\n\nZaokrąglanie w kontekście statystycznym\nCzęść ułamkowa to cyfry po przecinku dziesiętnym. W statystyce szczególnie ważne jest zachowanie odpowiedniej precyzji:\nStatystyki opisowe:\n\nŚrednia: \\bar{x} = 15.847693... \\rightarrow 15.85\nOdchylenie standardowe: s = 2.7488... \\rightarrow 2.75\nWspółczynnik korelacji: r = 0.78432... \\rightarrow 0.78\n\nBardzo małe liczby (p-wartości, prawdopodobieństwa):\n\np = 0.000347... \\rightarrow 0.00035 lub 3.5 \\times 10^{-4}\nP(X &gt; 2) = 0.0000891... \\rightarrow 0.000089 lub 8.9 \\times 10^{-5}\n\n\n\nCyfry znaczące w części ułamkowej\nW części ułamkowej cyfry znaczące to wszystkie cyfry oprócz zer wiodących:\n\n.78432 ma 5 cyfr znaczących → zaokrąglamy do .78 (2 c.z.)\n.000347 ma 3 cyfry znaczące → zaokrąglamy do .00035 (2 c.z.)\n.050600 ma 4 cyfry znaczące → zaokrąglamy do .051 (2 c.z.)\n\n\n\nZasady zaokrąglania w statystyce\n\nZaokrąglaj tylko część ułamkową do co najmniej 2 cyfr znaczących\nCzęść całkowita pozostaje niezmieniona\nW długich obliczeniach zachowuj 3-4 cyfry w części ułamkowej do ostatniego kroku\nNIGDY nie zaokrąglaj do zera - małe wartości mają znaczenie interpretacyjne\nDla bardzo małych liczb używaj notacji naukowej gdy to ułatwia odczyt\nP-wartości często wymagają większej precyzji — zachowaj 2-3 cyfry znaczące\n\n\n\nNotacja naukowa w statystyce\nW statystyce często spotykamy bardzo małe liczby. Używaj notacji naukowej gdy ułatwia to odczyt:\nP-wartości i prawdopodobieństwa:\n\np = 0.000347 = 3.47 \\times 10^{-4} (lepiej: 3.5 \\times 10^{-4})\nP(Z &gt; 3.5) = 0.000233 = 2.33 \\times 10^{-4}\n\nDuże liczby (rzadko w podstawowej statystyce):\n\nN = 1\\,234\\,567 = 1.23 \\times 10^6\n\nWątpliwości: Lepiej zachować dodatkową cyfrę niż zaokrąglić zbyt mocno\n\n\n\n\n\n\n\n\n\n\nPo Co Statystyka w Naukach Społecznych i Politologii (w tym SM)?\n\n\n\nStatystyka jest niezbędna w naukach społecznych i politologii z kilku kluczowych powodów:\nRozumienie Zjawisk Społecznych: Mierzenie nierówności, ubóstwa, bezrobocia, uczestnictwa politycznego; opisywanie wzorców demograficznych i trendów społecznych; kwantyfikowanie postaw, przekonań i zachowań w populacjach.\nTestowanie Teorii: Politolodzy tworzą teorie na temat demokracji, zachowań wyborczych, konfliktów i instytucji. Socjolodzy rozwijają teorie dotyczące mobilności społecznej, nierówności i dynamiki grupowej. Statystyka pozwala nam testować, czy te teorie odpowiadają rzeczywistości.\nWnioskowanie Przyczynowe (Causal Inference): Naukowcy społeczni chcą odpowiadać na pytania “dlaczego”—Czy wykształcenie zwiększa dochody? Czy demokracje rzadziej prowadzą wojny? Czy media społecznościowe wpływają na polaryzację polityczną? Statystyka pomaga odróżnić przyczynowość od zwykłej korelacji.\nEwaluacja Polityk (Policy): Ocena, czy interwencje (programy, polityki publiczne) działają—Czy program szkolenia zawodowego zmniejsza bezrobocie? Czy reforma wyborcza zwiększyła frekwencję? Czy programy walki z ubóstwem są skuteczne? Statystyka dostarcza narzędzi do oceny tego, co działa, a co nie.\nBadania Opinii Publicznej: Sondaże wyborcze i prognozy; mierzenie poparcia społecznego dla polityk; zrozumienie, jak opinie różnią się w grupach demograficznych; śledzenie zmian postaw w czasie.\nDokonywanie Uogólnień: Nie możemy przepytać wszystkich, więc pobieramy próbę (sample) i używamy statystyki do wnioskowania o całych populacjach. Ankieta wśród 1000 osób może nam powiedzieć coś o narodzie liczącym miliony (z oszacowaną niepewnością).\nRadzenie Sobie ze Złożonością: Społeczności ludzkie są skomplikowane—wiele czynników wzajemnie się warunkuje. Statystyka pomaga nam kontrolować zmienne zakłócające (confounding variables), izolować konkretne efekty (reguła ceteris paribus) i rozumieć wielowymiarowe zależności.\nUnikalność Nauk Społecznych: W przeciwieństwie do nauk przyrodniczych, nauki społeczne badają ludzkie zachowania, które są bardzo zmienne i zależne od kontekstu. Statystyka dostarcza narzędzi do znajdowania wzorców i wyciągania wniosków pomimo tej niepewności.\n\n\n\n\nPracując z danymi, statystycy stosują dwa różne podejścia: eksplorację i konfirmację/weryfikację (wnioskowanie statystyczne). Najpierw badamy dane, aby zrozumieć ich charakterystykę i zidentyfikować wzorce. Następnie używamy formalnych metod do testowania konkretnych hipotez i wyciągania wniosków.\n\n\n\n\n\n\n\nProcent vs punkty procentowe (pp)\n\n\n\nGdy w mediach słyszysz, że „bezrobocie spadło o 2”, czy chodzi o 2 punkty procentowe (pp), czy 2 procent?\nTo nie to samo:\n\n2 pp (zmiana absolutna): np. 10% → 8% (−2 pp).\n2% (zmiana względna): mnożymy starą stopę przez 0,98; np. 10% → 9,8% (−0,2 pp).\n\nZawsze pytaj:\n\nJaka jest wartość bazowa (wcześniejsza stopa)?\nCzy to zmiana absolutna (pp), czy względna (%)?\nCzy różnica może wynikać z błędu losowego / błędu próby?\nJak mierzono bezrobocie (badanie ankietowe vs dane administracyjne), kiedy i kogo uwzględniono?\n\nProsta zasada\n\nUżywaj punktów procentowych (pp), gdy porównujesz stopy/procenty wprost (bezrobocie, frekwencja).\nUżywaj procentów (%) dla zmian względnych (względem wartości wyjściowej).\n\nMała ściąga\n\n\n\n\n\n\n\n\nStopa początkowa\n„Spadek o 2%” (względny)\n„Spadek o 2 pp” (absolutny)\n\n\n\n\n6%\n6% × 0,98 = 5,88% (−0,12 pp)\n4%\n\n\n8%\n8% × 0,98 = 7,84% (−0,16 pp)\n6%\n\n\n10%\n10% × 0,98 = 9,8% (−0,2 pp)\n8%\n\n\n\nUwaga: 2% ≠ 2 punkty procentowe (pp).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#eksploracyjna-analiza-danych-eda---exploratory-data-analysis",
    "href": "rozdzial1.html#eksploracyjna-analiza-danych-eda---exploratory-data-analysis",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.2 Eksploracyjna Analiza Danych (EDA - Exploratory Data Analysis)",
    "text": "2.2 Eksploracyjna Analiza Danych (EDA - Exploratory Data Analysis)\nCzym jest EDA? Eksploracyjna Analiza Danych to początkowy etap, w którym systematycznie badamy dane, aby zrozumieć ich strukturę i charakterystykę. Ta faza nie obejmuje formalnego testowania hipotez statystycznych—koncentruje się na odkrywaniu tego, co dane zawierają.\nPo co przeprowadzamy EDA?\n\nWykrycie nieoczekiwanych wzorców i zależności\nIdentyfikacja wartości odstających (outliers) i problemów z jakością danych\nSprawdzenie założeń do późniejszego modelowania (wiele metod statystycznych ma określone wymagania dotyczące danych, aby działały prawidłowo. EDA pomaga sprawdzić, czy nasze dane spełniają te wymagania; np. normalność rozkładu, liniowość, “outliers”, jednorodność wariancji)\nGenerowanie hipotez wartych przetestowania\nZrozumienie struktury i charakterystyki zbioru danych\n\n\n\n\n\n\n\nPodejście EDA\n\n\n\nPrzeprowadzając EDA, zaczynamy bez z góry określonych hipotez. Zamiast tego badamy dane z wielu perspektyw, aby odkryć wzorce i wygenerować pytania do dalszych badań.\n\n\n\nNarzędzia do Eksploracji Danych\n1. Statystyki Opisowe (Descriptive Statistics)\nSą to podstawowe obliczenia, które opisują nasze dane:\nMiary Tendencji Centralnej - gdzie znajduje się centrum (średnia, “wartość typowa/oczekiwana”) danych?\n\nŚrednia arytmetyczna (Mean): Suma wszystkich wartości podzielona przez ich liczbę. Przykład: Jeśli 5 studentów uzyskało na teście 70, 80, 85, 90 i 100 punktów, średnia wynosi 85.\nMediana (Median): Wartość środkowa, gdy ustawimy wszystkie liczby od najmniejszej do największej. W naszym przykładzie mediana również wynosi 85.\nModa (Mode): Wartość występująca najczęściej. Jeśli dziesięć rodzin ma 1, 2, 2, 2, 2, 3, 3, 3, 4 i 5 dzieci, modą są 2 dzieci.\n\nMiary Zmienności (Measures of Variability) - jak bardzo rozproszone są dane?\n\nRozstęp (Range): Różnica między największą a najmniejszą wartością. Jeśli wiek studentów wynosi od 18 do 24 lat, rozstęp to 6 lat.\nOdchylenie Standardowe (Standard Deviation): Pokazuje, jak bardzo dane są rozproszone wokół średniej. Małe odchylenie standardowe oznacza, że większość wartości jest blisko średniej; duże oznacza większe rozproszenie.\n\n2. Wizualizacja Danych\nMetody graficzne pomagają ujawnić wzorce, których same podsumowania numeryczne mogą nie pokazać:\n\nPiramidy Wieku (Population Pyramids): Pokazują rozkład wieku i płci w populacji\nWykresy Pudełkowe (Box Plots): Pokazują środek danych i pomagają wykryć wartości nietypowe\nWykresy Rozrzutu (Scatter Plots): Pokazują związki między dwiema zmiennymi (np. godziny nauki a wyniki testów)\nWykresy Szeregów Czasowych (Time Series Graphs): Pokazują zmiany w czasie (np. temperatura w ciągu roku)\nHistogramy (Histograms): Histogram to graficzna reprezentacja danych, która pokazuje rozkład częstości zbioru danych. Składa się z przylegających do siebie słupków (bez przerw między nimi), gdzie każdy słupek reprezentuje przedział wartości (nazywany przedziałem klasowym), a wysokość słupka pokazuje, jaka część danych mieści się w tym przedziale. Histogramy służą do wizualizacji kształtu, rozrzutu i tendencji centralnej danych liczbowych.\n\n\n\n\nhttps://commons.wikimedia.org/wiki/File:%C5%81%C3%B3d%C5%BA_population_pyramid.svg\n\n\n3. Poszukiwanie Zależności:\n\nCzy dwie zmienne zmieniają się razem? (Kiedy jedna rośnie, czy druga też rośnie?)\nCzy można dopasować linię (linię regresji) do danych?\nCzy widoczne są jakieś wyraźne wzorce lub trendy?\n\n\n\n\n\n\n\n\nWykorzystanie Tych Samych Technik do Różnych Celów\n\n\n\nWiele technik statystycznych służy zarówno celom eksploracyjnym, jak i konfirmacyjnym:\nEksploracja: Obliczamy korelacje (correlations) lub dopasowujemy linie regresji (regression lines), aby zrozumieć, jakie zależności istnieją w danych. Koncentrujemy się na odkrywaniu wzorców.\nKonfirmacja: Stosujemy testy statystyczne, aby określić, czy zaobserwowane wzorce są istotne statystycznie, czy mogły wystąpić przypadkowo. Koncentrujemy się na formalnym testowaniu hipotez.\nTa sama technika może służyć różnym celom w zależności od fazy badania.\n\n\n4. Ważne Pytania do Zadania Podczas Eksploracji:\n\nJaki jest kształt rozkładu danych?\nCzy są wartości odstające?\nCzy widać jakieś wzorce?\nCzy brakuje jakichś danych?\nCzy różne grupy wykazują różne wzorce?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wnioskowanie-statystyczne-inferential-statistics",
    "href": "rozdzial1.html#wnioskowanie-statystyczne-inferential-statistics",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.3 Wnioskowanie Statystyczne (Inferential Statistics)",
    "text": "2.3 Wnioskowanie Statystyczne (Inferential Statistics)\nPo zbadaniu danych możemy chcieć wyciągnąć formalne wnioski. Wnioskowanie statystyczne (inferential statistics) nam to umożliwia.\nPodstawowa Idea: Mamy dane z pewnej grupy osób (próba, sample), ale chcemy wiedzieć coś o wszystkich (populacja, population). Wnioskowanie statystyczne pomaga nam wyciągać wnioski o większej grupie na podstawie mniejszej grupy.\n\n\n\n\n\n\nNote\n\n\n\nPróba losowa wymaga, aby każdy element populacji miał znane, niezerowe prawdopodobieństwo zostania wybranym, niekoniecznie równe.\nGdy każdy element ma równe prawdopodobieństwo wyboru, nazywamy to konkretnie prostą próbą losową - jest to najbardziej podstawowy typ.\n\n\n\n\n\n\n\n\nWnioskowanie z próby o cechach populacji: Analogia „próbowania zupy”\n\n\n\n\nRozważmy kucharza przygotowującego zupę dla 100 osób, który musi ocenić jej smak bez konsumowania całego garnka:\nPopulacja: Cały garnek zupy (100 porcji)\nPróba: Jedna łyżka do spróbowania\nParametr populacji: Prawdziwy średni poziom słoności całego garnka (nieznany)\nStatystyka z próby: Poziom słoności wykryty w łyżce (“estymacja punktowa”)\nWnioskowanie statystyczne: Używanie charakterystyk łyżki do wyciągania wniosków o całym garnku\n\nWażne\n1. Losowe próbkowanie jest niezbędne: Kucharz musi dokładnie wymieszać zupę przed pobraniem próbki. Konsekwentne pobieranie próbek z powierzchni może pominąć przyprawy, które osiadły, wprowadzając systematyczne obciążenie.\n2. Wielkość próby wpływa na precyzję: Większa łyżka dostarcza bardziej wiarygodnych informacji o ogólnym smaku niż mały łyk, choć np. koszty i czas ograniczają możliwość dowolnego zwiększania próby.\n3. Niepewność jest nieodłączna: Nawet przy właściwej technice próbkowania (sampling), łyżka może nie reprezentować idealnie charakterystyk całego garnka.\n4. Systematyczne obciążenie/stronniczość (bias) podważa wnioskowanie: Jeśli ktoś potajemnie doda sól tylko do obszaru próbkowania, wnioski o całym garnku stają się nieważne — ilustrując, jak obciążenie próbkowania zniekształca wnioskowanie statystyczne.\n5. Wnioskowanie ma ograniczenia zakresu: Próba może oszacować średnią słoność, ale nie może ujawnić, czy niektóre części są bardziej słone niż inne, podkreślając granice tego, co próby mogą nam powiedzieć o zmienności w populacji.\nTa analogia chwyta istotę rozumowania statystycznego: używanie starannie wybranych prób do poznawania większych populacji przy jednoczesnym jawnym uznawaniu i kwantyfikacji nieodłącznej niepewności w tym procesie.\n\n\n\n\n\nMyślenie Statystyczne\n\nScenariusz Badawczy\nWładze uniwersytetu rozważają udostępnienie biblioteki całodobowo. Administracja potrzebuje odpowiedzi na pytanie: Jaka część studentów popiera tę zmianę?\n\n\n\n\n\n\nPodstawowy Problem\n\n\n\nSytuacja idealna: Zapytanie wszystkich 20 000 studentów → Uzyskanie dokładnej odpowiedzi\nSytuacja rzeczywista: Ankietowanie 100 studentów → Uzyskanie oszacowania z niepewnością\n\n\n\n\n\nDwa Podejścia do Tych Samych Danych\nZałóżmy, że przeprowadzono ankietę wśród 100 losowo wybranych studentów i stwierdzono, że 60 z nich popiera całodobowe otwarcie biblioteki.\n\n\n❌ Bez Myślenia Statystycznego\n“60 ze 100 studentów odpowiedziało twierdząco.”\nWniosek: “Dokładnie 60% wszystkich studentów popiera zmianę.”\nDecyzja: “Ponieważ przekracza to 50%, mamy wyraźne poparcie większości.”\nProblem: Ignorowanie faktu, że inna próba mogłaby dać wynik 55% lub 65%\n\n✅ Z Zastosowaniem Myślenia Statystycznego\n“60 ze 100 studentów odpowiedziało twierdząco.”\nWniosek: “Szacujemy poparcie na poziomie 60% z marginesem błędu ±10 pp.”\nDecyzja: “Prawdziwe poparcie prawdopodobnie mieści się między 50% a 70% — potrzebujemy większej próby dla pewności większościowego poparcia.”\nPrzewaga: Uznanie niepewności prowadzi do lepszych decyzji\n\n\n\n\n\nWielkość Próby a Poziom Niepewności (random error)\nZałóżmy, że losujemy tzw. próbę prostą o liczebności n=1000 wyborców i obserwujemy \\hat p = 0{,}55 (tj. 55% poparcia dla kandydatki A). Wówczas:\n\nNaszą najlepszą, jednowartościową oceną odsetka w populacji wyborców p jest \\hat p = 0{,}55.\nOrientacyjny „95\\% zakres wartości plauzybilnych” wokół \\hat p można przybliżyć wzorem \\hat p \\pm \\text{Margin of (Random) Error}, tj., \n\\hat p \\,\\pm\\, 2\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\n\\;=\\;\n0{,}55 \\,\\pm\\, 2\\sqrt{\\frac{0{,}55\\cdot 0{,}45}{1000}}\n\\approx\n0{,}55 \\pm 0{,}031,\n czyli w przybliżeniu 52\\%\\text{–}58\\% (około \\pm 3{,}1 punktu procentowego, pp).\nSzerokość tego zakresu maleje wraz z liczebnością próby: \n\\text{MoE} \\;\\propto\\; \\frac{1}{\\sqrt{n}}.\n Przykładowo, zwiększenie n z 1000 do 4000 mniej więcej zmniejsza błąd o połowę (0.0158).\n\n\nWpływ wielkości próby na poziom zaufania (ufności) do wyników z próby:\n\n\n\n\n\n\n\n\n\n\nWielkość Próby\nTwój Wynik\nMargines Błędu Losowego (MoE)\nPrzedział Ufności\nInterpretacja\n\n\n\n\nn = 100\n60%\n±10 pkt. proc.\n50% do 70%\nNiepewność co do większości\n\n\nn = 400\n60%\n±5 pkt. proc.\n55% do 65%\nPrawdopodobne poparcie większości\n\n\nn = 1 000\n60%\n±3 pkt. proc.\n57% do 63%\nWyraźne poparcie większości\n\n\nn = 1 600\n60%\n±2,5 pkt. proc.\n57,5% do 62,5%\nSilne poparcie większości\n\n\nn = 10 000\n60%\n±1 pkt. proc.\n59% do 61%\nBardzo precyzyjne oszacowanie\n\n\n\nZasada Malejących Zwrotów: Zauważ, że zwiększenie próby ze 100 do 400 osób (4× więcej) zmniejsza margines błędu o połowę, ale zwiększenie z 1 600 do 10 000 osób (6× więcej) redukuje go tylko o 1,5 punktu procentowego. Aby zmniejszyć margines błędu o połowę, musisz poczwórnie zwiększyć wielkość próby.\nDlatego większość sondaży zatrzymuje się na około 1 000-1 500 respondentach — dalsze zwiększanie precyzji rzadko uzasadnia dodatkowe koszty i nakład pracy.\n\n\n\n\n\n\nNależy Pamiętać\n\n\n\nMyślenie statystyczne przekształca stwierdzenie “60 studentów odpowiedziało twierdząco” z pozornie precyzyjnej, lecz wprowadzającej w błąd informacji w rzetelną ocenę: “Z dużym prawdopodobieństwem można stwierdzić, że między 50% a 70% wszystkich studentów popiera tę propozycję.”\nTa ostrożność w formułowaniu wniosków prowadzi do lepszych decyzji.\n\n\n\n\n\n\n\n\nNote\n\n\n\nPodstawowa zasada: Statystyka nie eliminuje niepewności — pomaga nam ją mierzyć, zarządzać nią i skutecznie komunikować.\n\n\n\n\n\n\n\n\n\nPrzykład historyczny: Sondaż Literary Digest z 1936 roku\n\n\n\nLiterary Digest przeprowadził jeden z największych sondaży w historii z 2,4 miliona odpowiedzi, przewidując, że Alf Landon pokona Franklina D. Roosevelta w wyborach prezydenckich w 1936 roku.\nPomimo ogromnej wielkości próby:\nPrzewidywanie: Landon 57%, Roosevelt 43% Rzeczywisty wynik: Roosevelt 62%, Landon 38% Błąd: 25 punktów procentowych!\nCo poszło nie tak? Sondaż był wadliwy z powodu systematycznego błędu/obciążenia (bias):\nObciążenie selekcyjne (selection bias) w ramce/operacie losowania:\n\nŹródła: książki telefoniczne, rejestracje samochodów, członkostwo w klubach\nProblem: W 1936 roku źródła te nadreprezentowały bogatych Amerykanów, którzy faworyzowali Landona\nWynik: Próba systematycznie wykluczała zwolenników Roosevelta\n\nObciążenie braku odpowiedzi:\n\nTylko 24% skontaktowanych osób odpowiedziało (problem z “response rate”)\nPrawdopodobni respondenci: osoby z silnymi opiniami anty-Roosevelt\nStruktura odmów: wielu zwolenników Roosevelta nie czuło potrzeby uczestniczenia\n\nKluczowe wnioski:\n\nDuża obciążona (biased) próba jest gorsza niż mała reprezentatywna próba\nBłędy standardowe mierzą tylko błąd losowy, nie obciążenie\nWielkość próby nie może naprawić fundamentalnych problemów dotyczących nielosowego doboru próby\nReprezentatywne losowanie ma większe znaczenie niż wielkość próby\n\nTa katastrofa doprowadziła do znacznych ulepszeń w metodologii sondażowej, w tym rozwoju losowania probabilistycznego i śledzenia wskaźników odpowiedzi.\n\nWspółczesne sondaże\nDzisiejsze sondaże, choć znacznie mniejsze niż 2,4 miliona odpowiedzi Literary Digest, są zwykle znacznie dokładniejsze, ponieważ koncentrują się na:\nReprezentatywnym losowaniu: Używanie metod opartych na prawdopodobieństwie, aby zapewnić wszystkim grupom znane szanse selekcji\nWykrywanie i korekcja obciążenia: Monitorowanie wskaźników odpowiedzi w różnych grupach demograficznych i korygowanie znanych obciążeń (bias)\nKwantyfikacja niepewności: Raportowanie marginesów błędu (statystyczny błąd losowy), które uczciwie komunikują granice tego, co wiemy\nPrzykład: Współczesny sondaż 1000 losowo wybranych wyborców z 3 punktowym marginesem błędu jest zwykle znacznie bardziej wiarygodny niż masywne, ale obciążone badanie Literary Digest.\n\n\n\n\n\n\n\n\n\n\nCzym jest losowość?\n\n\n\nDoświadczenie losowe to każdy proces, którego wyniku nie można przewidzieć z pewnością, na przykład rzut monetą lub kostką. Wynik to pojedynczy możliwy rezultat tego doświadczenia—na przykład uzyskanie “orła” lub wyrzucenie “5”. Zdarzenie to zbiór jednego lub więcej wyników, którymi jesteśmy zainteresowani; może to być zdarzenie elementarne (jak wyrzucenie dokładnie 3) lub zdarzenie złożone (jak wyrzucenie liczby parzystej, co obejmuje wyniki 2, 4 i 6).\n\nPrawdopodobieństwo to sposób mierzenia, jaka jest szansa, że coś się wydarzy (miara niepewności). Jest to liczba między 0 a 1 (lub 0% i 100%), która reprezentuje szansę wystąpienia danego zdarzenia.\n\nJeśli coś ma prawdopodobieństwo 0, jest niemożliwe - nigdy się nie wydarzy. Jeśli coś ma prawdopodobieństwo 1, jest pewne - na pewno się wydarzy. Większość rzeczy mieści się gdzieś pomiędzy.\nNa przykład, gdy rzucasz uczciwą monetą, prawdopodobieństwo, że wypadnie orzeł, wynosi 0,5 (czyli 50%), ponieważ są dwa równie prawdopodobne wyniki, a orzeł jest jednym z nich.\nPrawdopodobieństwo pomaga nam zrozumieć niepewność i losowość w świecie.\n\n\nW statystyce losowość jest uporządkowanym sposobem opisywania niepewności. Choć każdy pojedynczy wynik jest nieprzewidywalny, stabilne wzorce pojawiają się po wielu powtórzeniach.\n\nPrzykład: Rzut uczciwą monetą:\n\nPojedynczy rzut: Całkowicie nieprzewidywalny—nie wiesz, czy wypadnie orzeł czy reszka\n100 rzutów: Otrzymasz blisko 50% orłów (być może 48 lub 53)\n10 000 rzutów: Prawie na pewno bardzo blisko 50% orłów (być może 49,8%)\n\nTo samo dotyczy kostki: nie możesz przewidzieć następnego rzutu, ale wykonaj 600 rzutów, a każda liczba (1–6) pojawi się około 100 razy. To przewidywalne długoterminowe zachowanie wynikające z nieprzewidywalnych pojedynczych zdarzeń jest istotą statystycznej losowości.\n\nRodzaje losowości\nLosowość epistemiczna vs. ontologiczna:\n\nEpistemiczna (wynikająca z niekompletnej wiedzy): Traktujemy wynik jako losowy, ponieważ nie wszystkie czynniki determinujące są obserwowane lub warunki nie są kontrolowane.\n\nPrzykłady: Decyzja indywidualnego respondenta w badaniu (nie znamy pełnego zestawu motywacji); błąd pomiaru w ankiecie (ograniczona precyzja, brak odpowiedzi na pytanie); rzut monetą modelowany jako losowy, ponieważ drobne, nieobserwowane różnice w warunkach początkowych determinują wynik.\n\nOntologiczna (wewnętrzna nieokreśloność): Nawet pełna wiedza nie usuwa niepewności wyniku.\n\nPrzykłady: Czas rozpadu radioaktywnego atomu; zjawiska kwantowo-mechaniczne.\n\n\n\n\nPokrewne pojęcia\nLosowość vs. bezładność: Statystyczna losowość ma strukturę matematyczną i podlega prawom prawdopodobieństwa—to uporządkowana niepewność. Bezładność sugeruje całkowity brak porządku, bez podstawowych wzorców czy reguł.\nChaos: Proces deterministyczny (rządzony stałymi regułami), który wydaje się losowy z powodu ekstremalnej wrażliwości na warunki początkowe. Systemy chaotyczne są przewidywalne w teorii, ale nieprzewidywalne w praktyce (np. pogoda).\nEntropia: Miara nieuporządkowania lub niepewności w systemie. Wysoka entropia oznacza wysoką nieprzewidywalność; niska entropia oznacza wysoki porządek. W statystyce entropia kwantyfikuje niepewność w rozkładzie prawdopodobieństwa.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#populacje-i-próby",
    "href": "rozdzial1.html#populacje-i-próby",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.4 Populacje i próby",
    "text": "2.4 Populacje i próby\nZrozumienie rozróżnienia między populacjami a próbami jest kluczowe dla właściwej analizy statystycznej. To rozróżnienie wpływa na każdy aspekt naszej analizy, od zbierania danych po interpretację wyników.\n\nPopulacja (Population)\nPopulacja to kompletny zbiór jednostek, obiektów lub pomiarów, o których chcemy wyciągnąć wnioski. Kluczowe słowo to „kompletny” — populacja obejmuje każdego pojedynczego członka grupy, którą badamy.\nPrzykłady populacji w demografii:\n\nWszyscy mieszkańcy Polski na dzień 1 stycznia 2024: Obejmuje każdą osobę mieszkającą w Polsce w tym konkretnym dniu — około 38 milionów osób.\nWszystkie urodzenia w Szwecji w 2023 roku: Każde dziecko urodzone w granicach Szwecji w tym roku kalendarzowym — około 100 000 urodzeń.\nWszystkie gospodarstwa domowe w Tokio: Każda jednostka mieszkalna, gdzie ludzie mieszkają, gotują i śpią — około 7 milionów gospodarstw.\nWszystkie zgony z powodu COVID-19 na świecie w 2020 roku: Każdy zgon, gdzie COVID-19 został wymieniony jako przyczyna — kilka milionów zgonów.\n\nPopulacje mogą być:\nSkończone (Finite): Mające policzalną liczbę członków (wszyscy obecni obywatele Polski, wszytkie gminy w Polsce w 2024 r.)\nNieskończone (Infinite): Teoretyczne lub niepoliczalnie duże (wszystkie możliwe przyszłe urodzenia)\nStałe (Fixed): Zdefiniowane w określonym punkcie czasu (wszyscy mieszkańcy w dniu spisu)\nDynamiczne (Dynamic): Zmieniające się w czasie (populacja miasta doświadczająca urodzeń, zgonów i migracji codziennie)\n\n\nPróba (Sample)\nPróba to podzbiór populacji, który jest faktycznie obserwowany lub mierzony. Badamy próby, ponieważ badanie całych populacji jest często niemożliwe, niepraktyczne lub niepotrzebne.\nDlaczego używamy prób:\nPraktyczna niemożliwość: Wyobraź sobie testowanie każdej osoby w Chinach na obecność pewnej choroby. Zanim skończyłbyś testować 1,4 miliarda ludzi, sytuacja chorobowa całkowicie by się zmieniła, a niektórzy ludzie testowani wcześnie wymagaliby ponownego testowania.\nWzględy kosztowe: Amerykański spis powszechny z 2020 roku kosztował około 16 miliardów dolarów. Przeprowadzanie tak kompletnych wyliczeń często byłoby zbyt kosztowne.\nOgraniczenia czasowe: Decydenci często potrzebują informacji szybko. Badanie ankietowe 10 000 osób można ukończyć w ciągu tygodni, podczas gdy spis wymaga lat planowania, wykonania i przetwarzania.\nPomiar destrukcyjny: Niektóre pomiary niszczą to, co jest mierzone. Testowanie żywotności żarówek wymaga użycia prób.\nWiększa dokładność: Co zaskakujące, próby mogą czasem być dokładniejsze niż badania pełne. Z próbą można pozwolić sobie na lepsze szkolenie ankieterów, bardziej staranne zbieranie danych i dokładniejsze kontrole jakości.\nPrzykład próby vs. populacja:\nPowiedzmy, że chcemy poznać średnią wielkość gospodarstwa domowego w Warszawie:\n\nPopulacja: Wszystkie 800 000 gospodarstw domowych w Warszawie\nPodejście spisowe: Próba skontaktowania się z każdym gospodarstwem (drogie, czasochłonne, niektóre zostaną pominięte)\nPodejście próbkowe: Losowo wybrać 5000 gospodarstw, dokładnie zmierzyć ich wielkości i użyć tego do oszacowania średniej dla wszystkich gospodarstw\nWynik: Próba może znaleźć średnią 2,43 osób na gospodarstwo z marginesem błędu ±0,05, co oznacza, że jesteśmy pewni, że prawdziwa średnia populacji mieści się między 2,38 a 2,48\n\n\n\n\n\n\n\n\nPrzegląd Metod Doboru Próby\n\n\n\nDobór próby polega na wyborze podzbioru z populacji w celu oszacowania jej charakterystyk. Operat losowania (lista, z której losujemy) powinien idealnie zawierać każdego członka dokładnie raz. Problemy operatu: niedobór pokrycia, nadmiar pokrycia, duplikacja i grupowanie.\n\nDobór Probabilistyczny (Możliwe Wnioskowanie Statystyczne)\n\nProsty Losowy Dobór Próby (SRS): Każda próba rozmiaru n ma równe prawdopodobieństwo. Złoty standard, ale często niepraktyczny. Każda jednostka ma prawdopodobieństwo selekcji n/N.\nDobór Systematyczny: Wybór co k-tego elementu, gdzie k = N/n. Prosty w implementacji, ale uwaga na ukrytą periodicność w operacie.\nDobór Warstwowy: Podział populacji na jednorodne warstwy, niezależne losowanie w każdej warstwie. Zapewnia reprezentację podgrup i może znacznie zwiększyć precyzję. Typy alokacji: proporcjonalna, optymalna (Neymana) lub równa.\nDobór Grupowy (Klastrowy): Wybór grup (klastrów) zamiast jednostek. Efektywny kosztowo dla populacji rozproszonych geograficznie, ale mniej wydajny niż SRS (efekt schematu: DEFF = Wariancja(klaster)/Wariancja(SRS)). Może być jedno- lub wielostopniowy.\n\n\n\nDobór Nieprobabilistyczny (Ograniczone Wnioskowanie Statystyczne)\n\nDobór Dogodny: Wybór według łatwości dostępu. Przydatny w badaniach pilotażowych/eksploracyjnych, ale prawdopodobne poważne obciążenie selekcji.\nDobór Celowy/Ekspertowy: Celowy wybór przypadków typowych, ekstremalnych lub bogatych informacyjnie. Wartościowy w badaniach jakościowych i rzadkich populacjach.\nDobór Kwotowy: Dopasowanie proporcji populacji, ale bez losowego wyboru. Szybki i tani, ale ukryte obciążenie selekcji i brak miary błędu próbkowania. (Zobacz: porażka sondaży Dewey-Truman 1948)\nDobór Kuli Śnieżnej: Uczestniczy rekrutują innych ze swoich sieci. Niezbędny dla ukrytych populacji (narkomani, nielegalni imigranci), ale obciążony w stronę dobrze połączonych jednostek.\n\nKluczowa Zasada: Dobór probabilistyczny umożliwia prawidłowe wnioskowanie statystyczne; metody nieprobabilistyczne mogą być konieczne ze względów praktycznych lub etycznych, ale ograniczają uogólnialność.\n\n\n\n\n\n\n\n\n\n\nSuperpopulacja i Proces Generowania Danych (DGP) (*)\n\n\n\n\n\nSuperpopulacja (Superpopulation)\nSuperpopulacja to teoretyczna nieskończona populacja, z której twoja skończona populacja jest traktowana jako jedna losowa próba.\nPomyśl o tym w trzech poziomach:\n\nSuperpopulacja: Nieskończony zbiór możliwych wartości (teoretyczny)\nPopulacja skończona (finite population): Rzeczywista populacja, którą teoretycznie możesz spisać (np. wszystkie 50 stanów USA, wszystkie 10 000 firm w branży)\nPróba (sample): Podzbiór, który faktycznie obserwujesz (np. 30 stanów, 500 firm)\n\nDlaczego potrzebujemy tego pojęcia?\nRozważmy 50 stanów USA. Możesz zmierzyć stopę bezrobocia dla wszystkich 50 stanów — pełny spis, bez próbkowania. Ale nadal chcesz:\n\nSprawdzić, czy bezrobocie jest powiązane z poziomem wykształcenia\nPrzewidzieć przyszłoroczne stopy bezrobocia\nOkreślić, czy różnice między stanami są „istotne statystycznie”\n\nBez koncepcji superpopulacji utkniesz — masz wszystkie dane, więc co pozostaje do wnioskowania? Odpowiedź: traktuj tegoroczne 50 wartości jako jedno losowanie z nieskończonej superpopulacji możliwych wartości, które mogłyby wystąpić w podobnych warunkach.\nReprezentacja matematyczna:\n\nWartość populacji skończonej: Y_i (stopa bezrobocia stanu i)\nModel superpopulacji: Y_i = \\mu + \\epsilon_i gdzie \\epsilon_i \\sim (0, \\sigma^2)\n50 zaobserwowanych wartości to jedna realizacja tego procesu\n\n\n\n\nProces Generowania Danych (Data Generating Process): Prawdziwa Recepta\nProces Generowania Danych (DGP) to rzeczywisty mechanizm, który tworzy twoje dane — włączając wszystkie czynniki, relacje i elementy losowe.\nIntuicyjny przykład: Załóżmy, że wyniki testów uczniów są naprawdę generowane przez:\n\\text{Wynik}_i = 50 + 2(\\text{GodzinyNauki}_i) + 3(\\text{GodzinySnu}_i) - 5(\\text{Stres}_i) + 1.5(\\text{Śniadanie}_i) + \\epsilon_i\nTo jest PRAWDZIWY DGP. Ale ty tego nie wiesz! Możesz estymować:\n\\text{Wynik}_i = \\alpha + \\beta(\\text{GodzinyNauki}_i) + u_i\nTwój model jest prostszy niż rzeczywistość. Brakuje ci zmiennych (sen, stres, śniadanie), więc twoje oszacowania mogą być obciążone (biased). Składnik u_i zawiera wszystko, co pominąłeś.\nKluczowa intuicja: Nigdy nie znamy prawdziwego DGP. Nasze modele statystyczne są zawsze przybliżeniami, próbującymi uchwycić najważniejsze części nieznanej, złożonej prawdy.\n\n\n\nDwa Podejścia do Wnioskowania Statystycznego\nAnalizując dane, szczególnie z badań czy prób, możemy przyjąć dwa filozoficzne podejścia:\n\n1. Wnioskowanie Oparte na Schemacie (Design-Based Inference)\n\nFilozofia: Wartości populacji są stałymi liczbami. Losowość pochodzi TYLKO z tego, które jednostki wylosowaliśmy.\nSkupienie: Jak wybraliśmy próbę (losowanie proste, warstwowe, gniazdowe itp.)\nPrzykład: Średni dochód hrabstw Kalifornii jest stałą liczbą. Losujemy 10 hrabstw. Nasza niepewność wynika z tego, które 10 losowo wybraliśmy.\nBez modeli: Nie zakładamy nic o rozkładzie wartości populacji\n\n\n\n2. Wnioskowanie Oparte na Modelu (Model-Based Inference)\n\nFilozofia: Same wartości populacji są realizacjami z pewnego modelu probabilistycznego (superpopulacji)\nSkupienie: Model statystyczny generujący wartości populacji\nPrzykład: Dochód każdego hrabstwa Kalifornii jest losowany z: Y_i = \\mu + \\epsilon_i gdzie \\epsilon_i \\sim N(0, \\sigma^2)\nWymagane modele: Przyjmujemy założenia o tym, jak dane zostały wygenerowane\n\nKtóre jest lepsze?\n\nDuże populacje, dobre próby losowe: Podejście oparte na schemacie działa dobrze\nMałe populacje (jak 50 stanów): Często konieczne podejście modelowe\nPełne spisanie: Tylko podejście modelowe umożliwia wnioskowanie\nWspółczesna praktyka: Często łączy oba podejścia\n\n\n\n\n\nPraktyczny Przykład: Analiza Wydatków Stanowych na Edukację\nZałóżmy, że zbierasz wydatki na edukację per uczeń dla wszystkich 50 stanów USA.\nBez myślenia superpopulacyjnego:\n\nMasz wszystkie 50 wartości — to wszystko\nŚrednia to średnia, bez niepewności\nNie możesz testować hipotez ani tworzyć prognoz\n\nZ myśleniem superpopulacyjnym:\n\nTegoroczne 50 wartości to jedna realizacja z superpopulacji\nModel: \\text{Wydatki}_i = \\mu + \\beta(\\text{DochódStanu}_i) + \\epsilon_i\nTeraz możesz:\n\nTestować, czy wydatki są powiązane z dochodem stanu (\\beta \\neq 0?)\nPrzewidywać przyszłoroczne wartości\nObliczać przedziały ufności\n\n\nKluczowa intuicja: Nawet z kompletnymi danymi, ramy superpopulacji umożliwiają wnioskowanie statystyczne poprzez traktowanie obserwowanych wartości jako jednego możliwego wyniku z podstawowego procesu stochastycznego.\n\n\n\nPodsumowanie\n\nSuperpopulacja: Traktuje twoją populację skończoną jako jedno losowanie z nieskończonej przestrzeni możliwości — niezbędne, gdy twoja populacja skończona jest mała lub całkowicie obserwowana\nDGP: Prawdziwy (nieznany) proces tworzący twoje dane — twoje modele próbują go przybliżyć",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#dane-i-rozkłady",
    "href": "rozdzial1.html#dane-i-rozkłady",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.5 Dane i rozkłady",
    "text": "2.5 Dane i rozkłady\nDane: Informacje zebrane podczas badania – obejmują odpowiedzi z ankiet, wyniki eksperymentów, wskaźniki ekonomiczne, treści z mediów społecznościowych lub wszelkie inne mierzalne obserwacje.\nZrozumienie typów danych i rozkładów jest fundamentalne dla wyboru odpowiednich analiz i poprawnej interpretacji wyników.\n\n\n\n\n\n\nRodzaje i Formaty Zbiorów Danych\n\n\n\n\nDane Przekrojowe\nObserwacje na zmiennych (kolumny w bazie danych) zebrane w jednym punkcie czasowym dla wielu podmiotów:\n\n\n\nOsoba\nWiek\nDochód\nWykształcenie\n\n\n\n\n1\n25\n5000\nLicencjat\n\n\n2\n35\n7500\nMagister\n\n\n3\n45\n9000\nDoktorat\n\n\n\n\n\nSzeregi Czasowe\nObserwacje jednego podmiotu w kolejnych punktach czasowych:\n\n\n\nRok\nPKB (w mld)\nStopa Bezrobocia\n\n\n\n\n2018\n20.580\n3,9%\n\n\n2019\n21.433\n3,7%\n\n\n2020\n20.933\n8,1%\n\n\n\n\n\nDane Panelowe (Longitudinalne)\nObserwacje wielu podmiotów w czasie:\n\n\n\nKraj\nRok\nPKB per capita\nDługość życia\n\n\n\n\nPolska\n2018\n32.794\n76,7\n\n\nPolska\n2019\n35.118\n76,8\n\n\nNiemcy\n2018\n46.194\n81,9\n\n\nNiemcy\n2019\n46.194\n82,0\n\n\n\n\n\nDane Przekrojowo-Czasowe (TSCS)\nSzczególny przypadek danych panelowych gdzie:\n\nLiczba punktów czasowych &gt; liczba podmiotów\nStruktura podobna do danych panelowych\nCzęsto stosowane w ekonomii i politologii\n\n\n\n\nFormaty Danych\n\nFormat Szeroki\nKażdy wiersz to podmiot; kolumny to zmienne/punkty czasowe:\n\n\n\nKraj\nPKB_2018\nPKB_2019\nDŻ_2018\nDŻ_2019\n\n\n\n\nPolska\n32.794\n35.118\n76,7\n76,8\n\n\nNiemcy\n46.194\n46.194\n81,9\n82,0\n\n\n\n\n\nFormat Długi\nKażdy wiersz to unikalna kombinacja podmiot-czas-zmienna:\n\n\n\nKraj\nRok\nZmienna\nWartość\n\n\n\n\nPolska\n2018\nPKB per capita\n32.794\n\n\nPolska\n2019\nPKB per capita\n35.118\n\n\nPolska\n2018\nDługość życia\n76,7\n\n\nPolska\n2019\nDługość życia\n76,8\n\n\nNiemcy\n2018\nPKB per capita\n46.194\n\n\nNiemcy\n2019\nPKB per capita\n46.194\n\n\nNiemcy\n2018\nDługość życia\n81,9\n\n\nNiemcy\n2019\nDługość życia\n82,0\n\n\n\nUwaga: Format długi jest zazwyczaj preferowany do:\n\nManipulacji danymi w R i Pythonie\nAnaliz statystycznych\nWizualizacji danych\n\n\n\n\n\n\n\nTypy danych\nDane składają się z zebranych obserwacji lub pomiarów. Typ danych określa, jakie operacje matematyczne są wykonalne i jakie metody statystyczne mają zastosowanie.\n\nDane ilościowe\nDane ciągłe mogą przyjmować dowolną wartość w przedziale:\nPrzykłady o znaczeniu demograficznym:\n\nWiek: Może wynosić 25,5 lat, 25,51 lat, 25,514 lat (precyzja ograniczona tylko dokładnością narzędzia pomiarowego)\nWskaźnik masy ciała: 23,7 kg/m²\nWspółczynnik dzietności: 1,73 dzieci na kobietę\nGęstość zaludnienia: 4521,3 osoby na km²\n\nWłaściwości:\n\nMożna wykonywać wszystkie operacje arytmetyczne\nMożna obliczać średnie, odchylenia standardowe\nCzęsto “podążają” za znanymi rozkładami prawdopodobieństwa (np. waga za rozkładem normalnym)\n\nDane dyskretne mogą przyjmować tylko określone wartości:\nPrzykłady:\n\nLiczba dzieci: 0, 1, 2, 3… (nie można mieć 2,5 dziecka)\nLiczba małżeństw: 0, 1, 2, 3…\nWielkość gospodarstwa domowego: 1, 2, 3, 4… osób\nLiczba wizyt u lekarza: 0, 1, 2, 3… rocznie\n\n\n\nDane jakościowe/kategorialne\nDane nominalne reprezentują kategorie bez naturalnego porządku:\nPrzykłady:\n\nKraj urodzenia: USA, Chiny, Indie, Brazylia…\nReligia: Chrześcijaństwo, Islam, Hinduizm, Buddyzm, Brak…\nStan cywilny: Kawaler/Panna, Żonaty/Mężatka, Rozwiedziony/a, Wdowiec/Wdowa\nPrzyczyna śmierci: Choroby serca, Rak, Wypadek, Udar…\nGrupa krwi: A, B, AB, 0\n\nCo możemy zrobić:\n\nLiczyć częstości\nObliczać proporcje\nZnaleźć modę\nTestować niezależność\n\nCzego nie możemy zrobić:\n\nObliczać średniej (średnia religia nie ma sensu)\nPorządkować kategorii znacząco\nObliczać odległości między kategoriami\n\nDane porządkowe reprezentują uporządkowane kategorie:\nPrzykłady:\n\nPoziom wykształcenia: Brak &lt; Podstawowe &lt; Średnie &lt; Wyższe\nStatus społeczno-ekonomiczny: Niski &lt; Średni &lt; Wysoki\nSamoocena zdrowia: Zły &lt; Przeciętny &lt; Dobry &lt; Doskonały\nSkala zgody: Zdecydowanie się nie zgadzam &lt; Nie zgadzam się &lt; Neutralny &lt; Zgadzam się &lt; Zdecydowanie się zgadzam\n\nUwaga: Interwały między kategoriami niekoniecznie są równe. „Odległość” od Złego do Przeciętnego zdrowia może nie równać się odległości od Dobrego do Doskonałego.\n\n\n\nRozkład Danych\nRozkład danych (data distribution) opisuje, jak wartości rozkładają się między możliwymi wynikami (jakie wartości przyjmuje zmienna i jak często). Rozkłady mówią nam, które wartości są powszechne, które są rzadkie i jakie wzorce istnieją w naszych danych.\nZrozumienie rozkładów jest fundamentalne dla statystyki, ponieważ pomaga nam podsumowywać duże zbiory danych, identyfikować wzorce i podejmować świadome decyzje.\nNa przykład, wiedza o tym, że większość studentów uzyskuje wyniki między 60-80 punktów na egzaminie, mówi nam więcej niż sama średnia.\n\nCzęstość, Częstość Względna i Gęstość\nAnalizując dane, często interesuje nas, ile razy pojawia się każda wartość (lub przedział wartości). Prowadzi nas to do trzech powiązanych pojęć:\nCzęstość (bezwzględna) (frequency) to po prostu liczba wystąpień danej wartości lub kategorii w naszych danych. Jeśli 15 studentów uzyskało wyniki między 70-80 punktów na egzaminie, częstość dla tego przedziału wynosi 15.\nCzęstość względna (relative frequency) wyraża częstość jako proporcję lub procent całości. Odpowiada na pytanie: “Jaka część wszystkich obserwacji należy do tej kategorii?” Częstość względna obliczana jest jako:\n\\text{Częstość względna} = \\frac{\\text{Częstość}}{\\text{Całkowita liczba obserwacji}}\nJeśli 15 ze 100 studentów uzyskało 70-80 punktów, częstość względna wynosi 15/100 = 0,15 lub 15%. Częstości względne zawsze sumują się do 1 (lub 100%), co czyni je użytecznymi do porównywania rozkładów o różnych liczebnościach próby.\nGęstość (density) jest podobna do częstości względnej, ale uwzględnia szerokość przedziałów. Gdy grupujemy dane ciągłe (takie jak czas czy stopa bezrobocia) w przedziały o różnych szerokościach, gęstość zapewnia uczciwe porównanie. Gęstość obliczana jest jako:\n\\text{Gęstość} = \\frac{\\text{Częstość względna}}{\\text{Szerokość przedziału}}\nGęstość jest szczególnie ważna dla zmiennych ciągłych, ponieważ zapewnia, że całkowite pole pod rozkładem równa się 1, co pozwala nam interpretować pola jako prawdopodobieństwa.\n\n\n\n\n\n\nTip\n\n\n\nPrawdopodobieństwo zdarzenia to liczba z przedziału od 0 do 1; im większe prawdopodobieństwo, tym bardziej prawdopodobne jest wystąpienie zdarzenia.\n\n\nCzęstość skumulowana (cumulative frequency) mówi nam, ile obserwacji znajduje się na danym poziomie lub poniżej niego. Zamiast pytać “ile obserwacji jest w tej kategorii?”, częstość skumulowana odpowiada na pytanie “ile obserwacji jest w tej kategorii lub w kategoriach poniżej?”. Obliczana jest przez sumowanie wszystkich częstości od najniższej wartości do bieżącej wartości włącznie.\nPodobnie, częstość względna skumulowana (cumulative relative frequency) wyraża to jako proporcję całości, odpowiadając na pytanie “jaki procent obserwacji znajduje się na tym poziomie lub poniżej?”. Na przykład, jeśli częstość względna skumulowana dla wyniku 70 wynosi 0,40, oznacza to, że 40% studentów uzyskało wynik 70 lub niższy.\n\n\nTablice Rozkładu (szereg rozdzielczy danych)\nTablica rozkładu częstości (frequency distribution table) organizuje dane, pokazując jak obserwacje rozkładają się między różnymi wartościami lub przedziałami. Oto przykład z wynikami egzaminów:\n\n\n\n\n\n\n\n\n\n\n\nPrzedział wyników\nCzęstość\nCzęstość względna\nCzęstość skumulowana\nCzęstość względna skumulowana\nGęstość\n\n\n\n\n0-50\n10\n0,10\n10\n0,10\n0,002\n\n\n50-70\n30\n0,30\n40\n0,40\n0,015\n\n\n70-90\n45\n0,45\n85\n0,85\n0,0225\n\n\n90-100\n15\n0,15\n100\n1,00\n0,015\n\n\nSuma\n100\n1,00\n-\n-\n-\n\n\n\nTa tablica pokazuje, że większość studentów uzyskała wyniki w przedziale 70-90, podczas gdy bardzo niewielu uzyskało wyniki poniżej 50 lub powyżej 90. Kolumny skumulowane pokazują nam, że 40% studentów uzyskało wyniki poniżej 70, a 85% poniżej 90.\nTakie tablice są użyteczne dla szybkiego przeglądu danych przed przeprowadzeniem bardziej złożonych analiz.\n\n\nWizualizacja Rozkładów: Histogramy\nHistogram to graficzna reprezentacja rozkładu częstości. Wyświetla dane używając słupków, gdzie:\n\nOś x pokazuje wartości lub przedziały (klasy, bins)\nOś y może pokazywać częstość, częstość względną lub gęstość\nWysokość każdego słupka reprezentuje liczbę, proporcję lub gęstość dla danego przedziału\nSłupki stykają się ze sobą (brak przerw) dla zmiennych ciągłych\n\nWybór szerokości klas: Liczba i szerokość klas znacząco wpływa na wygląd histogramu. Zbyt mało klas ukrywa ważne wzorce, podczas gdy zbyt wiele klas tworzy “szum” i utrudnia dostrzeżenie wzorców.\n\nW statystyce szum (noise) to niepożądana losowa zmienność, która przesłania wzorzec, który staramy się znaleźć. Można to porównać do trzasków w radiu — utrudniają one słyszenie muzyki (“sygnału”). W danych szum pochodzi z błędów pomiarowych, losowych fluktuacji lub naturalnej zmienności badanego zjawiska. Szum to losowa zmienność w danych, która ukrywa prawdziwe wzorce, które chcemy dostrzec, podobnie jak hałas w tle utrudnia usłyszenie rozmowy.\n\nKilka metod pomaga określić odpowiednie szerokości klas:\n\nReguła Sturgesa (Sturges’ rule): Użyj k = 1 + \\log_2(n) klas, gdzie n to liczebność próby. Działa dobrze dla w przybliżeniu symetrycznych rozkładów.\nReguła pierwiastka kwadratowego (square root rule): Użyj k = \\sqrt{n} klas. Proste, domyślne ustawienie działające w wielu przypadkach wystarczająco dobrze.\n\nW R możesz określić klasy na kilka sposobów:\n\n# Określenie liczby klas\nhist(exam_scores, breaks = 10)\n\n\n\n\n\n\n\n# Określenie dokładnych punktów podziału\nhist(exam_scores, breaks = seq(0, 100, by = 10))\n\n\n\n\n\n\n\n# Pozwól R wybrać automatycznie (domyślnie używa reguły Sturgesa)\nhist(exam_scores)\n\n\n\n\n\n\n\n\nNajlepszym podejściem jest często eksperymentowanie z różnymi szerokościami klas, aby znaleźć to, co najlepiej ujawnia wzorzec w danych. Zacznij od ustawienia domyślnego, następnie spróbuj mniej i więcej klas, aby zobaczyć, jak zmienia się obraz.\nDefiniowanie granic klas: Tworząc klasy dla tablicy częstości, musisz zdecydować, jak obsługiwać wartości, które dokładnie przypadają na granice przedziałów klasowych. Na przykład, jeśli masz klasy 0-10 i 10-20, do której klasy należy wartość 10?\nRozwiązaniem jest użycie notacji przedziałowej (interval notation), aby określić, czy każda granica jest włączona czy wyłączona:\n\nPrzedział domknięty (closed interval) [a, b] zawiera oba końce: a \\leq x \\leq b\nPrzedział otwarty (open interval) (a, b) wyklucza oba końce: a &lt; x &lt; b\nPrzedział lewostronnie domknięty (half-open interval) [a, b) zawiera lewy koniec, ale wyklucza prawy: a \\leq x &lt; b\nPrzedział prawostronnie domknięty (half-open interval) (a, b] wyklucza lewy koniec, ale zawiera prawy: a &lt; x \\leq b\n\nStandardowa konwencja: Większość oprogramowania statystycznego, włączając R, używa przedziałów lewostronnie domkniętych [a, b) dla wszystkich klas oprócz ostatniej, która jest w pełni domknięta [a, b]. Oznacza to:\n\nWartość na dolnej granicy jest włączona do klasy\nWartość na górnej granicy należy do następnej klasy\nSama ostatnia klasa zawiera obie granice, aby uchwycić wartość maksymalną\n\nNa przykład, dla klas 0-20, 20-40, 40-60, 60-80, 80-100:\n\n\n\nPrzedział wyników\nNotacja przedziałowa\nZawarte wartości\n\n\n\n\n0-20\n[0, 20)\n0 ≤ wynik &lt; 20\n\n\n20-40\n[20, 40)\n20 ≤ wynik &lt; 40\n\n\n40-60\n[40, 60)\n40 ≤ wynik &lt; 60\n\n\n60-80\n[60, 80)\n60 ≤ wynik &lt; 80\n\n\n80-100\n[80, 100]\n80 ≤ wynik ≤ 100\n\n\n\nTa konwencja zapewnia, że:\n\nKażda wartość jest liczona dokładnie raz (bez podwójnego liczenia)\nŻadne wartości nie przepadają\nKlasy w pełni pokrywają cały zakres\n\nPrzedstawiając tablice częstości w raportach, możesz po prostu napisać “0-20, 20-40, …” i zaznaczyć, że klasy są lewostronnie domknięte, lub jawnie pokazać notację przedziałową, jeśli precyzja jest ważna.\nHistogram częstości pokazuje surowe liczebności:\n\n# Przykład kodu R\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     main = \"Rozkład wyników egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Częstość\",\n     col = \"lightblue\")\n\n\n\n\n\n\n\n\nHistogram częstości względnej pokazuje proporcje (użyteczne przy porównywaniu grup o różnych liczebnościach):\n\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     freq = FALSE,  # Tworzy histogram częstości względnej/gęstości\n     main = \"Rozkład wyników egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Częstość względna\",\n     col = \"lightgreen\")\n\n\n\n\n\n\n\n\nHistogram gęstości dostosowuje się do szerokości przedziałów i jest używany z krzywymi gęstości:\n\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     freq = FALSE,  # Tworzy skalę gęstości\n     main = \"Rozkład wyników egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Gęstość\",\n     col = \"lightcoral\")\n\n\n\n\n\n\n\n\n\n\nKrzywe Gęstości\nKrzywa gęstości (density curve) to wygładzona linia, która przybliża/modeluje kształt rozkładu. W przeciwieństwie do histogramów, które pokazują rzeczywiste dane w dyskretnych klasach, krzywe gęstości pokazują ogólny wzorzec jako funkcję ciągłą. Pole pod całą krzywą zawsze równa się 1, a pole pod dowolną częścią krzywej reprezentuje proporcję obserwacji w tym zakresie.\n\n# Dodawanie krzywej gęstości do histogramu\nhist(exam_scores, \n     freq = FALSE,\n     main = \"Wyniki egzaminacyjne z krzywą gęstości\",\n     xlab = \"Wynik\",\n     ylab = \"Gęstość\",\n     col = \"lightblue\",\n     border = \"white\")\nlines(density(exam_scores), \n      col = \"darkred\", \n      lwd = 2)\n\n\n\n\n\n\n\n\nKrzywe gęstości są szczególnie użyteczne do:\n\nIdentyfikacji kształtu rozkładu (symetryczny, skośny, bimodalny)\nPorównywania wielu rozkładów na tym samym wykresie\nZrozumienia teoretycznego (“prawdziwego”) rozkładu leżącego u podstaw danych\n\n\n\n\n\n\n\nTip\n\n\n\nW statystyce percentyl (percentile) wskazuje względną pozycję punktu danych w zbiorze, pokazując procent obserwacji, które znajdują się na tym poziomie lub poniżej. Na przykład, jeśli student uzyskał wynik na 90. percentylu w teście, jego wynik jest równy lub wyższy niż 90% wszystkich innych wyników.\nKwartyle (quartiles) to specjalne percentyle, które dzielą dane na cztery równe części: pierwszy kwartyl (Q1, 25. percentyl), drugi kwartyl (Q2, 50. percentyl, czyli mediana), i trzeci kwartyl (Q3, 75. percentyl). Jeśli Q1 = 65 punktów, oznacza to, że 25% studentów uzyskało 65 punktów lub mniej.\nBardziej ogólnie, kwantyle (quantiles) to wartości, które dzielą dane na grupy o równej liczebności — percentyle dzielą na 100 części, kwartyle na 4 części, decyle (deciles) na 10 części, itp.\n\n\n\n\nWizualizacja Częstości Skumulowanej (*)\nWykresy częstości skumulowanej, zwane także ogiwami (ogives, wymawiane “oh-dżajw”), pokazują jak częstości kumulują się w zakresie wartości. Te wykresy używają linii zamiast słupków i zawsze rosną od lewej do prawej, ostatecznie osiągając całkowitą liczbę obserwacji (dla częstości skumulowanej) lub 1,0 (dla częstości względnej skumulowanej).\nWykresy częstości skumulowanej są wykorzytywane do:\n\nWizualnego odnajdywania percentyli i kwartyli\nOkreślania, jaka proporcja danych znajduje się poniżej lub powyżej określonej wartości\nPorównywania rozkładów różnych grup\n\n\n# Tworzenie danych częstości skumulowanej\nscore_breaks &lt;- seq(0, 100, by = 10)\nfreq_counts &lt;- hist(exam_scores, breaks = score_breaks, plot = FALSE)$counts\ncumulative_freq &lt;- cumsum(freq_counts)\n\n# Wykres częstości skumulowanej\nplot(score_breaks[-1], cumulative_freq,\n     type = \"b\",  # zarówno punkty, jak i linie\n     main = \"Częstość skumulowana wyników egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Częstość skumulowana\",\n     col = \"darkblue\",\n     lwd = 2,\n     pch = 19)\ngrid()\n\n\n\n\n\n\n\n\nDla częstości względnej skumulowanej (która jest częściej używana):\n\n# Częstość względna skumulowana\ncumulative_rel_freq &lt;- cumulative_freq / length(exam_scores)\n\nplot(score_breaks[-1], cumulative_rel_freq,\n     type = \"b\",\n     main = \"Częstość względna skumulowana wyników egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Częstość względna skumulowana\",\n     col = \"darkred\",\n     lwd = 2,\n     pch = 19,\n     ylim = c(0, 1))\ngrid()\nabline(h = c(0.25, 0.5, 0.75), lty = 2, col = \"gray\")  # Linie kwartyli\n\n\n\n\n\n\n\n\nKrzywa częstości względnej skumulowanej ułatwia odczytywanie percentyli. Na przykład, jeśli narysujesz linię poziomą na 0,75 i zobaczysz, gdzie przecina krzywą, odpowiadająca wartość x to 75. percentyl — wynik, poniżej którego znajduje się 75% studentów.\n\n\n\nRozkłady Dyskretne a Ciągłe\nTyp zmiennej, którą analizujesz, określa sposób wizualizacji jej rozkładu:\nRozkłady dyskretne (discrete distributions) stosują się do zmiennych, które mogą przyjmować tylko określone, policzalne wartości. Przykłady obejmują liczbę dzieci w rodzinie (0, 1, 2, 3…), liczbę skarg klientów dziennie lub odpowiedzi na 5-stopniowej skali Likerta.\nDla danych dyskretnych zazwyczaj używamy:\n\nWykresów słupkowych (z przerwami między słupkami) zamiast histogramów\nCzęstości lub częstości względnej na osi y\nKażda odrębna wartość otrzymuje własny słupek\n\n\n# Przykład: Liczba dzieci w rodzinie\nchildren &lt;- c(0, 1, 2, 2, 1, 3, 0, 2, 1, 4, 2, 1, 0, 2, 3)\nbarplot(table(children),\n        main = \"Rozkład liczby dzieci\",\n        xlab = \"Liczba dzieci\",\n        ylab = \"Częstość\",\n        col = \"skyblue\")\n\n\n\n\n\n\n\n\nRozkłady ciągłe (continuous distributions) stosują się do zmiennych, które mogą przyjmować dowolną wartość w zakresie. Przykłady obejmują temperaturę, czas reakcji, wzrost lub procent frekwencji.\nDla danych ciągłych używamy:\n\nHistogramów (ze stykającymi się słupkami), które grupują dane w przedziały\nKrzywych gęstości, aby pokazać wygładzony wzorzec\nGęstości na osi y przy używaniu krzywych gęstości\n\n\n# Przykład: Rozkład czasu reakcji\nhist(response_time, \n     breaks = 15,\n     freq = FALSE,\n     main = \"Rozkład czasu reakcji\",\n     xlab = \"Czas reakcji (sekundy)\",\n     ylab = \"Gęstość\",\n     col = \"lightgreen\",\n     border = \"white\")\nlines(density(response_time), \n      col = \"darkgreen\", \n      lwd = 2)\n\n\n\n\n\n\n\n\nKluczowa różnica polega na tym, że rozkłady dyskretne pokazują prawdopodobieństwo w konkretnych punktach, podczas gdy rozkłady ciągłe pokazują gęstość prawdopodobieństwa w zakresach. Dla zmiennych ciągłych prawdopodobieństwo jakiejkolwiek dokładnej wartości jest w zasadzie równe zeru — zamiast tego mówimy o prawdopodobieństwie znalezienia się w przedziale.\nZrozumienie, czy twoja zmienna jest dyskretna czy ciągła, kieruje wyborem wizualizacji i metod statystycznych, zapewniając, że twoja analiza dokładnie reprezentuje naturę twoich danych.\n\n\nOpisywanie rozkładów (*)\nCharakterystyki kształtu:\nSymetria vs. Skośność:\n\nSymetryczny: Lustrzane odbicie wokół środka (przykład: wzrost w jednorodnej populacji)\nPrawostronnie skośny (skośność dodatnia): Długi ogon po prawej stronie (przykład: dochód, bogactwo)\nLewostronnie skośny (skośność ujemna): Długi ogon po lewej stronie (przykład: liczba lat życia w krajach rozwiniętych)\n\nPrzykład wpływu skośności:\nRozkład dochodu w USA:\n\nMediana dochodu gospodarstwa domowego: ~70 000 USD\nŚredni dochód gospodarstwa domowego: ~100 000 USD\nŚrednia &gt; Mediana wskazuje na skośność prawostronną\nKilka bardzo wysokich dochodów podnosi średnią\n\n\nModalność:\n\nJednomodalny: Jeden szczyt (przykład: wyniki testów)\nDwumodalny: Dwa szczyty (przykład: wzrost przy mieszaniu mężczyzn i kobiet)\nWielomodalny: Wiele szczytów (przykład: rozkład wieku w mieście uniwersyteckim — szczyty w wieku studenckim i średnim wieku)\n\n\n\n\n\n\n\n\n\n\nWażne rozkłady prawdopodobieństwa:\nRozkład normalny (Gaussa):\n\nKształt dzwonu, symetryczny\nCharakteryzowany przez średnią (\\mu) i odchylenie standardowe (\\sigma)\nOkoło 68% wartości w granicach \\mu \\pm \\sigma\nOkoło 95% w granicach \\mu \\pm 2\\sigma\nOkoło 99,7% w granicach \\mu \\pm 3\\sigma\n\nZastosowania demograficzne:\n\nWzrost w jednorodnych populacjach\nBłędy pomiarowe\nRozkłady próbkowania średnich (Centralne Twierdzenie Graniczne)\n\nRozkład dwumianowy:\n\nLiczba sukcesów w n niezależnych próbach\nKażda próba ma prawdopodobieństwo p sukcesu\nŚrednia = np, Wariancja = np(1-p)\n\nPrzykład: Liczba urodzeń chłopców na 100 urodzeń (p \\approx 0,512)\nRozkład Poissona:\n\nLiczba zdarzeń w stałym czasie/przestrzeni\nŚrednia = Wariancja = \\lambda\nDobry dla rzadkich zdarzeń\n\nZastosowania demograficzne:\n\nLiczba zgonów dziennie w małym mieście\nLiczba urodzeń na godzinę w szpitalu\nLiczba wypadków na skrzyżowaniu miesięcznie\n\n\n\n\nWizualizacja rozkładów częstości (*)\nHistogram: Dla danych ciągłych, pokazuje częstość wysokościami słupków.\n\nOś X: Zakresy wartości (przedziały)\nOś Y: Częstość lub gęstość\nBrak przerw między słupkami (dane ciągłe)\nSzerokość przedziału wpływa na wygląd\n\nWykres słupkowy: Dla danych kategorycznych, pokazuje częstość z oddzielonymi słupkami.\n\nOś X: Kategorie\nOś Y: Częstość\nPrzerwy między słupkami (dyskretne kategorie)\nKolejność może mieć znaczenie lub nie\n\nDystrybuanta (Funkcja Rozkładu Skumulowanego): Pokazuje proporcję wartości ≤ każdego punktu danych. - Zawsze rośnie (lub pozostaje płaska) - Zaczyna się od 0, kończy na 1 - Strome nachylenia wskazują na częste wartości - Płaskie obszary wskazują na rzadkie wartości\nWykres Pudełkowy (Wykres Skrzynkowy): Wizualne podsumowanie, które przedstawia kluczowe statystyki rozkładu przy użyciu pięciu kluczowych wartości.\nPodsumowanie Pięciu Liczb:\n\nMinimum: Koniec lewego wąsa (z wyłączeniem wartości odstających)\nQ1 (Pierwszy Kwartyl): Lewa krawędź pudełka (25. percentyl)\nMediana (Q2): Linia wewnątrz pudełka (50. percentyl)\n\nQ3 (Trzeci Kwartyl): Prawa krawędź pudełka (75. percentyl)\nMaksimum: Koniec prawego wąsa (z wyłączeniem wartości odstających)\n\nCo Pokazuje:\n\nSkośność: Jeśli linia mediany jest przesunięta w pudełku lub wąsy są nierówne\nRozrzut: Szersze pudełka i dłuższe wąsy wskazują na większą zmienność\nWartości odstające: Natychmiast widoczne jako oddzielne punkty\nSymetria: Równe długości wąsów i wyśrodkowana mediana sugerują rozkład normalny\n\nSzybka Interpretacja:\n\nWąskie pudełko = spójne dane\nDługie wąsy = szeroki zakres wartości\n\nWiele wartości odstających = potencjalne problemy z jakością danych lub interesujące przypadki skrajne\nMediana bliżej Q1 = dane skośne prawostronnie (ogon rozciąga się w prawo)\nMediana bliżej Q3 = dane skośne lewostronnie (ogon rozciąga się w lewo)\n\nWykresy pudełkowe są szczególnie użyteczne do porównywania wielu grup obok siebie!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zmienne-i-skale-pomiarowe",
    "href": "rozdzial1.html#zmienne-i-skale-pomiarowe",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.6 Zmienne i skale pomiarowe",
    "text": "2.6 Zmienne i skale pomiarowe\n\nZmienna to każda charakterystyka, która może przyjmować różne wartości dla różnych jednostek obserwacji.\n\n\nPomiar: przekształcanie pojęć w liczby\n\nŚwiat polityki jest pełen danych\nPolitologia ewoluowała z dyscypliny głównie teoretycznej do takiej, która coraz bardziej opiera się na dowodach empirycznych. Niezależnie od tego, czy badamy:\n\nWyniki wyborów: Dlaczego ludzie głosują tak, jak głosują?\nOpinię publiczną: Co kształtuje postawy wobec imigracji lub polityki klimatycznej?\nStosunki międzynarodowe: Jakie czynniki przewidują konflikt między narodami/państwami?\nSkuteczność polityk: Czy nowa polityka edukacyjna rzeczywiście poprawiła wyniki uczniów?\n\nPotrzebujemy systematycznych sposobów analizowania danych i wyciągania wniosków, które wykraczają poza anegdoty i osobiste wrażenia.\n\nRozważ to pytanie: “Czy demokracja prowadzi do wzrostu gospodarczego?”\n\nTwoja intuicja może sugerować, że tak - kraje demokratyczne są zazwyczaj bogatsze. Ale czy to przyczynowość, czy korelacja? Czy są wyjątki? Jak pewni możemy być naszych wniosków?\nStatystyka dostarcza narzędzi do przejścia od przeczuć do odpowiedzi opartych na dowodach, pomagając nam rozróżnić między tym, co wydaje się prawdziwe, a tym, co rzeczywiście jest prawdziwe.\n\n\nPomiar w naukach społecznych\nW naukach społecznych często zmagamy się z tym, że kluczowe pojęcia nie przekładają się wprost na liczby:\n\nJak zmierzyć „demokrację”?\nJaka liczba oddaje „ideologię polityczną”?\nJak ilościowo ująć „siłę instytucji”?\nJak zmierzyć „partycypację polityczną”?\n\n\n\n\n\n\n\n\n🔍 Korelacja ≠ Przyczynowość: Zrozumienie Związków Pozornych (spurious correlation)\n\n\n\n\nFundamentalne Rozróżnienie\nKorelacja (correlation) mierzy, jak dwie zmienne poruszają się razem:\n\nDodatnia: Obie rosną razem (godziny nauki ↑, oceny ↑)\nUjemna: Jedna rośnie, gdy druga maleje (godziny TV ↑, oceny ↓)\nMierzona współczynnikiem korelacji: r \\in [-1, 1]\n\nPrzyczynowość (causation) oznacza, że jedna zmienna bezpośrednio wpływa na drugą:\n\nX \\rightarrow Y: Zmiany w X bezpośrednio powodują zmiany w Y\nWymaga: (1) korelacji, (2) poprzedzania czasowego, (3) braku alternatywnych wyjaśnień\n\n\n\nZagrożenie: Korelacja Pozorna\nKorelacja pozorna (spurious correlation) występuje, gdy dwie zmienne wydają się powiązane, ale w rzeczywistości obie są pod wpływem trzeciej zmiennej (czynnika zakłócającego/confoundera).\nKlasyczny przykład:\n\nObserwacja: Sprzedaż lodów koreluje z liczbą utonięć\nPozorny wniosek: Lody powodują utonięcia (❌)\nRzeczywistość: Letnia pogoda (czynnik zakłócający) powoduje oba zjawiska:\nLato → Więcej sprzedanych lodów\nLato → Więcej pływania → Więcej utonięć\n\nReprezentacja matematyczna:\n\nObserwowana korelacja: \\text{Cor}(X,Y) \\neq 0\nAle prawdziwy model: X = \\alpha Z + \\epsilon_1 oraz Y = \\beta Z + \\epsilon_2\nGdzie Z to zmienna zakłócająca powodująca oba zjawiska\n\n\n\nCzynniki Zakłócające (Confounding): Ukryty Wpływ\nZmienna zakłócająca (confounding variable/confounder):\n\nWpływa zarówno na domniemaną przyczynę, jak i skutek\nTworzy iluzję bezpośredniej przyczynowości\nMusi być kontrolowana dla ważnego wnioskowania przyczynowego\n\nPrzykład badawczy:\n\nObserwacja: Spożycie kawy koreluje z chorobami serca\nPotencjalny czynnik zakłócający: Palenie (osoby pijące kawę częściej palą)\nPrawdziwe relacje:\nPalenie → Choroby serca (przyczynowa)\nPalenie → Spożycie kawy (związek)\nKawa → Choroby serca (pozorna bez kontroli palenia)\n\n\n\nJak Identyfikować Związki Przyczynowe\n\nRandomizowane badania kontrolowane (RCTs): Losowy przydział przerywa wpływ czynników zakłócających\nEksperymenty naturalne (natural experiments): Zdarzenia zewnętrzne tworzą „jakby” losową zmienność\nKontrola statystyczna: Włączenie czynników zakłócających do modeli regresji\nZmienne instrumentalne (instrumental variables): Znalezienie zmiennych wpływających na X, ale nie bezpośrednio na Y\n\n\n\nKluczowy Wniosek\nZnalezienie korelacji jest łatwe. Ustalenie przyczynowości jest trudne. Zawsze pytaj: „Co jeszcze mogłoby wyjaśniać ten związek?”\nPamiętaj: Najbardziej niebezpieczne zdanie w badaniach empirycznych to „nasze dane pokazują, że X powoduje Y”, gdy tak naprawdę zmierzyłeś tylko korelację.\n\n\n\n\n\n\n\n\n\n\n📊 Szybki Test: Korelacja czy Przyczynowość?\n\n\n\n\n\nDla każdego scenariusza określ, czy związek jest prawdopodobnie przyczynowy czy pozorny:\n\nMiasta z większą liczbą kościołów mają więcej przestępstw\n\nOdpowiedź: Pozorny (czynnik zakłócający: wielkość populacji)\n\nPalenie prowadzi do raka płuc\n\nOdpowiedź: Przyczynowy (ustalony poprzez wiele projektów badawczych)\n\nUczniowie z większą liczbą książek w domu mają lepsze oceny\n\nOdpowiedź: Prawdopodobnie pozorny (czynniki zakłócające: wykształcenie rodziców, dochód)\n\nKraje z wyższym spożyciem czekolady mają więcej laureatów Nobla\n\nOdpowiedź: Pozorny (czynnik zakłócający: poziom zamożności/rozwoju)\n\n\n\n\n\n\n\n\n\nTypy zmiennych\nZmienne ilościowe (Quantitative Variables) reprezentują ilości lub wielkości i mogą być:\nZmienne ciągłe (Continuous Variables): Mogą przyjmować dowolną wartość w przedziale, ograniczoną tylko precyzją pomiaru.\n\nWiek (22,5 lat, 22,51 lat, 22,514 lat…)\nDochód (45 234,67 zł)\nWzrost (175,3 cm)\nGęstość zaludnienia (432,7 osób na kilometr kwadratowy)\n\nZmienne dyskretne (Discrete Variables): Mogą przyjmować tylko określone wartości, zazwyczaj liczenia.\n\nLiczba dzieci w rodzinie (0, 1, 2, 3…)\nLiczba małżeństw (0, 1, 2…)\nLiczba pokoi w mieszkaniu (1, 2, 3…)\nLiczba migrantów wjeżdżających do kraju rocznie\n\nZmienne jakościowe (Qualitative Variables) reprezentują kategorie lub cechy i mogą być:\nZmienne nominalne (Nominal Variables): Kategorie bez naturalnego porządku.\n\nKraj urodzenia (Polska, Meksyk, Kanada…)\nReligia (Chrześcijaństwo, Islam, Hinduizm, Buddyzm…)\nGrupa krwi (A, B, AB, 0)\nPrzyczyna śmierci (choroby serca, nowotwory, wypadek…)\n\nZmienne porządkowe (Ordinal Variables): Kategorie ze znaczącym porządkiem, ale nierównymi interwałami.\n\nPoziom wykształcenia (brak wykształcenia, podstawowe, średnie, wyższe)\nZadowolenie z opieki zdrowotnej (bardzo niezadowolony, niezadowolony, neutralny, zadowolony, bardzo zadowolony)\nStatus społeczno-ekonomiczny (niski, średni, wysoki)\nSamoocena stanu zdrowia (zły, przeciętny, dobry, doskonały)\n\n\n\nSkale pomiarowe\nZrozumienie skal pomiarowych jest kluczowe, ponieważ determinują, które metody statystyczne są odpowiednie:\nSkala nominalna (Nominal Scale): Tylko kategorie — możemy liczyć częstości, ale nie możemy porządkować ani wykonywać operacji arytmetycznych. Przykład: Możemy powiedzieć, że 45% mieszkańców urodziło się lokalnie, ale nie możemy obliczyć „średniego miejsca urodzenia”.\nSkala porządkowa (Ordinal Scale): Kolejność ma znaczenie, ale różnice między wartościami niekoniecznie są równe. Przykład: Różnica między „złym” a „przeciętnym” zdrowiem może nie równać się różnicy między „dobrym” a „doskonałym” zdrowiem.\nSkala interwałowa (Interval Scale): Równe interwały między wartościami, ale brak prawdziwego punktu zerowego. Przykład: Temperatura w stopniach Celsjusza — różnica między 20°C a 30°C równa się różnicy między 30°C a 40°C, ale 0°C nie oznacza „braku temperatury”.\nSkala ilorazowa (Ratio Scale): Równe interwały z prawdziwym punktem zerowym, umożliwiające wszystkie operacje matematyczne. Przykład: Dochód — 40 000 zł to dwa razy więcej niż 20 000 zł, a 0 zł oznacza brak dochodu.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#parametry-statystyki-i-estymacja",
    "href": "rozdzial1.html#parametry-statystyki-i-estymacja",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.7 Parametry, statystyki i estymacja",
    "text": "2.7 Parametry, statystyki i estymacja\nTe pojęcia stanowią rdzeń wnioskowania statystycznego — jak uczymy się o populacjach z prób. Zrozumienie relacji między tymi terminami jest niezbędne dla właściwego rozumowania statystycznego.\n\nParametr\nParametr to liczbowa charakterystyka populacji. Parametry są zazwyczaj nieznane, ponieważ nie możemy zmierzyć całej populacji. Są to wartości stałe (nie losowe), ale nieznane nam. Oznaczamy parametry literami greckimi.\nPowszechne parametry demograficzne:\n\n\\mu (mi): Średni wiek populacji. Na przykład, prawdziwy średni wiek wszystkich Europejczyków.\n\\sigma^2 (sigma kwadrat): Wariancja populacji w dochodzie we wszystkich gospodarstwach domowych w Brazylii.\np: Proporcja populacji. Na przykład, prawdziwa proporcja wszystkich dorosłych w Japonii, którzy są małżonkami.\n\\beta (beta): Współczynnik regresji. Prawdziwa relacja między wykształceniem a płodnością w populacji.\n\\lambda (lambda): Parametr stopy. Prawdziwa stopa migracji z obszarów wiejskich do miejskich.\n\nPrzykład: Prawdziwy średni wiek przy pierwszym porodzie dla wszystkich kobiet we Francji, które urodziły dziecko w 2023 roku, jest parametrem. Nazwijmy go \\mu = 31,2 lat. Nie znamy tej wartości bez zmierzenia każdego pojedynczego porodu.\n\n\nStatystyka\nStatystyka to liczbowa charakterystyka obliczona z danych z próby. Statystyki są zmiennymi losowymi — ich wartości różnią się od próby do próby. Używamy łacińskich liter dla statystyk.\nPowszechne statystyki z próby:\n\n\\bar{x} (x z kreską): Średnia wieku z próby z badania 1000 osób\ns^2: Wariancja dochodu z próby z 500 ankietowanych gospodarstw\n\\hat{p} (p z daszkiem): Proporcja małżonków z próby z badania\nr: Korelacja z próby między wykształceniem a dochodem\nb: Współczynnik regresji z próby\n\nPrzykład: Z próby 500 urodzeń we Francji obliczamy średni wiek przy pierwszym porodzie z próby \\bar{x} = 30,9 lat. To jest nasza statystyka. Inna próba może dać \\bar{x} = 31,4 lat.\n\n\nRelacja między parametrami a statystykami\nPomyśl o tej relacji jak o próbie zrozumienia głębokości jeziora:\n\nParametr: Prawdziwa średnia głębokość jeziora (nieznana, stała)\nStatystyka: Średnia głębokość z kilku punktów pomiarowych (znana, zmienia się z różnymi próbami)\nEstymacja: Używanie naszych pomiarów do zgadywania prawdziwej średniej głębokości\n\n\n\nEstymator\nEstymator to reguła lub formuła do obliczania oszacowania parametru populacji z danych z próby. Estymator to funkcja, która odwzorowuje dane z próby na oszacowania parametrów.\nWłaściwości dobrych estymatorów:\nNieobciążoność (Unbiasedness): Średnio estymator równa się prawdziwej wartości parametru. Gdybyśmy powtórzyli próbkowanie wiele razy, średnia wszystkich naszych oszacowań równałaby się prawdziwemu parametrowi.\nPrzykład: Średnia z próby \\bar{x} jest nieobciążonym estymatorem średniej populacji \\mu. Gdybyśmy wzięli 1000 różnych prób i obliczyli 1000 średnich z prób, ich średnia byłaby bardzo bliska \\mu.\nZgodność (Consistency): Gdy wielkość próby wzrasta, estymator zbiega się do prawdziwej wartości parametru.\nPrzykład: Z n=10, nasze oszacowanie średniego dochodu może być oddalone o 5000 zł. Z n=1000, możemy być oddaleni tylko o 500 zł. Z n=100 000, możemy być oddaleni tylko o 50 zł.\nEfektywność (Efficiency): Wśród nieobciążonych estymatorów, ten z najmniejszą wariancją. Średnia z próby jest bardziej efektywna niż mediana z próby do estymowania średniej populacji rozkładu normalnego.\nPowszechne estymatory:\n\nŚrednia z próby jako estymator średniej populacji: \\bar{x} = \\frac{\\sum x_i}{n}\nProporcja z próby jako estymator proporcji populacji: \\hat{p} = \\frac{x}{n} (gdzie x to liczba sukcesów)\nWariancja z próby jako estymator wariancji populacji: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\n\nUwaga: Dzielimy przez (n-1), a nie n dla wariancji z próby, aby uczynić ją nieobciążoną — to się nazywa korekta Bessela.\n\n\nEstimand\nEstimand to konkretny parametr populacji, który chcemy oszacować. To cel naszej procedury estymacji. Jasna specyfikacja estimandu jest kluczowa dla właściwego wnioskowania statystycznego i unikania błędnej interpretacji.\nPrzykłady jasno zdefiniowanych estimandów:\n\n„Mediana dochodu gospodarstwa domowego dla wszystkich gospodarstw w Kalifornii na dzień 1 stycznia 2024”\n„Różnica w oczekiwanej długości życia między mężczyznami a kobietami urodzonymi w Szwecji w 2023 roku”\n„Proporcja wszystkich dorosłych w wieku 25-34 lat w obszarach miejskich, którzy ukończyli edukację wyższą”\n\nDlaczego precyzyjna definicja estimandu ma znaczenie:\nRozważ badanie „stopy bezrobocia”. Estimand musi określić:\n\nKto liczy się jako bezrobotny? (Aktywnie poszukujący pracy? Zniechęceni pracownicy?)\nJaki zakres wieku? (15+? 16-64?)\nJaki obszar geograficzny?\nJaki okres czasu?\n\nRóżne definicje prowadzą do różnych liczb. Amerykańskie Biuro Statystyki Pracy publikuje sześć różnych stóp bezrobocia (U-1 do U-6) na podstawie różnych definicji.\n\n\nOszacowanie (Estimate)\nOszacowanie to konkretna wartość numeryczna obliczona przez zastosowanie estymatora do obserwowanych danych. To nasze najlepsze przypuszczenie o prawdziwej wartości parametru na podstawie dostępnych informacji.\nPrzykład kompletnego procesu:\n\nEstimand (cel): Proporcja wszystkich dorosłych Amerykanów, którzy popierają działalność prezydenta\nParametr (prawdziwa nieznana wartość): p = 0,42 (42%, ale tego nie wiemy)\nEstymator (metoda): Proporcja z próby \\hat{p} = \\frac{x}{n} gdzie x to liczba popierających, a n to wielkość próby\nPróba: Ankietujemy 1500 losowo wybranych dorosłych, 650 popiera\nOszacowanie (obliczona wartość): \\hat{p} = \\frac{650}{1500} = 0,433 (43,3%)\n\n\n\n\n\n\n\nEstimandy: Co dokładnie próbujemy oszacować?\n\n\n\nEstimand to konkretna wielkość, którą chcemy oszacować — na co celujemy naszą analizą statystyczną. Choć często jest to parametr populacji, estimandy mogą być bardziej złożone.\nPrzykłady różnych estimandów:\nProsty estimand parametru: Średni dochód populacji (\\mu) Porównawczy estimand: Różnica w średnim dochodzie między dwiema grupami (\\mu_1 - \\mu_2) Przyczynowy estimand: Średni efekt leczenia programu szkoleniowego na zarobki Warunkowy estimand: Oczekiwana frekwencja wyborcza przy konkretnych warunkach pogodowych\n\nKompletna struktura\nZrozumienie wnioskowania statystycznego wymaga rozróżnienia między tymi powiązanymi, ale odrębnymi pojęciami:\n\nParametr populacji: Prawdziwa charakterystyka populacji (np. \\mu)\nEstimand: Konkretna wielkość, którą chcemy oszacować (często, ale nie zawsze, parametr)\nEstymator: Metoda obliczania naszego oszacowania (np. średnia z próby)\nOszacowanie: Rzeczywista liczba, którą obliczamy z naszych danych\n\nPrzykład w kontekście:\n\nParametr: Prawdziwa średnia frekwencja wyborcza we wszystkich wyborach (\\mu)\nEstimand: Oczekiwana różnica frekwencji między deszczowymi a słonecznymi dniami wyborów (\\mu_{\\text{deszczowy}} - \\mu_{\\text{słoneczny}})\nEstymator: Różnica między średnimi z prób z deszczowych i słonecznych wyborów\nOszacowanie: 3,2 punkty procentowe niższa frekwencja w deszczowe dni\n\nTa struktura pomaga wyjaśnić dokładnie, na jakie pytanie odpowiadamy i zapewnia, że nasze metody są zgodne z naszymi celami badawczymi.\n\n\n\n\n\n\n\n\n\n\nWyjaśnienie różnych typów nieprzewidywalności\n\n\n\nNie wszystkie rodzaje niepewności są takie same. Zrozumienie różnych źródeł nieprzewidywalności pomaga w wyborze odpowiednich metod statystycznych i prawidłowej interpretacji wyników.\n\n\n\n\n\n\n\n\n\nPojęcie\nCzym jest?\nŹródło nieprzewidywalności\nPrzykład\n\n\n\n\nLosowość (randomness)\nPoszczególne wyniki są niepewne, ale rozkład prawdopodobieństwa jest znany lub modelowany.\nFluktuacje między realizacjami; brak informacji o konkretnym wyniku.\nRzut kostką, rzut monetą, próba sondażowa\n\n\nChaos\nDynamika deterministyczna bardzo wrażliwa na warunki początkowe (efekt motyla).\nNiewielkie różnice początkowe szybko narastają → duże rozbieżności trajektorii.\nPrognoza pogody, podwójne wahadło, dynamika populacyjna\n\n\nEntropia\nMiara niepewności/rozproszenia (teorioinformacyjna lub termodynamiczna).\nWiększa gdy wyniki są bardziej równomiernie rozłożone (mniej informacji predykcyjnej).\nEntropia Shannona w kompresji danych\n\n\n„Przypadkowość” (potoczne)\nOdczuwany brak porządku bez wyraźnego modelu; mieszanka mechanizmów.\nBrak uporządkowanego opisu lub stabilnych reguł; nakładające się procesy.\nWzorce ruchu, trendy w mediach społecznościowych\n\n\nLosowość kwantowa (quantum randomness)\nPojedynczy wynik nie jest zdeterminowany; tylko rozkład jest określony (reguła Borna).\nFundamentalna (ontologiczna) nieokreśloność poszczególnych pomiarów.\nPomiar spinu elektronu, polaryzacja fotonu\n\n\n\n\nKluczowe rozróżnienia dla praktyki statystycznej\nChaos deterministyczny ≠ losowość statystyczna: System chaotyczny jest w pełni deterministyczny, ale praktycznie nieprzewidywalny z powodu ekstremalnej wrażliwości na warunki początkowe. Losowość statystyczna modeluje natomiast niepewność poprzez rozkłady prawdopodobieństwa, gdzie poszczególne wyniki są rzeczywiście niepewne.\nDlaczego to ważne: W statystyce zazwyczaj modelujemy zjawiska jako procesy losowe, zakładając, że możemy określić rozkłady prawdopodobieństwa, nawet gdy poszczególne wyniki są nieprzewidywalne. To założenie stanowi podstawę większości wnioskowań statystycznych.\n\n\nMechanika kwantowa i fundamentalna losowość\nW interpretacji kopenhaskiej losowość jest fundamentalna (ontologiczna): pojedynczy wynik nie może być przewidziany, ale rozkład prawdopodobieństwa jest dany przez regułę Borna.\nTo reprezentuje prawdziwą losowość na najbardziej podstawowym poziomie natury, nie tylko naszą ignorancję czynników determinujących.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#błąd-statystyczny-i-niepewność",
    "href": "rozdzial1.html#błąd-statystyczny-i-niepewność",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.8 Błąd statystyczny i niepewność",
    "text": "2.8 Błąd statystyczny i niepewność\n\nWprowadzenie: Dlaczego niepewność ma znaczenie\nŻaden pomiar ani oszacowanie nie jest doskonałe. Zrozumienie różnych typów błędów jest kluczowe dla interpretacji wyników i poprawy projektu badania.\n\n\n\n\n\n\nCentralne wyzwanie\n\n\n\nZa każdym razem, gdy używamy próby (sample) do poznania populacji (population), wprowadzamy niepewność. Kluczem jest:\n\nUczciwe skwantyfikowanie tej niepewności\nRozróżnienie między różnymi źródłami błędu\nTransparentna komunikacja wyników\n\n\n\n\n\n\nTypy błędów\n\nBłąd losowy (random error)\nBłąd losowy (random error) reprezentuje nieprzewidywalne fluktuacje, które różnią się między obserwacjami bez stałego wzorca. Te błędy wynikają z różnych źródeł naturalnej zmienności w procesie zbierania i pomiaru danych.\n\n\n\n\n\n\nKluczowe cechy\n\n\n\n\nNieprzewidywalny kierunek: Czasami za wysoki, czasami za niski\nBrak stałego wzorca: Zmienia się losowo między obserwacjami\nŚrednio daje zero: Po wielu pomiarach dodatnie i ujemne błędy się znoszą\nMożliwy do skwantyfikowania: Można go oszacować i zredukować odpowiednimi metodami\n\n\n\nBłąd losowy obejmuje kilka podtypów:\n\nBłąd próbkowania (sampling error)\nBłąd próbkowania (sampling error) to najczęstszy typ błędu losowego—pojawia się, ponieważ obserwujemy próbę, a nie całą populację. Różne losowe próby z tej samej populacji dadzą różne oszacowania wyłącznie przez przypadek.\nKluczowe właściwości:\n\nMaleje wraz z wielkością próby: \\propto 1/\\sqrt{n}\nMożliwy do skwantyfikowania za pomocą teorii prawdopodobieństwa\nNieunikniony przy pracy z próbami\n\nPrzykład: Badanie dostępu do internetu\nWyobraźmy sobie ankietę 100 losowo wybranych gospodarstw domowych o dostępie do internetu:\n\n\n\n\n\n\n\n\n\nZmienność wokół prawdziwej wartości (czerwona linia) reprezentuje błąd próbkowania. Przy większych próbach oszacowania przedziałowe byłyby węższe.\n\n\nBłąd pomiaru (measurement error)\nBłąd pomiaru (measurement error) to losowa zmienność w samym procesie pomiaru—nawet przy wielokrotnym pomiarze tej samej rzeczy.\nPrzykłady:\n\nNiewielkie różnice przy odczycie termometru spowodowane paralaksą\nLosowe fluktuacje w przyrządach elektronicznych\nNiespójności w ludzkiej ocenie przy kodowaniu danych jakościowych\n\nW przeciwieństwie do błędu próbkowania (który wynika z tego, kogo/co obserwujemy), błąd pomiaru wynika z tego, jak obserwujemy.\n\n\nInne źródła błędu losowego\n\nBłąd przetwarzania (processing error): Losowe pomyłki we wprowadzaniu danych, kodowaniu lub obliczeniach\nBłąd specyfikacji modelu (model specification error): Gdy prawdziwa zależność jest bardziej złożona niż zakładano\nZmienność czasowa (temporal variation): Naturalne wahania z dnia na dzień w mierzonym zjawisku\n\n\n\n\nBłąd systematyczny (systematic error / bias)\nBłąd systematyczny (systematic error lub bias) reprezentuje stałe odchylenie w określonym kierunku. W przeciwieństwie do błędu losowego, nie zeruje się przy powtarzanym próbkowaniu lub pomiarze—utrzymuje się i konsekwentnie odsuwa wyniki od prawdy.\n\nBłąd selekcji (selection bias)Błąd pomiaru (measurement bias)Błąd odpowiedzi (response bias)Błąd braku odpowiedzi (non-response bias)Błąd przetrwania (survivorship bias)Błąd obserwatora/ankietera (observer/interviewer bias)\n\n\nMetoda doboru próby systematycznie wyklucza pewne grupy.\nPrzykład: Ankiety telefoniczne w godzinach pracy niedostatecznie reprezentują osoby zatrudnione.\n\n\nNarzędzie pomiarowe konsekwentnie zawyża/zaniża pomiar.\nPrzykład: Waga, która zawsze pokazuje 2 funty za dużo; pytania ankietowe, które nakłaniają respondentów do konkretnych odpowiedzi.\n\n\nRespondenci systematycznie fałszywie raportują.\nPrzykład: Ludzie zaniżają spożycie alkoholu, zawyżają uczestnictwo w wyborach lub dają odpowiedzi społecznie pożądane.\n\n\nOsoby nieudzielające odpowiedzi różnią się systematycznie od respondentów.\nPrzykład: Osoby bardzo chore i bardzo zdrowe rzadziej odpowiadają na ankiety zdrowotne, pozostawiając tylko osoby o umiarkowanym zdrowiu.\n\n\nObserwowanie wyłącznie „ocalałych” z danego procesu.\nPrzykład: Podczas II wojny światowej wojsko analizowało powracające bombowce, aby określić, gdzie należy dodać pancerz. Samoloty wykazywały największe uszkodzenia na skrzydłach i sekcjach ogonowych. Abraham Wald dostrzegł błąd: należy opancerzyć miejsca, gdzie nie było dziur po kulach—silnik i kokpit. Samoloty trafione w tych miejscach nigdy nie wracały, aby je przeanalizować. Badano wyłącznie ocalałe.\n\n\nObserwatorzy lub ankieterzy systematycznie wpływają na wyniki.\nPrzykład: Ankieterzy nieświadomie sugerują pewne odpowiedzi lub rejestrują obserwacje potwierdzające ich oczekiwania.\n\n\n\n\n\nDekompozycja obciążenia i wariancji (bias-variance decomposition)\nMatematycznie, całkowity błąd (błąd średniokwadratowy, Mean Squared Error) rozkłada się na:\n\\mathrm{MSE}(\\hat\\theta) = \\underbrace{\\mathrm{Var}(\\hat\\theta)}_{\\text{błąd losowy}} + \\underbrace{\\big(\\mathrm{Bias}(\\hat\\theta)\\big)^2}_{\\text{błąd systematyczny}}\n\n\n\n\n\n\nKluczowy wniosek\n\n\n\n\nDuża obciążona próba daje precyzyjnie błędną odpowiedź.\n\n\nZwiększ n → redukuje błąd losowy (szczególnie błąd próbkowania)\nPopraw projekt badania → redukuje błąd systematyczny\nLepsze narzędzia → redukuje błąd pomiaru\n\n\n\n\n\n\nRóżne kombinacje obciążenia i wariancji w estymacji\n\n\nIntuicyjna analogia: Pomyśl o próbie trafienia w środek tarczy:\n\nBłąd losowy = rozproszone strzały wokół celu (czasami w lewo, czasami w prawo, czasami wysoko, czasami nisko)\nBłąd systematyczny = konsekwentne trafianie w to samo złe miejsce (wszystkie strzały skupione, ale z dala od centrum)\nIdeał = strzały ciasno skupione w centrum tarczy\n\n\n\n\n\nKwantyfikowanie niepewności\n\nBłąd standardowy (standard error)\nBłąd standardowy (standard error, SE) kwantyfikuje, jak bardzo oszacowanie zmienia się między różnymi możliwymi próbami. Mierzy konkretnie błąd próbkowania.\n\n\nDla proporcji:\nSE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\nDla średniej:\nSE(\\bar{x}) = \\frac{s}{\\sqrt{n}}\n\nDla różnicy:\nSE(\\bar{x}_1 - \\bar{x}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\n\n\n\n\n\n\n\nCo mówi nam SE\n\n\n\nBłąd standardowy kwantyfikuje tylko błąd próbkowania. Nie uwzględnia błędów systematycznych (obciążenia), błędów pomiaru ani innych źródeł niepewności.\n\n\n\n\nMargines błędu (margin of error)\nMargines błędu (margin of error, MOE) reprezentuje oczekiwaną maksymalną różnicę między oszacowaniem z próby a prawdziwym parametrem.\n\\text{MOE} = \\text{Wartość krytyczna} \\times \\text{Błąd standardowy}\n\n\n\n\n\n\nWyjaśnienie wartości krytycznej\n\n\n\n\n\nDla 95% ufności używamy 1,96 (często upraszczane do 2). Zapewnia to, że ~95% przedziałów skonstruowanych w ten sposób będzie zawierać prawdziwy parametr.\n\n90% ufności: z = 1,645\n95% ufności: z = 1,96\n99% ufności: z = 2,576\n\n\n\n\n\n\nPrzedziały ufności (confidence intervals)\nPrzedział ufności (confidence interval) dostarcza zakres prawdopodobnych wartości:\n\\text{CI} = \\text{Oszacowanie} \\pm (\\text{Wartość krytyczna} \\times \\text{Błąd standardowy})\n\n\n\n\n\n\nWażne ograniczenie\n\n\n\nPrzedziały ufności kwantyfikują niepewność próbkowania, ale zakładają brak błędu systematycznego. Doskonale precyzyjne oszacowanie (wąski przedział ufności) może nadal być obciążone, jeśli projekt badania jest wadliwy.\n\n\n\n\n\n\nPraktyczne zastosowanie: Sondaże opinii publicznej\n\n\n\n\n\n\nStudium przypadku: Sondaże polityczne\n\n\n\nGdy sondaż raportuje “Kandydat A: 52%, Kandydat B: 48%”, jest to niekompletne bez kwantyfikacji niepewności.\n\n\n\nZłota zasada sondażowania\nPrzy ~1000 losowo wybranych respondentów:\n\nMargines błędu: ±3 punkty procentowe (95% ufności)\nInterpretacja: Raportowane 52% oznacza, że prawdziwe poparcie prawdopodobnie wynosi między 49% a 55%\nCo to obejmuje: Tylko losowy błąd próbkowania—zakłada brak systematycznego obciążenia\n\n\n\n\n\n\n\nKluczowe rozróżnienie\n\n\n\nMargines błędu ±3% kwantyfikuje tylko niepewność próbkowania. Nie uwzględnia:\n\nBłędu pokrycia (coverage bias): kto jest wykluczony z operatu losowania\nBłędu braku odpowiedzi (non-response bias): kto odmawia udziału\nBłędu odpowiedzi (response bias): ludzie nieprawdziwie raportujący swoje poglądy\nEfektów czasowych (timing effects): zmiany opinii między sondażem a wyborem\n\n\n\n\n\nWielkość próby a precyzja\n\n\n\nWielkość próby\nMargines błędu (95%)\nZastosowanie\n\n\n\n\nn = 100\n± 10 pp\nTylko ogólny kierunek\n\n\nn = 400\n± 5 pp\nOgólne trendy\n\n\nn = 1000\n± 3 pp\nStandardowe sondaże\n\n\nn = 2500\n± 2 pp\nWysoka precyzja\n\n\nn = 10000\n± 1 pp\nBardzo wysoka precyzja\n\n\n\n\n\n\n\n\n\nPrawo malejących przychodów\n\n\n\nAby zmniejszyć margines błędu o połowę, potrzeba czterokrotnie większej próby, ponieważ \\text{MOE} \\propto 1/\\sqrt{n}\nTo dotyczy tylko błędu próbkowania. Podwojenie próby z 1000 do 2000 nie naprawi systematycznych problemów, takich jak stronnicze sformułowanie pytań czy niereprezentacyjne metody doboru próby.\n\n\n\n\nCo powinny raportować jakościowe sondaże\nTransparentny sondaż ujawnia:\n\nDaty badania: Kiedy zebrano dane?\nPopulacja i metoda doboru próby: Kto został przebadany i jak zostali wybrani?\nWielkość próby: Ile osób odpowiedziało?\nWskaźnik odpowiedzi (response rate): Jaki odsetek skontaktowanych osób wziął udział?\nProcedury ważenia (weighting procedures): Jak próba została dostosowana do charakterystyk populacji?\nMargines błędu próbkowania: Kwantyfikacja niepewności próbkowania\nBrzmienie pytań: Dokładny tekst zadanych pytań\n\n\n\n\n\n\n\nLuka w raportowaniu\n\n\n\nWiększość doniesień medialnych wspomina tylko liczby wynikowe i czasami margines błędu. Rzadko omawiają potencjalne obciążenia systematyczne, które mogą być znacznie większe niż błąd próbkowania.\n\n\n\n\n\n\nWizualizacja: Zmienność próbkowania\nPoniższa symulacja demonstruje, jak zachowują się przedziały ufności przy powtarzanym próbkowaniu:\n\n\nPokaż kod symulacji\nlibrary(ggplot2)\nset.seed(42)\n\n# Parametry\nn_polls      &lt;- 20\nn_people     &lt;- 100\ntrue_support &lt;- 0.50\n\n# Symulacja niezależnych sondaży\nsupport &lt;- rbinom(n_polls, n_people, true_support) / n_people\n\n# Obliczenie błędów standardowych i marginesów błędu\nse   &lt;- sqrt(support * (1 - support) / n_people)\nmoe  &lt;- 2 * se  # Uproszczony mnożnik dla przejrzystości\n\n# Utworzenie przedziałów ufności\nlower &lt;- pmax(0, support - moe)\nupper &lt;- pmin(1, support + moe)\n\n# Sprawdzenie pokrycia\ncovers &lt;- (lower &lt;= true_support) & (upper &gt;= true_support)\nn_cover &lt;- sum(covers)\n\nresults &lt;- data.frame(\n  poll = seq_len(n_polls),\n  support, se, moe, lower, upper, covers\n)\n\n# Utworzenie wizualizacji\nggplot(results, aes(x = poll, y = support, color = covers)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                width = 0.3, alpha = 0.8, size = 1) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = true_support, \n             linetype = \"dashed\", \n             color = \"black\",\n             alpha = 0.7) +\n  scale_color_manual(\n    values = c(\"TRUE\" = \"forestgreen\", \"FALSE\" = \"darkorange\"),\n    labels = c(\"TRUE\" = \"Obejmuje prawdę\", \"FALSE\" = \"Mija prawdę\"),\n    name   = NULL\n  ) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0, 1)) +\n  labs(\n    title    = \"Zmienność próbkowania w 20 niezależnych sondażach\",\n    subtitle = paste0(\n      \"Każdy sondaż: n = \", n_people, \" | Prawdziwa wartość = \",\n      scales::percent(true_support),\n      \" | Pokrycie: \", n_cover, \"/\", n_polls,\n      \" (\", round(100 * n_cover / n_polls), \"%)\"\n    ),\n    x = \"Numer sondażu\",\n    y = \"Oszacowane poparcie\",\n    caption = \"Słupki błędów pokazują przybliżone 95% przedziały ufności\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKluczowa obserwacja\n\n\n\nWiększość przedziałów obejmuje prawdziwą wartość, ale niektóre “chybiają” wyłącznie z powodu losowości próbkowania. Jest to oczekiwane i możliwe do skwantyfikowania—taka jest natura losowego błędu próbkowania.\nWażne: Ta symulacja zakłada brak systematycznego obciążenia. W rzeczywistych sondażach błędy systematyczne (błąd braku odpowiedzi, problemy z pokryciem, efekty sformułowania pytań) mogą przesunąć wszystkie oszacowania w tym samym kierunku, czyniąc je konsekwentnie błędnymi nawet przy dużych próbach.\n\n\n\n\n\nPowszechne błędne przekonania\n\n\n\n\n\n\nBłędne przekonanie #1: Margines błędu obejmuje całą niepewność\n\n\n\n❌ Mit: “Prawdziwa wartość na pewno znajduje się w marginesie błędu”\n✅ Rzeczywistość:\n\nPrzy 95% ufności nadal istnieje 5% szansa, że prawdziwa wartość znajduje się poza przedziałem wyłącznie z powodu losowości próbkowania\nCo ważniejsze, margines błędu obejmuje tylko błąd próbkowania, nie obciążenia systematyczne\nRzeczywiste sondaże często mają większe błędy z powodu błędu braku odpowiedzi, sformułowania pytań czy problemów z pokryciem niż z błędu próbkowania\n\n\n\n\n\n\n\n\n\nBłędne przekonanie #2: Większe próby naprawiają wszystko\n\n\n\n❌ Mit: “Jeśli tylko przebadamy więcej ludzi, wyeliminujemy wszystkie błędy”\n✅ Rzeczywistość:\n\nWiększe próby redukują błąd losowy (szczególnie błąd próbkowania): bardziej precyzyjne oszacowania\nWiększe próby NIE redukują błędu systematycznego: obciążenie pozostaje niezmienione\nSondaż 10 000 osób z 70% wskaźnikiem odpowiedzi i obciążonym operatem losowania da precyzyjnie błędną odpowiedź\nLepiej mieć 1000 dobrze wybranych respondentów niż 10 000 źle wybranych\n\n\n\n\n\n\n\n\n\nBłędne przekonanie #3: Losowy = niedbały\n\n\n\n❌ Mit: “Błąd losowy oznacza, że doszło do pomyłki”\n✅ Rzeczywistość:\n\nBłąd losowy jest nieodłączny w próbkowaniu i pomiarze—to nie jest pomyłka\nNawet przy doskonałej metodologii różne losowe próby dają różne wyniki\nBłędy losowe są przewidywalne w agregacie, choć nieprzewidywalne indywidualnie\nTermin “losowy” odnosi się do wzorca (brak systematycznego kierunku), a nie do niedbalstwa\n\n\n\n\n\n\n\n\n\nBłędne przekonanie #4: Przedziały ufności są gwarancjami\n\n\n\n❌ Mit: “95% ufności oznacza, że istnieje 95% szansa, że prawdziwa wartość jest w tym konkretnym przedziale”\n✅ Rzeczywistość:\n\nPrawdziwa wartość jest stała (ale nieznana)—albo jest w przedziale, albo nie\n“95% ufności” oznacza: gdybyśmy powtórzyli ten proces wiele razy, około 95% skonstruowanych przedziałów zawierałoby prawdziwą wartość\nKażdy konkretny przedział albo obejmuje prawdę, albo nie—po prostu nie wiemy, co jest prawdą\n\n\n\n\n\n\n\n\n\nBłędne przekonanie #5: Obciążenie można obliczyć jak błąd losowy\n\n\n\n❌ Mit: “Możemy obliczyć obciążenie tak samo jak obliczamy błąd standardowy”\n✅ Rzeczywistość:\n\nBłąd losowy jest możliwy do skwantyfikowania za pomocą teorii prawdopodobieństwa, ponieważ znamy proces próbkowania\nBłąd systematyczny jest zazwyczaj nieznany i niemożliwy do poznania bez zewnętrznej walidacji\nNie można użyć samej próby do wykrycia obciążenia—potrzeba niezależnej informacji o populacji\nDlatego porównanie sondaży z wynikami wyborów jest wartościowe: ujawnia obciążenia, które nie były możliwe do skwantyfikowania wcześniej\n\n\n\n\n\n\nPrzykład z życia: Porażki sondażowe\n\n\n\n\n\n\nStudium przypadku: Gdy sondaże mylą\n\n\n\nRozważmy scenariusz, w którym 20 sondaży pokazuje, że Kandydat A prowadzi o 3-5 punktów, z marginesami błędu około ±3%. Sondaże wydają się spójne, ale wygrywa Kandydat B.\nCo się stało?\n\nNie błąd próbkowania: Wszystkie sondaże się zgadzały—mało prawdopodobne przy samej losowej zmienności\nPrawdopodobnie błąd systematyczny:\n\nBłąd braku odpowiedzi: Pewni wyborcy konsekwentnie odmawiali udziału\nBłąd społecznej pożądaności (social desirability bias): Niektórzy wyborcy nieprawdziwie raportowali swoje preferencje\nBłąd modelowania frekwencji (turnout modeling error): Błędne założenia o tym, kto rzeczywiście będzie głosować\nBłąd pokrycia: Operat losowania (np. listy telefonów) systematycznie wykluczał pewne grupy\n\n\nLekcja: Spójność między sondażami nie gwarantuje trafności. Wszystkie sondaże mogą dzielić te same obciążenia systematyczne, dając fałszywą pewność w błędnych oszacowaniach.\n\n\n\n\n\nKluczowe wnioski\n\n\n\n\n\n\nNajważniejsze punkty\n\n\n\nRozumienie typów błędów:\n\nBłąd losowy to nieprzewidywalna zmienność, która średnio daje zero\n\nBłąd próbkowania: Z obserwowania próby, a nie całej populacji\nBłąd pomiaru: Z niedoskonałych narzędzi lub procesów pomiarowych\nRedukowany przez: większe próby, lepsze narzędzia, więcej pomiarów\n\nBłąd systematyczny (obciążenie) to konsekwentne odchylenie w jednym kierunku\n\nBłąd selekcji, błąd pomiaru, błąd odpowiedzi, błąd braku odpowiedzi, itp.\nRedukowany przez: lepszy projekt badania, nie większe próby\n\n\nKwantyfikowanie niepewności:\n\nBłąd standardowy mierzy typową zmienność próbkowania (jeden typ błędu losowego)\nMargines błędu ≈ 2 × SE daje zakres dla 95% ufności o niepewności próbkowania\nWielkość próby i precyzja błędu próbkowania są związane: \\text{SE} \\propto 1/\\sqrt{n}\n\nPoczwórna próba zmniejsza błąd próbkowania o połowę\nMalejące przychody wraz ze wzrostem n\n\nPrzedziały ufności dostarczają prawdopodobnych zakresów, ale zakładają brak obciążenia systematycznego\n\nKluczowe wnioski:\n\nPrecyzyjnie błędna odpowiedź (duża obciążona próba) jest często gorsza niż nieprecyzyjnie poprawna odpowiedź (mała nieobciążona próba)\nZawsze rozważ zarówno błąd próbkowania ORAZ potencjalne obciążenia systematyczne—publikowane marginesy błędu zazwyczaj ignorują te drugie\nTransparentność ma znaczenie: Raportuj metodologię, wskaźniki odpowiedzi i potencjalne obciążenia, nie tylko oszacowania punktowe i marginesy błędu\nWalidacja jest niezbędna: Porównuj oszacowania ze znanymi wartościami, gdy to możliwe, aby wykryć błędy systematyczne\n\n\n\n\n\n\n\n\n\nPriorytety praktyka\n\n\n\nPrzy projektowaniu badań:\nNajpierw: Minimalizuj błąd systematyczny poprzez staranny projekt\n\nReprezentatywne metody doboru próby\nWysokie wskaźniki odpowiedzi\nNieobciążone narzędzia pomiarowe\nWłaściwe sformułowanie pytań\n\nNastępnie: Optymalizuj wielkość próby, aby osiągnąć akceptowalną precyzję\n\nWiększe próby pomagają tylko po zajęciu się obciążeniem\nRównowaga między kosztem a poprawą precyzji\nPamiętaj o malejących przychodach\n\nNa koniec: Raportuj niepewność uczciwie\n\nJasno określ założenia\nPrzyznaj się do potencjalnych obciążeń\nNie pozwól, aby precyzyjne oszacowania stworzyły fałszywą pewność",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#próbkowanie-i-metody-próbkowania",
    "href": "rozdzial1.html#próbkowanie-i-metody-próbkowania",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.9 Próbkowanie i metody próbkowania (*)",
    "text": "2.9 Próbkowanie i metody próbkowania (*)\nPróbkowanie to proces wyboru podzbioru jednostek z populacji w celu oszacowania charakterystyk całej populacji. Sposób, w jaki próbkujemy, głęboko wpływa na to, co możemy wywnioskować z naszych danych.\n\nOperat losowania (Sampling Frame)\nZanim omówimy metody, musimy zrozumieć operat losowania — listę lub urządzenie, z którego pobieramy naszą próbę. Operat powinien idealnie obejmować każdego członka populacji dokładnie raz.\nPowszechne operaty losowania:\n\nListy wyborcze (dla dorosłych obywateli)\nKsiążki telefoniczne (coraz bardziej problematyczne z powodu telefonów komórkowych i numerów nienotowanych)\nListy adresowe z poczty\nRejestracje urodzeń (dla noworodków)\nListy zapisów do szkół (dla dzieci)\nRejestry podatkowe (dla osób zarabiających)\nZdjęcia satelitarne (dla mieszkań w odległych obszarach)\n\nProblemy z operatami losowania:\n\nNiepełne pokrycie (Undercoverage): Operat pomija członków populacji (bezdomni nieobecni na listach adresowych)\nNadmierne pokrycie (Overcoverage): Operat obejmuje osoby spoza populacji (zmarli nadal na listach wyborców)\nDuplikacja: Ta sama jednostka pojawia się wielokrotnie (osoby z wieloma numerami telefonów)\nGrupowanie (Clustering): Wielu członków populacji na jednostkę operatu (wiele rodzin pod jednym adresem)\n\n\n\nMetody próbkowania probabilistycznego\nPróbkowanie probabilistyczne daje każdemu członkowi populacji znane, niezerowe prawdopodobieństwo selekcji. To pozwala nam dokonywać wnioskowań statystycznych o populacji.\n\nProste losowanie (Simple Random Sampling - SRS)\nKażda możliwa próba o wielkości n ma równe prawdopodobieństwo selekcji. To złoty standard teorii statystycznej, ale często niepraktyczny dla dużych populacji.\nJak to działa:\n\nPonumeruj każdą jednostkę w populacji od 1 do N\nUżyj liczb losowych do wybrania n jednostek\nKażda jednostka ma prawdopodobieństwo n/N selekcji\n\nPrzykład: Aby wybrać próbę 50 uczniów ze szkoły liczącej 1000:\n\nPrzypisz każdemu uczniowi numer od 1 do 1000\nWygeneruj 50 losowych liczb między 1 a 1000\nWybierz uczniów z tymi numerami\n\nZalety:\n\nStatystycznie optymalny\nŁatwy do analizy\nNie wymaga dodatkowych informacji o populacji\n\nWady:\n\nWymaga kompletnego operatu losowania\nMoże być kosztowny (wybrane jednostki mogą być daleko od siebie)\nMoże nie reprezentować dobrze ważnych podgrup przez przypadek\n\n\n\nLosowanie systematyczne (Systematic Sampling)\nWybierz co k-ty element z uporządkowanego operatu losowania, gdzie k = N/n (interwał próbkowania).\nJak to działa:\n\nOblicz interwał próbkowania k = N/n\nLosowo wybierz punkt początkowy między 1 a k\nWybierz co k-tą jednostkę następnie\n\nPrzykład: Aby wybrać próbę 100 domów z 5000 na liście ulic:\n\nk = 5000/100 = 50\nLosowy start: 23\nPróba gospodarstw domowych: 23, 73, 123, 173, 223…\n\nZalety:\n\nProste do wdrożenia w terenie\nRozprzestrzenia próbę w całej populacji\n\nWady:\n\nMoże wprowadzić obciążenie, jeśli jest okresowość w operacie\n\nPrzykład ukrytej okresowości: Próbkowanie co 10. mieszkania w budynkach, gdzie mieszkania narożne (numery kończące się na 0) są wszystkie większe. To zawyżyłoby nasze oszacowanie średniej wielkości mieszkania.\n\n\nLosowanie warstwowe (Stratified Sampling)\nPodziel populację na jednorodne podgrupy (warstwy) przed próbkowaniem. Próbkuj niezależnie w każdej warstwie.\nJak to działa:\n\nPodziel populację na nienachodzące warstwy\nPróbkuj niezależnie z każdej warstwy\nPołącz wyniki z odpowiednimi wagami\n\nPrzykład: Badanie dochodu w mieście z odrębnymi dzielnicami:\n\nWarstwa 1: Dzielnica wysokich dochodów (10% populacji) - próba 100\nWarstwa 2: Dzielnica średnich dochodów (60% populacji) - próba 600\nWarstwa 3: Dzielnica niskich dochodów (30% populacji) - próba 300\n\nTypy alokacji:\nProporcjonalna: Wielkość próby w każdej warstwie proporcjonalna do wielkości warstwy\n\nJeśli warstwa ma 20% populacji, dostaje 20% próby\n\nOptymalna (Neymana): Większe próby z bardziej zmiennych warstw\n\nJeśli dochód bardziej się różni w obszarach wysokich dochodów, próbkuj tam więcej\n\nRówna: Ta sama wielkość próby na warstwę niezależnie od wielkości populacji\n\nPrzydatna, gdy porównywanie warstw jest głównym celem\n\nZalety:\n\nZapewnia reprezentację wszystkich podgrup\nMoże znacznie zwiększyć precyzję\nPozwala na różne metody próbkowania w warstwie\nDostarcza oszacowania dla każdej warstwy\n\nWady:\n\nWymaga informacji do utworzenia warstw\nMoże być trudna do badania\n\n\n\nLosowanie grupowe (Cluster Sampling)\nWybierz grupy (klastry) zamiast jednostek. Często używane, gdy populacja jest naturalnie pogrupowana lub gdy utworzenie kompletnego operatu jest trudne.\nJednostopniowe losowanie grupowe:\n\nPodziel populację na klastry\nLosowo wybierz niektóre klastry\nUwzględnij wszystkie jednostki z wybranych klastrów\n\nDwustopniowe losowanie grupowe:\n\nLosowo wybierz klastry (Pierwotne Jednostki Losowania)\nW wybranych klastrach losowo wybierz jednostki (Wtórne Jednostki Losowania)\n\nPrzykład: Badanie gospodarstw wiejskich w dużym kraju:\n\nEtap 1: Losowo wybierz 50 wsi z 1000 wsi\nEtap 2: W każdej wybranej wsi losowo wybierz 20 gospodarstw\nCałkowita próba: 50 × 20 = 1000 gospodarstw\n\nPrzykład wielostopniowy: Krajowe badanie zdrowotne:\n\nEtap 1: Wybierz województwa\nEtap 2: Wybierz powiaty w wybranych województwach\nEtap 3: Wybierz obwody spisowe w wybranych powiatach\nEtap 4: Wybierz gospodarstwa w wybranych obwodach\nEtap 5: Wybierz jednego dorosłego w wybranych gospodarstwach\n\nZalety:\n\nNie wymaga kompletnej listy populacji\nRedukuje koszty podróży (jednostki zgrupowane geograficznie)\nMoże używać różnych metod na różnych etapach\nNaturalne dla populacji hierarchicznych\n\nWady:\n\nMniej statystycznie efektywne niż SRS\nZłożona estymacja wariancji\nWiększe próby potrzebne dla tej samej precyzji\n\nEfekt projektu (Design Effect): Losowanie grupowe zazwyczaj wymaga większych prób niż SRS. Efekt projektu (DEFF) kwantyfikuje to:\n\\text{DEFF} = \\frac{\\text{Wariancja(próba grupowa)}}{\\text{Wariancja(SRS)}}\nJeśli DEFF = 2, potrzebujesz dwukrotnie większej próby, aby osiągnąć taką samą precyzję jak SRS.\n\n\n\nMetody próbkowania nieprobabilistycznego\nPróbkowanie nieprobabilistyczne nie gwarantuje znanych prawdopodobieństw selekcji. Choć ogranicza wnioskowanie statystyczne, te metody mogą być konieczne lub przydatne w pewnych sytuacjach.\n\nPróbkowanie wygodne (Convenience Sampling)\nSelekcja oparta wyłącznie na łatwości dostępu. Brak próby reprezentacji.\nPrzykłady:\n\nAnkietowanie studentów w twojej klasie o nawykach nauki\nWywiadowanie ludzi w centrum handlowym o preferencjach konsumenckich\nAnkiety online, w których każdy może uczestniczyć\nBadania medyczne używające wolontariuszy, którzy odpowiadają na ogłoszenia\n\nKiedy może być akceptowalne:\n\nBadania pilotażowe do testowania instrumentów ankietowych\nBadania eksploracyjne do identyfikacji problemów\nGdy badane procesy uważa się za uniwersalne\n\nGłówne problemy:\n\nBrak podstaw do wnioskowania o populacji\nPrawdopodobne poważne obciążenie selekcyjne\nWyniki mogą być całkowicie mylące\n\nPrawdziwy przykład: Sondaż prezydencki Literary Digest z 1936 roku ankietował 2,4 miliona osób (ogromna próba!), ale używał książek telefonicznych i członkostwa w klubach jako operatów podczas Wielkiego Kryzysu, dramatycznie nadreprezentując bogatych wyborców i niepoprawnie przewidując, że Landon pokona Roosevelta.\n\n\nPróbkowanie celowe (Purposive/Judgmental Sampling)\nCelowy wybór konkretnych przypadków oparty na osądzie badacza o tym, co jest „typowe” lub „interesujące”.\nPrzykłady:\n\nWybór „typowych” wsi do reprezentowania obszarów wiejskich\nWybór konkretnych grup wiekowych do badania rozwojowego\nWybór skrajnych przypadków do zrozumienia zakresu zmienności\nWybór przypadków bogatych w informacje do dogłębnego badania\n\nTypy próbkowania celowego:\nTypowy przypadek: Wybierz przeciętne lub normalne przykłady\n\nBadanie „typowych” polskich przedmieść\n\nSkrajny/dewiacyjny przypadek: Wybierz niezwykłe przykłady\n\nBadanie wsi z niezwykle niską śmiertelnością niemowląt, aby zrozumieć czynniki sukcesu\n\nMaksymalna zmienność: Celowo wybierz różnorodne przypadki\n\nWybór różnych szkół (miejskich/wiejskich, bogatych/biednych, dużych/małych) do badań edukacyjnych\n\nPrzypadek krytyczny: Wybierz przypadki, które będą definitywne\n\n„Jeśli to nie działa tutaj, nie zadziała nigdzie”\n\nKiedy jest przydatne:\n\nBadania jakościowe skupiające się na głębi nad szerokością\nGdy badane są rzadkie populacje\nOgraniczenia zasobów poważnie limitują wielkość próby\nFazy eksploracyjne badań\n\nProblemy:\n\nCałkowicie zależne od osądu badacza\nNiemożliwe wnioskowanie statystyczne\nRóżni badacze mogą wybrać różne „typowe” przypadki\n\n\n\nPróbkowanie kwotowe (Quota Sampling)\nSelekcja w celu dopasowania proporcji populacji w kluczowych charakterystykach. Jak losowanie warstwowe, ale bez losowej selekcji w grupach.\nJak działa próbkowanie kwotowe:\n\nZidentyfikuj kluczowe charakterystyki (wiek, płeć, rasa, wykształcenie)\nOkreśl proporcje populacji dla tych charakterystyk\nUstaw kwoty dla każdej kombinacji\nAnkieterzy wypełniają kwoty używając metod wygodnych\n\nSzczegółowy przykład: Sondaż polityczny z kwotami:\nProporcje populacji:\n\nMężczyzna 18-34: 15%\nMężczyzna 35-54: 20%\nMężczyzna 55+: 15%\nKobieta 18-34: 16%\nKobieta 35-54: 19%\nKobieta 55+: 15%\n\nDla próby 1000:\n\nWywiad z 150 mężczyznami w wieku 18-34\nWywiad z 200 mężczyznami w wieku 35-54\nI tak dalej…\n\nAnkieterzy mogą stać na rogach ulic, podchodząc do osób, które wydają się pasować do potrzebnych kategorii, aż kwoty zostaną wypełnione.\nDlaczego jest popularne w badaniach rynkowych:\n\nSzybsze niż próbkowanie probabilistyczne\nTańsze (brak ponownych kontaktów dla konkretnych osób)\nZapewnia reprezentację demograficzną\nNie wymaga operatu losowania\n\nDlaczego jest problematyczne dla wnioskowania statystycznego:\nUkryte obciążenie selekcyjne: Ankieterzy podchodzą do osób, które wyglądają na przystępne, dobrze mówią językiem, nie spieszą się — systematycznie wykluczając pewne typy w każdej komórce kwotowej.\nPrzykład obciążenia: Ankieter wypełniający kwotę dla „kobiet 18-34” może podchodzić do kobiet w centrum handlowym we wtorek po południu, systematycznie pomijając:\n\nKobiety pracujące w dni powszednie\nKobiety, których nie stać na zakupy w centrach handlowych\nKobiety z małymi dziećmi, które unikają centrów handlowych\nKobiety robiące zakupy online\n\nMimo że końcowa próba ma „właściwą” proporcję młodych kobiet, nie są one reprezentatywne dla wszystkich młodych kobiet.\nBrak miary błędu próbkowania: Bez prawdopodobieństw selekcji nie możemy obliczyć błędów standardowych ani przedziałów ufności.\nHistoryczna przestroga: Próbkowanie kwotowe było standardem w sondażach do wyborów prezydenckich w USA w 1948 roku, gdy sondaże używające próbkowania kwotowego niepoprawnie przewidziały, że Dewey pokona Trumana. Niepowodzenie doprowadziło do przyjęcia próbkowania probabilistycznego w sondażach.\n\n\nPróbkowanie kuli śnieżnej (Snowball Sampling)\nUczestnicy rekrutują dodatkowych uczestników ze swoich znajomych. Próba rośnie jak tocząca się kula śnieżna.\nJak to działa:\n\nZidentyfikuj początkowych uczestników (nasiona)\nPoproś ich o polecenie innych z wymaganymi charakterystykami\nPoproś nowych uczestników o dalsze polecenia\nKontynuuj, aż osiągnięta zostanie wielkość próby lub wyczerpią się polecenia\n\nPrzykład: Badanie nieudokumentowanych imigrantów:\n\nZacznij od 5 imigrantów, których możesz zidentyfikować\nKażdy poleca 3 innych, których zna\nTych 15 każdy poleca 2-3 innych\nKontynuuj, aż masz 100+ uczestników\n\nKiedy jest wartościowe:\nUkryte populacje: Grupy bez operatów losowania\n\nUżytkownicy narkotyków\nOsoby bezdomne\nOsoby z rzadkimi chorobami\nCzłonkowie ruchów podziemnych\n\nPopulacje połączone społecznie: Gdy relacje mają znaczenie\n\nBadanie efektów sieci społecznych\nBadanie transmisji chorób w społeczności\nZrozumienie dyfuzji informacji\n\nBadania zależne od zaufania: Gdy polecenia zwiększają uczestnictwo\n\nWrażliwe tematy, gdzie zaufanie jest niezbędne\nZamknięte społeczności podejrzliwe wobec obcych\n\nGłówne ograniczenia:\n\nPróby obciążone w kierunku osób współpracujących, dobrze połączonych\nOdizolowani członkowie populacji całkowicie pominięci\nWnioskowanie statystyczne generalnie niemożliwe\nMoże wzmacniać podziały społeczne (łańcuchy rzadko przekraczają granice społeczne)\n\nZaawansowana wersja — Próbkowanie sterowane przez respondentów (Respondent-Driven Sampling - RDS):\nPróbuje uczynić próbkowanie kuli śnieżnej bardziej rygorystycznym poprzez:\n\nŚledzenie, kto zrekrutował kogo\nOgraniczanie liczby poleceń na osobę\nWażenie na podstawie wielkości sieci\nUżywanie modeli matematycznych do korekty obciążenia\n\nNadal kontrowersyjne, czy RDS naprawdę pozwala na ważne wnioskowanie.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#pojęcia-prawdopodobieństwa-w-analizie-statystycznej",
    "href": "rozdzial1.html#pojęcia-prawdopodobieństwa-w-analizie-statystycznej",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.10 Pojęcia prawdopodobieństwa w analizie statystycznej",
    "text": "2.10 Pojęcia prawdopodobieństwa w analizie statystycznej\nChoć to przede wszystkim kurs statystyki, zrozumienie podstawowego prawdopodobieństwa jest niezbędne dla wnioskowania statystycznego.\n\nPodstawowe prawdopodobieństwo\nPrawdopodobieństwo kwantyfikuje niepewność na skali od 0 (niemożliwe) do 1 (pewne).\nPrawdopodobieństwo klasyczne: P(\\text{zdarzenie}) = \\frac{\\text{Liczba korzystnych wyników}}{\\text{Całkowita liczba możliwych wyników}}\nPrzykład: Prawdopodobieństwo, że losowo wybrana osoba jest kobietą \\approx 0,5\nPrawdopodobieństwo empiryczne: Oparte na obserwowanych częstościach\nPrzykład: W wiosce 423 z 1000 mieszkańców to kobiety, więc P(\\text{kobieta}) \\approx 0,423\n\n\nPrawdopodobieństwo warunkowe\nPrawdopodobieństwo warunkowe to prawdopodobieństwo zdarzenia A, przy założeniu że zdarzenie B wystąpiło: P(A|B)\nPrzykład demograficzny: Prawdopodobieństwo śmierci w ciągu roku przy danym wieku:\n\nP(\\text{śmierć w ciągu roku} | \\text{wiek 30}) \\approx 0,001\nP(\\text{śmierć w ciągu roku} | \\text{wiek 80}) \\approx 0,05\n\nTe prawdopodobieństwa warunkowe stanowią podstawę tablic trwania życia.\n\n\nNiezależność\nZdarzenia A i B są niezależne, jeśli P(A|B) = P(A).\nTestowanie niezależności w danych demograficznych:\nCzy wykształcenie i płodność są niezależne?\n\nP(\\text{3+ dzieci}) = 0,3 ogólnie\nP(\\text{3+ dzieci} | \\text{wykształcenie wyższe}) = 0,15\nRóżne prawdopodobieństwa wskazują na zależność\n\n\n\nPrawo wielkich liczb\nGdy wielkość próby wzrasta, statystyki z próby zbiegają się do parametrów populacji.\nDemonstracja: Szacowanie proporcji płci przy urodzeniu:\n\n10 urodzeń: 7 chłopców (70% - bardzo niestabilne)\n100 urodzeń: 53 chłopców (53% - zbliżamy się do ~51,2%)\n1000 urodzeń: 515 chłopców (51,5% - całkiem blisko)\n10 000 urodzeń: 5118 chłopców (51,18% - bardzo blisko)\n\n\n\nWizualizacja Prawa wielkich liczb: rzuty monetą\nZobaczmy to w działaniu na przykładzie rzutów monetą. Uczciwa moneta ma 50% szansy na wypadnięcie orła, ale poszczególne rzuty są nieprzewidywalne.\n\n# Symulacja rzutów monetą i pokazanie zbieżności\nset.seed(42)\nn_flips &lt;- 1000\nflips &lt;- rbinom(n_flips, 1, 0.5)  # 1 = orzeł, 0 = reszka\n\n# Obliczanie skumulowanej proporcji orłów\ncumulative_prop &lt;- cumsum(flips) / seq_along(flips)\n\n# Utworzenie ramki danych do wizualizacji\nlln_data &lt;- data.frame(\n  flip_number = 1:n_flips,\n  cumulative_proportion = cumulative_prop\n)\n\n# Wykres zbieżności\nggplot(lln_data, aes(x = flip_number, y = cumulative_proportion)) +\n  geom_line(color = \"steelblue\", alpha = 0.7) +\n  geom_hline(yintercept = 0.5, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_hline(yintercept = c(0.45, 0.55), color = \"red\", linetype = \"dotted\", alpha = 0.7) +\n  labs(\n    title = \"Prawo wielkich liczb: Proporcje rzutów monetą zbiegają do 0,5\",\n    x = \"Liczba rzutów monetą\",\n    y = \"Skumulowana proporcja orłów\",\n    caption = \"Czerwona linia przerywana = prawdziwe prawdopodobieństwo (0,5)\\nLinie kropkowane = zakres ±5%\"\n  ) +\n  scale_y_continuous(limits = c(0.3, 0.7), breaks = seq(0.3, 0.7, 0.1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCo to pokazuje:\n\nPoczątkowe rzuty wykazują duże wahania (pierwsze 10 rzutów może dać 70% lub 30% orłów)\nW miarę dodawania kolejnych rzutów, proporcja stabilizuje się wokół 50%\n„Szum” poszczególnych wyników się uśrednia w czasie\n\n\n\nSformułowanie matematyczne\nNiech A oznacza zdarzenie nas interesujące (np. „orzeł w rzucie monetą”, „głos na partię X”, „suma kostek równa 7”). Jeśli P(A) = p i obserwujemy n niezależnych prób z tym samym rozkładem (i.i.d.), to częstość próbkowa zdarzenia A:\n\\hat{p}_n = \\frac{\\text{liczba wystąpień zdarzenia } A}{n}\nzbiega do p gdy n rośnie.\n\n\nPrzykłady w różnych kontekstach\nPrzykład z kostkami: Zdarzenie „suma = 7” przy dwóch kostkach ma prawdopodobieństwo 6/36 ≈ 16,7\\%, podczas gdy „suma = 4” ma 3/36 ≈ 8,3\\%. Przy wielu rzutach suma 7 pojawia się około dwa razy częściej niż suma 4.\nSondaże wyborcze: Jeśli poparcie populacyjne dla partii wynosi p, to przy losowym doborze próby o wielkości n obserwowana częstość \\hat{p}_n będzie zbliżać się do p w miarę wzrostu n (zakładając losowy dobór i niezależność prób).\nKontrola jakości: Jeśli 2% produktów jest wadliwych, to w dużych partiach około 2% zostanie uznanych za wadliwe (zakładając niezależną produkcję).\n\n\nDlaczego to ma znaczenie dla statystyki\nWniosek: Losowość stanowi podstawę wnioskowania statystycznego, przekształcając niepewność poszczególnych wyników w przewidywalne rozkłady dla estymatorów. Prawo wielkich liczb gwarantuje, że „szum” poszczególnych wyników się uśrednia, pozwalając nam:\n\nPrzewidywać długookresowe częstości\nKwantyfikować niepewność (marginesy błędu)\nWyciągać rzetelne wnioski z prób\nFormułować probabilistyczne stwierdzenia o populacjach\n\nTa zasada działa w sondażach, eksperymentach, a nawet w zjawiskach kwantowych (w interpretacji częstościowej).\n\n\n\nCentralne Twierdzenie Graniczne (CTG)\nCentralne Twierdzenie Graniczne stwierdza, że rozkład średnich próbkowych zbliża się do rozkładu normalnego wraz ze wzrostem wielkości próby, niezależnie od kształtu pierwotnego rozkładu populacji. Jest to prawdziwe nawet dla wysoce skośnych lub nienormalnych rozkładów populacji.\n\nImplikacje\n\nPróg Wielkości Próby: Wielkość próby n ≥ 30 jest zazwyczaj wystarczająca, aby zastosować CTG\nBłąd Standardowy: Odchylenie standardowe średnich próbkowych wynosi σ/√n, gdzie σ to odchylenie standardowe populacji\nFundament Statystyczny: Możemy dokonywać wnioskowań o parametrach populacji używając właściwości rozkładu normalnego, nawet gdy dane bazowe nie są normalne\n\n\n\nDlaczego To Ma Znaczenie\nRozważmy dane o dochodach, które zazwyczaj są prawostronnie skośne z długim ogonem wysokich zarobków. Podczas gdy indywidualne dochody nie podlegają rozkładowi normalnemu, dzieje się coś niezwykłego, gdy wielokrotnie pobieramy próby i obliczamy ich średnie:\nCo właściwie oznacza “normalnie rozłożone średnie próbkowe”:\n\nJeśli weźmiesz wiele różnych grup 30+ osób i obliczysz średni dochód każdej grupy\nTe średnie grupowe utworzą wzór w kształcie dzwonu po nanieseniu na wykres\nWiększość średnich grupowych skupi się blisko prawdziwej średniej populacji\nPrawdopodobieństwo otrzymania średniej grupowej daleko od średniej populacji staje się przewidywalne\n\nTen przewidywalny wzór (rozkład normalny) pozwala nam:\n\nObliczać przedziały ufności używając właściwości rozkładu normalnego\nPrzeprowadzać testy hipotez statystycznych\nDokonywać przewidywań dotyczących średnich próbkowych ze znanym prawdopodobieństwem\n\nKonkretny Przykład: Wyobraź sobie miasto, w którym indywidualne dochody wahają się od 80 000 zł do 40 000 000 zł, silnie skośne w prawo. Jeśli:\n\nLosowo wybierzesz 100 osób i obliczysz ich średni dochód: powiedzmy 300 000 zł\nPowtórzysz to 1000 razy (1000 różnych grup po 100 osób)\nNaniesieszz na wykres te 1000 średnich grupowych: utworzą krzywą dzwonową wycentrowaną wokół prawdziwej średniej populacji\nOkoło 95% tych średnich grupowych znajdzie się w przewidywalnym zakresie\nDzieje się tak mimo że indywidualne dochody są skrajnie skośne!\n\n\n\nPodstawy Matematyczne\nDla populacji ze średnią μ i skończoną wariancją σ²:\n\nRozkład próbkowy średniej: \\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n}) gdy n \\to \\infty\nBłąd standardowy średniej: SE_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\nStandaryzowana średnia próbkowa: Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1) dla dużych n\n\n\n\nNajważniejsze Wnioski\n\nUniwersalne Zastosowanie: CTG ma zastosowanie do każdego rozkładu ze skończoną wariancją\nZbieżność do Normalności: Aproksymacja do rozkładu normalnego poprawia się wraz ze wzrostem wielkości próby\nFundament Wnioskowania: Większość parametrycznych testów statystycznych opiera się na CTG\nKwestie Wielkości Próby: Chociaż n ≥ 30 jest podstawową wytyczną, wysoce skośne rozkłady mogą wymagać większych próbek dla dokładnej aproksymacji",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#istotność-statystyczna-wprowadzenie",
    "href": "rozdzial1.html#istotność-statystyczna-wprowadzenie",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.11 Istotność Statystyczna: Wprowadzenie",
    "text": "2.11 Istotność Statystyczna: Wprowadzenie\nWyobraź sobie, że rzucasz monetą 10 razy i wypadło 8 orłów. Czy moneta jest fałszywa, czy po prostu miałeś szczęście? To jest kluczowe pytanie, na które pomaga odpowiedzieć istotność statystyczna (wnioskowanie statystyczne).\nIstotność statystyczna to miara (p-value) tego, na ile możemy być pewni, że wzorce obserwowane w naszej próbie nie są dziełem przypadku. Gdy wynik jest statystycznie istotny (zwykle przyjmujemy p-value &lt; 0.05), oznacza to, że prawdopodobieństwo uzyskania takich danych przy braku rzeczywistego efektu jest bardzo niskie.\nIstotność statystyczna pomaga nam rozróżnić między rzeczywistymi zjawiskami a przypadkowymi fluktuacjami w danych. Gdy mówimy, że wynik jest statystycznie istotny, znaczy to, że prawdopodobnie nie powstał przez zwykły zbieg okoliczności.\n\nAnalogia do Sali Sądowej\nTestowanie hipotez statystycznych działa jak proces karny:\n\nHipoteza Zerowa (H_0): Oskarżony jest niewinny (nie ma efektu)\nHipoteza Alternatywna (H_1): Oskarżony jest winny (efekt istnieje)\nDowody: Twoje dane i wyniki testów\nWerdykt: “Winny” (odrzuć H_0) lub “Niewinny” (nie odrzucaj H_0)\n\nKluczowe rozróżnienie: “Niewinny” ≠ “Niewinny”\n\nWerdykt “niewinny” oznacza niewystarczające dowody do skazania\nPodobnie, “brak istotności statystycznej” oznacza niewystarczające dowody na istnienie efektu, NIE dowód braku efektu\n\n\n\nBrak efektu (“Domniemanie niewinności”)\nW statystyce zawsze zaczynamy od założenia, że nic specjalnego się nie dzieje:\n\nHipoteza Zerowa (H_0): “Nie ma efektu”\n\nMoneta jest uczciwa\nNowy lek nie działa\nCzas nauki nie wpływa na wyniki w nauce\n\nHipoteza Alternatywna (H_1): “Efekt ISTNIEJE”\n\nMoneta jest fałszywa\nLek działa\nWięcej nauki poprawia oceny\n\n\nKluczowa zasada: Podtrzymujemy hipotezę zerową (niewinność), chyba że dane dostarczą mocnych dowodów przeciwko niej — “ponad wszelką wątpliwość” w terminologii prawnej, lub “p &lt; 0,05” w terminologii statystycznej.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wartość-p-p-value-twój-miernik-zaskoczenia",
    "href": "rozdzial1.html#wartość-p-p-value-twój-miernik-zaskoczenia",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.12 Wartość p (p-value): Twój “Miernik Zaskoczenia”",
    "text": "2.12 Wartość p (p-value): Twój “Miernik Zaskoczenia”\nWartość p odpowiada na jedno konkretne pytanie:\n\n“Gdyby nic specjalnego się nie działo (hipoteza zerowa jest prawdziwa), jak zaskakujące byłyby nasze wyniki?”\n\n\nWartość p, p-wartość, prawdopodobieństwo testowe (ang. p-value, probability value) – prawdopodobieństwo uzyskania wyników testu co najmniej tak samo skrajnych, jak te zaobserwowane w rzeczywistości (w próbie badawczej), obliczone przy założeniu, że hipoteza zerowa (brak efektu, różnicy, itp.) jest prawdziwa.\n\n\nTrzy Sposoby Myślenia o Wartościach p\n\n1. Skala Zaskoczenia\n\np &lt; 0,01: Bardzo zaskakujące! (Mocne dowody przeciwko H_0)\np &lt; 0,05: Dość zaskakujące (Umiarkowane dowody przeciwko H_0)\np &gt; 0,05: Niezbyt zaskakujące (Niewystarczające dowody przeciwko H_0)\n\n\n\n2. Konkretny Przykład: Podejrzana Moneta\nRzucasz monetą 10 razy i wypadło 8 orłów. Jaka jest wartość p?\nObliczenie: Jeśli moneta byłaby uczciwa, prawdopodobieństwo uzyskania 8 lub więcej orłów wynosi:\np = P(≥8 \\text{ orłów w 10 rzutach}) \\approx 0.055 \\approx 5.5\\%\nP(X \\geq 8) = \\sum_{k=8}^{10} \\binom{10}{k} 0,5^{10} = \\frac{56}{1024} \\approx 0,0547\nInterpretacja: Jest 5,5% szans na uzyskanie tak ekstremalnych wyników z uczciwą monetą. To trochę nietypowe, ale nie jest to skrajnie nieprawdopodobny wynik.\n\n\n3. Formalna Definicja\nWartość p to prawdopodobieństwo uzyskania wyników co najmniej tak ekstremalnych jak zaobserwowane, zakładając że hipoteza zerowa jest prawdziwa.\n\n\n\n\n\n\nWarning\n\n\n\nCzęsty Błąd: Wartość p NIE jest prawdopodobieństwem, że hipoteza zerowa jest prawdziwa! Zakłada ona, że hipoteza zerowa jest prawdziwa i mówi, jak nietypowe byłyby twoje dane w tym świecie (w którym H_0 jest prawdziwa).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#błąd-rozumowania-prokuratorskiego-ostrzeżenie",
    "href": "rozdzial1.html#błąd-rozumowania-prokuratorskiego-ostrzeżenie",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.13 Błąd Rozumowania Prokuratorskiego: Ostrzeżenie",
    "text": "2.13 Błąd Rozumowania Prokuratorskiego: Ostrzeżenie\n\nWyjaśnienie błędu\nWyobraź sobie taką scenę w sądzie:\nProkurator: “Jeśli oskarżony byłby niewinny, istnieje tylko 1% szans, że znaleźlibyśmy jego DNA na miejscu zbrodni. Znaleźliśmy jego DNA. Zatem istnieje 99% pewności, że jest winny!”\nTo BŁĄD! Prokurator pomylił:\n\nP(Dowód | Niewinny) = 0,01 ← To, co wiemy\nP(Niewinny | Dowód) = ? ← To, co chcemy wiedzieć (ale nie możemy tego wywnioskować z samej wartości p!)\n\n\nGdy otrzymujemy p = 0,01, kuszące jest myślenie:\n❌ ŹLE: “Jest tylko 1% szans, że hipoteza zerowa jest prawdziwa”\n❌ ŹLE: “Jest 99% szans, że nasze leczenie działa”\n✅ DOBRZE: “Jeśli hipoteza zerowa byłaby prawdziwa, istnieje tylko 1% szans, że zobaczylibyśmy tak ekstremalne dane”\n\n\nDlaczego to ważne: Prosty przykład testu medycznego\nWyobraź sobie test na rzadką chorobę, który jest dokładny w 99%:\n\nJeśli masz chorobę, test jest pozytywny w 99% przypadków\nJeśli nie masz choroby, test jest negatywny w 99% przypadków (czyli 1% wyników fałszywie pozytywnych)\n\nOto klucz: Załóżmy, że tylko 1 na 1000 osób faktycznie ma tę chorobę.\nPrzetestujmy 10 000 osób:\n\n10 osób ma chorobę → 10 ma pozytywny wynik testu (w zaokrągleniu)\n9 990 osób nie ma choroby → około 100 ma pozytywny wynik przez pomyłkę (1% z 9 990)\nŁącznie pozytywnych testów: 110\n\nJeśli twój test jest pozytywny, jakie jest prawdopodobieństwo, że rzeczywiście masz chorobę?\n\nTylko 10 ze 110 pozytywnych testów to prawdziwe przypadki\nTo około 9%, nie 99%!\n\n\n\nAnalogia do badań naukowych\nTo samo dzieje się w badaniach:\n\nGdy testujemy wiele hipotez (jak testowanie wielu potencjalnych leków)\nWiększość nie działa (jak większość ludzi nie ma rzadkiej choroby)\nNawet przy “istotnych” wynikach (jak pozytywny test), większość odkryć może być fałszywie pozytywna\n\n\n\n\n\n\n\nImportant\n\n\n\nWartość p mówi ci, jak zaskakujące byłyby twoje dane, GDYBY hipoteza zerowa była prawdziwa. Nie mówi ci o prawdopodobieństwie, że hipoteza zerowa JEST prawdziwa.\nPomyśl o tym tak: Prawdopodobieństwo, że ziemia będzie mokra, JEŚLI padało, jest zupełnie inne niż prawdopodobieństwo, że padało, JEŚLI ziemia jest mokra — ziemia mogła być mokra od zraszacza!\nPamiętaj: Wartość p mówi ci P(Dane | Hipoteza zerowa jest prawdziwa), nie P(Hipoteza zerowa jest prawdziwa | Dane). To tak różne jak P(Mokra ziemia | Deszcz) i P(Deszcz | Mokra ziemia) — ziemia może być mokra od zraszacza!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wprowadzenie-do-analizy-regresji-modelowanie-relacji-między-zmiennymi",
    "href": "rozdzial1.html#wprowadzenie-do-analizy-regresji-modelowanie-relacji-między-zmiennymi",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.14 Wprowadzenie do analizy regresji: Modelowanie relacji między zmiennymi",
    "text": "2.14 Wprowadzenie do analizy regresji: Modelowanie relacji między zmiennymi\nJednym z najważniejszych narzędzi w analizie statystycznej jest analiza regresji — metoda zrozumienia i kwantyfikacji relacji między zmiennymi.\nPodstawowa idea jest prosta: Jak jedna rzecz odnosi się do drugiej i czy możemy użyć tej relacji do dokonywania przewidywań (np. jak liczba lat nauki wpływa na dochody?)?\n\nW jednym zdaniu: Regresja pomaga nam zrozumieć, jak różne zjawiska są ze sobą powiązane w skomplikowanym świecie, gdzie wszystko wpływa na wszystko inne.\n\n\nCzym jest analiza regresji?\nWyobraź sobie, że jesteś ciekawy relacji między wykształceniem a dochodem. Zauważasz, że ludzie z większym wykształceniem zwykle zarabiają więcej pieniędzy, ale chcesz zrozumieć tę relację bardziej precyzyjnie:\n\nO ile średnio każdy dodatkowy rok edukacji zwiększa dochód?\nJak silna jest ta relacja?\nCzy są inne czynniki, które powinniśmy rozważyć?\nCzy możemy przewidzieć prawdopodobny dochód kogoś, jeśli znamy jego poziom wykształcenia?\n\nAnaliza regresji w sposób systematyczny odpowiada na te pytania — szuka najlepiej dopasowanego opisu relacji między zmiennymi.\n\n\nZmienne i Zmienność\nZmienna to każda charakterystyka, która może przyjmować różne wartości dla różnych jednostek obserwacji. W naukach politycznych:\n\nJednostki analizy: Kraje, osoby, wybory, polityki, lata\nZmienne: PKB, preferencje wyborcze, wskaźnik demokracji, wystąpienie konfliktu\n\n\n💡 Mówiąc Prosto: Zmienna to wszystko, co się zmienia. Gdyby wszyscy głosowali tak samo, “preferencje wyborcze” nie byłyby zmienną - byłyby stałą. Badamy zmienne, ponieważ chcemy zrozumieć, dlaczego rzeczy się różnią.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRozważmy typowy nagłówek prasowy przed wyborami: „Poparcie dla kandydata Kowalskiego sięga 68%.” Najprawdopodobniej wyciągniesz wniosek, że Kowalski ma dobre perspektywy wyborcze—nie gwarantowane zwycięstwo, ale silną pozycję. Intuicyjnie rozumiesz, że wyższe poparcie zwykle przekłada się na lepsze wyniki wyborcze, nawet jeśli związek ten nie jest doskonały.\nTa intuicyjna ocena ilustruje istotę analizy regresji. Wykorzystałeś jedną informację (wskaźnik poparcia), aby przewidzieć inny wynik (sukces wyborczy). Co więcej, rozpoznałeś zarówno związek między tymi zmiennymi, jak i niepewność związaną z twoją prognozą.\nChociaż takie nieformalne rozumowanie dobrze nam służy w życiu codziennym, ma istotne ograniczenia. O ile lepsze są szanse Kowalskiego przy 68% poparciu w porównaniu do 58%? Co się dzieje, gdy musimy jednocześnie uwzględnić wiele czynników—poparcie, sytuację gospodarczą i status urzędującego kandydata? Jak pewni powinniśmy być naszych prognoz?\nAnaliza regresji dostarcza systematycznych ram do odpowiedzi na te pytania. Przekształca nasze intuicyjne rozumienie związków w precyzyjne modele matematyczne, które można testować i udoskonalać. Dzięki analizie regresji badacze mogą:\n\nGenerować precyzyjne prognozy: Wyjść poza ogólne oceny ku konkretnym liczbowym szacunkom—na przykład przewidywać nie tylko, że Kowalski „prawdopodobnie wygra”, ale oszacować oczekiwany procent głosów i zakres prawdopodobnych wyników.\nOkreślić, które czynniki są najważniejsze: Ustalić względne znaczenie różnych zmiennych—być może odkrywając, że warunki gospodarcze wpływają na wybory silniej niż wskaźniki poparcia.\nOkreślić ilościowo niepewność prognoz: Dokładnie zmierzyć, jak pewni powinniśmy być naszych przewidywań, rozróżniając między niemal pewnymi wynikami a edukowanymi przypuszczeniami.\nTestować propozycje teoretyczne danymi empirycznymi: Ocenić, czy nasze przekonania o związkach przyczynowo-skutkowych sprawdzają się, gdy testujemy je systematycznie na wielu obserwacjach.\n\n\nW istocie analiza regresji systematyzuje rozpoznawanie wzorców, które wykonujemy intuicyjnie, dostarczając narzędzi do tego, aby nasze prognozy były dokładniejsze, nasze porównania bardziej znaczące, a nasze wnioski bardziej wiarygodne.\n\n\n\nModel Podstawowy\nModel reprezentuje obiekt, osobę lub system w sposób informatywny. Modele dzielą się na reprezentacje fizyczne (takie jak modele architektoniczne) i abstrakcyjne (takie jak równania matematyczne opisujące dynamikę atmosfery).\nRdzeń myślenia statystycznego można wyrazić jako:\nY = f(X) + \\text{błąd}\nTo równanie stwierdza, że nasz wynik (Y) równa się jakiejś funkcji naszych predyktorów (X), plus nieprzewidywalna zmienność.\nSkładniki:\n\nY = Zmienna zależna (zjawisko, które chcemy wyjaśnić)\nX = Zmienna(e) niezależna(e) (czynniki wyjaśniające)\nf() = Związek funkcyjny (często zakładamy liniowy)\nbłąd (\\epsilon) = Niewyjaśniona zmienność\n\n\n💡 Co To Naprawdę Oznacza: Można to porównać do przepisu kulinarnego. Ocena z przedmiotu (Y) zależy od godzin nauki (X), ale nie doskonale. Dwóch studentów uczących się 10 godzin może otrzymać różne oceny z powodu stresu przed egzaminem, wcześniejszej wiedzy czy po prostu szczęścia (składnik błędu). Regresja znajduje średni związek.\n\nTen model stanowi podstawę całej analizy statystycznej - od prostych korelacji po złożone algorytmy uczenia maszynowego.\nRegresja pomaga odpowiedzieć na fundamentalne pytania takie jak:\n\nO ile edukacja zwiększa uczestnictwo polityczne?\nJakie czynniki przewidują sukces wyborczy?\nCzy instytucje demokratyczne promują wzrost gospodarczy?\n\n\n\n\n\n\n\nPodstawowa idea: Rysowanie najlepszej linii przez punkty\n\nProsta regresja liniowa (Simple Linear Regression)\nZacznijmy od najprostszego przypadku: relacji między dwiema zmiennymi. Załóżmy, że rysujemy wykształcenie (lata nauki) na osi x i roczny dochód na osi y dla 100 osób. Zobaczylibyśmy chmurę punktów, a regresja znajduje prostą linię, która najlepiej reprezentuje wzorzec w tych punktach.\nCo czyni linię „najlepszą”? Linia regresji minimalizuje całkowitą sumę kwadratów pionowych odległości od wszystkich punktów do linii. Pomyśl o tym jako o znalezieniu linii, która tworzy najmniejszy całkowity błąd predykcji.\nRównanie tej linii to: Y = a + bX + \\text{błąd}\nLub w naszym przykładzie: \\text{Dochód} = a + b \\times \\text{Wykształcenie} + \\text{błąd}\nGdzie:\n\na (wyraz wolny/intercept) = przewidywany dochód przy zerowym wykształceniu\nb (nachylenie/slope) = zmiana dochodu na każdy dodatkowy rok wykształcenia\nbłąd (e) = różnica między rzeczywistym a przewidywanym dochodem\n\nInterpretacja wyników:\nJeśli nasza analiza znajduje: \\text{Dochód} = 15000 + 4000 \\times \\text{Wykształcenie}\nTo mówi nam:\n\nKtoś z 0 latami wykształcenia przewidywany jest na zarobki 15 000 zł\nKażdy dodatkowy rok wykształcenia jest związany z 4000 zł większym dochodem\nKtoś z 12 latami wykształcenia przewidywany jest na zarobki: 15 000 + (4000 × 12) = 63 000 zł\nKtoś z 16 latami (licencjat) przewidywany jest na zarobki: 15 000 + (4000 × 16) = 79 000 zł\n\n\n\n\nZrozumienie relacji vs. dowodzenie przyczynowości\nKluczowe rozróżnienie: regresja pokazuje związek (association), niekoniecznie przyczynowość (causation). Nasza regresja wykształcenie-dochód pokazuje, że są powiązane, ale nie dowodzi, że wykształcenie powoduje wyższy dochód. Inne wyjaśnienia są możliwe:\n\nOdwrotna przyczynowość: Może bogatsze rodziny mogą sobie pozwolić na więcej edukacji dla swoich dzieci\nWspólna przyczyna: Być może inteligencja lub motywacja wpływa zarówno na wykształcenie, jak i dochód\nZbieg okoliczności: W małych próbach wzorce mogą pojawić się przez przypadek\n\nPrzykład pozornej korelacji: Regresja może pokazać, że sprzedaż lodów silnie przewiduje utopienia. Czy lody powodują utopienia? Nie! Oba wzrastają latem (wspólna przyczyna, confounding variable).\n\n\n\nRegresja wieloraka (Multiple Regression): Kontrolowanie innych czynników\nRzeczywistość jest skomplikowana — wiele czynników wpływa na wyniki jednocześnie. Regresja wieloraka pozwala nam badać jedną relację, jednocześnie „kontrolując” lub „utrzymując na stałym poziomie” inne zmienne.\n\nMoc kontroli statystycznej\nWracając do wykształcenia i dochodu, możemy się zastanawiać: Czy efekt wykształcenia wynika tylko z tego, że wykształceni ludzie są zwykle z bogatszych rodzin lub mieszkają w miastach? Regresja wieloraka może oddzielić te efekty:\n\\text{Dochód} = a + b_1 \\times \\text{Wykształcenie} + b_2 \\times \\text{Wiek} + b_3 \\times \\text{Miasto} + b_4 \\times \\text{Dochód rodziców} + \\text{błąd}\nTeraz b_1 reprezentuje efekt wykształcenia po uwzględnieniu wieku, lokalizacji i pochodzenia rodzinnego. Jeśli b_1 = 3000, oznacza to: „Porównując osoby w tym samym wieku, lokalizacji i pochodzeniu rodzinnym, każdy dodatkowy rok wykształcenia jest związany z 3000 zł większym dochodem.”\nPrzykład demograficzny: Płodność i wykształcenie kobiet\nBadacze badający płodność mogą znaleźć: \\text{Dzieci} = 4,5 - 0,3 \\times \\text{Wykształcenie}\nTo sugeruje, że każdy rok wykształcenia kobiet jest związany z 0,3 mniej dzieci. Ale czy wykształcenie jest przyczyną, czy wykształcone kobiety różnią się w innych aspektach? Dodając kontrole:\n\\text{Dzieci} = a - 0,15 \\times \\text{Wykształcenie} - 0,2 \\times \\text{Miasto} + 0,1 \\times \\text{Wykształcenie męża} - 0,4 \\times \\text{Dostęp do antykoncepcji}\nTeraz widzimy, że związek wykształcenia jest słabszy (-0,15 zamiast -0,3) po uwzględnieniu zamieszkania w mieście i dostępu do antykoncepcji. To sugeruje, że część pozornego efektu wykształcenia działa przez te inne ścieżki.\n\n\n\nTypy zmiennych w regresji\n\nZmienna wynikowa (zależna)\nTo jest to, co próbujemy zrozumieć lub przewidzieć:\n\nDochód w naszym pierwszym przykładzie\nLiczba dzieci w naszym przykładzie płodności\nOczekiwana długość życia w badaniach zdrowotnych\nPrawdopodobieństwo migracji w badaniach populacyjnych\n\n\n\nZmienne predykcyjne (niezależne)\nTo są czynniki, które według nas mogą wpływać na wynik:\n\nIlościowe: Wiek, lata wykształcenia, dochód, odległość\nJakościowe (kategorialne): Płeć, rasa, stan cywilny, region\nBinarne (dummy): Miasto/wieś, zatrudniony/bezrobotny, żonaty/nieżonaty\n\nObsługa zmiennych kategorialnych: Nie możemy bezpośrednio wstawić „religii” do równania. Zamiast tego tworzymy zmienne binarne:\n\nChrześcijanin = 1 jeśli chrześcijanin, 0 w przeciwnym razie\nMuzułmanin = 1 jeśli muzułmanin, 0 w przeciwnym razie\nBuddysta = 1 jeśli buddysta, 0 w przeciwnym razie\n(Jedna kategoria staje się grupą referencyjną)\n\n\n\n\nRóżne typy regresji dla różnych wyników\nPodstawowa idea regresji dostosowuje się do wielu sytuacji:\n\nRegresja liniowa\nDla wyników ilościowych (dochód, wzrost, ciśnienie krwi): Y = a + b_1X_1 + b_2X_2 + … + \\text{błąd}\n\n\nRegresja logistyczna\nDla wyników binarnych (zmarł/przeżył, wyemigrował/został, żonaty/nieżonaty):\nZamiast przewidywać wynik bezpośrednio, przewidujemy prawdopodobieństwo: \\log\\left(\\frac{p}{1-p}\\right) = a + b_1X_1 + b_2X_2 + …\nGdzie p to prawdopodobieństwo wystąpienia zdarzenia.\nPrzykład: Przewidywanie prawdopodobieństwa migracji na podstawie wieku, wykształcenia i stanu cywilnego. Model może stwierdzić, że młodzi, wykształceni, nieżonaci ludzie mają 40% prawdopodobieństwo migracji, podczas gdy starsi, mniej wykształceni, żonaci ludzie mają tylko 5% prawdopodobieństwo.\n\n\nRegresja Poissona\nDla wyników “zliczeniowych”/count data (liczba dzieci, liczba wizyt u lekarza): \\log(\\text{oczekiwana liczba}) = a + b_1X_1 + b_2X_2 + …\nPrzykład: Modelowanie liczby dzieci na podstawie charakterystyk kobiet. Przydatne, ponieważ zapewnia, że przewidywania nigdy nie są ujemne (nie można mieć -0,5 dziecka!).\n\n\nAnaliza przeżycia (model Coxa)/Regresja hazardu\nDo czego służy: Przewidywanie kiedy coś się stanie, nie tylko czy się stanie.\nProblem: Wyobraź sobie, że badasz jak długo trwają małżeństwa. Obserwujesz 1000 par przez 10 lat, ale na koniec badania: - 400 par się rozwiodło (wiesz dokładnie kiedy) - 600 par jest nadal w małżeństwie (nie wiesz czy/kiedy się rozwiodą)\nZwykła regresja nie radzi sobie z tym problemem “niekompletnej historii” — te 600 trwających małżeństw zawiera cenne informacje, ale nie znamy jeszcze ich zakończenia.\nJak pomagają modele Coxa: Zamiast próbować przewidzieć dokładny moment, skupiają się na ryzyku względnym — kto ma większą szansę na wcześniejsze doświadczenie zdarzenia. To jak pytanie “W dowolnym momencie, kto jest bardziej narażony?” zamiast “Dokładnie kiedy to się stanie?”\nZastosowania praktyczne: - Badania medyczne: Kto szybciej reaguje na leczenie? - Biznes: Którzy klienci wcześniej rezygnują z subskrypcji? - Nauki społeczne: Jakie czynniki powodują, że wydarzenia życiowe następują wcześniej/później?\n\n\n\n\nInterpretacja wyników regresji\n\nWspółczynniki\nWspółczynnik mówi nam o oczekiwanej zmianie wyniku przy wzroście predyktora o jedną jednostkę, przy zachowaniu stałości innych zmiennych.\nPrzykłady interpretacji:\nRegresja liniowa dla dochodu:\n\n„Każdy dodatkowy rok wykształcenia jest związany z 3500 zł wyższym rocznym dochodem, kontrolując wiek i doświadczenie”\n\nRegresja logistyczna dla śmiertelności niemowląt:\n\n„Każda dodatkowa wizyta prenatalna jest związana z 15% niższymi szansami śmierci niemowlęcia, kontrolując wiek i wykształcenie matki”\n\nRegresja wieloraka dla oczekiwanej długości życia:\n\n„Każde 1000 USD wzrostu PKB per capita jest związane z 0,4 roku dłuższą oczekiwaną długością życia, po kontroli wykształcenia i dostępu do opieki zdrowotnej”\n\n\n\nIstotność statystyczna\nRegresja testuje również, czy relacje mogą wynikać z przypadku:\n\nwartość p &lt; 0,05: Relacja nieprawdopodobna z powodu przypadku (statystycznie istotna)\nwartość p &gt; 0,05: Relacja może być prawdopodobnie losową zmiennością\n\n\nAle pamiętaj: Istotność statystyczna ≠ praktyczne znaczenie (“praktyczna istotność”). Przy dużych próbach malutkie efekty stają się „istotne”.\n\n\n\nPrzedziały ufności dla współczynników\nTak jak mamy przedziały ufności dla średnich lub propocji, mamy je dla współczynników regresji:\n„Efekt wykształcenia na dochód wynosi 3500 zł rocznie, 95% CI: [2800 zł, 4200 zł]”\nTo oznacza, że jesteśmy 95% pewni, że prawdziwy efekt mieści się między 2800 zł a 4200 zł.\n\n\nR-kwadrat: Jak dobrze model pasuje do danych?\nR^2 (R-kwadrat) mierzy proporcję zmienności wyniku wyjaśnioną przez predyktory:\n\nR^2 = 0: Predyktory nic nie wyjaśniają\nR^2 = 1: Predyktory wyjaśniają wszystko\nR^2 = 0,3: Predyktory wyjaśniają 30% zmienności\n\nPrzykład: Model dochodu z tylko wykształceniem może mieć R^2 = 0,15 (wykształcenie wyjaśnia 15% zmienności dochodu). Dodanie wieku, doświadczenia i lokalizacji może zwiększyć R^2 do 0,35 (razem wyjaśniają 35%).\n\n\n\n\n\n\nZałożenia i ograniczenia\n\n\n\nRegresja opiera się na założeniach, które mogą nie być spełnione:\n\nEgzogeniczność (brak ukrytych zależności)\nNajważniejsze założenie: predyktory nie mogą być skorelowane z błędami. Prościej mówiąc, nie powinny istnieć ukryte czynniki wpływające jednocześnie na zmienne objaśniające i wynik.\nPrzykład: Badając wpływ edukacji na dochód, ale pomijając “zdolności”, otrzymasz obciążone wyniki - zdolności wpływają zarówno na poziom wykształcenia, jak i dochód. To założenie zapisujemy jako: E[\\varepsilon | X] = 0\nDlaczego to kluczowe: Bez tego wszystkie twoje współczynniki są błędne, nawet przy milionach obserwacji!\n\n\nLiniowość\nZakłada związki prostoliniowe. A co jeśli wpływ edukacji na dochód jest silniejszy na wyższych poziomach? Możemy dodać człony wielomianowe: \\text{Dochód} = a + b_1 \\times \\text{Edukacja} + b_2 \\times \\text{Edukacja}^2\n\n\nNiezależność\nZakłada, że obserwacje są niezależne. Ale członkowie rodziny mogą być podobni, powtarzane pomiary tej samej osoby są powiązane, a sąsiedzi mogą na siebie wpływać. Specjalne metody radzą sobie z tymi zależnościami.\n\n\nHomoskedastyczność\nZakłada stałą wariancję błędów. Ale błędy predykcji mogą być większe dla osób o wysokich dochodach niż niskich. Wykresy diagnostyczne pomagają to wykryć.\n\n\nNormalność\nZakłada, że błędy mają rozkład normalny. Ważne dla małych prób i testów hipotez, mniej krytyczne dla dużych prób.\nUwaga: Pierwsze założenie (egzogeniczność) dotyczy otrzymania poprawnej odpowiedzi. Pozostałe dotyczą głównie precyzji i wnioskowania statystycznego. Naruszenie egzogeniczności oznacza, że model jest fundamentalnie błędny; naruszenie pozostałych oznacza, że przedziały ufności i p-wartości mogą być niedokładne.\n\n\n\n\n\n\n\n\n\nCzęste pułapki statystyczne\n\n\n\n\nEndogeniczność (obciążenie pominiętą zmienną): Zapominanie o ukrytych czynnikach wpływających zarówno na X jak i Y, co narusza fundamentalne założenie egzogeniczności. Przykład: Badanie edukacja→dochód bez uwzględnienia zdolności.\nSymultaniczność/Odwrotna przyczynowość: Gdy X i Y określają się wzajemnie w tym samym czasie. Prosta regresja zakłada jednokierunkową przyczynowość, ale rzeczywistość często jest dwukierunkowa. Przykład: Cena wpływa na popyt ORAZ popyt wpływa na cenę jednocześnie.\nZmienne zakłócające (confounding): Nieuwzględnienie zmiennych wpływających zarówno na predyktor jak i wynik, co prowadzi do pozornych zależności. Przykład: Sprzedaż lodów koreluje z utonięciami (oba powodowane przez lato).\nBłąd selekcji: Nielosowe próby systematycznie wykluczające pewne grupy, uniemożliwiające generalizację. Przykład: Badanie użycia internetu tylko wśród posiadaczy smartfonów.\nBłąd ekologiczny: Zakładanie, że wzorce grupowe dotyczą jednostek. Przykład: Bogate kraje mają niższą dzietność ≠ bogaci ludzie mają mniej dzieci.\nP-hacking (drążenie danych): Testowanie wielu hipotez aż do znalezienia istotności, lub modyfikowanie analizy aż p &lt; 0,05. Przy 20 testach spodziewasz się 1 fałszywego wyniku przez przypadek!\nPrzeuczenie (overfitting): Budowanie modelu zbyt złożonego dla twoich danych - idealny na danych treningowych, bezużyteczny do predykcji. Pamiętaj: Z wystarczającą liczbą parametrów możesz dopasować słonia.\nBłąd przetrwania: Analizowanie tylko “ocalałych” ignorując porażki. Przykład: Badanie firm sukcesu pomijając te, które zbankrutowały.\nNadmierna generalizacja: Rozszerzanie wniosków poza badaną populację, okres czasu lub kontekst. Przykład: Wyniki z amerykańskich studentów ≠ uniwersalne zachowanie ludzkie.\n\nPamiętaj: Pierwsze trzy to formy endogeniczności - naruszają E[\\varepsilon|X]=0 i sprawiają, że współczynniki są fundamentalnie błędne. Pozostałe czynią wyniki mylącymi lub niereprezentatywnymi.\n\n\n\n\n\n\nZastosowania w demografii\n\nAnaliza płodności\nZrozumienie, jakie czynniki wpływają na decyzje o płodności: \\text{Dzieci} = f(\\text{Wykształcenie, Dochód, Miasto, Religia, Antykoncepcja, …})\nPomaga zidentyfikować dźwignie polityczne dla krajów zaniepokojonych wysoką lub niską płodnością.\n\n\nModelowanie śmiertelności\nPrzewidywanie oczekiwanej długości życia lub ryzyka śmiertelności: \\text{Ryzyko śmiertelności} = f(\\text{Wiek, Płeć, Palenie, Wykształcenie, Dostęp do opieki zdrowotnej, …})\nUżywane przez firmy ubezpieczeniowe, urzędników zdrowia publicznego i badaczy.\n\n\nPrzewidywanie migracji\nZrozumienie, kto migruje i dlaczego: P(\\text{Migracja}) = f(\\text{Wiek, Wykształcenie, Zatrudnienie, Więzi rodzinne, Odległość, …})\nPomaga przewidywać przepływy populacji i planować zmiany demograficzne.\n\n\nMałżeństwo i rozwód\nAnalizowanie formowania i rozpadu związków: P(\\text{Rozwód}) = f(\\text{Wiek przy małżeństwie, Dopasowanie wykształcenia, Dochód, Dzieci, Czas trwania, …})\nInformuje politykę społeczną i usługi wsparcia.\n\n\n\nPowszechne pułapki i jak ich unikać\n\nPrzeuczenie (Overfitting)\nWłączenie zbyt wielu predyktorów może sprawić, że model idealnie pasuje do twojej próby, ale zawiedzie z nowymi danymi. Jak zapamiętywanie odpowiedzi na egzamin zamiast zrozumienia pojęć.\nRozwiązanie: Użyj prostszych modeli, walidacji krzyżowej lub zarezerwuj niektóre dane do testowania.\n\n\nWspółliniowość (Multicollinearity)\nGdy predyktory są silnie skorelowane (np. lata wykształcenia i poziom stopnia), model nie może oddzielić ich efektów.\nRozwiązanie: Wybierz jedną zmienną lub połącz je w indeks.\n\n\nObciążenie pominiętej zmiennej (Omitted Variable Bias)\nPominięcie ważnych zmiennych może sprawić, że inne efekty wydają się silniejsze lub słabsze niż naprawdę są.\nPrzykład: Relacja między sprzedażą lodów a wskaźnikami przestępczości znika, gdy kontrolujesz temperaturę.\n\n\nEkstrapolacja\nUżywanie modelu poza zakresem obserwowanych danych.\nPrzykład: Jeśli twoje dane obejmują wykształcenie od 0-20 lat, nie przewiduj dochodu dla kogoś z 30 latami wykształcenia.\n\n\n\nIntuicje\nPomyśl o regresji jako o wyrafinowanej technice uśredniania:\n\nProsta średnia: „Średni dochód wynosi 50 000 zł”\nŚrednia warunkowa: „Średni dochód dla absolwentów uczelni wynosi 70 000 zł”\nRegresja: „Średni dochód dla 35-letnich absolwentów uczelni w obszarach miejskich wynosi 78 000 zł”\n\nKażda dodana zmienna czyni nasze przewidywanie bardziej konkretnym i (miejmy nadzieję) dokładniejszym.\n\n\nRegresja w praktyce: Kompletny przykład\nPytanie badawcze: Jakie czynniki wpływają na wiek przy pierwszym porodzie?\nDane: Badanie 1000 kobiet, które miały co najmniej jedno dziecko\nZmienne:\n\nWynik: Wiek przy pierwszym porodzie (lata)\nPredyktory: Wykształcenie (lata), Miasto (0/1), Dochód (tysiące), Religijność (0/1)\n\nWynik prostej regresji: \\text{Wiek przy pierwszym porodzie} = 18 + 0,8 \\times \\text{Wykształcenie}\nInterpretacja: Każdy rok wykształcenia związany z 0,8 roku późniejszym pierwszym porodem.\nWynik regresji wielorakiej: \\text{Wiek przy pierwszym porodzie} = 16 + 0,5 \\times \\text{Wykształcenie} + 2 \\times \\text{Miasto} + 0,03 \\times \\text{Dochód} - 1,5 \\times \\text{Religijność}\nInterpretacja:\n\nEfekt wykształcenia zredukowany, ale nadal dodatni (0,5 roku na rok wykształcenia)\nKobiety miejskie mają pierwsze porody 2 lata później\nKażde 1000 zł dochodu związane z 0,03 roku (11 dni) później\nReligijne kobiety mają pierwsze porody 1,5 roku wcześniej\nR^2 = 0,42 (model wyjaśnia 42% zmienności)\n\nTen bogatszy model pomaga nam zrozumieć, że efekt wykształcenia częściowo działa przez zamieszkanie w mieście i dochód.\n\n\n\n\n\n\nWarning\n\n\n\nRegresja jest bramą do zaawansowanego modelowania statystycznego. Gdy zrozumiesz podstawową koncepcję — używanie zmiennych do przewidywania wyników i kwantyfikowania relacji — możesz eksplorować:\n\nEfekty interakcji: Gdy efekt jednej zmiennej zależy od innej\nRelacje nieliniowe: Krzywe, progi i złożone wzorce\nModele wielopoziomowe: Uwzględnianie zgrupowanych danych (uczniowie w szkołach, ludzie w dzielnicach)\nRegresja szeregów czasowych: Analizowanie zmian w czasie\nRozszerzenia uczenia maszynowego: Lasy losowe, sieci neuronowe i więcej\n\nKluczowy wgląd pozostaje: Próbujemy zrozumieć, jak rzeczy odnoszą się do siebie w systematyczny, kwantyfikowalny sposób.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#jakość-i-źródła-danych",
    "href": "rozdzial1.html#jakość-i-źródła-danych",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.15 Jakość i źródła danych",
    "text": "2.15 Jakość i źródła danych\nŻadna analiza nie jest lepsza niż dane, na których się opiera. Zrozumienie problemów jakości danych jest kluczowe dla badań demograficznych i społecznych.\n\nWymiary jakości danych\nDokładność (Accuracy): Jak blisko pomiarów są prawdziwe wartości?\nPrzykład: Raportowanie wieku często pokazuje „skupianie” na okrągłych liczbach (30, 40, 50), ponieważ ludzie zaokrąglają swój wiek.\nKompletność (Completeness): Jaka proporcja populacji jest objęta?\nPrzykład: Kompletność rejestracji urodzeń różni się znacznie:\n\nKraje rozwinięte: &gt;99%\nNiektóre kraje rozwijające się: &lt;50%\n\nAktualność (Timeliness): Jak aktualne są dane?\nPrzykład: Spis przeprowadzany co 10 lat staje się coraz bardziej nieaktualny, szczególnie w szybko zmieniających się obszarach.\nSpójność (Consistency): Czy definicje i metody są stabilne w czasie i przestrzeni?\nPrzykład: Definicja „miasta” różni się między krajami, utrudniając międzynarodowe porównania.\nDostępność (Accessibility): Czy badacze i decydenci mogą faktycznie używać danych?\n\n\nPowszechne źródła danych w demografii\nSpis powszechny (Census): Kompletne wyliczenie populacji\nZalety:\n\nKompletne pokrycie (w teorii)\nDane dla małych obszarów dostępne\nPunkt odniesienia dla innych oszacowań\n\nWady:\n\nDrogie i rzadkie\nNiektóre populacje trudne do policzenia\nOgraniczone zbierane zmienne\n\nRejestry urzędu stanu cywilnego (Vital Registration): Ciągłe rejestrowanie urodzeń, zgonów, małżeństw\nZalety:\n\nCiągłe i aktualne\nWymóg prawny zapewnia zgodność\nInformacje o medycznej przyczynie śmierci\n\nWady:\n\nPokrycie różni się według poziomu rozwoju\nJakość kodowania przyczyny śmierci się różni\nOpóźniona rejestracja powszechna w niektórych obszarach\n\nBadania próbkowe (Sample Surveys): Szczegółowe dane z podzbioru populacji\nPrzykłady:\n\nBadania demograficzne i zdrowotne (DHS)\nAmerykańskie Badanie Społeczności (ACS)\nBadania Siły Roboczej (np. BAEL GUS)\n\nZalety:\n\nMożna zbierać szczegółowe informacje\nCzęstsze niż spis\nMożna skupić się na konkretnych tematach\n\nWady:\n\nObecny błąd próbkowania\nMałe obszary niereprezentowane\nObciążenie odpowiedzi może zmniejszyć jakość\n\nRejestry administracyjne (Administrative Records): Dane zbierane do celów niestatystycznych\nPrzykłady:\n\nRejestry podatkowe\nZapisy szkolne\nRoszczenia ubezpieczenia zdrowotnego\nDane telefonii komórkowej\n\nZalety:\n\nJuż zebrane (bez dodatkowego obciążenia)\nCzęsto kompletne dla objętej populacji\nCiągle aktualizowane\n\nWady:\n\nPokrycie może być selektywne\nDefinicje mogą nie odpowiadać potrzebom badawczym\nDostęp często ograniczony\n\n\n\nProblemy jakości danych specyficzne dla demografii\nSkupianie wieku (Age Heaping): Tendencja do raportowania wieku kończącego się na 0 lub 5\nWykrywanie: Oblicz Indeks Whipple’a lub Indeks Myersa\nWpływ: Wpływa na wskaźniki specyficzne dla wieku i projekcje\nPreferencja cyfr (Digit Preference): Raportowanie niektórych końcowych cyfr częściej niż innych\nPrzykład: Wagi urodzeniowe często raportowane jako 3000g, 3500g zamiast dokładnych wartości\nObciążenie przypominania (Recall Bias): Trudność dokładnego przypominania przeszłych wydarzeń\nPrzykład: „Ile razy odwiedziłeś lekarza w zeszłym roku?” Często niedoszacowane dla częstych odwiedzających, przeszacowane dla rzadkich odwiedzających.\nRaportowanie przez pełnomocnika (Proxy Reporting): Informacje dostarczane przez kogoś innego\nWyzwanie: Głowa gospodarstwa domowego raportująca za wszystkich członków może nie znać dokładnego wieku lub wykształcenia każdego",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#względy-etyczne-w-demografii-statystycznej",
    "href": "rozdzial1.html#względy-etyczne-w-demografii-statystycznej",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.16 Względy etyczne w demografii statystycznej",
    "text": "2.16 Względy etyczne w demografii statystycznej\nStatystyka to nie tylko liczby — dotyczy prawdziwych ludzi i ma prawdziwe konsekwencje.\n\nŚwiadoma zgoda\nUczestnicy powinni zrozumieć:\n\nCel zbierania danych\nJak dane będą używane\nRyzyka i korzyści\nIch prawo do odmowy lub wycofania się\n\nWyzwanie w demografii: Uczestnictwo w spisie jest często obowiązkowe, co rodzi pytania etyczne o zgodę.\n\n\nPoufność i prywatność\nStatystyczna kontrola ujawniania: Ochrona tożsamości jednostek w opublikowanych danych\nMetody obejmują:\n\nTłumienie małych komórek (np. „&lt;5” zamiast „2”)\nAgregacja geograficzna\n\nPrzykład: W tabeli zawodu według wieku według płci dla małego miasta może być tylko jedna lekarka w wieku 60-65 lat, co czyni ją identyfikowalną.\n\n\nReprezentacja i uczciwość\nKto jest liczony?: Decyzje o tym, kogo uwzględnić, wpływają na reprezentację\n\nWięźniowie: Gdzie są liczeni — lokalizacja więzienia czy adres domowy?\nBezdomni: Jak zapewnić pokrycie?\nNieudokumentowani imigranci: Uwzględnić czy wykluczyć?\n\nPrywatność różnicowa (Differential Privacy): Matematyczna struktura ochrony prywatności przy zachowaniu użyteczności statystycznej\nKompromis: Większa ochrona prywatności = mniej dokładne statystyki\n\n\nNiewłaściwe użycie statystyk\nWybieranie wisienek (Cherry-Picking): Wybieranie tylko korzystnych wyników\nPrzykład: Raportowanie spadku ciąż nastolatek od roku szczytowego zamiast pokazywania pełnego trendu\nP-Hacking: Manipulowanie analizą w celu osiągnięcia istotności statystycznej\nBłąd ekologiczny: Wnioskowanie relacji indywidualnych z danych grupowych\nPrzykład: Powiaty z większą liczbą imigrantów mają wyższe średnie dochody ≠ imigranci mają wyższe dochody\n\n\nOdpowiedzialne raportowanie\nKomunikacja niepewności: Zawsze raportuj przedziały ufności lub marginesy błędu\nDostarczanie kontekstu: Uwzględnij odpowiednie grupy porównawcze i trendy historyczne\nUznanie ograniczeń: Jasno określ, co dane mogą i nie mogą pokazać",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#powszechne-nieporozumienia-w-statystyce",
    "href": "rozdzial1.html#powszechne-nieporozumienia-w-statystyce",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.17 Powszechne nieporozumienia w statystyce",
    "text": "2.17 Powszechne nieporozumienia w statystyce\nZrozumienie, czym statystyka NIE jest, jest równie ważne jak zrozumienie, czym jest.\n\nNieporozumienie 1: „Statystyki mogą udowodnić wszystko”\nRzeczywistość: Statystyki mogą dostarczyć tylko dowodów, nigdy absolutnego dowodu. A właściwa statystyka, uczciwie zastosowana, znacznie ogranicza wnioski.\nPrzykład: Badanie znajduje korelację między sprzedażą lodów a utopieniami. Statystyka nie „dowodzi”, że lody powodują utopienia — oba są związane z letnią pogodą.\n\n\nNieporozumienie 2: „Większe próby są zawsze lepsze”\nRzeczywistość: Poza pewnym punktem większe próby dodają niewiele precyzji, ale mogą dodać obciążenie.\nPrzykład: Ankieta online z 1 milionem odpowiedzi może być mniej dokładna niż próba probabilistyczna 1000 osób z powodu obciążenia samoselekcji.\nMalejące zyski:\n\nn = 100: Margines błędu \\approx 10 pp.\nn = 1000: Margines błędu \\approx 3,2 pp.\nn = 10 000: Margines błędu \\approx 1 pp.\nn = 100 000: Margines błędu \\approx 0,32 pp.\n\nSkok z 10 000 do 100 000 ledwo poprawia precyzję, ale kosztuje 10\\times więcej.\n\n\nNieporozumienie 3: “Istotność statystyczna = Praktyczne znaczenie”\nRzeczywistość: Przy dużych próbach malutkie różnice stają się „statystycznie istotne”, nawet jeśli są bez znaczenia.\nPrzykład: Badanie 100 000 osób stwierdza, że mężczyźni są średnio o 0,1 cm wyżsi (p &lt; 0,001). Statystycznie istotne, ale praktycznie nieistotne.\n\n\nNieporozumienie 4: “Korelacja implikuje przyczynowość”\nRzeczywistość: Korelacja jest konieczna, ale niewystarczająca dla przyczynowości.\nKlasyczne przykłady:\n\nMiasta z większą liczbą kościołów mają więcej przestępstw (oba korelują z wielkością populacji)\nKraje z większą liczbą telewizorów mają dłuższą oczekiwaną długość życia (oba korelują z rozwojem)\n\n\n\nNieporozumienie 5: “Losowy oznacza przypadkowy”\nRzeczywistość: Statystyczna losowość jest starannie kontrolowana i systematyczna.\nPrzykład: Losowe próbkowanie wymaga starannej procedury, a nie tylko chwytania kogokolwiek wygodnego.\n\n\nNieporozumienie 6: “Średnia reprezentuje wszystkich”\nRzeczywistość: Średnie mogą być mylące, gdy rozkłady są skośne lub wielomodalne.\nPrzykład: Średni dochód bywalców baru wynosi 50 000 zł. Bill Gates wchodzi. Teraz średnia wynosi 1 milion zł. Rzeczywisty dochód nikogo się nie zmienił.\n\n\nNieporozumienie 7: “Przeszłe wzorce gwarantują przyszłe wyniki”\nRzeczywistość: Ekstrapolacja zakłada, że warunki pozostają stałe.\nPrzykład: Liniowa projekcja wzrostu populacji z lat 1950-2000 źle przeszacowałaby populację 2050 roku, ponieważ pomija spadek płodności.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zastosowania-w-demografii-1",
    "href": "rozdzial1.html#zastosowania-w-demografii-1",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.18 Zastosowania w demografii",
    "text": "2.18 Zastosowania w demografii\n\nSzacowanie i projekcja populacji\nOszacowania międzyspisowe: Szacowanie populacji między spisami\nMetoda komponentów: P(t+1) = P(t) + B - D + I - E\nGdzie:\n\nP(t) = Populacja w czasie t\nB = Urodzenia\nD = Zgony\nI = Imigracja\nE = Emigracja\n\nKażdy komponent szacowany z różnych źródeł z różnymi strukturami błędów.\nProjekcje populacji: Prognozowanie przyszłej populacji\nMetoda komponentów kohortowych:\n\nPrognozuj wskaźniki przeżycia według wieku\nPrognozuj wskaźniki płodności\nPrognozuj wskaźniki migracji\nZastosuj do populacji bazowej\nZagreguj wyniki\n\nNiepewność wzrasta z horyzontem projekcji.\n\n\nObliczanie wskaźników demograficznych\nWskaźniki surowe (Crude Rates): Zdarzenia na 1000 populacji\n\\text{Surowy współczynnik urodzeń} = \\frac{\\text{Urodzenia}}{\\text{Populacja w połowie roku}} \\times 1000\nWskaźniki specyficzne dla wieku (Age-Specific Fertility Rate): Kontrola struktury wieku\n\\text{Współczynnik płodności specyficzny dla wieku} = \\frac{\\text{Urodzenia kobietom w wieku } x}{\\text{Kobiety w wieku } x} \\times 1000\nStandaryzacja: Porównywanie populacji z różnymi strukturami\nStandaryzacja bezpośrednia: Zastosuj wskaźniki populacji do standardowej struktury wieku Standaryzacja pośrednia: Zastosuj standardowe wskaźniki do struktury wieku populacji\n\n\nAnaliza tablic trwania życia\nTablice życia podsumowują doświadczenie śmiertelności populacji.\nKluczowe kolumny:\n\nq_x: Prawdopodobieństwo śmierci między wiekiem x a x+1\nl_x: Liczba przeżywających do wieku x (ze 100 000 urodzeń)\nd_x: Zgony między wiekiem x a x+1\nL_x: Osobo-lata przeżyte między wiekiem x a x+1\ne_x: Oczekiwana długość życia w wieku x\n\nPrzykład interpretacji: Jeśli q_{65} = 0,015, to 1,5% 65-latków umrze przed osiągnięciem 66 lat. Jeśli e_{65} = 18,5, to 65-latkowie średnio żyją jeszcze 18,5 roku.\n\n\nAnaliza płodności\nWspółczynnik dzietności całkowitej (TFR - Total Fertility Rate): Średnia liczba dzieci na kobietę przy obecnych wskaźnikach płodności specyficznych dla wieku (ASFR - Age-Specific Fertility Rate)\n\\text{TFR} = \\sum (\\text{ASFR} \\times \\text{szerokość przedziału wieku})\nPrzykład: Jeśli każda 5-letnia grupa wiekowa od 15-49 ma ASFR = 20 na 1000: \\text{TFR} = 7 \\text{ grup wiekowych} \\times \\frac{20}{1000} \\times 5 \\text{ lat} = 0,7 \\text{ dzieci na kobietę}\nTen bardzo niski TFR wskazuje na płodność poniżej poziomu zastępowalności.\n\n\nAnaliza migracji\nWspółczynnik migracji netto: \\text{NMR} = \\frac{\\text{Imigranci} - \\text{Emigranci}}{\\text{Populacja}} \\times 1000\nWskaźnik efektywności migracji: \\text{MEI} = \\frac{|\\text{Napływ} - \\text{Odpływ}|}{\\text{Napływ} + \\text{Odpływ}}\n\nWartości blisko 0: Wysoka rotacja, mała zmiana netto\nWartości blisko 1: Głównie przepływ jednokierunkowy\n\n\n\nMetryki zdrowia populacji\nLata życia skorygowane o niepełnosprawność (DALYs): Utracone lata zdrowego życia\nDALY = Utracone lata życia (YLL) + Lata przeżyte z niepełnosprawnością (YLD)\nOczekiwana długość życia w zdrowiu: Oczekiwane lata w dobrym zdrowiu\nŁączy informacje o śmiertelności i chorobowości.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#oprogramowanie-i-narzędzia",
    "href": "rozdzial1.html#oprogramowanie-i-narzędzia",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.19 Oprogramowanie i narzędzia",
    "text": "2.19 Oprogramowanie i narzędzia\nWspółczesna statystyka demograficzna opiera się w dużej mierze na narzędziach obliczeniowych.\n\nPakiety oprogramowania statystycznego\nR: Darmowy, otwarty, rozbudowane pakiety demograficzne\n\nPakiety: demography, popReconstruct, bayesPop\nZalety: Powtarzalne badania, najnowocześniejsze metody\nWady: Stroma krzywa uczenia\n\nStata: Szeroko używany w naukach społecznych\n\nMocne strony: Analiza danych z badań, dane panelowe\nPowszechny w: Ekonomii, epidemiologii\n\nSPSS: Przyjazny interfejs użytkownika\n\nMocne strony: Interfejs wskaż-i-kliknij\nPowszechny w: Naukach społecznych, badaniach rynkowych\n\nPython: Język programowania ogólnego przeznaczenia z bibliotekami statystycznymi\n\nBiblioteki: pandas, numpy, scipy, statsmodels\nZalety: Integracja z innymi aplikacjami",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zakończenie",
    "href": "rozdzial1.html#zakończenie",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.20 Zakończenie",
    "text": "2.20 Zakończenie\n\nPodsumowanie kluczowych terminów\nStatystyka: Nauka o zbieraniu, organizowaniu, analizowaniu, interpretowaniu i prezentowaniu danych w celu zrozumienia zjawisk i wsparcia podejmowania decyzji\nStatystyka opisowa: Metody podsumowywania i prezentowania danych w znaczący sposób bez rozszerzania wniosków poza obserwowane dane\nStatystyka wnioskowania: Techniki wyciągania wniosków o populacjach z prób, w tym estymacja i testowanie hipotez\nPopulacja: Kompletny zbiór jednostek, obiektów lub pomiarów, o których chcemy wyciągnąć wnioski\nPróba: Podzbiór populacji, który jest faktycznie obserwowany lub mierzony w celu dokonania wniosków o populacji\nSuperpopulacja: Teoretyczna nieskończona populacja, z której obserwowane skończone populacje są uważane za próby\nParametr: Liczbowa charakterystyka populacji (zazwyczaj nieznana i oznaczana literami greckimi)\nStatystyka: Liczbowa charakterystyka obliczona z danych z próby (znana i oznaczana literami łacińskimi)\nEstymator: Reguła lub formuła do obliczania oszacowań parametrów populacji z danych z próby\nEstimand: Konkretny parametr populacji będący celem estymacji\nOszacowanie: Wartość liczbowa uzyskana przez zastosowanie estymatora do obserwowanych danych\nBłąd losowy: Nieprzewidywalna zmienność wynikająca z procesu próbkowania, która maleje z większymi próbami\nBłąd systematyczny (Obciążenie): Konsekwentne odchylenie od prawdziwych wartości, którego nie można zmniejszyć przez zwiększenie wielkości próby\nPróbkowanie: Proces wyboru podzbioru jednostek z populacji do pomiaru\nOperat losowania: Lista lub urządzenie, z którego pobierana jest próba, idealnie zawierające wszystkich członków populacji\nPróbkowanie probabilistyczne: Metody próbkowania, w których każdy członek populacji ma znane, niezerowe prawdopodobieństwo selekcji\nProste losowanie: Każda możliwa próba wielkości n ma równe prawdopodobieństwo selekcji\nLosowanie systematyczne: Wybór co k-tego elementu z uporządkowanego operanta losowania\nLosowanie warstwowe: Podział populacji na jednorodne podgrupy przed próbkowaniem w każdej\nLosowanie grupowe: Wybór grup (klastrów) zamiast jednostek\nPróbkowanie nieprobabilistyczne: Metody próbkowania bez gwarantowanych znanych prawdopodobieństw selekcji\nPróbkowanie wygodne: Wybór oparty wyłącznie na łatwości dostępu\nPróbkowanie celowe: Celowy wybór oparty na osądzie badacza\nPróbkowanie kwotowe: Wybór w celu dopasowania proporcji populacji w kluczowych charakterystykach bez losowej selekcji\nPróbkowanie kuli śnieżnej: Uczestnicy rekrutują dodatkowych uczestników ze swoich znajomych\nBłąd standardowy: Odchylenie standardowe rozkładu próbkowania statystyki\nMargines błędu: Maksymalna oczekiwana różnica między oszacowaniem a parametrem przy określonym poziomie ufności\nPrzedział ufności: Zakres prawdopodobnych wartości dla parametru przy określonym poziomie ufności\nPoziom ufności: Prawdopodobieństwo, że metoda przedziału ufności wytworzy przedziały zawierające parametr\nDane: Zebrane obserwacje lub pomiary\nDane ilościowe: Pomiary liczbowe (ciągłe lub dyskretne)\nDane jakościowe: Informacje kategoryczne (nominalne lub porządkowe)\nRozkład danych: Opis tego, jak wartości rozkładają się na możliwe wyniki\nRozkład częstości: Podsumowanie pokazujące, jak często każda wartość występuje w danych\nCzęstość bezwzględna: Liczba obserwacji dla każdej wartości\nCzęstość względna: Proporcja obserwacji w każdej kategorii\nCzęstość skumulowana: Suma bieżąca częstości do każdej wartości",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#załącznik-a-visualizations-for-statistics-demography",
    "href": "rozdzial1.html#załącznik-a-visualizations-for-statistics-demography",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.21 Załącznik A: Visualizations for Statistics & Demography",
    "text": "2.21 Załącznik A: Visualizations for Statistics & Demography\n\n## ============================================\n## Visualizations for Statistics & Demography\n## Chapter 1: Foundations\n## ============================================\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(gridExtra)\nlibrary(scales)\nlibrary(patchwork)  # for combining plots\n\n# Set theme for all plots\ntheme_set(theme_minimal(base_size = 12))\n\n# Color palette for consistency\ncolors &lt;- c(\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#6A994E\")\n\n\n# ==================================================\n# 1. POPULATION vs SAMPLE VISUALIZATION\n# ==================================================\n\n# Create a population and sample visualization\nset.seed(123)\n\n# Generate population data (e.g., ages of 10,000 people)\npopulation &lt;- data.frame(\n  id = 1:10000,\n  age = round(rnorm(10000, mean = 40, sd = 15))\n)\npopulation$age[population$age &lt; 0] &lt;- 0\npopulation$age[population$age &gt; 100] &lt;- 100\n\n# Take a random sample\nsample_size &lt;- 500\nsample_data &lt;- population[sample(nrow(population), sample_size), ]\n\n# Create visualization\np1 &lt;- ggplot(population, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = colors[1], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = mean(population$age), \n             color = colors[2], linetype = \"dashed\", size = 1.2) +\n  labs(title = \"Population Distribution (N = 10,000)\",\n       subtitle = paste(\"Population mean (μ) =\", round(mean(population$age), 2), \"years\"),\n       x = \"Age (years)\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(sample_data, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = colors[3], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = mean(sample_data$age), \n             color = colors[4], linetype = \"dashed\", size = 1.2) +\n  labs(title = paste(\"Sample Distribution (n =\", sample_size, \")\"),\n       subtitle = paste(\"Sample mean (x̄) =\", round(mean(sample_data$age), 2), \"years\"),\n       x = \"Age (years)\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Combine plots\npopulation_sample_plot &lt;- p1 / p2\nprint(population_sample_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 2. TYPES OF DATA DISTRIBUTIONS\n# ==================================================\n\n# Generate different distribution types\nset.seed(456)\nn &lt;- 5000\n\n# Normal distribution\nnormal_data &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Right-skewed distribution (income-like)\nright_skewed &lt;- rgamma(n, shape = 2, scale = 15)\n\n# Left-skewed distribution (age at death in developed country)\nleft_skewed &lt;- 90 - rgamma(n, shape = 3, scale = 5)\nleft_skewed[left_skewed &lt; 0] &lt;- 0\n\n# Bimodal distribution (e.g., height of mixed male/female population)\nn2  &lt;- 20000\nnf &lt;- n2 %/% 2; nm &lt;- n2 - nf\nbimodal &lt;- c(rnorm(nf, mean = 164, sd = 5),\n             rnorm(nm, mean = 182, sd = 5))\n\n\n# Create data frame\ndistributions_df &lt;- data.frame(\n  Normal = normal_data,\n  `Right Skewed` = right_skewed,\n  `Left Skewed` = left_skewed,\n  Bimodal = bimodal\n) %&gt;%\n  pivot_longer(everything(), names_to = \"Distribution\", values_to = \"Value\")\n\n# Plot distributions\ndistributions_plot &lt;- ggplot(distributions_df, aes(x = Value, fill = Distribution)) +\n  geom_histogram(bins = 30, alpha = 0.7, color = \"white\") +\n  facet_wrap(~Distribution, scales = \"free\", nrow = 2) +\n  scale_fill_manual(values = colors[1:4]) +\n  labs(title = \"Types of Data Distributions\",\n       subtitle = \"Common patterns in demographic data\",\n       x = \"Value\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"none\")\n\nprint(distributions_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 3. NORMAL DISTRIBUTION WITH 68-95-99.7 RULE\n# ==================================================\n\n# Generate normal distribution data\nset.seed(789)\nmean_val &lt;- 100\nsd_val &lt;- 15\nx &lt;- seq(mean_val - 4*sd_val, mean_val + 4*sd_val, length.out = 1000)\ny &lt;- dnorm(x, mean = mean_val, sd = sd_val)\ndf_norm &lt;- data.frame(x = x, y = y)\n\n# Create the plot\nnormal_plot &lt;- ggplot(df_norm, aes(x = x, y = y)) +\n  # Fill areas under the curve\n  geom_area(data = subset(df_norm, x &gt;= mean_val - sd_val & x &lt;= mean_val + sd_val),\n            aes(x = x, y = y), fill = colors[1], alpha = 0.3) +\n  geom_area(data = subset(df_norm, x &gt;= mean_val - 2*sd_val & x &lt;= mean_val + 2*sd_val),\n            aes(x = x, y = y), fill = colors[2], alpha = 0.2) +\n  geom_area(data = subset(df_norm, x &gt;= mean_val - 3*sd_val & x &lt;= mean_val + 3*sd_val),\n            aes(x = x, y = y), fill = colors[3], alpha = 0.1) +\n  # Add the curve\n  geom_line(size = 1.5, color = \"black\") +\n  # Add vertical lines for standard deviations\n  geom_vline(xintercept = mean_val, linetype = \"solid\", size = 1, color = \"black\") +\n  geom_vline(xintercept = c(mean_val - sd_val, mean_val + sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[1]) +\n  geom_vline(xintercept = c(mean_val - 2*sd_val, mean_val + 2*sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[2]) +\n  geom_vline(xintercept = c(mean_val - 3*sd_val, mean_val + 3*sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[3]) +\n  # Add labels\n  annotate(\"text\", x = mean_val, y = max(y) * 0.5, label = \"68%\", \n           size = 5, fontface = \"bold\", color = colors[1]) +\n  annotate(\"text\", x = mean_val, y = max(y) * 0.3, label = \"95%\", \n           size = 5, fontface = \"bold\", color = colors[2]) +\n  annotate(\"text\", x = mean_val, y = max(y) * 0.1, label = \"99.7%\", \n           size = 5, fontface = \"bold\", color = colors[3]) +\n  # Labels\n  scale_x_continuous(breaks = c(mean_val - 3*sd_val, mean_val - 2*sd_val, \n                                mean_val - sd_val, mean_val, \n                                mean_val + sd_val, mean_val + 2*sd_val, \n                                mean_val + 3*sd_val),\n                     labels = c(\"μ-3σ\", \"μ-2σ\", \"μ-σ\", \"μ\", \"μ+σ\", \"μ+2σ\", \"μ+3σ\")) +\n  labs(title = \"Normal Distribution: The 68-95-99.7 Rule\",\n       subtitle = \"Proportion of data within standard deviations from the mean\",\n       x = \"Value\", y = \"Probability Density\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\nprint(normal_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 4. SIMPLE LINEAR REGRESSION\n# ==================================================\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(scales)\n\n# Define color palette (this was missing in original code)\ncolors &lt;- c(\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#592E83\")\n\n# Generate data for regression example (Education vs Income)\nset.seed(2024)\nn_reg &lt;- 200\neducation &lt;- round(rnorm(n_reg, mean = 14, sd = 3))\neducation[education &lt; 8] &lt;- 8\neducation[education &gt; 22] &lt;- 22\n\n# Create income with linear relationship plus noise\nincome &lt;- 15000 + 4000 * education + rnorm(n_reg, mean = 0, sd = 8000)\nincome[income &lt; 10000] &lt;- 10000\n\nreg_data &lt;- data.frame(education = education, income = income)\n\n# Fit linear model\nlm_model &lt;- lm(income ~ education, data = reg_data)\n\n# Create subset of data for residual lines\nsubset_indices &lt;- sample(nrow(reg_data), 20)\nsubset_data &lt;- reg_data[subset_indices, ]\nsubset_data$predicted &lt;- predict(lm_model, newdata = subset_data)\n\n# Create regression plot\nregression_plot &lt;- ggplot(reg_data, aes(x = education, y = income)) +\n  # Add points\n  geom_point(alpha = 0.6, size = 2, color = colors[1]) +\n  \n  # Add regression line with confidence interval\n  geom_smooth(method = \"lm\", se = TRUE, color = colors[2], fill = colors[2], alpha = 0.2) +\n  \n  # Add residual lines for a subset of points to show the concept\n  geom_segment(data = subset_data,\n               aes(x = education, xend = education, \n                   y = income, yend = predicted),\n               color = colors[4], alpha = 0.5, linetype = \"dotted\") +\n  \n  # Add equation to plot (adjusted position based on data range)\n  annotate(\"text\", x = min(reg_data$education) + 1, y = max(reg_data$income) * 0.9, \n           label = paste(\"Income = $\", format(round(coef(lm_model)[1]), big.mark = \",\"), \n                        \" + $\", format(round(coef(lm_model)[2]), big.mark = \",\"), \" × Education\",\n                        \"\\nR² = \", round(summary(lm_model)$r.squared, 3), sep = \"\"),\n           hjust = 0, size = 4, fontface = \"italic\") +\n  \n  # Labels and formatting\n  scale_y_continuous(labels = dollar_format()) +\n  labs(title = \"Simple Linear Regression: Education and Income\",\n       subtitle = \"Each year of education associated with higher income\",\n       x = \"Years of Education\", \n       y = \"Annual Income\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\nprint(regression_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 5. SAMPLING ERROR AND SAMPLE SIZE\n# ==================================================\n\n# Show how standard error decreases with sample size\nset.seed(111)\nsample_sizes &lt;- c(10, 25, 50, 100, 250, 500, 1000, 2500, 5000)\nn_simulations &lt;- 1000\n\n# True population parameters\ntrue_mean &lt;- 50\ntrue_sd &lt;- 10\n\n# Run simulations for each sample size\nse_results &lt;- data.frame()\nfor (n in sample_sizes) {\n  sample_means &lt;- replicate(n_simulations, mean(rnorm(n, true_mean, true_sd)))\n  se_results &lt;- rbind(se_results, \n                      data.frame(n = n, \n                                se_empirical = sd(sample_means),\n                                se_theoretical = true_sd / sqrt(n)))\n}\n\n# Create the plot\nse_plot &lt;- ggplot(se_results, aes(x = n)) +\n  geom_line(aes(y = se_empirical, color = \"Empirical SE\"), size = 1.5) +\n  geom_point(aes(y = se_empirical, color = \"Empirical SE\"), size = 3) +\n  geom_line(aes(y = se_theoretical, color = \"Theoretical SE\"), \n            size = 1.5, linetype = \"dashed\") +\n  scale_x_log10(breaks = sample_sizes) +\n  scale_color_manual(values = c(\"Empirical SE\" = colors[1], \n                               \"Theoretical SE\" = colors[2])) +\n  labs(title = \"Standard Error Decreases with Sample Size\",\n       subtitle = \"The precision of estimates improves with larger samples\",\n       x = \"Sample Size (log scale)\", \n       y = \"Standard Error\",\n       color = \"\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"top\")\n\nprint(se_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 6. CONFIDENCE INTERVALS VISUALIZATION\n# ==================================================\n\n# Simulate multiple samples and their confidence intervals\nset.seed(999)\nn_samples &lt;- 20\nsample_size_ci &lt;- 100\ntrue_mean_ci &lt;- 50\ntrue_sd_ci &lt;- 10\n\n# Generate samples and calculate CIs\nci_data &lt;- data.frame()\nfor (i in 1:n_samples) {\n  sample_i &lt;- rnorm(sample_size_ci, true_mean_ci, true_sd_ci)\n  mean_i &lt;- mean(sample_i)\n  se_i &lt;- sd(sample_i) / sqrt(sample_size_ci)\n  ci_lower &lt;- mean_i - 1.96 * se_i\n  ci_upper &lt;- mean_i + 1.96 * se_i\n  contains_true &lt;- (true_mean_ci &gt;= ci_lower) & (true_mean_ci &lt;= ci_upper)\n  \n  ci_data &lt;- rbind(ci_data,\n                   data.frame(sample = i, mean = mean_i, \n                             lower = ci_lower, upper = ci_upper,\n                             contains = contains_true))\n}\n\n# Create CI plot\nci_plot &lt;- ggplot(ci_data, aes(x = sample, y = mean)) +\n  geom_hline(yintercept = true_mean_ci, color = \"red\", \n             linetype = \"dashed\", size = 1) +\n  geom_errorbar(aes(ymin = lower, ymax = upper, color = contains), \n                width = 0.3, size = 0.8) +\n  geom_point(aes(color = contains), size = 2) +\n  scale_color_manual(values = c(\"TRUE\" = colors[1], \"FALSE\" = colors[4]),\n                    labels = c(\"Misses true value\", \"Contains true value\")) +\n  coord_flip() +\n  labs(title = \"95% Confidence Intervals from 20 Different Samples\",\n       subtitle = paste(\"True population mean = \", true_mean_ci, \n                       \" (red dashed line)\", sep = \"\"),\n       x = \"Sample Number\", \n       y = \"Sample Mean with 95% CI\",\n       color = \"\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"bottom\")\n\nprint(ci_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 7. SAMPLING DISTRIBUTIONS (CENTRAL LIMIT THEOREM)\n# ==================================================\n\n# ---- Setup ----\nlibrary(tidyverse)\nlibrary(ggplot2)\ntheme_set(theme_minimal(base_size = 13))\nset.seed(2025)\n\n# Skewed population (Gamma); change if you want another DGP\nNpop &lt;- 100000\npopulation &lt;- rgamma(Npop, shape = 2, scale = 10)  # skewed right\nmu    &lt;- mean(population)\nsigma &lt;- sd(population)\n\n# ---- CLT: sampling distribution of the mean ----\nsample_sizes &lt;- c(1, 5, 10, 30, 100)\nB &lt;- 2000  # resamples per n\n\nclt_df &lt;- purrr::map_dfr(sample_sizes, \\(n) {\n  tibble(n = n,\n         mean = replicate(B, mean(sample(population, n, replace = TRUE))))\n})\n\n# Normal overlays: N(mu, sigma/sqrt(n))\nclt_range &lt;- clt_df |&gt;\n  group_by(n) |&gt;\n  summarise(min_x = min(mean), max_x = max(mean), .groups = \"drop\")\n\nnormal_df &lt;- clt_range |&gt;\n  rowwise() |&gt;\n  mutate(x = list(seq(min_x, max_x, length.out = 200))) |&gt;\n  unnest(x) |&gt;\n  mutate(density = dnorm(x, mean = mu, sd = sigma / sqrt(n)))\n\nclt_plot &lt;- ggplot(clt_df, aes(mean)) +\n  geom_histogram(aes(y = after_stat(density), fill = factor(n)),\n                 bins = 30, alpha = 0.6, color = \"white\") +\n  geom_line(data = normal_df, aes(x, density), linewidth = 0.8) +\n  geom_vline(xintercept = mu, linetype = \"dashed\") +\n  facet_wrap(~ n, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"CLT: Sampling distribution of the mean → Normal(μ, σ/√n)\",\n    subtitle = sprintf(\"Skewed population: Gamma(shape=2, scale=10).  μ≈%.2f, σ≈%.2f; B=%d resamples each.\", mu, sigma, B),\n    x = \"Sample mean\", y = \"Density\"\n  ) +\n  guides(fill = \"none\")\n\nclt_plot\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 8. TYPES OF SAMPLING ERROR\n# ==================================================\n\n# Create data to show random vs systematic error\nset.seed(321)\nn_measurements &lt;- 100\ntrue_value &lt;- 50\n\n# Random error only\nrandom_error &lt;- rnorm(n_measurements, mean = true_value, sd = 5)\n\n# Systematic error (bias) only\nsystematic_error &lt;- rep(true_value + 10, n_measurements) + rnorm(n_measurements, 0, 0.5)\n\n# Both errors\nboth_errors &lt;- rnorm(n_measurements, mean = true_value + 10, sd = 5)\n\nerror_data &lt;- data.frame(\n  measurement = 1:n_measurements,\n  `Random Error Only` = random_error,\n  `Systematic Error Only` = systematic_error,\n  `Both Errors` = both_errors\n) %&gt;%\n  pivot_longer(-measurement, names_to = \"Error_Type\", values_to = \"Value\")\n\n# Create error visualization\nerror_plot &lt;- ggplot(error_data, aes(x = measurement, y = Value, color = Error_Type)) +\n  geom_hline(yintercept = true_value, linetype = \"dashed\", size = 1, color = \"black\") +\n  geom_point(alpha = 0.6, size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 1.2) +\n  facet_wrap(~Error_Type, nrow = 1) +\n  scale_color_manual(values = colors[1:3]) +\n  labs(title = \"Random Error vs Systematic Error (Bias)\",\n       subtitle = paste(\"True value = \", true_value, \" (black dashed line)\", sep = \"\"),\n       x = \"Measurement Number\", \n       y = \"Measured Value\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"none\")\n\nprint(error_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 9. DEMOGRAPHIC PYRAMID\n# ==================================================\n\n# Create age pyramid data\nset.seed(777)\nage_groups &lt;- c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \n               \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \n               \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80+\")\n\n# Create data for a developing country pattern\nmale_pop &lt;- c(12, 11.5, 11, 10.5, 10, 9.5, 9, 8.5, 8, 7.5, 7, \n             6, 5, 4, 3, 2, 1.5)\nfemale_pop &lt;- c(11.8, 11.3, 10.8, 10.3, 9.8, 9.3, 8.8, 8.3, 7.8, \n               7.3, 6.8, 5.8, 4.8, 3.8, 2.8, 2.2, 2)\n\npyramid_data &lt;- data.frame(\n  Age = factor(rep(age_groups, 2), levels = rev(age_groups)),\n  Population = c(-male_pop, female_pop),  # Negative for males\n  Sex = c(rep(\"Male\", length(male_pop)), rep(\"Female\", length(female_pop)))\n)\n\n# Create population pyramid\npyramid_plot &lt;- ggplot(pyramid_data, aes(x = Age, y = Population, fill = Sex)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  scale_y_continuous(labels = function(x) paste0(abs(x), \"%\")) +\n  scale_fill_manual(values = c(\"Male\" = colors[1], \"Female\" = colors[3])) +\n  coord_flip() +\n  labs(title = \"Population Pyramid\",\n       subtitle = \"Age and sex distribution (typical developing country pattern)\",\n       x = \"Age Group\", \n       y = \"Percentage of Population\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"top\")\n\nprint(pyramid_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 10. REGRESSION RESIDUALS AND DIAGNOSTICS\n# ==================================================\n\n# Use the previous regression model for diagnostics\nreg_diagnostics &lt;- data.frame(\n  fitted = fitted(lm_model),\n  residuals = residuals(lm_model),\n  standardized_residuals = rstandard(lm_model),\n  education = reg_data$education,\n  income = reg_data$income\n)\n\n# Create diagnostic plots\n# 1. Residuals vs Fitted\np_resid_fitted &lt;- ggplot(reg_diagnostics, aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.5, color = colors[1]) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = colors[2], size = 0.8) +\n  labs(title = \"Residuals vs Fitted Values\",\n       subtitle = \"Check for homoscedasticity\",\n       x = \"Fitted Values\", y = \"Residuals\")\n\n# 2. Q-Q plot\np_qq &lt;- ggplot(reg_diagnostics, aes(sample = standardized_residuals)) +\n  stat_qq(color = colors[1]) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Normal Q-Q Plot\",\n       subtitle = \"Check for normality of residuals\",\n       x = \"Theoretical Quantiles\", y = \"Standardized Residuals\")\n\n# 3. Histogram of residuals\np_hist_resid &lt;- ggplot(reg_diagnostics, aes(x = residuals)) +\n  geom_histogram(bins = 30, fill = colors[3], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Distribution of Residuals\",\n       subtitle = \"Should be approximately normal\",\n       x = \"Residuals\", y = \"Frequency\")\n\n# 4. Residuals vs Predictor\np_resid_x &lt;- ggplot(reg_diagnostics, aes(x = education, y = residuals)) +\n  geom_point(alpha = 0.5, color = colors[4]) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = colors[2], size = 0.8) +\n  labs(title = \"Residuals vs Predictor\",\n       subtitle = \"Check for patterns\",\n       x = \"Education (years)\", y = \"Residuals\")\n\n# Combine diagnostic plots\ndiagnostic_plots &lt;- (p_resid_fitted + p_qq) / (p_hist_resid + p_resid_x)\nprint(diagnostic_plots)\n\n\n\n\n\n\n\n# ==================================================\n# 11. SAVE ALL PLOTS (Optional)\n# ==================================================\n\n# Uncomment to save plots as high-resolution images\n# ggsave(\"population_sample.png\", population_sample_plot, width = 10, height = 8, dpi = 300)\n# ggsave(\"distributions.png\", distributions_plot, width = 12, height = 8, dpi = 300)\n# ggsave(\"normal_distribution.png\", normal_plot, width = 10, height = 6, dpi = 300)\n# ggsave(\"regression.png\", regression_plot, width = 10, height = 7, dpi = 300)\n# ggsave(\"standard_error.png\", se_plot, width = 10, height = 6, dpi = 300)\n# ggsave(\"confidence_intervals.png\", ci_plot, width = 10, height = 8, dpi = 300)\n# ggsave(\"central_limit_theorem.png\", clt_plot, width = 14, height = 5, dpi = 300)\n# ggsave(\"error_types.png\", error_plot, width = 14, height = 5, dpi = 300)\n# ggsave(\"population_pyramid.png\", pyramid_plot, width = 8, height = 8, dpi = 300)\n# ggsave(\"regression_diagnostics.png\", diagnostic_plots, width = 12, height = 10, dpi = 300)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#załącznik-b-centralne-twierdzenie-graniczne-ctg",
    "href": "rozdzial1.html#załącznik-b-centralne-twierdzenie-graniczne-ctg",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.22 Załącznik B: Centralne Twierdzenie Graniczne (CTG)",
    "text": "2.22 Załącznik B: Centralne Twierdzenie Graniczne (CTG)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#centralne-twierdzenie-graniczne-ctg-1",
    "href": "rozdzial1.html#centralne-twierdzenie-graniczne-ctg-1",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.23 Centralne Twierdzenie Graniczne (CTG)",
    "text": "2.23 Centralne Twierdzenie Graniczne (CTG)\nCentralne Twierdzenie Graniczne stwierdza, że rozkład średnich próbkowych zbliża się do rozkładu normalnego wraz ze wzrostem wielkości próby, niezależnie od kształtu pierwotnego rozkładu populacji.\n\nImplikacje\n\nPróg Wielkości Próby: Wielkość próby n ≥ 30 jest zazwyczaj wystarczająca, aby zastosować CTG\nBłąd Standardowy: Odchylenie standardowe średnich próbkowych wynosi σ/√n, gdzie σ to odchylenie standardowe populacji\nFundament Wnioskowania Statystycznego: Możemy dokonywać wnioskowań o parametrach populacji używając właściwości rozkładu normalnego",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wizualna-demonstracja-progresja-krok-po-kroku",
    "href": "rozdzial1.html#wizualna-demonstracja-progresja-krok-po-kroku",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.24 Wizualna Demonstracja: Progresja Krok po Kroku",
    "text": "2.24 Wizualna Demonstracja: Progresja Krok po Kroku\nNajlepszym sposobem na zrozumienie CTG jest obserwowanie ewolucji rozkładu wraz ze wzrostem liczby kostek. Zaczynając od 1 kostki (rozkład jednostajny), zobaczymy, jak dodawanie kolejnych kostek stopniowo przekształca rozkład w idealną krzywą dzwonową!\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n\nProgresywna Transformacja\n\n# Wielkości próby do demonstracji\nwielkosci_prob &lt;- c(1, 2, 5, 10, 30, 50)\nliczba_symulacji &lt;- 100000\n\n# Symulacja dla każdej wielkości próby\nwszystkie_dane &lt;- data.frame()\n\nfor (n in wielkosci_prob) {\n  srednie &lt;- replicate(liczba_symulacji, {\n    kostki &lt;- sample(1:6, n, replace = TRUE)\n    mean(kostki)\n  })\n  \n  temp_df &lt;- data.frame(\n    srednia = srednie,\n    n = n,\n    etykieta = paste(n, ifelse(n == 1, \"kostka\", \n                               ifelse(n &lt; 5, \"kostki\", \"kostek\")))\n  )\n  wszystkie_dane &lt;- rbind(wszystkie_dane, temp_df)\n}\n\n# Utworzenie uporządkowanego czynnika\nwszystkie_dane$etykieta &lt;- factor(wszystkie_dane$etykieta, \n                                  levels = paste(wielkosci_prob, \n                                                ifelse(wielkosci_prob == 1, \"kostka\",\n                                                      ifelse(wielkosci_prob &lt; 5, \"kostki\", \"kostek\"))))\n\n# Wykres progresji\nggplot(wszystkie_dane, aes(x = srednia)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 bins = 50, fill = \"#3b82f6\", color = \"white\", alpha = 0.7) +\n  facet_wrap(~etykieta, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"Centralne Twierdzenie Graniczne: Progresja Krok po Kroku\",\n    subtitle = sprintf(\"Każdy panel pokazuje %s symulacji rzutu kostkami i obliczenia średniej\", \n                      format(liczba_symulacji, big.mark = \" \")),\n    x = \"Wartość Średnia\",\n    y = \"Gęstość\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    strip.text = element_text(face = \"bold\", size = 12),\n    strip.background = element_rect(fill = \"#f0f0f0\", color = NA)\n  )\n\n\n\n\n\n\n\n\n\nAnaliza Poszczególnych Etapów:\n\n1 kostka: Rozkład jednostajny (równomierny) - wszystkie wartości 1-6 jednakowo prawdopodobne\n2 kostki: Rozkład z tendencją trójkątną - środkowe wartości występują częściej\n5 kostek: Wyłaniający się kształt dzwonowy - obserwowalne skupienie wokół wartości 3,5\n10 kostek: Wyraźnie normalny - formująca się wąska krzywa Gaussa\n30 kostek: Rozkład normalny - praktyczna demonstracja CTG\n50 kostek: Rozkład bliski idealnemu normalnemu - bardzo silna koncentracja\n\nZauważ, jak rozkład charakteryzuje się coraz mniejszą zmiennością i bardziej wyraźnym kształtem dzwonowym wraz ze wzrostem n.\n\n\n\nPorównanie Obok Siebie\nZobaczmy czystsze porównanie kluczowych etapów:\n\nkluczowe_wielkosci &lt;- wszystkie_dane %&gt;%\n  filter(n %in% c(1, 2, 5, 10, 30))\n\nggplot(kluczowe_wielkosci, aes(x = srednia)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 bins = 40, fill = \"#3b82f6\", color = \"white\", alpha = 0.7) +\n  facet_wrap(~etykieta, scales = \"free_x\", nrow = 1) +\n  labs(\n    title = \"Ewolucja CTG: Od Jednostajnego do Normalnego\",\n    x = \"Wartość Średnia\",\n    y = \"Gęstość\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    strip.text = element_text(face = \"bold\", size = 11),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\nNałożone Rozkłady\nInny sposób zobaczenia transformacji - wszystkie rozkłady na jednym wykresie:\n\ndane_porownawcze &lt;- wszystkie_dane %&gt;%\n  filter(n %in% c(1, 5, 10, 30))\n\nggplot(dane_porownawcze, aes(x = srednia, fill = etykieta, color = etykieta)) +\n  geom_density(alpha = 0.3, linewidth = 1.2) +\n  scale_fill_manual(values = c(\"#991b1b\", \"#ea580c\", \"#ca8a04\", \"#16a34a\")) +\n  scale_color_manual(values = c(\"#991b1b\", \"#ea580c\", \"#ca8a04\", \"#16a34a\")) +\n  labs(\n    title = \"Progresja CTG: Nałożone Rozkłady\",\n    subtitle = \"Analiza związku między wielkością próby a zmiennością rozkładu próbkowego\",\n    x = \"Wartość Średnia\",\n    y = \"Gęstość\",\n    fill = \"Wielkość Próby\",\n    color = \"Wielkość Próby\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nKluczowa Obserwacja: Wraz ze wzrostem wielkości próby rozkład charakteryzuje się następującymi właściwościami:\n\nZwiększona symetria (kształt dzwonowy)\nWiększa koncentracja wokół wartości oczekiwanej (3,5)\nLepsza zgodność z rozkładem normalnym\n\n\n\nZbieżność Błędu Standardowego\nRozrzut (odchylenie standardowe) maleje zgodnie ze wzorem SE = σ/√n:\n\ndane_wariancji &lt;- wszystkie_dane %&gt;%\n  group_by(n, etykieta) %&gt;%\n  summarise(\n    obserwowane_sd = sd(srednia),\n    teoretyczne_se = sqrt(35/12) / sqrt(n),\n    .groups = \"drop\"\n  )\n\nggplot(dane_wariancji, aes(x = n)) +\n  geom_line(aes(y = obserwowane_sd, color = \"Obserwowane SD\"), \n            linewidth = 1.5) +\n  geom_point(aes(y = obserwowane_sd, color = \"Obserwowane SD\"), \n             size = 4) +\n  geom_line(aes(y = teoretyczne_se, color = \"Teoretyczne SE\"), \n            linewidth = 1.5, linetype = \"dashed\") +\n  geom_point(aes(y = teoretyczne_se, color = \"Teoretyczne SE\"), \n             size = 4) +\n  scale_color_manual(values = c(\"Obserwowane SD\" = \"#3b82f6\", \n                                \"Teoretyczne SE\" = \"#ef4444\")) +\n  scale_x_continuous(breaks = wielkosci_prob) +\n  labs(\n    title = \"Błąd Standardowy Maleje wraz ze Wzrostem Wielkości Próby\",\n    subtitle = \"Zgodnie ze związkiem SE = σ/√n\",\n    x = \"Wielkość Próby (n)\",\n    y = \"Odchylenie Standardowe / Błąd Standardowy\",\n    color = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    legend.position = \"top\",\n    legend.text = element_text(size = 11)\n  )\n\n\n\n\n\n\n\n\n\n\nPodsumowanie Numeryczne\n\nstatystyki_podsumowanie &lt;- wszystkie_dane %&gt;%\n  group_by(etykieta) %&gt;%\n  summarise(\n    n = first(n),\n    Obserwowana_Srednia = round(mean(srednia), 3),\n    Obserwowane_SD = round(sd(srednia), 3),\n    Teoretyczna_Srednia = 3.5,\n    Teoretyczne_SE = round(sqrt(35/12) / sqrt(first(n)), 3),\n    Zakres = paste0(\"[\", round(min(srednia), 2), \", \", round(max(srednia), 2), \"]\")\n  ) %&gt;%\n  select(-etykieta)\n\nknitr::kable(statystyki_podsumowanie, \n             caption = \"Wartości Obserwowane vs Teoretyczne dla Różnych Wielkości Próby\")\n\n\nWartości Obserwowane vs Teoretyczne dla Różnych Wielkości Próby\n\n\n\n\n\n\n\n\n\n\nn\nObserwowana_Srednia\nObserwowane_SD\nTeoretyczna_Srednia\nTeoretyczne_SE\nZakres\n\n\n\n\n1\n3.495\n1.707\n3.5\n1.708\n[1, 6]\n\n\n2\n3.503\n1.205\n3.5\n1.208\n[1, 6]\n\n\n5\n3.500\n0.765\n3.5\n0.764\n[1, 6]\n\n\n10\n3.499\n0.540\n3.5\n0.540\n[1.3, 5.6]\n\n\n30\n3.501\n0.313\n3.5\n0.312\n[2.17, 4.77]\n\n\n50\n3.501\n0.241\n3.5\n0.242\n[2.36, 4.54]\n\n\n\n\n\nObserwacje:\n\nWartość oczekiwana pozostaje stała na poziomie 3,5 (niezależnie od wielkości próby)\nBłąd standardowy wykazuje systematyczny spadek wraz ze wzrostem n (zgodnie ze związkiem SE ∝ 1/√n)\nRozstęp wartości ulega znacznemu zawężeniu wraz ze wzrostem wielkości próby",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#podstawy-matematyczne-1",
    "href": "rozdzial1.html#podstawy-matematyczne-1",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.25 Podstawy Matematyczne",
    "text": "2.25 Podstawy Matematyczne\nDla populacji ze średnią μ i skończoną wariancją σ²:\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\text{ gdy } n \\to \\infty\nBłąd standardowy średniej:\nSE_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\nDla uczciwej kostki: μ = 3,5, σ² = 35/12 ≈ 2,917",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#najważniejsze-wnioski-1",
    "href": "rozdzial1.html#najważniejsze-wnioski-1",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.26 Najważniejsze Wnioski",
    "text": "2.26 Najważniejsze Wnioski\n\nPunkt Wyjścia: Pojedyncza kostka charakteryzuje się rozkładem jednostajnym (równomiernym)\nStopniowa Transformacja: Wraz ze wzrostem liczby obserwacji kształt rozkładu stopniowo ewoluuje\nKonwergencja do Normalności: Przy n=30 obserwujemy wyraźny rozkład normalny\nRedukcja Zmienności: Rozkład charakteryzuje się coraz większą koncentracją wokół wartości oczekiwanej\nUniwersalność: Twierdzenie ma zastosowanie do każdego rozkładu populacji z skończoną wariancją",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#dlaczego-to-ma-znaczenie-1",
    "href": "rozdzial1.html#dlaczego-to-ma-znaczenie-1",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.27 Dlaczego To Ma Znaczenie",
    "text": "2.27 Dlaczego To Ma Znaczenie\nTa transformacja pozwala nam:\n\nUżywać tablic i właściwości rozkładu normalnego do wnioskowania\nObliczać przedziały ufności ze znanym prawdopodobieństwem\nPrzeprowadzać testy hipotez (testy t, testy z)\nDokonywać przewidywań dotyczących średnich próbkowych\n\nKluczowa właściwość CTG: Mimo że rozkład pojedynczych rzutów kostką jest jednostajny, rozkład średnich z wielu kostek zbliża się do rozkładu normalnego w sposób przewidywalny i zgodny z teorią matematyczną.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "3.1 Foundations in Number Sets\nIn social science research, understanding the nature of our data is crucial for selecting appropriate analysis methods and drawing valid conclusions.\nBefore diving into data types, it’s essential to understand the basic number sets that form the foundation of our understanding of data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#foundations-in-number-sets",
    "href": "chapter2.html#foundations-in-number-sets",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "Basic Number Sets\n\nNatural Numbers (ℕ): The counting numbers {0, 1, 2, 3, …}\nIntegers (ℤ): Includes natural numbers, their negatives, and zero {…, -2, -1, 0, 1, 2, …}\nRational Numbers (ℚ): Numbers that can be expressed as a fraction of two integers\nReal Numbers (ℝ): All numbers on the number line, including rationals and irrationals\n\n\n\nProperties of Sets\n\nCountable Sets: Sets whose elements can be put in a one-to-one correspondence with the natural numbers. For example, the set of integers is countable.\nUncountable Sets: Sets that are not countable. The set of real numbers is uncountable.\nDiscrete Sets: Sets where each element is separated from other elements by a finite gap. The integers form a discrete set.\nDense Sets: Sets where between any two elements, there is always another element of the set. The rational numbers and real numbers are dense sets.\n\n\n\n\n\n\n\nNote\n\n\n\nUnderstanding these set properties is crucial for grasping the nature of different data types in social sciences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#discrete-vs.-continuous-data",
    "href": "chapter2.html#discrete-vs.-continuous-data",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.2 Discrete vs. Continuous Data",
    "text": "3.2 Discrete vs. Continuous Data\nIn data science and statistics, we categorize variables as either discrete or continuous. This distinction shapes how we analyze data and which statistical methods we apply. However, the boundary between these categories is not always clear-cut, and some variables exhibit characteristics of both types. This section explores discrete and continuous data, their differences, and the interesting cases of variables that challenge our intuitive understanding.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDiscrete Data\nDiscrete data can only take on specific, countable values. These values are often (but not always) integers.\n\nCharacteristics of Discrete Data:\n\nCountable set of possible values\nOften represented by integers\nCan be finite or infinite\nNo values exist between two adjacent data points\n\n\n\nExamples:\n\nNumber of students in a class\nNumber of cars sold by a dealership\nNumber of coins in a piggy bank\nDice rolls (1, 2, 3, 4, 5, or 6)\n\n\n\n\nContinuous Data\nContinuous data can take on any value within a given range, including fractional and decimal values. Importantly, continuity is characterized by density: between any two values, there are infinitely many other possible values.\n\nCharacteristics of Continuous Data:\n\nValues from dense sets (rational numbers or real numbers)\nCan be measured to any level of precision (theoretically)\nThere are infinitely many values between any two data points\nTypically represented on a continuous scale\n\n\n\nExamples:\n\nHeight\nWeight\nTemperature\nTime duration\n\n\n\n\n\n\n\nMathematical Note: Density and Continuity\n\n\n\nContinuous data comes from dense sets, where between any two distinct values, there exists another value from the set. The most common examples are:\n\nReal numbers: Uncountable and dense\nRational numbers: Countable but dense\n\nThis density property is what gives continuous data its characteristic “smoothness” and allows us to apply calculus-based statistical methods.\n\n\n\n\n\nThe Discrete-Continuous Spectrum\nIn practice, some variables that are mathematically discrete are often treated as if they are continuous. This dual nature provides flexibility in how these variables can be analyzed and interpreted.\n\nReasons for Treating Discrete Data as Continuous:\n\nFine Granularity (Small Discrete Increments)\n\nWhen a discrete variable has very small increments between possible values, it can approximate continuity.\nExample: Income measured in individual cents. While technically discrete, the tiny increments and vast number of possible values make it behave similarly to a continuous variable.\n\nAnalytical Convenience\n\nContinuous methods often yield reasonable and useful results even for dense discrete variables.\nAssuming continuity allows the use of calculus-based methods and existing statistical tools.\n\nApproximation of Underlying Phenomena\n\nA discrete measurement might represent an underlying continuous process.\nExample: While we measure time in discrete units (seconds, minutes, hours), time itself flows continuously.\n\n\n\n\nExamples of Variables with Dual Discrete-Continuous Nature:\n\nAge\n\nDiscrete: Typically reported in whole years\nContinuous: Can be considered as a continuous variable in many analyses, especially when dealing with large populations or when precision matters\n\nPrice and Income\n\nDiscrete: Prices and incomes are actually measured in discrete units (e.g., cents or smallest currency unit)\nContinuous: In economic models and many analyses, treated as continuous variables due to their dense nature and analytical convenience\n\nTest Scores\n\nDiscrete: Often given as whole numbers or fixed increments\nContinuous: In statistical analyses, may be treated as continuous, especially when the range of possible scores is large\n\n\n\n\n\nSpecial Case: Percentages and Proportions\nPercentages and proportions present an interesting case in the discrete-continuous spectrum. In most practical applications, they are treated as continuous due to their dense nature, even though they have some discrete characteristics.\n\n\n\n\n\n\nPercentages: Bridging Discrete and Continuous\n\n\n\nVariables measured in percentages, such as unemployment rates or voter turnout, challenge our intuitive understanding of discreteness and continuity:\n\nRational Nature: Percentages are essentially fractions (m/100), making them rational numbers.\nDense but Countable: The set of rational numbers is dense (between any two rationals, there’s another rational) but also countable—illustrating that density and countability are independent properties.\nPractical Continuity: In most applications, percentages are treated as continuous due to their dense nature and analytical convenience.\nFinite Precision: In reality, percentages are often reported to a limited number of decimal places, creating a finite (discrete) set of possible values.\n\nThis duality allows for flexible analytical approaches, depending on the specific research context and required precision.\n\n\n\n\nImplications for Data Analysis\nUnderstanding the nuanced nature of variables as discrete, continuous, or somewhere in between has important implications for data analysis:\n\nFlexibility in Modeling: Allows for the use of a wider range of statistical techniques.\nSimplified Calculations: Treating dense discrete data as continuous can simplify calculations and make certain analyses more tractable.\nImproved Interpretability: In some cases, treating discrete data as continuous can lead to more intuitive or useful interpretations of results.\nPotential for Error: It’s important to be aware of when approximations are appropriate and when they might lead to misleading results.\nTheoretical vs. Practical Considerations: While the mathematical nature of the data is important, practical considerations in measurement and analysis often guide how we treat variables.\n\n\n\nConclusion\nThe distinction between discrete and continuous data is not always rigid in practice. Many variables, including those involving money, percentages, or dense measurements, can be viewed through both discrete and continuous lenses. When in doubt, consider both your measurement precision and your analytical goals when deciding how to treat a variable. The choice should be guided by the nature of the data, the goals of the analysis, and the potential implications of the choice. This flexibility, when used thoughtfully, provides powerful tools for researchers to gain insights from their data.\n\n\n\n\n\n\nDiscrete vs. Continuous Numerical Data: A Language-Based Analogies\n\n\n\n\nThe Language Connection\nThink about how you naturally ask questions about quantities:\n\n“How many cookies are in the jar?” (counting)\n“How much water is in the glass?” (measuring)\n\nThis natural language distinction reflects the two fundamental types of numerical data:\n\n\nDiscrete Data = “How Many?” Questions\n\nLike counting whole objects (countable nouns)\nTakes specific values with gaps between them\nExamples:\n\nNumber of pets: 0, 1, 2, 3… (can’t have 2.5 pets)\nDice rolls: 1, 2, 3, 4, 5, 6\nStudents in a class: 20, 21, 22…\n\n\n🤔 Self-Check: Can you find a value between 2 and 3 students? Why not?\n\n\nContinuous Data = “How Much?” Questions\n\nLike measuring quantities (uncountable nouns)\nCan take any value within a range\nExamples:\n\nHeight: 1.7231… meters\nTemperature: 36.8325… °C\nTime: 3.5792… hours\n\n\n🤔 Self-Check: Write down three different values between 1.72 and 1.73 meters\n\n\nQuick Recognition Guide\n\nIf you naturally ask “How many?” → Discrete\nIf you naturally ask “How much?” → Continuous\nIf you can measure it more precisely → Continuous\nIf you can only use whole numbers → Discrete\n\n✍️ Practice: Classify these quantities as discrete or continuous\n\nYour age in years: _____\nYour height: _____\nNumber of songs in a playlist: _____\nVolume of water: _____",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#introduction-to-stevens-data-typology",
    "href": "chapter2.html#introduction-to-stevens-data-typology",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.3 Introduction to Stevens’ Data Typology",
    "text": "3.3 Introduction to Stevens’ Data Typology\nStanley S. Stevens, an American psychologist, introduced a classification system for scales of measurement in his 1946 paper “On the Theory of Scales of Measurement.” This system, known as Stevens’ data typology or levels of measurement, has become fundamental in understanding how different types of data should be analyzed and interpreted.\nStevens proposed four levels of measurement:\n\nNominal\nOrdinal\nInterval\nRatio\n\nEach level has specific properties and allows for different types of statistical operations and analyses.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\nNominal Scale\n\nDefinition\nThe nominal scale is the most basic level of measurement. It uses labels or categories to classify data without any quantitative value or order.\n\n\nProperties\n\nCategories are mutually exclusive\nNo inherent order among categories\nNo meaningful arithmetic operations can be performed\n\n\n\nExamples\n\nNationality (Polish, English, …)\nBlood types (A, B, AB, O)\nEye color (Blue, Brown, Green, Hazel)\nBinary variables (“Success” versus “Failure”)\n\n\n\n\nOrdinal Scale\n\nDefinition\nThe ordinal scale categorizes data into ordered categories, but the intervals between categories are not necessarily equal or meaningful.\n\n\nProperties\n\nCategories have a defined order\nDifferences between categories are not quantifiable\nArithmetic operations on the numbers are not meaningful\n\n\n\nExamples\n\nEducation levels (High School, Bachelor’s, Master’s, PhD)\nLikert scales (Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree)\nSocioeconomic status (Low, Medium, High)\n\n\n\n\nInterval Scale\n\nDefinition\nThe interval scale has ordered categories with equal intervals between adjacent categories. However, it lacks a true zero point.\n\n\nProperties\n\nEqual intervals between adjacent categories\nNo true zero point (zero is arbitrary)\nRatios between values are not meaningful\n\n\n\nExamples\n\nTemperature in Celsius or Fahrenheit\nCalendar years\npH scale (the difference between pH 4 and 5 represents the same change in hydrogen ion concentration as between pH 6 and 7)\nElevation above sea level\n\n\n\n\nRatio Scale\n\nDefinition\nThe ratio scale is the highest level of measurement. It has all the properties of the interval scale plus a true zero point, making ratios between values meaningful.\n\n\nProperties\n\nAll properties of interval scales\nTrue zero point\nRatios between values are meaningful\n\n\n\nExamples\n\nHeight\nWeight\nAge\nIncome\n\n\n\n\n\n\n\nWhy Some Statistics Work (and Others Don’t) for Interval Scales\n\n\n\n\nKey Idea\nAn interval scale is one where the distances between values are meaningful, but the zero point is arbitrary. For interval scales (e.g., temperature):\n\nAllowed: Addition/subtraction of values and multiplication/division by constants.\nNot allowed: Multiplication/division of values from the scale by each other, as this leads to results without physical interpretation.\n\n\n\nProperties of Interval Scales\n\nEqual intervals represent the same differences:\n\nThe difference between 20°C and 25°C (5°C) represents the same change as between 30°C and 35°C.\nProportions of differences are preserved: 10°C is twice the change of 5°C.\n\nThe zero point is arbitrary:\n\n0°C is the freezing point of water, not the absence of temperature.\nThe same physical state has different values in different scales: 0°C = 32°F.\n\nLinear transformation:\n\nGeneral formula: y = ax + b, where a \\neq 0.\nFor temperature: F = C \\times \\frac{9}{5} + 32.\n\n\n\n\nTheoretical Conclusions\n\nAllowed operations:\n\nAddition/subtraction (preserves differences).\nMultiplication/division by constants (scaling).\nArithmetic means.\nComparing temperature differences.\n\nNot allowed operations:\n\nMultiplying temperatures by each other.\nDividing temperatures by each other.\nGeometric means.\nCoefficient of variation.\n\nPractical implications:\n\nVariance and standard deviation require careful interpretation.\nBetter to use measures based on differences (e.g., MAD - mean absolute deviation).\nWhen comparing variability, it is advisable to standardize the data.\n\n\n\n\nPractical Rule\nIf your calculations involve multiplying values from an interval scale by each other, be particularly cautious in interpreting the results!\n\n\n\n\n\n\nImportance in Research and Analysis\nUnderstanding Stevens’ data typology is crucial for several reasons:\n\nChoosing appropriate statistical tests: The level of measurement determines which statistical analyses are appropriate for a given dataset.\nInterpreting results: The meaning of statistical results depends on the level of measurement of the variables involved.\nDesigning measurement instruments: When creating surveys or other measurement tools, researchers must consider the level of measurement they want to achieve.\nData transformation: Sometimes, data can be transformed from one level to another, but this must be done carefully to avoid misinterpretation.\n\n\n\nControversies and Limitations\nWhile Stevens’ typology is widely used, it has faced some criticisms:\n\nRigidity: Some argue that the typology is too rigid and that many real-world measurements fall between these categories.\nTreatment of ordinal data: There’s ongoing debate about when it’s appropriate to treat ordinal data as interval for certain analyses.\nPsychological scaling: Some psychological constructs (like intelligence) are difficult to categorize definitively within this system.\n\n\n\nConclusion\nStevens’ data typology provides a fundamental framework for understanding different types of data and their properties. By recognizing the level of measurement of their variables, researchers can make informed decisions about data collection, analysis, and interpretation. However, it’s important to remember that while this typology is a useful guide, real-world data often requires nuanced consideration and may not always fit neatly into these categories.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not “twice as acidic” as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.\n\n\n\n\nReality: IQ is Fundamentally an Ordinal Scale\nHow IQ scores are created – step by step:\n\nCollecting raw scores: People take a test and receive a number of correct answers (e.g., 45 out of 60 questions)\nOrdering: All raw scores are arranged from worst to best\nAssigning ranks: Each score is assigned a position in the ranking\nTransformation to IQ scale: Ranks are mathematically transformed so that the mean equals 100 and standard deviation equals 15\n\nKey problem: This process forces a normal distribution onto data that may not have been normal in its original form. This means that equal differences in IQ points (e.g., difference between IQ 100 and 115 vs. difference between IQ 115 and 130) may not correspond to equal differences in actual cognitive abilities.\n\n\n\n\n\n\nKey Point\n\n\n\nIQ 130 does not mean “twice the intelligence” of IQ 65. IQ points only show a person’s position relative to other people in the sample, not the actual amount of intelligence. This is similar to places in a competition – the winner might win by a hair or by miles, but will still be in first place.\n\n\nIn research practice: why do we sometimes treat IQ as an interval scale?\nThis is a methodological compromise that allows for the use of more precise statistical tools:\n✅ Treating IQ as an interval scale is acceptable when:\n\nUsing standard statistical tests (correlations, regressions, t-tests)\nComparing groups within the same test and population - Being aware of the limitations of this approach\nOur conclusions don’t depend on differences being exactly equal\n\n⚠️ Remember the limitations:\n\nThis is a simplification of reality\nThe assumption works better for scores near the mean (IQ 85-115) than at the extremes\nResults must be interpreted carefully\n\n❌ Never:\n\nSay that IQ differences mean equal differences in intelligence\nUse statements like “twice as intelligent”\n\n\n\nPractical Guidelines for Researchers\n\nBe transparent:\n\nClearly state: “We treat IQ as an interval scale for statistical purposes, remembering that it is fundamentally an ordinal scale”\n\nConsider alternatives:\n\nUse non-parametric tests when sample size allows\nCompare results from different analytical methods\n\nInterpret cautiously:\n\nFocus on statements about order (“group A achieved higher scores than group B”)\nAvoid precise statements about the magnitude of differences\nRemember: a 15-point IQ difference means “one standard deviation in the sample,” not “a specific amount of additional intelligence”\n\n\n\n\n\n\n\n\nTip\n\n\n\nIQ is an ordinal scale that has been transformed to look like an interval scale. It can be used in statistical analyses requiring an interval scale, but one must always remember its true nature when interpreting results. The key is understanding that IQ points tell us about position in a group, not about the absolute amount of intelligence.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#common-ordinal-scales-in-behavioural-research",
    "href": "chapter2.html#common-ordinal-scales-in-behavioural-research",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.4 Common Ordinal Scales in Behavioural Research",
    "text": "3.4 Common Ordinal Scales in Behavioural Research\n\nLikert Scales\nLikert scales are widely used in psychology and social sciences to measure attitudes, opinions, and perceptions. Named after psychologist Rensis Likert, these scales typically consist of a series of statements or questions that respondents rate on a scale, often from “Strongly Disagree” to “Strongly Agree.”\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nWhy Likert Scales are Ordinal Variables\nLikert scales are considered ordinal variables for several reasons:\n\nOrder without equal intervals: While the responses have a clear order (e.g., “Strongly Disagree” &lt; “Disagree” &lt; “Neutral” &lt; “Agree” &lt; “Strongly Agree”), the intervals between these categories are not necessarily equal.\nSubjective interpretation: The difference between “Strongly Disagree” and “Disagree” may not be the same as the difference between “Agree” and “Strongly Agree” for all respondents.\nLack of true zero point: Likert scales typically don’t have a true zero point, which is a characteristic of interval or ratio scales.\n\n\n\n\nIQ and Other Psychological Variables as Ordinal Measures\nMany psychological measures, including IQ, are often treated as interval scales but are, in fact, ordinal. Here’s why:\n\nIQ Scores:\n\nWhile IQ scores are presented as numbers, the difference between an IQ of 100 and 110 may not represent the same cognitive difference as between 130 and 140.\nThe scale is normalized and adjusted over time, making it difficult to claim true interval properties.\n\nOther Psychological Measures:\n\nDepression scales (e.g., Beck Depression Inventory)\nAnxiety measures (e.g., State-Trait Anxiety Inventory)\nPersonality assessments (e.g., Big Five Inventory)\n\n\nThese measures often use summed Likert-type items or other scoring methods that don’t guarantee equal intervals between scores.\n\n\nImplications for Analysis\nRecognizing these measures as ordinal has important implications for data analysis:\n\nAppropriate statistical tests: Use non-parametric tests (e.g., Mann-Whitney U, Kruskal-Wallis) instead of parametric ones.\nCorrelation analysis: Use Spearman’s rank correlation instead of Pearson’s correlation.\nCentral tendency: Report median and mode rather than mean.\nData visualization: Use methods appropriate for ordinal data, such as bar plots or stacked bar charts.\n\n\n\nConclusion\nWhile Likert scales and many behavioural measures are often treated as interval data for practical reasons, it’s crucial to remember their ordinal nature.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n\n\n\n\nExercise: Identifying Measurement Scales\n\n\n\nFor each of the following variables, determine the most appropriate scale of measurement (Nominal, Ordinal, Interval, or Ratio). Also evaluate whether the variable is discrete or continuous.\n\nGender: nominal level of measurement, and discrete;\nCustomer satisfaction: Poor, Fair, Good, Excellent\nHeight (questionnaire): “I am: very short, short, average, tall, very tall”\nHeight (inches)\nReaction time (milliseconds)\nPostal codes: e.g., 61548, 61761, 62461, 47424, 65233\nAge (years)\nNationality\nStreet addresses\nMilitary ranks\nLeft-Right political scale placement\nFamily size: 1 child, 2 children, 3 children, …\nIQ score\nShirt size (S, M, L, …)\nMovie ratings (1 star, 2 stars, 3 stars)\nTemperature (Celsius)\nTemperature (Kelvin)\nBlood types: A, B, AB, O\nIncome categories: low, medium, high\nVoter turnout\nPolitical party affiliation\nElectoral district magnitude\n\nRemember to justify your choices for each variable.\nFor instance: In Stevens’ typology of measurement scales, street addresses are nominal data. This is because:\nThey serve purely as labels/identifiers. They have no inherent ordering (123 Main St isn’t “more than” 23 Oak St). You can’t perform meaningful mathematical operations on them.The only valid operation is testing for equality/inequality (is this the same address or different?)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#appendix-a",
    "href": "chapter2.html#appendix-a",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.5 Appendix A",
    "text": "3.5 Appendix A\n\n\n\n\n\n\nWhy Addition/Subtraction Works for Interval Data, but Multiplication/Division Requires Ratio Data\n\n\n\nInterval scales have equal intervals between values but an arbitrary zero point (e.g., Celsius temperature, calendar dates). Ratio scales have both equal intervals and an absolute zero (e.g., Kelvin temperature, height, weight).\n\nAddition and Subtraction: Valid for Interval Scales\nDifferences maintain consistent proportional relationships under linear transformations. When converting between scales using y = a + bx where b &gt; 0, the additive constant a cancels out:\n(y_2 - y_1) = (a + bx_2) - (a + bx_1) = b(x_2 - x_1)\nExample: Using Celsius to Fahrenheit conversion where F = 1.8C + 32:\n\nAny 10°C difference always converts to 18°F: 18 = 1.8 \\times 10\nTry (20°C - 10°C) \\rightarrow (68°F - 50°F) = 18°F\nTry (100°C - 90°C) \\rightarrow (212°F - 194°F) = 18°F\n\nThe relationship is consistent: a 10-degree Celsius difference always corresponds to an 18-degree Fahrenheit difference, regardless of where on the scale we measure.\n\n\nMultiplication and Division: Require Ratio Scales\nRatios are inconsistent when the zero point is arbitrary. The additive constant a does NOT cancel out in ratios:\n\\frac{y_2}{y_1} = \\frac{a + bx_2}{a + bx_1} \\neq b \\cdot \\frac{x_2}{x_1}\nUnless a = 0 (absolute zero), ratios change unpredictably depending on which values you compare.\nExample: Temperature ratios give inconsistent results:\n\nIs 20°C “twice as hot” as 10°C?\n\nCelsius: 20/10 = 2.0\nFahrenheit: 68/50 = 1.36\nKelvin: 293.15/283.15 = 1.035\n\nWhat about 100°C vs. 90°C?\n\nCelsius: 100/90 = 1.11\nFahrenheit: 212/194 = 1.09\n\n\nThe ratios vary depending on both the scale AND which temperatures we pick. Only with an absolute zero do ratios have consistent physical meaning.\n\n\nImplications for Statistical Measures\nArithmetic mean is valid for interval scales because it uses addition:\n\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\nWhen we transform to scale y, the mean transforms consistently: \\bar{y} = a + b\\bar{x}\nGeometric mean requires ratio scales because it uses multiplication:\nGM = \\sqrt[n]{x_1 \\times x_2 \\times \\cdots \\times x_n}\nThe geometric mean of temperatures in Celsius gives a different result than the geometric mean of the same temperatures in Fahrenheit (after converting back). This makes the geometric mean meaningless for interval data.\nExample: For temperatures 10°C and 20°C:\n\nGeometric mean in Celsius: \\sqrt{10 \\times 20} = 14.14°C \\rightarrow 57.45°F\nGeometric mean in Fahrenheit: \\sqrt{50 \\times 68} = 58.31°F \\rightarrow 14.62°C\n\nThese don’t match! The geometric mean depends on the arbitrary zero point.\n\n\nVariance and Standard Deviation: Valid for Interval Scales\nVariance and standard deviation are acceptable for interval data because they operate on deviations from the mean, which are differences. Critically, variance is translation-invariant: adding a constant to all values doesn’t change the variance because the deviations remain the same.\nUnder linear transformation y = a + bx, variance transforms predictably:\n\\text{Var}(y) = b^2 \\text{Var}(x)\nThe constant a cancels out when computing deviations, just as it does for simple differences.\nExample proof: For temperatures 10°C and 20°C:\nIn Celsius:\n\nMean: \\bar{x} = (10 + 20)/2 = 15°C\nDeviations: (10 - 15) = -5, (20 - 15) = 5\nVariance: \\text{Var}(X) = [(-5)^2 + (5)^2]/2 = 50/2 = 25°C^2\nStandard deviation: \\text{SD}(X) = 5°C\n\nIn Fahrenheit:\n\nConvert: 10°C → 50°F, 20°C → 68°F\nMean: \\bar{y} = (50 + 68)/2 = 59°F\nDeviations: (50 - 59) = -9, (68 - 59) = 9\nVariance: \\text{Var}(Y) = [(-9)^2 + (9)^2]/2 = 162/2 = 81°F^2\nStandard deviation: \\text{SD}(Y) = 9°F\n\nCheck the transformation:\n\nConversion slope: b = 1.8 (from F = 1.8C + 32)\nPredicted variance: 1.8^2 \\times 25 = 3.24 \\times 25 = 81°F^2 ✓\nPredicted SD: 1.8 \\times 5 = 9°F ✓\n\nPerfect match! The variance and standard deviation transform consistently and predictably, making them valid measures of spread for interval data.\nTranslation-invariance demonstration: If we shift all temperatures by +100°C (adding 110°C and 120°C):\n\nNew mean: (110 + 120)/2 = 115°C\nNew deviations: (110 - 115) = -5, (120 - 115) = 5\nNew variance: \\text{Var}(X) = [(-5)^2 + (5)^2]/2 = 25°C^2 (unchanged!)\n\nThe variance remains 25°C² because the spread hasn’t changed, only the location shifted.\nKey principle: Operations based on addition/subtraction and differences work for interval scales because the arbitrary constant a cancels out. Operations involving multiplication/division or ratios require ratio scales because a distorts the results. Variance and SD work because they’re translation-invariant and based on deviations from the mean (differences), not ratios.\n\n\nSummary: Valid Statistical Measures by Measurement Scale\n\n\n\nStatistical Measure\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nMode\n✓\n✓\n✓\n✓\n\n\nMedian\n✗\n✓\n✓\n✓\n\n\nArithmetic Mean\n✗\n✗\n✓\n✓\n\n\nGeometric Mean\n✗\n✗\n✗\n✓\n\n\nVariance & SD\n✗\n✗\n✓\n✓\n\n\nCovariance\n✗\n✗\n✓\n✓\n\n\nPearson Correlation\n✗\n✗\n✓\n✓\n\n\nSpearman Correlation\n✗\n✓\n✓\n✓\n\n\nCoefficient of Variation\n✗\n✗\n✗\n✓\n\n\n\nNotes:\n\nNominal scales (e.g., colors, categories) only support frequency-based measures like mode\nOrdinal scales (e.g., rankings, Likert scales) add median and rank-based correlations\nInterval scales (e.g., Celsius, calendar dates) support all measures based on addition/subtraction\nRatio scales (e.g., height, weight, Kelvin) additionally support measures requiring multiplication/division and meaningful ratios\nCoefficient of variation (\\text{CV} = \\text{SD}/\\text{Mean}) requires a meaningful zero point, so only ratio scales",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html",
    "href": "rozdzial2.html",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "",
    "text": "4.1 Podstawy Zbiorów Liczbowych\nW badaniach z obszaru nauk społecznych zrozumienie natury danych jest kluczowe dla wyboru odpowiednich metod analizy i wyciągania prawidłowych wniosków.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#podstawy-zbiorów-liczbowych",
    "href": "rozdzial2.html#podstawy-zbiorów-liczbowych",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "",
    "text": "Note\n\n\n\nZrozumienie właściwości zbiorów liczbowych jest kluczowe dla uchwycenia natury różnych typów danych w naukach społecznych.\n\n\n\nPodstawowe Zbiory Liczbowe\n\nLiczby Naturalne (ℕ): Liczby używane do liczenia obiektów {0, 1, 2, 3, …}\nLiczby Całkowite (ℤ): Obejmują liczby naturalne, ich przeciwności i zero {…, -2, -1, 0, 1, 2, …}\nLiczby Wymierne (ℚ): Liczby, które można wyrazić jako ułamek dwóch liczb całkowitych\nLiczby Rzeczywiste (ℝ): Wszystkie liczby na osi liczbowej, w tym wymierne i niewymierne\n\n\n\nWłaściwości Zbiorów\n\nZbiory Przeliczalne: Zbiory, których elementy można ustawić w relacji jeden do jednego z liczbami naturalnymi. Na przykład, zbiór liczb całkowitych jest przeliczalny.\nZbiory Nieprzeliczalne: Zbiory, które nie są przeliczalne. Zbiór liczb rzeczywistych jest nieprzeliczalny.\nZbiory Dyskretne: Zbiory, w których każdy element jest oddzielony od innych elementów skończoną przerwą. Liczby całkowite tworzą zbiór dyskretny.\nZbiory Gęste: Zbiory, w których między dowolnymi dwoma elementami zawsze znajduje się inny element zbioru. Liczby wymierne i rzeczywiste są zbiorami gęstymi.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#dane-dyskretne-a-dane-ciągłe",
    "href": "rozdzial2.html#dane-dyskretne-a-dane-ciągłe",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.2 Dane dyskretne a dane ciągłe",
    "text": "4.2 Dane dyskretne a dane ciągłe\nW analizie danych i statystyce kategoryzujemy zmienne jako dyskretne lub ciągłe. Ten podział kształtuje sposób, w jaki analizujemy dane i które metody statystyczne stosujemy. Jednak granica między tymi kategoriami nie zawsze jest ostra, a niektóre zmienne wykazują cechy obu typów. Niniejszy rozdział omawia dane dyskretne i ciągłe, ich różnice oraz interesujące przypadki zmiennych, które kwestionują nasze intuicyjne rozumienie.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDane dyskretne\nDane dyskretne mogą przyjmować tylko określone, policzalne wartości. Wartości te są często (choć nie zawsze) liczbami całkowitymi.\n\nCharakterystyka danych dyskretnych:\n\nPrzeliczalny zbiór możliwych wartości\nCzęsto reprezentowane przez liczby całkowite\nMogą być skończone lub nieskończone\nBrak wartości pomiędzy dwoma sąsiednimi punktami danych\n\n\n\nPrzykłady:\n\nLiczba studentów w klasie\nLiczba samochodów sprzedanych przez salon\nLiczba monet w skarbonce\nWynik rzutu kostką (1, 2, 3, 4, 5 lub 6)\n\n\n\n\nDane ciągłe\nDane ciągłe mogą przyjmować dowolną wartość w danym zakresie, w tym wartości ułamkowe i dziesiętne. Co ważne, ciągłość charakteryzuje się gęstością: między dowolnymi dwiema wartościami istnieje nieskończenie wiele innych możliwych wartości.\n\nCharakterystyka danych ciągłych:\n\nWartości ze zbiorów gęstych (liczby wymierne lub rzeczywiste)\nMogą być mierzone z dowolną precyzją (teoretycznie)\nMiędzy dowolnymi dwoma punktami danych istnieje nieskończenie wiele wartości\nTypowo reprezentowane na skali ciągłej\n\n\n\nPrzykłady:\n\nWzrost\nWaga\nTemperatura\nCzas trwania\n\n\n\n\n\n\n\nUwaga matematyczna: Gęstość i ciągłość\n\n\n\nDane ciągłe pochodzą ze zbiorów gęstych, w których między dowolnymi dwiema różnymi wartościami istnieje inna wartość ze zbioru. Najczęstsze przykłady to:\n\nLiczby rzeczywiste: nieprzeliczalne i gęste\nLiczby wymierne: przeliczalne, ale gęste\n\nTa właściwość gęstości nadaje danym ciągłym charakterystyczną „gładkość” i pozwala stosować statystyczne metody oparte na rachunku różniczkowym i całkowym.\n\n\n\n\n\nSpektrum dyskretne-ciągłe\nW praktyce niektóre zmienne, które matematycznie są dyskretne, często traktuje się tak, jakby były ciągłe. Ta dualna natura zapewnia elastyczność w analizie i interpretacji tych zmiennych.\n\nPowody traktowania danych dyskretnych jako ciągłych:\n\nDrobna granulacja (małe przyrosty dyskretne)\n\nGdy zmienna dyskretna ma bardzo małe przyrosty między możliwymi wartościami, może aproksymować ciągłość.\nPrzykład: Dochód mierzony w groszach. Choć technicznie dyskretny, bardzo małe przyrosty i ogromna liczba możliwych wartości sprawiają, że zachowuje się podobnie do zmiennej ciągłej.\n\nWygoda analityczna\n\nMetody ciągłe często dają rozsądne i użyteczne wyniki nawet dla gęstych zmiennych dyskretnych.\nZałożenie ciągłości pozwala na wykorzystanie metod opartych na rachunku różniczkowym i istniejących narzędzi statystycznych.\n\nAproksymacja zjawisk podstawowych\n\nPomiar dyskretny może reprezentować proces ciągły u podstaw.\nPrzykład: Choć mierzymy czas w jednostkach dyskretnych (sekundy, minuty, godziny), czas sam w sobie płynie w sposób ciągły.\n\n\n\n\nPrzykłady zmiennych o dualnej naturze dyskretno-ciągłej:\n\nWiek\n\nDyskretny: Typowo podawany w pełnych latach\nCiągły: Może być traktowany jako zmienna ciągła w wielu analizach, szczególnie przy dużych populacjach lub gdy liczy się precyzja\n\nCena i dochód\n\nDyskretne: Ceny i dochody są faktycznie mierzone w dyskretnych jednostkach (np. grosze lub najmniejsza jednostka walutowa)\nCiągłe: W modelach ekonomicznych i wielu analizach traktowane jako zmienne ciągłe ze względu na ich gęstą naturę i wygodę analityczną\n\nWyniki testów\n\nDyskretne: Często podawane jako liczby całkowite lub stałe przyrosty\nCiągłe: W analizach statystycznych mogą być traktowane jako ciągłe, szczególnie gdy zakres możliwych wyników jest duży\n\n\n\n\n\nPrzypadek szczególny: Procenty i proporcje\nProcenty i proporcje stanowią interesujący przypadek w spektrum dyskretno-ciągłym. W większości praktycznych zastosowań są traktowane jako ciągłe ze względu na ich gęstą naturę, mimo że mają pewne cechy dyskretne.\n\n\n\n\n\n\nProcenty: Łączenie dyskretności i ciągłości\n\n\n\nZmienne mierzone w procentach, takie jak stopa bezrobocia czy frekwencja wyborcza, kwestionują nasze intuicyjne rozumienie dyskretności i ciągłości:\n\nNatura wymierna: Procenty są zasadniczo ułamkami (m/100), co czyni je liczbami wymiernymi.\nGęste, ale przeliczalne: Zbiór liczb wymiernych jest gęsty (między dowolnymi dwiema liczbami wymiernymi jest inna wymierna), ale również przeliczalny – ilustruje to, że gęstość i przeliczalność to niezależne właściwości.\nPraktyczna ciągłość: W większości zastosowań procenty są traktowane jako ciągłe ze względu na ich gęstą naturę i wygodę analityczną.\nSkończona precyzja: W rzeczywistości procenty są często podawane z ograniczoną liczbą miejsc po przecinku, tworząc skończony (dyskretny) zbiór możliwych wartości.\n\nTa dwoistość pozwala na elastyczne podejścia analityczne, w zależności od konkretnego kontekstu badawczego i wymaganej precyzji.\n\n\n\n\nImplikacje dla analizy danych\nZrozumienie niuansów zmiennych jako dyskretnych, ciągłych lub gdzieś pomiędzy ma istotne implikacje dla analizy danych:\n\nElastyczność w modelowaniu: Umożliwia wykorzystanie szerszego zakresu technik statystycznych.\nUproszczone obliczenia: Traktowanie gęstych danych dyskretnych jako ciągłych może uprościć obliczenia i uczynić niektóre analizy bardziej wykonalnymi.\nLepsza interpretowalność: W niektórych przypadkach traktowanie danych dyskretnych jako ciągłych może prowadzić do bardziej intuicyjnych lub użytecznych interpretacji wyników.\nPotencjał do błędów: Ważne jest, aby być świadomym, kiedy aproksymacje są odpowiednie, a kiedy mogą prowadzić do mylących wyników.\nWzględy teoretyczne vs. praktyczne: Choć matematyczna natura danych jest ważna, to praktyczne względy związane z pomiarem i analizą często kierują tym, jak traktujemy zmienne.\n\n\n\nPodsumowanie\nRozróżnienie między danymi dyskretnymi a ciągłymi nie zawsze jest sztywne w praktyce. Wiele zmiennych, w tym te obejmujące pieniądze, procenty czy gęste pomiary, można postrzegać zarówno przez pryzmat dyskretny, jak i ciągły. W razie wątpliwości należy rozważyć zarówno precyzję pomiaru, jak i cele analityczne przy podejmowaniu decyzji, jak traktować zmienną. Wybór powinien być kierowany naturą danych, celami analizy i potencjalnymi implikacjami wyboru. Ta elastyczność, stosowana rozważnie, zapewnia badaczom potężne narzędzia do wydobywania wniosków z danych.\n\n\n\n\n\n\nDane Dyskretne vs. Ciągłe: Analogia Językowa\n\n\n\n\nKluczowe Rozróżnienie Językowe\nW języku polskim mamy precyzyjne rozróżnienie:\n\n“Liczba” → używamy dla rzeczy policzalnych\n“Ilość” → używamy dla rzeczy niepoliczalnych\n\nTo rozróżnienie doskonale odzwierciedla dwa podstawowe typy danych liczbowych:\n\n\nDane Dyskretne = “Liczba czegoś”\n\nUżywamy słowa “liczba” (tak jak mówimy “liczba studentów”)\nWartości są rozdzielone jak pojedyncze elementy\nPrzykłady:\n\nLiczba książek: 0, 1, 2, 3…\nLiczba punktów w teście: 0, 1, 2…\nLiczba mieszkańców: 100, 101, 102…\n\n\n🤔 Czy poprawne jest powiedzenie “ilość studentów” czy “liczba studentów”? (Poprawna forma pomoże Ci rozpoznać typ danych)\n\n\nDane Ciągłe = “Ilość czegoś”\n\nUżywamy słowa “ilość” (tak jak mówimy “ilość wody”)\nWartości płynnie przechodzą jedna w drugą\nPrzykłady:\n\nIlość cieczy: 1,5231… litra\nIlość czasu: 2,3891… godziny\nIlość energii: 5,7123… kWh\n\n\n🤔 Czy mówimy “ilość wody” czy “liczba wody”? (Poprawna forma wskazuje na typ danych)\n\n\nSposób Rozpoznawania\n\nCzy użyłbyś słowa “liczba”? → Dane dyskretne\nCzy użyłbyś słowa “ilość”? → Dane ciągłe\n\n✍️ Ćwiczenie: Uzupełnij poprawnym słowem i określ typ danych\n\n_____ uczniów w klasie (liczba/ilość): typ _____\n_____ deszczu (liczba/ilość): typ _____\n_____ piosenek (liczba/ilość): typ _____\n_____ temperatury (liczba/ilość): typ _____",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "href": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.3 Wprowadzenie do Typologii Danych Stevensa",
    "text": "4.3 Wprowadzenie do Typologii Danych Stevensa\nStanley S. Stevens, amerykański psycholog, wprowadził system klasyfikacji skal pomiarowych w swoim artykule z 1946 roku “On the Theory of Scales of Measurement”. Ten system, znany jako typologia danych Stevensa lub poziomy pomiaru, stał się fundamentalny dla zrozumienia, jak różne typy danych powinny być analizowane i interpretowane.\nStevens zaproponował cztery poziomy pomiaru:\n\nNominalny\nPorządkowy\nInterwałowy\nIlorazowy\n\nKażdy poziom ma specyficzne właściwości i pozwala na różne rodzaje operacji statystycznych i analiz.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\nSkala Nominalna\n\nDefinicja\nSkala nominalna jest najbardziej podstawowym poziomem pomiaru. Używa etykiet lub kategorii do klasyfikacji danych bez żadnej wartości ilościowej ani porządku.\n\n\nWłaściwości\n\nKategorie są wzajemnie wykluczające się\nBrak inherentnego porządku między kategoriami\nNie można wykonywać znaczących operacji arytmetycznych\n\n\n\nPrzykłady\n\nNarodowość (Polak, Niemiec, …)\nGrupy krwi (A, B, AB, O)\nKolor oczu (Niebieskie, Brązowe, Zielone, Piwne)\nZmienne binarne (“Sukces” versus “Niepowodzenie”)\n\n\n\n\nSkala Porządkowa\n\nDefinicja\nSkala porządkowa kategoryzuje dane w uporządkowane kategorie, ale odstępy między kategoriami niekoniecznie są równe lub znaczące.\n\n\nWłaściwości\n\nKategorie mają zdefiniowany porządek\nRóżnice między kategoriami nie są kwantyfikowalne\nOperacje arytmetyczne na liczbach nie są znaczące\n\n\n\nPrzykłady\n\nPoziomy wykształcenia (Szkoła Średnia, Licencjat, Magister, Doktorat)\nSkale Likerta (Zdecydowanie się nie zgadzam, Nie zgadzam się, Neutralnie, Zgadzam się, Zdecydowanie się zgadzam)\nStatus społeczno-ekonomiczny (Niski, Średni, Wysoki)\n\n\n\n\nSkala Interwałowa\n\nDefinicja\nSkala interwałowa ma uporządkowane kategorie z równymi odstępami między sąsiednimi kategoriami. Jednak brakuje jej prawdziwego punktu zerowego.\n\n\nWłaściwości\n\nRówne odstępy między sąsiednimi kategoriami\nBrak prawdziwego punktu zerowego (zero jest umowne)\nStosunki między wartościami nie są znaczące\n\n\n\nPrzykłady\n\nTemperatura w stopniach Celsjusza lub Fahrenheita\nLata kalendarzowe\nSkala pH\nWysokość nad poziomem morza\n\n\n\n\nSkala Ilorazowa\n\nDefinicja\nSkala ilorazowa jest najwyższym poziomem pomiaru. Ma wszystkie właściwości skali interwałowej plus prawdziwy punkt zerowy, co sprawia, że stosunki między wartościami są znaczące.\n\n\nWłaściwości\n\nWszystkie właściwości skal interwałowych\nPrawdziwy punkt zerowy\nStosunki między wartościami są znaczące\n\n\n\nPrzykłady\n\nWzrost\nWaga\nWiek\nDochód\n\n\n\n\nZnaczenie w Badaniach i Analizie\nZrozumienie typologii danych Stevensa jest kluczowe z kilku powodów:\n\nWybór odpowiednich testów statystycznych: Poziom pomiaru determinuje, które analizy statystyczne są odpowiednie dla danego zbioru danych.\nInterpretacja wyników: Znaczenie wyników statystycznych zależy od poziomu pomiaru zaangażowanych zmiennych.\nProjektowanie narzędzi pomiarowych: Przy tworzeniu ankiet lub innych narzędzi pomiarowych badacze muszą wziąć pod uwagę poziom pomiaru, który chcą osiągnąć.\nTransformacja danych: Czasami dane mogą być przekształcane z jednego poziomu na drugi, ale musi to być robione ostrożnie, aby uniknąć błędnej interpretacji.\n\n\n\nKontrowersje i Ograniczenia\nChociaż typologia Stevensa jest szeroko stosowana, spotkała się z pewnymi krytykami:\n\nSztywność: Niektórzy twierdzą, że typologia jest zbyt sztywna i że wiele rzeczywistych pomiarów mieści się pomiędzy tymi kategoriami.\nTraktowanie danych porządkowych: Trwa debata na temat tego, kiedy właściwe jest traktowanie danych porządkowych jako interwałowych dla pewnych analiz.\nSkalowanie psychologiczne: Niektóre konstrukty psychologiczne (jak inteligencja) są trudne do jednoznacznego skategoryzowania w ramach tego systemu.\n\n\n\nPodsumowanie\nTypologia danych Stevensa dostarcza fundamentalnych ram dla zrozumienia różnych rodzajów danych i ich właściwości. Rozpoznając poziom pomiaru swoich zmiennych, badacze mogą podejmować świadome decyzje dotyczące gromadzenia danych, analizy i interpretacji. Jednak ważne jest, aby pamiętać, że chociaż ta typologia jest użytecznym przewodnikiem, rzeczywiste dane często wymagają niuansowego podejścia i nie zawsze pasują idealnie do tych kategorii.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not “twice as acidic” as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#popularne-skale-porządkowe-w-badaniach-behawioralnych",
    "href": "rozdzial2.html#popularne-skale-porządkowe-w-badaniach-behawioralnych",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.4 Popularne Skale Porządkowe w Badaniach Behawioralnych",
    "text": "4.4 Popularne Skale Porządkowe w Badaniach Behawioralnych\n\nSkale Likerta\nSkale Likerta są szeroko stosowane w psychologii i naukach społecznych do pomiaru postaw, opinii i percepcji. Nazwane na cześć psychologa Rensisa Likerta, skale te zazwyczaj składają się z serii stwierdzeń lub pytań, które respondenci oceniają na skali, często od “Zdecydowanie się nie zgadzam” do “Zdecydowanie się zgadzam”.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDlaczego Skale Likerta są Zmiennymi Porządkowymi\nSkale Likerta są uważane za zmienne porządkowe z kilku powodów:\n\nPorządek bez równych odstępów: Chociaż odpowiedzi mają wyraźną kolejność (np. “Zdecydowanie się nie zgadzam” &lt; “Nie zgadzam się” &lt; “Neutralnie” &lt; “Zgadzam się” &lt; “Zdecydowanie się zgadzam”), odstępy między tymi kategoriami niekoniecznie są równe.\nSubiektywna interpretacja: Różnica między “Zdecydowanie się nie zgadzam” a “Nie zgadzam się” może nie być taka sama jak różnica między “Zgadzam się” a “Zdecydowanie się zgadzam” dla wszystkich respondentów.\nBrak prawdziwego punktu zerowego: Skale Likerta zazwyczaj nie mają prawdziwego punktu zerowego, co jest cechą charakterystyczną skal interwałowych lub ilorazowych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#skale-pomiarowe-według-typologii-stevensa-zmienne-ilościowe-vs.-porządkowe",
    "href": "rozdzial2.html#skale-pomiarowe-według-typologii-stevensa-zmienne-ilościowe-vs.-porządkowe",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.5 Skale pomiarowe według typologii Stevensa: zmienne ilościowe vs. porządkowe",
    "text": "4.5 Skale pomiarowe według typologii Stevensa: zmienne ilościowe vs. porządkowe\n\nCzym są zmienne ilościowe (numeryczne)?\nW typologii Stevensa wyróżniamy cztery skale pomiarowe. Zmienne ilościowe to te, które mierzone są na skalach interwałowych lub ilorazowych:\n\nSkala interwałowa: Posiada równe odstępy między jednostkami, ale brak naturalnego punktu zerowego\nSkala ilorazowa: Posiada równe odstępy między jednostkami oraz naturalny punkt zerowy\n\nZmienne ilościowe charakteryzują się następującymi właściwościami:\n\nRówne interwały: Różnica między 5 a 6 reprezentuje taką samą wielkość jak różnica między 95 a 96\nSpójne jednostki: Każdy przyrost reprezentuje taką samą ilość mierzonej cechy\nObiektywny pomiar: Mierzą rzeczywiste ilości, a nie tylko względne pozycje\nDopuszczalne operacje matematyczne: Można wykonywać operacje arytmetyczne\n\nPrzykłady zmiennych mierzonych na skali ilorazowej:\n\nWzrost: 170 cm jest dokładnie o 10 cm wyższe niż 160 cm, a 170 cm jest dokładnie dwa razy wyższe niż 85 cm\nWaga: Różnica między 50 kg a 60 kg to taka sama ilość wagi jak między 80 kg a 90 kg\nCzas: 4 godziny to dwa razy dłużej niż 2 godziny, a różnica między 3 a 4 godzinami jest taka sama jak między 9 a 10 godzinami\nTemperatura w Kelwinach: 200K jest dwa razy cieplejsza niż 100K (ponieważ 0K to zero absolutne)\n\nPrzykład skali interwałowej:\n\nTemperatura w stopniach Celsjusza: Różnica między 20°C a 30°C jest taka sama jak między 70°C a 80°C, ale 40°C nie jest “dwa razy cieplejsze” niż 20°C (brak naturalnego zera)\nRok kalendarzowy: Różnica między 2020 a 2021 jest taka sama jak między 1950 a 1951, ale rok 2000 nie jest “dwa razy starszy” niż rok 1000\n\n\n\nRzeczywistość: IQ to fundamentalnie skala porządkowa\nJak powstają wyniki IQ – krok po kroku:\n\nZbieranie surowych wyników: Ludzie rozwiązują test i otrzymują liczbę poprawnych odpowiedzi (np. 45 z 60 pytań)\nUporządkowanie: Wszystkie surowe wyniki są uszeregowane od najgorszego do najlepszego\nPrzypisanie rang: Każdemu wynikowi przypisuje się pozycję w rankingu\nPrzekształcenie na skalę IQ: Rangi są przekształcane matematycznie tak, aby średnia wynosiła 100, a odchylenie standardowe 15\n\nKluczowy problem: Proces ten wymusza rozkład normalny na dane, które może wcale nie były normalne w pierwotnej postaci. To znaczy, że równe różnice w punktach IQ (np. różnica między IQ 100 a 115 vs. różnica między IQ 115 a 130) nie muszą odpowiadać równym różnicom w rzeczywistych zdolnościach poznawczych.\n\n\n\n\n\n\nImportant\n\n\n\nIQ 130 nie oznacza „dwukrotnie większej inteligencji” niż IQ 65. Punkty IQ pokazują tylko pozycję danej osoby względem innych ludzi w próbie, nie rzeczywistą ilość inteligencji. To podobnie jak miejsca w konkursie – zwycięzca może wygrać o włos lub o kilometry, ale nadal będzie pierwszym miejscem.\n\n\nW praktyce badawczej: dlaczego czasem traktujemy IQ jako skalę interwałową?\nJest to metodologiczny kompromis, który pozwala na użycie bardziej precyzyjnych narzędzi statystycznych:\n✅ Traktowanie IQ jako skali interwałowej jest dopuszczalne gdy:\n\nUżywamy standardowych testów statystycznych (korelacje, regresje, testy t)\nPorównujemy grupy w ramach tego samego testu i populacji\nJesteśmy świadomi ograniczeń tego podejścia - Nasze wnioski nie zależą od tego, czy różnice są dokładnie równe\n\n⚠️ Pamiętaj o ograniczeniach:\n\nTo uproszczenie rzeczywistości\nZałożenie działa lepiej dla wyników bliskich średniej (IQ 85-115) niż na krańcach\nWyniki trzeba interpretować ostrożnie\n\n❌ Nigdy nie wolno:\n\nMówić, że różnice IQ oznaczają równe różnice w inteligencji\nUżywać stwierdzeń typu “dwa razy bardziej inteligentny”\n\n\n\nPraktyczne wskazówki dla badaczy\n\nBądź transparentny:\n\nWyraźnie wspominaj: “Traktujemy IQ jako skalę interwałową do celów statystycznych, pamiętając że fundamentalnie jest to skala porządkowa”\n\nRozważaj alternatywy:\n\nUżywaj testów nieparametrycznych, gdy wielkość próby na to pozwala\nPorównaj wyniki różnych metod analitycznych\n\nInterpretuj ostrożnie:\n\nSkupiaj się na stwierdzeniach o kolejności („grupa A osiągnęła wyższe wyniki niż grupa B”)\nUnikaj precyzyjnych stwierdzeń o wielkości różnic\nPamiętaj: różnica 15 punktów IQ oznacza „jedno odchylenie standardowe w próbie”, nie „konkretną ilość dodatkowej inteligencji”\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIQ to skala porządkowa, która została przekształcona tak, aby wyglądała jak skala interwałowa. Można używać jej w analizach statystycznych wymagających skali interwałowej, ale zawsze należy pamiętać o jej rzeczywistej naturze przy interpretacji wyników. Kluczowe jest zrozumienie, że punkty IQ mówią nam o relatywnej pozycji w grupie, nie o bezwzględnej ilości inteligencji.\n\n\nMimo liczbowego zapisu, wyniki IQ są w istocie zmienną porządkową (w typologii Stevensa), a nie zmienną interwałową, ponieważ:\n\nBrak jednolitej jednostki miary: Nie istnieje naturalna jednostka mierząca “inteligencję”\nKonstrukcja oparta na rangach: Skala powstaje przez uszeregowanie ludzi względem siebie, a następnie przekształcenie tych rang w wyniki liczbowe\nNierówne interwały: Różnica między IQ 100 a 110 niekoniecznie reprezentuje taką samą różnicę poznawczą jak między 130 a 140\nBrak absolutnego zera: Nie istnieje znaczące pojęcie “zerowej inteligencji”\nZależność od testu: Różne testy IQ mogą dać różne wyniki dla tej samej osoby\n\nPrzykład: Rozważmy trzy osoby z wynikami IQ 85, 100 i 115. Choć moglibyśmy chcieć powiedzieć, że różnica między pierwszą a drugą osobą równa się różnicy między drugą a trzecią, nie jest to faktycznie znaczące—zdolności poznawcze reprezentowane przez te wyniki nie wzrastają w równych krokach, mimo że cyfry sugerują równe odstępy. Wyniki te informują nas głównie o pozycji osoby względem innych, co jest cechą skali porządkowej.\nInny przykład: Osoba z IQ 140 nie jest “dwa razy inteligentniejsza” niż osoba z IQ 70, mimo że stosunek liczb wynosi 2:1. Takie porównanie nie ma sensu na skali porządkowej.\n\n\nPunkty egzaminacyjne - pomiędzy skalą porządkową a interwałową\nChoć traktujemy punkty egzaminacyjne jak zmienne ilościowe (interwałowe), często mają one cechy zmiennych porządkowych:\n\nNierówna trudność pytań: Pytanie za 10 punktów z fizyki kwantowej nie mierzy tej samej ilości wiedzy co pytanie za 10 punktów z podstawowej arytmetyki\nRóżne rodzaje kompetencji: Różne pytania testują różne umiejętności (zapamiętywanie, rozumienie, zastosowanie, analiza)\nSubiektywne przydzielanie punktów: Punktacja zależy od oceny egzaminatora, a nie obiektywnych jednostek miary\nBrak addytywności: Student zdobywający 90 punktów niekoniecznie jest “dwa razy bardziej wykształcony” niż student zdobywający 45 punktów\n\nPrzykład: Wyobraźmy sobie dwóch studentów:\n\nStudent A: Odpowiada prawidłowo na wszystkie łatwe i średnie pytania (zdobywa 75 punktów)\nStudent B: Odpowiada prawidłowo na wszystkie trudne pytania, ale żadne łatwe (zdobywa 75 punktów)\n\nMimo równej punktacji, ich wiedza jest jakościowo różna. Punkty sugerują równość, ale w rzeczywistości są to różne profile kompetencji — typowy problem ze zmiennymi, które nie są w pełni ilościowe.\n\n\nTraktowanie zmiennych porządkowych jako ilościowych w praktyce\nZe względów praktycznych często traktujemy zmienne porządkowe jak zmienne ilościowe, ponieważ:\n\nUmożliwia to stosowanie znanych operacji matematycznych (średnie, odchylenia)\nUpraszcza komunikację i interpretację wyników (“średni wynik 78%”)\nPozwala na stosowanie bardziej zaawansowanych metod statystycznych\n\nPrzykład: Średnia ocen\n\nObliczamy średnią, przypisując wartości liczbowe ocenom (5, 4, 3, 2, 1)\nTraktujemy te wartości jak zmienne ilościowe, obliczając np. średnią 4,5\nAle czy różnica między oceną 5 a 4 (5-4=1) reprezentuje taką samą różnicę wiedzy jak między oceną 2 a 1 (2-1=1)?\nI czy średnia 5.0 jest naprawdę “dwa razy lepsza” niż średnia 2.5?\n\nInny przykład: Skale Likerta\n\nW ankietach często stosujemy skale typu: 1 = “zdecydowanie nie zgadzam się”, 5 = “zdecydowanie zgadzam się”\nObliczamy średnie odpowiedzi, zakładając równe odstępy między kategoriami\nAle czy odległość między “zdecydowanie nie zgadzam się” a “raczej nie zgadzam się” jest naprawdę taka sama jak między “raczej zgadzam się” a “zdecydowanie zgadzam się”?\n\n\n\nZnaczenie rozróżnienia skal pomiarowych\nRozumienie typologii skal pomiarowych Stevensa ma istotne konsekwencje praktyczne:\n\nZmienne jakościowe porządkowe: Pozwalają na stwierdzenie, że coś jest “większe/lepsze” lub “mniejsze/gorsze”, ale nie określają “o ile” (dopuszczalne porównania typu &gt;, &lt;, =)\nZmienne ilościowe: Pozwalają na określenie dokładnych różnic i proporcji (dopuszczalne operacje +, -, ×, ÷)\n\nŚwiadomość ograniczeń skali pomiarowej pomaga w poprawnej interpretacji danych i doborze odpowiednich metod analizy.\nPrzykład: Jeśli Anna uzyskała 75 punktów na teście z historii i 85 punktów na teście z matematyki, nie możemy jednoznacznie stwierdzić, że jest “lepsza z matematyki o 10 jednostek umiejętności”. Punkty z różnych testów nie są bezpośrednio porównywalne, a interwały mogą nie być równoważne.\nPoprawniejsze podejście: Lepiej porównać jej wyniki z rozkładem wyników innych uczniów. Jeśli w historii 75 punktów plasuje ją w 50. percentylu, a 85 punktów z matematyki w 90. percentylu, to możemy powiedzieć, że względnie rzecz biorąc, radzi sobie lepiej z matematyką niż historią - co jest wnioskiem opartym na skali porządkowej.\n\n\nPodsumowanie\nChociaż skale Likerta i wiele miar psychologicznych jest często traktowanych jako dane interwałowe ze względów praktycznych, ważne jest, aby pamiętać o ich porządkowym charakterze.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n\n\n\n\nĆwiczenie: Identyfikacja Skal Pomiarowych\n\n\n\nDla każdej z poniższych zmiennych określ najbardziej odpowiednią skalę pomiaru (Nominalna, Porządkowa, Przedziałowa lub Stosunkowa). Czy zmienna jest dyskretna, czy ciągła?\n\nPłeć: skala nominalna; zmienna dyskretna;\nSatysfakcja klienta: Niska, Średnia, Dobra, Doskonała\nWzrost (ankieta): “Jestem: bardzo niski, niski, przeciętnego wzrostu, wysoki, bardzo wysoki”\nWzrost mierzony w centymetrach\nCzas reakcji (w milisekundach)\nKody pocztowe: np. 00-001, 00-950, 80-452, 31-072\nWiek (w latach)\nMarki samochodów\nNarodowość\nLiczba dzieci w rodzinie: 1 dziecko, 2 dzieci, 3 dzieci, …\nWynik testu IQ\nTemperatura (skala Celsjusza)\nTemperatura (skala Kelvina)\nFrekwencja wyborcza\nPrzynależność partyjna\nWielkość okręgu wyborczego\nWspółrzędne w układzie kartezjańskim\nData (względem określonej epoki, np. n.e.)\nWysokość nad poziomem morza\nGrupy krwi: A, B, AB, 0\nKategorie dochodów: niskie, średnie, wysokie\nStopnie wojskowe\n\nPamiętaj, aby uzasadnić swój wybór skali dla każdej zmiennej.\nDla przykładu: W typologii skal pomiarowych Stevensa, adresy uliczne są danymi nominalnymi. Dlaczego?\nPełnią wyłącznie funkcję etykiet/identyfikatorów Nie mają naturalnego uporządkowania (ul. Mickiewicza 5 nie jest “większa” niż ul. Słowackiego 10) Nie można wykonywać na nich sensownych operacji matematycznych Jedyna dozwolona operacja to sprawdzanie równości/nierówności (czy to ten sam adres czy inny?)\nMimo że numery domów są liczbami, w systemie adresowym funkcjonują jako etykiety, a nie wartości ilościowe. Liczba 100 w adresie “ul. Kilińskiego 100” nie jest używana matematycznie - równie dobrze mogłaby to być “ul. Jabłkowa” czy “ul. Zeusa”, jeśli chodzi o jej funkcję w adresie.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#appendix-a",
    "href": "rozdzial2.html#appendix-a",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.6 Appendix A",
    "text": "4.6 Appendix A\n\n\n\n\n\n\nDlaczego dodawanie/odejmowanie działa dla danych interwałowych, ale mnożenie/dzielenie wymaga danych ilorazowych\n\n\n\nSkale interwałowe mają równe odstępy między wartościami, ale arbitralny punkt zerowy (np. temperatura w Celsjuszach, daty kalendarzowe). Skale ilorazowe mają zarówno równe odstępy, jak i bezwzględne zero (np. temperatura w Kelwinach, wzrost, masa).\n\nDodawanie i odejmowanie: prawidłowe dla skal interwałowych\nRóżnice zachowują spójne proporcjonalne relacje przy transformacjach liniowych. Przy konwersji między skalami za pomocą y = a + bx gdzie b &gt; 0, stała addytywna a się redukuje:\n(y_2 - y_1) = (a + bx_2) - (a + bx_1) = b(x_2 - x_1)\nPrzykład: Przy konwersji z Celsjusza na Fahrenheita gdzie F = 1.8C + 32:\n\nKażda różnica 10°C zawsze konwertuje się na 18°F: 18 = 1.8 \\times 10\nSprawdźmy (20°C - 10°C) \\rightarrow (68°F - 50°F) = 18°F\nSprawdźmy (100°C - 90°C) \\rightarrow (212°F - 194°F) = 18°F\n\nRelacja jest spójna: różnica 10 stopni Celsjusza zawsze odpowiada różnicy 18 stopni Fahrenheita, niezależnie od tego, gdzie na skali mierzymy.\n\n\nMnożenie i dzielenie: wymagają skal ilorazowych\nIlorazy są niespójne, gdy punkt zerowy jest arbitralny. Stała addytywna a NIE redukuje się w ilorazach:\n\\frac{y_2}{y_1} = \\frac{a + bx_2}{a + bx_1} \\neq b \\cdot \\frac{x_2}{x_1}\nJeśli a \\neq 0 (brak bezwzględnego zera), ilorazy zmieniają się nieprzewidywalnie w zależności od porównywanych wartości.\nPrzykład: Ilorazy temperatur dają niespójne wyniki:\n\nCzy 20°C jest “dwa razy cieplej” niż 10°C?\n\nCelsjusz: 20/10 = 2.0\nFahrenheit: 68/50 = 1.36\nKelwin: 293.15/283.15 = 1.035\n\nA co z 100°C vs. 90°C?\n\nCelsjusz: 100/90 = 1.11\nFahrenheit: 212/194 = 1.09\n\n\nIlorazy różnią się w zależności zarówno od skali, JAK I od porównywanych temperatur. Tylko przy bezwzględnym zerze ilorazy mają spójne fizyczne znaczenie.\n\n\nImplikacje dla miar statystycznych\nŚrednia arytmetyczna jest prawidłowa dla skal interwałowych, ponieważ wykorzystuje dodawanie:\n\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\nPrzy transformacji do skali y, średnia transformuje się spójnie: \\bar{y} = a + b\\bar{x}\nŚrednia geometryczna wymaga skal ilorazowych, ponieważ wykorzystuje mnożenie:\n\\text{SG} = \\sqrt[n]{x_1 \\times x_2 \\times \\cdots \\times x_n}\nŚrednia geometryczna temperatur w Celsjuszach daje inny wynik niż średnia geometryczna tych samych temperatur w Fahrenheitach (po konwersji). To czyni średnią geometryczną bezsensowną dla danych interwałowych.\nPrzykład: Dla temperatur 10°C i 20°C:\n\nŚrednia geometryczna w Celsjuszach: \\sqrt{10 \\times 20} = 14.14°C \\rightarrow 57.45°F\nŚrednia geometryczna w Fahrenheitach: \\sqrt{50 \\times 68} = 58.31°F \\rightarrow 14.62°C\n\nTe wartości się nie zgadzają! Średnia geometryczna zależy od arbitralnego punktu zerowego.\n\n\nWariancja i odchylenie standardowe: prawidłowe dla skal interwałowych\nWariancja i odchylenie standardowe są dopuszczalne dla danych interwałowych, ponieważ operują na odchyleniach od średniej, które są różnicami. Co kluczowe, wariancja jest niezmienna względem translacji: dodanie stałej do wszystkich wartości nie zmienia wariancji, ponieważ odchylenia pozostają takie same.\nPrzy transformacji liniowej y = a + bx, wariancja transformuje się przewidywalnie:\n\\text{Var}(y) = b^2 \\text{Var}(x)\nStała a redukuje się przy obliczaniu odchyleń, tak jak przy prostych różnicach.\nPrzykład dowodu: Dla temperatur 10°C i 20°C:\nW Celsjuszach:\n\nŚrednia: \\bar{x} = (10 + 20)/2 = 15°C\nOdchylenia: (10 - 15) = -5, (20 - 15) = 5\nWariancja: \\text{Var}(X) = [(-5)^2 + (5)^2]/2 = 50/2 = 25°C^2\nOdchylenie standardowe: \\text{SD}(X) = 5°C\n\nW Fahrenheitach:\n\nKonwersja: 10°C → 50°F, 20°C → 68°F\nŚrednia: \\bar{y} = (50 + 68)/2 = 59°F\nOdchylenia: (50 - 59) = -9, (68 - 59) = 9\nWariancja: \\text{Var}(Y) = [(-9)^2 + (9)^2]/2 = 162/2 = 81°F^2\nOdchylenie standardowe: \\text{SD}(Y) = 9°F\n\nSprawdzenie transformacji:\n\nNachylenie konwersji: b = 1.8 (z F = 1.8C + 32)\nPrzewidywana wariancja: 1.8^2 \\times 25 = 3.24 \\times 25 = 81°F^2 ✓\nPrzewidywane odchylenie: 1.8 \\times 5 = 9°F ✓\n\nIdealne dopasowanie! Wariancja i odchylenie standardowe transformują się spójnie i przewidywalnie, czyniąc je prawidłowymi miarami rozproszenia dla danych interwałowych.\nDemonstracja niezmienności względem translacji: Jeśli przesuniemy wszystkie temperatury o +100°C (dodając 110°C i 120°C):\n\nNowa średnia: (110 + 120)/2 = 115°C\nNowe odchylenia: (110 - 115) = -5, (120 - 115) = 5\nNowa wariancja: \\text{Var}(X) = [(-5)^2 + (5)^2]/2 = 25°C^2 (niezmieniona!)\n\nWariancja pozostaje 25°C², ponieważ rozproszenie się nie zmieniło, zmienił się tylko punkt odniesienia.\nKluczowa zasada: Operacje oparte na dodawaniu/odejmowaniu i różnicach działają dla skal interwałowych, ponieważ arbitralna stała a się redukuje. Operacje obejmujące mnożenie/dzielenie lub ilorazy wymagają skal ilorazowych, ponieważ a zniekształca wyniki. Wariancja i odchylenie standardowe działają, ponieważ są niezmienne względem translacji i oparte na odchyleniach od średniej (różnicach), a nie ilorazach.\n\n\nPodsumowanie: prawidłowe miary statystyczne według skali pomiarowej\n\n\n\n\n\n\n\n\n\n\nMiary statystyczne\nNominalna\nPorządkowa\nInterwałowa\nIlorazowa\n\n\n\n\nDominanta\n✓\n✓\n✓\n✓\n\n\nMediana\n✗\n✓\n✓\n✓\n\n\nŚrednia arytmetyczna\n✗\n✗\n✓\n✓\n\n\nŚrednia geometryczna\n✗\n✗\n✗\n✓\n\n\nWariancja i odchylenie stand.\n✗\n✗\n✓\n✓\n\n\nKowariancja\n✗\n✗\n✓\n✓\n\n\nKorelacja Pearsona\n✗\n✗\n✓\n✓\n\n\nKorelacja Spearmana\n✗\n✓\n✓\n✓\n\n\nWspółczynnik zmienności\n✗\n✗\n✗\n✓\n\n\n\nUwagi:\n\nSkale nominalne (np. kolory, kategorie) obsługują tylko miary oparte na częstościach, jak dominanta\nSkale porządkowe (np. rankingi, skale Likerta) dodają medianę i korelacje oparte na rangach\nSkale interwałowe (np. Celsjusz, daty kalendarzowe) obsługują wszystkie miary oparte na dodawaniu/odejmowaniu\nSkale ilorazowe (np. wzrost, masa, Kelwin) dodatkowo obsługują miary wymagające mnożenia/dzielenia i sensownych ilorazów\nWspółczynnik zmienności (\\text{CV} = \\text{SD}/\\text{Średnia}) wymaga sensownego punktu zerowego, więc tylko skale ilorazowe",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "5.1 Introduction to Sigma Notation (Σ)\nDescriptive statistics are fundamental tools in social science research, providing a concise summary of data characteristics. They serve several crucial functions:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction-to-sigma-notation-σ",
    "href": "chapter5.html#introduction-to-sigma-notation-σ",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "What is Sigma summation notation? Sigma (Σ) is a mathematical operator that instructs us to sum (add) a sequence of terms - it functions as a directive to perform addition of all elements within a specified range.\nPurpose: Provides a concise way to write sums of many similar terms using a single symbol, avoiding lengthy addition expressions.\n\n\nBasic Formula\n\nThe general form of sigma notation is: \\sum_{i=a}^{b} f(i)\nSummation index: i\nLower bound: a\nUpper bound: b\nFunction: f(i)\n\n\n\nExamples of Sigma Notation Applications\n\nSimple Example: Sum of Natural Numbers\n\nSuppose you want to add the first five positive integers: \\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15\nThe above notation adds the first five positive integers.\n\n\n\nSum of Squares\n\nSuppose you want to sum the squares of the first four positive integers: \\sum_{i=1}^{4} i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 1 + 4 + 9 + 16 = 30\nThis is the sum of squares of the first four positive integers.\n\n\n\nSum of a Constant Value\n\nSumming a constant value c for n terms: \\sum_{i=1}^{n} c = c + c + c + ... + c \\text{ (n times)} = n \\cdot c\nExample: Sum of five fives: \\sum_{i=1}^{5} 5 = 5 + 5 + 5 + 5 + 5 = 5 \\cdot 5 = 25\n\n\n\n\nSimple Examples in Statistical Context\n\\sum_{i=1}^{n} x_i - Summation index: i (typically denotes a specific observation in a dataset) - Lower bound: 1 (we usually start from the first observation) - Upper bound: n (total number of observations in our dataset) - Expression: x_i (value of the ith observation)\n\nSumming Observation Values\n\nWe have a dataset: 5, 8, 12, 15, 20\nSum of all values: \\sum_{i=1}^{5} x_i = x_1 + x_2 + x_3 + x_4 + x_5 = 5 + 8 + 12 + 15 + 20 = 60\nThis sum is a key element when calculating the arithmetic mean.\n\n\n\nSum of Deviations from the Mean\n\nFor the same dataset (5, 8, 12, 15, 20), the mean is \\bar{x} = 60/5 = 12\nSum of deviations from the mean: \\sum_{i=1}^{5} (x_i - \\bar{x}) = (5-12) + (8-12) + (12-12) + (15-12) + (20-12) = -7 + (-4) + 0 + 3 + 8 = 0\nImportant observation: The sum of deviations from the mean always equals 0, which is a fundamental property of the arithmetic mean.\n\n\n\n\nSummary\n\nSigma Notation (Σ) allows for concise expression of key statistical formulas\nThe most important applications include calculating:\n\nArithmetic mean\nVariance and standard deviation\nVarious sums of squares used in regression analysis\n\n\n\n\n\n\n\n\nSummation (Σ) and Product (Π) Operators\n\n\n\n\nSigma (Σ) Operator\n\\sum is a summation operator that instructs us to add terms:\n\\sum_{i=1}^{n} x_i = x_1 + x_2 + ... + x_n\nwhere: - i is the index variable - The lower value under Σ (here i=1) is the starting point - The upper value (here n) is the ending point\n\n\nPi (Π) Operator\n\\prod is a product operator that instructs us to multiply terms:\n\\prod_{i=1}^{n} x_i = x_1 \\times x_2 \\times ... \\times x_n\nwhere: - i is the index variable - The lower value under Π (here i=1) is the starting point - The upper value (here n) is the ending point\n\n\n\n\n\n\n\n\n\nExample of Σ\n\n\n\n\\sum_{i=1}^{4} i = 1 + 2 + 3 + 4 = 10\n\n\n\n\n\n\n\n\nExample of Π\n\n\n\n\\prod_{i=1}^{4} i = 1 \\times 2 \\times 3 \\times 4 = 24\n\n\n\n\n\n\n\n\nKey Differences\n\n\n\n\nΣ represents repeated addition\nΠ represents repeated multiplication",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#types-of-data-distributions",
    "href": "chapter5.html#types-of-data-distributions",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.2 Types of Data Distributions",
    "text": "5.2 Types of Data Distributions\n\n\n\n\n\n\nImportant\n\n\n\nData distribution informs what values a variable takes and how often.\n\n\nUnderstanding data distributions is crucial for data analysis and visualization. In this document, we’ll explore various types of distributions and how to visualize them using ggplot2 in R.\n\nNormal Distribution\nThe normal distribution, also known as the Gaussian distribution, is symmetric and bell-shaped.\n\n# Generate normal distribution data\nnormal_data &lt;- data.frame(x = rnorm(1000))\n\n# Plot\nggplot(normal_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Normal Distribution\", x = \"Value\", y = \"Density\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\nUniform Distribution\nIn a uniform distribution, all values have an equal probability of occurrence.\n\n# Generate uniform distribution data\nuniform_data &lt;- data.frame(x = runif(1000))\n\n# Plot\nggplot(uniform_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Uniform Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\nSkewed Distributions\nSkewed distributions are asymmetric, with one tail longer than the other.\n\n# Generate right-skewed data\nright_skewed &lt;- data.frame(x = rlnorm(1000))\n\n# Plot\nggplot(right_skewed, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightyellow\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Right-Skewed Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\nBimodal Distribution\nA bimodal distribution has two peaks, indicating two distinct subgroups in the data.\n\n# Generate bimodal data\nbimodal_data &lt;- data.frame(x = c(rnorm(500, mean = -2), rnorm(500, mean = 2)))\n\n# Plot\nggplot(bimodal_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightpink\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Bimodal Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nKey Properties\nExamples\n\n\n\n\nSymmetric (Normal)\nSymmetric, bell-shaped, most values close to the mean\nAdult height in population, IQ test scores, measurement errors, standardized exam results\n\n\nUniform\nEqual probability across the entire range\nLast digit of phone numbers, random day of the week selection, position of pointer after spinning a wheel of fortune\n\n\nBimodal\nTwo distinct peaks, suggests presence of subgroups\nAge structure in university towns (students and permanent residents), opinions on strongly polarizing topics, traffic intensity hours (morning and afternoon peak)\n\n\nRight-skewed (Positively skewed)\nExtended “tail” on the right side, most values less than the mean\nQueue waiting time, commute time to work, age at first marriage\n\n\nHeavy-tailed skewed (Log-normal)\nStrong right asymmetry, values cannot be negative, long “fat tail”\nPersonal income, housing prices, household size\n\n\nExtreme-tailed skewed (Power law)\nExtreme asymmetry, “rich get richer” effect, no characteristic scale\nWealth of the richest individuals, city populations, number of followers on social media, number of citations of scientific publications",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#visualizing-real-world-data-distributions",
    "href": "chapter5.html#visualizing-real-world-data-distributions",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.3 Visualizing Real-World Data Distributions",
    "text": "5.3 Visualizing Real-World Data Distributions\nLet’s use the palmerpenguins dataset to explore data distributions.\n\nHistogram and Density Plot\n\n\n\n\n\n\nUnderstanding Histograms and Density\n\n\n\n⭐ A histogram is a special graph for numerical data where:\n\nData is grouped into ranges (called “bins”)\nBars touch each other (unlike bar charts!) because the data is continuous\nEach bar’s height shows how many values fall into that range\n\nThink of density as showing how common or concentrated certain values are in your data:\n\nA higher point on a density curve (or taller bar in a histogram) means those values appear more frequently in your data\nA lower point means those values are less common\n\nJust like a crowded area has more people per space (higher density), a taller part of the graph shows values that appear more often in your dataset!\n\n\n\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Distribution of Penguin Flipper Lengths\", \n       x = \"Flipper Length (mm)\", \n       y = \"Density\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\nBox Plot\nBox plots are useful for comparing distributions across categories.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Distribution of Penguin Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nViolin Plot\nViolin plots combine box plot and density plot features.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Distribution of Penguin Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nRidgeline Plot\nRidgeline plots are useful for comparing multiple distributions.\n\nlibrary(ggridges)\n\nggplot(penguins, aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.6) +\n  labs(title = \"Distribution of Flipper Length by Penguin Species\",\n       x = \"Flipper Length (mm)\",\n       y = \"Species\")\n\nPicking joint bandwidth of 2.38\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nUnderstanding and visualizing data distributions is crucial in data analysis. ggplot2 provides a flexible and powerful toolkit for creating various types of distribution plots. By exploring different visualization techniques, we can gain insights into the underlying patterns and characteristics of our data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-outliers",
    "href": "chapter5.html#understanding-outliers",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.4 Understanding Outliers",
    "text": "5.4 Understanding Outliers\nBefore diving into specific measures, it’s crucial to understand the concept of outliers, as they can significantly impact many descriptive statistics.\nOutliers are data points that differ significantly from other observations in the dataset. They can occur due to:\n\nMeasurement or recording errors\nGenuine extreme values in the population\n\nOutliers can have a substantial effect on many statistical measures, especially those based on means or sums of squared deviations. Therefore, it’s essential to:\n\nIdentify outliers through both statistical methods and domain knowledge\nInvestigate the cause of outliers\nMake informed decisions about whether to include or exclude them in analyses\n\nThroughout this guide, we’ll discuss how different descriptive measures are affected by outliers.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#statistical-symbols-and-notations---summary",
    "href": "chapter5.html#statistical-symbols-and-notations---summary",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.5 Statistical Symbols and Notations - Summary",
    "text": "5.5 Statistical Symbols and Notations - Summary\n\n\n\n\n\n\n\n\n\n\nMeasure\nPopulation Parameter\nSample Statistic\nAlternative Notations\nUsage Notes\n\n\n\n\nSize\nN\nn\n-\nTotal count of observations\n\n\nMean\n\\mu\n\\bar{x}, m\nM, E(X)\nE(X) used in probability theory\n\n\nVariance\n\\sigma^2\ns^2\n\\text{Var}(X), V(X)\nSquared deviations from mean\n\n\nStandard Deviation\n\\sigma\ns\n\\text{SD}, \\text{std}\nSquare root of variance\n\n\nProportion\n\\pi, P\n\\hat{p}\n\\text{prop}\nRelative frequencies\n\n\nCorrelation\n\\rho\nr\n\\text{corr}(x,y)\nRanges from -1 to +1\n\n\nStandard Error\n\\sigma_{\\bar{x}}\ns_{\\bar{x}}\n\\text{SE}\nStandard error of mean\n\n\nSum\n\\sum\n\\sum\n\\sum_{i=1}^n\nWith indexing\n\n\nIndividual Value\nX_i\nx_i\n-\nith observation\n\n\nCovariance\n\\sigma_{xy}\ns_{xy}\n\\text{Cov}(X,Y)\nJoint variation\n\n\nMedian\n\\eta\n\\text{Med}\nM\nCentral value\n\n\nRange\nR\nr\n\\text{max}(X) - \\text{min}(X)\nSpread measure\n\n\nMode\n\\text{Mo}\n\\text{mo}\n\\text{mod}\nMost frequent value\n\n\nSkewness\n\\gamma_1\ng_1\n\\text{SK}\nDistribution asymmetry\n\n\nKurtosis\n\\gamma_2\ng_2\n\\text{KU}\nDistribution peakedness\n\n\n\nAdditional useful notations:\n\nSample moments: m_k = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^k\nPopulation moments: \\mu_k = E[(X - \\mu)^k]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-central-tendency",
    "href": "chapter5.html#measures-of-central-tendency",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.6 Measures of Central Tendency",
    "text": "5.6 Measures of Central Tendency\nMeasures of central tendency aim to identify the “typical” or “central” value in a dataset. The three primary measures are mean, median, and mode.\n\nArithmetic Mean\nThe arithmetic mean is the sum of all values divided by the number of values.\nFormula: \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\nImportant Property: The mean is a balancing point in the data. The sum of deviations from the mean is always zero:\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0\nThis property makes the mean useful in many statistical calculations.\n\n\n\n\n\n\nUnderstanding Mean as a Balance Point 🎯\n\n\n\nLet’s consider a dataset X = \\{1, 2, 6, 7, 9\\} on a number line, imagining it as a seesaw:\n\n\n\nhttps://www.gastonsanchez.com/matrix4sl/mean-as-a-balancing-point.html\n\n\nThe mean (\\mu) acts as the perfect balance point of this seesaw. For our data:\n\\mu = \\frac{1 + 2 + 6 + 7 + 9}{5} = 5\n\nWhat happens at different support points? 🤔\n\nSupport point at 6 (too high):\n\nLeft side: Values (1, 2) are below\nRight side: Values (7, 9) are above\n\\sum distances from left = (6-1) + (6-2) = 9\n\\sum distances from right = (7-6) + (9-6) = 4\nThe seesaw tilts left! ⬅️ because 9 &gt; 4\n\nSupport point at 4 (too low):\n\nLeft side: Values (1, 2) are below\nRight side: Values (6, 7, 9) are above\n\\sum distances from left = (4-1) + (4-2) = 5\n\\sum distances from right = (6-4) + (7-4) + (9-4) = 10\nThe seesaw tilts right! ➡️ because 5 &lt; 10\n\nSupport point at mean (5) (perfect balance):\n\n\\sum distances below = \\sum distances above\n((5-1) + (5-2)) = ((6-5) + (7-5) + (9-5))\n7 = 7 ✨ Perfect balance!\n\n\nThis shows why the mean is the unique balance point, where:\n\\sum_{i=1}^n (x_i - \\mu) = 0\nThe seesaw will always tilt unless the support point is placed exactly at the mean! 🎪\n\n\n\n\n\n\n\n\n\nMean as a Balance Point\n\n\n\nThis visualization shows how the arithmetic mean (5) acts as a balance point between clustered points on the left and dispersed points on the right:\nLeft side of the mean: - Points with values 2 and 3 - Close together (difference of 1 unit) - Distances from mean: 3 and 2 units - Sum of “pull” = 5 units\nRight side of the mean: - Points with values 6 and 9 - More spread out (difference of 3 units) - Distances from mean: 1 and 4 units - Sum of “pull” = 5 units\nKey observations:\n\nThe mean (5) is a balance point, even though:\n\nPoints on the left are clustered (2,3)\nPoints on the right are dispersed (6,9)\nGreen arrows show distances from the mean\n\nBalance is maintained because:\n\nSum of distances balances out: (5-2) + (5-3) = (6-5) + (9-5)\nTotal sum of distances = 5 units on each side\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual Calculation Example:\nLet’s calculate the mean for the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nSum all values\n2 + 4 + 4 + 5 + 5 + 7 + 9 = 36\n\n\n2\nCount the number of values\nn = 7\n\n\n3\nDivide the sum by n\n36 / 7 = 5.14\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(data)\n\n[1] 5.142857\n\n\nPros:\n\nEasy to calculate and understand\nUses all data points\nUseful for further statistical calculations\n\nCons:\n\nSensitive to outliers\nNot ideal for skewed distributions\n\nExample with outlier:\n\ndata_with_outlier &lt;- c(2, 4, 4, 5, 5, 7, 100)\nmean(data_with_outlier)\n\n[1] 18.14286\n\n\nAs we can see, the outlier (100) drastically affects the mean.\n\n\nMedian\nThe median is the middle value when the data is ordered.\nManual Calculation Example:\nUsing the same dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nResult\n\n\n\n\n1\nOrder the data\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nFind the middle value\n5\n\n\n\nFor even number of values, take the average of the two middle values.\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmedian(data)\n\n[1] 5\n\nmedian(data_with_outlier)\n\n[1] 5\n\n\nPros:\n\nNot affected by extreme outliers\nBetter for skewed distributions\n\nCons:\n\nDoesn’t use all data points\nLess useful for further statistical calculations\n\n\n\n\n\n\n\nWarning\n\n\n\nTo find the position of the median in a dataset:\n\nFirst sort the data in ascending order\nIf n is odd:\n\nMedian position = \\frac{n + 1}{2}\n\nIf n is even:\n\nFirst median position = \\frac{n}{2}\nSecond median position = \\frac{n}{2} + 1\nMedian = \\frac{\\text{value at }\\frac{n}{2} + \\text{value at }(\\frac{n}{2}+1)}{2}\n\n\nFor example:\n\nOdd n=7: position = \\frac{7+1}{2} = 4th value\nEven n=8: positions = \\frac{8}{2} = 4th and 4+1 = 5th value\n\n\n\n\n\nMode\nThe mode is the most frequently occurring value.\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nValue\nFrequency\n\n\n\n\n2\n1\n\n\n4\n2\n\n\n5\n2\n\n\n7\n1\n\n\n9\n1\n\n\n\nThe mode is 4 and 5 (bimodal).\nR calculation:\n\nlibrary(modeest)\nmfv(data)  # Most frequent value\n\n[1] 4 5\n\n\nPros:\n\nOnly measure of central tendency for nominal data\nCan identify multiple peaks in the data\n\nCons:\n\nNot always uniquely defined\nNot useful for continuous data\n\n\n\nWeighted (arithmetic) Mean (*)\nThe weighted mean is used when some data points are more important than others. There are two types of weighted means: with not normalized weights and with normalized weights.\n\nWeighted Mean with Not Normalized Weights\nThis is the standard form of the weighted mean, where weights can be any positive numbers representing the importance of each data point.\nFormula: \\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\nManual Calculation Example:\nLet’s calculate the weighted mean for the dataset: 2, 4, 5, 7 with weights 1, 2, 3, 1\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nMultiply each value by its weight\n(2 * 1) + (4 * 2) + (5 * 3) + (7 * 1) = 2 + 8 + 15 + 7 = 32\n\n\n2\nSum the weights\n1 + 2 + 3 + 1 = 7\n\n\n3\nDivide the result from step 1 by the result from step 2\n32 / 7 = 4.57\n\n\n\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\nWeighted Mean with Normalized Weights (Fractions)\nIn this case, the weights are fractions that sum to 1, representing the proportion of importance for each data point.\nFormula: \\bar{x}_w = \\sum_{i=1}^n w_i x_i, where \\sum_{i=1}^n w_i = 1\nManual Calculation Example:\nLet’s calculate the weighted mean for the dataset: 2, 4, 5, 7 with normalized weights 0.1, 0.3, 0.4, 0.2\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nMultiply each value by its weight\n(2 * 0.1) + (4 * 0.3) + (5 * 0.4) + (7 * 0.2)\n\n\n2\nSum the results\n0.2 + 1.2 + 2.0 + 1.4 = 4.8\n\n\n\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Note: these sum to 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nPros of Weighted Means:\n\nAccount for varying importance of data points\nUseful in survey analysis with different sample sizes or importance levels\nCan adjust for unequal probabilities in sampling designs\n\nCons of Weighted Means:\n\nRequire justification for weights\nCan be misused to manipulate results\nMay be less intuitive to interpret than simple arithmetic mean",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-variability",
    "href": "chapter5.html#measures-of-variability",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.7 Measures of Variability",
    "text": "5.7 Measures of Variability\nThese measures describe how spread out the data is. They are crucial for understanding the dispersion of data points around the central tendency.\n\n\n\n\n\n\nUnderstanding Variance\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.1: Three dot plots showing increasing variance with constant mean\n\n\n\n\n\nThe three dot plots above demonstrate how variance measures the spread of data around a central value:\n\nAll distributions have the same mean (μ = 10), shown by the dashed line\nLow Variance (σ² = 1): Points cluster tightly around the mean\nMedium Variance (σ² = 4): Points show moderate spread\nHigh Variance (σ² = 9): Points spread widely around the mean\n\n\n\n\n\n\n\n\n\nUnderstanding Different Levels of Variability\n\n\n\n\n\n\n\n\n\n\n\n\nThis visualization shows three normal distributions with the same mean (μ = 10) but different levels of variability:\n\nLow Variability (σ = 0.5)\n\nData points cluster tightly around the mean\nThe density curve is tall and narrow\nMost observations fall within ±0.5 units of the mean\n\nMedium Variability (σ = 2.0)\n\nData points spread out more from the mean\nThe density curve is lower and wider\nMost observations fall within ±2 units of the mean\n\nHigh Variability (σ = 4.0)\n\nData points spread widely from the mean\nThe density curve is much flatter and wider\nMost observations fall within ±4 units of the mean\n\n\n\n\n\nRange\nThe range is the difference between the maximum and minimum values.\nFormula: R = x_{max} - x_{min}\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nFind the maximum value\n9\n\n\n2\nFind the minimum value\n2\n\n\n3\nSubtract minimum from maximum\n9 - 2 = 7\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nrange(data)\n\n[1] 2 9\n\nmax(data) - min(data)\n\n[1] 7\n\n\nPros:\n\nSimple to calculate and understand\nGives an immediate sense of data spread\n\nCons:\n\nExtremely sensitive to outliers\nDoesn’t provide information about the distribution between extremes\n\n\n\nInterquartile Range (IQR)\nThe IQR is the difference between the 75th and 25th percentiles.\nFormula: IQR = Q_3 - Q_1\nTo find quartiles manually:\n\nFor odd number of values:\n\nQ2 (median) is the middle value\nQ1 is the median of the lower half (excluding the median of all observations)\nQ3 is the median of the upper half (excluding the median of all observations)\n\nFor even number of values:\n\nQ2 is the average of the two middle values\nQ1 is the median of the lower half (excluding the median of all observations)\nQ3 is the median of the upper half (excluding the median of all observations)\n\n\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nOrder the data\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nFind Q2 (median)\n5\n\n\n3\nFind Q1 (median of lower half)\n4\n\n\n4\nFind Q3 (median of upper half)\n7\n\n\n5\nCalculate IQR\nQ3 - Q1 = 7 - 4 = 3\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nprint(data)\n\n[1] 2 4 4 5 5 7 9\n\nquantile(data, type = 1)\n\n  0%  25%  50%  75% 100% \n   2    4    5    7    9 \n\nIQR(data, type = 1)\n\n[1] 3\n\n\nPros:\n\nRobust to outliers\nProvides information about the spread of the middle 50% of the data\n\nCons:\n\nIgnores the tails of the distribution\nLess efficient than standard deviation for normal distributions\n\n\n\nVariance\nVariance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n\n\n\n\n\n\nVariance: Understanding Average Squared Deviations\n\n\n\nWhat is Variance? Variance measures how “spread out” numbers are from their mean - it’s the average of squared deviations from the mean.\nFormula: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nSimple Example: Consider numbers: 2, 4, 6, 8, 10 Mean (\\bar{x}) = 6\n\n\n\n\n\n\n\n\n\nCalculating Deviations:\n\n\n\n\n\n\n\n\n\n\n\n\nValue\nDeviation from mean\nSquare of deviation\n\n\n\n\n2\n-4\n16\n\n\n4\n-2\n4\n\n\n6\n0\n0\n\n\n8\n+2\n4\n\n\n10\n+4\n16\n\n\n\nVariance = \\frac{16 + 4 + 0 + 4 + 16}{4} = 10\nKey Points:\n\nMean acts as a reference line (blue dashed line)\nDeviations show distance from mean (red dotted lines)\nSquaring makes all deviations positive (blue bars)\nLarger deviations contribute more to variance\n\n\n\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the mean\n\\bar{x} = 5.14\n\n\n2\nSubtract the mean from each value and square the result\n(2 - 5.14)^2 = 9.86\n\n\n\n\n(4 - 5.14)^2 = 1.30\n\n\n\n\n(4 - 5.14)^2 = 1.30\n\n\n\n\n(5 - 5.14)^2 = 0.02\n\n\n\n\n(5 - 5.14)^2 = 0.02\n\n\n\n\n(7 - 5.14)^2 = 3.46\n\n\n\n\n(9 - 5.14)^2 = 14.90\n\n\n3\nSum the squared differences\n30.86\n\n\n4\nDivide by (n-1), i.e. by the number of observations - 1\n30.86 / 6 = 5.14\n\n\n\nR calculation:\n\nvar(data)\n\n[1] 5.142857\n\n\nPros:\n\nUses all data points\nFoundation for many statistical tests\n\nCons:\n\nUnits are squared, making interpretation less intuitive\nSensitive to outliers\n\n\n\n\n\n\n\nBessel’s Correction: Why We Divide by (n-1) And Not by n\n\n\n\nThe Key Insight:\nWhen we calculate deviations from the mean, they must sum to zero. This is a mathematical fact: \\sum(x_i - \\bar{x}) = 0\nThink of it Like This:\nIf you have 5 numbers and their mean:\n\nOnce you calculate 4 deviations from the mean\nThe 5th deviation MUST be whatever makes the sum zero\nYou don’t really have 5 independent deviations\nYou only have 4 truly “free” deviations\n\nSimple Example:\nNumbers: 2, 4, 6, 8, 10\n\nMean = 6\nDeviations: -4, -2, 0, +2, +4\nNotice they sum to zero\nIf you know any 4 deviations, the 5th is predetermined!\n\nThis is Why:\n\nWhen calculating variance: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nWe divide by (n-1) not n\nBecause only (n-1) deviations are truly independent\nThe last one is determined by the others\n\nDegrees of Freedom:\n\nn = number of observations\n1 = constraint (deviations must sum to zero)\nn-1 = degrees of freedom = number of truly independent deviations\n\nWhen to Use It:\n\nWhen calculating sample variance\nWhen calculating sample standard deviation\n\nWhen NOT to Use It:\n\nPopulation calculations (when you have all data)\n\nRemember:\n\nIt’s not just a statistical trick\nDeviations from the mean must sum to zero\nThis constraint costs us one degree of freedom\n\n\n\n\n\nStandard Deviation\nThe standard deviation is the square root of the variance and measures the average dispersion of the data about their arithmetic mean. In contrast to the variance, it has the advantage of being expressed in the same units as the original measurements, making its interpretation more intuitive.\nFormula: s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the variance\ns^2 = 5.14 (from previous calculation)\n\n\n2\nTake the square root\ns = \\sqrt{5.14} = 2.27\n\n\n\nR calculation:\n\nsd(data)\n\n[1] 2.267787\n\n\nPros:\n\nIn same units as original data\nWidely used and understood\n\nCons:\n\nStill sensitive to outliers\nAssumes data is roughly “normally” distributed\n\n\n\nCoefficient of Variation (*)\nThe coefficient of variation is the standard deviation divided by the mean, often expressed as a percentage.\nFormula: CV = \\frac{s}{\\bar{x}} \\times 100\\%\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the mean\n\\bar{x} = 5.14\n\n\n2\nCalculate the standard deviation\ns = 2.27\n\n\n3\nDivide s by the mean and multiply by 100\n(2.27 / 5.14) * 100 = 44.16\\%\n\n\n\nR calculation:\n\n(sd(data) / mean(data)) * 100\n\n[1] 44.09586\n\n\nPros:\n\nAllows comparison of variability between datasets with different units or means\nUseful in fields like finance for risk assessment\n\nCons:\n\nNot meaningful for data with both positive and negative values\nCan be misleading when mean is close to zero\n\n\n\n\n\n\n\nLimitations of Coefficient of Variation (CV)\n\n\n\nThe coefficient of variation, calculated as (σ/μ) × 100\\%, has two important limitations:\n\nNot meaningful for data with both positive and negative values\n\nThe mean could be close to zero due to positive and negative values cancelling out\nExample: Dataset {-5, -3, 2, 6} has mean = 0\n\nCV = (std dev / 0) × 100%\nThis leads to division by zero\nEven if mean isn’t exactly zero, the CV doesn’t represent true relative variability when data cross zero\n\nThe CV assumes a natural zero point and meaningful ratios between values\n\n\n\nMisleading when mean is close to zero\n\nSince CV = (σ/μ) × 100\\%, as μ approaches zero:\n\nThe denominator becomes very small\nResults in extremely large CV values\nThese large values don’t meaningfully represent relative variability\n\nExample:\n\nDataset A: {0.001, 0.002, 0.003} has mean = 0.002\nEven small standard deviations will produce very large CVs\nThe resulting large CV might suggest extreme variability when the data are actually quite close together\n\n\n\n\nBest Use Cases\nCV is most useful for:\n\nStrictly positive data\nData measured on a ratio scale\nData with means well above zero\nComparing variability between datasets with different units or scales",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-relative-position-standing",
    "href": "chapter5.html#measures-of-relative-position-standing",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.8 Measures of Relative Position (Standing)",
    "text": "5.8 Measures of Relative Position (Standing)\nUnderstanding where values sit within a dataset is crucial for data analysis. Let’s explore these concepts step by step.\n\nQuartiles (Q): The Basics\nThink of quartiles as special numbers that split your ordered data into four equal parts.\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\n\nWhat Are Quartiles?\nFirst Quartile (Q1):\n\nSeparates the lowest 25% of data from the rest\nAlso called the 25th percentile\nExample: If Q1 = 50 in a test score dataset, 25% of students scored below 50\n\nSecond Quartile (Q2):\n\nThe median - splits data in half\nAlso called the 50th percentile\nExample: If Q2 = 70, half the students scored below 70\n\nThird Quartile (Q3):\n\nSeparates the highest 25% of data from the rest\nAlso called the 75th percentile\nExample: If Q3 = 85, 75% of students scored below 85\n\n\n\nHow to Calculate Quartiles (Step by Step) - Two Methods\nLet’s examine student test scores using both common quartile calculation methods:\nExample 1: Odd Number Case (11 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 88, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 11 values (odd)\nMedian position = (n + 1)/2 = 6\nQ2 = 78\n\nStep 2: Find Q1\n\nTukey’s Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (n + 1)/4 = (11 + 1)/4 = 3\nQ1 = 70 (3rd value)\n\n\nStep 3: Find Q3\n\nTukey’s Method:\n\nLook at upper half: 80, 82, 85, 88, 90\nQ3 = median of upper half = 85\n\nInterpolation Method:\n\nPosition = 3(n + 1)/4 = 3(12)/4 = 9\nQ3 = 85 (9th value)\n\n\nExample 2: Even Number Case (10 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 10 values (even)\nMedian positions = 5 and 6\nQ2 = (75 + 78)/2 = 76.5\n\nStep 2: Find Q1\n\nTukey’s Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (10 + 1)/4 = 2.75\nQ1 = 65 + 0.75(70 - 65) = 68.75\n\n\nStep 3: Find Q3\n\nTukey’s Method:\n\nLook at upper half: 78, 80, 82, 85, 90\nQ3 = median of upper half = 82\n\nInterpolation Method:\n\nPosition = 3(10 + 1)/4 = 8.25\nQ3 = 82 + 0.25(85 - 82) = 82.75\n\n\nImportant Notes:\n\nTukey’s Method:\n\nFirst find the median (Q2)\nSplit the data into lower and upper halves\nFind Q1 as the median of the lower half\nFind Q3 as the median of the upper half\nWhen n is odd, the median is not included in either half\n\nInterpolation Method:\n\nUses positions (n+1)/4 for Q1 and 3(n+1)/4 for Q3\nWhen position falls between values, uses linear interpolation\nDoesn’t require splitting data into halves\n\n\nBoth methods give the same results for simple positions (Example 1) but can differ when interpolation is needed (Example 2).\n\n\n\n\n\n\nManual Construction of Tukey Boxplot\n\n\n\nStep 1: Calculate Key Components\n\nFind quartiles: Q_1, Q_2 (median), Q_3\nCalculate Interquartile Range: IQR = Q_3 - Q_1\n\nStep 2: Determine Whisker Boundaries\n\nLower fence: Q_1 - 1.5 \\times IQR\nUpper fence: Q_3 + 1.5 \\times IQR\n\nStep 3: Identify Outliers Data points are outliers if they are:\n\nBelow lower fence: x &lt; Q_1 - 1.5 \\times IQR\nAbove upper fence: x &gt; Q_3 + 1.5 \\times IQR\n\nExample: Given data: 2, 4, 6, 8, 9, 10, 11, 12, 14, 16, 50\n\nFind quartiles:\n\nQ_1 = 6\nQ_2 = 10\nQ_3 = 14\n\nCalculate IQR:\n\nIQR = 14 - 6 = 8\n\nCalculate fences:\n\nLower: 6 - (1.5 \\times 8) = -6\nUpper: 14 + (1.5 \\times 8) = 26\n\nIdentify outliers:\n\n50 &gt; 26, therefore 50 is an outlier\n\n\nGraphical Elements:\n\nBox: Draw from Q_1 to Q_3\nLine inside box: Draw at Q_2\nWhiskers: Extend to most extreme non-outlier points\nPoints: Plot outliers individually beyond whiskers\n\n\n\n\n\n\nPercentiles: A More Precise Measure of Relative Standing (*)\n\nWhat Are Percentiles?\nPercentiles give us a more detailed view by dividing data into 100 equal parts.\nKey Points:\n\nThe 25th percentile equals Q1\nThe 50th percentile equals Q2 (median)\nThe 75th percentile equals Q3\n\n\n\nCalculating Percentiles\nThe Formula: P_k = \\frac{k(n+1)}{100}\nWhere:\n\nP_k is the position for the kth percentile\nk is the percentile we want (1-100)\nn is the number of observations\n\nExample 3: Finding the 60th Percentile Let’s use student homework scores: 72, 75, 78, 80, 82, 85, 88, 90, 92, 95\nStep 1: Calculate position\n\nn = 10 scores\nFor 60th percentile: P_{60} = \\frac{60(10+1)}{100} = 6.6\n\nStep 2: Find surrounding values\n\nPosition 6: score of 85\nPosition 7: score of 88\n\nStep 3: Interpolate (important: percentiles use linear interpolation)\n\nWe need to go 0.6 of the way between 85 and 88 P_{60} = 85 + 0.6(88-85) P_{60} = 85 + 0.6(3) P_{60} = 85 + 1.8 = 86.8\n\nWhat this means: 60% of students scored 86.8 or below.\n\n\n\nPercentile Ranks (PR) (*)\n\nWhat is a Percentile Rank?\nWhile percentiles tell us the value at a certain position, percentile rank tells us what percentage of values fall below a specific score. Think of it as answering the question “What percentage of the class did I score higher than?”\nPR = \\frac{\\text{number of values below } + 0.5 \\times \\text{number of equal values}}{\\text{total number of values}} \\times 100\nExample 4: Finding a Percentile Rank Consider these exam scores:\n65, 70, 70, 75, 75, 75, 80, 85, 85, 90\nLet’s find the PR for a score of 75.\nStep 1: Count carefully\n\nValues below 75: 65, 70, 70 (3 values)\nValues equal to 75: 75, 75, 75 (3 values)\nTotal values: 10\n\nStep 2: Apply the formula\nPR = \\frac{3 + 0.5(3)}{10} \\times 100 PR = \\frac{3 + 1.5}{10} \\times 100 PR = \\frac{4.5}{10} \\times 100 = 45\\%\nInterpretation: A score of 75 is higher than 45% of the class scores.\nRemark:\nQ1: “Why do we use 0.5 for equal values in PR?”\nA1: This is because we’re assuming people with the same score are evenly spread across that position. It’s like saying they share the position equally.\n\n\n\nUnderstanding and Interpreting Box Plots\nBox plots (also known as box-and-whisker plots) are powerful visualization tools for understanding data distributions. In this section, we’ll explore how to construct and interpret box plots using height measurements from two groups.\n\nConstruction of the Tukey Box Plot\nThe box plot was introduced by John Tukey as part of his exploratory data analysis toolkit. It provides a standardized way of displaying the distribution of data based on a five-number summary.\n\nThe Five-Number Summary\nA box plot represents five key statistical values:\n\nMinimum: The smallest value in the dataset (excluding outliers)\nFirst Quartile (Q1): The 25th percentile, below which 25% of observations fall\nMedian (Q2): The 50th percentile, which divides the dataset into two equal halves\nThird Quartile (Q3): The 75th percentile, below which 75% of observations fall\nMaximum: The largest value in the dataset (excluding outliers)\n\n\n\nBox Plot Components\n\n\n\n\n\n\n\n\nFigure 5.2: Boxplot diagram showing its key components.\n\n\n\n\n\nThe components of a box plot include:\n\nThe Box:\n\nRepresents the interquartile range (IQR), containing the middle 50% of the data\nLower edge represents Q1\nUpper edge represents Q3\nLine inside the box represents the median (Q2)\n\nThe Whiskers:\n\nExtend from the box to show the range of non-outlier data\nIn a Tukey box plot, whiskers extend up to 1.5 × IQR from the box edges:\n\nLower whisker: extends to the minimum value ≥ (Q1 - 1.5 × IQR)\nUpper whisker: extends to the maximum value ≤ (Q3 + 1.5 × IQR)\n\n\nOutliers:\n\nPoints that fall beyond the whiskers\nIndividually plotted as dots or symbols\nValues that are &lt; (Q1 - 1.5 × IQR) or &gt; (Q3 + 1.5 × IQR)\n\n\n\n\nKey Features to Observe\nWhen interpreting box plots, look for these characteristics:\n\nCentral Tendency: Location of the median line within the box\nDispersion: Width of the box (IQR) and length of the whiskers\nSkewness:\n\nSymmetrical data: median is approximately in the middle of the box, whiskers are roughly equal in length\nRight (positive) skew: median is closer to the bottom of the box, upper whisker is longer\nLeft (negative) skew: median is closer to the top of the box, lower whisker is longer\n\nOutliers: Presence of individual points beyond the whiskers\n\n\n\n\nCase Study: Comparing Heights Between Groups\nLet’s apply our understanding of box plots to a real dataset. We have height measurements (in centimeters) from two groups of 25 students each.\n\n# Create the dataset\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Transform dataset from wide to long format\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Display the first few rows\nhead(data_height_l)\n\n  Group_number height\n1      group_1    150\n2      group_1    160\n3      group_1    165\n4      group_1    168\n5      group_1    172\n6      group_1    173\n\n\nLet’s calculate some summary statistics for each group:\n\n# Calculate summary statistics for each group\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Calculate IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Create a comparison table\nstats_table &lt;- rbind(\n  group1_stats,\n  group2_stats\n)\nrownames(stats_table) &lt;- c(\"Group 1\", \"Group 2\")\n\n# Display the table\nstats_table\n\n        Min. 1st Qu. Median Mean 3rd Qu. Max.\nGroup 1  150     175    180  179     183  200\nGroup 2  138     165    175  172     182  210\n\n# Display IQR values\ncat(\"IQR for Group 1:\", group1_iqr, \"\\n\")\n\nIQR for Group 1: 8 \n\ncat(\"IQR for Group 2:\", group2_iqr, \"\\n\")\n\nIQR for Group 2: 17 \n\n\n\n\nVisualizing the Height Data\nNow, let’s visualize the data using box plots and density plots:\n\n# Create horizontal boxplots\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Height Distribution by Group\",\n       x = \"Group\",\n       y = \"Height (cm)\")\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\n\n\n\n\n\n\nFigure 5.3: Box plots comparing height distributions between groups.\n\n\n\n\n\nTo complement our box plots, let’s also look at the density distributions:\n\n# Create density plots\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Height Density by Group\",\n       x = \"Height (cm)\",\n       y = \"Density\")\n\n\n\n\n\n\n\nFigure 5.4: Density plots showing the height distributions for each group.\n\n\n\n\n\n\n\nBox Plot Interpretation Exercise\nBased on the box plots and density plots above, determine whether each of the following statements is True or False. For each statement, provide a brief explanation based on evidence from the visualizations.\n\n\n\n\n\n\nExercise Questions\n\n\n\n\nStudents from group 2 (G2) in the studied sample are, on average, taller than those from group 1 (G1).\nGroup 1 (G1) height measurements are more dispersed/spread out than group 2 (G2).\nThe lowest person is in group 2 (G2).\nBoth data sets are negatively (left) skewed.\nHalf of the students in group 2 (G2) measure at least 175 cm.\n\n\n\n\nHints for Interpretation\nWhen answering these questions, consider:\n\nThe position of the median line within each box\nThe relative sizes of the boxes (IQR)\nThe positions of the minimum and maximum values\nThe symmetry of the distributions (balanced or skewed)\nThe lengths of the whiskers\n\nFor each statement, determine whether it is True or False and provide your explanation:\n\n\n\n\n\n\nAnswer Template\n\n\n\n\n\n\nStudents from G2 are, on average, taller than G1: [True/False]\n\nExplanation:\n\nG1 height is more dispersed/spread out: [True/False]\n\nExplanation:\n\nThe lowest person is in G2: [True/False]\n\nExplanation:\n\nBoth data sets are negatively (left) skewed: [True/False]\n\nExplanation:\n\nHalf of G2 measure at least 175 cm: [True/False]\n\nExplanation:\n\n\n\n\n\nLet’s review the answers to our box plot interpretation questions:\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nStudents from G2 are, on average, taller than G1: False\n\nExplanation: The median height (middle line in the boxplot) for G1 is higher than G2.\n\nG1 height is more dispersed/spread out: False\n\nExplanation: G2 shows greater dispersion. This is visible in the boxplot where G2 has a larger interquartile range (IQR) of 17.5 cm compared to G1’s 9.5 cm. G2 also has a wider range from minimum to maximum values.\n\nThe lowest person is in G2: True\n\nExplanation: The minimum value in G2 is 138 cm, which is lower than the minimum value in G1 (150 cm).\n\nBoth data sets are negatively (left) skewed: True\n\nExplanation: In both groups, the median line is positioned toward the upper part of the box, and the lower whisker is longer than the upper whisker. This indicates that there’s a longer tail on the left side of the distribution, which means negative skewness.\n\nHalf of G2 measure at least 175 cm: True\n\nExplanation: The median (middle line in the boxplot) for G2 is 175 cm, which means that 50% of the values are greater than or equal to 175 cm.\n\n\n\n\n\n\n\n\nR Code Reference\nHere’s the complete R code used in this section:\n\n# Load required packages\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n# Set display options\noptions(scipen = 999, digits = 3)\n\n# Create the dataset\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Transform dataset from wide to long format\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Display the first few rows\nhead(data_height_l)\n\n# Calculate summary statistics for each group\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Calculate IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Create horizontal boxplots\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Height Distribution by Group\",\n       x = \"Group\",\n       y = \"Height (cm)\")\n\n# Create density plots\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Height Density by Group\",\n       x = \"Height (cm)\",\n       y = \"Density\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#shape-measures",
    "href": "chapter5.html#shape-measures",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.9 Shape Measures",
    "text": "5.9 Shape Measures\n\nSkewness\n\nDefinition\nSkewness quantifies the asymmetry of a data distribution. It indicates whether data tends to cluster more on one side of the mean than the other.\n\n\nMathematical Expression\nSK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3 where: - n is the sample size - x_i is the i-th observation - \\bar{x} is the sample mean - s is the sample standard deviation\n\n\nSimplified Numerical Example\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Three example datasets with different types of skewness\n# 1. Positive skewness (right tail)\npositive_skew_data &lt;- c(2, 3, 4, 4, 5, 5, 5, 6, 6, 7, 8, 12, 15, 20)\n# 2. Negative skewness (left tail)\nnegative_skew_data &lt;- c(1, 5, 10, 13, 14, 15, 16, 16, 17, 17, 18, 18, 19, 20)\n# 3. Near-zero skewness (symmetry)\nsymmetric_data &lt;- c(1, 3, 5, 7, 9, 10, 11, 12, 13, 15, 17, 19, 21)\n\n# Calculating skewness\npositive_skewness &lt;- skewness(positive_skew_data)\nnegative_skewness &lt;- skewness(negative_skew_data)\nsymmetric_skewness &lt;- skewness(symmetric_data)\n\n# Summary of results\nskewness_data &lt;- data.frame(\n  \"Distribution Type\" = c(\"Positive skewness\", \"Negative skewness\", \"Symmetric distribution\"),\n  \"Skewness value\" = round(c(positive_skewness, negative_skewness, symmetric_skewness), 3),\n  \"Interpretation\" = c(\n    \"Longer right tail (majority of data on the left side)\",\n    \"Longer left tail (majority of data on the right side)\",\n    \"Data distributed symmetrically\"\n  )\n)\n\n# Display table\nskewness_data\n\n       Distribution.Type Skewness.value\n1      Positive skewness           1.42\n2      Negative skewness          -1.33\n3 Symmetric distribution           0.00\n                                         Interpretation\n1 Longer right tail (majority of data on the left side)\n2 Longer left tail (majority of data on the right side)\n3                        Data distributed symmetrically\n\n\n\n\nVisualizations of Skewness Types\n\n# Create a data frame for all sets\ndf_skewness &lt;- rbind(\n  data.frame(value = positive_skew_data, type = \"Positive skewness\", \n             skewness = round(positive_skewness, 2)),\n  data.frame(value = negative_skew_data, type = \"Negative skewness\", \n             skewness = round(negative_skewness, 2)),\n  data.frame(value = symmetric_data, type = \"Symmetric distribution\", \n             skewness = round(symmetric_skewness, 2))\n)\n\n# Histograms for three types of skewness\np1 &lt;- ggplot(df_skewness, aes(x = value)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~type, scales = \"free_x\") +\n  geom_vline(data = df_skewness %&gt;% group_by(type) %&gt;% summarise(mean = mean(value)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_skewness %&gt;% group_by(type) %&gt;% summarise(median = median(value)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = unique(df_skewness[, c(\"type\", \"skewness\")]),\n           aes(x = Inf, y = Inf, label = paste(\"SK =\", skewness)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histograms showing different types of skewness\",\n    subtitle = \"Red line: mean, Green line: median\",\n    x = \"Value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n# Box plots\np2 &lt;- ggplot(df_skewness, aes(x = type, y = value, fill = type)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"skyblue\", \"lightgreen\", \"lightsalmon\")) +\n  labs(\n    title = \"Box plots for different types of skewness\",\n    x = \"Distribution type\",\n    y = \"Value\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Display plots\ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nExample: Voter Turnout Analysis\n\n# Generate three datasets reflecting different types of skewness\nset.seed(123)\n\n# 1. Positive skewness - typical for turnout in regions with low engagement\npositive_turnout &lt;- c(\n  runif(50, min = 20, max = 30),  # Small group with low turnout\n  rbeta(200, shape1 = 2, shape2 = 5) * 50 + 30  # Majority of results shifted to the left\n)\n\n# 2. Negative skewness - typical for regions with high political engagement\nnegative_turnout &lt;- c(\n  rbeta(200, shape1 = 5, shape2 = 2) * 30 + 50,  # Majority of results shifted to the right\n  runif(50, min = 40, max = 50)  # Small group with lower turnout\n)\n\n# 3. Symmetric distribution - typical for regions with uniform engagement\nsymmetric_turnout &lt;- rnorm(250, mean = 65, sd = 8)\n\n# Create data frame\ndf_turnout &lt;- rbind(\n  data.frame(turnout = positive_turnout, region = \"Region A: Positive skewness\"),\n  data.frame(turnout = negative_turnout, region = \"Region B: Negative skewness\"),\n  data.frame(turnout = symmetric_turnout, region = \"Region C: Symmetric distribution\")\n)\n\n# Calculate skewness for each region\nregion_skewness &lt;- df_turnout %&gt;%\n  group_by(region) %&gt;%\n  summarise(skewness = round(skewness(turnout), 2))\n\n# Histogram of turnout by region\np3 &lt;- ggplot(df_turnout, aes(x = turnout)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~region, ncol = 1) +\n  geom_vline(data = df_turnout %&gt;% group_by(region) %&gt;% summarise(mean = mean(turnout)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_turnout %&gt;% group_by(region) %&gt;% summarise(median = median(turnout)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = region_skewness,\n           aes(x = 25, y = 20, label = paste(\"SK =\", skewness)),\n           size = 3.5) +\n  labs(\n    title = \"Voter turnout in different regions\",\n    subtitle = \"Showing three types of skewness\",\n    x = \"Voter turnout (%)\",\n    y = \"Number of districts\"\n  ) +\n  theme_minimal()\n\n# Box plot\np4 &lt;- ggplot(df_turnout, aes(x = region, y = turnout, fill = region)) +\n  geom_boxplot() +\n  labs(\n    title = \"Comparison of turnout distributions across regions\",\n    x = \"Region\",\n    y = \"Voter turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Display plots\ngrid.arrange(p3, p4, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nInterpretation Guide\n\nPositive Skewness (&gt; 0): Distribution has a longer right tail - most values are concentrated on the left side\nNegative Skewness (&lt; 0): Distribution has a longer left tail - most values are concentrated on the right side\nZero Skewness: Distribution is approximately symmetric - values are evenly distributed around the mean\n\n\n\n\nKurtosis\n\nDefinition\nKurtosis measures the “tailedness” of a distribution, indicating the presence of extreme values compared to a normal distribution.\n\n\nMathematical Expression\nK = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\n\n\nSimplified Numerical Example\n\n# Three example datasets with different levels of kurtosis\n# 1. Leptokurtic distribution (high kurtosis, \"heavy tails\")\nleptokurtic_data &lt;- c(\n  rnorm(80, mean = 50, sd = 5),  # Most data clustered around the mean\n  c(20, 25, 30, 70, 75, 80)      # A few extreme values\n)\n\n# 2. Platykurtic distribution (low kurtosis, \"flat\")\nplatykurtic_data &lt;- c(\n  runif(50, min = 30, max = 70)  # Uniform distribution of values\n)\n\n# 3. Mesokurtic distribution (normal kurtosis)\nmesokurtic_data &lt;- rnorm(50, mean = 50, sd = 10)\n\n# Calculate kurtosis\nkurtosis_lepto &lt;- kurtosis(leptokurtic_data)\nkurtosis_platy &lt;- kurtosis(platykurtic_data)\nkurtosis_meso &lt;- kurtosis(mesokurtic_data)\n\n# Summary of results\nkurtosis_data &lt;- data.frame(\n  \"Distribution Type\" = c(\"Leptokurtic\", \"Platykurtic\", \"Mesokurtic\"),\n  \"Kurtosis value\" = round(c(kurtosis_lepto, kurtosis_platy, kurtosis_meso), 3),\n  \"Interpretation\" = c(\n    \"Many values near the mean, but also more extreme values\",\n    \"Values more uniformly distributed - flat distribution\",\n    \"Similar to normal distribution\"\n  )\n)\n\n# Display table\nkurtosis_data\n\n  Distribution.Type Kurtosis.value\n1       Leptokurtic           7.39\n2       Platykurtic           1.85\n3        Mesokurtic           2.25\n                                           Interpretation\n1 Many values near the mean, but also more extreme values\n2   Values more uniformly distributed - flat distribution\n3                          Similar to normal distribution\n\n\n\n\nVisualizations of Kurtosis Levels\n\n# Create a data frame for all sets\ndf_kurtosis &lt;- rbind(\n  data.frame(value = leptokurtic_data, type = \"Leptokurtic (K &gt; 3)\", \n             kurtosis = round(kurtosis_lepto, 2)),\n  data.frame(value = platykurtic_data, type = \"Platykurtic (K &lt; 3)\", \n             kurtosis = round(kurtosis_platy, 2)),\n  data.frame(value = mesokurtic_data, type = \"Mesokurtic (K ≈ 3)\", \n             kurtosis = round(kurtosis_meso, 2))\n)\n\n# Histograms for three types of kurtosis\np5 &lt;- ggplot(df_kurtosis, aes(x = value)) +\n  geom_histogram(bins = 15, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~type, scales = \"free_y\") +\n  geom_text(data = unique(df_kurtosis[, c(\"type\", \"kurtosis\")]),\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtosis)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histograms showing different levels of kurtosis\",\n    x = \"Value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n# Box plots\np6 &lt;- ggplot(df_kurtosis, aes(x = type, y = value, fill = type)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"lightgreen\", \"lightsalmon\", \"skyblue\")) +\n  labs(\n    title = \"Box plots for different levels of kurtosis\",\n    x = \"Distribution type\",\n    y = \"Value\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Display plots\ngrid.arrange(p5, p6, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nExample: Parliamentary Voting Analysis\n\n# Generate three datasets reflecting different levels of kurtosis\nset.seed(456)\n\n# 1. Leptokurtic distribution - typical for votes with strong party discipline\nlepto_voting &lt;- c(\n  rnorm(150, mean = 75, sd = 3),  # Most votes with high agreement\n  c(20, 25, 30, 35, 40, 95, 96, 97, 98, 99)  # A few outlier votes\n)\n\n# 2. Platykurtic distribution - typical for controversial votes\nplaty_voting &lt;- c(\n  runif(80, min = 40, max = 60),  # Votes with moderate agreement\n  runif(80, min = 60, max = 80)   # Votes with higher agreement\n)\n\n# 3. Mesokurtic distribution - typical for normal votes\nmeso_voting &lt;- rnorm(160, mean = 65, sd = 10)\n\n# Create data frame\ndf_voting &lt;- rbind(\n  data.frame(agreement = lepto_voting, bill_type = \"Bills A: Leptokurtic\"),\n  data.frame(agreement = platy_voting, bill_type = \"Bills B: Platykurtic\"),\n  data.frame(agreement = meso_voting, bill_type = \"Bills C: Mesokurtic\")\n)\n\n# Calculate kurtosis for each bill type\nbill_kurtosis &lt;- df_voting %&gt;%\n  group_by(bill_type) %&gt;%\n  summarise(kurtosis = round(kurtosis(agreement), 2))\n\n# Histogram of voting agreement\np7 &lt;- ggplot(df_voting, aes(x = agreement)) +\n  geom_histogram(bins = 20, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~bill_type, ncol = 1) +\n  geom_text(data = bill_kurtosis,\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtosis)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Voting agreement for different types of bills\",\n    subtitle = \"Showing three levels of kurtosis\",\n    x = \"Voting agreement index (%)\",\n    y = \"Number of votes\"\n  ) +\n  theme_minimal()\n\n# Box plot\np8 &lt;- ggplot(df_voting, aes(x = bill_type, y = agreement, fill = bill_type)) +\n  geom_boxplot() +\n  labs(\n    title = \"Comparison of voting agreement distributions\",\n    x = \"Bill type\",\n    y = \"Voting agreement index (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Display plots\ngrid.arrange(p7, p8, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nInterpretation Guide\n\nLeptokurtic (K &gt; 3): “Slender” distribution with heavy tails - more extreme values than in a normal distribution\nPlatykurtic (K &lt; 3): “Flat” distribution - fewer extreme values than in a normal distribution\nMesokurtic (K ≈ 3): Distribution similar to normal in terms of extreme values",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "href": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.10 Exercise 1. Center and dispersion of data",
    "text": "5.10 Exercise 1. Center and dispersion of data\n\nData\nWe have salary data (in thousands of euros) from two small European companies:\n\n\n\nIndex\nCompany X\nCompany Y\n\n\n\n\n1\n2\n3\n\n\n2\n2\n3\n\n\n3\n2\n4\n\n\n4\n3\n4\n\n\n5\n3\n4\n\n\n6\n3\n4\n\n\n7\n3\n4\n\n\n8\n3\n4\n\n\n9\n3\n5\n\n\n10\n4\n5\n\n\n11\n4\n5\n\n\n12\n4\n5\n\n\n13\n4\n5\n\n\n14\n4\n5\n\n\n15\n5\n6\n\n\n16\n5\n6\n\n\n17\n5\n6\n\n\n18\n5\n7\n\n\n19\n20\n7\n\n\n20\n35\n8\n\n\n\nThis table presents the data for both Company X and Company Y side by side, with an index column for easy reference.\n\n\nMeasures of Central Tendency\n\nMean\nThe mean is the average of all values in a dataset.\nFormula: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\nMożna też zapisać ten wzór w postaci:\n\\bar{x} = \\frac{\\sum_{i=1}^{k} x_i f_i}{n}\ngdzie f_i to częstość bezwzględna (liczba wystąpień, waga bezwzględna) i-tej wartości, a k to liczba różnych wartości cechy (liczba wartości wyróżnionych).\nZ użyciem częstości względnych:\n\\bar{x} = \\sum_{i=1}^{k} x_i p_i\ngdzie p_i to częstość względna (frakcja, waga znormalizowana) i-tej wartości, a k to liczba różnych wartości cechy (liczba wartości wyróżnionych).\n\nManual Calculation for Company X\n\n\n\nValue (x_i)\nFrequency (f_i)\nx_i \\cdot f_i\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nTotal\nn = 20\nSum = 119\n\n\n\n\\bar{x} = \\frac{119}{20} = 5.95\n\n\nManual Calculation for Company Y\n\n\n\nValue (x_i)\nFrequency (f_i)\nx_i \\cdot f_i\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nTotal\nn = 20\nSum = 100\n\n\n\n\\bar{y} = \\frac{100}{20} = 5\n\n\nR Verification\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\nMedian\nThe median is the middle value when the data is ordered.\n\nManual Calculation for Company X\nOrdered data: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\frac{4 + 4}{2} = 4\n\n\nManual Calculation for Company Y\nOrdered data: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\frac{5 + 5}{2} = 5\n\n\nR Verification\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\nMode\nThe mode is the most frequent value in the dataset.\nFor Company X, the mode is 3 (appears 6 times). For Company Y, there are two modes: 4 and 5 (both appear 6 times).\n\n# Function to calculate mode\nget_mode &lt;- function(x) {\n  unique_x &lt;- unique(x)\n  unique_x[which.max(tabulate(match(x, unique_x)))]\n}\n\nget_mode(X)\n\n[1] 3\n\nget_mode(Y)\n\n[1] 4\n\n\n\n\n\nMeasures of Dispersion\n\nVariance\nThe variance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\nPoprawka Bessela jest stosowana przy obliczaniu wariancji z próby, aby uzyskać nieobciążony estymator wariancji populacji. W standardowym wzorze na wariancję z próby dzielimy przez (n-1) zamiast przez n.\nModyfikacje wzoru dla danych pogrupowanych (szereg częstości):\nMożna też zapisać ten wzór w postaci:\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{k} f_i(x_i - \\bar{x})^2\ngdzie f_i to częstość bezwzględna (liczba wystąpień) i-tej wartości.\nGdy w obliczeniach stosujemy częstości względne p = f_i/n, gdzie:\n\nf_i to częstość (liczba wystąpień)\nn to całkowita liczebność próby\n\nWzór na wariancję z uwzględnieniem poprawki Bessela przyjmuje postać:\ns^2 = \\frac{n}{n-1} \\sum_{i=1}^{k} p_i(x_i - \\bar{x})^2\ngdzie:\n\ns^2 to wariancja z próby\nn to liczebność próby\np_i to częstość względna i-tej wartości\nx_i to i-ta wartość cechy\n\\bar{x} to średnia arytmetyczna\nk to liczba różnych wartości cechy\n\nKluczowe jest to, że przy stosowaniu częstości względnych mnożymy całe wyrażenie przez czynnik \\frac{n}{n-1}, który wprowadza poprawkę Bessela.\n\nManual Calculation for Company X\n\n\n\n\n\n\n\n\n\n\nx_i\nf_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\nf_i(x_i - \\bar{x})^2\n\n\n\n\n2\n3\n-3.95\n15.6025\n46.8075\n\n\n3\n6\n-2.95\n8.7025\n52.215\n\n\n4\n5\n-1.95\n3.8025\n19.0125\n\n\n5\n4\n-0.95\n0.9025\n3.61\n\n\n20\n1\n14.05\n197.4025\n197.4025\n\n\n35\n1\n29.05\n843.9025\n843.9025\n\n\nTotal\n20\n\n\n1162.95\n\n\n\ns^2 = \\frac{1162.95}{19} = 61.21\n\n\nManual Calculation for Company Y\n\n\n\n\n\n\n\n\n\n\ny_i\nf_i\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\nf_i(y_i - \\bar{y})^2\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nTotal\n20\n\n\n34\n\n\n\ns^2 = \\frac{34}{19} = 1.79\n\n\nR Verification\n\nvar(X)\n\n[1] 61.2\n\nvar(Y)\n\n[1] 1.79\n\n\n\n\n\nStandard Deviation\nThe standard deviation is the square root of the variance.\nFormula: s = \\sqrt{s^2}\n\nFor Company X: s = \\sqrt{61.21} = 7.82\nFor Company Y: s = \\sqrt{1.79} = 1.34\n\n\nR Verification\n\nsd(X)\n\n[1] 7.82\n\nsd(Y)\n\n[1] 1.34\n\n\n\n\n\n\nQuartiles\nQuartiles divide the dataset into four equal parts.\n\nManual Calculation for Company X\nOrdered data: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\n\nQ1 (25th percentile): median of first 10 numbers = 3\nQ2 (50th percentile, median): 4\nQ3 (75th percentile): median of last 10 numbers = 5\n\n\n\nManual Calculation for Company Y\nOrdered data: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\n\nQ1 (25th percentile): median of first 10 numbers = 4\nQ2 (50th percentile, median): 5\nQ3 (75th percentile): median of last 10 numbers = 6\n\n\n\nR Verification\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\nIQR\n\nIQR_x = 5 - 3 = 2\nIQR_y = 6 - 4 = 2\n\n\n\n\nTukey Box Plot\nA Tukey box plot visually represents the distribution of data based on quartiles. We’ll use ggplot2 to create the plot.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Prepare the data\ndata &lt;- data.frame(\n  Company = rep(c(\"X\", \"Y\"), each = 20),\n  Salary = c(X, Y)\n)\n\n# Create the box plot\nggplot(data, aes(x = Company, y = Salary, fill = Company)) +\n  geom_boxplot() +\n  labs(title = \"Salary Distribution in Companies X and Y\",\n       x = \"Company\",\n       y = \"Salary (thousands of euros)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n# Create the box plot\nggplot(data, aes(x = Company, y = Salary, fill = Company)) +\n  geom_boxplot(outliers = F) +\n  labs(title = \"Salary Distribution in Companies X and Y\",\n       x = \"Company\",\n       y = \"Salary (thousands of euros)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\nInterpreting the Box Plot\n\nThe box represents the interquartile range (IQR) from Q1 to Q3.\nThe line inside the box is the median (Q2).\nWhiskers extend to the smallest and largest values within 1.5 * IQR.\nPoints beyond the whiskers are considered outliers.\n\n\n\n\nComparison of Results\n\n\n\nMeasure\nCompany X\nCompany Y\n\n\n\n\nMean\n5.95\n5.00\n\n\nMedian\n4\n5\n\n\nMode\n3\n4 and 5\n\n\nVariance\n61.21\n1.79\n\n\nStandard Deviation\n7.82\n1.34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\nKey Observations:\n\nCentral Tendency: Company X has a higher mean but lower median than Company Y, indicating a right-skewed distribution for Company X.\nDispersion: Company X shows much higher variance and standard deviation, suggesting greater salary disparities.\nDistribution Shape: Company Y’s salaries are more tightly clustered, while Company X has extreme values (potential outliers) that significantly affect its mean and variance.\nQuartiles: Company Y’s interquartile range (Q3 - Q1) is slightly larger, but its overall range is much smaller than Company X’s.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-2.-comparing-electoral-district-size-variation-between-countries",
    "href": "chapter5.html#exercise-2.-comparing-electoral-district-size-variation-between-countries",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.11 Exercise 2. Comparing Electoral District Size Variation Between Countries",
    "text": "5.11 Exercise 2. Comparing Electoral District Size Variation Between Countries\n\nData\nWe have electoral district size data from two countries:\n\nx &lt;- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)  # Country high variance\ny &lt;- c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13)  # Country low variance\n\nkable(data.frame(\n  \"Country X (High var.)\" = x,\n  \"Country Y (Low var.)\" = y\n))\n\n\n\n\nCountry.X..High.var..\nCountry.Y..Low.var..\n\n\n\n\n1\n8\n\n\n3\n9\n\n\n5\n9\n\n\n7\n10\n\n\n9\n10\n\n\n11\n11\n\n\n13\n11\n\n\n15\n12\n\n\n17\n12\n\n\n19\n13\n\n\n\n\n\n\n\nMeasures of Central Tendency\n\nArithmetic Mean\nFormula: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\nCalculations for Country X\n\n\n\nElement\nValue\n\n\n\n\n1\n1\n\n\n2\n3\n\n\n3\n5\n\n\n4\n7\n\n\n5\n9\n\n\n6\n11\n\n\n7\n13\n\n\n8\n15\n\n\n9\n17\n\n\n10\n19\n\n\nSum\n100\n\n\n\n\\bar{x} = \\frac{100}{10} = 10\n\nmean_x &lt;- mean(x)\nc(\"Manual\" = 10, \"R\" = mean_x)\n\nManual      R \n    10     10 \n\n\n\n\nCalculations for Country Y\n\n\n\nElement\nValue\n\n\n\n\n1\n8\n\n\n2\n9\n\n\n3\n9\n\n\n4\n10\n\n\n5\n10\n\n\n6\n11\n\n\n7\n11\n\n\n8\n12\n\n\n9\n12\n\n\n10\n13\n\n\nSum\n105\n\n\n\n\\bar{y} = \\frac{105}{10} = 10.5\n\nmean_y &lt;- mean(y)\nc(\"Manual\" = 10.5, \"R\" = mean_y)\n\nManual      R \n  10.5   10.5 \n\n\n\n\n\nMedian\nThe median is the middle value in an ordered dataset.\n\nCalculations for Country X\nOrdered data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nFor n = 10 (even number of observations): Middle positions: 5 and 6 Middle values: 9 and 11\nMedian = \\frac{9 + 11}{2} = 10\n\nmedian_x &lt;- median(x)\nc(\"Manual\" = 10, \"R\" = median_x)\n\nManual      R \n    10     10 \n\n\n\n\nCalculations for Country Y\nOrdered data: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nFor n = 10 (even number of observations): Middle positions: 5 and 6 Middle values: 10 and 11\nMedian = \\frac{10 + 11}{2} = 10.5\n\nmedian_y &lt;- median(y)\nc(\"Manual\" = 10.5, \"R\" = median_y)\n\nManual      R \n  10.5   10.5 \n\n\n\n\n\nMode\n\nCalculations for Country X\n\n\n\nValue\nFrequency\n\n\n\n\n1\n1\n\n\n3\n1\n\n\n5\n1\n\n\n7\n1\n\n\n9\n1\n\n\n11\n1\n\n\n13\n1\n\n\n15\n1\n\n\n17\n1\n\n\n19\n1\n\n\n\nConclusion: No mode (all values occur once)\n\n\nCalculations for Country Y\n\n\n\nValue\nFrequency\n\n\n\n\n8\n1\n\n\n9\n2\n\n\n10\n2\n\n\n11\n2\n\n\n12\n2\n\n\n13\n1\n\n\n\nConclusion: Four modes: 9, 10, 11, 12 (each occurs twice)\n\n# Frequency tables\ntable_x &lt;- table(x)\ntable_y &lt;- table(y)\n\nlist(\n  \"Country X\" = table_x,\n  \"Country Y\" = table_y\n)\n\n$`Country X`\nx\n 1  3  5  7  9 11 13 15 17 19 \n 1  1  1  1  1  1  1  1  1  1 \n\n$`Country Y`\ny\n 8  9 10 11 12 13 \n 1  2  2  2  2  1 \n\n\n\n\n\nVariance\nVariance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\nCalculations for Country X\n\n\n\nx_i\n(x_i - \\bar{x})\n(x_i - \\bar{x})^2\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSum\n\n330\n\n\n\ns^2_X = \\frac{330}{9} = 36.67\n\nvar_x &lt;- var(x)\nc(\"Manual\" = 36.67, \"R\" = var_x)\n\nManual      R \n 36.67  36.67 \n\n\n\n\nCalculations for Country Y\n\n\n\nx_i\n(y_i - \\bar{y})\n(y_i - \\bar{y})^2\n\n\n\n\n8\n-2.5\n6.25\n\n\n9\n-1.5\n2.25\n\n\n9\n-1.5\n2.25\n\n\n10\n-0.5\n0.25\n\n\n10\n-0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n12\n1.5\n2.25\n\n\n12\n1.5\n2.25\n\n\n13\n2.5\n6.25\n\n\nSum\n\n22.5\n\n\n\ns^2_Y = \\frac{22.5}{9} = 2.5\n\nvar_y &lt;- var(y)\nc(\"Manual\" = 2.5, \"R\" = var_y)\n\nManual      R \n   2.5    2.5 \n\n\n\n\n\nStandard Deviation\nStandard deviation is the square root of variance. It measures variability in the same units as the data.\nFormula: s = \\sqrt{s^2}\n\nCalculations for Country X\nUsing previously calculated variance: s^2_X = 36.67\nCalculate square root: s_X = \\sqrt{36.67} \\approx 6.06\n\n\n\nStep\nCalculation\nResult\n\n\n\n\n1. Variance\ns^2_X\n36.67\n\n\n2. Square root\n\\sqrt{36.67}\n6.06\n\n\n\n\nsd_x &lt;- sd(x)\nc(\"Manual\" = 6.06, \"R\" = sd_x)\n\nManual      R \n 6.060  6.055 \n\n\n\n\nCalculations for Country Y\nUsing previously calculated variance: s^2_Y = 2.5\nCalculate square root: s_Y = \\sqrt{2.5} \\approx 1.58\n\n\n\nStep\nCalculation\nResult\n\n\n\n\n1. Variance\ns^2_Y\n2.5\n\n\n2. Square root\n\\sqrt{2.5}\n1.58\n\n\n\n\nsd_y &lt;- sd(y)\nc(\"Manual\" = 1.58, \"R\" = sd_y)\n\nManual      R \n 1.580  1.581 \n\n\nInterpretation:\n\nCountry X: Average deviation from the mean is about 6 seats\nCountry Y: Average deviation from the mean is about 1.6 seats\n\n\n\n\n\nCoefficient of Variation (CV)\nThe coefficient of variation is the ratio of standard deviation to mean, expressed as a percentage.\nFormula: CV = \\frac{s}{\\bar{x}} \\times 100\\%\n\nCalculations for Country X\nCV_X = \\frac{6.06}{10} \\times 100\\% = 60.6\\%\n\n\n\nComponent\nValue\n\n\n\n\nStandard deviation (s)\n6.06\n\n\nMean (\\bar{x})\n10\n\n\nCV\n60.6%\n\n\n\n\ncv_x &lt;- sd(x) / mean(x) * 100\nc(\"Manual\" = 60.6, \"R\" = cv_x)\n\nManual      R \n 60.60  60.55 \n\n\n\n\nCalculations for Country Y\nCV_Y = \\frac{1.58}{10.5} \\times 100\\% = 15.0\\%\n\n\n\nComponent\nValue\n\n\n\n\nStandard deviation (s)\n1.58\n\n\nMean (\\bar{x})\n10.5\n\n\nCV\n15.0%\n\n\n\n\ncv_y &lt;- sd(y) / mean(y) * 100\nc(\"Manual\" = 15.0, \"R\" = cv_y)\n\nManual      R \n 15.00  15.06 \n\n\n\n\n\nQuartiles and Interquartile Range (IQR)\n\nMethods for Calculating Quartiles\nThere are different methods for calculating quartiles. In our manual calculations, we’ll use the median-excluding method:\n\nSplit the series at the median\nMedian is not included in quartile calculations\nCalculate median of each part - these will be Q1 and Q3 respectively\n\n\n\nCalculations for Country X\nOrdered data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMedian = 10 (not included in quartile calculations)\nLower half: 1, 3, 5, 7, 9 Q1 = median of lower half = 5\nUpper half: 11, 13, 15, 17, 19 Q3 = median of upper half = 15\nIQR = Q3 - Q1 = 15 - 5 = 10\n\n\nCalculations for Country Y\nOrdered data: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMedian = 10.5 (not included in quartile calculations)\nLower half: 8, 9, 9, 10, 10 Q1 = median of lower half = 9\nUpper half: 11, 11, 12, 12, 13 Q3 = median of upper half = 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n# Comparison of different quartile calculation methods in R\nmethods_comparison &lt;- data.frame(\n  Method = c(\"Manual (excl. median)\", \n             \"R type=1\", \"R type=2\", \"R type=7 (default)\"),\n  \"Q1 Country X\" = c(5, \n                    quantile(x, 0.25, type=1),\n                    quantile(x, 0.25, type=2),\n                    quantile(x, 0.25, type=7)),\n  \"Q3 Country X\" = c(15,\n                    quantile(x, 0.75, type=1),\n                    quantile(x, 0.75, type=2),\n                    quantile(x, 0.75, type=7)),\n  \"Q1 Country Y\" = c(9,\n                    quantile(y, 0.25, type=1),\n                    quantile(y, 0.25, type=2),\n                    quantile(y, 0.25, type=7)),\n  \"Q3 Country Y\" = c(12,\n                    quantile(y, 0.75, type=1),\n                    quantile(y, 0.75, type=2),\n                    quantile(y, 0.75, type=7))\n)\n\nkable(methods_comparison, digits = 2,\n      caption = \"Comparison of different quartile calculation methods\")\n\n\nComparison of different quartile calculation methods\n\n\n\n\n\n\n\n\n\nMethod\nQ1.Country.X\nQ3.Country.X\nQ1.Country.Y\nQ3.Country.Y\n\n\n\n\nManual (excl. median)\n5.0\n15.0\n9.00\n12.00\n\n\nR type=1\n5.0\n15.0\n9.00\n12.00\n\n\nR type=2\n5.0\n15.0\n9.00\n12.00\n\n\nR type=7 (default)\n5.5\n14.5\n9.25\n11.75\n\n\n\n\n\n\n\nExplanation of Different Quartile Calculation Methods\n\nManual method (excluding median):\n\nSplits data into two parts\nExcludes median\nFinds median of each part\n\nR type=1:\n\nFirst method in R\nUses whole positions\nNo interpolation\n\nR type=2:\n\nSecond method in R\nUses whole positions\nInterpolates when position is not whole\n\nR type=7 (default):\n\nDefault method in R\nUses quantile()[5] from SAS\nInterpolates according to Hyndman and Fan method\n\n\n\n\n\nResults Comparison\n\nsummary_df &lt;- data.frame(\n  Measure = c(\"Mean\", \"Median\", \"Mode\", \"Range\", \"Variance\", \n              \"Std. Dev.\", \"Q1\", \"Q3\", \"IQR\", \"CV (%)\"),\n  \"Country X\" = c(10, 10, \"none\", 18, 36.67, 6.06, 5, 15, 10, 60.6),\n  \"Country Y\" = c(10.5, 10.5, \"9,10,11,12\", 5, 2.5, 1.58, 9, 12, 3, 15.0)\n)\n\nkable(summary_df, \n      caption = \"Summary of all statistical measures\",\n      align = c('l', 'r', 'r'))\n\n\nSummary of all statistical measures\n\n\nMeasure\nCountry.X\nCountry.Y\n\n\n\n\nMean\n10\n10.5\n\n\nMedian\n10\n10.5\n\n\nMode\nnone\n9,10,11,12\n\n\nRange\n18\n5\n\n\nVariance\n36.67\n2.5\n\n\nStd. Dev.\n6.06\n1.58\n\n\nQ1\n5\n9\n\n\nQ3\n15\n12\n\n\nIQR\n10\n3\n\n\nCV (%)\n60.6\n15\n\n\n\n\n\n\n\nComparison using Box Plot\n\ndf_long &lt;- data.frame(\n  country = rep(c(\"X\", \"Y\"), each = 10),\n  size = c(x, y)\n)\n\n# Basic plot\np &lt;- ggplot(df_long, aes(x = country, y = size, fill = country)) +\n  geom_boxplot(outlier.shape = NA) +  # Disable default outlier points\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Add points with transparency\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\")) +\n  labs(\n    title = \"Comparison of Electoral District Size Variation\",\n    subtitle = paste(\"CV: Country X =\", round(cv_x, 1), \"%, Country Y =\", round(cv_y, 1), \"%\"),\n    x = \"Country\",\n    y = \"District Size\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Add quartile annotations\np + annotate(\n  \"text\", \n  x = c(1, 1, 1, 2, 2, 2), \n  y = c(max(x)+1, mean(x), min(x)-1, max(y)+1, mean(y), min(y)-1),\n  label = c(\n    paste(\"Q3 =\", quantile(x, 0.75, type=1)),\n    paste(\"M =\", median(x)),\n    paste(\"Q1 =\", quantile(x, 0.25, type=1)),\n    paste(\"Q3 =\", quantile(y, 0.75, type=1)),\n    paste(\"M =\", median(y)),\n    paste(\"Q1 =\", quantile(y, 0.25, type=1))\n  ),\n  size = 3\n)\n\n\n\n\n\n\n\n\n\n\nMethodological Notes\n\nQuartile Calculations:\n\nThe median-excluding method used may give different results than R’s default functions\nDifferences in calculation methods don’t affect overall conclusions\nAlways important to specify the method used in reports\n\nVisualization:\n\nBox plot effectively shows differences in distributions\nAdditional points show actual values\nAnnotations facilitate interpretation\n\n\n\n\nApplication Notes\n\nUsing the Analysis:\n\nAll calculations can be reproduced using the provided R code\nCode chunks are self-contained and documented\nData format requirements are clearly specified\n\nCustomization:\n\nAnalysis can be adapted for different district size datasets\nVisualization parameters can be adjusted for different presentation needs\nStatistical methods can be modified based on specific requirements\n\n\n\n\nConclusion\n\nSummary Statistics Comparison\n\n\n\nMeasure\nCountry X\nCountry Y\nRelative Difference\n\n\n\n\nMean\n10.0\n10.5\nSimilar\n\n\nMedian\n10.0\n10.5\nSimilar\n\n\nMode\nNone\nMultiple (9,10,11,12)\n-\n\n\nRange\n18\n5\n3.6× larger in X\n\n\nVariance\n36.67\n2.5\n14.7× larger in X\n\n\nIQR\n10\n3\n3.3× larger in X\n\n\nCV\n60.6%\n15.0%\n4.0× larger in X\n\n\n\n\n\nDistribution Characteristics\nCountry X:\n\nUniform distribution pattern\nNo dominant district size (no mode)\nWide range: 1 to 19 seats\nHigh variability (CV = 60.6%) - Even spread of values across range\n\nCountry Y:\n\nClustered distribution pattern\nMultiple common sizes (four modes)\nNarrow range: 8 to 13 seats\nLow variability (CV = 15.0%) - Values concentrated around mean\n\n\n\nBox Plot Interpretation\nThe box plot visualization reveals:\nStructure Elements:\n\nBox: Shows interquartile range (IQR)\nLower edge: First quartile (Q1)\nUpper edge: Third quartile (Q3)\nInternal line: Median (Q2)\nWhiskers: Extend to ±1.5 IQR - Points: Individual district sizes\n\nKey Visual Findings:\n\nBox Size:\n\n\nCountry X: Large box indicates wide spread of middle 50%\nCountry Y: Small box shows tight clustering of middle values\n\n\nWhisker Length:\n\nCountry X: Long whiskers indicate broad overall distribution\nCountry Y: Short whiskers show limited total spread\n\nPoint Distribution:\n\nCountry X: Points widely dispersed\nCountry Y: Points densely clustered\n\n\n\n\nKey Observations\n\nCentral Tendency:\n\nSimilar average district sizes\nDifferent distribution patterns\nDistinct approaches to standardization\n\nVariability Measures:\n\nAll metrics show Country X with 3-15 times more variation\nConsistent pattern across different statistical measures\nSystematic difference in district design\n\nSystem Design:\n\nCountry X: Flexible, varied approach\nCountry Y: Standardized, uniform approach\nDifferent philosophical approaches to representation\n\nRepresentative Implications:\n\nCountry X: Variable voter-to-representative ratios\nCountry Y: More consistent representation levels\nDifferent approaches to democratic representation\n\n\nThis analysis demonstrates fundamental differences in electoral system design between the two countries, with Country X adopting a more varied approach and Country Y maintaining greater uniformity in district sizes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-3.-understanding-boxplots-through-life-expectancy-data",
    "href": "chapter5.html#exercise-3.-understanding-boxplots-through-life-expectancy-data",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.12 Exercise 3. Understanding Boxplots Through Life Expectancy Data",
    "text": "5.12 Exercise 3. Understanding Boxplots Through Life Expectancy Data\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# Prepare data\ndata_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction-to-boxplots",
    "href": "chapter5.html#introduction-to-boxplots",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.13 Introduction to Boxplots",
    "text": "5.13 Introduction to Boxplots\nA boxplot (also known as a box-and-whisker plot) reveals key statistics about your data:\n\nMedian: The middle line in the box (50th percentile)\nFirst quartile (Q1): Bottom of the box (25th percentile)\nThird quartile (Q3): Top of the box (75th percentile)\nInterquartile Range (IQR): The height of the box (Q3 - Q1)\nWhiskers: Extend to the most extreme non-outlier values (Tukey’s method: 1.5 × IQR)\nOutliers: Individual points beyond the whiskers\n\n\nVisualizing Life Expectancy\n\nggplot(data_2007, aes(x = reorder(continent, lifeExp, FUN = median), y = lifeExp)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 24, \n               outlier.fill = \"red\", outlier.alpha = 0.6, outlier.size = 4) +\n  geom_jitter(width = 0.2, alpha = 0.4, color = \"darkblue\") +\n  labs(title = \"Life Expectancy by Continent (2007)\",\n       subtitle = \"Individual points show raw data; red points indicate outliers\",\n       x = \"Continent\",\n       y = \"Life Expectancy (years)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 14)\n  ) +\n  scale_y_continuous(breaks = seq(40, 85, by = 5))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-the-data",
    "href": "chapter5.html#understanding-the-data",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.14 Understanding the Data",
    "text": "5.14 Understanding the Data\n\nMedian and Distribution\nAnswer True or False:\n\n50% of African countries have life expectancy below 54 years\nThe median life expectancy in Europe is approximately 78 years\nMore than 75% of countries in Oceania have life expectancy above 74 years\n25% of Asian countries have life expectancy below 65 years\nThe middle 50% of life expectancies in Europe fall between 74 and 80 years\n\n\n\nSpread and Variation\nAnswer True or False:\n\nAsia shows the largest spread (IQR) in life expectancy\nEurope has the smallest IQR among all continents\nThe variation in Africa’s life expectancy is greater than in the Americas\nOceania shows the least variation in life expectancy\nThe range (excluding outliers) in Asia is approximately 20 years\n\n\n\nOutliers and Extremes\nAnswer True or False:\n\nAfrica has two countries with unusually low life expectancy\nThere are no outliers in Oceania’s distribution\nAsia has both high and low outliers",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#changes-over-time",
    "href": "chapter5.html#changes-over-time",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.15 Changes Over Time",
    "text": "5.15 Changes Over Time\n\ntime_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  mutate(year = factor(year))\n\nggplot(time_comparison, aes(x = continent, y = lifeExp, fill = year)) +\n  geom_boxplot(alpha = 0.7, position = \"dodge\", outlier.shape = 21,\n               outlier.alpha = 0.6) +\n  labs(title = \"Life Expectancy: 1957 vs 2007\",\n       subtitle = \"Comparing distribution changes over 50 years\",\n       x = \"Continent\",\n       y = \"Life Expectancy (years)\",\n       fill = \"Year\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(breaks = seq(30, 85, by = 5))\n\n\n\n\n\n\n\n\n\nTime Comparison Questions\nAnswer True or False:\n\nThe median life expectancy increased in all continents between 1957 and 2007\nThe variation in life expectancy (IQR) decreased in most continents over time\nAfrica showed the smallest improvement in median life expectancy\nThe spread of life expectancies in Asia decreased substantially from 1957 to 2007\nOceania maintained the highest median life expectancy in both time periods\n\n\n\nStatistical Summary\n\n# Calculate summary statistics\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    median = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    n_outliers = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Summary Statistics by Continent and Year\")\n\n\nSummary Statistics by Continent and Year\n\n\ncontinent\nyear\nmedian\nq1\nq3\niqr\nn_outliers\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n0",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#key-learning-points",
    "href": "chapter5.html#key-learning-points",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.16 Key Learning Points",
    "text": "5.16 Key Learning Points\n\nDistribution Center:\n\nMedian shows the typical life expectancy\nChanges in median reflect overall improvements\n\nSpread and Variation:\n\nIQR (box height) indicates data dispersion\nWider boxes suggest more inequality in life expectancy\n\nOutliers and Extremes:\n\nOutliers often represent countries with unique circumstances\n\nTime Comparison:\n\nShows both absolute improvements and changes in variation\nHighlights persistent regional disparities\nReveals different rates of progress across continents",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "href": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.17 Appendix: Summary Tables for Data Types and Applicable Statistical Measures",
    "text": "5.17 Appendix: Summary Tables for Data Types and Applicable Statistical Measures\n\nTable 1: Pros and Cons of Various Statistical Measures\n\nMeasures of Center\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nMean\n- Uses all data points- Allows for further statistical calculations- Ideal for normally distributed data\n- Sensitive to outliers- Not ideal for skewed distributions- Not meaningful for nominal data\nInterval, Ratio, some Discrete, Continuous\n\n\nMedian\n- Not affected by outliers- Good for skewed distributions- Can be used with ordinal data\n- Ignores the actual values of most data points- Less useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nMode\n- Can be used with any data type- Good for finding most common category\n- May not be unique (multimodal)- Not useful for many types of analyses- Ignores magnitude of differences between values\nAll types\n\n\n\n\n\nMeasures of Variability\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nRange\n- Simple to calculate and understand- Gives quick idea of data spread\n- Very sensitive to outliers- Ignores all data between extremes- Not useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nInterquartile Range (IQR)\n- Not affected by outliers- Good for skewed distributions\n- Ignores 50% of the data- Less intuitive than range\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nVariance\n- Uses all data points- Basis for many statistical procedures\n- Sensitive to outliers- Units are squared (less intuitive)\nInterval, Ratio, some Discrete, Continuous\n\n\nStandard Deviation\n- Uses all data points- Same units as original data- Widely used and understood\n- Sensitive to outliers- Assumes roughly normal distribution for interpretation\nInterval, Ratio, some Discrete, Continuous\n\n\nCoefficient of Variation\n- Allows comparison between datasets with different units or means\n- Can be misleading when means are close to zero- Not meaningful for data with negative values\nRatio, some Interval\n\n\n\n\n\nMeasures of Correlation/Association\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nPearson’s r\n- Measures linear relationship- Widely used and understood\n- Assumes normal distribution- Sensitive to outliers- Only captures linear relationships\nInterval, Ratio, Continuous\n\n\nSpearman’s rho\n- Can be used with ordinal data- Captures monotonic relationships- Less sensitive to outliers\n- Loses information by converting to ranks- May miss some types of relationships\nOrdinal, Interval, Ratio\n\n\nKendall’s tau\n- Can be used with ordinal data- More robust than Spearman’s for small samples- Has nice interpretation (probability of concordance)\n- Loses information by only considering order- Computationally more intensive\nOrdinal, Interval, Ratio\n\n\nChi-square\n- Can be used with nominal data- Tests independence of categorical variables\n- Requires large sample sizes- Sensitive to sample size- Doesn’t measure strength of association\nNominal, Ordinal\n\n\nCramér’s V\n- Can be used with nominal data- Provides measure of strength of association- Normalized to [0,1] range\n- Interpretation can be subjective- May overestimate association in small samples\nNominal, Ordinal\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓*\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\nNotes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html",
    "href": "rozdzial5.html",
    "title": "6  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "6.1 Wprowadzenie do Notacji Sigma (Σ)\nStatystyki opisowe są fundamentalnymi narzędziami w badaniach nauk społecznych, zapewniającymi zwięzłe podsumowanie charakterystyk danych. Pełnią kilka kluczowych funkcji:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wprowadzenie-do-notacji-sigma-σ",
    "href": "rozdzial5.html#wprowadzenie-do-notacji-sigma-σ",
    "title": "6  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "Co to jest notacja sumacyjna Sigma? Sigma (Σ) to operator matematyczny, który nakazuje nam zsumować (dodać) sekwencję wyrazów - działa jak instrukcja wykonania dodawania wszystkich elementów w określonym zakresie.\nCel: Zapewnia zwięzły sposób zapisu sum wielu podobnych wyrazów za pomocą jednego symbolu, unikając długich wyrażeń dodawania.\n\n\nPodstawowa formuła\n\nOgólna forma notacji sigma to:\n\n\\sum_{i=a}^{b} f(i)\n\nIndeks sumowania: i\nDolna granica: a\nGórna granica: b\nFunkcja: f(i)\n\n\n\nPrzykłady zastosowania notacji Sigma\n\nProsty przykład: Suma liczb naturalnych\n\nZałóżmy, że chcesz dodać pierwsze pięć dodatnich liczb całkowitych:\n\n\\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15\n\nPowyższy zapis dodaje pierwsze pięć dodatnich liczb całkowitych.\n\n\n\nSuma kwadratów\n\nZałóżmy, że chcesz zsumować kwadraty pierwszych czterech dodatnich liczb całkowitych:\n\n\\sum_{i=1}^{4} i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 1 + 4 + 9 + 16 = 30\n\nJest to suma kwadratów pierwszych czterech dodatnich liczb całkowitych.\n\n\n\nSuma wartości stałej\n\nSumowanie stałej wartości c dla n wyrazów:\n\n\\sum_{i=1}^{n} c = c + c + c + ... + c \\text{ (n razy)} = n \\cdot c\n\nPrzykład: Suma pięciu piątek:\n\n\\sum_{i=1}^{5} 5 = 5 + 5 + 5 + 5 + 5 = 5 \\cdot 5 = 25\n\n\n\nProste przykłady w kontekście statystyki\n\\sum_{i=1}^{n} x_i\n\nIndeks sumowania: i (zazwyczaj oznacza konkretną obserwację w zbiorze danych)\nDolna granica: 1 (zwykle zaczynamy od pierwszej obserwacji)\nGórna granica: n (całkowita liczba obserwacji w naszym zbiorze danych)\nWyrażenie: x_i (wartość i-tej obserwacji)\n\n\nSumowanie wartości obserwacji\n\nMamy zbiór danych: 5, 8, 12, 15, 20\nSuma wszystkich wartości:\n\n\\sum_{i=1}^{5} x_i = x_1 + x_2 + x_3 + x_4 + x_5 = 5 + 8 + 12 + 15 + 20 = 60\n\nTa suma jest kluczowym elementem przy obliczaniu średniej arytmetycznej.\n\n\n\nSuma odchyleń od średniej\n\nDla tego samego zbioru danych (5, 8, 12, 15, 20), średnia wynosi \\bar{x} = 60/5 = 12\nSuma odchyleń od średniej:\n\n\\sum_{i=1}^{5} (x_i - \\bar{x}) = (5-12) + (8-12) + (12-12) + (15-12) + (20-12) = -7 + (-4) + 0 + 3 + 8 = 0\n\nWażna obserwacja: Suma odchyleń od średniej zawsze wynosi 0, co jest podstawową właściwością średniej arytmetycznej.\n\n\n\n\nPodsumowanie\n\nNotacja Sigma (Σ) pozwala na zwięzły zapis kluczowych wzorów statystycznych\nNajważniejsze zastosowania obejmują obliczanie:\n\nŚredniej arytmetycznej\nWariancji i odchylenia standardowego\nRóżnych sum kwadratów używanych w analizie regresji\n\n\n\n\n\n\n\n\nOperatory Sumy (Σ) i Iloczynu (Π)\n\n\n\n\nOperator Sigma (Σ)\n\\sum to operator sumowania, który nakazuje nam dodać wyrazy:\n\\sum_{i=1}^{n} x_i = x_1 + x_2 + ... + x_n\ngdzie: - i to zmienna indeksowa - Dolna wartość pod Σ (tutaj i=1) to punkt początkowy - Górna wartość (tutaj n) to punkt końcowy\n\n\nOperator Pi (Π)\n\\prod to operator iloczynu, który nakazuje nam pomnożyć wyrazy:\n\\prod_{i=1}^{n} x_i = x_1 \\times x_2 \\times ... \\times x_n\ngdzie: - i to zmienna indeksowa - Dolna wartość pod Π (tutaj i=1) to punkt początkowy - Górna wartość (tutaj n) to punkt końcowy\n\n\n\n\n\n\n\n\n\nPrzykład Σ\n\n\n\n\\sum_{i=1}^{4} i = 1 + 2 + 3 + 4 = 10\n\n\n\n\n\n\n\n\nPrzykład Π\n\n\n\n\\prod_{i=1}^{4} i = 1 \\times 2 \\times 3 \\times 4 = 24\n\n\n\n\n\n\n\n\nKluczowe Różnice\n\n\n\n\nΣ oznacza wielokrotne dodawanie\nΠ oznacza wielokrotne mnożenie",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#typy-rozkładów-danych",
    "href": "rozdzial5.html#typy-rozkładów-danych",
    "title": "6  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.2 Typy rozkładów danych",
    "text": "6.2 Typy rozkładów danych\n\n\n\n\n\n\nImportant\n\n\n\nRozkład danych informuje o tym, jakie wartości przyjmuje zmienna i jak często.\n\n\nZrozumienie rozkładów danych jest kluczowe dla analizy i wizualizacji danych. W tym dokumencie przyjrzymy się różnym typom rozkładów i sposobom ich wizualizacji przy użyciu ggplot2 w R.\n\nRozkład normalny\nRozkład normalny, znany również jako rozkład Gaussa, jest symetryczny i ma kształt dzwonu.\n\n# Generowanie danych o rozkładzie normalnym\ndane_normalne &lt;- data.frame(x = rnorm(1000))\n\n# Wykres\nggplot(dane_normalne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład normalny\", x = \"Wartość\", y = \"Gęstość\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\nRozkład jednostajny\nW rozkładzie jednostajnym wszystkie wartości mają równe prawdopodobieństwo wystąpienia.\n\n# Generowanie danych o rozkładzie jednostajnym\ndane_jednostajne &lt;- data.frame(x = runif(1000))\n\n# Wykres\nggplot(dane_jednostajne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład jednostajny\", x = \"Wartość\", y = \"Gęstość\")\n\n\n\n\n\n\n\n\n\n\nRozkłady skośne\nRozkłady skośne są asymetryczne, z jednym ogonem dłuższym niż drugi.\n\n# Generowanie danych o rozkładzie prawoskośnym\ndane_prawoskosne &lt;- data.frame(x = rlnorm(1000))\n\n# Wykres\nggplot(dane_prawoskosne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightyellow\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład prawoskośny\", x = \"Wartość\", y = \"Gęstość\")\n\n\n\n\n\n\n\n\n\n\nRozkład bimodalny\nRozkład bimodalny ma dwa szczyty (dwie dominanty), wskazujące na dwie odrębne podgrupy w danych.\n\n# Generowanie danych bimodalnych\ndane_bimodalne &lt;- data.frame(x = c(rnorm(500, mean = -2), rnorm(500, mean = 2)))\n\n# Wykres\nggplot(dane_bimodalne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightpink\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład bimodalny\", x = \"Wartość\", y = \"Gęstość\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRozkład\nKluczowe właściwości\nPrzykłady\n\n\n\n\nSymetryczny (Normalny)\nSymetryczny, kształt dzwonu, większość wartości blisko średniej\nWzrost dorosłych w populacji, wyniki testów IQ, błędy pomiarowe, wyniki egzaminów standaryzowanych\n\n\nRównomierny (Jednostajny)\nJednakowe prawdopodobieństwo w całym zakresie\nOstatnia cyfra numeru telefonu, wybór losowego dnia tygodnia, pozycja wskazówki po zakręceniu kołem fortuny\n\n\nDwumodalny (Bimodalny)\nDwa wyraźne szczyty, sugeruje istnienie podgrup\nStruktura wieku w miastach uniwersyteckich (studenci i stali mieszkańcy), opinie na tematy silnie polaryzujące społeczeństwo, godziny natężenia ruchu drogowego (poranny i popołudniowy szczyt)\n\n\nSkośny w prawo (Prawostronnie asymetryczny)\nWydłużony “ogon” po prawej stronie, większość wartości mniejsza od średniej\nCzas oczekiwania w kolejce, czas dojazdu do pracy, wiek zawarcia pierwszego małżeństwa\n\n\nSkośny z grubym ogonem (Log-normalny)\nSilna asymetria w prawo, wartości nie mogą być ujemne, długi “gruby ogon”\nDochody osobiste, ceny mieszkań, wielkość gospodarstw domowych\n\n\nSkośny o ekstremalnym ogonie (Potęgowy)\nEkstremalna asymetria, efekt “bogaty staje się bogatszym”, brak charakterystycznej skali\nMajątek najbogatszych osób, populacja miast, liczba obserwujących w mediach społecznościowych, liczba cytowań publikacji naukowych",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wizualizacja-rozkładów-danych-rzeczywistych",
    "href": "rozdzial5.html#wizualizacja-rozkładów-danych-rzeczywistych",
    "title": "6  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.3 Wizualizacja rozkładów danych rzeczywistych",
    "text": "6.3 Wizualizacja rozkładów danych rzeczywistych\nUżyjemy zbioru danych palmerpenguins do wizualizacji rozkładów danych.\n\nHistogram i wykres gęstości\n\n\n\n\n\n\nUnderstanding Histograms and Density\n\n\n\n⭐ A histogram is a special graph for numerical data where:\n\nData is grouped into ranges (called “bins”)\nBars touch each other (unlike bar charts!) because the data is continuous\nEach bar’s height shows how many values fall into that range\n\nThink of density as showing how common or concentrated certain values are in your data:\n\nA higher point on a density curve (or taller bar in a histogram) means those values appear more frequently in your data\nA lower point means those values are less common\n\nJust like a crowded area has more people per space (higher density), a taller part of the graph shows values that appear more often in your dataset!\n\n\n\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład długości płetw pingwinów\", \n       x = \"Długość płetwy (mm)\", \n       y = \"Gęstość\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\nWykres pudełkowy\nWykresy pudełkowe są przydatne do porównywania rozkładów między kategoriami.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Rozkład masy ciała pingwinów według gatunku\", \n       x = \"Gatunek\", \n       y = \"Masa ciała (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nWykres skrzypcowy\nWykresy skrzypcowe łączą cechy wykresu pudełkowego i wykresu gęstości.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Rozkład masy ciała pingwinów według gatunku\", \n       x = \"Gatunek\", \n       y = \"Masa ciała (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nWykres grzbietowy\nWykresy grzbietowe są przydatne do porównywania wielu rozkładów.\n\nlibrary(ggridges)\n\nggplot(penguins, aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.6) +\n  labs(title = \"Rozkład długości płetw według gatunku pingwina\",\n       x = \"Długość płetwy (mm)\",\n       y = \"Gatunek\")\n\nPicking joint bandwidth of 2.38\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n\n\nPodsumowanie\nZrozumienie i wizualizacja rozkładów danych są kluczowe w analizie danych. ggplot2 zapewnia elastyczny i potężny zestaw narzędzi do tworzenia różnych typów wykresów rozkładów. Badając różne techniki wizualizacji, możemy uzyskać wgląd w podstawowe wzorce i charakterystyki naszych danych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wartości-odstające-outliers",
    "href": "rozdzial5.html#wartości-odstające-outliers",
    "title": "6  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.4 Wartości Odstające (Outliers)",
    "text": "6.4 Wartości Odstające (Outliers)\nPrzed zagłębieniem się w konkretne miary, kluczowe jest zrozumienie pojęcia wartości odstających, ponieważ mogą one znacząco wpływać na wiele statystyk opisowych.\nWartości odstające to punkty danych, które znacznie różnią się od innych obserwacji w zbiorze danych. Mogą wystąpić z powodu:\n\nBłędów pomiaru lub zapisu\nPrawdziwych ekstremalnych wartości w populacji\n\nWartości odstające mogą mieć istotny wpływ na wiele miar statystycznych, szczególnie tych opartych na średnich lub sumach kwadratów odchyleń. Dlatego ważne jest, aby:\n\nIdentyfikować wartości odstające zarówno poprzez metody statystyczne, jak i wiedzę dziedzinową\nBadać przyczyny wartości odstających\nPodejmować świadome decyzje o tym, czy włączać je do analiz, czy nie",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#symbole-stosowane-w-statystyce---podsumowanie",
    "href": "rozdzial5.html#symbole-stosowane-w-statystyce---podsumowanie",
    "title": "6  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.5 Symbole Stosowane w Statystyce - podsumowanie",
    "text": "6.5 Symbole Stosowane w Statystyce - podsumowanie\n\n\n\n\n\n\n\n\n\n\nMiara\nParametr Populacji\nStatystyka z Próby\nAlternatywne Oznaczenia\nUwagi\n\n\n\n\nLiczebność\nN\nn\n-\nCałkowita liczba obserwacji\n\n\nŚrednia\n\\mu\n\\bar{x}\nE(X), M\nE(X) stosowane w rachunku prawdopodobieństwa\n\n\nWariancja\n\\sigma^2\ns^2\n\\text{Var}(X), V(X)\nKwadrat odchyleń od średniej\n\n\nOdchylenie standardowe\n\\sigma\ns\n\\text{OS}, \\text{std}\nPierwiastek z wariancji\n\n\nFrakcja/Proporcja\n\\pi, P\n\\hat{p}\n\\text{fr}\nCzęstości względne\n\n\nWspółczynnik korelacji\n\\rho\nr\n\\text{kor}(x,y)\nWartości od -1 do +1\n\n\nBłąd standardowy\n\\sigma_{\\bar{x}}\ns_{\\bar{x}}\n\\text{BS}\nBłąd standardowy średniej\n\n\nSuma\n\\sum\n\\sum\n\\sum_{i=1}^n\nZ indeksowaniem\n\n\nPojedyncza obserwacja\nX_i\nx_i\n-\ni-ta obserwacja\n\n\nKowariancja\n\\sigma_{xy}\ns_{xy}\n\\text{Cov}(X,Y)\nWspólna zmienność\n\n\nMediana\n\\eta\n\\text{Me}\nM\nWartość środkowa\n\n\nRozstęp\nR\nr\n\\text{max}(X) - \\text{min}(X)\nMiara rozproszenia\n\n\nDominanta\n\\text{Mo}\n\\text{mo}\n\\text{mod}\nWartość najczęstsza\n\n\nSkośność\n\\gamma_1\ng_1\n\\text{SK}\nAsymetria rozkładu\n\n\nKurtoza\n\\gamma_2\ng_2\n\\text{KU}\nSpłaszczenie rozkładu\n\n\n\nDodatkowe ważne wzory:\n\nMomenty z próby: m_k = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^k\nMomenty populacji: \\mu_k = E[(X - \\mu)^k]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-tendencji-centralnej",
    "href": "rozdzial5.html#miary-tendencji-centralnej",
    "title": "6  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.6 Miary Tendencji Centralnej",
    "text": "6.6 Miary Tendencji Centralnej\nMiary tendencji centralnej mają na celu identyfikację “typowej” lub “centralnej” wartości w zbiorze danych. Trzy podstawowe miary to średnia, mediana i moda.\n\nŚrednia Arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez liczbę wartości.\nWzór: \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\nWażna Właściwość: Średnia jest punktem równowagi w danych. Suma odchyleń od średniej zawsze wynosi zero:\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0\nTa właściwość sprawia, że średnia jest użyteczna w wielu obliczeniach statystycznych.\n\n\n\n\n\n\nZrozumienie średniej jako punktu równowagi 🎯\n\n\n\nRozważmy zbiór danych X = \\{1, 2, 6, 7, 9\\} na osi liczbowej, wyobrażając go sobie jako huśtawkę:\n\n\n\nhttps://www.gastonsanchez.com/matrix4sl/mean-as-a-balancing-point.html\n\n\nŚrednia (\\mu) działa jak idealny punkt równowagi tej huśtawki. Dla naszych danych:\n\\mu = \\frac{1 + 2 + 6 + 7 + 9}{5} = 5\n\nCo się dzieje przy różnych punktach podparcia? 🤔\n\nPunkt podparcia w 6 (za wysoko):\n\nLewa strona: Wartości (1, 2) są poniżej\nPrawa strona: Wartości (7, 9) są powyżej\n\\sum odległości z lewej = (6-1) + (6-2) = 9\n\\sum odległości z prawej = (7-6) + (9-6) = 4\nHuśtawka przechyla się w lewo! ⬅️ bo 9 &gt; 4\n\nPunkt podparcia w 4 (za nisko):\n\nLewa strona: Wartości (1, 2) są poniżej\nPrawa strona: Wartości (6, 7, 9) są powyżej\n\\sum odległości z lewej = (4-1) + (4-2) = 5\n\\sum odległości z prawej = (6-4) + (7-4) + (9-4) = 10\nHuśtawka przechyla się w prawo! ➡️ bo 5 &lt; 10\n\nPunkt podparcia w średniej (5) (idealna równowaga):\n\n\\sum odległości poniżej = \\sum odległości powyżej\n((5-1) + (5-2)) = ((6-5) + (7-5) + (9-5))\n7 = 7 ✨ Idealna równowaga!\n\n\nTo pokazuje, dlaczego średnia jest unikalnym punktem równowagi, gdzie:\n\\sum_{i=1}^n (x_i - \\mu) = 0\nHuśtawka zawsze będzie się przechylać, chyba że punkt podparcia zostanie umieszczony dokładnie w średniej! 🎪\n\n\n\n\n\n\n\n\n\nŚrednia jako punkt równowagi\n\n\n\nTa wizualizacja pokazuje, jak średnia arytmetyczna (5) działa jako punkt równowagi pomiędzy skupionymi punktami z lewej strony a rozproszonymi punktami z prawej strony:\nLewa strona średniej:\n\nPunkty o wartościach 2 i 3\nBlisko siebie (różnica 1 jednostka)\nOdległości od średniej: 3 i 2 jednostki\nSuma “ciążenia” = 5 jednostek\n\nPrawa strona średniej:\n\nPunkty o wartościach 6 i 9\nBardziej oddalone (różnica 3 jednostki)\nOdległości od średniej: 1 i 4 jednostki\nSuma “ciążenia” = 5 jednostek\n\nKluczowe obserwacje:\n\nŚrednia (5) jest punktem równowagi, mimo że:\n\nPunkty po lewej są skupione (2,3)\nPunkty po prawej są rozproszone (6,9)\nZielone strzałki pokazują odległości od średniej\n\nRównowaga jest zachowana ponieważ:\n\nSuma odległości się równoważy: (5-2) + (5-3) = (6-5) + (9-5)\nCałkowita suma odległości = 5 jednostek po każdej stronie\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrzykład Ręcznego Obliczenia:\nObliczmy średnią dla zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nSumuj wszystkie wartości\n2 + 4 + 4 + 5 + 5 + 7 + 9 = 36\n\n\n2\nPolicz liczbę wartości\nn = 7\n\n\n3\nPodziel sumę przez n\n36 / 7 = 5,14\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nŁatwa do obliczenia i zrozumienia\nWykorzystuje wszystkie punkty danych\n\nWady:\n\nWrażliwa na wartości odstające\nMoże nie być dobrą miarą dla silnie asymetrycznych rozkładów danych\n\n\n\nMediana\nMediana to środkowa wartość, gdy dane są uporządkowane.\nPrzykład Ręcznego Obliczenia:\nUżywając tego samego zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nWynik\n\n\n\n\n1\nUporządkuj dane\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nZnajdź środkową wartość\n5\n\n\n\nDla parzystej liczby wartości, weź średnią z dwóch środkowych wartości.\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmedian(dane)\n\n[1] 5\n\n\nZalety:\n\nNie jest zniekształcona przez skrajne wartości odstające (outliers)\nLepsza dla rozkładów skośnych\n\nWady:\n\nNie wykorzystuje wszystkich punktów danych\n\n\n\n\n\n\n\nWarning\n\n\n\nJak znaleźć pozycję mediany w zbiorze danych:\n\nNajpierw posortuj dane rosnąco\nGdy n jest nieparzyste:\n\nPozycja mediany = \\frac{n + 1}{2}\n\nGdy n jest parzyste:\n\nPierwsza pozycja mediany = \\frac{n}{2}\nDruga pozycja mediany = \\frac{n}{2} + 1\nMediana = \\frac{\\text{wartość na pozycji }\\frac{n}{2} + \\text{wartość na pozycji }(\\frac{n}{2}+1)}{2}\n\n\nPrzykłady:\n\nNieparzyste n=7: pozycja = \\frac{7+1}{2} = 4-ta wartość\nParzyste n=8: pozycje = \\frac{8}{2} = 4-ta i 4+1 = 5-ta wartość\n\n\n\n\n\nModa (Dominanta)\nModa to najczęściej występująca wartość.\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nWartość\nCzęstość\n\n\n\n\n2\n1\n\n\n4\n2\n\n\n5\n2\n\n\n7\n1\n\n\n9\n1\n\n\n\nModa to 4 i 5 (rozkład bimodalny).\nObliczenie w R:\n\nlibrary(modeest)\nmfv(dane)  # Najczęściej występująca wartość\n\n[1] 4 5\n\n\nZalety:\n\nJedyna miara tendencji centralnej dla danych nominalnych\nMoże identyfikować wiele punktów szczytowych (dominujących) w danych\n\nWady:\n\nNie zawsze jednoznacznie zdefiniowana\nNie jest odpowiednia dla danych ciągłych\n\n\n\nŚrednia (arytmetyczna) Ważona (*)\nŚrednia ważona jest używana, gdy niektóre punkty danych są ważniejsze niż inne. Występują dwa typy średnich ważonych: z wagami nienormalizowanymi i z wagami znormalizowanymi.\n\nŚrednia Ważona z Wagami Nienormalizowanymi\nJest to standardowa forma średniej ważonej, gdzie wagi mogą być dowolnymi liczbami dodatnimi reprezentującymi ważność każdego punktu danych.\nWzór: \\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\nPrzykład Obliczeń Ręcznych: Obliczmy średnią ważoną dla zbioru danych: 2, 4, 5, 7 z wagami 1, 2, 3, 1\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPomnóż każdą wartość przez jej wagę\n(2 * 1) + (4 * 2) + (5 * 3) + (7 * 1) = 2 + 8 + 15 + 7 = 32\n\n\n2\nZsumuj wagi\n1 + 2 + 3 + 1 = 7\n\n\n3\nPodziel wynik z kroku 1 przez wynik z kroku 2\n32 / 7 = 4.57\n\n\n\nObliczenia w R:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\nŚrednia Ważona z Wagami Znormalizowanymi (Ułamki)\nW tym przypadku wagi są ułamkami sumującymi się do 1, reprezentującymi proporcję ważności dla każdego punktu danych.\nWzór: \\bar{x}_w = \\sum_{i=1}^n w_i x_i, gdzie \\sum_{i=1}^n w_i = 1\nPrzykład Obliczeń Ręcznych:\nObliczmy średnią ważoną dla zbioru danych: 2, 4, 5, 7 z wagami znormalizowanymi 0.1, 0.3, 0.4, 0.2\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPomnóż każdą wartość przez jej wagę\n(2 * 0.1) + (4 * 0.3) + (5 * 0.4) + (7 * 0.2)\n\n\n2\nZsumuj wyniki\n0.2 + 1.2 + 2.0 + 1.4 = 4.8\n\n\n\nObliczenia w R:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Uwaga: sumują się do 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nZalety Średnich Ważonych:\n\nUwzględniają różną ważność punktów danych\n\nWady Średnich Ważonych:\n\nWymagają uzasadnienia dla wag\nMogą być niewłaściwie wykorzystane w celu manipulacji wynikami\nMogą być mniej intuicyjne w interpretacji niż prosta średnia arytmetyczna",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-zmienności-rozproszenia",
    "href": "rozdzial5.html#miary-zmienności-rozproszenia",
    "title": "6  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.7 Miary Zmienności (Rozproszenia)",
    "text": "6.7 Miary Zmienności (Rozproszenia)\nTe miary opisują, jak bardzo rozproszone są dane.\n\n\n\n\n\n\nZrozumienie Wariancji\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.1: Trzy wykresy punktowe pokazujące rosnącą wariancję przy stałej średniej\n\n\n\n\n\nPowyższe trzy wykresy punktowe pokazują, w jaki sposób wariancja mierzy rozproszenie danych wokół wartości centralnej:\n\nWszystkie rozkłady mają tę samą średnią (μ = 10), oznaczoną linią przerywaną\nMała Wariancja (σ² = 1): Punkty są skupione blisko średniej\nŚrednia Wariancja (σ² = 4): Punkty wykazują umiarkowane rozproszenie\nDuża Wariancja (σ² = 9): Punkty są szeroko rozproszone wokół średniej\n\n\n\n\n\n\n\n\n\nRóżne Poziomy Zmienności\n\n\n\n\n\n\n\n\n\n\n\n\nTa wizualizacja przedstawia trzy rozkłady normalne o tej samej średniej (μ = 10), ale różnych poziomach zmienności:\n\nMała zmienność (σ = 0.5)\n\nPunkty danych grupują się ściśle wokół średniej\nKrzywa gęstości jest wysoka i wąska\nWiększość obserwacji mieści się w przedziale ±0.5 jednostki (odchylenia stand.) od średniej\n\nŚrednia zmienność (σ = 2.0)\n\nPunkty danych są bardziej rozproszone wokół średniej\nKrzywa gęstości jest niższa i szersza\nWiększość obserwacji mieści się w przedziale ±2 jednostki od średniej\n\nDuża zmienność (σ = 4.0)\n\nPunkty danych są szeroko rozproszone wokół średniej\nKrzywa gęstości jest znacznie bardziej płaska i szeroka\nWiększość obserwacji mieści się w przedziale ±4 jednostki od średniej\n\n\nZwróć uwagę, jak odchylenie standardowe (σ) bezpośrednio powiązane jest z rozproszeniem rozkładu - większe wartości σ wskazują na większą zmienność danych, podczas gdy mniejsze wartości oznaczają, że punkty danych mają tendencję do grupowania się bliżej średniej.\n\n\n\nRozstęp\nRozstęp to różnica między wartością maksymalną a minimalną.\nWzór: R = x_{max} - x_{min}\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nZnajdź wartość maksymalną\n9\n\n\n2\nZnajdź wartość minimalną\n2\n\n\n3\nOdejmij minimum od maksimum\n9 - 2 = 7\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nrange(dane)\n\n[1] 2 9\n\nmax(dane) - min(dane)\n\n[1] 7\n\n\nZalety:\n\nProsty do obliczenia i zrozumienia\nSzybka informacja o ogólnym rozproszeniu danych\n\nWady:\n\nBardzo wrażliwy na wartości odstające\nNie dostarcza informacji o rozkładzie między skrajnościami\n\n\n\nRozstęp Międzykwartylowy (IQR)\nIQR to różnica między 75. a 25. percentylem (3. a 1. kwartylem).\nWzór: IQR = Q_3 - Q_1\nAby znaleźć kwartyle ręcznie:\n\nDla nieparzystej liczby wartości:\n\nQ2 (mediana) to środkowa wartość\nQ1 to mediana dolnej połowy (wyłączając medianę dla wszystkich obserwacji)\nQ3 to mediana górnej połowy (wyłączając medianę dla wszystkich obserwacji)\n\nDla parzystej liczby wartości:\n\nQ2 to średnia z dwóch środkowych wartości\nQ1 to mediana dolnej połowy (wyłączając medianę dla wszystkich obserwacji)\nQ3 to mediana górnej połowy (wyłączając medianę dla wszystkich obserwacji)\n\n\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nUporządkuj dane\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nZnajdź Q2 (medianę)\n5\n\n\n3\nZnajdź Q1 (medianę dolnej połowy)\n4\n\n\n4\nZnajdź Q3 (medianę górnej połowy)\n7\n\n\n5\nOblicz IQR\nQ3 - Q1 = 7 - 4 = 3\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nprint(dane)\n\n[1] 2 4 4 5 5 7 9\n\nquantile(dane, type = 1)\n\n  0%  25%  50%  75% 100% \n   2    4    5    7    9 \n\nIQR(dane, type = 1)\n\n[1] 3\n\n\nZalety:\n\nOdporny na wartości odstające\nDostarcza informacji o rozproszeniu środkowych 50% danych\n\nWady:\n\nIgnoruje ogony rozkładu\nMniej efektywny niż odchylenie standardowe dla rozkładów normalnych\n\n\n\nWariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n\n\n\n\n\n\nWariancja: Zrozumienie Średniego Odchylenia Kwadratowego\n\n\n\nCzym jest Wariancja? Wariancja mierzy, jak bardzo punkty danych są “rozrzucone” wokół średniej - jest średnią kwadratów odchyleń od średniej.\nWzór: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nProsty Przykład: Rozważmy liczby: 2, 4, 6, 8, 10 Średnia (\\bar{x}) = 6\n\n\n\n\n\n\n\n\n\nObliczanie Odchyleń:\n\n\n\n\n\n\n\n\n\n\n\n\nWartość\nOdchylenie od średniej\nKwadrat odchylenia\n\n\n\n\n2\n-4\n16\n\n\n4\n-2\n4\n\n\n6\n0\n0\n\n\n8\n+2\n4\n\n\n10\n+4\n16\n\n\n\nWariancja = \\frac{16 + 4 + 0 + 4 + 16}{4} = 10\nKluczowe Punkty:\n\nŚrednia służy jako punkt odniesienia (niebieska przerywana linia)\nOdchylenia pokazują odległość od średniej (czerwone kropkowane linie)\nPodniesienie do kwadratu sprawia, że wszystkie odchylenia są dodatnie (niebieskie słupki)\nWiększe odchylenia mają większy wpływ na wariancję\n\n\n\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz średnią\n\\bar{x} = 5,14\n\n\n2\nOdejmij średnią od każdej obserwacji i podnieś wynik do kwadratu\n(2 - 5,14)^2 = 9,86\n\n\n\n\n(4 - 5,14)^2 = 1,30\n\n\n\n\n(4 - 5,14)^2 = 1,30\n\n\n\n\n(5 - 5,14)^2 = 0,02\n\n\n\n\n(5 - 5,14)^2 = 0,02\n\n\n\n\n(7 - 5,14)^2 = 3,46\n\n\n\n\n(9 - 5,14)^2 = 14,90\n\n\n3\nSumuj kwadraty różnic\n30,86\n\n\n4\nPodziel przez (n-1), czyli przez liczbę obserwacji - 1\n30,86 / 6 = 5,14\n\n\n\nObliczenie w R:\n\nvar(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nWykorzystuje wszystkie punkty danych\nPodstawa dla wielu testów statystycznych*\n\nWady:\n\nJednostki są podniesione do kwadratu, co utrudnia interpretację\nWrażliwa na wartości odstające\n\n\n\n\n\n\n\nPoprawka Bessela: Dlaczego Dzielimy przez (n-1), a nie po prostu przez n\n\n\n\nGdy obliczamy odchylenia od średniej, ich suma musi wynosić zero. To matematyczny fakt: \\sum(x_i - \\bar{x}) = 0\nPomyśl o tym Tak:\nJeśli masz 5 liczb i ich średnią:\n\nPo obliczeniu 4 odchyleń od średniej\n5-te odchylenie MUSI być takie, żeby suma była zero\nNie masz tak naprawdę 5 niezależnych odchyleń\nMasz tylko 4 prawdziwie “swobodne” odchylenia\n\nProsty Przykład:\nLiczby: 2, 4, 6, 8, 10\n\nŚrednia = 6\nOdchylenia: -4, -2, 0, +2, +4\nZauważ, że sumują się do zera\nJeśli znasz dowolne 4 odchylenia, 5-te jest z góry określone!\n\nDlatego Właśnie:\n\nPrzy obliczaniu wariancji: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nDzielimy przez (n-1), a nie n\nPonieważ tylko (n-1) odchyleń jest naprawdę niezależnych\nOstatnie jest określone przez pozostałe\n\nStopnie Swobody:\n\nn = liczba obserwacji\n1 = ograniczenie (odchylenia muszą sumować się do zera)\nn-1 = stopnie swobody = liczba prawdziwie niezależnych odchyleń\n\nKiedy Stosować:\n\nPrzy obliczaniu wariancji z próby\nPrzy obliczaniu odchylenia standardowego z próby\n\nKiedy NIE Stosować:\n\nW obliczeniach dla całej populacji (gdy mamy wszystkie dane)\nPrzy obliczaniu odchylenia od ustalonej, znanej wartości parametru populacji statystycznej\n\nPamiętaj:\n\nTo nie jest tylko statystyczny trik\nOdchylenia od średniej muszą sumować się do zera\nTo ograniczenie kosztuje nas jeden stopień swobody\n\n\n\n\n\nOdchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji i mierzy przeciętne rozproszenie danych względem ich średniej arytmetycznej. W przeciwieństwie do wariancji, jest to miara mianowana i interpretowana w jednostkach bdanej zmiennej.\nWzór: s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz wariancję\ns^2 = 5,14 (z poprzedniego obliczenia)\n\n\n2\nWyciągnij pierwiastek kwadratowy\ns = \\sqrt{5,14} = 2,27\n\n\n\nObliczenie w R:\n\nsd(dane)\n\n[1] 2.267787\n\n\nZalety:\n\nW tych samych jednostkach co oryginalne dane\nSzeroko stosowane i zrozumiałe\n\nWady:\n\nNadal wrażliwe na wartości odstające\nZakłada, że dane są w przybliżeniu “normalnie” rozłożone\n\n\n\nWspółczynnik zmienności (*)\nWspółczynnik zmienności to odchylenie standardowe podzielone przez średnią arytmetyczną, często wyrażany jako wartość procentowa.\nWzór: CV = \\frac{s}{\\bar{x}} \\times 100\\%\nPrzykład obliczeń ręcznych:\nDla zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nOblicz średnią arytmetyczną\n\\bar{x} = 5,14\n\n\n2\nOblicz odchylenie standardowe\ns = 2,27\n\n\n3\nPodziel s przez średnią i pomnóż przez 100\n(2,27 / 5,14) * 100 = 44,16\\%\n\n\n\nObliczenia w R:\n\n(sd(dane) / mean(dane)) * 100\n\n[1] 44.09586\n\n\nZalety:\n- Umożliwia porównanie zmienności między zbiorami danych o różnych jednostkach lub średnich\n- Przydatny w dziedzinach takich jak finanse do oceny ryzyka\nWady:\n- Nie ma znaczenia dla danych zawierających zarówno wartości dodatnie, jak i ujemne\n- Może być mylący, gdy średnia jest bliska zeru\n\n\n\n\n\n\nOgraniczenia Współczynnika Zmienności (CV)\n\n\n\nWspółczynnik zmienności, obliczany jako (σ/μ) × 100\\%, ma dwa istotne ograniczenia:\n\nNie ma interpretacji dla danych zawierających wartości dodatnie i ujemne\n\nŚrednia może być bliska zeru ze względu na wzajemne znoszenie się wartości dodatnich i ujemnych\nPrzykład: Zbiór danych {-5, -3, 2, 6} ma średnią = 0\n\nCV = (odch. std. / 0) × 100%\nProwadzi to do dzielenia przez zero\nNawet gdy średnia nie jest dokładnie zero, CV nie reprezentuje prawdziwej względnej zmienności, gdy dane przechodzą przez zero\n\nCV zakłada naturalny punkt zerowy i sensowne proporcje między wartościami\n\n\n\nMylący gdy średnia jest bliska zeru\n\nPonieważ CV = (σ/μ) × 100\\%, gdy μ zbliża się do zera:\n\nMianownik staje się bardzo mały\nSkutkuje to ekstremalnie dużymi wartościami CV\nTe duże wartości nie reprezentują sensownie względnej zmienności\n\nPrzykład:\n\nZbiór danych A: {0.001, 0.002, 0.003} ma średnią = 0.002\nNawet małe odchylenia standardowe dadzą bardzo duże CV\nWynikający z tego duży CV może sugerować ekstremalne zróżnicowanie, gdy w rzeczywistości dane są dość skoncentrowane\n\n\n\n\nNajlepsze zastosowania\nCV jest najbardziej użyteczny dla:\n\nDanych ściśle dodatnich\nDanych mierzonych na skali ilorazowej\nDanych ze średnią znacznie powyżej zera\nPorównywania zmienności między zbiorami danych o różnych jednostkach lub skalach",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-położenia-względnego-względnej-pozycji",
    "href": "rozdzial5.html#miary-położenia-względnego-względnej-pozycji",
    "title": "6  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.8 Miary Położenia Względnego (Względnej Pozycji)",
    "text": "6.8 Miary Położenia Względnego (Względnej Pozycji)\nZrozumienie relatywnej (względnej) pozycji wartości w zbiorze danych.\n\nKwartyle (Q): Podstawy\nKwartyle to specjalne liczby, które dzielą uporządkowane dane na cztery równe części.\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\n\nCzym są Kwartyle?\nPierwszy Kwartyl (Q1):\n\nOddziela najniższe 25% danych od reszty\nNazywany również 25-tym percentylem\nPrzykład: Jeśli Q1 = 50 w zbiorze wyników testu, 25% uczniów uzyskało wynik poniżej 50\n\nDrugi Kwartyl (Q2):\n\nMediana - dzieli dane na pół\nNazywany również 50-tym percentylem\nPrzykład: Jeśli Q2 = 70, połowa uczniów uzyskała wynik poniżej 70\n\nTrzeci Kwartyl (Q3):\n\nOddziela najwyższe 25% danych od reszty\nNazywany również 75-tym percentylem\nPrzykład: Jeśli Q3 = 85, 75% uczniów uzyskało wynik poniżej 85\n\nZadanie 1: Kwartyle\nDane: 10, 12, 15, 15, 18, 20, 22, 25, 25 Znajdź: Q1, Q2, Q3\nRozwiązanie:\n\nQ2 (n = 9, nieparzyste)\n\nPozycja = (9 + 1)/2 = 5\nQ2 = 18\n\nQ1\n\nPozycja = (9 + 1)/4 = 2.5\nMiędzy 12 a 15\nQ1 = (12 + 15)/2 = 13.5\n\nQ3\n\nPozycja = 3(9 + 1)/4 = 7.5\nMiędzy 22 a 25\nQ3 = (22 + 25)/2 = 23.5\n\n\n\n\nJak Obliczać Kwartyle (Krok po Kroku) - Dwie Metody\nPrzeanalizujmy wyniki testów uczniów używając obu popularnych metod wyznaczania kwartyli:\nPrzykład 1: Przypadek Nieparzystej Liczby Wyników (11 wyników)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 88, 90\nKrok 1: Znajdź Q2 (medianę) - Tak samo dla obu metod\n\nPrzy n = 11 wartościach (nieparzyste)\nPozycja mediany = 2(n + 1)/4 = (n + 1)/2 = 6\nQ2 = 78\n\nKrok 2: Znajdź Q1\n\nMetoda Tukeya:\n\nSpójrz na dolną połowę: 60, 65, 70, 72, 75\nQ1 = mediana dolnej połowy = 70\n\nMetoda Interpolacji:\n\nPozycja = (n + 1)/4 = (11 + 1)/4 = 3\nQ1 = 70 (3-cia wartość)\n\n\nKrok 3: Znajdź Q3\n\nMetoda Tukeya:\n\nSpójrz na górną połowę: 80, 82, 85, 88, 90\nQ3 = mediana górnej połowy = 85\n\nMetoda Interpolacji:\n\nPozycja = 3(n + 1)/4 = 3(12)/4 = 9\nQ3 = 85 (9-ta wartość)\n\n\nPrzykład 2: Przypadek Parzystej Liczby (10 wyników)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 90\nKrok 1: Znajdź Q2 (medianę) - Tak samo dla obu metod\n\nPrzy n = 10 wartościach (parzyste)\nPozycje mediany = 5 i 6\nQ2 = (75 + 78)/2 = 76.5\n\nKrok 2: Znajdź Q1\n\nMetoda Tukeya:\n\nSpójrz na dolną połowę: 60, 65, 70, 72, 75\nQ1 = mediana dolnej połowy = 70\n\nMetoda Interpolacji:\n\nPozycja = (10 + 1)/4 = 2.75\nQ1 = 65 + 0.75(70 - 65) = 68.75\n\n\nKrok 3: Znajdź Q3\n\nMetoda Tukeya:\n\nSpójrz na górną połowę: 78, 80, 82, 85, 90\nQ3 = mediana górnej połowy = 82\n\nMetoda Interpolacji:\n\nPozycja = 3(10 + 1)/4 = 8.25\nQ3 = 82 + 0.25(85 - 82) = 82.75\n\n\nWażne Uwagi:\n\nMetoda Tukeya:\n\nNajpierw znajdź medianę (Q2)\nPodziel dane na dolną i górną połowę\nZnajdź Q1 jako medianę dolnej połowy\nZnajdź Q3 jako medianę górnej połowy\nGdy n jest nieparzyste, mediana nie jest uwzględniana w żadnej połowie\n\nMetoda Interpolacji:\n\nUżywa pozycji (n+1)/4 dla Q1 i 3(n+1)/4 dla Q3\nGdy pozycja wypada między wartościami, stosuje interpolację liniową\nNie wymaga podziału danych na połowy\n\n\n\n\n\n\n\n\nManual Construction of Tukey Boxplot\n\n\n\nStep 1: Calculate Key Components\n\nFind quartiles: Q_1, Q_2 (median), Q_3\nCalculate Interquartile Range: IQR = Q_3 - Q_1\n\nStep 2: Determine Whisker Boundaries\n\nLower fence: Q_1 - 1.5 \\times IQR\nUpper fence: Q_3 + 1.5 \\times IQR\n\nStep 3: Identify Outliers Data points are outliers if they are:\n\nBelow lower fence: x &lt; Q_1 - 1.5 \\times IQR\nAbove upper fence: x &gt; Q_3 + 1.5 \\times IQR\n\nExample: Given data: 2, 4, 6, 8, 9, 10, 11, 12, 14, 16, 50\n\nFind quartiles:\n\nQ_1 = 6\nQ_2 = 10\nQ_3 = 14\n\nCalculate IQR:\n\nIQR = 14 - 6 = 8\n\nCalculate fences:\n\nLower: 6 - (1.5 \\times 8) = -6\nUpper: 14 + (1.5 \\times 8) = 26\n\nIdentify outliers:\n\n50 &gt; 26, therefore 50 is an outlier\n\n\nGraphical Elements:\n\nBox: Draw from Q_1 to Q_3\nLine inside box: Draw at Q_2\nWhiskers: Extend to most extreme non-outlier points\nPoints: Plot outliers individually beyond whiskers\n\n\n\n\n\n\nPercentyle: Bardziej Precyzyjna Miara Względnej Pozycji (*)\n\nCzym są Percentyle?\nPercentyle dają nam bardziej szczegółowy obraz, dzieląc dane na 100 równych części.\nKluczowe Punkty:\n\n25-ty percentyl równa się Q1\n50-ty percentyl równa się Q2 (mediana)\n75-ty percentyl równa się Q3\n\n\n\nObliczanie Percentyli\nWzór: P_k = \\frac{k(n+1)}{100}\nGdzie:\n\nP_k to pozycja dla k-tego percentyla\nk to percentyl, który chcemy znaleźć (1-100)\nn to liczba obserwacji\n\nPrzykład 3: Znajdowanie 60-tego Percentyla Użyjmy wyników zadań domowych uczniów: 72, 75, 78, 80, 82, 85, 88, 90, 92, 95\nKrok 1: Oblicz pozycję\n\nn = 10 wyników\nDla 60-tego percentyla: P_{60} = \\frac{60(10+1)}{100} = 6.6\n\nKrok 2: Znajdź otaczające wartości\n\nPozycja 6: wynik 85\nPozycja 7: wynik 88\n\nKrok 3: Interpoluj (ważne: percentyle używają interpolacji liniowej)\n\nMusimy przejść 0.6 drogi między 85 a 88 P_{60} = 85 + 0.6(88-85) P_{60} = 85 + 0.6(3) P_{60} = 85 + 1.8 = 86.8\n\nCo to oznacza: 60% uczniów uzyskało wynik 86.8 lub niższy.\n\n\n\nRangi Percentylowe (PR) (*)\n\nCzym jest Ranga Percentylowa?\nPodczas gdy percentyle mówią nam o wartości na określonej pozycji, ranga percentylowa mówi nam, jaki procent wartości znajduje się poniżej określonego wyniku. Można to traktować jako odpowiedź na pytanie “Jaki procent klasy uzyskał wynik niższy niż ja?”\nPR = \\frac{\\text{liczba wartości poniżej } + 0.5 \\times \\text{liczba równych wartości}}{\\text{całkowita liczba wartości}} \\times 100\nPrzykład 4: Znajdowanie Rangi Percentylowej Rozważmy te wyniki egzaminu:\n65, 70, 70, 75, 75, 75, 80, 85, 85, 90\nZnajdźmy PR dla wyniku 75.\nKrok 1: Dokładnie policz\n\nWartości poniżej 75: 65, 70, 70 (3 wartości)\nWartości równe 75: 75, 75, 75 (3 wartości)\nCałkowita liczba wartości: 10\n\nKrok 2: Zastosuj wzór\nPR = \\frac{3 + 0.5(3)}{10} \\times 100 PR = \\frac{3 + 1.5}{10} \\times 100 PR = \\frac{4.5}{10} \\times 100 = 45\\%\nInterpretacja: Wynik 75 jest wyższy niż 45% wyników w klasie.\nUwaga:\nP1: “Dlaczego używamy 0.5 dla równych wartości w PR?”\nO1: Jest tak, ponieważ zakładamy, że osoby z tym samym wynikiem są równomiernie rozłożone na tej pozycji. To jak powiedzenie, że dzielą pozycję po równo.\n\n\n\n\n\n\nPodwójna Rola Mediany\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.2: Wizualizacja podwójnej roli mediany\n\n\n\n\n\nMediana pełni dwie odrębne, ale powiązane ze sobą role:\nA. Jako Miara Centrum:\n\nReprezentuje środkowy punkt danych\nRównoważy liczbę obserwacji po obu stronach\nJest odporna na wartości odstające (w przeciwieństwie do średniej arytmetycznej)\n\nB. Jako Miara Pozycji Względnej:\n\nWyznacza 50-ty percentyl\nDzieli dane na dwie równe części\nKażdą wartość można do niej odnieść:\n\nPoniżej mediany: dolne 50%\nPowyżej mediany: górne 50%\n\n\nTa podwójna natura sprawia, że mediana jest szczególnie przydatna do:\n\nOpisywania wartości typowych (tendencja centralna)\nZrozumienia pozycji w rozkładzie (pozycja względna)\nDokonywania porównań między różnymi zbiorami danych\n\n\n\n\n\n\nWykres pudełkowy\nWykresy pudełkowe (znane również jako wykresy skrzynkowe lub box-and-whisker plots) są użytecznymi narzędziami wizualizacji rozkładów danych.\n\nKonstrukcja wykresu pudełkowego Tukeya\nWykres pudełkowy został wprowadzony przez Johna Tukeya jako część jego zestawu narzędzi eksploracyjnej analizy danych. Wykres wizualizuje rozkład danych na podstawie pięciu podstawowych statystyk.\n\nPodsumowanie pięciu liczb\nWykres pudełkowy reprezentuje pięć kluczowych wartości statystycznych:\n\nMinimum: Najmniejsza wartość w zbiorze danych (z wyłączeniem wartości odstających)\nPierwszy kwartyl (Q1): 25. percentyl, poniżej którego znajduje się 25% obserwacji\nMediana (Q2): 50. percentyl, który dzieli zbiór danych na dwie równe połowy\nTrzeci kwartyl (Q3): 75. percentyl, poniżej którego znajduje się 75% obserwacji\nMaksimum: Największa wartość w zbiorze danych (z wyłączeniem wartości odstających)\n\n\n\nKomponenty wykresu pudełkowego\n\n\n\n\n\n\n\n\nFigure 6.3: Diagram wykresu pudełkowego pokazujący jego kluczowe komponenty.\n\n\n\n\n\nKomponenty wykresu pudełkowego obejmują:\n\nPudełko:\n\nReprezentuje rozstęp międzykwartylowy (IQR), zawierający środkowe 50% danych\nDolna krawędź reprezentuje Q1\nGórna krawędź reprezentuje Q3\nLinia wewnątrz pudełka reprezentuje medianę (Q2)\n\nWąsy:\n\nRozciągają się od pudełka, aby pokazać zakres danych niebędących wartościami odstającymi\nW wykresie pudełkowym Tukeya wąsy rozciągają się do 1,5 × IQR od krawędzi pudełka:\n\nDolny wąs: rozciąga się do minimalnej wartości ≥ (Q1 - 1,5 × IQR)\nGórny wąs: rozciąga się do maksymalnej wartości ≤ (Q3 + 1,5 × IQR)\n\n\nWartości odstające:\n\nPunkty, które wykraczają poza wąsy\nIndywidualnie zaznaczone jako kropki lub inne symbole\nWartości, które są &lt; (Q1 - 1,5 × IQR) lub &gt; (Q3 + 1,5 × IQR)\n\n\n\n\nKluczowe cechy do obserwacji\nInterpretując wykresy pudełkowe, zwróć uwagę na następujące cechy:\n\nTendencja centralna: Położenie linii mediany wewnątrz pudełka\nRozproszenie: Szerokość pudełka (IQR) i długość wąsów\nSkośność:\n\nDane symetryczne: mediana znajduje się w przybliżeniu na środku pudełka, wąsy mają podobną długość\nSkośność prawostronna (dodatnia): mediana jest bliżej dolnej części pudełka, górny wąs jest dłuższy\nSkośność lewostronna (ujemna): mediana jest bliżej górnej części pudełka, dolny wąs jest dłuższy\n\nWartości odstające: Obecność pojedynczych punktów poza wąsami\n\n\n\n\nStudium przypadku: Porównanie wzrostu między grupami\nZastosujmy nasze zrozumienie wykresów pudełkowych do rzeczywistego zbioru danych. Mamy pomiary wzrostu (w centymetrach) z dwóch grup, każda po 25 studentów.\n\n# Utworzenie zbioru danych\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Przekształcenie zbioru danych z formatu szerokiego na długi\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Wyświetlenie pierwszych kilku wierszy\nhead(data_height_l)\n\n  Group_number height\n1      group_1    150\n2      group_1    160\n3      group_1    165\n4      group_1    168\n5      group_1    172\n6      group_1    173\n\n\nObliczmy kilka statystyk podsumowujących dla każdej grupy:\n\n# Obliczenie statystyk podsumowujących dla każdej grupy\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Obliczenie IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Utworzenie tabeli porównawczej\nstats_table &lt;- rbind(\n  group1_stats,\n  group2_stats\n)\nrownames(stats_table) &lt;- c(\"Grupa 1\", \"Grupa 2\")\n\n# Wyświetlenie tabeli\nstats_table\n\n        Min. 1st Qu. Median Mean 3rd Qu. Max.\nGrupa 1  150     175    180  179     183  200\nGrupa 2  138     165    175  172     182  210\n\n# Wyświetlenie wartości IQR\ncat(\"IQR dla Grupy 1:\", group1_iqr, \"\\n\")\n\nIQR dla Grupy 1: 8 \n\ncat(\"IQR dla Grupy 2:\", group2_iqr, \"\\n\")\n\nIQR dla Grupy 2: 17 \n\n\n\n\nWizualizacja danych dotyczących wzrostu\nTeraz zwizualizujmy dane za pomocą wykresów pudełkowych i wykresów gęstości:\n\n# Tworzenie poziomych wykresów pudełkowych\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Rozkład wzrostu według grupy\",\n       x = \"Grupa\",\n       y = \"Wzrost (cm)\")\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\n\n\n\n\n\n\nFigure 6.4: Wykresy pudełkowe porównujące rozkłady wzrostu między grupami.\n\n\n\n\n\nAby uzupełnić nasze wykresy pudełkowe, przyjrzyjmy się również rozkładom gęstości:\n\n# Tworzenie wykresów gęstości\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Gęstość wzrostu według grupy\",\n       x = \"Wzrost (cm)\",\n       y = \"Gęstość\")\n\n\n\n\n\n\n\nFigure 6.5: Wykresy gęstości pokazujące rozkłady wzrostu dla każdej grupy.\n\n\n\n\n\n\n\nĆwiczenie z interpretacji wykresów pudełkowych\nNa podstawie powyższych wykresów pudełkowych i wykresów gęstości określ, czy każde z poniższych stwierdzeń jest Prawdziwe czy Fałszywe. Dla każdego stwierdzenia podaj krótkie wyjaśnienie oparte na dowodach z wizualizacji.\n\n\n\n\n\n\nPytania ćwiczeniowe\n\n\n\n\nStudenci z grupy 2 (G2) w badanej próbie są, średnio, wyżsi niż ci z grupy 1 (G1).\nWzrost w grupie 1 (G1) jest bardziej rozproszony/rozłożony niż w grupie 2 (G2).\nNajniższa osoba jest w grupie 2 (G2).\nOba zbiory danych mają skośność ujemną (lewostronną).\nPołowa studentów w grupie 2 (G2) ma wzrost co najmniej 175 cm.\n\n\n\n\nWskazówki do interpretacji\nOdpowiadając na te pytania, weź pod uwagę:\n\nPozycję linii mediany w każdym pudełku\nWzględne rozmiary pudełek (IQR)\nPozycje wartości minimalnych i maksymalnych\nSymetrię rozkładów (zrównoważone czy z skośnością)\nDługości wąsów\n\nDla każdego stwierdzenia ustal, czy jest Prawdziwe czy Fałszywe i podaj swoje wyjaśnienie:\n\n\n\n\n\n\nSzablon odpowiedzi\n\n\n\n\n\n\nStudenci z G2 są, średnio, wyżsi niż z G1: [Prawda/Fałsz]\n\nWyjaśnienie:\n\nWzrost G1 jest bardziej rozproszony/rozłożony: [Prawda/Fałsz]\n\nWyjaśnienie:\n\nNajniższa osoba jest w G2: [Prawda/Fałsz]\n\nWyjaśnienie:\n\nOba zbiory danych mają skośność ujemną (lewostronną): [Prawda/Fałsz]\n\nWyjaśnienie:\n\nPołowa G2 ma wzrost co najmniej 175 cm: [Prawda/Fałsz]\n\nWyjaśnienie:\n\n\n\n\n\nPrzeanalizujmy odpowiedzi na nasze pytania dotyczące interpretacji wykresów pudełkowych:\n\n\n\n\n\n\nRozwiązania\n\n\n\n\n\n\nStudenci z G2 są, średnio, wyżsi niż z G1: Fałsz\n\nWyjaśnienie: Mediana wzrostu (środkowa linia w wykresie pudełkowym) dla G1 jest wyższa niż dla G2.\n\nWzrost G1 jest bardziej rozproszony/rozłożony: Fałsz\n\nWyjaśnienie: G2 wykazuje większe rozproszenie. Jest to widoczne na wykresie pudełkowym, gdzie G2 ma większy rozstęp międzykwartylowy (IQR) wynoszący 17,5 cm w porównaniu z 9,5 cm dla G1. G2 ma również szerszy zakres od wartości minimalnej do maksymalnej.\n\nNajniższa osoba jest w G2: Prawda\n\nWyjaśnienie: Wartość minimalna w G2 wynosi 138 cm, co jest niższe niż wartość minimalna w G1 (150 cm).\n\nOba zbiory danych mają skośność ujemną (lewostronną): Prawda\n\nWyjaśnienie: W obu grupach linia mediany jest umieszczona w kierunku górnej części pudełka, a dolny wąs jest dłuższy niż górny. Wskazuje to na dłuższy ogon po lewej stronie rozkładu, co oznacza skośność ujemną.\n\nPołowa G2 ma wzrost co najmniej 175 cm: Prawda\n\nWyjaśnienie: Mediana (środkowa linia w wykresie pudełkowym) dla G2 wynosi 175 cm, co oznacza, że 50% wartości jest większych lub równych 175 cm.\n\n\n\n\n\n\n\n\nKod R\n\n# Wczytanie wymaganych pakietów\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n# Ustawienie opcji wyświetlania\noptions(scipen = 999, digits = 3)\n\n# Utworzenie zbioru danych\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Przekształcenie zbioru danych z formatu szerokiego na długi\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Wyświetlenie pierwszych kilku wierszy\nhead(data_height_l)\n\n# Obliczenie statystyk podsumowujących dla każdej grupy\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Obliczenie IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Tworzenie poziomych wykresów pudełkowych\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Rozkład wzrostu według grupy\",\n       x = \"Grupa\",\n       y = \"Wzrost (cm)\")\n\n# Tworzenie wykresów gęstości\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Gęstość wzrostu według grupy\",\n       x = \"Wzrost (cm)\",\n       y = \"Gęstość\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-kształtu",
    "href": "rozdzial5.html#miary-kształtu",
    "title": "6  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.9 Miary Kształtu",
    "text": "6.9 Miary Kształtu\n\nSkośność\n\nDefinicja\nSkośność kwantyfikuje asymetrię rozkładu danych. Wskazuje, czy dane grupują się bardziej po jednej stronie średniej niż po drugiej.\n\n\nWyrażenie Matematyczne\nSK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3 gdzie: - n to wielkość próby - x_i to i-ta obserwacja - \\bar{x} to średnia z próby - s to odchylenie standardowe z próby\n\n\nUproszczony Przykład Numeryczny\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Trzy przykładowe zestawy danych z różnymi typami skośności\n# 1. Skośność dodatnia (prawy ogon)\ndane_skosnosc_dodatnia &lt;- c(2, 3, 4, 4, 5, 5, 5, 6, 6, 7, 8, 12, 15, 20)\n# 2. Skośność ujemna (lewy ogon)\ndane_skosnosc_ujemna &lt;- c(1, 5, 10, 13, 14, 15, 16, 16, 17, 17, 18, 18, 19, 20)\n# 3. Skośność bliska zeru (symetria)\ndane_skosnosc_symetryczna &lt;- c(1, 3, 5, 7, 9, 10, 11, 12, 13, 15, 17, 19, 21)\n\n# Obliczenie skośności\nskosnosc_dodatnia &lt;- skewness(dane_skosnosc_dodatnia)\nskosnosc_ujemna &lt;- skewness(dane_skosnosc_ujemna)\nskosnosc_symetryczna &lt;- skewness(dane_skosnosc_symetryczna)\n\n# Zestawienie wyników\ndane_skosnosci &lt;- data.frame(\n  \"Typ rozkładu\" = c(\"Skośność dodatnia\", \"Skośność ujemna\", \"Rozkład symetryczny\"),\n  \"Wartość skośności\" = round(c(skosnosc_dodatnia, skosnosc_ujemna, skosnosc_symetryczna), 3),\n  \"Interpretacja\" = c(\n    \"Dłuższy prawy ogon (większość danych po lewej stronie)\",\n    \"Dłuższy lewy ogon (większość danych po prawej stronie)\",\n    \"Dane rozłożone symetrycznie\"\n  )\n)\n\n# Wyświetlenie tabeli\ndane_skosnosci\n\n         Typ.rozkładu Wartość.skośności\n1   Skośność dodatnia              1.42\n2     Skośność ujemna             -1.33\n3 Rozkład symetryczny              0.00\n                                           Interpretacja\n1 Dłuższy prawy ogon (większość danych po lewej stronie)\n2 Dłuższy lewy ogon (większość danych po prawej stronie)\n3                            Dane rozłożone symetrycznie\n\n\n\n\nWizualizacje Typów Skośności\n\n# Tworzymy ramkę danych dla wszystkich zestawów\ndf_skosnosc &lt;- rbind(\n  data.frame(wartosc = dane_skosnosc_dodatnia, typ = \"Skośność dodatnia\", \n             skosnosc = round(skosnosc_dodatnia, 2)),\n  data.frame(wartosc = dane_skosnosc_ujemna, typ = \"Skośność ujemna\", \n             skosnosc = round(skosnosc_ujemna, 2)),\n  data.frame(wartosc = dane_skosnosc_symetryczna, typ = \"Rozkład symetryczny\", \n             skosnosc = round(skosnosc_symetryczna, 2))\n)\n\n# Histogramy dla trzech typów skośności\np1 &lt;- ggplot(df_skosnosc, aes(x = wartosc)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~typ, scales = \"free_x\") +\n  geom_vline(data = df_skosnosc %&gt;% group_by(typ) %&gt;% summarise(mean = mean(wartosc)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_skosnosc %&gt;% group_by(typ) %&gt;% summarise(median = median(wartosc)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = unique(df_skosnosc[, c(\"typ\", \"skosnosc\")]),\n           aes(x = Inf, y = Inf, label = paste(\"SK =\", skosnosc)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histogramy pokazujące różne typy skośności\",\n    subtitle = \"Czerwona linia: średnia, Zielona linia: mediana\",\n    x = \"Wartość\",\n    y = \"Częstość\"\n  ) +\n  theme_minimal()\n\n# Wykresy pudełkowe\np2 &lt;- ggplot(df_skosnosc, aes(x = typ, y = wartosc, fill = typ)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"skyblue\", \"lightgreen\", \"lightsalmon\")) +\n  labs(\n    title = \"Wykresy pudełkowe dla różnych typów skośności\",\n    x = \"Typ rozkładu\",\n    y = \"Wartość\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Wyświetlenie wykresów\ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nPrzykład: Analiza Frekwencji Wyborczej\n\n# Generujemy trzy zestawy danych odzwierciedlające różne typy skośności\nset.seed(123)\n\n# 1. Skośność dodatnia - typowa dla frekwencji w regionach o niskim zaangażowaniu\nfrekwencja_dodatnia &lt;- c(\n  runif(50, min = 20, max = 30),  # Mała grupa z niską frekwencją\n  rbeta(200, shape1 = 2, shape2 = 5) * 50 + 30  # Większość wyników przesuniętych w lewo\n)\n\n# 2. Skośność ujemna - typowa dla regionów z wysokim zaangażowaniem politycznym\nfrekwencja_ujemna &lt;- c(\n  rbeta(200, shape1 = 5, shape2 = 2) * 30 + 50,  # Większość wyników przesuniętych w prawo\n  runif(50, min = 40, max = 50)  # Mała grupa z niższą frekwencją\n)\n\n# 3. Rozkład symetryczny - typowy dla regionów z równomiernym zaangażowaniem\nfrekwencja_symetryczna &lt;- rnorm(250, mean = 65, sd = 8)\n\n# Tworzymy ramkę danych\ndf_frekwencja &lt;- rbind(\n  data.frame(frekwencja = frekwencja_dodatnia, region = \"Region A: Skośność dodatnia\"),\n  data.frame(frekwencja = frekwencja_ujemna, region = \"Region B: Skośność ujemna\"),\n  data.frame(frekwencja = frekwencja_symetryczna, region = \"Region C: Rozkład symetryczny\")\n)\n\n# Obliczamy skośność dla każdego regionu\nskosnosci_regionow &lt;- df_frekwencja %&gt;%\n  group_by(region) %&gt;%\n  summarise(skosnosc = round(skewness(frekwencja), 2))\n\n# Histogram frekwencji według regionów\np3 &lt;- ggplot(df_frekwencja, aes(x = frekwencja)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~region, ncol = 1) +\n  geom_vline(data = df_frekwencja %&gt;% group_by(region) %&gt;% summarise(mean = mean(frekwencja)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_frekwencja %&gt;% group_by(region) %&gt;% summarise(median = median(frekwencja)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = skosnosci_regionow,\n           aes(x = 25, y = 20, label = paste(\"SK =\", skosnosc)),\n           size = 3.5) +\n  labs(\n    title = \"Frekwencja wyborcza w różnych regionach\",\n    subtitle = \"Pokazuje trzy rodzaje skośności\",\n    x = \"Frekwencja wyborcza (%)\",\n    y = \"Liczba obwodów\"\n  ) +\n  theme_minimal()\n\n# Wykres pudełkowy\np4 &lt;- ggplot(df_frekwencja, aes(x = region, y = frekwencja, fill = region)) +\n  geom_boxplot() +\n  labs(\n    title = \"Porównanie rozkładów frekwencji w regionach\",\n    x = \"Region\",\n    y = \"Frekwencja wyborcza (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Wyświetlenie wykresów\ngrid.arrange(p3, p4, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nPrzewodnik Interpretacji\n\nSkośność Dodatnia (&gt; 0): Rozkład ma dłuższy ogon prawy - większość wartości jest skupiona po lewej stronie\nSkośność Ujemna (&lt; 0): Rozkład ma dłuższy ogon lewy - większość wartości jest skupiona po prawej stronie\nSkośność Zero: Rozkład w przybliżeniu symetryczny - wartości rozłożone równomiernie wokół średniej\n\n\n\n\nKurtoza\n\nDefinicja\nKurtoza mierzy “ogoniastość” rozkładu, wskazując na obecność wartości ekstremalnych w porównaniu z rozkładem normalnym.\n\n\nWyrażenie Matematyczne\nK = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\n\n\nUproszczony Przykład Numeryczny\n\n# Trzy przykładowe zestawy danych z różnymi poziomami kurtozy\n# 1. Rozkład leptokurtyczny (wysoka kurtoza, \"ciężkie ogony\")\ndane_leptokurtyczne &lt;- c(\n  rnorm(80, mean = 50, sd = 5),  # Większość danych skupiona wokół średniej\n  c(20, 25, 30, 70, 75, 80)      # Kilka wartości ekstremalnych\n)\n\n# 2. Rozkład platykurtyczny (niska kurtoza, \"płaski\")\ndane_platykurtyczne &lt;- c(\n  runif(50, min = 30, max = 70)  # Równomierny rozkład wartości\n)\n\n# 3. Rozkład mezokurtyczny (normalna kurtoza)\ndane_mezokurtyczne &lt;- rnorm(50, mean = 50, sd = 10)\n\n# Obliczenie kurtozy\nkurtoza_lepto &lt;- kurtosis(dane_leptokurtyczne)\nkurtoza_platy &lt;- kurtosis(dane_platykurtyczne)\nkurtoza_mezo &lt;- kurtosis(dane_mezokurtyczne)\n\n# Zestawienie wyników\ndane_kurtozy &lt;- data.frame(\n  \"Typ rozkładu\" = c(\"Leptokurtyczny\", \"Platykurtyczny\", \"Mezokurtyczny\"),\n  \"Wartość kurtozy\" = round(c(kurtoza_lepto, kurtoza_platy, kurtoza_mezo), 3),\n  \"Interpretacja\" = c(\n    \"Wiele wartości blisko średniej, ale też więcej wartości ekstremalnych\",\n    \"Wartości rozłożone bardziej równomiernie - płaski rozkład\",\n    \"Podobny do rozkładu normalnego\"\n  )\n)\n\n# Wyświetlenie tabeli\ndane_kurtozy\n\n    Typ.rozkładu Wartość.kurtozy\n1 Leptokurtyczny            7.39\n2 Platykurtyczny            1.85\n3  Mezokurtyczny            2.25\n                                                          Interpretacja\n1 Wiele wartości blisko średniej, ale też więcej wartości ekstremalnych\n2             Wartości rozłożone bardziej równomiernie - płaski rozkład\n3                                        Podobny do rozkładu normalnego\n\n\n\n\nWizualizacje Poziomów Kurtozy\n\n# Tworzymy ramkę danych dla wszystkich zestawów\ndf_kurtoza &lt;- rbind(\n  data.frame(wartosc = dane_leptokurtyczne, typ = \"Leptokurtyczny (K &gt; 3)\", \n             kurtoza = round(kurtoza_lepto, 2)),\n  data.frame(wartosc = dane_platykurtyczne, typ = \"Platykurtyczny (K &lt; 3)\", \n             kurtoza = round(kurtoza_platy, 2)),\n  data.frame(wartosc = dane_mezokurtyczne, typ = \"Mezokurtyczny (K ≈ 3)\", \n             kurtoza = round(kurtoza_mezo, 2))\n)\n\n# Histogramy dla trzech typów kurtozy\np5 &lt;- ggplot(df_kurtoza, aes(x = wartosc)) +\n  geom_histogram(bins = 15, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~typ, scales = \"free_y\") +\n  geom_text(data = unique(df_kurtoza[, c(\"typ\", \"kurtoza\")]),\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtoza)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histogramy pokazujące różne poziomy kurtozy\",\n    x = \"Wartość\",\n    y = \"Częstość\"\n  ) +\n  theme_minimal()\n\n# Wykresy pudełkowe\np6 &lt;- ggplot(df_kurtoza, aes(x = typ, y = wartosc, fill = typ)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"lightgreen\", \"lightsalmon\", \"skyblue\")) +\n  labs(\n    title = \"Wykresy pudełkowe dla różnych poziomów kurtozy\",\n    x = \"Typ rozkładu\",\n    y = \"Wartość\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Wyświetlenie wykresów\ngrid.arrange(p5, p6, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nPrzykład: Analiza Głosowań Parlamentarnych\n\n# Generujemy trzy zestawy danych odzwierciedlające różne poziomy kurtozy\nset.seed(456)\n\n# 1. Rozkład leptokurtyczny - typowy dla głosowań z silną dyscypliną partyjną\nglosowania_lepto &lt;- c(\n  rnorm(150, mean = 75, sd = 3),  # Większość głosowań z wysoką zgodnością\n  c(20, 25, 30, 35, 40, 95, 96, 97, 98, 99)  # Kilka głosowań odstających\n)\n\n# 2. Rozkład platykurtyczny - typowy dla głosowań kontrowersyjnych\nglosowania_platy &lt;- c(\n  runif(80, min = 40, max = 60),  # Głosowania z umiarkowaną zgodnością\n  runif(80, min = 60, max = 80)   # Głosowania z wyższą zgodnością\n)\n\n# 3. Rozkład mezokurtyczny - typowy dla normalnych głosowań\nglosowania_mezo &lt;- rnorm(160, mean = 65, sd = 10)\n\n# Tworzymy ramkę danych\ndf_glosowania &lt;- rbind(\n  data.frame(zgodnosc = glosowania_lepto, typ_ustawy = \"Ustawy A: Leptokurtyczne\"),\n  data.frame(zgodnosc = glosowania_platy, typ_ustawy = \"Ustawy B: Platykurtyczne\"),\n  data.frame(zgodnosc = glosowania_mezo, typ_ustawy = \"Ustawy C: Mezokurtyczne\")\n)\n\n# Obliczamy kurtozę dla każdego typu ustaw\nkurtozy_ustaw &lt;- df_glosowania %&gt;%\n  group_by(typ_ustawy) %&gt;%\n  summarise(kurtoza = round(kurtosis(zgodnosc), 2))\n\n# Histogram zgodności głosowań\np7 &lt;- ggplot(df_glosowania, aes(x = zgodnosc)) +\n  geom_histogram(bins = 20, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~typ_ustawy, ncol = 1) +\n  geom_text(data = kurtozy_ustaw,\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtoza)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Zgodność głosowań dla różnych typów ustaw\",\n    subtitle = \"Pokazuje trzy poziomy kurtozy\",\n    x = \"Wskaźnik zgodności głosowań (%)\",\n    y = \"Liczba głosowań\"\n  ) +\n  theme_minimal()\n\n# Wykres pudełkowy\np8 &lt;- ggplot(df_glosowania, aes(x = typ_ustawy, y = zgodnosc, fill = typ_ustawy)) +\n  geom_boxplot() +\n  labs(\n    title = \"Porównanie rozkładów zgodności głosowań\",\n    x = \"Typ ustawy\",\n    y = \"Wskaźnik zgodności głosowań (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Wyświetlenie wykresów\ngrid.arrange(p7, p8, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nPrzewodnik Interpretacji\n\nLeptokurtyczny (K &gt; 3): “Wysmukły” rozkład z ciężkimi ogonami - więcej wartości skrajnych niż w rozkładzie normalnym\nPlatykurtyczny (K &lt; 3): “Płaski” rozkład - mniej wartości skrajnych niż w rozkładzie normalnym\nMezokurtyczny (K ≈ 3): Rozkład podobny do normalnego pod względem wartości ekstremalnych",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ćwiczenie-1.-porównanie-wynagrodzeń",
    "href": "rozdzial5.html#ćwiczenie-1.-porównanie-wynagrodzeń",
    "title": "6  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.10 Ćwiczenie 1. Porównanie wynagrodzeń",
    "text": "6.10 Ćwiczenie 1. Porównanie wynagrodzeń\n\nDane\nMamy dane o wynagrodzeniach (w tysiącach euro) z dwóch małych firm europejskich:\n\n\n\nIndex\nFirma X\nFirma Y\n\n\n\n\n1\n2\n3\n\n\n2\n2\n3\n\n\n3\n2\n4\n\n\n4\n3\n4\n\n\n5\n3\n4\n\n\n6\n3\n4\n\n\n7\n3\n4\n\n\n8\n3\n4\n\n\n9\n3\n5\n\n\n10\n4\n5\n\n\n11\n4\n5\n\n\n12\n4\n5\n\n\n13\n4\n5\n\n\n14\n4\n5\n\n\n15\n5\n6\n\n\n16\n5\n6\n\n\n17\n5\n6\n\n\n18\n5\n7\n\n\n19\n20\n7\n\n\n20\n35\n8\n\n\n\n\n\nMiary tendencji centralnej\n\nŚrednia arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez ich liczbę.\nWzór: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\nMożna też zapisać ten wzór w postaci:\n\\bar{x} = \\frac{\\sum_{i=1}^{k} x_i f_i}{n}\ngdzie f_i to częstość bezwzględna (liczba wystąpień, waga bezwzględna) i-tej wartości, a k to liczba różnych wartości cechy (liczba wartości wyróżnionych).\nZ użyciem częstości względnych:\n\\bar{x} = \\sum_{i=1}^{k} x_i p_i\ngdzie p_i to częstość względna (frakcja, waga znormalizowana) i-tej wartości, a k to liczba różnych wartości cechy (liczba wartości wyróżnionych).\n\nObliczenia ręczne dla Firmy X\n\n\n\nWartość (x_i)\nCzęstość (f_i)\nx_i \\cdot f_i\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nSuma\nn = 20\nSuma = 119\n\n\n\n\\bar{x} = \\frac{119}{20} = 5,95\n\n\nObliczenia ręczne dla Firmy Y\n\n\n\nWartość (x_i)\nCzęstość (f_i)\nx_i \\cdot f_i\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nSuma\nn = 20\nSuma = 100\n\n\n\n\\bar{y} = \\frac{100}{20} = 5\n\n\nWeryfikacja w R\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\nMediana\nMediana to wartość środkowa w uporządkowanym zbiorze danych.\n\nObliczenia ręczne dla Firmy X\nUporządkowane dane: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\nn = 20 (parzyste), więc bierzemy średnią z 10. i 11. wartości:\nMediana = \\frac{4 + 4}{2} = 4\n\n\nObliczenia ręczne dla Firmy Y\nUporządkowane dane: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\nn = 20 (parzyste), więc bierzemy średnią z 10. i 11. wartości:\nMediana = \\frac{5 + 5}{2} = 5\n\n\nWeryfikacja w R\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\nDominanta (moda)\nDominanta to najczęściej występująca wartość w zbiorze danych.\nDla Firmy X dominanta wynosi 3 (występuje 6 razy). Dla Firmy Y są dwie dominanty: 4 i 5 (obie występują 6 razy).\n\n# Funkcja do obliczania dominanty\nznajdz_dominante &lt;- function(x) {\n  unikalne_x &lt;- unique(x)\n  unikalne_x[which.max(tabulate(match(x, unikalne_x)))]\n}\n\nznajdz_dominante(X)\n\n[1] 3\n\nznajdz_dominante(Y)\n\n[1] 4\n\n\n\n\n\nMiary rozproszenia\n\nWariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\nPoprawka Bessela jest stosowana przy obliczaniu wariancji z próby, aby uzyskać nieobciążony estymator wariancji populacji. W standardowym wzorze na wariancję z próby dzielimy przez (n-1) zamiast przez n.\nModyfikacje wzoru dla danych pogrupowanych (szereg częstości):\nMożna też zapisać ten wzór w postaci:\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{k} f_i(x_i - \\bar{x})^2\ngdzie f_i to częstość bezwzględna (liczba wystąpień) i-tej wartości.\nGdy w obliczeniach stosujemy częstości względne p = f_i/n, gdzie:\n\nf_i to częstość (liczba wystąpień)\nn to całkowita liczebność próby\n\nWzór na wariancję z uwzględnieniem poprawki Bessela przyjmuje postać:\ns^2 = \\frac{n}{n-1} \\sum_{i=1}^{k} p_i(x_i - \\bar{x})^2\ngdzie:\n\ns^2 to wariancja z próby\nn to liczebność próby\np_i = f_i/n to częstość względna i-tej wartości\nx_i to i-ta wartość cechy\n\\bar{x} to średnia arytmetyczna\nk to liczba różnych wartości cechy\n\nKluczowe jest to, że przy stosowaniu częstości względnych mnożymy całe wyrażenie przez czynnik \\frac{n}{n-1}, który wprowadza poprawkę Bessela.\n\nObliczenia ręczne dla Firmy X\n\n\n\n\n\n\n\n\n\n\nx_i\nf_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\nf_i(x_i - \\bar{x})^2\n\n\n\n\n2\n3\n-3,95\n15,6025\n46,8075\n\n\n3\n6\n-2,95\n8,7025\n52,215\n\n\n4\n5\n-1,95\n3,8025\n19,0125\n\n\n5\n4\n-0,95\n0,9025\n3,61\n\n\n20\n1\n14,05\n197,4025\n197,4025\n\n\n35\n1\n29,05\n843,9025\n843,9025\n\n\nSuma\n20\n\n\n1162,95\n\n\n\ns^2 = \\frac{1162,95}{19} = 61,21\n\n\nObliczenia ręczne dla Firmy Y\n\n\n\n\n\n\n\n\n\n\ny_i\nf_i\ny_i - \\bar{x}\n(y_i - \\bar{y})^2\nf_i(y_i - \\bar{y})^2\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nSuma\n20\n\n\n34\n\n\n\ns^2 = \\frac{34}{19} = 1,79\n\n\nWeryfikacja w R\n\nvar(X)\n\n[1] 61.2\n\nvar(Y)\n\n[1] 1.79\n\n\n\n\n\nOdchylenie standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji.\nWzór: s = \\sqrt{s^2}\n\nDla Firmy X: s = \\sqrt{61,21} = 7,82\nDla Firmy Y: s = \\sqrt{1,79} = 1,34\n\n\nWeryfikacja w R\n\nsd(X)\n\n[1] 7.82\n\nsd(Y)\n\n[1] 1.34\n\n\n\n\n\n\nKwartyle\nKwartyle dzielą zbiór danych na cztery równe części.\n\nObliczenia ręczne dla Firmy X\nUporządkowane dane: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 3\nQ2 (50. percentyl, mediana): 4\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 5\n\n\n\nObliczenia ręczne dla Firmy Y\nUporządkowane dane: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 4\nQ2 (50. percentyl, mediana): 5\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 6\n\n\n\nWeryfikacja w R\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\nIQR\n\nIQR_x = 5 - 3 = 2\nIQR_y = 6 - 4 = 2\n\n\n\n\nWykres pudełkowy Tukeya\nWykres pudełkowy Tukeya wizualnie przedstawia rozkład danych na podstawie kwartyli. Użyjemy biblioteki ggplot2 do stworzenia wykresu.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Przygotowanie danych\ndane &lt;- data.frame(\n  Firma = rep(c(\"X\", \"Y\"), each = 20),\n  Wynagrodzenie = c(X, Y)\n)\n\n# Tworzenie wykresu pudełkowego\nggplot(dane, aes(x = Firma, y = Wynagrodzenie, fill = Firma)) +\n  geom_boxplot() +\n  labs(title = \"Rozkład wynagrodzeń w firmach X i Y\",\n       x = \"Firma\",\n       y = \"Wynagrodzenie (tysiące euro)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n# Tworzenie wykresu pudełkowego\nggplot(dane, aes(x = Firma, y = Wynagrodzenie, fill = Firma)) +\n  geom_boxplot(outliers = F) +\n  labs(title = \"Rozkład wynagrodzeń w firmach X i Y\",\n       x = \"Firma\",\n       y = \"Wynagrodzenie (tysiące euro)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\nInterpretacja wykresu pudełkowego\n\nPudełko reprezentuje rozstęp międzykwartylowy (IQR) od Q1 do Q3.\nLinia wewnątrz pudełka to mediana (Q2).\nWąsy rozciągają się do najmniejszych i największych wartości w granicach 1,5 * IQR.\nPunkty poza wąsami są uznawane za wartości odstające.\n\n\n\n\nPorównanie wyników\n\n\n\nMiara\nFirma X\nFirma Y\n\n\n\n\nŚrednia\n5,95\n5,00\n\n\nMediana\n4\n5\n\n\nDominanta\n3\n4 i 5\n\n\nWariancja\n61,21\n1,79\n\n\nOdchylenie standard.\n7,82\n1,34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\nKluczowe obserwacje:\n\nTendencja centralna: Firma X ma wyższą średnią, ale niższą medianę niż Firma Y, co wskazuje na prawostronnie skośny rozkład dla Firmy X.\n\nRozproszenie: Firma X wykazuje znacznie wyższą wariancję i odchylenie standardowe, sugerując większe dysproporcje w wynagrodzeniach.\nKształt rozkładu: Wynagrodzenia w Firmie Y są bardziej skupione, podczas gdy Firma X ma wartości ekstremalne (potencjalne wartości odstające), które znacząco wpływają na jej średnią i wariancję.\nKwartyle: Rozstęp międzykwartylowy (Q3 - Q1) Firmy Y jest nieznacznie większy, ale jej ogólny zakres jest znacznie mniejszy niż Firmy X.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ćwiczenie-2.-porównanie-zmienności-wielkości-okręgów-wyborczych-między-krajami",
    "href": "rozdzial5.html#ćwiczenie-2.-porównanie-zmienności-wielkości-okręgów-wyborczych-między-krajami",
    "title": "6  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.11 Ćwiczenie 2. Porównanie Zmienności Wielkości Okręgów Wyborczych Między Krajami",
    "text": "6.11 Ćwiczenie 2. Porównanie Zmienności Wielkości Okręgów Wyborczych Między Krajami\n\nDane\nMamy dane o wielkości okręgów wyborczych z dwóch krajów:\n\nx &lt;- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)  # Kraj wysoka zmienność\ny &lt;- c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13)  # Kraj niska zmienność\n\nkable(data.frame(\n  \"Kraj X (Wysoka zm.)\" = x,\n  \"Kraj Y (Niska zm.)\" = y\n))\n\n\n\n\nKraj.X..Wysoka.zm..\nKraj.Y..Niska.zm..\n\n\n\n\n1\n8\n\n\n3\n9\n\n\n5\n9\n\n\n7\n10\n\n\n9\n10\n\n\n11\n11\n\n\n13\n11\n\n\n15\n12\n\n\n17\n12\n\n\n19\n13\n\n\n\n\n\n\n\nMiary Tendencji Centralnej\n\nŚrednia Arytmetyczna\nWzór: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\nObliczenia dla Kraju X\n\n\n\nElement\nWartość\n\n\n\n\n1\n1\n\n\n2\n3\n\n\n3\n5\n\n\n4\n7\n\n\n5\n9\n\n\n6\n11\n\n\n7\n13\n\n\n8\n15\n\n\n9\n17\n\n\n10\n19\n\n\nSuma\n100\n\n\n\n\\bar{x} = \\frac{100}{10} = 10\n\nmean_x &lt;- mean(x)\nc(\"Ręcznie\" = 10, \"R\" = mean_x)\n\nRęcznie       R \n     10      10 \n\n\n\n\nObliczenia dla Kraju Y\n\n\n\nElement\nWartość\n\n\n\n\n1\n8\n\n\n2\n9\n\n\n3\n9\n\n\n4\n10\n\n\n5\n10\n\n\n6\n11\n\n\n7\n11\n\n\n8\n12\n\n\n9\n12\n\n\n10\n13\n\n\nSuma\n105\n\n\n\n\\bar{y} = \\frac{105}{10} = 10,5\n\nmean_y &lt;- mean(y)\nc(\"Ręcznie\" = 10.5, \"R\" = mean_y)\n\nRęcznie       R \n   10.5    10.5 \n\n\n\n\n\nMediana\nMediana to wartość środkowa w uporządkowanym zbiorze danych.\n\nObliczenia dla Kraju X\nUporządkowane dane: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nDla n = 10 (parzysta liczba obserwacji): Pozycje środkowe: 5 i 6 Wartości środkowe: 9 i 11\nMediana = \\frac{9 + 11}{2} = 10\n\nmedian_x &lt;- median(x)\nc(\"Ręcznie\" = 10, \"R\" = median_x)\n\nRęcznie       R \n     10      10 \n\n\n\n\nObliczenia dla Kraju Y\nUporządkowane dane: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nDla n = 10 (parzysta liczba obserwacji): Pozycje środkowe: 5 i 6 Wartości środkowe: 10 i 11\nMediana = \\frac{10 + 11}{2} = 10,5\n\nmedian_y &lt;- median(y)\nc(\"Ręcznie\" = 10.5, \"R\" = median_y)\n\nRęcznie       R \n   10.5    10.5 \n\n\n\n\n\nDominanta\n\nObliczenia dla Kraju X\n\n\n\nWartość\nCzęstość\n\n\n\n\n1\n1\n\n\n3\n1\n\n\n5\n1\n\n\n7\n1\n\n\n9\n1\n\n\n11\n1\n\n\n13\n1\n\n\n15\n1\n\n\n17\n1\n\n\n19\n1\n\n\n\nWniosek: Brak dominanty (wszystkie wartości występują jednokrotnie)\n\n\nObliczenia dla Kraju Y\n\n\n\nWartość\nCzęstość\n\n\n\n\n8\n1\n\n\n9\n2\n\n\n10\n2\n\n\n11\n2\n\n\n12\n2\n\n\n13\n1\n\n\n\nWniosek: Cztery dominanty: 9, 10, 11, 12 (każda występuje dwukrotnie)\n\n# Tabele częstości\ntable_x &lt;- table(x)\ntable_y &lt;- table(y)\n\nlist(\n  \"Kraj X\" = table_x,\n  \"Kraj Y\" = table_y\n)\n\n$`Kraj X`\nx\n 1  3  5  7  9 11 13 15 17 19 \n 1  1  1  1  1  1  1  1  1  1 \n\n$`Kraj Y`\ny\n 8  9 10 11 12 13 \n 1  2  2  2  2  1 \n\n\n\n\n\nWariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\nObliczenia dla Kraju X\n\n\n\nx_i\n(x_i - \\bar{x})\n(x_i - \\bar{x})^2\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSuma\n\n330\n\n\n\ns^2_X = \\frac{330}{9} = 36,67\n\nvar_x &lt;- var(x)\nc(\"Ręcznie\" = 36.67, \"R\" = var_x)\n\nRęcznie       R \n  36.67   36.67 \n\n\n\n\nObliczenia dla Kraju Y\n\n\n\nx_i\n(y_i - \\bar{y})\n(y_i - \\bar{y})^2\n\n\n\n\n8\n-2,5\n6,25\n\n\n9\n-1,5\n2,25\n\n\n9\n-1,5\n2,25\n\n\n10\n-0,5\n0,25\n\n\n10\n-0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n12\n1,5\n2,25\n\n\n12\n1,5\n2,25\n\n\n13\n2,5\n6,25\n\n\nSuma\n\n22,5\n\n\n\ns^2_Y = \\frac{22,5}{9} = 2,5\n\nvar_y &lt;- var(y)\nc(\"Ręcznie\" = 2.5, \"R\" = var_y)\n\nRęcznie       R \n    2.5     2.5 \n\n\n\n\n\nOdchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji. Jest miarą zmienności wyrażoną w tych samych jednostkach co dane.\nWzór: s = \\sqrt{s^2}\n\nObliczenia dla Kraju X\nWykorzystujemy wcześniej obliczoną wariancję: s^2_X = 36,67\nObliczamy pierwiastek: s_X = \\sqrt{36,67} \\approx 6,06\n\n\n\nKrok\nObliczenie\nWynik\n\n\n\n\n1. Wariancja\ns^2_X\n36,67\n\n\n2. Pierwiastek\n\\sqrt{36,67}\n6,06\n\n\n\n\nsd_x &lt;- sd(x)\nc(\"Ręcznie\" = 6.06, \"R\" = sd_x)\n\nRęcznie       R \n  6.060   6.055 \n\n\n\n\nObliczenia dla Kraju Y\nWykorzystujemy wcześniej obliczoną wariancję: s^2_Y = 2,5\nObliczamy pierwiastek: s_Y = \\sqrt{2,5} \\approx 1,58\n\n\n\nKrok\nObliczenie\nWynik\n\n\n\n\n1. Wariancja\ns^2_Y\n2,5\n\n\n2. Pierwiastek\n\\sqrt{2,5}\n1,58\n\n\n\n\nsd_y &lt;- sd(y)\nc(\"Ręcznie\" = 1.58, \"R\" = sd_y)\n\nRęcznie       R \n  1.580   1.581 \n\n\nInterpretacja:\n\nKraj X: Przeciętne odchylenie wielkości okręgu od średniej wynosi około 6 mandatów\nKraj Y: Przeciętne odchylenie wielkości okręgu od średniej wynosi około 1,6 mandatu\n\n\n\n\n\nWspółczynnik Zmienności (CV)\nWspółczynnik zmienności to stosunek odchylenia standardowego do średniej, wyrażony w procentach.\nWzór: CV = \\frac{s}{\\bar{x}} \\times 100\\%\n\nObliczenia dla Kraju X\nCV_X = \\frac{6,06}{10} \\times 100\\% = 60,6\\%\n\n\n\nSkładowa\nWartość\n\n\n\n\nOdchylenie standardowe (s)\n6,06\n\n\nŚrednia (\\bar{x})\n10\n\n\nCV\n60,6%\n\n\n\n\ncv_x &lt;- sd(x) / mean(x) * 100\nc(\"Ręcznie\" = 60.6, \"R\" = cv_x)\n\nRęcznie       R \n  60.60   60.55 \n\n\n\n\nObliczenia dla Kraju Y\nCV_Y = \\frac{1,58}{10,5} \\times 100\\% = 15,0\\%\n\n\n\nSkładowa\nWartość\n\n\n\n\nOdchylenie standardowe (s)\n1,58\n\n\nŚrednia (\\bar{x})\n10,5\n\n\nCV\n15,0%\n\n\n\n\ncv_y &lt;- sd(y) / mean(y) * 100\nc(\"Ręcznie\" = 15.0, \"R\" = cv_y)\n\nRęcznie       R \n  15.00   15.06 \n\n\n\n\n\nKwartyle i Rozstęp Międzykwartylowy (IQR)\n\nMetody obliczania kwartyli\nIstnieją różne metody obliczania kwartyli. W naszych obliczeniach ręcznych zastosujemy metodę wyłączającą medianę:\n\nDzielimy szereg na dwie części względem mediany\nMediana nie jest uwzględniana w obliczeniach kwartyli\nDla każdej części obliczamy jej medianę - będzie to odpowiednio Q1 i Q3\n\n\n\nObliczenia dla Kraju X\nUporządkowane dane: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMediana = 10 (nie uwzględniamy w obliczeniach kwartyli)\nDolna połowa: 1, 3, 5, 7, 9 Q1 = mediana dolnej połowy = 5\nGórna połowa: 11, 13, 15, 17, 19 Q3 = mediana górnej połowy = 15\nIQR = Q3 - Q1 = 15 - 5 = 10\n\n\nObliczenia dla Kraju Y\nUporządkowane dane: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMediana = 10.5 (nie uwzględniamy w obliczeniach kwartyli)\nDolna połowa: 8, 9, 9, 10, 10 Q1 = mediana dolnej połowy = 9\nGórna połowa: 11, 11, 12, 12, 13 Q3 = mediana górnej połowy = 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n# Porównanie różnych metod obliczania kwartyli w R\nmethods_comparison &lt;- data.frame(\n  Metoda = c(\"Ręcznie (bez mediany)\", \n             \"R type=1\", \"R type=2\", \"R type=7 (domyślna)\"),\n  \"Q1 Kraj X\" = c(5, \n                  quantile(x, 0.25, type=1),\n                  quantile(x, 0.25, type=2),\n                  quantile(x, 0.25, type=7)),\n  \"Q3 Kraj X\" = c(15,\n                  quantile(x, 0.75, type=1),\n                  quantile(x, 0.75, type=2),\n                  quantile(x, 0.75, type=7)),\n  \"Q1 Kraj Y\" = c(9,\n                  quantile(y, 0.25, type=1),\n                  quantile(y, 0.25, type=2),\n                  quantile(y, 0.25, type=7)),\n  \"Q3 Kraj Y\" = c(12,\n                  quantile(y, 0.75, type=1),\n                  quantile(y, 0.75, type=2),\n                  quantile(y, 0.75, type=7))\n)\n\nkable(methods_comparison, digits = 2,\n      caption = \"Porównanie różnych metod obliczania kwartyli\")\n\n\nPorównanie różnych metod obliczania kwartyli\n\n\nMetoda\nQ1.Kraj.X\nQ3.Kraj.X\nQ1.Kraj.Y\nQ3.Kraj.Y\n\n\n\n\nRęcznie (bez mediany)\n5.0\n15.0\n9.00\n12.00\n\n\nR type=1\n5.0\n15.0\n9.00\n12.00\n\n\nR type=2\n5.0\n15.0\n9.00\n12.00\n\n\nR type=7 (domyślna)\n5.5\n14.5\n9.25\n11.75\n\n\n\n\n\n\n\nWyjaśnienie różnic w metodach obliczania kwartyli\n\nMetoda ręczna (bez mediany):\n\nDzieli dane na dwie części\nNie uwzględnia mediany\nZnajduje medianę każdej części\n\nR type=1:\n\nMetoda pierwsza w R\nUżywa pozycji całkowitych\nNie interpoluje\n\nR type=2:\n\nMetoda druga w R\nUżywa pozycji całkowitych\nInterpoluje gdy pozycja nie jest całkowita\n\nR type=7 (domyślna):\n\nMetoda domyślna w R\nUżywa quantile()[5] z SAS\nInterpoluje według metody opisanej przez Hyndmana i Fana\n\n\n\n\n\nPorównanie Wyników\n\nsummary_df &lt;- data.frame(\n  Miara = c(\"Średnia\", \"Mediana\", \"Dominanta\", \"Rozstęp\", \"Wariancja\", \n            \"Odch. Stand.\", \"Q1\", \"Q3\", \"IQR\", \"CV (%)\"),\n  \"Kraj X\" = c(10, 10, \"brak\", 18, 36.67, 6.06, 5, 15, 10, 60.6),\n  \"Kraj Y\" = c(10.5, 10.5, \"9,10,11,12\", 5, 2.5, 1.58, 9, 12, 3, 15.0)\n)\n\nkable(summary_df, \n      caption = \"Zestawienie wszystkich miar statystycznych\",\n      align = c('l', 'r', 'r'))\n\n\nZestawienie wszystkich miar statystycznych\n\n\nMiara\nKraj.X\nKraj.Y\n\n\n\n\nŚrednia\n10\n10.5\n\n\nMediana\n10\n10.5\n\n\nDominanta\nbrak\n9,10,11,12\n\n\nRozstęp\n18\n5\n\n\nWariancja\n36.67\n2.5\n\n\nOdch. Stand.\n6.06\n1.58\n\n\nQ1\n5\n9\n\n\nQ3\n15\n12\n\n\nIQR\n10\n3\n\n\nCV (%)\n60.6\n15\n\n\n\n\n\n\n\nPorównanie za pomocą Wykresu Pudełkowego\n\ndf_long &lt;- data.frame(\n  kraj = rep(c(\"X\", \"Y\"), each = 10),\n  wielkosc = c(x, y)\n)\n\n# Wykres podstawowy\np &lt;- ggplot(df_long, aes(x = kraj, y = wielkosc, fill = kraj)) +\n  geom_boxplot(outlier.shape = NA) +  # Wyłączamy domyślne punkty odstające\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Dodajemy punkty z przezroczystością\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\")) +\n  labs(\n    title = \"Porównanie Zmienności Wielkości Okręgów Wyborczych\",\n    subtitle = paste(\"CV: Kraj X =\", round(cv_x, 1), \"%, Kraj Y =\", round(cv_y, 1), \"%\"),\n    x = \"Kraj\",\n    y = \"Wielkość Okręgu\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Dodajemy adnotacje z kwartylami\np + annotate(\n  \"text\", \n  x = c(1, 1, 1, 2, 2, 2), \n  y = c(max(x)+1, mean(x), min(x)-1, max(y)+1, mean(y), min(y)-1),\n  label = c(\n    paste(\"Q3 =\", quantile(x, 0.75, type=1)),\n    paste(\"M =\", median(x)),\n    paste(\"Q1 =\", quantile(x, 0.25, type=1)),\n    paste(\"Q3 =\", quantile(y, 0.75, type=1)),\n    paste(\"M =\", median(y)),\n    paste(\"Q1 =\", quantile(y, 0.25, type=1))\n  ),\n  size = 3\n)\n\n\n\n\n\n\n\n\n\n\nUwagi Metodologiczne\n\nObliczenia kwartyli:\n\nZastosowana metoda wyłączająca medianę może dawać inne wyniki niż domyślne funkcje R\nRóżnice w metodach obliczeniowych nie wpływają na ogólne wnioski\nWarto zawsze zaznaczyć stosowaną metodę w raportach\n\nWizualizacja:\n\nWykres pudełkowy skutecznie pokazuje różnice w rozkładach\nDodatkowe punkty pokazują rzeczywiste wartości\nAdnotacje ułatwiają interpretację\n\n\n\n\nPodsumowanie\n\nPorównanie Miar Statystycznych\n\n\n\nMiara\nKraj X\nKraj Y\nRóżnica względna\n\n\n\n\nŚrednia\n10,0\n10,5\nPodobna\n\n\nMediana\n10,0\n10,5\nPodobna\n\n\nDominanta\nBrak\nWielokrotna (9,10,11,12)\n-\n\n\nRozstęp\n18\n5\n3,6× większy w X\n\n\nWariancja\n36,67\n2,5\n14,7× większa w X\n\n\nIQR\n10\n3\n3,3× większy w X\n\n\nCV\n60,6%\n15,0%\n4,0× większy w X\n\n\n\n\n\nCharakterystyka Rozkładów\nKraj X:\n\nRozkład równomierny\nBrak dominującej wielkości okręgu (brak dominanty)\nSzeroki zakres: od 1 do 19 mandatów\nWysoka zmienność (CV = 60,6%)\nRównomierne rozłożenie wartości w zakresie\n\nKraj Y:\n\nRozkład skupiony\nWiele typowych wielkości (cztery dominanty)\nWąski zakres: od 8 do 13 mandatów\nNiska zmienność (CV = 15,0%)\nWartości skoncentrowane wokół średniej\n\n\n\nInterpretacja Wykresu Pudełkowego\nWizualizacja w formie wykresu pudełkowego pokazuje:\nElementy Struktury:\n\nPudełko: Pokazuje rozstęp międzykwartylowy (IQR)\nDolna krawędź: Pierwszy kwartyl (Q1)\nGórna krawędź: Trzeci kwartyl (Q3)\nLinia wewnętrzna: Mediana (Q2)\nWąsy: Rozciągają się do ±1,5 IQR - Punkty: Pojedyncze wielkości okręgów\n\nGłówne Wnioski Wizualne:\n\nRozmiar Pudełka:\n\n\nKraj X: Duże pudełko wskazuje na szeroki rozrzut środkowych 50%\nKraj Y: Małe pudełko pokazuje skupienie wartości środkowych\n\n\nDługość Wąsów:\n\nKraj X: Długie wąsy wskazują na szeroki rozkład całkowity\nKraj Y: Krótkie wąsy pokazują ograniczony rozrzut\n\nRozkład Punktów:\n\nKraj X: Punkty szeroko rozproszone\nKraj Y: Punkty gęsto skupione\n\n\n\n\nKluczowe Obserwacje\n\nTendencja Centralna:\n\nPodobne średnie wielkości okręgów\nRóżne wzorce rozkładu\nOdmienne podejścia do standaryzacji\n\nMiary Zmienności:\n\nWszystkie miary pokazują 3-15 razy większą zmienność w Kraju X\nSpójny wzorzec w różnych miarach statystycznych\nSystematyczna różnica w projekcie okręgów\n\nProjekt Systemu:\n\nKraj X: Elastyczne, zróżnicowane podejście\nKraj Y: Ustandaryzowane, jednolite podejście\nRóżne filozoficzne podejścia do reprezentacji\n\nImplikacje Reprezentatywności:\n\nKraj X: Zmienna proporcja wyborców do przedstawicieli\nKraj Y: Bardziej spójne poziomy reprezentacji\nRóżne podejścia do reprezentacji demokratycznej\n\n\nAnaliza ta pokazuje fundamentalne różnice w projektowaniu systemów wyborczych między dwoma krajami, gdzie Kraj X przyjmuje bardziej zróżnicowane podejście, a Kraj Y utrzymuje większą jednolitość w wielkości okręgów wyborczych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#cwiczenie-3.-wykresy-pudełkowe-na-przykładzie-danych-o-długości-życia",
    "href": "rozdzial5.html#cwiczenie-3.-wykresy-pudełkowe-na-przykładzie-danych-o-długości-życia",
    "title": "6  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.12 Cwiczenie 3. Wykresy Pudełkowe na Przykładzie Danych o Długości Życia",
    "text": "6.12 Cwiczenie 3. Wykresy Pudełkowe na Przykładzie Danych o Długości Życia\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# Przygotowanie danych\ndata_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007)\n\nWykres pudełkowy (ang. box-and-whisker plot) przedstawia pięć kluczowych statystyk opisowych danych:\n\nMediana: Środkowa linia w pudełku (50. percentyl)\nPierwszy kwartyl (Q1): Dolna krawędź pudełka (25. percentyl)\nTrzeci kwartyl (Q3): Górna krawędź pudełka (75. percentyl)\nRozstęp międzykwartylowy (IQR): Wysokość pudełka (Q3 - Q1)\nWąsy: Rozciągają się do najbardziej skrajnych wartości niebędących obserwacjami odstającymi (metoda Tukeya: 1.5 × IQR)\nObserwacje odstające: Pojedyncze punkty poza wąsami\n\n\nWizualizacja Długości Życia\n\nggplot(data_2007, aes(x = reorder(continent, lifeExp, FUN = median), y = lifeExp)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 24, \n               outlier.fill = \"red\", outlier.alpha = 0.6, outlier.size = 4) +\n  geom_jitter(width = 0.2, alpha = 0.4, color = \"darkblue\") +\n  labs(title = \"Długość Życia według Kontynentów (2007)\",\n       subtitle = \"Pojedyncze punkty pokazują surowe dane; czerwone punkty oznaczają wartości odstające\",\n       x = \"Kontynent\",\n       y = \"Długość życia (w latach)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_y_continuous(breaks = seq(40, 85, by = 5))\n\n\n\n\n\n\n\n\n\n\nAnaliza Danych\n\n\nMediana i Rozkład\nOdpowiedz Prawda lub Fałsz:\n\n50% krajów afrykańskich ma długość życia poniżej 52 lat\nMediana długości życia w Europie wynosi około 78 lat\nPonad 75% krajów Oceanii ma długość życia powyżej 75 lat\n25% krajów azjatyckich ma długość życia poniżej 68 lat\nŚrodkowe 50% długości życia w Europie mieści się między 76 a 80 lat\n\n\n\nRozrzut i Zmienność\nOdpowiedz Prawda lub Fałsz:\n\nAzja wykazuje największy rozrzut (IQR) w długości życia\nEuropa ma najmniejszy IQR wśród wszystkich kontynentów\nZmienność długości życia w Afryce jest większa niż w obu Amerykach\nOceania wykazuje najmniejszą zmienność w długości życia\nWąsy dla Azji rozciągają się w przybliżeniu od 58 do 82 lat (z wyłączeniem wartości odstających)\n\n\n\nWartości Odstające i Ekstrema\nOdpowiedz Prawda lub Fałsz:\n\nAfryka ma dwa kraje z wyjątkowo niską długością życia\nW rozkładzie dla Oceanii nie ma wartości odstających\nAzja ma kilka niskich wartości odstających (poniżej 55 lat)\n\n\n\nZmiany w Czasie\n\ntime_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  mutate(year = factor(year))\n\nggplot(time_comparison, aes(x = continent, y = lifeExp, fill = year)) +\n  geom_boxplot(alpha = 0.7, position = \"dodge\", outlier.shape = 21,\n               outlier.alpha = 0.6) +\n  labs(title = \"Długość Życia: 1957 vs 2007\",\n       subtitle = \"Porównanie zmian rozkładu na przestrzeni 50 lat\",\n       x = \"Kontynent\",\n       y = \"Długość życia (w latach)\",\n       fill = \"Rok\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 14)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(breaks = seq(30, 85, by = 5))\n\n\n\n\n\n\n\n\n\n\nPytania dotyczące Zmian w Czasie\nOdpowiedz Prawda lub Fałsz:\n\nMediana długości życia wzrosła na wszystkich kontynentach między 1957 a 2007 rokiem\nZmienność długości życia (IQR) zmniejszyła się na większości kontynentów w czasie\nAfryka wykazała najmniejszą poprawę mediany długości życia\nRozrzut długości życia w Azji znacząco się zmniejszył od 1957 do 2007 roku\nOceania utrzymała najwyższą medianę długości życia w obu okresach\n\n\n\nPodsumowanie Statystyczne\n\n# Obliczenie statystyk opisowych\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    mediana = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    min = min(lifeExp),\n    max = max(lifeExp),\n    liczba_odstających = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Statystyki Opisowe według Kontynentu i Roku\")\n\n\nStatystyki Opisowe według Kontynentu i Roku\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontinent\nyear\nmediana\nq1\nq3\niqr\nmin\nmax\nliczba_odstających\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n31.6\n58.1\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n39.6\n76.4\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n40.7\n70.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n60.9\n80.7\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n30.3\n67.8\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n43.8\n82.6\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n48.1\n73.5\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n71.8\n81.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n70.3\n70.3\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n80.2\n81.2\n0\n\n\n\n\n\n\n\nNajważniejsze Wnioski\n\nCentrum Rozkładu:\n\nMediana pokazuje typową długość życia\nZmiany mediany odzwierciedlają ogólną poprawę\n\nRozrzut i Zmienność:\n\nIQR (wysokość pudełka) wskazuje na rozproszenie danych\nSzersze pudełka sugerują większe nierówności w długości życia\n\nWartości Odstające i Ekstrema:\n\nWartości odstające często reprezentują kraje o wyjątkowej sytuacji\n\nPorównanie w Czasie:\n\nPokazuje zarówno bezwzględną poprawę, jak i zmiany w wariancji\nUwydatnia utrzymujące się różnice regionalne\nUjawnia różne tempo postępu na poszczególnych kontynentach\n\n\n\n\nStatystyki opisowe (podsumowanie)\n\n# Calculate summary statistics\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    median = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    min = min(lifeExp),\n    max = max(lifeExp),\n    n_outliers = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Summary Statistics by Continent and Year\")\n\n\nSummary Statistics by Continent and Year\n\n\ncontinent\nyear\nmedian\nq1\nq3\niqr\nmin\nmax\nn_outliers\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n31.6\n58.1\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n39.6\n76.4\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n40.7\n70.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n60.9\n80.7\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n30.3\n67.8\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n43.8\n82.6\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n48.1\n73.5\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n71.8\n81.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n70.3\n70.3\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n80.2\n81.2\n0",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#appendix-tabele-podsumowujące-typy-danych-i-odpowiednie-miary-statystyczne",
    "href": "rozdzial5.html#appendix-tabele-podsumowujące-typy-danych-i-odpowiednie-miary-statystyczne",
    "title": "6  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.13 Appendix: Tabele Podsumowujące Typy Danych i Odpowiednie Miary Statystyczne",
    "text": "6.13 Appendix: Tabele Podsumowujące Typy Danych i Odpowiednie Miary Statystyczne\n\nZalety i Wady Różnych Miar Statystycznych\n\nMiary Tendencji Centralnej\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nŚrednia\n- Wykorzystuje wszystkie punkty danych- Pozwala na dalsze obliczenia statystyczne- Idealna dla danych o rozkładzie normalnym\n- Wrażliwa na wartości odstające- Nieodpowiednia dla rozkładów skośnych- Bez znaczenia dla danych nominalnych\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nMediana\n- Niewrażliwa na wartości odstające- Dobra dla rozkładów skośnych- Może być stosowana do danych porządkowych\n- Ignoruje rzeczywiste wartości większości punktów danych- Mniej użyteczna do dalszych analiz statystycznych\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nModa\n- Może być stosowana do każdego typu danych- Dobra do znajdowania najczęstszej kategorii\n- Może nie być unikalna (rozkłady multimodalne)- Nieprzydatna do wielu typów analiz- Ignoruje wielkość różnic między wartościami\nWszystkie typy\n\n\n\n\n\nMiary Zmienności\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nZakres\n- Prosty do obliczenia i zrozumienia- Daje szybki obraz rozproszenia danych\n- Bardzo wrażliwy na wartości odstające- Ignoruje wszystkie dane między ekstremami- Nieprzydatny do dalszych analiz statystycznych\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nRozstęp międzykwartylowy (IQR)\n- Niewrażliwy na wartości odstające- Dobry dla rozkładów skośnych\n- Ignoruje 50% danych- Mniej intuicyjny niż zakres\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nWariancja\n- Wykorzystuje wszystkie punkty danych- Podstawa wielu procedur statystycznych\n- Wrażliwa na wartości odstające- Jednostki są podniesione do kwadratu (mniej intuicyjne)\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nOdchylenie standardowe\n- Wykorzystuje wszystkie punkty danych- Te same jednostki co oryginalne dane- Szeroko stosowane i zrozumiałe\n- Wrażliwe na wartości odstające- Zakłada w przybliżeniu rozkład normalny dla interpretacji\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nWspółczynnik zmienności\n- Pozwala na porównanie między zbiorami danych o różnych jednostkach lub średnich\n- Może być mylący, gdy średnie są bliskie zeru- Bez znaczenia dla danych z wartościami ujemnymi\nIlorazowe, niektóre Interwałowe\n\n\n\n\n\nMiary Korelacji/Asocjacji\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nr Pearsona\n- Mierzy zależność liniową- Szeroko stosowany i zrozumiały\n- Zakłada rozkład normalny- Wrażliwy na wartości odstające- Uchwytuje tylko zależności liniowe\nInterwałowe, Ilorazowe, Ciągłe\n\n\nRho Spearmana\n- Może być stosowany do danych porządkowych- Uchwytuje zależności monotoniczne- Mniej wrażliwy na wartości odstające\n- Traci informacje przez konwersję na rangi- Może pominąć niektóre typy zależności\nPorządkowe, Interwałowe, Ilorazowe\n\n\nTau Kendalla\n- Może być stosowany do danych porządkowych- Bardziej odporny niż Spearman dla małych próbek- Ma ładną interpretację (prawdopodobieństwo zgodności)\n- Traci informacje, biorąc pod uwagę tylko porządek- Bardziej intensywny obliczeniowo\nPorządkowe, Interwałowe, Ilorazowe\n\n\nChi-kwadrat\n- Może być stosowany do danych nominalnych- Testuje niezależność zmiennych kategorycznych\n- Wymaga dużych rozmiarów próbek- Wrażliwy na rozmiar próbki- Nie mierzy siły asocjacji\nNominalne, Porządkowe\n\n\nV Craméra\n- Może być stosowany do danych nominalnych- Dostarcza miarę siły asocjacji- Znormalizowany do zakresu [0,1]\n- Interpretacja może być subiektywna- Może przeszacować asocjację w małych próbkach\nNominalne, Porządkowe\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓*\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\nNotes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "7  Data Visualization: with examples in R",
    "section": "",
    "text": "7.1 Introduction to Data Types and Visualization\nThis chapter explores fundamental types of data visualizations: bar plots, histograms, and box plots, in particular.\nBefore diving into specific visualization techniques, it’s crucial to understand the different types of data you might encounter and how they influence your choice of visualization method. We’ll explore these concepts with practical examples using the ggplot2 library in R.\nFirst, let’s load the necessary libraries:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#bar-plots",
    "href": "chapter6.html#bar-plots",
    "title": "7  Data Visualization: with examples in R",
    "section": "7.2 Bar Plots",
    "text": "7.2 Bar Plots\nBar plots are excellent for displaying categorical data or summarizing continuous data by groups.\n\nUnderstanding Bar Plots\nA bar plot represents data using rectangular bars with heights proportional to the values they represent. They are used to compare different categories or groups.\nKey components of a bar plot: 1. X-axis: Represents categories 2. Y-axis: Represents values (can be counts, percentages, or any numerical value) 3. Bars: Rectangle for each category, height corresponds to its value\n\nExample Data\nLet’s use a simple dataset of fruit sales:\n\nfruits &lt;- c(\"Apple\", \"Banana\", \"Orange\", \"Grape\")\nsales &lt;- c(120, 85, 70, 100)\n\n# Create a data frame\ndf &lt;- data.frame(fruit = fruits, sales = sales)\n\n\n\n\nHand-Drawn Bar Plot\nTo create a bar plot by hand:\n\nDraw a horizontal line (x-axis) and a vertical line (y-axis) perpendicular to each other.\nLabel the x-axis with your categories (fruits), evenly spaced.\nLabel the y-axis with a suitable scale for your values (sales, 0 to 120 in increments of 20).\nFor each category, draw a rectangle (bar) whose height corresponds to its value on the y-axis scale.\nColor or shade each bar if desired.\nAdd a title and labels for both axes.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen drawing by hand, use graph paper for more precise measurements and straighter lines. Choose a scale that allows all your data to fit while maximizing the use of space.\n\n\n\n\nBar Plot in Base R\n\n# Create bar plot\nbarplot(sales, names.arg = fruits, \n        main = \"Fruit Sales\",\n        xlab = \"Fruit Types\", ylab = \"Sales\")\n\n\n\n\n\n\n\n\n\n\nBar Plot with ggplot2\n\n# Create bar plot with ggplot2\nggplot(df, aes(x = fruit, y = sales)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Fruit Sales\",\n       x = \"Fruit Types\", y = \"Sales\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpreting Bar Plots\nWhen interpreting a bar plot, consider the following:\n\nRelative Heights: Compare the heights of the bars to understand which categories have higher or lower values.\nOrdering: Sometimes, bars are ordered by height to make comparisons easier.\nPatterns: Look for any patterns or trends across categories.\nOutliers: Identify any bars that are much taller or shorter than the others.\n\n\nExample Interpretation\nFor our fruit sales data:\n\nApples have the highest sales (120), followed by Grapes (100).\nOranges have the lowest sales (70).\nThere’s a considerable difference between the highest (Apples) and lowest (Oranges) sales.\nBananas and Grapes have similar sales figures, in the middle range.\n\nThis information could be useful for inventory management or marketing strategies in a fruit shop.\n\n\n\n\n\n\nNote\n\n\n\nBar plots are great for comparing categories, but they don’t show the distribution within each category. For that, you might need other plot types like box plots.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#histograms",
    "href": "chapter6.html#histograms",
    "title": "7  Data Visualization: with examples in R",
    "section": "7.3 Histograms",
    "text": "7.3 Histograms\nHistograms visualize the distribution of a continuous variable by dividing it into intervals (bins) and showing the frequency or density of data points in each bin.\n\nUnderstanding Histograms\nKey components of a histogram: 1. X-axis: Represents the variable’s values, divided into bins 2. Y-axis: Represents frequency, relative frequency, or density 3. Bars: Rectangle for each bin, height corresponds to the y-axis measure\nThere are three main types of histograms:\n\nFrequency Histogram: The y-axis shows the count of data points in each bin.\nRelative Frequency Histogram: The y-axis shows the proportion of data points in each bin (frequency divided by total number of data points).\nDensity Histogram: The y-axis shows the density, which is the relative frequency divided by the bin width. The total area of all bars sums to 1.\n\n\nExample Data\nLet’s use a dataset of 50 student exam scores (out of 100):\n\nset.seed(123)  # for reproducibility\nscores &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\nHand-Drawn Histogram\nTo create a frequency histogram by hand:\n\nFind the range of your data.\nChoose a number of bins (let’s use 7 bins).\nCreate a frequency table.\nDraw x and y axes.\nLabel x-axis with bin ranges and y-axis with frequency.\nDraw a rectangle for each bin, with height corresponding to its frequency.\nAdd a title and labels for both axes.\n\nFor a relative frequency histogram, divide each frequency by the total number of data points before drawing the bars.\nFor a density histogram, divide the relative frequency by the bin width before drawing the bars.\n\n\n\n\n\n\nTip\n\n\n\nThe number of bins can affect the interpretation. Too few bins may obscure important features, while too many may introduce noise. A common rule of thumb is to use the square root of the number of data points as the number of bins.\n\n\n\n\nHistograms in Base R\n\n# Frequency Histogram\nhist(scores, breaks = 7, \n     main = \"Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Relative Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Relative Frequency\")\n\n\n\n\n\n\n\n# Density Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Density Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Density\")\nlines(density(scores), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\nHistograms with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(score = scores)\n\n# Frequency Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nggplot(df, aes(x = score, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Relative Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Relative Frequency\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Density Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Density Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Density\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nInterpreting Histograms\nWhen interpreting a histogram, consider:\n\nCentral Tendency: Where is the peak of the distribution?\nSpread: How wide is the distribution?\nShape: Is it symmetric, skewed, or multi-modal?\nOutliers: Are there any unusual values far from the main distribution?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#box-plots-and-tukey-box-plots",
    "href": "chapter6.html#box-plots-and-tukey-box-plots",
    "title": "7  Data Visualization: with examples in R",
    "section": "7.4 Box Plots and Tukey Box Plots",
    "text": "7.4 Box Plots and Tukey Box Plots\nBox plots, also known as box-and-whisker plots, provide a concise summary of a distribution. We’ll focus on the Tukey-style box plot, named after the statistician John Tukey who popularized this type of plot.\n\nUnderstanding Box Plots\nA box plot represents five key statistics:\n\nMinimum value (excluding outliers)\nFirst quartile (Q1)\nMedian\nThird quartile (Q3)\nMaximum value (excluding outliers)\n\nAdditionally, box plots show:\n\nWhiskers: Lines extending from the box to the minimum and maximum values (excluding outliers)\nOutliers: Individual points beyond the whiskers\n\n\nCalculating Quartiles and Outliers\nTo create a box plot, follow these steps:\n\nOrder your data from smallest to largest.\nFind the median (middle value if odd number of data points, average of two middle values if even).\nFind Q1 (median of lower half of data) and Q3 (median of upper half of data).\nCalculate the Interquartile Range (IQR) = Q3 - Q1\nDetermine outliers using Tukey’s rule:\n\nLower outliers: &lt; Q1 - 1.5 * IQR\nUpper outliers: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe factor 1.5 in Tukey’s outlier rule is based on the properties of the normal distribution. For normally distributed data, this rule identifies about 0.7% of the data as potential outliers.\n\n\n\n\nExample Data\nLet’s use a small dataset to illustrate:\n\ndata &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\nHand-Drawn Tukey Box Plot\nTo create a Tukey box plot by hand:\n\nDraw a vertical line representing the range from minimum to maximum (2 to 15 in our example, excluding the outlier).\nDraw a box from Q1 to Q3.\nDraw a horizontal line through the box at the median.\nDraw whiskers from the box to the minimum and maximum values (excluding outliers).\nRepresent the outlier (50) as an individual point beyond the whisker.\nAdd a scale to the vertical axis and label it.\n\n\n\nBox Plot in Base R\n\n# Create box plot\nboxplot(data, main = \"Box Plot of Sample Data\",\n        ylab = \"Values\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\nTukey Box Plot with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(value = data)\n\n# Create Tukey box plot with ggplot2\nggplot(df, aes(x = \"\", y = value)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Tukey Box Plot of Sample Data\",\n       x = \"\", y = \"Values\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\nInterpreting Box Plots\nWhen interpreting a box plot, consider the following:\n\nCentral Tendency: The median shows the center of the distribution.\nSpread: The box (IQR) represents the middle 50% of the data.\nSkewness: If the median line is closer to one end of the box, the distribution is skewed.\nOutliers: Points beyond the whiskers are potential outliers.\nComparisons: When comparing multiple box plots, look at relative positions of medians, box sizes, and presence of outliers.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#conclusion",
    "href": "chapter6.html#conclusion",
    "title": "7  Data Visualization: with examples in R",
    "section": "7.5 Conclusion",
    "text": "7.5 Conclusion\nIn this chapter, we explored three fundamental types of data visualizations: bar plots, histograms, and box plots. We demonstrated how to create these plots by hand, using R’s base plotting system, and using the ggplot2 library.\nEach type of plot serves a different purpose: - Bar plots are excellent for comparing categories. - Histograms show the distribution of a continuous variable. - Box plots provide a concise summary of a distribution, highlighting central tendency, spread, and outliers.\nRemember, the choice of visualization depends on your data type and the insights you want to convey. Always consider your audience and the story you want to tell with your data when selecting and designing your visualizations.\nPractice creating these plots by hand to deepen your understanding of their construction and interpretation. Then, leverage the power of R and ggplot2 to quickly create and customize these visualizations for larger datasets and more complex analyses.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html",
    "href": "rozdzial6.html",
    "title": "8  Wizualizacja Danych: z przykładami w R",
    "section": "",
    "text": "8.1 Wprowadzenie do Typów Danych i Wizualizacji\nW tym rozdziale poznamy podstawowe typy wizualizacji danych: wykresy słupkowe, histogramy i wykresy pudełkowe. Omówimy ich tworzenie zarówno ręcznie, jak i przy użyciu R.\nPrzed zagłębieniem się w konkretne techniki wizualizacji, ważne jest zrozumienie różnych typów danych i ich wpływu na wybór metody wizualizacji. Przeanalizujemy te koncepcje na praktycznych przykładach z użyciem biblioteki ggplot2 w R.\nNajpierw załadujmy niezbędne biblioteki:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-słupkowe",
    "href": "rozdzial6.html#wykresy-słupkowe",
    "title": "8  Wizualizacja Danych: z przykładami w R",
    "section": "8.2 Wykresy Słupkowe",
    "text": "8.2 Wykresy Słupkowe\nWykresy słupkowe doskonale nadają się do prezentacji danych kategorycznych lub podsumowania danych ciągłych w grupach.\n\nZrozumienie Wykresów Słupkowych\nWykres słupkowy przedstawia dane za pomocą prostokątnych słupków, których wysokość jest proporcjonalna do reprezentowanych przez nie wartości. Służą do porównywania różnych kategorii lub grup.\nGłówne elementy wykresu słupkowego: 1. Oś X: Reprezentuje kategorie 2. Oś Y: Reprezentuje wartości (mogą to być liczebności, procenty lub dowolne wartości numeryczne) 3. Słupki: Prostokąt dla każdej kategorii, wysokość odpowiada jej wartości\n\nPrzykładowe Dane\nUżyjmy prostego zestawu danych dotyczącego sprzedaży owoców:\n\nowoce &lt;- c(\"Jabłko\", \"Banan\", \"Pomarańcza\", \"Winogrono\")\nsprzedaz &lt;- c(120, 85, 70, 100)\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(owoc = owoce, sprzedaz = sprzedaz)\n\n\n\n\nRęcznie Rysowany Wykres Słupkowy\nAby stworzyć wykres słupkowy ręcznie:\n\nNarysuj linię poziomą (oś X) i pionową (oś Y) prostopadłe do siebie.\nOznacz oś X swoimi kategoriami (owocami), równomiernie rozmieszczonymi.\nOznacz oś Y odpowiednią skalą dla Twoich wartości (sprzedaż, od 0 do 120 z przyrostami co 20).\nDla każdej kategorii narysuj prostokąt (słupek), którego wysokość odpowiada jej wartości na skali osi Y.\nJeśli chcesz, pokoloruj lub zacienuj każdy słupek.\nDodaj tytuł i etykiety dla obu osi.\n\n\n\n\n\n\n\nTip\n\n\n\nPrzy rysowaniu ręcznym użyj papieru milimetrowego dla dokładniejszych pomiarów i prostszych linii. Wybierz skalę, która pozwoli zmieścić wszystkie dane, maksymalnie wykorzystując dostępną przestrzeń.\n\n\n\n\nWykres Słupkowy w Podstawowym R\n\n# Tworzenie wykresu słupkowego\nbarplot(sprzedaz, names.arg = owoce, \n        main = \"Sprzedaż Owoców\",\n        xlab = \"Rodzaje Owoców\", ylab = \"Sprzedaż\")\n\n\n\n\n\n\n\n\n\n\nWykres Słupkowy z ggplot2\n\n# Tworzenie wykresu słupkowego z ggplot2\nggplot(df, aes(x = owoc, y = sprzedaz)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Sprzedaż Owoców\",\n       x = \"Rodzaje Owoców\", y = \"Sprzedaż\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpretacja Wykresów Słupkowych\nPodczas interpretacji wykresu słupkowego zwróć uwagę na:\n\nWzględne Wysokości: Porównaj wysokości słupków, aby zrozumieć, które kategorie mają wyższe lub niższe wartości.\nKolejność: Czasami słupki są uporządkowane według wysokości, aby ułatwić porównania.\nWzorce: Poszukaj wzorców lub trendów między kategoriami.\nWartości Odstające: Zidentyfikuj słupki, które są znacznie wyższe lub niższe od pozostałych.\n\n\nPrzykładowa Interpretacja\nDla naszych danych o sprzedaży owoców:\n\nJabłka mają najwyższą sprzedaż (120), następnie Winogrona (100).\nPomarańcze mają najniższą sprzedaż (70).\nIstnieje znaczna różnica między najwyższą (Jabłka) a najniższą (Pomarańcze) sprzedażą.\nBanany i Winogrona mają podobne wartości sprzedaży, w średnim zakresie.\n\nTa informacja może być przydatna dla zarządzania zapasami lub strategii marketingowych w sklepie owocowym.\n\n\n\n\n\n\nNote\n\n\n\nWykresy słupkowe są świetne do porównywania kategorii, ale nie pokazują rozkładu wewnątrz każdej kategorii. Do tego mogą być potrzebne inne typy wykresów, jak wykresy pudełkowe.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#histogramy",
    "href": "rozdzial6.html#histogramy",
    "title": "8  Wizualizacja Danych: z przykładami w R",
    "section": "8.3 Histogramy",
    "text": "8.3 Histogramy\nHistogramy wizualizują rozkład zmiennej ciągłej poprzez podzielenie jej na przedziały (bins) i pokazanie częstości lub gęstości punktów danych w każdym przedziale.\n\nZrozumienie Histogramów\nGłówne elementy histogramu: 1. Oś X: Reprezentuje wartości zmiennej, podzielone na przedziały 2. Oś Y: Reprezentuje częstość, względną częstość lub gęstość 3. Słupki: Prostokąt dla każdego przedziału, wysokość odpowiada mierze na osi Y\nIstnieją trzy główne typy histogramów:\n\nHistogram Częstości: Oś Y pokazuje liczbę punktów danych w każdym przedziale.\nHistogram Częstości Względnej: Oś Y pokazuje proporcję punktów danych w każdym przedziale (częstość podzielona przez całkowitą liczbę punktów danych).\nHistogram Gęstości: Oś Y pokazuje gęstość, która jest częstością względną podzieloną przez szerokość przedziału. Całkowita powierzchnia wszystkich słupków sumuje się do 1.\n\n\nPrzykładowe Dane\nUżyjmy zbioru 50 wyników egzaminów studentów (na 100 punktów):\n\nset.seed(123)  # dla powtarzalności\nwyniki &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\nRęcznie Rysowany Histogram\nAby stworzyć histogram częstości ręcznie:\n\nZnajdź zakres danych.\nWybierz liczbę przedziałów (użyjmy 7 przedziałów).\nUtwórz tabelę częstości.\nNarysuj osie X i Y.\nOznacz oś X zakresami przedziałów, a oś Y częstością.\nNarysuj prostokąt dla każdego przedziału, z wysokością odpowiadającą jego częstości.\nDodaj tytuł i etykiety dla obu osi.\n\nDla histogramu częstości względnej, podziel każdą częstość przez całkowitą liczbę punktów danych przed narysowaniem słupków.\nDla histogramu gęstości, podziel częstość względną przez szerokość przedziału przed narysowaniem słupków.\n\n\n\n\n\n\nTip\n\n\n\nLiczba przedziałów może wpłynąć na interpretację. Zbyt mało przedziałów może ukryć ważne cechy, podczas gdy zbyt wiele może wprowadzić szum. Powszechną regułą jest użycie pierwiastka kwadratowego z liczby punktów danych jako liczby przedziałów.\n\n\n\n\nHistogramy w Podstawowym R\n\n# Histogram Częstości\nhist(wyniki, breaks = 7, \n     main = \"Histogram Częstości Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Częstość\")\n\n\n\n\n\n\n\n# Histogram Częstości Względnej\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Częstości Względnej Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Częstość Względna\")\n\n\n\n\n\n\n\n# Histogram Gęstości\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Gęstości Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Gęstość\")\nlines(density(wyniki), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\nHistogramy z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wynik = wyniki)\n\n# Histogram Częstości\nggplot(df, aes(x = wynik)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Częstości Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Częstość\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram Częstości Względnej\nggplot(df, aes(x = wynik, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Częstości Względnej Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Częstość Względna\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Histogram Gęstości\nggplot(df, aes(x = wynik)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Histogram Gęstości Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Gęstość\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nInterpretacja Histogramów\nPodczas interpretacji histogramu zwróć uwagę na:\n\nTendencję Centralną: Gdzie znajduje się szczyt rozkładu?\nRozrzut: Jak szeroki jest rozkład?\nKształt: Czy jest symetryczny, skośny, czy wielomodalny?\nWartości Odstające: Czy są nietypowe wartości daleko od głównego rozkładu?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-pudełkowe-i-wykresy-pudełkowe-tukeya",
    "href": "rozdzial6.html#wykresy-pudełkowe-i-wykresy-pudełkowe-tukeya",
    "title": "8  Wizualizacja Danych: z przykładami w R",
    "section": "8.4 Wykresy Pudełkowe i Wykresy Pudełkowe Tukeya",
    "text": "8.4 Wykresy Pudełkowe i Wykresy Pudełkowe Tukeya\nWykresy pudełkowe, znane również jako wykresy skrzynkowe, dostarczają zwięzłego podsumowania rozkładu. Skupimy się na wykresie pudełkowym w stylu Tukeya, nazwanym na cześć statystyka Johna Tukeya, który spopularyzował ten typ wykresu.\n\nZrozumienie Wykresów Pudełkowych\nWykres pudełkowy przedstawia pięć kluczowych statystyk:\n\nWartość minimalna (z wyłączeniem wartości odstających)\nPierwszy kwartyl (Q1)\nMediana\nTrzeci kwartyl (Q3)\nWartość maksymalna (z wyłączeniem wartości odstających)\n\nDodatkowo wykresy pudełkowe pokazują:\n\nWąsy: Linie rozciągające się od pudełka do wartości minimalnej i maksymalnej (z wyłączeniem wartości odstających)\nWartości odstające: Indywidualne punkty poza wąsami\n\n\nObliczanie Kwartyli i Wartości Odstających\nAby stworzyć wykres pudełkowy, postępuj zgodnie z tymi krokami:\n\nUporządkuj dane od najmniejszej do największej wartości.\nZnajdź medianę (środkowa wartość dla nieparzystej liczby punktów danych, średnia z dwóch środkowych wartości dla parzystej).\nZnajdź Q1 (mediana dolnej połowy danych) i Q3 (mediana górnej połowy danych).\nOblicz Rozstęp Międzykwartylowy (IQR) = Q3 - Q1\nOkreśl wartości odstające używając reguły Tukeya:\n\nDolne wartości odstające: &lt; Q1 - 1.5 * IQR\nGórne wartości odstające: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nWspółczynnik 1.5 w regule Tukeya dla wartości odstających opiera się na właściwościach rozkładu normalnego. Dla danych o rozkładzie normalnym, ta reguła identyfikuje około 0.7% danych jako potencjalne wartości odstające.\n\n\n\n\nPrzykładowe Dane\nUżyjmy małego zbioru danych do ilustracji:\n\ndane &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\nRęcznie Rysowany Wykres Pudełkowy Tukeya\nAby stworzyć wykres pudełkowy Tukeya ręcznie:\n\nNarysuj linię pionową reprezentującą zakres od minimum do maksimum (2 do 15 w naszym przykładzie, z wyłączeniem wartości odstającej).\nNarysuj pudełko od Q1 do Q3.\nNarysuj poziomą linię przez pudełko na poziomie mediany.\nNarysuj wąsy od pudełka do wartości minimalnej i maksymalnej (z wyłączeniem wartości odstających).\nPrzedstaw wartość odstającą (50) jako indywidualny punkt poza wąsem.\nDodaj skalę do osi pionowej i oznacz ją.\n\n\n\nWykres Pudełkowy w Podstawowym R\n\n# Tworzenie wykresu pudełkowego\nboxplot(dane, main = \"Wykres Pudełkowy Przykładowych Danych\",\n        ylab = \"Wartości\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\nWykres Pudełkowy Tukeya z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wartosc = dane)\n\n# Tworzenie wykresu pudełkowego Tukeya z ggplot2\nggplot(df, aes(x = \"\", y = wartosc)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Wykres Pudełkowy Tukeya Przykładowych Danych\",\n       x = \"\", y = \"Wartości\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\nInterpretacja Wykresów Pudełkowych\nPodczas interpretacji wykresu pudełkowego zwróć uwagę na następujące elementy:\n\nTendencja Centralna: Mediana pokazuje środek rozkładu.\nRozrzut: Pudełko (IQR) reprezentuje środkowe 50% danych.\nSkośność: Jeśli linia mediany jest bliżej jednego końca pudełka, rozkład jest skośny.\nWartości Odstające: Punkty poza wąsami są potencjalnymi wartościami odstającymi.\nPorównania: Przy porównywaniu wielu wykresów pudełkowych, zwróć uwagę na względne położenie median, rozmiary pudełek i obecność wartości odstających.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "href": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "title": "8  Wizualizacja Danych: z przykładami w R",
    "section": "8.5 Zaawansowane Techniki Wizualizacji",
    "text": "8.5 Zaawansowane Techniki Wizualizacji\nOprócz podstawowych typów wykresów, warto poznać kilka bardziej zaawansowanych technik wizualizacji, które mogą być przydatne w analizie danych.\n\nWykresy Skrzypcowe\nWykresy skrzypcowe łączą cechy wykresów pudełkowych i wykresów gęstości, dając bardziej kompletny obraz rozkładu danych.\n\n# Tworzenie wykresu skrzypcowego\nggplot(df, aes(x = \"\", y = wartosc)) +\n  geom_violin(fill = \"lightblue\") +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Wykres Skrzypcowy Przykładowych Danych\",\n       x = \"\", y = \"Wartości\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWykresy Rozrzutu z Marginesami\nŁączenie wykresów rozrzutu z histogramami na marginesach może dostarczyć więcej informacji o rozkładzie danych w dwóch wymiarach.\n\n# Generowanie danych do wykresu rozrzutu\nset.seed(123)\ndf_scatter &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100)\n)\n\n# Tworzenie wykresu rozrzutu z marginesami\nlibrary(ggExtra)\np &lt;- ggplot(df_scatter, aes(x = x, y = y)) +\n  geom_point() +\n  theme_minimal()\nggMarginal(p, type = \"histogram\", fill = \"lightblue\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wnioski",
    "href": "rozdzial6.html#wnioski",
    "title": "8  Wizualizacja Danych: z przykładami w R",
    "section": "8.6 Wnioski",
    "text": "8.6 Wnioski\nW tym rozdziale poznaliśmy trzy podstawowe typy wizualizacji danych: wykresy słupkowe, histogramy i wykresy pudełkowe. Pokazaliśmy, jak tworzyć te wykresy ręcznie, używając podstawowego systemu wykresów R oraz biblioteki ggplot2.\nKażdy typ wykresu służy innemu celowi: - Wykresy słupkowe doskonale nadają się do porównywania kategorii. - Histogramy pokazują rozkład zmiennej ciągłej. - Wykresy pudełkowe dostarczają zwięzłego podsumowania rozkładu, podkreślając tendencję centralną, rozrzut i wartości odstające.\nPamiętaj, że wybór wizualizacji zależy od typu danych i wniosków, które chcesz przekazać. Zawsze bierz pod uwagę swoją docelową grupę odbiorców i historię, którą chcesz opowiedzieć za pomocą swoich danych, wybierając i projektując wizualizacje.\nĆwicz tworzenie tych wykresów ręcznie, aby pogłębić zrozumienie ich konstrukcji i interpretacji. Następnie wykorzystaj moc R i ggplot2, aby szybko tworzyć i dostosowywać te wizualizacje dla większych zbiorów danych i bardziej złożonych analiz.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#ćwiczenia-praktyczne",
    "href": "rozdzial6.html#ćwiczenia-praktyczne",
    "title": "8  Wizualizacja Danych: z przykładami w R",
    "section": "8.7 Ćwiczenia Praktyczne",
    "text": "8.7 Ćwiczenia Praktyczne\n\nZbierz dane o popularności różnych gatunków muzycznych wśród Twoich znajomych. Stwórz wykres słupkowy przedstawiający te dane.\nZmierz czas reakcji 30 osób na bodziec dźwiękowy (w milisekundach). Utwórz histogram tych danych.\nZbierz dane o wzroście 50 osób w Twojej społeczności. Stwórz wykres pudełkowy dla tych danych, osobno dla mężczyzn i kobiet.\nZnajdź zestaw danych online (np. na Kaggle) i stwórz trzy różne wizualizacje dla tych danych. Opisz, jakie wnioski można wyciągnąć z każdej wizualizacji.\nStwórz wykres skrzypcowy dla danych o cenach domów w różnych dzielnicach miasta. Porównaj go z wykresem pudełkowym tych samych danych. Jakie dodatkowe informacje dostarcza wykres skrzypcowy?\n\nPamiętaj, że praktyka jest kluczem do opanowania sztuki wizualizacji danych. Eksperymentuj z różnymi typami wykresów i parametrami, aby znaleźć najlepszy sposób przedstawienia swoich danych.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "correg_en.html",
    "href": "correg_en.html",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "",
    "text": "9.1 Introduction\nThe distinction between correlation and causation represents a fundamental challenge in statistical analysis. While correlation measures the statistical association between variables, causation implies a direct influence of one variable on another.\nStatistical relationships form the backbone of data-driven decision making across disciplines—from economics and public health to psychology and environmental science. Understanding when a relationship indicates mere association versus genuine causality is crucial for valid inference and effective policy recommendations.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#covariance",
    "href": "correg_en.html#covariance",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.2 Covariance",
    "text": "9.2 Covariance\nCovariance measures how two variables vary together, indicating both the direction and magnitude of their linear relationship.\nFormula: \\text{cov}(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\nWhere:\n\nx_i and y_i are individual data points\n\\bar{x} and \\bar{y} are the means of variables X and Y\nn is the number of observations\nWe divide by (n-1) for sample covariance (Bessel’s correction)\n\n\nStep-by-Step Manual Calculation Process\nExample 1: Student Study Hours vs. Test Scores\nData:\n\nStudy Hours (X): 2, 4, 6, 8, 10\nTest Scores (Y): 65, 70, 80, 85, 95\n\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate means\n\\bar{x} = \\frac{2+4+6+8+10}{5} = 6 hours\n\n\n\n\n\\bar{y} = \\frac{65+70+80+85+95}{5} = 79 points\n\n\n2\nCalculate deviations\n(x_i - \\bar{x}): -4, -2, 0, 2, 4\n\n\n\n\n(y_i - \\bar{y}): -14, -9, 1, 6, 16\n\n\n3\nCalculate products\n(x_i - \\bar{x})(y_i - \\bar{y}):\n\n\n\n\n(-4)(-14) = 56\n\n\n\n\n(-2)(-9) = 18\n\n\n\n\n(0)(1) = 0\n\n\n\n\n(2)(6) = 12\n\n\n\n\n(4)(16) = 64\n\n\n4\nSum the products\n\\sum = 56 + 18 + 0 + 12 + 64 = 150\n\n\n5\nDivide by (n-1)\n\\text{cov}(X,Y) = \\frac{150}{5-1} = \\frac{150}{4} = 37.5\n\n\n\nR Verification:\n\n# Define the data\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Calculate covariance\ncov_manual &lt;- cov(study_hours, test_scores)\nprint(paste(\"Covariance:\", round(cov_manual, 2)))\n\n[1] \"Covariance: 37.5\"\n\n# Verify step by step\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\nproducts &lt;- x_dev * y_dev\ncov_calculated &lt;- sum(products) / (length(study_hours) - 1)\n\n# Display calculation steps\ndata.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_deviation = x_dev,\n  Y_deviation = y_dev,\n  Product = products\n)\n\n   X  Y X_deviation Y_deviation Product\n1  2 65          -4         -14      56\n2  4 70          -2          -9      18\n3  6 80           0           1       0\n4  8 85           2           6      12\n5 10 95           4          16      64\n\n\nInterpretation: The positive covariance (37.5) indicates that study hours and test scores tend to increase together.\n\n\nPractice Problem with Solution\nCalculate covariance manually for:\n\nTemperature (°F): 32, 50, 68, 86, 95\nIce Cream Sales ($): 100, 200, 400, 600, 800\n\nSolution:\n\n\n\nStep\nCalculation\n\n\n\n\n1. Means\n\\bar{x} = \\frac{32+50+68+86+95}{5} = 66.2°F\n\n\n\n\\bar{y} = \\frac{100+200+400+600+800}{5} = 420\n\n\n2. Deviations\nX: -34.2, -16.2, 1.8, 19.8, 28.8\n\n\n\nY: -320, -220, -20, 180, 380\n\n\n3. Products\n10944, 3564, -36, 3564, 10944\n\n\n4. Sum\n28980\n\n\n5. Covariance\n\\frac{28980}{4} = 7245\n\n\n\n\n# Verify practice problem\ntemp &lt;- c(32, 50, 68, 86, 95)\nsales &lt;- c(100, 200, 400, 600, 800)\nprint(paste(\"Covariance for practice problem:\", round(cov(temp, sales), 2)))\n\n[1] \"Covariance for practice problem: 7245\"",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#correlation-coefficient",
    "href": "correg_en.html#correlation-coefficient",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.3 Correlation Coefficient",
    "text": "9.3 Correlation Coefficient\nThe correlation coefficient standardizes covariance to eliminate scale dependency, producing values between -1 and +1.\n\nInterpretation Guidelines\n\n\n\n\n\n\n\n\n\nCorrelation Value\nStrength\nInterpretation\nExample\n\n\n\n\n±0.90 to ±1.00\nVery Strong\nAlmost perfect relationship\nHeight of parents and children\n\n\n±0.70 to ±0.89\nStrong\nHighly related variables\nStudy time and grades\n\n\n±0.50 to ±0.69\nModerate\nModerately related\nExercise and weight loss\n\n\n±0.30 to ±0.49\nWeak\nWeakly related\nShoe size and reading ability\n\n\n±0.00 to ±0.29\nVery Weak/None\nLittle to no relationship\nBirth month and intelligence\n\n\n\n\n\nTypes of Correlations Visualization\n\n# Generate sample data with different correlation patterns\nn &lt;- 100\n\n# Positive linear correlation\nset.seed(123)\nyears_education &lt;- rnorm(n, 14, 3)\nannual_income &lt;- 15000 + 3500 * years_education + rnorm(n, 0, 10000)\npos_cor &lt;- round(cor(years_education, annual_income), 3)\n\n# Negative linear correlation\nscreen_time &lt;- runif(n, 1, 8)\nsleep_hours &lt;- 9 - 0.5 * screen_time + rnorm(n, 0, 0.5)\nneg_cor &lt;- round(cor(screen_time, sleep_hours), 3)\n\n# No correlation\nx_random &lt;- rnorm(n, 50, 10)\ny_random &lt;- rnorm(n, 50, 10)\nno_cor &lt;- round(cor(x_random, y_random), 3)\n\n# Non-linear correlation (quadratic)\nhours_studied &lt;- runif(n, 0, 12)\ntest_score &lt;- -2 * (hours_studied - 6)^2 + 90 + rnorm(n, 0, 5)\nnonlin_cor &lt;- round(cor(hours_studied, test_score), 3)\n\n# Create data frames with correlation values\npositive_data &lt;- data.frame(\n  x = years_education, \n  y = annual_income,\n  label = paste0(\"Positive Linear (r = \", pos_cor, \")\")\n)\n\nnegative_data &lt;- data.frame(\n  x = screen_time, \n  y = sleep_hours,\n  label = paste0(\"Negative Linear (r = \", neg_cor, \")\")\n)\n\nno_corr_data &lt;- data.frame(\n  x = x_random, \n  y = y_random,\n  label = paste0(\"No Correlation (r = \", no_cor, \")\")\n)\n\nnonlinear_data &lt;- data.frame(\n  x = hours_studied, \n  y = test_score,\n  label = paste0(\"Non-linear (Pearson r = \", nonlin_cor, \")\")\n)\n\n# Combine data\nall_data &lt;- rbind(positive_data, negative_data, no_corr_data, nonlinear_data)\n\n# Create faceted plot\nggplot(all_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE, alpha = 0.3) +\n  facet_wrap(~ label, scales = \"free\", ncol = 2) +\n  labs(\n    title = \"Different Types of Correlations\",\n    subtitle = \"Linear regression line shown in red with confidence band\",\n    x = \"\", \n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, color = \"gray50\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#pearson-correlation",
    "href": "correg_en.html#pearson-correlation",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.4 Pearson Correlation",
    "text": "9.4 Pearson Correlation\nFormula: r = \\frac{\\text{cov}(X,Y)}{s_X s_Y} = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}}\n\nComplete Manual Calculation Example\nUsing our study hours example:\n\nStudy Hours (X): 2, 4, 6, 8, 10\nTest Scores (Y): 65, 70, 80, 85, 95\n\nDetailed Calculation Steps:\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate covariance\nFrom above: \\text{cov}(X,Y) = 37.5\n\n\n2\nCalculate deviations squared\n\n\n\n\nFor X\n(x_i - \\bar{x})^2: 16, 4, 0, 4, 16\n\n\n\n\nSum = 40\n\n\n\nFor Y\n(y_i - \\bar{y})^2: 196, 81, 1, 36, 256\n\n\n\n\nSum = 570\n\n\n3\nCalculate standard deviations\n\n\n\n\ns_X\ns_X = \\sqrt{\\frac{40}{4}} = \\sqrt{10} = 3.162\n\n\n\ns_Y\ns_Y = \\sqrt{\\frac{570}{4}} = \\sqrt{142.5} = 11.937\n\n\n4\nCalculate correlation\nr = \\frac{37.5}{3.162 \\times 11.937}\n\n\n\n\nr = \\frac{37.5}{37.73} = 0.994\n\n\n\n\n# Manual calculation verification\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Calculate Pearson correlation\nr_value &lt;- cor(study_hours, test_scores, method = \"pearson\")\nprint(paste(\"Pearson correlation coefficient:\", round(r_value, 3)))\n\n[1] \"Pearson correlation coefficient: 0.993\"\n\n# Detailed calculation\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\n\n# Show calculation table\ncalc_table &lt;- data.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_dev = x_dev,\n  Y_dev = y_dev,\n  X_dev_sq = x_dev^2,\n  Y_dev_sq = y_dev^2,\n  Product = x_dev * y_dev\n)\nprint(calc_table)\n\n   X  Y X_dev Y_dev X_dev_sq Y_dev_sq Product\n1  2 65    -4   -14       16      196      56\n2  4 70    -2    -9        4       81      18\n3  6 80     0     1        0        1       0\n4  8 85     2     6        4       36      12\n5 10 95     4    16       16      256      64\n\n# Summary statistics\ncat(\"\\nSummary:\")\n\n\nSummary:\n\ncat(\"\\nSum of (X-mean)²:\", sum(x_dev^2))\n\n\nSum of (X-mean)²: 40\n\ncat(\"\\nSum of (Y-mean)²:\", sum(y_dev^2))\n\n\nSum of (Y-mean)²: 570\n\ncat(\"\\nSum of products:\", sum(x_dev * y_dev))\n\n\nSum of products: 150\n\ncat(\"\\nStandard deviation of X:\", sd(study_hours))\n\n\nStandard deviation of X: 3.162278\n\ncat(\"\\nStandard deviation of Y:\", sd(test_scores))\n\n\nStandard deviation of Y: 11.93734\n\n# Calculate confidence interval and p-value\ncor_test &lt;- cor.test(study_hours, test_scores, method = \"pearson\")\nprint(cor_test)\n\n\n    Pearson's product-moment correlation\n\ndata:  study_hours and test_scores\nt = 15, df = 3, p-value = 0.0006431\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8994446 0.9995859\nsample estimates:\n      cor \n0.9933993 \n\n\nInterpretation: r = 0.994 indicates an almost perfect positive linear relationship between study hours and test scores. The p-value &lt; 0.05 suggests this relationship is statistically significant.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#spearman-rank-correlation",
    "href": "correg_en.html#spearman-rank-correlation",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.5 Spearman Rank Correlation",
    "text": "9.5 Spearman Rank Correlation\nSpearman correlation measures monotonic relationships using ranks instead of raw values.\nFormula: \\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2 - 1)}\nWhere d_i is the difference between ranks for observation i.\n\nComplete Manual Example\nData: Math and English Scores\n\n\n\nStudent\nMath Score\nEnglish Score\n\n\n\n\nA\n78\n82\n\n\nB\n85\n90\n\n\nC\n88\n88\n\n\nD\n92\n94\n\n\nE\n95\n96\n\n\n\nRanking and Calculation:\n\n\n\n\n\n\n\n\n\n\n\n\nStudent\nMath Score\nMath Rank\nEnglish Score\nEnglish Rank\nd = (Math Rank - English Rank)\nd²\n\n\n\n\nA\n78\n1\n82\n1\n0\n0\n\n\nB\n85\n2\n90\n3\n-1\n1\n\n\nC\n88\n3\n88\n2\n1\n1\n\n\nD\n92\n4\n94\n4\n0\n0\n\n\nE\n95\n5\n96\n5\n0\n0\n\n\n\n\n\n\n\nSum:\n2\n\n\n\nCalculation: \\rho = 1 - \\frac{6 \\times 2}{5(25-1)} = 1 - \\frac{12}{120} = 1 - 0.1 = 0.9\n\n# Data\nmath_scores &lt;- c(78, 85, 88, 92, 95)\nenglish_scores &lt;- c(82, 90, 88, 94, 96)\n\n# Show ranks\nrank_table &lt;- data.frame(\n  Student = LETTERS[1:5],\n  Math_Score = math_scores,\n  Math_Rank = rank(math_scores),\n  English_Score = english_scores,\n  English_Rank = rank(english_scores)\n)\n\nrank_table$d &lt;- rank_table$Math_Rank - rank_table$English_Rank\nrank_table$d_squared &lt;- rank_table$d^2\n\nprint(rank_table)\n\n  Student Math_Score Math_Rank English_Score English_Rank  d d_squared\n1       A         78         1            82            1  0         0\n2       B         85         2            90            3 -1         1\n3       C         88         3            88            2  1         1\n4       D         92         4            94            4  0         0\n5       E         95         5            96            5  0         0\n\ncat(\"\\nSum of d²:\", sum(rank_table$d_squared))\n\n\nSum of d²: 2\n\n# Calculate Spearman correlation\nrho &lt;- cor(math_scores, english_scores, method = \"spearman\")\ncat(\"\\nSpearman correlation (R):\", round(rho, 3))\n\n\nSpearman correlation (R): 0.9\n\n# Manual calculation\nn &lt;- length(math_scores)\nrho_manual &lt;- 1 - (6 * sum(rank_table$d_squared)) / (n * (n^2 - 1))\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.9",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#cross-tabulation-and-categorical-data",
    "href": "correg_en.html#cross-tabulation-and-categorical-data",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.6 Cross-tabulation and Categorical Data",
    "text": "9.6 Cross-tabulation and Categorical Data\nCross-tabulation shows relationships between categorical variables.\n\n# Create more realistic sample data\nset.seed(123)\nn_total &lt;- 120\n\n# Create education and employment data with realistic relationship\neducation &lt;- factor(\n  c(rep(\"High School\", 40), \n    rep(\"College\", 50), \n    rep(\"Graduate\", 30)),\n  levels = c(\"High School\", \"College\", \"Graduate\")\n)\n\n# Employment status with education-related probabilities\nemployment &lt;- factor(\n  c(# High School: 75% employed\n    sample(c(rep(\"Employed\", 30), rep(\"Unemployed\", 10)), 40),\n    # College: 90% employed  \n    sample(c(rep(\"Employed\", 45), rep(\"Unemployed\", 5)), 50),\n    # Graduate: 95% employed\n    sample(c(rep(\"Employed\", 28), rep(\"Unemployed\", 2)), 30)),\n  levels = c(\"Employed\", \"Unemployed\")\n)\n\n# Create contingency table\ncontingency_table &lt;- table(education, employment)\nprint(\"Contingency Table:\")\n\n[1] \"Contingency Table:\"\n\nprint(contingency_table)\n\n             employment\neducation     Employed Unemployed\n  High School       30         10\n  College           45          5\n  Graduate          28          2\n\n# Calculate row percentages\nprint(\"\\nRow Percentages (% within each education level):\")\n\n[1] \"\\nRow Percentages (% within each education level):\"\n\nprint(round(prop.table(contingency_table, 1) * 100, 1))\n\n             employment\neducation     Employed Unemployed\n  High School     75.0       25.0\n  College         90.0       10.0\n  Graduate        93.3        6.7\n\n# Chi-square test for independence\nchi_test &lt;- chisq.test(contingency_table)\n\nWarning in chisq.test(contingency_table): Chi-squared approximation may be\nincorrect\n\nprint(chi_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 5.9623, df = 2, p-value = 0.05073",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#practical-exercises-with-solutions",
    "href": "correg_en.html#practical-exercises-with-solutions",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.7 Practical Exercises with Solutions",
    "text": "9.7 Practical Exercises with Solutions\n\nExercise 1: Calculate Pearson Correlation Manually\nData:\n\nHeight (inches): 66, 68, 70, 72, 74\nWeight (pounds): 140, 155, 170, 185, 200\n\nSolution:\n\n# Data\nheight &lt;- c(66, 68, 70, 72, 74)\nweight &lt;- c(140, 155, 170, 185, 200)\n\n# Step 1: Calculate means\nh_mean &lt;- mean(height)  # 70\nw_mean &lt;- mean(weight)  # 170\n\n# Step 2: Calculate deviations and products\ncalculation_df &lt;- data.frame(\n  Height = height,\n  Weight = weight,\n  H_dev = height - h_mean,\n  W_dev = weight - w_mean,\n  H_dev_sq = (height - h_mean)^2,\n  W_dev_sq = (weight - w_mean)^2,\n  Product = (height - h_mean) * (weight - w_mean)\n)\n\nprint(calculation_df)\n\n  Height Weight H_dev W_dev H_dev_sq W_dev_sq Product\n1     66    140    -4   -30       16      900     120\n2     68    155    -2   -15        4      225      30\n3     70    170     0     0        0        0       0\n4     72    185     2    15        4      225      30\n5     74    200     4    30       16      900     120\n\n# Step 3: Calculate correlation\ncov_hw &lt;- sum(calculation_df$Product) / (length(height) - 1)\nsd_h &lt;- sqrt(sum(calculation_df$H_dev_sq) / (length(height) - 1))\nsd_w &lt;- sqrt(sum(calculation_df$W_dev_sq) / (length(weight) - 1))\nr_manual &lt;- cov_hw / (sd_h * sd_w)\n\ncat(\"\\nManual calculation:\")\n\n\nManual calculation:\n\ncat(\"\\nCovariance:\", round(cov_hw, 2))\n\n\nCovariance: 75\n\ncat(\"\\nSD Height:\", round(sd_h, 2))\n\n\nSD Height: 3.16\n\ncat(\"\\nSD Weight:\", round(sd_w, 2))\n\n\nSD Weight: 23.72\n\ncat(\"\\nCorrelation:\", round(r_manual, 3))\n\n\nCorrelation: 1\n\n# Verify with R\ncat(\"\\n\\nR calculation:\", round(cor(height, weight), 3))\n\n\n\nR calculation: 1\n\n\n\n\nExercise 2: Calculate Spearman Correlation Manually\nData:\n\nStudent rankings in Math: 1, 3, 2, 5, 4\nStudent rankings in Science: 2, 4, 1, 5, 3\n\nSolution:\n\n# Rankings (already ranked)\nmath_rank &lt;- c(1, 3, 2, 5, 4)\nscience_rank &lt;- c(2, 4, 1, 5, 3)\n\n# Calculate differences\nd &lt;- math_rank - science_rank\nd_squared &lt;- d^2\n\n# Create table\nrank_df &lt;- data.frame(\n  Student = 1:5,\n  Math_Rank = math_rank,\n  Science_Rank = science_rank,\n  d = d,\n  d_squared = d_squared\n)\n\nprint(rank_df)\n\n  Student Math_Rank Science_Rank  d d_squared\n1       1         1            2 -1         1\n2       2         3            4 -1         1\n3       3         2            1  1         1\n4       4         5            5  0         0\n5       5         4            3  1         1\n\n# Calculate Spearman correlation\nn &lt;- length(math_rank)\nsum_d_sq &lt;- sum(d_squared)\nrho_manual &lt;- 1 - (6 * sum_d_sq) / (n * (n^2 - 1))\n\ncat(\"\\nSum of d²:\", sum_d_sq)\n\n\nSum of d²: 4\n\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.8\n\ncat(\"\\nSpearman correlation (R):\", round(cor(math_rank, science_rank, method = \"spearman\"), 3))\n\n\nSpearman correlation (R): 0.8\n\n\n\n\nExercise 3: Interpretation Practice\nInterpret these correlation values:\n\nr = 0.85 between hours of practice and performance score\n\nAnswer: Strong positive relationship. As practice hours increase, performance scores tend to increase substantially.\n\nr = -0.72 between outside temperature and heating costs\n\nAnswer: Strong negative relationship. As temperature increases, heating costs decrease substantially.\n\nr = 0.12 between shoe size and intelligence\n\nAnswer: Very weak/no meaningful relationship. Shoe size and intelligence are essentially unrelated.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#important-points-to-remember",
    "href": "correg_en.html#important-points-to-remember",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.8 Important Points to Remember",
    "text": "9.8 Important Points to Remember\n\nCorrelation measures relationship strength: Values range from -1 to +1\nCorrelation ≠ Causation: High correlation doesn’t prove one variable causes another\nChoose the right method:\n\nPearson: For linear relationships in continuous data\nSpearman: For monotonic relationships or ranked data\n\nCheck assumptions:\n\nPearson assumes linear relationship and normal distribution\nSpearman only assumes monotonic relationship\n\nWatch for outliers: Extreme values can greatly affect Pearson correlation\nVisualize your data: Always plot before calculating correlation",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-decision-tree-for-correlation-analysis",
    "href": "correg_en.html#summary-decision-tree-for-correlation-analysis",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.9 Summary: Decision Tree for Correlation Analysis",
    "text": "9.9 Summary: Decision Tree for Correlation Analysis\n\n\n\nCHOOSING THE RIGHT CORRELATION METHOD:\n\nIs your data numerical?\n├─ YES → Is the relationship linear?\n│   ├─ YES → Use PEARSON correlation\n│   └─ NO → Is it monotonic?\n│       ├─ YES → Use SPEARMAN correlation\n│       └─ NO → Consider non-linear methods\n└─ NO → Is it ordinal (ranked)?\n    ├─ YES → Use SPEARMAN correlation\n    └─ NO → Use CROSS-TABULATION for categorical data\n\n\n\nQuick Reference Card\n\n\n\n\n\n\n\n\n\nMeasure\nUse When\nFormula\nRange\n\n\n\n\nCovariance\nInitial exploration of relationship\n\\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n-∞ to +∞\n\n\nPearson r\nLinear relationships, continuous data\n\\frac{\\text{cov}(X,Y)}{s_X s_Y}\n-1 to +1\n\n\nSpearman ρ\nMonotonic relationships, ranked data\n1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n-1 to +1\n\n\nCross-tabs\nCategorical variables\nFrequency counts\nN/A",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#understanding-ordinary-least-squares-ols-a-quick-start-guide",
    "href": "correg_en.html#understanding-ordinary-least-squares-ols-a-quick-start-guide",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.10 Understanding Ordinary Least Squares (OLS): A Quick-start Guide",
    "text": "9.10 Understanding Ordinary Least Squares (OLS): A Quick-start Guide\n\n\n\n\n\n\nUnderstanding Ordinary Least Squares (OLS): A Quick-start Guide\n\n\n\n\nIntroduction: What is Regression Analysis?\nRegression analysis helps us understand and measure relationships between things we can observe. It provides mathematical tools to identify patterns in data that help us make predictions.\nConsider these research questions:\n\nHow does study time affect test scores?\nHow does experience affect salary?\nHow does advertising spending influence sales?\n\nRegression gives us systematic methods to answer these questions with real data.\n\n\nThe Starting Point: A Simple Example\nLet’s begin with something concrete. You’ve collected data from 20 students in your class:\n\n\n\nStudent\nStudy Hours\nExam Score\n\n\n\n\nAlex\n2\n68\n\n\nBeth\n4\n74\n\n\nCarlos\n6\n85\n\n\nDiana\n8\n91\n\n\n…\n…\n…\n\n\n\nWhen you plot this data, you get a scatter plot with dots all over. Your goal: find the straight line that best describes the relationship between study hours and exam scores.\nBut what does “best” mean? That’s what we’ll discover.\n\n\nWhy Real Data Doesn’t Fall on a Perfect Line\nBefore diving into the math, let’s understand why data points don’t line up perfectly.\n\nDeterministic vs. Stochastic Models\nDeterministic Models describe relationships with no uncertainty. Think of physics equations: \\text{Distance} = \\text{Speed} × \\text{Time}\nIf you drive at exactly 60 mph for exactly 2 hours, you’ll always travel exactly 120 miles. No variation, no exceptions.\nStochastic Models acknowledge that real-world data contains natural variation. The fundamental structure is: Y = f(X) + \\epsilon\nWhere:\n\nY is what we’re trying to predict (exam scores)\nf(X) is the systematic pattern (how study hours typically affect scores)\n\\epsilon (epsilon) represents all the random stuff we can’t measure\n\nIn our example, two students might study for 5 hours but get different scores because:\n\nOne slept better the night before\nOne is naturally better at test-taking\nOne had a noisy roommate during the exam\nPure chance in which questions were asked\n\nThis randomness is natural and expected - that’s what \\epsilon captures.\n\n\n\nThe Simple Linear Regression Model\nWe express the relationship between study hours and exam scores as: Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\nLet’s decode this:\n\nY_i = exam score for student i\nX_i = study hours for student i\n\\beta_0 = the intercept (baseline score with zero study hours)\n\\beta_1 = the slope (points gained per study hour)\n\\epsilon_i = everything else affecting student i’s score\n\nKey insight: We never know the true values of \\beta_0 and \\beta_1. Instead, we use our data to estimate them, calling our estimates \\hat{\\beta}_0 and \\hat{\\beta}_1 (the “hats” mean “estimated”).\n\n\nUnderstanding Residuals: How Wrong Are Our Predictions?\nOnce we draw a line through our data, we can make predictions. For each student:\n\nActual score (y_i): What they really got\nPredicted score (\\hat{y}_i): What our line says they should have gotten\nResidual (e_i): The difference = Actual - Predicted\n\nVisual Example:\nDiana: Studied 8 hours, scored 91\nOur line predicts: 88 points\nResidual: 91 - 88 = +3 points (we underestimated)\n\nEric: Studied 5 hours, scored 70\nOur line predicts: 79 points  \nResidual: 70 - 79 = -9 points (we overestimated)\n\n\nThe Key Insight: Why Square the Residuals?\nHere’s a puzzle. Consider these residuals from four students:\n\nStudent A: +5 points\nStudent B: -5 points\nStudent C: +3 points\nStudent D: -3 points\n\nIf we just add them: (+5) + (-5) + (+3) + (-3) = 0\nThis suggests perfect predictions, but every prediction was wrong! The positive and negative errors canceled out.\nThe solution: Square each residual before adding:\n\nStudent A: (+5)^2 = 25\nStudent B: (-5)^2 = 25\nStudent C: (+3)^2 = 9\nStudent D: (-3)^2 = 9\nTotal squared error: 68\n\nWhy squaring works:\n\nNo more cancellation: All squared values are positive\nBigger errors matter more: A 10-point error counts 4× as much as a 5-point error\nMathematical convenience: Squared functions are smooth and differentiable\n\n\n\nThe Ordinary Least Squares Method\nOLS finds the line that minimizes the Sum of Squared Errors (SSE):\n\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\nExpanding this: \\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1x_i))^2\nIn plain English: “Find the intercept and slope that make the total squared prediction error as small as possible.”\n\n\nThe Mathematical Solution (Formal Derivation)\nTo minimize SSE, we use calculus. Taking partial derivatives and setting them to zero:\n\\frac{\\partial SSE}{\\partial \\beta_0} = -2\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i) = 0\n\\frac{\\partial SSE}{\\partial \\beta_1} = -2\\sum_{i=1}^n x_i(y_i - \\beta_0 - \\beta_1x_i) = 0\nSolving this system of equations yields:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\nWhere \\bar{x} and \\bar{y} are the sample means.\nWhat this tells us:\n\nThe slope depends on how X and Y vary together (covariance) relative to how much X varies alone (variance)\nThe line always passes through the center point (\\bar{x}, \\bar{y})\n\n\n\nMaking Sense of Variation: How Good Is Our Line?\nTo evaluate our model’s performance, we break down the variation in exam scores:\n\nTotal Sum of Squares (SST)\n“How much do exam scores vary overall?” SST = \\sum_{i=1}^n(y_i - \\bar{y})^2\nThis measures how spread out the scores are from the class average.\n\n\nRegression Sum of Squares (SSR)\n“How much variation does our line explain?” SSR = \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2\nThis measures how much better our predictions are than just guessing the average for everyone.\n\n\nError Sum of Squares (SSE)\n“How much variation is left unexplained?” SSE = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\nThis is the variation our model couldn’t capture (the squared residuals).\n\n\nThe Fundamental Equation\nSST = SSR + SSE \\text{Total Variation} = \\text{Explained} + \\text{Unexplained}\n\n\n\nR-Squared: The Model Report Card\nThe coefficient of determination (R²) tells us what percentage of variation our model explains:\nR^2 = \\frac{\\text{Explained Variation}}{\\text{Total Variation}} = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nHow to interpret R²:\n\nR² = 0.75: “Study hours explain 75% of the variation in exam scores”\nR² = 0.30: “Our model captures 30% of what makes scores different”\nR² = 1.00: Perfect prediction (never happens with real data)\nR² = 0.00: Our line is no better than guessing the average\n\nImportant reality check: In social sciences, R² = 0.30 might be excellent. In engineering, R² = 0.95 might be the minimum acceptable. Context matters.\n\n\nInterpreting Your Results\nWhen you run OLS and get \\hat{\\beta}_0 = 60 and \\hat{\\beta}_1 = 4:\nThe Slope (\\hat{\\beta}_1 = 4):\n\n“Each additional hour of study is associated with 4 more points on the exam”\nThis is an average effect across all students\nIt’s not a guarantee for any individual student\n\nThe Intercept (\\hat{\\beta}_0 = 60):\n\n“A student who studies 0 hours is predicted to score 60”\nOften this is just a mathematical anchor point\nMay not make practical sense (who studies 0 hours?)\n\nThe Prediction Equation: \\text{Predicted Score} = 60 + 4 \\times \\text{Study Hours}\nSo a student studying 5 hours: Predicted score = 60 + 4(5) = 80 points\n\n\nEffect Size and Practical Significance\nStatistical significance tells us whether an effect exists. Practical significance tells us whether it matters. Understanding both is crucial for proper interpretation.\n\nCalculating and Interpreting Raw Effect Sizes\nThe raw (unstandardized) effect size is simply your slope coefficient \\hat{\\beta}_1.\nExample: If \\hat{\\beta}_1 = 4 points per hour:\n\nThis is the raw effect size\nInterpretation: “One hour of additional study yields 4 exam points”\n\nTo assess practical significance, consider:\n\nScale of the outcome: 4 points on a 100-point exam (4%) vs. 4 points on a 500-point exam (0.8%)\nCost of the intervention: Is one hour of study time worth 4 points?\nContext-specific thresholds: Does 4 points change a letter grade?\n\n\n\nCalculating Standardized Effect Sizes\nStandardized effects allow comparison across different scales and studies.\nFormula for standardized coefficient (beta weight): \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y}\nWhere:\n\ns_X = standard deviation of X (study hours)\ns_Y = standard deviation of Y (exam scores)\n\nStep-by-step calculation:\n\nCalculate the standard deviation of X: s_X = \\sqrt{\\frac{\\sum(x_i - \\bar{x})^2}{n-1}}\nCalculate the standard deviation of Y: s_Y = \\sqrt{\\frac{\\sum(y_i - \\bar{y})^2}{n-1}}\nMultiply: \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y}\n\nExample calculation:\n\nSuppose s_X = 2.5 hours and s_Y = 12 points\nWith \\hat{\\beta}_1 = 4: \\beta_{std} = 4 \\times \\frac{2.5}{12} = 0.83\nInterpretation: “A one standard deviation increase in study hours (2.5 hours) is associated with 0.83 standard deviations increase in exam score”\n\n\n\nCohen’s Guidelines for Effect Sizes\nFor standardized regression coefficients:\n\nSmall effect: |β| ≈ 0.10 (explains ~1% of variance)\nMedium effect: |β| ≈ 0.30 (explains ~9% of variance)\nLarge effect: |β| ≈ 0.50 (explains ~25% of variance)\n\nFor R² (proportion of variance explained):\n\nSmall effect: R² ≈ 0.02\nMedium effect: R² ≈ 0.13\nLarge effect: R² ≈ 0.26\n\nImportant: These are general benchmarks. Field-specific standards often differ:\n\nPsychology/Education: R² = 0.10 might be meaningful\nPhysics/Engineering: R² &lt; 0.90 might be unacceptable\nEconomics: R² = 0.30 might be excellent\n\n\n\nCalculating Confidence Intervals for Effect Sizes\nTo quantify uncertainty in your effect size:\nFor the raw coefficient: CI = \\hat{\\beta}_1 \\pm t_{critical} \\times SE(\\hat{\\beta}_1)\nWhere:\n\nt_{critical} = critical value from t-distribution (usually ≈ 2 for 95% CI)\nSE(\\hat{\\beta}_1) = standard error of the slope\n\nPractical interpretation: If 95% CI = [3.2, 4.8], we can say: “We’re 95% confident that each study hour adds between 3.2 and 4.8 exam points.”\n\n\nMaking Decisions About Practical Significance\nTo determine if an effect is practically significant, consider:\n\nMinimum meaningful difference: What’s the smallest effect that would matter?\n\nIn education: Often 0.25 standard deviations\nIn medicine: Determined by clinical relevance\nIn business: Based on cost-benefit analysis\n\nNumber needed to treat (NNT) analog: How much X must change for meaningful Y change?\n\nIf passing requires 10 more points and \\hat{\\beta}_1 = 4\nStudents need 2.5 more study hours to pass\n\nCost-effectiveness ratio: \\text{Efficiency} = \\frac{\\text{Effect Size}}{\\text{Cost of Intervention}}\n\nExample practical significance assessment:\n\nEffect: 4 points per study hour\nPassing threshold: 70 points\nCurrent average: 68 points\nConclusion: 30 minutes of extra study could change fail to pass\nDecision: Practically significant for borderline students\n\n\n\n\nUnderstanding Uncertainty: Nothing Is Perfect\nYour estimates come from a sample, not the entire population. This creates uncertainty.\n\nWhy We Have Uncertainty\n\nYou studied 20 students, not all students ever\nYour sample might be slightly unusual by chance\nMeasurement isn’t perfect (did students report hours accurately?)\n\n\n\nConfidence Intervals: Being Honest About Uncertainty\nInstead of saying “the effect is exactly 4 points per hour,” we say:\n\n“We estimate 4 points per hour”\n“We’re 95% confident the true effect is between 3.2 and 4.8 points”\n\nThis range (3.2 to 4.8) is called a 95% confidence interval.\nWhat it means: If we repeated this study many times with different samples, 95% of the intervals we calculate would contain the true effect.\nWhat it doesn’t mean: There’s a 95% chance the true value is in this specific interval (it either is or isn’t).\n\n\nTesting If There’s Really a Relationship\nThe big question: “Is there actually a relationship, or did we just get lucky with our sample?”\nWe test this by asking: “If study hours truly had zero effect on scores, how likely would we be to see a pattern this strong just by chance?”\nThe process (simplified):\n\nAssume there’s no relationship (the “null hypothesis”)\nCalculate how unlikely our data would be if that were true\nIf it’s very unlikely (typically less than 5% chance), we conclude there probably is a relationship\n\nP-values in plain English:\n\np = 0.03: “If study hours didn’t matter at all, there’s only a 3% chance we’d see a pattern this strong by luck”\np = 0.40: “This pattern could easily happen by chance even if there’s no real relationship”\n\nRule of thumb: p &lt; 0.05 → “statistically significant” (probably a real relationship)\n\n\n\nWhen Things Go Wrong: Model Diagnostics\n\nQuick Visual Checks\n\nPlot your data first: Does it look roughly linear?\nPlot residuals vs. predicted values: Should look like a random cloud\nLook for outliers: Any points way off from the others?\n\n\n\nWarning Signs Your Model Might Be Misleading\nPattern in residuals: If residuals show a curve or trend, you’re missing something\nIncreasing spread: If residuals get more spread out as predictions increase, standard errors might be wrong\nInfluential outliers: One or two weird points can drag your whole line off\nMissing variables: If you forgot something important (like prior knowledge), your estimates might be biased\n\n\n\nKey Assumptions: When OLS Works Well\n\nLinearity: The true relationship is approximately straight\n\nCheck: Look at your scatter plot\n\nIndependence: Each observation is separate\n\nCheck: Make sure students didn’t work together or copy\n\nConstant variance: The spread of residuals is similar everywhere\n\nCheck: Residual plot shouldn’t fan out\n\nNo perfect multicollinearity: (For multiple regression) Predictors aren’t perfectly related\n\nCheck: Make sure you didn’t include the same variable twice\n\nRandom sampling: Your data represents the population you care about\n\nCheck: Did you sample fairly?\n\n\n\n\nSummary: Your OLS Toolkit\nWhat OLS Does:\n\nFinds the straight line that minimizes squared prediction errors\nEstimates how much Y changes when X changes by one unit\nTells you how much variation your model explains (R²)\nQuantifies uncertainty in your estimates\n\nYour Step-by-Step Process:\n\nPlot your data - does a line make sense?\nRun OLS to get \\hat{\\beta}_0 and \\hat{\\beta}_1\nCheck R² - how much variation do you explain?\nCalculate effect sizes (raw and standardized)\nAssess practical significance using context-specific criteria\nLook at confidence intervals - how uncertain are you?\nCheck residuals - any obvious problems?\nMake decisions based on both statistical and practical significance\n\n\n\nKluczowe interpretacje / Key interpretations\nDomyślny model: regresja OLS Y=\\beta_0+\\beta_1 X+\\varepsilon (lub wieloraka: Y=\\beta_0+\\beta_1 X_1+\\cdots+\\beta_p X_p+\\varepsilon).\n\nNachylenie / Slope (\\beta_1) PL: Przy wzroście X o 1 jednostkę (ceteris paribus), przeciętna wartość Y zmienia się o \\beta_1 jednostek. ENG: When X increases by 1 unit (ceteris paribus), the expected value of Y changes by \\beta_1 units.\nStandaryzowane nachylenie / Standardized slope \\big(\\beta_{1}^{(\\mathrm{std})}\\big) Definicja:\n\n\\beta_{1}^{(\\mathrm{std})} \\;=\\; \\beta_1 \\cdot \\frac{s_X}{s_Y},\n\ngdzie s_X i s_Y to odchylenia standardowe X i Y. PL: Przy wzroście X o 1 odchylenie standardowe (SD), przeciętna wartość Y zmienia się o \\beta_{1}^{(\\mathrm{std})} odchyleń standardowych Y. ENG: For a 1 standard deviation (SD) increase in X, the expected value of Y changes by \\beta_{1}^{(\\mathrm{std})} SDs of Y. Uwaga/Note: W regresji prostej \\beta_{1}^{(\\mathrm{std})} = r (Pearson). / In simple regression, \\beta_{1}^{(\\mathrm{std})} = r (Pearson).\nWspółczynnik determinacji / R^2 Definicja:\n\nR^2 \\;=\\; 1 - \\frac{SS_{\\mathrm{res}}}{SS_{\\mathrm{tot}}}.\n\nPL: Model wyjaśnia 100\\times R^2% zmienności Y względem modelu tylko z wyrazem wolnym (in-sample). ENG: The model explains 100\\times R^2% of the variance in Y relative to the intercept-only model (in-sample). W wielu zmiennych rozważ: \\text{adjusted } R^2. / With multiple predictors consider: adjusted R^2.\nWartość p / P-value Formalnie/Formally:\n\np \\;=\\; \\Pr\\!\\big(\\,|T|\\ge |t_{\\mathrm{obs}}| \\mid H_0\\,\\big),\n\ngdzie T ma rozkład t przy H_0. PL: Zakładając prawdziwość H_0 i spełnione założenia modelu, prawdopodobieństwo uzyskania co najmniej tak ekstremalnej statystyki jak obserwowana wynosi p. ENG: Assuming H_0 and the model assumptions hold, p is the probability of observing a test statistic at least as extreme as the one obtained.\nPrzedział ufności / Confidence interval (np. dla \\beta_1) Konstrukcja/Construction:\n\n\\hat{\\beta}_1 \\;\\pm\\; t_{1-\\alpha/2,\\ \\mathrm{df}} \\cdot \\mathrm{SE}\\!\\left(\\hat{\\beta}_1\\right).\n\nPL (ściśle): W długiej serii powtórzeń 95% tak skonstruowanych przedziałów zawiera prawdziwą wartość \\beta_1; dla naszych danych oszacowanie mieści się w [\\text{lower},\\ \\text{upper}]. ENG (strict): Over many repetitions, 95% of such intervals would contain the true \\beta_1; for our data, the estimate lies within [\\text{lower},\\ \\text{upper}]. PL (skrót dydaktyczny): „Jesteśmy 95% pewni, że \\beta_1 leży w [\\text{lower},\\ \\text{upper}].” ENG (teaching shorthand): “We are 95% confident that \\beta_1 lies in [\\text{lower},\\ \\text{upper}].”\n\n\nNajczęstsze nieporozumienia / Common pitfalls\n\nPL: p nie jest prawdopodobieństwem, że H_0 jest prawdziwa. ENG: p is not the probability that H_0 is true.\nPL: 95% CI nie zawiera 95% obserwacji (od tego jest przedział predykcji). ENG: A 95% CI does not contain 95% of observations (that’s a prediction interval).\nPL/ENG: Wysokie R^2 ≠ przyczynowość / High R^2 ≠ causality. Zawsze sprawdzaj/Always check diagnozy reszt, skalę efektu, i dopasowanie poza próbą.\n\nCritical Reminders:\n\nAssociation does not imply causation\nStatistical significance does not guarantee practical importance\nEvery model is wrong, but some are useful\nAlways visualize your data and residuals\nConsider both effect size and uncertainty when making decisions\n\nOLS provides a principled, mathematical approach to finding patterns in real-world data. While it cannot provide perfect predictions, it offers the best linear approximation possible along with honest assessments of that approximation’s quality and uncertainty.\n\n\n\n9.11 Complete Manual OLS Calculation: A Step-by-Step Example\nA professor wants to understand the relationship between hours spent studying and exam scores. She collects data from 6 students:\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nOur goal: Find the best-fitting line \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X using OLS.\n\n\n9.12 Step 1: Calculate the Means\nFirst, we need the mean of X and Y.\nFor X (study hours): \\bar{X} = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5\nFor Y (exam scores): \\bar{Y} = \\frac{65 + 70 + 75 + 85 + 88 + 95}{6} = \\frac{478}{6} = 79.67\n\n\n9.13 Step 2: Calculate Deviations from Means\nFor each observation, calculate (X_i - \\bar{X}) and (Y_i - \\bar{Y}):\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n1 - 3.5 = -2.5\n65 - 79.67 = -14.67\n\n\nB\n2\n70\n2 - 3.5 = -1.5\n70 - 79.67 = -9.67\n\n\nC\n3\n75\n3 - 3.5 = -0.5\n75 - 79.67 = -4.67\n\n\nD\n4\n85\n4 - 3.5 = 0.5\n85 - 79.67 = 5.33\n\n\nE\n5\n88\n5 - 3.5 = 1.5\n88 - 79.67 = 8.33\n\n\nF\n6\n95\n6 - 3.5 = 2.5\n95 - 79.67 = 15.33\n\n\n\n\n\n9.14 Step 3: Calculate Products and Squares\nNow calculate (X_i - \\bar{X})(Y_i - \\bar{Y}) and (X_i - \\bar{X})^2:\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n(-2.5)(-14.67) = 36.68\n(-2.5)² = 6.25\n\n\nB\n(-1.5)(-9.67) = 14.51\n(-1.5)² = 2.25\n\n\nC\n(-0.5)(-4.67) = 2.34\n(-0.5)² = 0.25\n\n\nD\n(0.5)(5.33) = 2.67\n(0.5)² = 0.25\n\n\nE\n(1.5)(8.33) = 12.50\n(1.5)² = 2.25\n\n\nF\n(2.5)(15.33) = 38.33\n(2.5)² = 6.25\n\n\nSum\n107.03\n17.50\n\n\n\n\n\n9.15 Step 4: Calculate the Slope (\\hat{\\beta}_1)\nUsing the OLS formula: \\hat{\\beta}_1 = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} = \\frac{107.03}{17.50} = 6.12\nInterpretation: Each additional hour of study is associated with a 6.12-point increase in exam score.\n\n\n9.16 Step 5: Calculate the Intercept (\\hat{\\beta}_0)\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X} = 79.67 - (6.12 \\times 3.5) = 79.67 - 21.42 = 58.25\nInterpretation: A student who studies 0 hours is predicted to score 58.25 points.\n\n\n9.17 Step 6: Write the Regression Equation\n\\hat{Y} = 58.25 + 6.12X\n\n\n9.18 Step 7: Calculate Predicted Values and Residuals\nUsing our equation to predict each student’s score:\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nResidual e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n58.25 + 6.12(1) = 64.37\n65 - 64.37 = 0.63\n\n\nB\n2\n70\n58.25 + 6.12(2) = 70.49\n70 - 70.49 = -0.49\n\n\nC\n3\n75\n58.25 + 6.12(3) = 76.61\n75 - 76.61 = -1.61\n\n\nD\n4\n85\n58.25 + 6.12(4) = 82.73\n85 - 82.73 = 2.27\n\n\nE\n5\n88\n58.25 + 6.12(5) = 88.85\n88 - 88.85 = -0.85\n\n\nF\n6\n95\n58.25 + 6.12(6) = 94.97\n95 - 94.97 = 0.03\n\n\n\nCheck: Sum of residuals = 0.63 - 0.49 - 1.61 + 2.27 - 0.85 + 0.03 ≈ 0 ✓\n\n\n9.19 Step 8: Calculate Sum of Squares\n\nTotal Sum of Squares (SST)\nHow much total variation exists in exam scores?\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n(65 - 79.67)² = (-14.67)² = 215.21\n\n\nB\n70\n(70 - 79.67)² = (-9.67)² = 93.51\n\n\nC\n75\n(75 - 79.67)² = (-4.67)² = 21.81\n\n\nD\n85\n(85 - 79.67)² = (5.33)² = 28.41\n\n\nE\n88\n(88 - 79.67)² = (8.33)² = 69.39\n\n\nF\n95\n(95 - 79.67)² = (15.33)² = 235.01\n\n\nSum\n\nSST = 663.34\n\n\n\n\n\nRegression Sum of Squares (SSR)\nHow much variation does our model explain?\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n(64.37 - 79.67)² = (-15.30)² = 234.09\n\n\nB\n70.49\n(70.49 - 79.67)² = (-9.18)² = 84.27\n\n\nC\n76.61\n(76.61 - 79.67)² = (-3.06)² = 9.36\n\n\nD\n82.73\n(82.73 - 79.67)² = (3.06)² = 9.36\n\n\nE\n88.85\n(88.85 - 79.67)² = (9.18)² = 84.27\n\n\nF\n94.97\n(94.97 - 79.67)² = (15.30)² = 234.09\n\n\nSum\n\nSSR = 655.44\n\n\n\n\n\nError Sum of Squares (SSE)\nHow much variation is unexplained?\n\n\n\nStudent\nResidual e_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n-0.49\n0.24\n\n\nC\n-1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n-0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSum\n\nSSE = 9.10\n\n\n\nVerification: SST = SSR + SSE 663.34 ≈ 655.44 + 9.10 = 664.54 ✓ (small rounding difference)\n\n\n\n9.20 Step 9: Calculate R-Squared\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternative formula: R^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 1 - 0.014 = 0.986\n(Small difference due to rounding)\nInterpretation: Study hours explain 98.8% of the variation in exam scores. This is an extremely strong relationship.\n\n\n9.21 Step 10: Calculate Effect Sizes\n\nRaw Effect Size\nThe raw effect size is simply the slope: 6.12 points per hour\n\n\nStandardized Effect Size\nFirst, calculate standard deviations:\nFor X: s_X = \\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{n-1}} = \\sqrt{\\frac{17.50}{5}} = \\sqrt{3.50} = 1.87\nFor Y: s_Y = \\sqrt{\\frac{\\sum(Y_i - \\bar{Y})^2}{n-1}} = \\sqrt{\\frac{663.34}{5}} = \\sqrt{132.67} = 11.52\nStandardized coefficient: \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y} = 6.12 \\times \\frac{1.87}{11.52} = 6.12 \\times 0.162 = 0.99\nInterpretation: A one standard deviation increase in study hours (1.87 hours) is associated with a 0.99 standard deviation increase in exam score.\nAccording to Cohen’s guidelines:\n\nSmall effect: |β| = 0.10\nMedium effect: |β| = 0.30\nLarge effect: |β| = 0.50\n\nOur standardized effect of 0.99 is nearly twice Cohen’s “large effect” threshold.\n\n\n\n9.22 Step 11: Practical Significance Assessment\n\nContext Analysis\n\nScale consideration:\n\nEffect: 6.12 points per hour\nExam scale: 0-100 points\nPercentage impact: 6.12% per hour\n\nPractical thresholds:\n\nLetter grade difference: Often 10 points\nTime to improve one letter grade: 10/6.12 = 1.63 hours\nConclusion: Less than 2 hours of extra study could change a letter grade\n\nCost-benefit analysis:\n\nBenefit: 6.12 points per hour\nCost: 1 hour of time\nDecision: Highly favorable return on investment\n\n\n\n\n\n9.23 Summary of Results\nRegression equation: \\hat{Y} = 58.25 + 6.12X\nKey statistics:\n\nSlope (\\hat{\\beta}_1): 6.12 points/hour\nIntercept (\\hat{\\beta}_0): 58.25 points\nR²: 0.988 (98.8% of variance explained)\nStandardized effect: 0.99 (very large effect)\n\nPractical interpretation:\n\nEach hour of study adds about 6 points to exam score\nThe model fits extremely well (R² near 1)\nThe effect is both statistically and practically significant\nStudents can meaningfully improve grades with modest increases in study time\n\n\n\n9.24 Verification Check\nTo verify our calculations, let’s check that the regression line passes through (\\bar{X}, \\bar{Y}):\n\\hat{Y} = 58.25 + 6.12(3.5) = 58.25 + 21.42 = 79.67 = \\bar{Y} ✓\nThe calculation confirms our regression line passes through the point of means, as it should.\n\n\n9.25 R Code to Verify Manual Calculations\nBelow is R code that checks all our manual calculations. You can run this code to confirm every step.\n\n# Step 1: Create the data\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)  # X variable\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95)  # Y variable\nn &lt;- length(study_hours)  # Sample size\n\n# Create a data frame for easier handling\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Step 2: Calculate means\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Step 3: Calculate deviations from means\ndata$x_dev &lt;- data$X - x_bar  # X deviations\ndata$y_dev &lt;- data$Y - y_bar  # Y deviations\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Step 4: Calculate products and squares for OLS formula\ndata$xy_product &lt;- data$x_dev * data$y_dev  # (Xi - X̄)(Yi - Ȳ)\ndata$x_dev_sq &lt;- data$x_dev^2  # (Xi - X̄)²\n\n# Sum of products and squares\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - X̄)(Yi - Ȳ):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - X̄)(Yi - Ȳ): 107 \n\ncat(\"Sum of (Xi - X̄)²:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - X̄)²: 17.5 \n\n# Step 5: Calculate slope (beta_1) manually\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (β₁) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (β₁) calculated manually: 6.11 \n\n# Step 6: Calculate intercept (beta_0) manually\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (β₀) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (β₀) calculated manually: 58.27 \n\n# Step 7: Compare with R's lm() function\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Step 8: Calculate predicted values and residuals\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X  # Predicted values\ndata$residual &lt;- data$Y - data$Y_hat  # Residuals\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ≈ 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ≈ 0): 0 \n\n# Step 9: Calculate Sum of Squares\n# Total Sum of Squares (SST)\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\n# Regression Sum of Squares (SSR)\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\n# Error Sum of Squares (SSE)\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\n# Verify that SST = SSR + SSE\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Step 10: Calculate R-squared\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R² (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR² (Method 1: SSR/SST): 0.9863 \n\ncat(\"R² (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR² (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R² (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR² (from lm function): 0.9863 \n\n# Step 11: Calculate Effect Sizes\n# Raw effect size (just the slope)\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\n# Standard deviations for standardized effect\nsd_x &lt;- sd(data$X)  # Standard deviation of X\nsd_y &lt;- sd(data$Y)  # Standard deviation of Y\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\n# Standardized effect size\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Correlation coefficient (should equal sqrt(R²) for simple regression)\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R²:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R²: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n# Step 12: Create visualization\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\n# Plot the data and regression line\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5)\n\n# Add the regression line\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Add the mean point\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Add vertical lines for residuals\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\n# Add legend\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# Add the equation to the plot\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R² = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\n\n\n\n# Final summary\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )\n\n\n\nRunning the Code\nTo run this R code:\n\nCopy the entire code block above\nPaste it into RStudio or any R console\nExecute the code\nCompare the output with our manual calculations\n\nThe code will:\n\nRecreate all our manual calculations step by step\nVerify results using R’s built-in lm() function\nGenerate a visualization of the data with the regression line\nDisplay all intermediate calculations with clear labels\n\n\n\nExpected Output Highlights\nWhen you run this code, you should see:\n\nSlope: 6.12 (matching our manual calculation)\nIntercept: 58.25 (matching our manual calculation)\nR²: 0.988 (matching our manual calculation)\nStandardized effect: 0.99 (matching our manual calculation)\nA plot showing the data points, regression line, and residuals\n\nThis verification confirms that our pen-and-paper calculations were correct!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#complete-manual-ols-calculation-a-step-by-step-example",
    "href": "correg_en.html#complete-manual-ols-calculation-a-step-by-step-example",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.11 Complete Manual OLS Calculation: A Step-by-Step Example",
    "text": "9.11 Complete Manual OLS Calculation: A Step-by-Step Example\nA professor wants to understand the relationship between hours spent studying and exam scores. She collects data from 6 students:\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nOur goal: Find the best-fitting line \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X using OLS.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-1-calculate-the-means",
    "href": "correg_en.html#step-1-calculate-the-means",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.12 Step 1: Calculate the Means",
    "text": "9.12 Step 1: Calculate the Means\nFirst, we need the mean of X and Y.\nFor X (study hours): \\bar{X} = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5\nFor Y (exam scores): \\bar{Y} = \\frac{65 + 70 + 75 + 85 + 88 + 95}{6} = \\frac{478}{6} = 79.67",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-2-calculate-deviations-from-means",
    "href": "correg_en.html#step-2-calculate-deviations-from-means",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.13 Step 2: Calculate Deviations from Means",
    "text": "9.13 Step 2: Calculate Deviations from Means\nFor each observation, calculate (X_i - \\bar{X}) and (Y_i - \\bar{Y}):\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n1 - 3.5 = -2.5\n65 - 79.67 = -14.67\n\n\nB\n2\n70\n2 - 3.5 = -1.5\n70 - 79.67 = -9.67\n\n\nC\n3\n75\n3 - 3.5 = -0.5\n75 - 79.67 = -4.67\n\n\nD\n4\n85\n4 - 3.5 = 0.5\n85 - 79.67 = 5.33\n\n\nE\n5\n88\n5 - 3.5 = 1.5\n88 - 79.67 = 8.33\n\n\nF\n6\n95\n6 - 3.5 = 2.5\n95 - 79.67 = 15.33",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-3-calculate-products-and-squares",
    "href": "correg_en.html#step-3-calculate-products-and-squares",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.14 Step 3: Calculate Products and Squares",
    "text": "9.14 Step 3: Calculate Products and Squares\nNow calculate (X_i - \\bar{X})(Y_i - \\bar{Y}) and (X_i - \\bar{X})^2:\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n(-2.5)(-14.67) = 36.68\n(-2.5)² = 6.25\n\n\nB\n(-1.5)(-9.67) = 14.51\n(-1.5)² = 2.25\n\n\nC\n(-0.5)(-4.67) = 2.34\n(-0.5)² = 0.25\n\n\nD\n(0.5)(5.33) = 2.67\n(0.5)² = 0.25\n\n\nE\n(1.5)(8.33) = 12.50\n(1.5)² = 2.25\n\n\nF\n(2.5)(15.33) = 38.33\n(2.5)² = 6.25\n\n\nSum\n107.03\n17.50",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-4-calculate-the-slope-hatbeta_1",
    "href": "correg_en.html#step-4-calculate-the-slope-hatbeta_1",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.15 Step 4: Calculate the Slope (\\hat{\\beta}_1)",
    "text": "9.15 Step 4: Calculate the Slope (\\hat{\\beta}_1)\nUsing the OLS formula: \\hat{\\beta}_1 = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} = \\frac{107.03}{17.50} = 6.12\nInterpretation: Each additional hour of study is associated with a 6.12-point increase in exam score.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-5-calculate-the-intercept-hatbeta_0",
    "href": "correg_en.html#step-5-calculate-the-intercept-hatbeta_0",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.16 Step 5: Calculate the Intercept (\\hat{\\beta}_0)",
    "text": "9.16 Step 5: Calculate the Intercept (\\hat{\\beta}_0)\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X} = 79.67 - (6.12 \\times 3.5) = 79.67 - 21.42 = 58.25\nInterpretation: A student who studies 0 hours is predicted to score 58.25 points.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-6-write-the-regression-equation",
    "href": "correg_en.html#step-6-write-the-regression-equation",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.17 Step 6: Write the Regression Equation",
    "text": "9.17 Step 6: Write the Regression Equation\n\\hat{Y} = 58.25 + 6.12X",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-7-calculate-predicted-values-and-residuals",
    "href": "correg_en.html#step-7-calculate-predicted-values-and-residuals",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.18 Step 7: Calculate Predicted Values and Residuals",
    "text": "9.18 Step 7: Calculate Predicted Values and Residuals\nUsing our equation to predict each student’s score:\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nResidual e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n58.25 + 6.12(1) = 64.37\n65 - 64.37 = 0.63\n\n\nB\n2\n70\n58.25 + 6.12(2) = 70.49\n70 - 70.49 = -0.49\n\n\nC\n3\n75\n58.25 + 6.12(3) = 76.61\n75 - 76.61 = -1.61\n\n\nD\n4\n85\n58.25 + 6.12(4) = 82.73\n85 - 82.73 = 2.27\n\n\nE\n5\n88\n58.25 + 6.12(5) = 88.85\n88 - 88.85 = -0.85\n\n\nF\n6\n95\n58.25 + 6.12(6) = 94.97\n95 - 94.97 = 0.03\n\n\n\nCheck: Sum of residuals = 0.63 - 0.49 - 1.61 + 2.27 - 0.85 + 0.03 ≈ 0 ✓",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-8-calculate-sum-of-squares",
    "href": "correg_en.html#step-8-calculate-sum-of-squares",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.19 Step 8: Calculate Sum of Squares",
    "text": "9.19 Step 8: Calculate Sum of Squares\n\nTotal Sum of Squares (SST)\nHow much total variation exists in exam scores?\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n(65 - 79.67)² = (-14.67)² = 215.21\n\n\nB\n70\n(70 - 79.67)² = (-9.67)² = 93.51\n\n\nC\n75\n(75 - 79.67)² = (-4.67)² = 21.81\n\n\nD\n85\n(85 - 79.67)² = (5.33)² = 28.41\n\n\nE\n88\n(88 - 79.67)² = (8.33)² = 69.39\n\n\nF\n95\n(95 - 79.67)² = (15.33)² = 235.01\n\n\nSum\n\nSST = 663.34\n\n\n\n\n\nRegression Sum of Squares (SSR)\nHow much variation does our model explain?\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n(64.37 - 79.67)² = (-15.30)² = 234.09\n\n\nB\n70.49\n(70.49 - 79.67)² = (-9.18)² = 84.27\n\n\nC\n76.61\n(76.61 - 79.67)² = (-3.06)² = 9.36\n\n\nD\n82.73\n(82.73 - 79.67)² = (3.06)² = 9.36\n\n\nE\n88.85\n(88.85 - 79.67)² = (9.18)² = 84.27\n\n\nF\n94.97\n(94.97 - 79.67)² = (15.30)² = 234.09\n\n\nSum\n\nSSR = 655.44\n\n\n\n\n\nError Sum of Squares (SSE)\nHow much variation is unexplained?\n\n\n\nStudent\nResidual e_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n-0.49\n0.24\n\n\nC\n-1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n-0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSum\n\nSSE = 9.10\n\n\n\nVerification: SST = SSR + SSE 663.34 ≈ 655.44 + 9.10 = 664.54 ✓ (small rounding difference)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-9-calculate-r-squared",
    "href": "correg_en.html#step-9-calculate-r-squared",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.20 Step 9: Calculate R-Squared",
    "text": "9.20 Step 9: Calculate R-Squared\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternative formula: R^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 1 - 0.014 = 0.986\n(Small difference due to rounding)\nInterpretation: Study hours explain 98.8% of the variation in exam scores. This is an extremely strong relationship.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-10-calculate-effect-sizes",
    "href": "correg_en.html#step-10-calculate-effect-sizes",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.21 Step 10: Calculate Effect Sizes",
    "text": "9.21 Step 10: Calculate Effect Sizes\n\nRaw Effect Size\nThe raw effect size is simply the slope: 6.12 points per hour\n\n\nStandardized Effect Size\nFirst, calculate standard deviations:\nFor X: s_X = \\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{n-1}} = \\sqrt{\\frac{17.50}{5}} = \\sqrt{3.50} = 1.87\nFor Y: s_Y = \\sqrt{\\frac{\\sum(Y_i - \\bar{Y})^2}{n-1}} = \\sqrt{\\frac{663.34}{5}} = \\sqrt{132.67} = 11.52\nStandardized coefficient: \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y} = 6.12 \\times \\frac{1.87}{11.52} = 6.12 \\times 0.162 = 0.99\nInterpretation: A one standard deviation increase in study hours (1.87 hours) is associated with a 0.99 standard deviation increase in exam score.\nAccording to Cohen’s guidelines:\n\nSmall effect: |β| = 0.10\nMedium effect: |β| = 0.30\nLarge effect: |β| = 0.50\n\nOur standardized effect of 0.99 is nearly twice Cohen’s “large effect” threshold.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-11-practical-significance-assessment",
    "href": "correg_en.html#step-11-practical-significance-assessment",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.22 Step 11: Practical Significance Assessment",
    "text": "9.22 Step 11: Practical Significance Assessment\n\nContext Analysis\n\nScale consideration:\n\nEffect: 6.12 points per hour\nExam scale: 0-100 points\nPercentage impact: 6.12% per hour\n\nPractical thresholds:\n\nLetter grade difference: Often 10 points\nTime to improve one letter grade: 10/6.12 = 1.63 hours\nConclusion: Less than 2 hours of extra study could change a letter grade\n\nCost-benefit analysis:\n\nBenefit: 6.12 points per hour\nCost: 1 hour of time\nDecision: Highly favorable return on investment",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-of-results",
    "href": "correg_en.html#summary-of-results",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.23 Summary of Results",
    "text": "9.23 Summary of Results\nRegression equation: \\hat{Y} = 58.25 + 6.12X\nKey statistics:\n\nSlope (\\hat{\\beta}_1): 6.12 points/hour\nIntercept (\\hat{\\beta}_0): 58.25 points\nR²: 0.988 (98.8% of variance explained)\nStandardized effect: 0.99 (very large effect)\n\nPractical interpretation:\n\nEach hour of study adds about 6 points to exam score\nThe model fits extremely well (R² near 1)\nThe effect is both statistically and practically significant\nStudents can meaningfully improve grades with modest increases in study time",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#verification-check",
    "href": "correg_en.html#verification-check",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.24 Verification Check",
    "text": "9.24 Verification Check\nTo verify our calculations, let’s check that the regression line passes through (\\bar{X}, \\bar{Y}):\n\\hat{Y} = 58.25 + 6.12(3.5) = 58.25 + 21.42 = 79.67 = \\bar{Y} ✓\nThe calculation confirms our regression line passes through the point of means, as it should.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#r-code-to-verify-manual-calculations",
    "href": "correg_en.html#r-code-to-verify-manual-calculations",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.25 R Code to Verify Manual Calculations",
    "text": "9.25 R Code to Verify Manual Calculations\nBelow is R code that checks all our manual calculations. You can run this code to confirm every step.\n\n# Step 1: Create the data\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)  # X variable\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95)  # Y variable\nn &lt;- length(study_hours)  # Sample size\n\n# Create a data frame for easier handling\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Step 2: Calculate means\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Step 3: Calculate deviations from means\ndata$x_dev &lt;- data$X - x_bar  # X deviations\ndata$y_dev &lt;- data$Y - y_bar  # Y deviations\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Step 4: Calculate products and squares for OLS formula\ndata$xy_product &lt;- data$x_dev * data$y_dev  # (Xi - X̄)(Yi - Ȳ)\ndata$x_dev_sq &lt;- data$x_dev^2  # (Xi - X̄)²\n\n# Sum of products and squares\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - X̄)(Yi - Ȳ):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - X̄)(Yi - Ȳ): 107 \n\ncat(\"Sum of (Xi - X̄)²:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - X̄)²: 17.5 \n\n# Step 5: Calculate slope (beta_1) manually\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (β₁) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (β₁) calculated manually: 6.11 \n\n# Step 6: Calculate intercept (beta_0) manually\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (β₀) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (β₀) calculated manually: 58.27 \n\n# Step 7: Compare with R's lm() function\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Step 8: Calculate predicted values and residuals\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X  # Predicted values\ndata$residual &lt;- data$Y - data$Y_hat  # Residuals\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ≈ 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ≈ 0): 0 \n\n# Step 9: Calculate Sum of Squares\n# Total Sum of Squares (SST)\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\n# Regression Sum of Squares (SSR)\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\n# Error Sum of Squares (SSE)\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\n# Verify that SST = SSR + SSE\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Step 10: Calculate R-squared\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R² (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR² (Method 1: SSR/SST): 0.9863 \n\ncat(\"R² (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR² (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R² (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR² (from lm function): 0.9863 \n\n# Step 11: Calculate Effect Sizes\n# Raw effect size (just the slope)\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\n# Standard deviations for standardized effect\nsd_x &lt;- sd(data$X)  # Standard deviation of X\nsd_y &lt;- sd(data$Y)  # Standard deviation of Y\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\n# Standardized effect size\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Correlation coefficient (should equal sqrt(R²) for simple regression)\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R²:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R²: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n# Step 12: Create visualization\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\n# Plot the data and regression line\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5)\n\n# Add the regression line\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Add the mean point\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Add vertical lines for residuals\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\n# Add legend\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# Add the equation to the plot\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R² = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\n\n\n\n# Final summary\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )\n\n\n\nRunning the Code\nTo run this R code:\n\nCopy the entire code block above\nPaste it into RStudio or any R console\nExecute the code\nCompare the output with our manual calculations\n\nThe code will:\n\nRecreate all our manual calculations step by step\nVerify results using R’s built-in lm() function\nGenerate a visualization of the data with the regression line\nDisplay all intermediate calculations with clear labels\n\n\n\nExpected Output Highlights\nWhen you run this code, you should see:\n\nSlope: 6.12 (matching our manual calculation)\nIntercept: 58.25 (matching our manual calculation)\nR²: 0.988 (matching our manual calculation)\nStandardized effect: 0.99 (matching our manual calculation)\nA plot showing the data points, regression line, and residuals\n\nThis verification confirms that our pen-and-paper calculations were correct!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-linear-regression-model",
    "href": "correg_en.html#the-linear-regression-model",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.26 The Linear Regression Model",
    "text": "9.26 The Linear Regression Model\nRegression analysis provides a statistical framework for modeling relationships between a dependent variable and one or more independent variables. This methodology enables researchers to quantify relationships, test hypotheses, and make predictions based on observed data.\n\nSimple Linear Regression\nThe simple linear regression model expresses the relationship between a dependent variable and a single independent variable:\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\nWhere: - Y_i represents the dependent variable for observation i - X_i represents the independent variable for observation i - \\beta_0 is the intercept parameter - \\beta_1 is the slope parameter - \\varepsilon_i is the error term for observation i\n\n\nMultiple Linear Regression\nThe multiple linear regression model extends this framework to incorporate k independent variables:\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + ... + \\beta_k X_{ki} + \\varepsilon_i\nThis formulation allows for the simultaneous analysis of multiple predictors and their respective contributions to the dependent variable.\n\n\nOrdinary Least Squares Estimation\n\nDefining the Optimization Criterion\nThe estimation of regression parameters requires a criterion for determining the “best” fit. Consider three potential approaches for defining the optimal line through a set of data points:\n\n\nApproach 1: Minimizing the Sum of Residuals\n\\min \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i) = \\min \\sum_{i=1}^{n} e_i\nThis approach is fundamentally flawed. For any line passing through the data, we can always find another line where positive and negative residuals sum to zero. In fact, infinitely many lines satisfy \\sum e_i = 0. This criterion fails to uniquely identify an optimal solution. Moreover, a horizontal line through the mean of Y would achieve zero sum of residuals while ignoring the relationship with X entirely.\n\n\nApproach 2: Minimizing the Sum of Absolute Residuals\n\\min \\sum_{i=1}^{n} |Y_i - \\hat{Y}_i| = \\min \\sum_{i=1}^{n} |e_i|\nThis criterion, known as Least Absolute Deviations (LAD), addresses the cancellation problem by taking absolute values. It produces estimates that are more robust to outliers than OLS. However, this approach presents significant challenges:\n\nThe absolute value function is not differentiable at zero, complicating analytical solutions\nMultiple solutions may exist (the objective function may have multiple minima)\nNo closed-form solution exists; iterative numerical methods are required\nStatistical inference is more complex, lacking the elegant properties of OLS estimators\n\n\n\nApproach 3: Minimizing the Sum of Squared Residuals\n\\min \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\min \\sum_{i=1}^{n} e_i^2\nThe Ordinary Least Squares (OLS) approach minimizes the sum of squared residuals. This criterion offers several advantages:\n\nPrevents cancellation of positive and negative errors\nProvides a unique solution (except in cases of perfect multicollinearity)\nYields closed-form analytical solutions through differentiation\nProduces estimators with optimal statistical properties under classical assumptions\nFacilitates straightforward statistical inference\n\n\n\n\nVisualizing the Sum of Squared Errors\nThe OLS method can be understood geometrically through the following conceptual framework:\n\nEach error appears as a vertical line from the data point to the regression line\nEach of these vertical lines represents a residual (e_i)\nWe square each residual, which can be visualized as creating a square area\nThe sum of all these squared areas is what OLS minimizes\n\n\n# Visualization of squared errors as geometric areas\nlibrary(ggplot2)\n\n# Generate sample data with clear pattern\nset.seed(42)\nn &lt;- 8  # Small number for clarity\nx &lt;- seq(2, 16, length.out = n)\ny &lt;- 10 + 1.5*x + rnorm(n, 0, 3)\ndf &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, df)\ndf$fitted &lt;- fitted(model)\ndf$residual &lt;- residuals(model)\n\n# Create visualization with actual squares\np &lt;- ggplot(df, aes(x = x, y = y)) +\n  # Add regression line first (bottom layer)\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\", linewidth = 1.2) +\n  \n  # Add squares for each residual\n  # For positive residuals\n  geom_rect(data = subset(df, residual &gt; 0),\n            aes(xmin = x - abs(residual)/2, \n                xmax = x + abs(residual)/2, \n                ymin = fitted, \n                ymax = fitted + abs(residual)),\n            fill = \"red\", alpha = 0.25, color = \"red\", linewidth = 0.5) +\n  \n  # For negative residuals  \n  geom_rect(data = subset(df, residual &lt; 0),\n            aes(xmin = x - abs(residual)/2, \n                xmax = x + abs(residual)/2, \n                ymin = fitted - abs(residual), \n                ymax = fitted),\n            fill = \"red\", alpha = 0.25, color = \"red\", linewidth = 0.5) +\n  \n  # Add residual lines\n  geom_segment(aes(xend = x, yend = fitted), \n               color = \"red\", linewidth = 0.8, linetype = \"solid\") +\n  \n  # Add data points\n  geom_point(size = 3.5, color = \"black\") +\n  \n  # Add text annotations for selected squared values\n  geom_text(data = subset(df, abs(residual) &gt; 2),\n            aes(x = x, \n                y = fitted + sign(residual) * abs(residual)/2,\n                label = paste0(\"e²=\", round(residual^2, 1))),\n            size = 3, color = \"darkred\", fontface = \"italic\") +\n  \n  # Styling\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(face = \"italic\")\n  ) +\n  coord_equal() +  # Ensures squares appear as squares\n  labs(\n    title = \"Geometric Visualization of Sum of Squared Errors\",\n    subtitle = paste(\"SSE =\", round(sum(df$residual^2), 1), \n                     \"- Red squares represent squared residuals\"),\n    x = \"Independent Variable (X)\",\n    y = \"Dependent Variable (Y)\"\n  ) +\n  # Add SSE annotation\n  annotate(\"rect\", \n           xmin = max(df$x) - 3, xmax = max(df$x) - 0.5,\n           ymin = min(df$y) - 2, ymax = min(df$y),\n           fill = \"lightyellow\", alpha = 0.8, color = \"gray40\") +\n  annotate(\"text\", \n           x = max(df$x) - 1.75, y = min(df$y) - 1,\n           label = paste(\"Σe² =\", round(sum(df$residual^2), 1)),\n           size = 4, fontface = \"bold\", color = \"darkred\")\n\nprint(p)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nMathematical Derivation of OLS Estimators\nFor simple linear regression, the OLS estimators are obtained by minimizing the sum of squared residuals:\nSSE = \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2\nTaking partial derivatives with respect to \\beta_0 and \\beta_1 and setting them equal to zero yields the normal equations. Solving this system produces:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} = \\frac{Cov(X,Y)}{Var(X)}\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\n\n\nProperties of OLS Estimators\nThe OLS procedure guarantees several important properties:\n\nZero sum of residuals: \\sum_{i=1}^{n} e_i = 0\nOrthogonality of residuals and predictors: \\sum_{i=1}^{n} X_i e_i = 0\nThe fitted regression line passes through the point (\\bar{X}, \\bar{Y})\nZero covariance between fitted values and residuals: \\sum_{i=1}^{n} \\hat{Y}_i e_i = 0\n\n\n\nClassical Linear Model Assumptions\n\nCore Assumptions\nFor OLS estimators to possess desirable statistical properties, the following assumptions must hold:\n\n\nAssumption 1: Linearity in Parameters\nThe relationship between the dependent and independent variables is linear in the parameters: Y_i = \\beta_0 + \\beta_1 X_{1i} + ... + \\beta_k X_{ki} + \\varepsilon_i\n\n\nAssumption 2: Strict Exogeneity\nThe error term has zero conditional expectation given all values of the independent variables: E[\\varepsilon_i | X] = 0\nThis assumption implies that the independent variables contain no information about the mean of the error term. It is stronger than contemporaneous exogeneity and rules out feedback from past errors to current regressors. This assumption is critical for unbiased estimation and is often violated in time series contexts with lagged dependent variables or in the presence of omitted variables.\nThis assumption is particularly important for our discussion of spurious correlations. Violations of the exogeneity assumption lead to endogeneity problems, which we will discuss later.\n\n\nAssumption 3: No Perfect Multicollinearity\nIn multiple regression, no independent variable can be expressed as a perfect linear combination of other independent variables. The matrix X'X must be invertible.\n\n\nAssumption 4: Homoscedasticity\nThe variance of the error term is constant across all observations: Var(\\varepsilon_i | X) = \\sigma^2\nThis assumption ensures that the precision of the regression does not vary systematically with the level of the independent variables.\n\n\nAssumption 5: No Autocorrelation\nThe error terms are uncorrelated with each other: Cov(\\varepsilon_i, \\varepsilon_j | X) = 0 \\text{ for } i \\neq j\n\n\nAssumption 6: Normality of Errors (for inference)\nThe error terms follow a normal distribution: \\varepsilon_i \\sim N(0, \\sigma^2)\nThis assumption is not required for the unbiasedness or consistency of OLS estimators but is necessary for exact finite-sample inference.\n\n\n\nGauss-Markov Theorem\nUnder Assumptions 1-5, the OLS estimators are BLUE (Best Linear Unbiased Estimators):\n\nBest: Minimum variance among the class of linear unbiased estimators\nLinear: The estimators are linear functions of the dependent variable\nUnbiased: E[\\hat{\\beta}] = \\beta\n\n\n\nVisualization of OLS Methodology\n\nGeometric Interpretation\n\n# Comprehensive visualization of OLS regression\nlibrary(ggplot2)\n\n# Generate sample data\nset.seed(42)\nn &lt;- 50\nx &lt;- runif(n, 0, 100)\nepsilon &lt;- rnorm(n, 0, 15)\ny &lt;- 20 + 0.8*x + epsilon\n\n# Create data frame\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, data = data)\ndata$fitted &lt;- fitted(model)\ndata$residuals &lt;- residuals(model)\n\n# Create comprehensive plot\nggplot(data, aes(x = x, y = y)) +\n  # Add confidence interval\n  geom_smooth(method = \"lm\", se = TRUE, alpha = 0.15, fill = \"blue\") +\n  # Add regression line\n  geom_line(aes(y = fitted), color = \"blue\", linewidth = 1.2) +\n  # Add residual segments\n  geom_segment(aes(xend = x, yend = fitted), \n               color = \"red\", alpha = 0.5, linewidth = 0.7) +\n  # Add observed points\n  geom_point(size = 2.5, alpha = 0.8) +\n  # Add fitted values\n  geom_point(aes(y = fitted), color = \"blue\", size = 1.5, alpha = 0.6) +\n  # Annotations\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(size = 11),\n    plot.title = element_text(size = 12, face = \"bold\")\n  ) +\n  labs(\n    title = \"Ordinary Least Squares Regression\",\n    subtitle = sprintf(\"Estimated equation: Y = %.2f + %.3f X  (R² = %.3f, RSE = %.2f)\",\n                      coef(model)[1], coef(model)[2], \n                      summary(model)$r.squared, \n                      summary(model)$sigma),\n    x = \"Independent Variable (X)\",\n    y = \"Dependent Variable (Y)\"\n  ) +\n  annotate(\"text\", x = min(x) + 5, y = max(y) - 5,\n           label = sprintf(\"SSE = %.1f\", sum(residuals(model)^2)),\n           hjust = 0, size = 3.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nVisualization of Squared Residuals\n\n# Demonstrate why squaring is necessary\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Generate example with clear pattern\nset.seed(123)\nn &lt;- 20\nx &lt;- seq(1, 10, length.out = n)\ny &lt;- 2 + 1.5*x + rnorm(n, 0, 2)\ndf &lt;- data.frame(x = x, y = y)\n\n# Fit model\nmodel &lt;- lm(y ~ x, df)\ndf$fitted &lt;- fitted(model)\ndf$residual &lt;- residuals(model)\n\n# Plot 1: Raw residuals\np1 &lt;- ggplot(df, aes(x = x)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(aes(y = 0, yend = residual, xend = x), \n               color = ifelse(df$residual &gt; 0, \"darkgreen\", \"darkred\"),\n               linewidth = 1) +\n  geom_point(aes(y = residual), size = 3,\n             color = ifelse(df$residual &gt; 0, \"darkgreen\", \"darkred\")) +\n  theme_minimal() +\n  labs(title = \"Residuals (ei)\",\n       subtitle = sprintf(\"Sum = %.2f (not meaningful)\", sum(df$residual)),\n       x = \"X\", y = \"Residual\") +\n  ylim(c(-6, 6))\n\n# Plot 2: Squared residuals\np2 &lt;- ggplot(df, aes(x = x)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(aes(y = 0, yend = residual^2, xend = x), \n               color = \"darkred\", linewidth = 1) +\n  geom_point(aes(y = residual^2), size = 3, color = \"darkred\") +\n  theme_minimal() +\n  labs(title = \"Squared Residuals (ei²)\",\n       subtitle = sprintf(\"Sum = %.2f (minimized by OLS)\", sum(df$residual^2)),\n       x = \"X\", y = \"Squared Residual\") +\n  ylim(c(0, 36))\n\n# Combine plots\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\nDiagnostic Analysis\n\nResidual Diagnostics\nAssessment of model assumptions requires careful examination of residual patterns:\n\n# Generate diagnostic plots\npar(mfrow = c(2, 2))\n\n# Residuals vs Fitted Values\nplot(model, which = 1)\n# Tests linearity and homoscedasticity assumptions\n\n# Normal Q-Q Plot\nplot(model, which = 2)\n# Tests normality assumption\n\n# Scale-Location Plot\nplot(model, which = 3)\n# Tests homoscedasticity assumption\n\n# Cook's Distance\nplot(model, which = 4)\n\n\n\n\n\n\n\n# Identifies influential observations\n\n\n\nTesting Assumptions Formally\n\n# Formal statistical tests\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(car)\n\n# Test for heteroscedasticity\n# Breusch-Pagan test\nbptest(model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model\nBP = 0.37876, df = 1, p-value = 0.5383\n\n# Test for autocorrelation\n# Durbin-Watson test\ndwtest(model)\n\n\n    Durbin-Watson test\n\ndata:  model\nDW = 2.1781, p-value = 0.5599\nalternative hypothesis: true autocorrelation is greater than 0\n\n# Test for normality\n# Shapiro-Wilk test on residuals\nshapiro.test(residuals(model))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(model)\nW = 0.98221, p-value = 0.9593\n\n# Test for linearity\n# Rainbow test\nraintest(model)\n\n\n    Rainbow test\n\ndata:  model\nRain = 1.2139, df1 = 10, df2 = 8, p-value = 0.3995\n\n\n\n\n\nExtensions and Alternatives\n\nWhen OLS Assumptions Fail\nWhen classical assumptions are violated, alternative approaches may be necessary:\n\nHeteroscedasticity: Weighted Least Squares (WLS) or robust standard errors\nAutocorrelation: Generalized Least Squares (GLS) or Newey-West standard errors\nNon-normality: Bootstrap inference or robust regression methods\nMulticollinearity: Ridge regression or LASSO\nEndogeneity: Instrumental Variables (IV) or Two-Stage Least Squares (2SLS)\n\n\n\nRobust Regression Methods\nWhen outliers are present, robust alternatives to OLS include:\n\nM-estimators (Huber regression)\nLeast Trimmed Squares (LTS)\nMM-estimators\n\n\n\n\nPractical Implementation\n\nComplete Analysis Example\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(car)\n\n# Generate dataset\nset.seed(2024)\nn &lt;- 200\ndata &lt;- data.frame(\n  x1 = rnorm(n, 50, 10),\n  x2 = rnorm(n, 30, 5),\n  x3 = rbinom(n, 1, 0.5)\n)\ndata$y &lt;- 10 + 2*data$x1 + 3*data$x2 + 15*data$x3 + rnorm(n, 0, 10)\n\n# Fit multiple regression model\nfull_model &lt;- lm(y ~ x1 + x2 + x3, data = data)\n\n# Model summary\nsummary(full_model)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25.6507  -6.9674  -0.7472   6.3670  30.4959 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 13.59158    5.97565   2.274               0.024 *  \nx1           2.05837    0.06876  29.934 &lt;0.0000000000000002 ***\nx2           2.76233    0.15259  18.103 &lt;0.0000000000000002 ***\nx3          14.80771    1.42654  10.380 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.897 on 196 degrees of freedom\nMultiple R-squared:  0.8744,    Adjusted R-squared:  0.8725 \nF-statistic: 454.9 on 3 and 196 DF,  p-value: &lt; 0.00000000000000022\n\n# ANOVA table\nanova(full_model)\n\nAnalysis of Variance Table\n\nResponse: y\n           Df Sum Sq Mean Sq F value                Pr(&gt;F)    \nx1          1  82189   82189  839.04 &lt; 0.00000000000000022 ***\nx2          1  40936   40936  417.90 &lt; 0.00000000000000022 ***\nx3          1  10555   10555  107.75 &lt; 0.00000000000000022 ***\nResiduals 196  19200      98                                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Confidence intervals for parameters\nconfint(full_model, level = 0.95)\n\n                2.5 %    97.5 %\n(Intercept)  1.806741 25.376410\nx1           1.922758  2.193977\nx2           2.461401  3.063262\nx3          11.994367 17.621053\n\n# Variance Inflation Factors (multicollinearity check)\nvif(full_model)\n\n      x1       x2       x3 \n1.009487 1.044192 1.038735 \n\n# Model diagnostics\npar(mfrow = c(2, 2))\nplot(full_model)\n\n\n\n\n\n\n\n# Tidy output\ntidy(full_model, conf.int = TRUE)\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    13.6     5.98        2.27 2.40e- 2     1.81     25.4 \n2 x1              2.06    0.0688     29.9  4.90e-75     1.92      2.19\n3 x2              2.76    0.153      18.1  1.06e-43     2.46      3.06\n4 x3             14.8     1.43       10.4  2.14e-20    12.0      17.6 \n\nglance(full_model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.874         0.872  9.90      455. 5.21e-88     3  -740. 1490. 1507.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\nConclusion\nOrdinary Least Squares regression remains a fundamental tool in statistical analysis. The method’s mathematical elegance, combined with its optimal properties under the classical assumptions, explains its widespread application. However, practitioners must carefully verify assumptions and consider alternatives when these conditions are not met. Understanding both the theoretical foundations and practical limitations of OLS is essential for proper statistical inference and prediction.\n\n\nVisualizing OLS Through Different Regression Lines\nA simple but effective way to visualize the concept of “best fit” is to compare multiple lines and their resulting SSE values:\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Create sample data\nset.seed(123)\nx &lt;- 1:20\ny &lt;- 2 + 3*x + rnorm(20, 0, 5)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, data = data)\ncoef &lt;- coefficients(model)\n\n# Define different lines: optimal and sub-optimal with clearer differences\nlines &lt;- data.frame(\n  label = c(\"Best Fit (OLS)\", \"Line A\", \"Line B\", \"Line C\"),\n  intercept = c(coef[1], coef[1] - 8, coef[1] + 8, coef[1] - 4),\n  slope = c(coef[2], coef[2] - 1.2, coef[2] + 0.8, coef[2] - 0.7)\n)\n\n# Calculate SSE for each line\nlines$sse &lt;- sapply(1:nrow(lines), function(i) {\n  predicted &lt;- lines$intercept[i] + lines$slope[i] * x\n  sum((y - predicted)^2)\n})\n\n# Add percentage increase over optimal SSE\nlines$pct_increase &lt;- round((lines$sse / lines$sse[1] - 1) * 100, 1)\nlines$pct_text &lt;- ifelse(lines$label == \"Best Fit (OLS)\", \n                         \"Optimal\", \n                         paste0(\"+\", lines$pct_increase, \"%\"))\n\n# Assign distinct colors for better visibility\nline_colors &lt;- c(\"Best Fit (OLS)\" = \"blue\", \n                \"Line A\" = \"red\", \n                \"Line B\" = \"darkgreen\", \n                \"Line C\" = \"purple\")\n\n# Create data for mini residual plots\nmini_data &lt;- data.frame()\nfor(i in 1:nrow(lines)) {\n  line_data &lt;- data.frame(\n    x = x,\n    y = y,\n    predicted = lines$intercept[i] + lines$slope[i] * x,\n    residuals = y - (lines$intercept[i] + lines$slope[i] * x),\n    line = lines$label[i]\n  )\n  mini_data &lt;- rbind(mini_data, line_data)\n}\n\n# Create main comparison plot with improved visibility\np1 &lt;- ggplot(data, aes(x = x, y = y)) +\n  # Add background grid for reference\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_line(color = \"gray90\"),\n    panel.grid.major = element_line(color = \"gray85\"),\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 13),\n    axis.title = element_text(size = 13, face = \"bold\"),\n    axis.text = element_text(size = 12)\n  ) +\n  # Add data points\n  geom_point(size = 2.5, alpha = 0.8) +\n  # Add lines with improved visibility\n  geom_abline(data = lines, \n              aes(intercept = intercept, slope = slope, \n                  color = label, linetype = label == \"Best Fit (OLS)\"),\n              size = 1.2) +\n  # Use custom colors\n  scale_color_manual(values = line_colors) +\n  scale_linetype_manual(values = c(\"TRUE\" = \"solid\", \"FALSE\" = \"dashed\"), guide = \"none\") +\n  # Better legends\n  labs(title = \"Comparing Different Regression Lines\",\n       subtitle = \"The OLS line minimizes the sum of squared errors\",\n       x = \"X\", y = \"Y\",\n       color = \"Regression Line\") +\n  guides(color = guide_legend(override.aes = list(size = 2)))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# Create mini residual plots with improved visibility\np_mini &lt;- list()\n\nfor(i in 1:nrow(lines)) {\n  line_data &lt;- subset(mini_data, line == lines$label[i])\n  \n  p_mini[[i]] &lt;- ggplot(line_data, aes(x = x, y = residuals)) +\n    # Add reference line\n    geom_hline(yintercept = 0, linetype = \"dashed\", size = 0.8, color = \"gray50\") +\n    # Add residual points with line color\n    geom_point(color = line_colors[lines$label[i]], size = 2.5) +\n    # Add squares to represent squared errors\n    geom_rect(aes(xmin = x - 0.3, xmax = x + 0.3,\n                  ymin = 0, ymax = residuals),\n              fill = line_colors[lines$label[i]], alpha = 0.2) +\n    # Improved titles\n    labs(title = lines$label[i],\n         subtitle = paste(\"SSE =\", round(lines$sse[i], 1), \n                          ifelse(i == 1, \" (Optimal)\", \n                                 paste0(\" (+\", lines$pct_increase[i], \"%)\"))),\n         x = NULL, y = NULL) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(size = 12, face = \"bold\", color = line_colors[lines$label[i]]),\n      plot.subtitle = element_text(size = 10),\n      panel.grid.minor = element_blank()\n    )\n}\n\n# Create SSE comparison table with better visibility\nsse_df &lt;- data.frame(\n  x = rep(1, nrow(lines)),\n  y = nrow(lines):1,\n  label = paste0(lines$label, \": SSE = \", round(lines$sse, 1), \" (\", lines$pct_text, \")\"),\n  color = line_colors[lines$label]\n)\n\nsse_table &lt;- ggplot(sse_df, aes(x = x, y = y, label = label, color = color)) +\n  geom_text(hjust = 0, size = 5, fontface = \"bold\") +\n  scale_color_identity() +\n  theme_void() +\n  xlim(1, 10) +\n  ylim(0.5, nrow(lines) + 0.5) +\n  labs(title = \"Sum of Squared Errors (SSE) Comparison\") +\n  theme(plot.title = element_text(hjust = 0, face = \"bold\", size = 14))\n\n# Arrange the plots with better spacing\ngrid.arrange(\n  p1, \n  arrangeGrob(p_mini[[1]], p_mini[[2]], p_mini[[3]], p_mini[[4]], \n              ncol = 2, padding = unit(1, \"cm\")),\n  sse_table, \n  ncol = 1, \n  heights = c(4, 3, 1)\n)\n\n\n\n\nComparing different regression lines\n\n\n\n\n\n\nKey Learning Points\n\nThe Sum of Squared Errors (SSE) is what Ordinary Least Squares (OLS) regression minimizes\nEach residual contributes its squared value to the total SSE\nThe OLS line has a lower SSE than any other possible line\nLarge residuals contribute disproportionately to the SSE due to the squaring operation\nThis is why outliers can have such a strong influence on regression lines\n\n\nStep-by-Step SSE Minimization\nTo illustrate the process of finding the minimum SSE, we can create a sequence that passes through the optimal point, showing how the SSE first decreases to a minimum and then increases again:\n\n# Create sample data\nset.seed(123)\nx &lt;- 1:20\ny &lt;- 2 + 3*x + rnorm(20, 0, 5)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, data = data)\ncoef &lt;- coefficients(model)\n\n# Create a sequence of steps that passes through the optimal OLS line\nsteps &lt;- 9  # Use odd number to have a middle point at the optimum\nstep_seq &lt;- data.frame(\n  step = 1:steps,\n  intercept = seq(coef[1] - 8, coef[1] + 8, length.out = steps),\n  slope = seq(coef[2] - 1.5, coef[2] + 1.5, length.out = steps)\n)\n\n# Mark the middle step (optimal OLS solution)\noptimal_step &lt;- ceiling(steps/2)\n\n# Calculate SSE for each step\nstep_seq$sse &lt;- sapply(1:nrow(step_seq), function(i) {\n  predicted &lt;- step_seq$intercept[i] + step_seq$slope[i] * x\n  sum((y - predicted)^2)\n})\n\n# Create a \"journey through the SSE valley\" plot\np2 &lt;- ggplot(data, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_abline(data = step_seq, \n              aes(intercept = intercept, slope = slope, \n                  color = sse, group = step),\n              size = 1) +\n  # Highlight the optimal line\n  geom_abline(intercept = step_seq$intercept[optimal_step], \n              slope = step_seq$slope[optimal_step],\n              color = \"green\", size = 1.5) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  labs(title = \"Journey Through the SSE Valley\",\n       subtitle = \"The green line represents the OLS solution with minimum SSE\",\n       color = \"SSE Value\") +\n  theme_minimal()\n\n# Create an SSE valley plot\np3 &lt;- ggplot(step_seq, aes(x = step, y = sse)) +\n  geom_line(size = 1) +\n  geom_point(size = 3, aes(color = sse)) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  # Highlight the optimal point\n  geom_point(data = step_seq[optimal_step, ], aes(x = step, y = sse), \n             size = 5, color = \"green\") +\n  # Add annotation\n  annotate(\"text\", x = optimal_step, y = step_seq$sse[optimal_step] * 1.1, \n           label = \"Minimum SSE\", color = \"darkgreen\", fontface = \"bold\") +\n  labs(title = \"The SSE Valley: Decreasing Then Increasing\",\n       subtitle = \"The SSE reaches its minimum at the OLS solution\",\n       x = \"Step\",\n       y = \"Sum of Squared Errors\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Display both plots\ngrid.arrange(p2, p3, ncol = 1, heights = c(3, 2))\n\n\n\n\nSSE minimization visualization\n\n\n\n\nIn R, the lm() function fits linear regression models:\n\nmodel &lt;- lm(y ~ x, data = data_frame)\n\n\n\n\nModel Interpretation: A Beginner’s Guide\nLet’s create a simple dataset to understand regression output better. Imagine we’re studying how years of education affect annual income:\n\n# Create a simple dataset - this is our Data Generating Process (DGP)\nset.seed(123) # For reproducibility\neducation_years &lt;- 10:20  # Education from 10 to 20 years\nn &lt;- length(education_years)\n\n# True parameters in our model - using more realistic values for Poland\ntrue_intercept &lt;- 3000   # Base monthly income with no education (in PLN)\ntrue_slope &lt;- 250        # Each year of education increases monthly income by 250 PLN\n\n# Generate monthly incomes with some random noise\nincome &lt;- true_intercept + true_slope * education_years + rnorm(n, mean=0, sd=300)\n\n# Create our dataset\neducation_income &lt;- data.frame(\n  education = education_years,\n  income = income\n)\n\n# Let's visualize our data\nlibrary(ggplot2)\nggplot(education_income, aes(x = education, y = income)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  scale_y_continuous(limits = c(5000, 8500), \n                     breaks = seq(5000, 8500, by = 500),\n                     labels = scales::comma) +\n  scale_x_continuous(breaks = 10:20) +\n  labs(\n    title = \"Relationship between Education and Income in Poland\",\n    x = \"Years of Education\",\n    y = \"Monthly Income (PLN)\",\n    subtitle = \"Red line shows the estimated linear relationship\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\")\n  ) +\n  annotate(\"text\", x = 11, y = 8000, \n           label = \"Each point represents\\none person's data\", \n           hjust = 0, size = 4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nFitting the Model\nNow let’s fit a linear regression model to this data:\n\n# Fit a simple regression model\nedu_income_model &lt;- lm(income ~ education, data = education_income)\n\n# Display the results\nmodel_summary &lt;- summary(edu_income_model)\nmodel_summary\n\n\nCall:\nlm(formula = income ~ education, data = education_income)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-427.72 -206.04  -38.12  207.32  460.78 \n\nCoefficients:\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)   3095.3      447.6   6.915 0.0000695 ***\neducation      247.2       29.2   8.467 0.0000140 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 306.3 on 9 degrees of freedom\nMultiple R-squared:  0.8885,    Adjusted R-squared:  0.8761 \nF-statistic: 71.69 on 1 and 9 DF,  p-value: 0.00001403\n\n\n\n\nUnderstanding the Regression Output Step by Step\nLet’s break down what each part of this output means in simple terms:\n\n1. The Formula\nAt the top, you see income ~ education, which means we’re predicting income based on education.\n\n\n2. Residuals\nThese show how far our predictions are from the actual values. Ideally, they should be centered around zero.\n\n\n3. Coefficients Table\n\n\n\nCoefficient Estimates\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n3095.27\n447.63\n6.91\n0\n\n\neducation\n247.23\n29.20\n8.47\n0\n\n\n\n\n\nIntercept (\\beta_0):\n\nValue: Approximately 3095\nInterpretation: This is the predicted monthly income for someone with 0 years of education\nNote: Sometimes the intercept isn’t meaningful in real-world terms, especially if x=0 is outside your data range\n\nEducation (\\beta_1):\n\nValue: Approximately 247\nInterpretation: For each additional year of education, we expect monthly income to increase by this amount in PLN\nThis is our main coefficient of interest!\n\nStandard Error:\n\nMeasures how precise our estimates are\nSmaller standard errors mean more precise estimates\nThink of it as “give or take how much” for our coefficients\n\nt value:\n\nThis is the coefficient divided by its standard error\nIt tells us how many standard errors away from zero our coefficient is\nLarger absolute t values (above 2) suggest the effect is statistically significant\n\np-value:\n\nThe probability of seeing our result (or something more extreme) if there was actually no relationship\nTypically, p &lt; 0.05 is considered statistically significant\nFor education, p = 0.000014, which is significant!\n\n\n\n4. Model Fit Statistics\n\n\n\nModel Fit Statistics\n\n\nStatistic\nValue\n\n\n\n\nR-squared\n0.888\n\n\nAdjusted R-squared\n0.876\n\n\nF-statistic\n71.686\n\n\np-value\n0.000\n\n\n\n\n\nR-squared:\n\nValue: 0.888\nInterpretation: 89% of the variation in income is explained by education\nHigher is better, but be cautious of very high values (could indicate overfitting)\n\nF-statistic:\n\nTests whether the model as a whole is statistically significant\nA high F-statistic with a low p-value indicates a significant model\n\n\n\n\nVisualizing the Model Results\nLet’s visualize what our model actually tells us:\n\n# Predicted values\neducation_income$predicted &lt;- predict(edu_income_model)\neducation_income$residuals &lt;- residuals(edu_income_model)\n\n# Create a more informative plot\nggplot(education_income, aes(x = education, y = income)) +\n  # Actual data points\n  geom_point(size = 3, color = \"blue\") +\n  \n  # Regression line\n  geom_line(aes(y = predicted), color = \"red\", size = 1.2) +\n  \n  # Residual lines\n  geom_segment(aes(xend = education, yend = predicted), \n               color = \"darkgray\", linetype = \"dashed\") +\n  \n  # Set proper scales\n  scale_y_continuous(limits = c(5000, 8500), \n                     breaks = seq(5000, 8500, by = 500),\n                     labels = scales::comma) +\n  scale_x_continuous(breaks = 10:20) +\n  \n  # Annotations\n  annotate(\"text\", x = 19, y = 7850, \n           label = paste(\"Slope =\", round(coef(edu_income_model)[2]), \"PLN per year\"),\n           color = \"red\", hjust = 1, fontface = \"bold\") +\n  \n  annotate(\"text\", x = 10.5, y = 5500, \n           label = paste(\"Intercept =\", round(coef(edu_income_model)[1]), \"PLN\"),\n           color = \"red\", hjust = 0, fontface = \"bold\") +\n  \n  annotate(\"text\", x = 14, y = 8200, \n           label = paste(\"R² =\", round(model_summary$r.squared, 2)),\n           color = \"black\", fontface = \"bold\") +\n  \n  # Labels\n  labs(\n    title = \"Interpreting the Education-Income Regression Model\",\n    subtitle = \"Red line shows predicted income for each education level\",\n    x = \"Years of Education\",\n    y = \"Monthly Income (PLN)\",\n    caption = \"Gray dashed lines represent residuals (prediction errors)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\nReal-World Interpretation\n\nA person with 16 years of education (college graduate) would be predicted to earn about: \\hat{Y} = 3095 + 247 \\times 16 = 7051 \\text{ PLN monthly}\nThe model suggests that each additional year of education is associated with a 247 PLN increase in monthly income.\nOur model explains approximately 89% of the variation in income in our sample.\nThe relationship is statistically significant (p &lt; 0.001), meaning it’s very unlikely to observe this relationship if education truly had no effect on income.\n\n\n\nImportant Cautions for Beginners\n\nCorrelation ≠ Causation: Our model shows association, not necessarily causation\nOmitted Variables: Other factors might influence both education and income\nExtrapolation: Be careful predicting outside the range of your data\nLinear Relationship: We’ve assumed the relationship is linear, which may not always be true",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#regression-analysis-and-ordinary-least-squares",
    "href": "correg_en.html#regression-analysis-and-ordinary-least-squares",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.27 Regression Analysis and Ordinary Least Squares (*)",
    "text": "9.27 Regression Analysis and Ordinary Least Squares (*)\n\nFoundations of Regression Analysis\nRegression analysis constitutes a fundamental statistical methodology for examining relationships between variables. At its core, regression provides a systematic framework for understanding how changes in one or more independent variables influence a dependent variable.\nThe primary objectives of regression analysis include: - Quantifying relationships between variables - Making predictions based on observed patterns - Testing hypotheses about variable associations - Understanding the proportion of variation explained by predictors\n\n\nDeterministic versus Stochastic Models\nStatistical modeling encompasses two fundamental approaches:\nDeterministic models assume precise, invariant relationships between variables. Given specific inputs, these models yield identical outputs without variation. Consider the physics equation:\n\\text{Distance} = \\text{Speed} \\times \\text{Time}\nThis relationship exhibits no randomness; identical inputs always produce identical outputs.\nStochastic models, in contrast, acknowledge inherent variability in real-world phenomena. Regression analysis employs stochastic modeling through the fundamental equation:\nY = f(X) + \\epsilon\nWhere: - Y represents the outcome variable - f(X) captures the systematic relationship between predictors and outcome - \\epsilon represents random variation inherent in the data\nThis formulation recognizes that real-world relationships contain both systematic patterns and random variation.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#simple-linear-regression-model",
    "href": "correg_en.html#simple-linear-regression-model",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.28 Simple Linear Regression Model",
    "text": "9.28 Simple Linear Regression Model\n\nModel Specification\nSimple linear regression models the relationship between a single predictor variable and an outcome variable through a linear equation:\nY_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\nThe model components represent: - Y_i: The dependent variable for observation i - X_i: The independent variable for observation i - \\beta_0: The population intercept parameter - \\beta_1: The population slope parameter - \\epsilon_i: The random error term for observation i\n\n\nInterpretation of Parameters\nThe parameters possess specific interpretations:\n\nIntercept (\\beta_0): The expected value of Y when X = 0. This represents the baseline level of the outcome variable.\nSlope (\\beta_1): The expected change in Y for a one-unit increase in X. This quantifies the strength and direction of the linear relationship.\nError term (\\epsilon_i): Captures all factors affecting Y not explained by X, including measurement error, omitted variables, and inherent randomness.\n\n\n\nEstimation versus True Parameters\nThe distinction between population parameters and sample estimates proves crucial:\n\nPopulation parameters (\\beta_0, \\beta_1) represent true, unknown values\nSample estimates (\\hat{\\beta}_0, \\hat{\\beta}_1) represent our best approximations based on available data\nThe hat notation (^) consistently denotes estimated values\n\nThe fitted regression equation becomes:\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1X_i",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-ordinary-least-squares-method-1",
    "href": "correg_en.html#the-ordinary-least-squares-method-1",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.29 The Ordinary Least Squares Method",
    "text": "9.29 The Ordinary Least Squares Method\n\nThe Fundamental Challenge\nGiven a dataset with observations (X_i, Y_i), we need a systematic method to determine the “best” values for \\hat{\\beta}_0 and \\hat{\\beta}_1. The challenge lies in defining what constitutes “best” and developing a practical method to find these values.\nConsider that for any given line through the data, each observation will have a prediction error or residual:\ne_i = Y_i - \\hat{Y}_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1X_i)\nThese residuals represent how far our predictions deviate from actual values. A good fitting line should make these residuals as small as possible overall.\n\n\nWhy Minimize the Sum of Squared Residuals?\nThe Ordinary Least Squares method determines optimal parameter estimates by minimizing the sum of squared residuals. This choice requires justification, as we could conceivably minimize other quantities. The rationale for squaring residuals includes:\nMathematical tractability: Squaring creates a smooth, differentiable function that yields closed-form solutions through calculus. The derivatives of squared terms lead to linear equations that can be solved analytically.\nEqual treatment of positive and negative errors: Simply summing raw residuals would allow positive and negative errors to cancel, potentially yielding a sum of zero even when predictions are poor. Squaring ensures all deviations contribute positively to the total error measure.\nPenalization of large errors: Squaring gives progressively greater weight to larger errors. An error of 4 units contributes 16 to the sum, while an error of 2 units contributes only 4. This property encourages finding a line that avoids extreme prediction errors.\nStatistical optimality: Under certain assumptions (including normally distributed errors), OLS estimators possess desirable statistical properties, including being the Best Linear Unbiased Estimators (BLUE) according to the Gauss-Markov theorem.\nConnection to variance: The sum of squared deviations directly relates to variance, a fundamental measure of spread in statistics. Minimizing squared residuals thus minimizes the variance of prediction errors.\n\n\nThe OLS Optimization Problem\nThe OLS method formally seeks values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that minimize:\n\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1X_i))^2\nThis optimization problem can be solved using calculus by: 1. Taking partial derivatives with respect to \\hat{\\beta}_0 and \\hat{\\beta}_1 2. Setting these derivatives equal to zero 3. Solving the resulting system of equations\n\n\nDerivation of OLS Estimators\nThe minimization yields closed-form solutions:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n(X_i - \\bar{X})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\nThese formulas reveal that: - The slope estimate depends on the covariance between variables relative to the predictor’s variance - The intercept ensures the regression line passes through the point of means (\\bar{X}, \\bar{Y})\n\n\nProperties of OLS Estimators\nOLS estimators possess several desirable properties:\n\nUnbiasedness: Under appropriate conditions, E[\\hat{\\beta}_j] = \\beta_j\nEfficiency: OLS provides minimum variance among linear unbiased estimators\nConsistency: As sample size increases, estimates converge to true values\nThe regression line passes through the centroid: The point (\\bar{X}, \\bar{Y}) always lies on the fitted line\n\n\n\nExtension to Multiple Regression\nWhile this guide focuses on simple linear regression with one predictor, the OLS framework extends naturally to multiple regression with several predictors:\nY_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + ... + \\beta_kX_{ki} + \\epsilon_i\nThe same principle applies: we minimize the sum of squared residuals, though the mathematics involves matrix algebra rather than simple formulas. The fundamental logic—finding parameter values that minimize prediction errors—remains unchanged.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#understanding-variance-decomposition",
    "href": "correg_en.html#understanding-variance-decomposition",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.30 Understanding Variance Decomposition",
    "text": "9.30 Understanding Variance Decomposition\n\nThe Baseline Model Concept\nBefore introducing predictors, consider the simplest possible model: predicting every observation using the overall mean \\bar{Y}. This baseline model represents our best prediction in the absence of additional information.\nThe baseline model’s predictions: \\hat{Y}_i^{\\text{baseline}} = \\bar{Y} \\text{ for all } i\nThis model serves as a reference point for evaluating improvement gained through incorporating predictors. The baseline model essentially asks: “If we knew nothing about the relationship between X and Y, what would be our best constant prediction?”\n\n\nComponents of Total Variation\nThe total variation in the outcome variable decomposes into three fundamental components:\n\nTotal Sum of Squares (SST)\nSST quantifies the total variation in the outcome variable relative to its mean:\n\\text{SST} = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\nInterpretation: SST represents the total variance that requires explanation. It measures the prediction error when using only the mean as our model—essentially the variance explained by the baseline (zero) model. This is the starting point: the total amount of variation we hope to explain by introducing predictors.\n\n\nRegression Sum of Squares (SSR)\nSSR measures the variation explained by the regression model:\n\\text{SSR} = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\nInterpretation: SSR quantifies the improvement in prediction achieved by incorporating the predictor variable. It represents the reduction in prediction error relative to the baseline model—the portion of total variation that our regression line successfully captures.\n\n\nError Sum of Squares (SSE)\nSSE captures the unexplained variation remaining after regression:\n\\text{SSE} = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\nInterpretation: SSE represents the residual variation that the model cannot explain, reflecting the inherent randomness and effects of omitted variables. This is the variation that remains even after using our best-fitting line.\n\n\n\nThe Fundamental Decomposition Identity\nThese components relate through the fundamental equation:\n\\text{SST} = \\text{SSR} + \\text{SSE}\nThis identity demonstrates that: - Total variation equals the sum of explained and unexplained components - The regression model partitions total variation into systematic and random parts - Model improvement can be assessed by comparing SSR to SST\n\n\nConceptual Framework for Variance Decomposition\n# Demonstration of Variance Decomposition\nlibrary(ggplot2)\n\n# Generate sample data\nset.seed(42)\nn &lt;- 50\nx &lt;- runif(n, 1, 10)\ny &lt;- 3 + 2*x + rnorm(n, 0, 2)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit model\nmodel &lt;- lm(y ~ x, data = data)\ny_mean &lt;- mean(y)\ny_pred &lt;- predict(model)\n\n# Calculate components\nSST &lt;- sum((y - y_mean)^2)\nSSR &lt;- sum((y_pred - y_mean)^2)\nSSE &lt;- sum((y - y_pred)^2)\n\n# Display decomposition\ncat(\"Variance Decomposition\\n\")\ncat(\"======================\\n\")\ncat(\"Total SS (SST):\", round(SST, 2), \n    \"- Total variation from mean\\n\")\ncat(\"Regression SS (SSR):\", round(SSR, 2), \n    \"- Variation explained by model\\n\")\ncat(\"Error SS (SSE):\", round(SSE, 2), \n    \"- Unexplained variation\\n\")\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\ncat(round(SST, 2), \"=\", round(SSR, 2), \"+\", \n    round(SSE, 2), \"\\n\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-coefficient-of-determination-r²",
    "href": "correg_en.html#the-coefficient-of-determination-r²",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.31 The Coefficient of Determination (R²)",
    "text": "9.31 The Coefficient of Determination (R²)\n\nDefinition and Calculation\nThe coefficient of determination, denoted R², quantifies the proportion of total variation explained by the regression model:\nR^2 = \\frac{\\text{SSR}}{\\text{SST}} = \\frac{\\text{Explained Variation}}{\\text{Total Variation}}\nAlternatively expressed as:\nR^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}} = 1 - \\frac{\\text{Unexplained Variation}}{\\text{Total Variation}}\nR² directly answers the question: “What proportion of the total variation in Y (relative to the baseline mean model) does our regression model explain?”\n\n\nInterpretation Guidelines\nR² values range from 0 to 1, with specific interpretations:\n\nR² = 0: The model explains no variation beyond the baseline mean model. The regression line provides no improvement over simply using \\bar{Y}.\nR² = 0.25: The model explains 25% of total variation. Three-quarters of the variation remains unexplained.\nR² = 0.75: The model explains 75% of total variation. This represents substantial explanatory power.\nR² = 1.00: The model explains all variation (perfect fit). All data points fall exactly on the regression line.\n\n\n\nContextual Considerations\nThe interpretation of R² requires careful consideration of context:\nField-specific standards: Acceptable R² values vary dramatically across disciplines - Physical sciences often expect R² &gt; 0.90 due to controlled conditions - Social sciences may consider R² = 0.30 meaningful given human complexity - Biological systems typically show intermediate values due to natural variation\nSample size effects: Small samples can artificially inflate R², leading to overly optimistic assessments of model fit.\nModel complexity: In multiple regression, additional predictors mechanically increase R², even if they lack true explanatory power.\nPractical significance: Statistical fit should align with substantive importance. A model with R² = 0.95 may be less useful than one with R² = 0.60 if the latter addresses more relevant questions.\n\n\nAdjusted R² for Multiple Regression\nWhen extending to multiple regression, adjusted R² accounts for the number of predictors:\nR^2_{\\text{adj}} = 1 - \\frac{\\text{SSE}/(n-p)}{\\text{SST}/(n-1)}\nWhere: - n = sample size - p = number of parameters (including intercept)\nAdjusted R² penalizes model complexity, providing a more conservative measure when comparing models with different numbers of predictors.\n\n\nMeasures of Fit\n\nR-squared (R^2): R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nRoot Mean Square Error (RMSE): RMSE = \\sqrt{\\frac{SSE}{n}}\nMean Absolute Error (MAE): MAE = \\frac{1}{n}\\sum_{i=1}^n |Y_i - \\hat{Y}_i|",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#practical-implementation-1",
    "href": "correg_en.html#practical-implementation-1",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.32 Practical Implementation",
    "text": "9.32 Practical Implementation\n\nComplete Regression Analysis Example\n\n# Comprehensive regression analysis\nlibrary(ggplot2)\n\n# Generate educational data\nset.seed(123)\nn &lt;- 100\nstudy_hours &lt;- runif(n, 0, 10)\nexam_scores &lt;- 50 + 4*study_hours + rnorm(n, 0, 5)\neducation_data &lt;- data.frame(\n  study_hours = study_hours,\n  exam_scores = exam_scores\n)\n\n# Fit regression model\nmodel &lt;- lm(exam_scores ~ study_hours, data = education_data)\n\n# Extract key statistics\ny_mean &lt;- mean(exam_scores)\ny_pred &lt;- predict(model)\n\n# Variance decomposition\nSST &lt;- sum((exam_scores - y_mean)^2)\nSSR &lt;- sum((y_pred - y_mean)^2)\nSSE &lt;- sum((exam_scores - y_pred)^2)\nr_squared &lt;- SSR/SST\n\n# Display results\ncat(\"OLS Regression Results\\n\")\n\nOLS Regression Results\n\ncat(\"======================\\n\")\n\n======================\n\ncat(\"\\nParameter Estimates:\\n\")\n\n\nParameter Estimates:\n\ncat(\"Intercept (β₀):\", round(coef(model)[1], 2), \"\\n\")\n\nIntercept (β₀): 49.96 \n\ncat(\"Slope (β₁):\", round(coef(model)[2], 2), \"\\n\")\n\nSlope (β₁): 3.96 \n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Expected score with 0 study hours:\", \n    round(coef(model)[1], 2), \"\\n\")\n\n- Expected score with 0 study hours: 49.96 \n\ncat(\"- Score increase per study hour:\", \n    round(coef(model)[2], 2), \"\\n\")\n\n- Score increase per study hour: 3.96 \n\ncat(\"\\nVariance Decomposition:\\n\")\n\n\nVariance Decomposition:\n\ncat(\"Total variation (SST):\", round(SST, 2), \"\\n\")\n\nTotal variation (SST): 14880.14 \n\ncat(\"Explained by model (SSR):\", round(SSR, 2), \"\\n\")\n\nExplained by model (SSR): 12578.3 \n\ncat(\"Unexplained (SSE):\", round(SSE, 2), \"\\n\")\n\nUnexplained (SSE): 2301.84 \n\ncat(\"\\nModel Performance:\\n\")\n\n\nModel Performance:\n\ncat(\"R-squared:\", round(r_squared, 4), \"\\n\")\n\nR-squared: 0.8453 \n\ncat(\"Interpretation: The model explains\", \n    round(r_squared * 100, 1), \n    \"% of the variation in exam scores\\n\")\n\nInterpretation: The model explains 84.5 % of the variation in exam scores\n\ncat(\"beyond what the mean alone could explain.\\n\")\n\nbeyond what the mean alone could explain.\n\n\n\n\nVisualization of Key Concepts\n\n# Create comprehensive visualization\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Plot 1: Baseline model (mean only)\np1 &lt;- ggplot(education_data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = y_mean, color = \"blue\", size = 1) +\n  geom_segment(aes(xend = study_hours, yend = y_mean), \n               alpha = 0.3, color = \"red\") +\n  labs(title = \"Baseline Model: Predicting with Mean\",\n       subtitle = paste(\"Total variation (SST) =\", round(SST, 1)),\n       x = \"Study Hours\", y = \"Exam Scores\") +\n  theme_minimal() +\n  annotate(\"text\", x = 8, y = y_mean + 1, \n           label = \"Mean of Y\", color = \"blue\")\n\n# Plot 2: Regression model\np2 &lt;- ggplot(education_data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_segment(aes(xend = study_hours, yend = y_pred), \n               alpha = 0.3, color = \"red\") +\n  labs(title = \"Regression Model: Improved Predictions\",\n       subtitle = paste(\"Unexplained variation (SSE) =\", \n                       round(SSE, 1), \n                       \"| R² =\", round(r_squared, 3)),\n       x = \"Study Hours\", y = \"Exam Scores\") +\n  theme_minimal() +\n  annotate(\"text\", x = 8, y = max(y_pred) + 1, \n           label = \"Regression Line\", color = \"blue\")\n\n# Plot 3: Variance components\nvariance_data &lt;- data.frame(\n  Component = c(\"Total (SST)\", \"Explained (SSR)\", \"Unexplained (SSE)\"),\n  Value = c(SST, SSR, SSE),\n  Type = c(\"Total\", \"Explained\", \"Unexplained\")\n)\n\np3 &lt;- ggplot(variance_data, aes(x = Component, y = Value, fill = Type)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"Total\" = \"gray50\", \n                               \"Explained\" = \"green4\", \n                               \"Unexplained\" = \"red3\")) +\n  labs(title = \"Variance Decomposition\",\n       y = \"Sum of Squares\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Combine plots\ngrid.arrange(p1, p2, p3, layout_matrix = rbind(c(1, 2), c(3, 3)))\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-and-key-insights",
    "href": "correg_en.html#summary-and-key-insights",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.33 Summary and Key Insights",
    "text": "9.33 Summary and Key Insights\n\nCore Concepts Review\nRegression analysis models relationships between variables using stochastic frameworks that acknowledge inherent variation. The simple linear regression model expresses this relationship as Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i.\nOrdinary Least Squares provides optimal parameter estimates by minimizing the sum of squared residuals. This choice of minimizing squared errors stems from mathematical tractability, equal treatment of positive and negative errors, appropriate penalization of large errors, and desirable statistical properties.\nVariance decomposition partitions total variation into: - Total variation from the baseline mean model (SST) - Variation explained by the regression model (SSR)\n- Unexplained residual variation (SSE)\nThe fundamental identity SST = SSR + SSE shows how regression improves upon the baseline model.\nR² quantifies model performance as the proportion of total variation explained, providing a standardized measure of how much better the regression model performs compared to simply using the mean.\n\n\nCritical Considerations\nThe baseline model (predicting with the mean) serves as the fundamental reference point. All regression improvement is measured relative to this simple model. SST represents the total variance requiring explanation when we start with no predictors.\nParameter estimates (\\hat{\\beta}_0, \\hat{\\beta}_1) represent sample-based approximations of unknown population values (\\beta_0, \\beta_1). The distinction between population parameters and sample estimates remains crucial for proper inference.\nModel assessment requires considering both statistical fit (R²) and practical significance. A model with modest R² may still provide valuable insights, while high R² does not guarantee causation or practical utility.\n\n\nExtensions and Applications\nWhile this guide focuses on simple linear regression, the framework extends naturally to: - Multiple regression with several predictors - Polynomial regression for nonlinear relationships - Interaction terms to capture conditional effects - Categorical predictors through appropriate coding\nThe fundamental principles—minimizing prediction errors, decomposing variation, and assessing model fit—remain consistent across these extensions. The mathematical complexity increases, but the conceptual foundation established here continues to apply.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#key-assumptions-of-linear-regression",
    "href": "correg_en.html#key-assumptions-of-linear-regression",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.34 Key Assumptions of Linear Regression",
    "text": "9.34 Key Assumptions of Linear Regression\n\nStrict Exogeneity: The Fundamental Assumption\nThe most crucial assumption in regression is strict exogeneity:\nE[\\varepsilon|X] = 0\nThis means:\n\nThe error term has zero mean conditional on X\nX contains no information about the average error\nThere are no systematic patterns in how our predictions are wrong\n\nLet’s visualize when this assumption holds and when it doesn’t:\n\n# Generate data\nset.seed(789)\nx &lt;- seq(1, 10, by = 0.2)\n\n# Case 1: Exogenous errors\ny_exog &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\n\n# Case 2: Non-exogenous errors (error variance increases with x)\ny_nonexog &lt;- 2 + 3*x + 0.5*x*rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_exog &lt;- data.frame(\n  x = x,\n  y = y_exog,\n  type = \"Exogenous Errors\\n(Assumption Satisfied)\"\n)\n\ndata_nonexog &lt;- data.frame(\n  x = x,\n  y = y_nonexog,\n  type = \"Non-Exogenous Errors\\n(Assumption Violated)\"\n)\n\ndata_combined &lt;- rbind(data_exog, data_nonexog)\n\n# Create plots with residuals\nplot_residuals &lt;- function(data, title) {\n  model &lt;- lm(y ~ x, data = data)\n  data$predicted &lt;- predict(model)\n  data$residuals &lt;- residuals(model)\n  \n  p1 &lt;- ggplot(data, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    theme_minimal() +\n    labs(title = title)\n  \n  p2 &lt;- ggplot(data, aes(x = x, y = residuals)) +\n    geom_point() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n    theme_minimal() +\n    labs(y = \"Residuals\")\n  \n  list(p1, p2)\n}\n\n# Generate plots\nplots_exog &lt;- plot_residuals(data_exog, \"Exogenous Errors\")\nplots_nonexog &lt;- plot_residuals(data_nonexog, \"Non-Exogenous Errors\")\n\n# Arrange plots\ngridExtra::grid.arrange(\n  plots_exog[[1]], plots_exog[[2]],\n  plots_nonexog[[1]], plots_nonexog[[2]],\n  ncol = 2\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 9.1: Exogeneity vs. Non-Exogeneity Examples\n\n\n\n\n\n\n\nLinearity: The Form Assumption\nThe relationship between X and Y should be linear in parameters:\nE[Y|X] = \\beta_0 + \\beta_1X\nNote that this doesn’t mean X and Y must have a straight-line relationship - we can transform variables. Let’s see different types of relationships:\n\n# Generate data\nset.seed(101)\nx &lt;- seq(1, 10, by = 0.1)\n\n# Different relationships\ndata_relationships &lt;- data.frame(\n  x = rep(x, 3),\n  y = c(\n    # Linear\n    2 + 3*x + rnorm(length(x), 0, 2),\n    # Quadratic\n    2 + 0.5*x^2 + rnorm(length(x), 0, 2),\n    # Exponential\n    exp(0.3*x) + rnorm(length(x), 0, 2)\n  ),\n  type = rep(c(\"Linear\", \"Quadratic\", \"Exponential\"), each = length(x))\n)\n\n# Plot\nggplot(data_relationships, aes(x = x, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  facet_wrap(~type, scales = \"free_y\") +\n  theme_minimal() +\n  labs(subtitle = \"Red: linear fit, Blue: true relationship\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 9.2: Linear and Nonlinear Relationships\n\n\n\n\n\n\n\nUnderstanding Violations and Solutions\nWhen linearity is violated:\n\nTransform variables:\n\nLog transformation: for exponential relationships\nSquare root: for moderate nonlinearity\nPower transformations: for more complex relationships\n\n\n\n# Generate exponential data\nset.seed(102)\nx &lt;- seq(1, 10, by = 0.2)\ny &lt;- exp(0.3*x) + rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_trans &lt;- data.frame(\n  x = x,\n  y = y,\n  log_y = log(y)\n)\n\nWarning in log(y): NaNs produced\n\n# Original scale plot\np1 &lt;- ggplot(data_trans, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Original Scale\")\n\n# Log scale plot\np2 &lt;- ggplot(data_trans, aes(x = x, y = log_y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Log-Transformed Y\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFigure 9.3: Effect of Variable Transformations",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#spurious-correlation-causes-and-examples",
    "href": "correg_en.html#spurious-correlation-causes-and-examples",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.35 Spurious Correlation: Causes and Examples",
    "text": "9.35 Spurious Correlation: Causes and Examples\nSpurious correlation occurs when variables appear related but the relationship is not causal. These misleading correlations arise from several sources:\n\nRandom coincidence (chance)\nConfounding variables (hidden third factors)\nSelection biases\nImproper statistical analysis\nReverse causality\nEndogeneity problems (including simultaneity)\n\n\nRandom Coincidence (Chance)\nWith sufficient data mining or small sample sizes, seemingly meaningful correlations can emerge purely by chance. This is especially problematic when researchers conduct multiple analyses without appropriate corrections for multiple comparisons, a practice known as “p-hacking.”\n\n# Create a realistic example of spurious correlation based on actual country data\n# Using country data on chocolate consumption and Nobel prize winners\n# This example is inspired by a published correlation (Messerli, 2012)\nset.seed(123)\ncountries &lt;- c(\"Switzerland\", \"Sweden\", \"Denmark\", \"Belgium\", \"Austria\", \n               \"Norway\", \"Germany\", \"Netherlands\", \"United Kingdom\", \"Finland\", \n               \"France\", \"Italy\", \"Spain\", \"Poland\", \"Greece\", \"Portugal\")\n\n# Create realistic data: Chocolate consumption correlates with GDP per capita\n# Higher GDP countries tend to consume more chocolate and have better research funding\ngdp_per_capita &lt;- c(87097, 58977, 67218, 51096, 53879, 89154, 51860, 57534, \n                    46510, 53982, 43659, 35551, 30416, 17841, 20192, 24567)\n\n# Normalize GDP values to make them more manageable\ngdp_normalized &lt;- (gdp_per_capita - min(gdp_per_capita)) / \n                 (max(gdp_per_capita) - min(gdp_per_capita))\n\n# More realistic chocolate consumption - loosely based on real consumption patterns\n# plus some randomness, but influenced by GDP\nchocolate_consumption &lt;- 4 + 8 * gdp_normalized + rnorm(16, 0, 0.8)\n\n# Nobel prizes - also influenced by GDP (research funding) with noise\n# The relationship is non-linear, but will show up as correlated\nnobel_prizes &lt;- 2 + 12 * gdp_normalized^1.2 + rnorm(16, 0, 1.5)\n\n# Create dataframe\ncountry_data &lt;- data.frame(\n  country = countries,\n  chocolate = round(chocolate_consumption, 1),\n  nobel = round(nobel_prizes, 1),\n  gdp = gdp_per_capita\n)\n\n# Fit regression model - chocolate vs nobel without controlling for GDP\nchocolate_nobel_model &lt;- lm(nobel ~ chocolate, data = country_data)\n\n# Better model that reveals the confounding\nfull_model &lt;- lm(nobel ~ chocolate + gdp, data = country_data)\n\n# Plot the apparent relationship\nggplot(country_data, aes(x = chocolate, y = nobel)) +\n  geom_point(color = \"darkblue\", size = 3, alpha = 0.7) +\n  geom_text(aes(label = country), hjust = -0.2, vjust = 0, size = 3) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  labs(\n    title = \"Apparent Correlation: Chocolate Consumption vs. Nobel Prizes\",\n    subtitle = \"Demonstrates how confounding variables create spurious correlations\",\n    x = \"Chocolate Consumption (kg per capita)\",\n    y = \"Nobel Prizes per 10M Population\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Show regression results\nsummary(chocolate_nobel_model)\n\n\nCall:\nlm(formula = nobel ~ chocolate, data = country_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9080 -1.4228  0.0294  0.5962  3.2977 \n\nCoefficients:\n            Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept)  -4.0518     1.3633  -2.972     0.0101 *  \nchocolate     1.3322     0.1682   7.921 0.00000154 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.626 on 14 degrees of freedom\nMultiple R-squared:  0.8176,    Adjusted R-squared:  0.8045 \nF-statistic: 62.75 on 1 and 14 DF,  p-value: 0.000001536\n\n# Demonstrate multiple testing problem\np_values &lt;- numeric(100)\nfor(i in 1:100) {\n  # Generate two completely random variables with n=20\n  x &lt;- rnorm(20)\n  y &lt;- rnorm(20)\n  # Test for correlation and store p-value\n  p_values[i] &lt;- cor.test(x, y)$p.value\n}\n\n# How many \"significant\" results at alpha = 0.05?\nsum(p_values &lt; 0.05)\n\n[1] 3\n\n# Visualize the multiple testing phenomenon\nhist(p_values, breaks = 20, main = \"P-values from 100 Tests of Random Data\",\n     xlab = \"P-value\", col = \"lightblue\", border = \"white\")\nabline(v = 0.05, col = \"red\", lwd = 2, lty = 2)\ntext(0.15, 20, paste(\"Approximately\", sum(p_values &lt; 0.05),\n                     \"tests are 'significant'\\nby random chance alone!\"), \n     col = \"darkred\")\n\n\n\n\n\n\n\n\nThis example demonstrates how seemingly compelling correlations can emerge between unrelated variables due to confounding factors and chance. The correlation between chocolate consumption and Nobel prizes appears significant (p &lt; 0.05) when analyzed directly, even though it’s explained by a third variable - national wealth (GDP per capita).\nWealthier countries typically consume more chocolate and simultaneously invest more in education and research, leading to more Nobel prizes. Without controlling for this confounding factor, we would mistakenly conclude a direct relationship between chocolate and Nobel prizes.\nThe multiple testing demonstration further illustrates why spurious correlations appear so frequently in research. When conducting 100 statistical tests on completely random data, we expect approximately 5 “significant” results at α = 0.05 purely by chance. In real research settings where hundreds of variables might be analyzed, the probability of finding false positive correlations increases dramatically.\nThis example underscores three critical points:\n\nSmall sample sizes (16 countries) are particularly vulnerable to chance correlations\nConfounding variables can create strong apparent associations between unrelated factors\nMultiple testing without appropriate corrections virtually guarantees finding “significant” but meaningless patterns\n\nSuch findings explain why replication is essential in research and why most initial “discoveries” fail to hold up in subsequent studies.\n\n\nConfounding Variables (Hidden Third Factors)\nConfounding occurs when an external variable influences both the predictor and outcome variables, creating an apparent relationship that may disappear when the confounder is accounted for.\n\n# Create sample data\nn &lt;- 200\nability &lt;- rnorm(n, 100, 15)                       # Natural ability \neducation &lt;- 10 + 0.05 * ability + rnorm(n, 0, 2)  # Education affected by ability\nincome &lt;- 10000 + 2000 * education + 100 * ability + rnorm(n, 0, 5000)  # Income affected by both\n\nomitted_var_data &lt;- data.frame(\n  ability = ability,\n  education = education,\n  income = income\n)\n\n# Model without accounting for ability\nmodel_naive &lt;- lm(income ~ education, data = omitted_var_data)\n\n# Model accounting for ability\nmodel_full &lt;- lm(income ~ education + ability, data = omitted_var_data)\n\n# Show results\nsummary(model_naive)\n\n\nCall:\nlm(formula = income ~ education, data = omitted_var_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14422.9  -3362.1    142.7   3647.7  14229.6 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  18982.0     2410.5   7.875    0.000000000000221 ***\neducation     2050.9      158.7  12.926 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5066 on 198 degrees of freedom\nMultiple R-squared:  0.4576,    Adjusted R-squared:  0.4549 \nF-statistic: 167.1 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_full)\n\n\nCall:\nlm(formula = income ~ education + ability, data = omitted_var_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12739.9  -3388.7    -41.1   3572.1  14976.8 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 13203.84    3018.85   4.374            0.0000198 ***\neducation    1871.43     166.03  11.272 &lt; 0.0000000000000002 ***\nability        85.60      27.87   3.071              0.00243 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4961 on 197 degrees of freedom\nMultiple R-squared:  0.4824,    Adjusted R-squared:  0.4772 \nF-statistic: 91.81 on 2 and 197 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Create visualization with ability shown through color\nggplot(omitted_var_data, aes(x = education, y = income, color = ability)) +\n  geom_point(alpha = 0.8) +\n  scale_color_viridis_c(name = \"Ability Score\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) + \n  labs(\n    title = \"Income vs. Education, Colored by Ability\",\n    subtitle = \"Visualizing the confounding variable\",\n    x = \"Years of Education\",\n    y = \"Annual Income (PLN)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis example illustrates omitted variable bias: without accounting for ability, the estimated effect of education on income is exaggerated (2,423 PLN per year vs. 1,962 PLN per year). The confounding occurs because ability influences both education and income, creating a spurious component in the observed correlation.\n\nClassic Example: Ice Cream and Drownings\nA classic example of confounding involves the correlation between ice cream sales and drowning incidents, both influenced by temperature:\n\n# Create sample data\nn &lt;- 100\ntemperature &lt;- runif(n, 5, 35)  # Temperature in Celsius\n\n# Both ice cream sales and drownings are influenced by temperature\nice_cream_sales &lt;- 100 + 10 * temperature + rnorm(n, 0, 20)\ndrownings &lt;- 1 + 0.3 * temperature + rnorm(n, 0, 1)\n\nconfounding_data &lt;- data.frame(\n  temperature = temperature,\n  ice_cream_sales = ice_cream_sales,\n  drownings = drownings\n)\n\n# Model without controlling for temperature\nmodel_naive &lt;- lm(drownings ~ ice_cream_sales, data = confounding_data)\n\n# Model controlling for temperature\nmodel_full &lt;- lm(drownings ~ ice_cream_sales + temperature, data = confounding_data)\n\n# Show results\nsummary(model_naive)\n\n\nCall:\nlm(formula = drownings ~ ice_cream_sales, data = confounding_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8163 -0.7597  0.0118  0.7846  2.5797 \n\nCoefficients:\n                 Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)     -1.503063   0.370590  -4.056              0.0001 ***\nice_cream_sales  0.028074   0.001205  23.305 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.088 on 98 degrees of freedom\nMultiple R-squared:  0.8471,    Adjusted R-squared:  0.8456 \nF-statistic: 543.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_full)\n\n\nCall:\nlm(formula = drownings ~ ice_cream_sales + temperature, data = confounding_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.85074 -0.61169  0.01186  0.60556  2.01776 \n\nCoefficients:\n                 Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)      1.243785   0.530123   2.346         0.021 *  \nice_cream_sales -0.002262   0.004839  -0.467         0.641    \ntemperature      0.317442   0.049515   6.411 0.00000000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9169 on 97 degrees of freedom\nMultiple R-squared:  0.8926,    Adjusted R-squared:  0.8904 \nF-statistic: 403.2 on 2 and 97 DF,  p-value: &lt; 0.00000000000000022\n\n# Create visualization\nggplot(confounding_data, aes(x = ice_cream_sales, y = drownings, color = temperature)) +\n  geom_point(alpha = 0.8) +\n  scale_color_viridis_c(name = \"Temperature (°C)\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"The Ice Cream and Drownings Correlation\",\n    subtitle = \"Temperature as a confounding variable\",\n    x = \"Ice Cream Sales\",\n    y = \"Drowning Incidents\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe naive model shows a statistically significant relationship between ice cream sales and drownings. However, once temperature is included in the model, the coefficient for ice cream sales decreases substantially and becomes statistically insignificant. This demonstrates how failing to account for confounding variables can lead to spurious correlations.\n\n\n\nReverse Causality\nReverse causality occurs when the assumed direction of causation is incorrect. Consider this example of anxiety and relaxation techniques:\n\n# Create sample data\nn &lt;- 200\nanxiety_level &lt;- runif(n, 1, 10)  # Anxiety level (1-10)\n\n# People with higher anxiety tend to use more relaxation techniques\nrelaxation_techniques &lt;- 1 + 0.7 * anxiety_level + rnorm(n, 0, 1)\n\nreverse_data &lt;- data.frame(\n  anxiety = anxiety_level,\n  relaxation = relaxation_techniques\n)\n\n# Fit models in both directions\nmodel_incorrect &lt;- lm(anxiety ~ relaxation, data = reverse_data)\nmodel_correct &lt;- lm(relaxation ~ anxiety, data = reverse_data)\n\n# Show regression results\nsummary(model_incorrect)\n\n\nCall:\nlm(formula = anxiety ~ relaxation, data = reverse_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9651 -0.7285 -0.0923  0.7247  3.7996 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) -0.09482    0.21973  -0.432               0.667    \nrelaxation   1.15419    0.04105  28.114 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.182 on 198 degrees of freedom\nMultiple R-squared:  0.7997,    Adjusted R-squared:  0.7987 \nF-statistic: 790.4 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_correct)\n\n\nCall:\nlm(formula = relaxation ~ anxiety, data = reverse_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.15178 -0.51571 -0.00222  0.55513  2.04334 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  1.05726    0.15286   6.917      0.0000000000624 ***\nanxiety      0.69284    0.02464  28.114 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9161 on 198 degrees of freedom\nMultiple R-squared:  0.7997,    Adjusted R-squared:  0.7987 \nF-statistic: 790.4 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(reverse_data, aes(x = relaxation, y = anxiety)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Anxiety and Relaxation Techniques\",\n    subtitle = \"Example of reverse causality\",\n    x = \"Use of Relaxation Techniques (frequency/week)\",\n    y = \"Anxiety Level (1-10 scale)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nBoth regression models show statistically significant relationships, but they imply different causal mechanisms. The incorrect model suggests that relaxation techniques increase anxiety, while the correct model reflects the true data generating process: anxiety drives the use of relaxation techniques.\n\n\nCollider Bias (Selection Bias)\nCollider bias occurs when conditioning on a variable that is affected by both the independent and dependent variables of interest, creating an artificial relationship between variables that are actually independent.\n\n# Create sample data\nn &lt;- 1000\n\n# Generate two independent variables (no relationship between them)\nintelligence &lt;- rnorm(n, 100, 15)  # IQ score\nfamily_wealth &lt;- rnorm(n, 50, 15)  # Wealth score (independent from intelligence)\n  \n# True data-generating process: admission depends on both intelligence and wealth\nadmission_score &lt;- 0.4 * intelligence + 0.4 * family_wealth + rnorm(n, 0, 10)\nadmitted &lt;- admission_score &gt; median(admission_score)  # Binary admission variable\n\n# Create full dataset\nfull_data &lt;- data.frame(\n  intelligence = intelligence,\n  wealth = family_wealth,\n  admission_score = admission_score,\n  admitted = admitted\n)\n\n# Regression in full population (true model)\nfull_model &lt;- lm(intelligence ~ wealth, data = full_data)\nsummary(full_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth, data = full_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-49.608 -10.115   0.119  10.832  55.581 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 101.42330    1.73139   58.58 &lt;0.0000000000000002 ***\nwealth       -0.02701    0.03334   -0.81               0.418    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.41 on 998 degrees of freedom\nMultiple R-squared:  0.0006569, Adjusted R-squared:  -0.0003444 \nF-statistic: 0.656 on 1 and 998 DF,  p-value: 0.4182\n\n# Get just the admitted students\nadmitted_only &lt;- full_data[full_data$admitted, ]\n\n# Regression in admitted students (conditioning on the collider)\nadmitted_model &lt;- lm(intelligence ~ wealth, data = admitted_only)\nsummary(admitted_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth, data = admitted_only)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.511  -9.064   0.721   8.965  48.267 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 115.4750     2.6165  44.133 &lt; 0.0000000000000002 ***\nwealth       -0.1704     0.0462  -3.689              0.00025 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.91 on 498 degrees of freedom\nMultiple R-squared:  0.0266,    Adjusted R-squared:  0.02464 \nF-statistic: 13.61 on 1 and 498 DF,  p-value: 0.0002501\n\n# Additional analysis - regression with the collider as a control variable\n# This demonstrates how controlling for a collider introduces bias\ncollider_control_model &lt;- lm(intelligence ~ wealth + admitted, data = full_data)\nsummary(collider_control_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth + admitted, data = full_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.729  -8.871   0.700   8.974  48.044 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  102.90069    1.56858  65.601 &lt; 0.0000000000000002 ***\nwealth        -0.19813    0.03224  -6.145        0.00000000116 ***\nadmittedTRUE  14.09944    0.94256  14.959 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.93 on 997 degrees of freedom\nMultiple R-squared:  0.1838,    Adjusted R-squared:  0.1822 \nF-statistic: 112.3 on 2 and 997 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Plot for full population\np1 &lt;- ggplot(full_data, aes(x = wealth, y = intelligence)) +\n  geom_point(alpha = 0.3, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Full Population\",\n    subtitle = paste(\"Correlation:\", round(cor(full_data$intelligence, full_data$wealth), 3)),\n    x = \"Family Wealth Score\",\n    y = \"Intelligence Score\"\n  ) +\n  theme_minimal()\n\n# Plot for admitted students\np2 &lt;- ggplot(admitted_only, aes(x = wealth, y = intelligence)) +\n  geom_point(alpha = 0.3, color = \"darkgreen\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Admitted Students Only\",\n    subtitle = paste(\"Correlation:\", round(cor(admitted_only$intelligence, admitted_only$wealth), 3)),\n    x = \"Family Wealth Score\",\n    y = \"Intelligence Score\"\n  ) +\n  theme_minimal()\n\n# Display plots side by side\nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis example demonstrates collider bias in three ways:\n\nIn the full population, intelligence and wealth have no relationship (coefficient near zero, p-value = 0.87)\nAmong admitted students (conditioning on the collider), a significant negative relationship appears (coefficient = -0.39, p-value &lt; 0.001)\nWhen controlling for admission status in a regression, a spurious relationship is introduced (coefficient = -0.16, p-value &lt; 0.001)\n\nThe collider bias creates relationships between variables that are truly independent. This can be represented in a directed acyclic graph (DAG):\n\\text{Intelligence} \\rightarrow \\text{Admission} \\leftarrow \\text{Wealth}\nWhen we condition on admission (the collider), we create a spurious association between intelligence and wealth.\n\n\nImproper Analysis\nInappropriate statistical methods can produce spurious correlations. Common issues include using linear models for non-linear relationships, ignoring data clustering, or mishandling time series data.\n\n# Generate data with a true non-linear relationship\nn &lt;- 100\nx &lt;- seq(-3, 3, length.out = n)\ny &lt;- x^2 + rnorm(n, 0, 1)  # Quadratic relationship\n\nimproper_data &lt;- data.frame(x = x, y = y)\n\n# Fit incorrect linear model\nwrong_model &lt;- lm(y ~ x, data = improper_data)\n\n# Fit correct quadratic model\ncorrect_model &lt;- lm(y ~ x + I(x^2), data = improper_data)\n\n# Show results\nsummary(wrong_model)\n\n\nCall:\nlm(formula = y ~ x, data = improper_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2176 -2.1477 -0.6468  2.4365  7.3457 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  3.14689    0.28951  10.870 &lt;0.0000000000000002 ***\nx            0.08123    0.16548   0.491               0.625    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.895 on 98 degrees of freedom\nMultiple R-squared:  0.002453,  Adjusted R-squared:  -0.007726 \nF-statistic: 0.2409 on 1 and 98 DF,  p-value: 0.6246\n\nsummary(correct_model)\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = improper_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81022 -0.65587  0.01935  0.61168  2.68894 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  0.12407    0.14498   0.856               0.394    \nx            0.08123    0.05524   1.470               0.145    \nI(x^2)       0.98766    0.03531  27.972 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9664 on 97 degrees of freedom\nMultiple R-squared:   0.89, Adjusted R-squared:  0.8877 \nF-statistic: 392.3 on 2 and 97 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(improper_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), color = \"green\", se = FALSE) +\n  labs(\n    title = \"Improper Analysis Example\",\n    subtitle = \"Linear model (red) vs. Quadratic model (green)\",\n    x = \"Variable X\",\n    y = \"Variable Y\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe linear model incorrectly suggests no relationship between x and y (coefficient near zero, p-value = 0.847), while the quadratic model reveals the true relationship (R^2 = 0.90). This demonstrates how model misspecification can create spurious non-correlations, masking real relationships that exist in different forms.\n\n\nEndogeneity and Its Sources\nEndogeneity occurs when an explanatory variable is correlated with the error term in a regression model. This violates the exogeneity assumption of OLS regression and leads to biased estimates. There are several sources of endogeneity:\n\nOmitted Variable Bias\nAs shown in the education-income example, when important variables are omitted from the model, their effects are absorbed into the error term, which becomes correlated with included variables.\n\n\nMeasurement Error\nWhen variables are measured with error, the observed values differ from true values, creating correlation between the error term and the predictors.\n\n\nSimultaneity (Bidirectional Causality)\nWhen the dependent variable also affects the independent variable, creating a feedback loop. Let’s demonstrate this:\n\n# Create sample data with mutual influence\nn &lt;- 100\n\n# Initialize variables\neconomic_growth &lt;- rnorm(n, 2, 1)\nemployment_rate &lt;- rnorm(n, 60, 5)\n\n# Create mutual influence through iterations\nfor(i in 1:3) {\n  economic_growth &lt;- 2 + 0.05 * employment_rate + rnorm(n, 0, 0.5)\n  employment_rate &lt;- 50 + 5 * economic_growth + rnorm(n, 0, 2)\n}\n\nsimultaneity_data &lt;- data.frame(\n  growth = economic_growth,\n  employment = employment_rate\n)\n\n# Model estimating effect of growth on employment\nmodel_growth_on_emp &lt;- lm(employment ~ growth, data = simultaneity_data)\n\n# Model estimating effect of employment on growth\nmodel_emp_on_growth &lt;- lm(growth ~ employment, data = simultaneity_data)\n\n# Show results\nsummary(model_growth_on_emp)\n\n\nCall:\nlm(formula = employment ~ growth, data = simultaneity_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.603 -1.500 -0.099  1.387  5.673 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  49.9665     2.0717   24.12 &lt;0.0000000000000002 ***\ngrowth        5.0151     0.3528   14.22 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.045 on 98 degrees of freedom\nMultiple R-squared:  0.6734,    Adjusted R-squared:  0.6701 \nF-statistic: 202.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_emp_on_growth)\n\n\nCall:\nlm(formula = growth ~ employment, data = simultaneity_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11417 -0.20626 -0.02185  0.22646  0.72941 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) -4.801257   0.749557  -6.405        0.00000000523 ***\nemployment   0.134283   0.009446  14.216 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3346 on 98 degrees of freedom\nMultiple R-squared:  0.6734,    Adjusted R-squared:  0.6701 \nF-statistic: 202.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(simultaneity_data, aes(x = growth, y = employment)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Simultaneity Between Economic Growth and Employment\",\n    x = \"Economic Growth (%)\",\n    y = \"Employment Rate (%)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe true data generating process is a system of simultaneous equations:\n\\text{Growth}_i = \\alpha_0 + \\alpha_1 \\text{Employment}_i + u_i \\text{Employment}_i = \\beta_0 + \\beta_1 \\text{Growth}_i + v_i\nStandard OLS regression cannot consistently estimate either equation because each explanatory variable is correlated with the error term in its respective equation.\n\n\nSelection Bias\nWhen the sample is not randomly selected from the population, the selection process can introduce correlation between the error term and the predictors. The collider bias example demonstrates a form of selection bias.\nThe consequences of endogeneity include: - Biased coefficient estimates - Incorrect standard errors - Invalid hypothesis tests - Misleading causal interpretations\nAddressing endogeneity requires specialized methods such as instrumental variables, system estimation, panel data methods, or experimental designs.\n\n\n\n\n\n\nUnderstanding Endogeneity in Regression\n\n\n\nEndogeneity is a critical concept in statistical analysis that occurs when an explanatory variable in a regression model is correlated with the error term. This creates challenges for accurately understanding cause-and-effect relationships in research. Let’s examine the three main types of endogeneity and how they affect research outcomes.\n\nOmitted Variable Bias (OVB)\nOmitted Variable Bias occurs when an important variable that affects both the dependent and independent variables is left out of the analysis. This omission leads to incorrect conclusions about the relationship between the variables we’re studying.\nConsider a study examining the relationship between education and income:\nExample: Education and Income The observed relationship shows that more education correlates with higher income. However, an individual’s inherent abilities affect both their educational attainment and their earning potential. Without accounting for ability, we may overestimate education’s direct effect on income.\nThe statistical representation shows why this matters:\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2z_i + \\epsilon_i (Complete model)\ny_i = \\beta_0 + \\beta_1x_i + u_i (Incomplete model)\nWhen we omit an important variable, our estimates of the remaining relationships become biased and unreliable.\n\n\nSimultaneity\nSimultaneity occurs when two variables simultaneously influence each other, making it difficult to determine the direction of causation. This creates a feedback loop that complicates statistical analysis.\nCommon Examples of Simultaneity:\nAcademic Performance and Study Habits represent a clear case of simultaneity. Academic performance influences how much time students dedicate to studying, while study time affects academic performance. This two-way relationship makes it challenging to measure the isolated effect of either variable.\nMarket Dynamics provide another example. Prices influence demand, while demand influences prices. This concurrent relationship requires special analytical approaches to understand the true relationships.\n\n\nMeasurement Error\nMeasurement error occurs when we cannot accurately measure our variables of interest. This imprecision can significantly impact our analysis and conclusions.\nCommon Sources of Measurement Error:\nSelf-Reported Data presents a significant challenge. When participants report their own behaviors or characteristics, such as study time, the reported values often differ from actual values. This discrepancy affects our ability to measure true relationships.\nTechnical Limitations also contribute to measurement error through imprecise measuring tools, inconsistent measurement conditions, and recording or data entry errors.\n\n\nAddressing Endogeneity in Research\n\nIdentification Strategies\n\n# Example of controlling for omitted variables\nmodel_simple &lt;- lm(income ~ education, data = df)\nmodel_full &lt;- lm(income ~ education + ability + experience + region, data = df)\n\n# Compare coefficients\nsummary(model_simple)\nsummary(model_full)\n\n\nInclude Additional Variables: Collect data on potentially important omitted variables and include relevant control variables in your analysis. For example, including measures of ability when studying education’s effect on income.\nUse Panel Data: Collect data across multiple time periods to control for unobserved fixed characteristics and analyze changes over time.\nInstrumental Variables: Find variables that affect your independent variable but not your dependent variable to isolate the relationship of interest.\n\n\n\nImproving Measurement\n\nMultiple Measurements: Take several measurements of key variables, use averaging to reduce random error, and compare different measurement methods.\nBetter Data Collection: Use validated measurement instruments, implement quality control procedures, and document potential sources of error.\n\n\n\n\nBest Practices for Researchers\nResearch Design fundamentally shapes your ability to address endogeneity. Plan for potential endogeneity issues before collecting data, include measures for potentially important control variables, and consider using multiple measurement approaches.\nAnalysis should include testing for endogeneity when possible, using appropriate statistical methods for your specific situation, and documenting assumptions and limitations.\nReporting must clearly describe potential endogeneity concerns, explain how you addressed these issues, and discuss implications for your conclusions.\n\n\n\n\n\n\n\n\n\nFormal Derivation of OLS Estimators: A Complete Mathematical Treatment\n\n\n\n\nObjective and Setup\nWe seek to find the values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that minimize the sum of squared residuals:\nSSE = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nThis is an unconstrained optimization problem where we treat SSE as a function of two variables: SSE(\\hat{\\beta}_0, \\hat{\\beta}_1).\n\n\nMathematical Prerequisites\nChain Rule for Composite Functions: For f(g(x)), the derivative is: \\frac{d}{dx}[f(g(x))] = f'(g(x)) \\cdot g'(x)\nIn our context:\n\nOuter function: f(u) = u^2 with derivative f'(u) = 2u\nInner function: u = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\n\nFirst-Order Conditions: At a minimum, both partial derivatives equal zero: \\frac{\\partial SSE}{\\partial \\hat{\\beta}_0} = 0 \\quad \\text{and} \\quad \\frac{\\partial SSE}{\\partial \\hat{\\beta}_1} = 0\n\n\nDerivation of \\hat{\\beta}_0\nStep 1: Take the partial derivative with respect to \\hat{\\beta}_0:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_0} = \\frac{\\partial}{\\partial \\hat{\\beta}_0} \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2\nStep 2: Apply the chain rule to each term: = \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot \\frac{\\partial}{\\partial \\hat{\\beta}_0}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\n= \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot (-1)\n= -2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\nStep 3: Set equal to zero and solve: -2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\nDividing by -2 and expanding: \\sum_{i=1}^n y_i - n\\hat{\\beta}_0 - \\hat{\\beta}_1\\sum_{i=1}^n x_i = 0\nStep 4: Isolate \\hat{\\beta}_0: n\\hat{\\beta}_0 = \\sum_{i=1}^n y_i - \\hat{\\beta}_1\\sum_{i=1}^n x_i\n\\boxed{\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}}\nInterpretation: The intercept adjusts to ensure the regression line passes through the point of means (\\bar{x}, \\bar{y}).\n\n\nDerivation of \\hat{\\beta}_1\nStep 1: Take the partial derivative with respect to \\hat{\\beta}_1:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_1} = \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot (-x_i)\n= -2\\sum_{i=1}^n x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\nStep 2: Substitute the expression for \\hat{\\beta}_0: = -2\\sum_{i=1}^n x_i(y_i - (\\bar{y} - \\hat{\\beta}_1\\bar{x}) - \\hat{\\beta}_1x_i)\n= -2\\sum_{i=1}^n x_i((y_i - \\bar{y}) - \\hat{\\beta}_1(x_i - \\bar{x}))\nStep 3: Set equal to zero and expand: \\sum_{i=1}^n x_i(y_i - \\bar{y}) - \\hat{\\beta}_1\\sum_{i=1}^n x_i(x_i - \\bar{x}) = 0\nStep 4: Use the algebraic identity \\sum_{i=1}^n x_i(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}):\nThis identity holds because: \\sum_{i=1}^n x_i(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x} + \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) + \\bar{x}\\sum_{i=1}^n(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) + 0\nSimilarly, \\sum_{i=1}^n x_i(x_i - \\bar{x}) = \\sum_{i=1}^n (x_i - \\bar{x})^2.\nStep 5: Solve for \\hat{\\beta}_1: \\hat{\\beta}_1\\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\n\\boxed{\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}}\n\n\nVerification of Minimum (Second-Order Conditions)\nTo confirm we have found a minimum (not a maximum or saddle point), we examine the Hessian matrix of second partial derivatives:\nSecond partial derivatives:\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_0^2} = 2n &gt; 0\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_1^2} = 2\\sum_{i=1}^n x_i^2 &gt; 0\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_0 \\partial \\hat{\\beta}_1} = 2\\sum_{i=1}^n x_i\nHessian matrix: \\mathbf{H} = 2\\begin{bmatrix} n & \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i & \\sum_{i=1}^n x_i^2 \\end{bmatrix}\nPositive definiteness check:\n\nFirst leading principal minor: 2n &gt; 0 ✓\nSecond leading principal minor (determinant): \\det(\\mathbf{H}) = 4\\left(n\\sum_{i=1}^n x_i^2 - \\left(\\sum_{i=1}^n x_i\\right)^2\\right) = 4n\\sum_{i=1}^n(x_i - \\bar{x})^2 &gt; 0 ✓\n\nSince the Hessian is positive definite, we have confirmed a minimum.\n\n\nGeometric Interpretation\n\n# Visualizing the optimization surface\nlibrary(tidyverse)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Generate sample data\nset.seed(42)\nx &lt;- runif(20, 1, 8)\ny &lt;- 2 + 3*x + rnorm(20, 0, 1)\n\n# Create grid of beta values\nbeta0_seq &lt;- seq(0, 4, length.out = 50)\nbeta1_seq &lt;- seq(2, 4, length.out = 50)\ngrid &lt;- expand.grid(beta0 = beta0_seq, beta1 = beta1_seq)\n\n# Calculate SSE for each combination\ngrid$SSE &lt;- apply(grid, 1, function(params) {\n  sum((y - (params[1] + params[2]*x))^2)\n})\n\n# Create contour plot\nggplot(grid, aes(x = beta0, y = beta1, z = SSE)) +\n  geom_contour_filled(aes(fill = after_stat(level))) +\n  geom_point(x = coef(lm(y ~ x))[1], \n             y = coef(lm(y ~ x))[2], \n             color = \"red\", size = 3) +\n  labs(title = \"SSE Surface in Parameter Space\",\n       subtitle = \"Red point shows the OLS minimum\",\n       x = expression(hat(beta)[0]),\n       y = expression(hat(beta)[1])) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.1-understanding-correlation-measures-a-self-study-tutorial-using-stress-level-and-cognitive-performance-data",
    "href": "correg_en.html#appendix-a.1-understanding-correlation-measures-a-self-study-tutorial-using-stress-level-and-cognitive-performance-data",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.36 Appendix A.1: Understanding Correlation Measures: A Self-Study Tutorial (Using Stress Level and Cognitive Performance Data)",
    "text": "9.36 Appendix A.1: Understanding Correlation Measures: A Self-Study Tutorial (Using Stress Level and Cognitive Performance Data)\n\nDataset Overview\n\ndata &lt;- data.frame(\n  anxiety_level = c(8, 5, 11, 14, 7, 10),\n  cognitive_performance = c(85, 90, 62, 55, 80, 65)\n)\n\n\n\n1. Covariance Calculation\n\nStep 1: Calculate Means\n\n\n\n\n\n\n\n\nVariable\nCalculation\nResult\n\n\n\n\nMean Anxiety (\\bar{x})\n(8 + 5 + 11 + 14 + 7 + 10) ÷ 6\n9.17\n\n\nMean Cognitive (\\bar{y})\n(85 + 90 + 62 + 55 + 80 + 65) ÷ 6\n72.83\n\n\n\n\n\nStep 2: Calculate Deviations and Products\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\n(x_i - \\bar{x})\n(y_i - \\bar{y})\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n1\n8\n85\n-1.17\n12.17\n-14.24\n\n\n2\n5\n90\n-4.17\n17.17\n-71.60\n\n\n3\n11\n62\n1.83\n-10.83\n-19.82\n\n\n4\n14\n55\n4.83\n-17.83\n-86.12\n\n\n5\n7\n80\n-2.17\n7.17\n-15.56\n\n\n6\n10\n65\n0.83\n-7.83\n-6.50\n\n\nSum\n55\n437\n0.00\n0.00\n-213.84\n\n\n\n\n\nStep 3: Calculate Covariance\n \\text{Cov}(X,Y) = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n-1} = \\frac{-213.84}{5} = -42.77 \n\n\n\n2. Pearson Correlation Coefficient\n\nStep 1: Calculate Squared Deviations\n\n\n\n\n\n\n\n\n\n\ni\n(x_i - \\bar{x})\n(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n-1.17\n12.17\n1.37\n148.11\n\n\n2\n-4.17\n17.17\n17.39\n294.81\n\n\n3\n1.83\n-10.83\n3.35\n117.29\n\n\n4\n4.83\n-17.83\n23.33\n317.91\n\n\n5\n-2.17\n7.17\n4.71\n51.41\n\n\n6\n0.83\n-7.83\n0.69\n61.31\n\n\nSum\n0.00\n0.00\n50.84\n990.84\n\n\n\n\n\nStep 2: Calculate Standard Deviations\n\n\n\n\n\n\n\n\n\nMeasure\nFormula\nCalculation\nResult\n\n\n\n\ns_x\n\\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n-1}}\n\\sqrt{\\frac{50.84}{5}}\n3.19\n\n\ns_y\n\\sqrt{\\frac{\\sum (y_i - \\bar{y})^2}{n-1}}\n\\sqrt{\\frac{990.84}{5}}\n14.08\n\n\n\n\n\nStep 3: Calculate Pearson Correlation\n r = \\frac{\\text{Cov}(X,Y)}{s_x s_y} = \\frac{-42.77}{3.19 \\times 14.08} = -0.95 \n\n\n\n3. Spearman Rank Correlation\n\nStep 1: Assign Ranks\n\n\n\ni\nx_i\ny_i\nRank x_i\nRank y_i\nd_i\nd_i^2\n\n\n\n\n1\n8\n85\n3\n2\n1\n1\n\n\n2\n5\n90\n1\n1\n0\n0\n\n\n3\n11\n62\n5\n5\n0\n0\n\n\n4\n14\n55\n6\n6\n0\n0\n\n\n5\n7\n80\n2\n3\n-1\n1\n\n\n6\n10\n65\n4\n4\n0\n0\n\n\nSum\n\n\n\n\n\n2\n\n\n\n\n\nStep 2: Calculate Spearman Correlation\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} = 1 - \\frac{6(2)}{6(36-1)} = 1 - \\frac{12}{210} = 0.94 \n\n\n\nVerification using R\n\n# Calculate correlations using R\ncor(data$anxiety_level, data$cognitive_performance, method = \"pearson\")\n\n[1] -0.9527979\n\ncor(data$anxiety_level, data$cognitive_performance, method = \"spearman\")\n\n[1] -0.9428571\n\n\n\n\nInterpretation\n\nThe strong negative Pearson correlation (r = -0.95) indicates a very strong negative linear relationship between anxiety level and cognitive performance.\nThe strong positive Spearman correlation (ρ = 0.94) shows that the relationship is also strongly monotonic.\nThe difference between Pearson and Spearman correlations suggests that while there is a strong relationship, it might not be perfectly linear.\n\n\n\nExercise\n\nVerify each calculation step in the tables above.\nTry calculating these measures with a modified dataset:\n\nAdd one outlier and observe how it affects both correlation coefficients\nChange one pair of values and recalculate",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.2-calculating-covariance-pearson-spearman-correlation-and-ols---worked-example",
    "href": "correg_en.html#appendix-a.2-calculating-covariance-pearson-spearman-correlation-and-ols---worked-example",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.37 Appendix A.2: Calculating Covariance, Pearson, Spearman Correlation, and OLS - worked example",
    "text": "9.37 Appendix A.2: Calculating Covariance, Pearson, Spearman Correlation, and OLS - worked example\nA political science student is investigating the relationship between district magnitude (DM) and Gallagher’s disproportionality index (GH) in parliamentary elections across 10 randomly selected democracies.\nData on electoral district magnitudes (\\text{DM}) and Gallagher index:\n\n\n\n\\text{DM} (X)\nGallagher (Y)\n\n\n\n\n2\n18.2\n\n\n3\n16.7\n\n\n4\n15.8\n\n\n5\n15.3\n\n\n6\n15.0\n\n\n7\n14.8\n\n\n8\n14.7\n\n\n9\n14.6\n\n\n10\n14.55\n\n\n11\n14.52\n\n\n\n\nStep 1: Calculate Basic Statistics\nCalculation of means:\nFor \\text{DM} (X): \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\nDetailed calculation:\n2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 65 \\bar{x} = \\frac{65}{10} = 6.5\nFor Gallagher index (Y): \\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\nDetailed calculation:\n18.2 + 16.7 + 15.8 + 15.3 + 15.0 + 14.8 + 14.7 + 14.6 + 14.55 + 14.52 = 154.17 \\bar{y} = \\frac{154.17}{10} = 15.417\n\n\nStep 2: Detailed Covariance Calculations\nComplete working table showing all calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18.2\n-4.5\n2.783\n-12.5235\n20.25\n7.7451\n\n\n2\n3\n16.7\n-3.5\n1.283\n-4.4905\n12.25\n1.6461\n\n\n3\n4\n15.8\n-2.5\n0.383\n-0.9575\n6.25\n0.1467\n\n\n4\n5\n15.3\n-1.5\n-0.117\n0.1755\n2.25\n0.0137\n\n\n5\n6\n15.0\n-0.5\n-0.417\n0.2085\n0.25\n0.1739\n\n\n6\n7\n14.8\n0.5\n-0.617\n-0.3085\n0.25\n0.3807\n\n\n7\n8\n14.7\n1.5\n-0.717\n-1.0755\n2.25\n0.5141\n\n\n8\n9\n14.6\n2.5\n-0.817\n-2.0425\n6.25\n0.6675\n\n\n9\n10\n14.55\n3.5\n-0.867\n-3.0345\n12.25\n0.7517\n\n\n10\n11\n14.52\n4.5\n-0.897\n-4.0365\n20.25\n0.8047\n\n\nSum\n65\n154.17\n0\n0\n-28.085\n82.5\n12.8442\n\n\n\nCovariance calculation: \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\text{Cov}(X,Y) = \\frac{-28.085}{9} = -3.120556\n\n\nStep 3: Standard Deviation Calculations\nFor \\text{DM} (X): \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}}\n\\sigma_x = \\sqrt{\\frac{82.5}{9}} = \\sqrt{9.1667} = 3.026582\nFor Gallagher (Y): \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}}\n\\sigma_y = \\sqrt{\\frac{12.8442}{9}} = \\sqrt{1.4271} = 1.194612\n\n\nStep 4: Pearson Correlation Calculation\nr = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\nr = \\frac{-3.120556}{3.026582 \\times 1.194612} = \\frac{-3.120556}{3.615752} = -0.863044\n\n\nStep 5: Spearman Rank Correlation Calculation\nComplete ranking table with all calculations:\n\n\n\ni\nX_i\nY_i\nRank X_i\nRank Y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n18.2\n1\n10\n-9\n81\n\n\n2\n3\n16.7\n2\n9\n-7\n49\n\n\n3\n4\n15.8\n3\n8\n-5\n25\n\n\n4\n5\n15.3\n4\n7\n-3\n9\n\n\n5\n6\n15.0\n5\n6\n-1\n1\n\n\n6\n7\n14.8\n6\n5\n1\n1\n\n\n7\n8\n14.7\n7\n4\n3\n9\n\n\n8\n9\n14.6\n8\n3\n5\n25\n\n\n9\n10\n14.55\n9\n2\n7\n49\n\n\n10\n11\n14.52\n10\n1\n9\n81\n\n\nSum\n\n\n\n\n\n330\n\n\n\nSpearman correlation calculation: \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n\\rho = 1 - \\frac{6 \\times 330}{10(100 - 1)} = 1 - \\frac{1980}{990} = 1 - 2 = -1\n\n\nStep 6: R Verification\n\n# Create vectors\nDM &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nGH &lt;- c(18.2, 16.7, 15.8, 15.3, 15.0, 14.8, 14.7, 14.6, 14.55, 14.52)\n\n# Calculate covariance\ncov(DM, GH)\n\n[1] -3.120556\n\n# Calculate correlations\ncor(DM, GH, method = \"pearson\")\n\n[1] -0.8627742\n\ncor(DM, GH, method = \"spearman\")\n\n[1] -1\n\n\n\n\nStep 7: Basic Visualization\n\nlibrary(ggplot2)\n\n# Create data frame\ndata &lt;- data.frame(DM = DM, GH = GH)\n\n# Create scatter plot\nggplot(data, aes(x = DM, y = GH)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"District Magnitude vs Gallagher Index\",\n    x = \"District Magnitude (DM)\",\n    y = \"Gallagher Index (GH)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nOLS Estimation and Goodness-of-Fit Measures\n\n\nStep 1: Calculate OLS Estimates\nUsing previously calculated values:\n\n\\sum(X_i - \\bar{X})(Y_i - \\bar{Y}) = -28.085\n\\sum(X_i - \\bar{X})^2 = 82.5\n\\bar{X} = 6.5\n\\bar{Y} = 15.417\n\nCalculate slope (\\hat{\\beta_1}): \\hat{\\beta_1} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\hat{\\beta_1} = -28,085 ÷ 82,5 = -0,3404\nCalculate intercept (\\hat{\\beta_0}): \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 15,417 - (-0,3404 × 6,5)\n   = 15,417 + 2,2126\n   = 17,6296\nTherefore, the OLS regression equation is: \\hat{Y} = 17.6296 - 0.3404X\n\n\nStep 2: Calculate Fitted Values and Residuals\nComplete table showing all calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i\ne_i = Y_i - \\hat{Y}_i\ne_i^2\n(Y_i - \\bar{Y})^2\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18.2\n16.9488\n1.2512\n1.5655\n7.7451\n2.3404\n\n\n2\n3\n16.7\n16.6084\n0.0916\n0.0084\n1.6461\n1.4241\n\n\n3\n4\n15.8\n16.2680\n-0.4680\n0.2190\n0.1467\n0.7225\n\n\n4\n5\n15.3\n15.9276\n-0.6276\n0.3939\n0.0137\n0.2601\n\n\n5\n6\n15.0\n15.5872\n-0.5872\n0.3448\n0.1739\n0.0289\n\n\n6\n7\n14.8\n15.2468\n-0.4468\n0.1996\n0.3807\n0.0290\n\n\n7\n8\n14.7\n14.9064\n-0.2064\n0.0426\n0.5141\n0.2610\n\n\n8\n9\n14.6\n14.5660\n0.0340\n0.0012\n0.6675\n0.7241\n\n\n9\n10\n14.55\n14.2256\n0.3244\n0.1052\n0.7517\n1.4184\n\n\n10\n11\n14.52\n13.8852\n0.6348\n0.4030\n0.8047\n2.3439\n\n\nSum\n65\n154.17\n154.17\n0\n3.2832\n12.8442\n9.5524\n\n\n\nCalculations for fitted values:\nFor X = 2:\nŶ = 17.6296 + (-0.3404 × 2) = 16.9488\n\nFor X = 3:\nŶ = 17.6296 + (-0.3404 × 3) = 16.6084\n\n[... continue for all values]\n\n\nStep 3: Calculate Goodness-of-Fit Measures\nSum of Squared Errors (SSE): SSE = \\sum e_i^2\nSSE = 3.2832\nSum of Squared Total (SST): SST = \\sum(Y_i - \\bar{Y})^2\nSST = 12.8442\nSum of Squared Regression (SSR): SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\nSSR = 9.5524\nVerify decomposition: SST = SSR + SSE\n12.8442 = 9.5524 + 3.2832 (within rounding error)\nR-squared calculation: R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nR² = 9.5524 ÷ 12.8442\n   = 0.7438\n\n\nStep 4: R Verification\n\n# Fit linear model\nmodel &lt;- lm(GH ~ DM, data = data)\n\n# View summary statistics\nsummary(model)\n\n\nCall:\nlm(formula = GH ~ DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62764 -0.46274 -0.08615  0.26624  1.25109 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept) 17.62976    0.50121  35.174 0.000000000467 ***\nDM          -0.34042    0.07053  -4.827        0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6406 on 8 degrees of freedom\nMultiple R-squared:  0.7444,    Adjusted R-squared:  0.7124 \nF-statistic:  23.3 on 1 and 8 DF,  p-value: 0.00131\n\n# Calculate R-squared manually\nSST &lt;- sum((GH - mean(GH))^2)\nSSE &lt;- sum(residuals(model)^2)\nSSR &lt;- SST - SSE\nR2_manual &lt;- SSR/SST\nR2_manual\n\n[1] 0.7443793\n\n\n\n\nStep 5: Residual Analysis\n\n# Create residual plots\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\nStep 6: Predicted vs Actual Values Plot\n\n# Create predicted vs actual plot\nggplot(data.frame(\n  Actual = GH,\n  Predicted = fitted(model)\n), aes(x = Predicted, y = Actual)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Predicted vs Actual Values\",\n    x = \"Predicted Gallagher Index\",\n    y = \"Actual Gallagher Index\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nLog-Transformed Models\n\n\nStep 1: Data Transformation\nFirst, calculate natural logarithms of variables:\n\n\n\ni\nX_i\nY_i\n\\ln(X_i)\n\\ln(Y_i)\n\n\n\n\n1\n2\n18.2\n0.6931\n2.9014\n\n\n2\n3\n16.7\n1.0986\n2.8154\n\n\n3\n4\n15.8\n1.3863\n2.7600\n\n\n4\n5\n15.3\n1.6094\n2.7278\n\n\n5\n6\n15.0\n1.7918\n2.7081\n\n\n6\n7\n14.8\n1.9459\n2.6946\n\n\n7\n8\n14.7\n2.0794\n2.6878\n\n\n8\n9\n14.6\n2.1972\n2.6810\n\n\n9\n10\n14.55\n2.3026\n2.6777\n\n\n10\n11\n14.52\n2.3979\n2.6757\n\n\n\n\n\nStep 2: Compare Different Model Specifications\nWe estimate three alternative specifications:\n\nLog-linear model: \\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nLinear-log model: Y_i = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\nLog-log model: \\ln(Y_i) = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\n\n\n# Create transformed variables\ndata$log_DM &lt;- log(data$DM)\ndata$log_GH &lt;- log(data$GH)\n\n# Fit models\nmodel_linear &lt;- lm(GH ~ DM, data = data)\nmodel_loglinear &lt;- lm(log_GH ~ DM, data = data)\nmodel_linearlog &lt;- lm(GH ~ log_DM, data = data)\nmodel_loglog &lt;- lm(log_GH ~ log_DM, data = data)\n\n# Compare R-squared values\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Linear\", \"Log-linear\", \"Linear-log\", \"Log-log\"),\n  R_squared = c(\n    summary(model_linear)$r.squared,\n    summary(model_loglinear)$r.squared,\n    summary(model_linearlog)$r.squared,\n    summary(model_loglog)$r.squared\n  )\n)\n\n# Display comparison\nmodels_comparison\n\n       Model R_squared\n1     Linear 0.7443793\n2 Log-linear 0.7670346\n3 Linear-log 0.9141560\n4    Log-log 0.9288088\n\n\n\n\nStep 3: Visual Comparison\n\n# Create plots for each model\np1 &lt;- ggplot(data, aes(x = DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Linear Model\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Log-linear Model\") +\n  theme_minimal()\n\np3 &lt;- ggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Linear-log Model\") +\n  theme_minimal()\n\np4 &lt;- ggplot(data, aes(x = log_DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Log-log Model\") +\n  theme_minimal()\n\n# Arrange plots in a grid\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Residual Analysis for Best Model\nBased on R-squared values, analyze residuals for the best-fitting model:\n\n# Residual plots for best model\npar(mfrow = c(2, 2))\nplot(model_linearlog)\n\n\n\n\n\n\n\n\n\n\nStep 5: Interpretation of Best Model\nThe linear-log model coefficients:\n\nsummary(model_linearlog)\n\n\nCall:\nlm(formula = GH ~ log_DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40702 -0.30207 -0.04907  0.22905  0.60549 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)  19.0223     0.4079   46.64 0.0000000000494 ***\nlog_DM       -2.0599     0.2232   -9.23 0.0000153880425 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3712 on 8 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9034 \nF-statistic: 85.19 on 1 and 8 DF,  p-value: 0.00001539\n\n\nInterpretation: - \\hat{\\beta_0} represents the expected Gallagher Index when ln(DM) = 0 (i.e., when DM = 1) - \\hat{\\beta_1} represents the change in Gallagher Index associated with a one-unit increase in ln(DM)\n\n\nStep 6: Model Predictions\n\n# Create prediction plot for best model\nggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Linear-log Model: Gallagher Index vs ln(District Magnitude)\",\n    x = \"ln(District Magnitude)\",\n    y = \"Gallagher Index\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nStep 7: Elasticity Analysis\nFor the log-log model, coefficients represent elasticities directly. Calculate average elasticity for the linear-log model:\n\n# Calculate elasticity at means\nmean_DM &lt;- mean(data$DM)\nmean_GH &lt;- mean(data$GH)\nbeta1 &lt;- coef(model_linearlog)[2]\nelasticity &lt;- beta1 * (1/mean_GH)\nelasticity\n\n    log_DM \n-0.1336136 \n\n\nThis represents the percentage change in the Gallagher Index for a 1% change in District Magnitude.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.3-understanding-pearson-spearman-and-kendall",
    "href": "correg_en.html#appendix-a.3-understanding-pearson-spearman-and-kendall",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.38 Appendix A.3: Understanding Pearson, Spearman, and Kendall",
    "text": "9.38 Appendix A.3: Understanding Pearson, Spearman, and Kendall\n\nDataset\n\ndata &lt;- data.frame(\n  x = c(2, 4, 5, 3, 8),\n  y = c(3, 5, 4, 4, 7)\n)\n\n\n\nPearson Correlation\n r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}} \n\nStep-by-Step Calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n2\n3\n-2.4\n-1.6\n3.84\n5.76\n2.56\n\n\n2\n4\n5\n-0.4\n0.4\n-0.16\n0.16\n0.16\n\n\n3\n5\n4\n0.6\n-0.6\n-0.36\n0.36\n0.36\n\n\n4\n3\n4\n-1.4\n-0.6\n0.84\n1.96\n0.36\n\n\n5\n8\n7\n3.6\n2.4\n8.64\n12.96\n5.76\n\n\nSum\n22\n23\n0\n0\n12.8\n21.2\n9.2\n\n\n\n\\bar{x} = 4.4 \\bar{y} = 4.6\n r = \\frac{12.8}{\\sqrt{21.2 \\times 9.2}} = \\frac{12.8}{\\sqrt{195.04}} = \\frac{12.8}{13.97} = 0.92 \n\n\n\nSpearman Correlation\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} \n\nStep-by-Step Calculations:\n\n\n\ni\nx_i\ny_i\nRank x_i\nRank y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n3\n1\n1\n0\n0\n\n\n2\n4\n5\n3\n5\n-2\n4\n\n\n3\n5\n4\n4\n2.5\n1.5\n2.25\n\n\n4\n3\n4\n2\n2.5\n-0.5\n0.25\n\n\n5\n8\n7\n5\n4\n1\n1\n\n\nSum\n\n\n\n\n\n7.5\n\n\n\n \\rho = 1 - \\frac{6(7.5)}{5(25-1)} = 1 - \\frac{45}{120} = 0.82 \n\n\n\nKendall’s Tau\n \\tau = \\frac{\\text{number of concordant pairs} - \\text{number of discordant pairs}}{\\frac{1}{2}n(n-1)} \n\nStep-by-Step Calculations:\n\n\n\nPair (i,j)\nx_i,x_j\ny_i,y_j\nx_j-x_i\ny_j-y_i\nResult\n\n\n\n\n(1,2)\n2,4\n3,5\n+2\n+2\nC\n\n\n(1,3)\n2,5\n3,4\n+3\n+1\nC\n\n\n(1,4)\n2,3\n3,4\n+1\n+1\nC\n\n\n(1,5)\n2,8\n3,7\n+6\n+4\nC\n\n\n(2,3)\n4,5\n5,4\n+1\n-1\nD\n\n\n(2,4)\n4,3\n5,4\n-1\n-1\nC\n\n\n(2,5)\n4,8\n5,7\n+4\n+2\nC\n\n\n(3,4)\n5,3\n4,4\n-2\n0\nD\n\n\n(3,5)\n5,8\n4,7\n+3\n+3\nC\n\n\n(4,5)\n3,8\n4,7\n+5\n+3\nC\n\n\n\nNumber of concordant pairs = 8 Number of discordant pairs = 2  \\tau = \\frac{8-2}{10} = 0.74 \n\n\n\nVerification in R\n\ncat(\"Pearson:\", round(cor(data$x, data$y, method=\"pearson\"), 2), \"\\n\")\n\nPearson: 0.92 \n\ncat(\"Spearman:\", round(cor(data$x, data$y, method=\"spearman\"), 2), \"\\n\")\n\nSpearman: 0.82 \n\ncat(\"Kendall:\", round(cor(data$x, data$y, method=\"kendall\"), 2), \"\\n\")\n\nKendall: 0.74 \n\n\n\n\nInterpretation of Results\n\nPearson Correlation (r = 0.92)\n\nStrong positive linear correlation\nIndicates a very strong linear relationship between variables\n\nSpearman Correlation (ρ = 0.82)\n\nAlso strong positive correlation\nSlightly lower than Pearson’s, suggesting some deviations from monotonicity\n\nKendall’s Tau (τ = 0.74)\n\nLowest of the three values, but still indicates strong association\nMore robust to outliers\n\n\n\n\nComparison of Measures\n\nDifferences in Values:\n\nPearson (0.92) - highest value, strong linearity\nSpearman (0.82) - considers only ranking\nKendall (0.74) - most conservative measure\n\nPractical Application:\n\nAll measures confirm strong positive association\nDifferences between measures indicate slight deviations from perfect linearity\nKendall provides the most conservative estimate of relationship strength\n\n\n\n\nExercises\n\nChange y[3] from 4 to 6 and recalculate all three correlations\nAdd an outlier (x=10, y=2) and recalculate correlations\nCompare which measure is most sensitive to changes in the data\n\n\n\nKey Points to Remember\n\nPearson Correlation:\n\nMeasures linear relationship\nMost sensitive to outliers\nRequires interval or ratio data\n\nSpearman Correlation:\n\nMeasures monotonic relationship\nLess sensitive to outliers\nWorks with ordinal data\n\nKendall’s Tau:\n\nMeasures ordinal association\nMost robust to outliers\nBest for small samples and tied ranks",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-b-bias-in-ols-estimation-with-endogenous-regressors",
    "href": "correg_en.html#appendix-b-bias-in-ols-estimation-with-endogenous-regressors",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.39 Appendix B: Bias in OLS Estimation with Endogenous Regressors",
    "text": "9.39 Appendix B: Bias in OLS Estimation with Endogenous Regressors\nIn this tutorial, we will explore the bias in Ordinary Least Squares (OLS) estimation when the error term is correlated with the explanatory variable, a situation known as endogeneity. We will first derive the bias mathematically and then illustrate it using a simulated dataset in R.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#theoretical-derivation",
    "href": "correg_en.html#theoretical-derivation",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.40 Theoretical Derivation",
    "text": "9.40 Theoretical Derivation\nConsider a data generating process (DGP) where the true relationship between x and y is:\n y = 2x + e \nHowever, there is an endogeneity problem because the error term e is correlated with x in the following way:\n e = 1x + u \nwhere u is an independent error term.\nIf we estimate the simple linear model y = \\hat{\\beta_0} + \\hat{\\beta_1}x + \\varepsilon using OLS, the OLS estimator of \\hat{\\beta_1} will be biased due to the endogeneity issue.\nTo understand the bias, let’s derive the expected value of the OLS estimator \\hat{\\beta}_1:\n\\begin{align*}\nE[\\hat{\\beta}_1] &= E[(X'X)^{-1}X'y] \\\\\n                 &= E[(X'X)^{-1}X'(2x + 1x + u)] \\\\\n                 &= E[(X'X)^{-1}X'(3x + u)] \\\\\n                 &= 3 + E[(X'X)^{-1}X'u]\n\\end{align*}\nIf the error term u is uncorrelated with x, then E[(X'X)^{-1}X'u] = 0, and the OLS estimator would be unbiased: E[\\hat{\\beta}_1] = 3. However, in this case, the original error term e is correlated with x, so u is also likely to be correlated with x.\nAssuming E[(X'X)^{-1}X'u] \\neq 0, the OLS estimator will be biased:\n\\begin{align*}\n\\text{Bias}(\\hat{\\beta}_1) &= E[\\hat{\\beta}_1] - \\beta_{1,\\text{true}} \\\\\n                           &= 3 + E[(X'X)^{-1}X'u] - 2 \\\\\n                           &= 1 + E[(X'X)^{-1}X'u]\n\\end{align*}\nThe direction and magnitude of the bias will depend on the correlation between x and u. If x and u are positively correlated, the bias will be positive, and the OLS estimator will overestimate the true coefficient. Conversely, if x and u are negatively correlated, the bias will be negative, and the OLS estimator will underestimate the true coefficient.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#simulation-in-r",
    "href": "correg_en.html#simulation-in-r",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.41 Simulation in R",
    "text": "9.41 Simulation in R\nLet’s create a simple dataset with 10 observations where x is in the interval 1:10, and generate y values based on the given DGP: y = 2x + e, where e = 1x + u, and u is a random error term.\n\nset.seed(123)  # for reproducibility\nx &lt;- 1:10\nu &lt;- rnorm(10, mean = 0, sd = 1)\ne &lt;- 1*x + u\n# e &lt;- 1*x\ny &lt;- 2*x + e\n\n# Generate the data frame\ndata &lt;- data.frame(x = x, y = y)\n\n# Estimate the OLS model\nmodel &lt;- lm(y ~ x, data = data)\n\n# Print the model summary\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1348 -0.5624 -0.1393  0.3854  1.6814 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)   0.5255     0.6673   0.787         0.454    \nx             2.9180     0.1075  27.134 0.00000000367 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9768 on 8 degrees of freedom\nMultiple R-squared:  0.9893,    Adjusted R-squared:  0.9879 \nF-statistic: 736.3 on 1 and 8 DF,  p-value: 0.000000003666\n\n\nIn this example, the true relationship is y = 2x + e, where e = 1x + u. However, when we estimate the OLS model, we get:\n \\hat{y} = 0.18376 + 3.05874x \nThe estimated coefficient for x is 3.05874, which is biased upward from the true value of 2. This bias is due to the correlation between the error term e and the explanatory variable x.\nTo visualize the bias using ggplot2, we can plot the true relationship (y = 2x) and the estimated OLS relationship:\n\nlibrary(ggplot2)\n\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 2, color = \"blue\", linewidth = 1, linetype = \"dashed\") +\n  geom_abline(intercept = coef(model)[1], slope = coef(model)[2], color = \"red\", linewidth = 1) +\n  labs(title = \"True vs. Estimated Relationship\", x = \"x\", y = \"y\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  scale_color_manual(name = \"Lines\", values = c(\"blue\", \"red\"), \n                     labels = c(\"True\", \"OLS\"))\n\n\n\n\n\n\n\n\nThe plot will show that the estimated OLS line (red) is steeper than the true relationship line (blue), illustrating the upward bias in the estimated coefficient.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#conclusion-1",
    "href": "correg_en.html#conclusion-1",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.42 Conclusion",
    "text": "9.42 Conclusion\nIn summary, when the error term is correlated with the explanatory variable (endogeneity), the OLS estimator will be biased. The direction and magnitude of the bias depend on the nature of the correlation between the error term and the explanatory variable. This tutorial demonstrated the bias both mathematically and through a simulated example in R, using ggplot2 for visualization.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-c.-ols---worked-examples",
    "href": "correg_en.html#appendix-c.-ols---worked-examples",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.43 Appendix C. OLS - Worked Examples",
    "text": "9.43 Appendix C. OLS - Worked Examples",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#example-1.-practice-hours-vs.-skill-rating-visual-guide-to-ols-and-r-squared",
    "href": "correg_en.html#example-1.-practice-hours-vs.-skill-rating-visual-guide-to-ols-and-r-squared",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.44 Example 1. Practice Hours vs. Skill Rating: Visual Guide to OLS and R-squared",
    "text": "9.44 Example 1. Practice Hours vs. Skill Rating: Visual Guide to OLS and R-squared\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\n\n# Our simple dataset\npractice &lt;- c(1, 2, 3, 4, 5, 6)\nskill &lt;- c(3, 7, 5, 8, 10, 9)\n\n# Create data frame\ndata &lt;- data.frame(\n  Student = 1:6,\n  Practice = practice,\n  Skill = skill\n)\n\nprint(data)\n\n  Student Practice Skill\n1       1        1     3\n2       2        2     7\n3       3        3     5\n4       4        4     8\n5       5        5    10\n6       6        6     9\n\n\n\nManual Calculations\n\nStep 1: Calculate the Means\nThe mean is the average value, calculated by summing all observations and dividing by the number of observations.\nMean of Practice Hours (X̄):\n\\bar{X} = \\frac{\\sum_{i=1}^{n} X_i}{n} = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5\nMean of Skill Ratings (Ȳ):\n\\bar{Y} = \\frac{\\sum_{i=1}^{n} Y_i}{n} = \\frac{3 + 7 + 5 + 8 + 10 + 9}{6} = \\frac{42}{6} = 7\n\n# Calculate means\nmean_x &lt;- mean(practice)\nmean_y &lt;- mean(skill)\n\ncat(\"Mean Practice Hours:\", mean_x, \"\\n\")\n\nMean Practice Hours: 3.5 \n\ncat(\"Mean Skill Rating:\", mean_y, \"\\n\")\n\nMean Skill Rating: 7 \n\n\n\n\nStep 2: Calculate Variance and Standard Deviation\nVariance measures how spread out the data is from the mean. We use the sample variance formula (dividing by n-1).\nVariance of X (Practice Hours):\nFirst, calculate deviations from the mean (X_i - \\bar{X}):\n\n\n\nStudent\nX_i\nX_i - \\bar{X}\n(X_i - \\bar{X})^2\n\n\n\n\n1\n1\n1 - 3.5 = -2.5\n(-2.5)^2 = 6.25\n\n\n2\n2\n2 - 3.5 = -1.5\n(-1.5)^2 = 2.25\n\n\n3\n3\n3 - 3.5 = -0.5\n(-0.5)^2 = 0.25\n\n\n4\n4\n4 - 3.5 = 0.5\n(0.5)^2 = 0.25\n\n\n5\n5\n5 - 3.5 = 1.5\n(1.5)^2 = 2.25\n\n\n6\n6\n6 - 3.5 = 2.5\n(2.5)^2 = 6.25\n\n\nSum\n\n\n17.5\n\n\n\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1} = \\frac{17.5}{5} = 3.5\ns_X = \\sqrt{3.5} = 1.871\nVariance of Y (Skill Ratings):\n\n\n\nStudent\nY_i\nY_i - \\bar{Y}\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n3\n3 - 7 = -4\n(-4)^2 = 16\n\n\n2\n7\n7 - 7 = 0\n(0)^2 = 0\n\n\n3\n5\n5 - 7 = -2\n(-2)^2 = 4\n\n\n4\n8\n8 - 7 = 1\n(1)^2 = 1\n\n\n5\n10\n10 - 7 = 3\n(3)^2 = 9\n\n\n6\n9\n9 - 7 = 2\n(2)^2 = 4\n\n\nSum\n\n\n34\n\n\n\ns^2_Y = \\frac{\\sum(Y_i - \\bar{Y})^2}{n-1} = \\frac{34}{5} = 6.8\ns_Y = \\sqrt{6.8} = 2.608\n\n# Verify variance and standard deviation\ncat(\"Variance of Practice:\", var(practice), \"\\n\")\n\nVariance of Practice: 3.5 \n\ncat(\"SD of Practice:\", sd(practice), \"\\n\")\n\nSD of Practice: 1.870829 \n\ncat(\"Variance of Skill:\", var(skill), \"\\n\")\n\nVariance of Skill: 6.8 \n\ncat(\"SD of Skill:\", sd(skill), \"\\n\")\n\nSD of Skill: 2.607681 \n\n\n\n\nStep 3: Calculate Covariance\nCovariance measures how two variables vary together. A positive covariance indicates that as one variable increases, the other tends to increase.\ns_{XY} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nLet’s calculate the products for each observation:\n\n\n\n\n\n\n\n\n\nStudent\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n\n\n\n\n1\n-2.5\n-4\n(-2.5) \\times (-4) = 10.0\n\n\n2\n-1.5\n0\n(-1.5) \\times (0) = 0.0\n\n\n3\n-0.5\n-2\n(-0.5) \\times (-2) = 1.0\n\n\n4\n0.5\n1\n(0.5) \\times (1) = 0.5\n\n\n5\n1.5\n3\n(1.5) \\times (3) = 4.5\n\n\n6\n2.5\n2\n(2.5) \\times (2) = 5.0\n\n\nSum\n\n\n21.0\n\n\n\ns_{XY} = \\frac{21.0}{5} = 4.2\n\n# Verify covariance\ncat(\"Covariance:\", cov(practice, skill), \"\\n\")\n\nCovariance: 4.2 \n\n\n\n\nStep 4: Calculate Pearson Correlation Coefficient\nThe correlation coefficient standardizes the covariance to a scale from -1 to +1:\nr = \\frac{s_{XY}}{s_X \\cdot s_Y} = \\frac{4.2}{1.871 \\times 2.608} = \\frac{4.2}{4.879} = 0.861\nThis gives us a correlation of 0.861, indicating a strong positive relationship between practice hours and skill rating.\n\n# Verify correlation\ncat(\"Correlation:\", cor(practice, skill), \"\\n\")\n\nCorrelation: 0.8609161 \n\n\n\n\nStep 5: Calculate OLS Regression Coefficients\nThe Ordinary Least Squares (OLS) method finds the values of \\beta_0 and \\beta_1 that minimize the sum of squared errors.\nSlope estimator:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\nUsing our calculated values:\n\\hat{\\beta_1} = \\frac{4.2}{3.5} = 1.2\nIntercept estimator:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 7 - (1.2 \\times 3.5) = 7 - 4.2 = 2.8\nOur regression equation is:\n\\hat{Y} = 2.8 + 1.2X\nThis means:\n\nWhen practice hours = 0, predicted skill = 2.8\nFor each 1-hour increase in practice, skill rating increases by 1.2 points\n\n\n# Fit the model\nmodel &lt;- lm(skill ~ practice)\ncoef_model &lt;- coef(model)\n\ncat(\"Intercept (β₀):\", coef_model[1], \"\\n\")\n\nIntercept (β₀): 2.8 \n\ncat(\"Slope (β₁):\", coef_model[2], \"\\n\")\n\nSlope (β₁): 1.2 \n\n# Get predictions\ndata$predicted &lt;- predict(model)\ndata$residual &lt;- residuals(model)\n\n\n\nStep 6: Calculate Predicted Values and Residuals\nUsing \\hat{Y} = 2.8 + 1.2X:\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 2.8 + 1.2X_i\nResidual (Y_i - \\hat{Y}_i)\n\n\n\n\n1\n1\n3\n2.8 + 1.2(1) = 4.0\n3 - 4.0 = -1.0\n\n\n2\n2\n7\n2.8 + 1.2(2) = 5.2\n7 - 5.2 = 1.8\n\n\n3\n3\n5\n2.8 + 1.2(3) = 6.4\n5 - 6.4 = -1.4\n\n\n4\n4\n8\n2.8 + 1.2(4) = 7.6\n8 - 7.6 = 0.4\n\n\n5\n5\n10\n2.8 + 1.2(5) = 8.8\n10 - 8.8 = 1.2\n\n\n6\n6\n9\n2.8 + 1.2(6) = 10.0\n9 - 10.0 = -1.0\n\n\n\n\n\nStep 7: Calculate Sum of Squares\nSST (Total Sum of Squares) - Total variation in Y:\nSST = \\sum(Y_i - \\bar{Y})^2\nFrom our earlier variance calculation:\nSST = (n-1) \\times s^2_Y = 5 \\times 6.8 = 34\nSSE (Sum of Squared Errors) - Unexplained variation:\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n\n\n\n\n\n\n\n\n\n\nStudent\nY_i\n\\hat{Y}_i\n(Y_i - \\hat{Y}_i)\n(Y_i - \\hat{Y}_i)^2\n\n\n\n\n1\n3\n4.0\n-1.0\n1.00\n\n\n2\n7\n5.2\n1.8\n3.24\n\n\n3\n5\n6.4\n-1.4\n1.96\n\n\n4\n8\n7.6\n0.4\n0.16\n\n\n5\n10\n8.8\n1.2\n1.44\n\n\n6\n9\n10.0\n-1.0\n1.00\n\n\nSum\n\n\n\n8.80\n\n\n\nSSE = 8.80\nSSR (Sum of Squares Regression) - Explained variation:\nSSR = SST - SSE = 34 - 8.80 = 25.20\n\n\nStep 8: Calculate R-squared\nR-squared tells us what proportion of the total variation in Y is explained by our model:\nR^2 = \\frac{SSR}{SST} = \\frac{25.20}{34} = 0.741\nAlternative formula:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{8.80}{34} = 1 - 0.259 = 0.741\nOr simply:\nR^2 = r^2 = (0.861)^2 = 0.741\nThis means our model explains 74.1% of the variation in skill ratings!\n\n# Verify sum of squares and R-squared\nSST &lt;- sum((skill - mean_y)^2)\nSSR &lt;- sum((data$predicted - mean_y)^2)\nSSE &lt;- sum(data$residual^2)\nr_squared &lt;- SSR / SST\n\ncat(\"SST (Total):\", SST, \"\\n\")\n\nSST (Total): 34 \n\ncat(\"SSR (Explained):\", SSR, \"\\n\")\n\nSSR (Explained): 25.2 \n\ncat(\"SSE (Unexplained):\", SSE, \"\\n\")\n\nSSE (Unexplained): 8.8 \n\ncat(\"R-squared:\", r_squared, \"\\n\")\n\nR-squared: 0.7411765 \n\ncat(\"R-squared (from correlation):\", cor(practice, skill)^2, \"\\n\")\n\nR-squared (from correlation): 0.7411765 \n\n\n\n\n\nVisualization 1: The OLS Best-Fit Line\nThis plot shows how OLS minimizes the sum of squared residuals (vertical distances from points to the line).\n\nggplot(data, aes(x = Practice, y = Skill)) +\n  geom_hline(yintercept = mean_y, linetype = \"dashed\", \n             color = \"gray50\", linewidth = 0.8, alpha = 0.7) +\n  geom_segment(aes(xend = Practice, yend = predicted), \n               color = \"red\", linetype = \"dotted\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1.2) +\n  geom_point(size = 5, color = \"darkblue\") +\n  geom_point(aes(y = predicted), size = 3, color = \"blue\", shape = 17) +\n  annotate(\"text\", x = 1.5, y = mean_y + 0.3, \n           label = paste0(\"Mean (ȳ = \", mean_y, \")\"), \n           color = \"gray30\", size = 4) +\n  annotate(\"text\", x = 5, y = 4, \n           label = \"Residuals (errors)\\nOLS minimizes Σ(residuals²)\", \n           color = \"red\", size = 3.5, hjust = 0) +\n  annotate(\"text\", x = 2, y = 10.5, \n           label = paste0(\"ŷ = \", round(coef_model[1], 1), \n                         \" + \", round(coef_model[2], 1), \"x\"), \n           color = \"blue\", size = 5, fontface = \"bold\") +\n  labs(\n    title = \"OLS Regression: Minimizing Squared Residuals\",\n    subtitle = \"Blue triangles are predicted values; red lines show residuals\",\n    x = \"Practice Hours\",\n    y = \"Skill Rating\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKey Insight: OLS finds the unique line that makes the sum of squared red distances as small as possible!\n\n\nVisualization 2: Variance Decomposition (SST = SSR + SSE)\nThis shows how total variation is split into explained and unexplained components.\n\n# Create the decomposition plot\nggplot(data, aes(x = Practice)) +\n  # Total deviation (SST)\n  geom_segment(aes(y = mean_y, yend = Skill, xend = Practice), \n               color = \"purple\", linewidth = 1.2, alpha = 0.6,\n               arrow = arrow(length = unit(0.15, \"inches\"))) +\n  \n  # Explained deviation (SSR)\n  geom_segment(aes(y = mean_y, yend = predicted, xend = Practice), \n               color = \"green\", linewidth = 1.5, alpha = 0.8) +\n  \n  # Unexplained deviation (SSE)\n  geom_segment(aes(y = predicted, yend = Skill, xend = Practice), \n               color = \"red\", linewidth = 1.2, alpha = 0.8,\n               linetype = \"dashed\") +\n  \n  # Mean line\n  geom_hline(yintercept = mean_y, linetype = \"solid\", \n             color = \"gray40\", linewidth = 1) +\n  \n  # Regression line\n  geom_smooth(aes(y = Skill), method = \"lm\", se = FALSE, \n              color = \"blue\", linewidth = 1) +\n  \n  # Points\n  geom_point(aes(y = Skill), size = 5, color = \"darkblue\") +\n  geom_point(aes(y = predicted), size = 3, color = \"blue\", shape = 15) +\n  \n  # Annotations\n  annotate(\"text\", x = 6.5, y = mean_y, \n           label = \"Mean\", color = \"gray40\", size = 4, hjust = 0) +\n  annotate(\"text\", x = 6.5, y = 9.5, \n           label = \"Regression Line\", color = \"blue\", size = 4, hjust = 0) +\n  \n  # Legend\n  annotate(\"segment\", x = 0.5, xend = 0.5, y = 2, yend = 3.5,\n           color = \"purple\", linewidth = 1.2, \n           arrow = arrow(length = unit(0.12, \"inches\"))) +\n  annotate(\"text\", x = 0.7, y = 2.75, \n           label = \"Total (SST)\", color = \"purple\", \n           size = 3.5, hjust = 0, fontface = \"bold\") +\n  \n  annotate(\"segment\", x = 0.5, xend = 0.5, y = 4.5, yend = 5.5,\n           color = \"green\", linewidth = 1.5) +\n  annotate(\"text\", x = 0.7, y = 5, \n           label = \"Explained (SSR)\", color = \"green\", \n           size = 3.5, hjust = 0, fontface = \"bold\") +\n  \n  annotate(\"segment\", x = 0.5, xend = 0.5, y = 6.5, yend = 7.5,\n           color = \"red\", linewidth = 1.2, linetype = \"dashed\") +\n  annotate(\"text\", x = 0.7, y = 7, \n           label = \"Unexplained (SSE)\", color = \"red\", \n           size = 3.5, hjust = 0, fontface = \"bold\") +\n  \n  labs(\n    title = \"Variance Decomposition: SST = SSR + SSE\",\n    subtitle = \"Purple = total deviation | Green = explained by model | Red = residual error\",\n    x = \"Practice Hours\",\n    y = \"Skill Rating\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 14)) +\n  coord_cartesian(xlim = c(0.3, 7.5), ylim = c(2, 11))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nMathematical Decomposition:\nFor each observation: (Y_i - \\bar{Y})^2 = (\\hat{Y}_i - \\bar{Y})^2 + (Y_i - \\hat{Y}_i)^2\n\nPurple arrows: Total deviation from mean = (Y_i - \\bar{Y})\nGreen bars: Model’s explanation = (\\hat{Y}_i - \\bar{Y})\nRed dashed: What’s left over = (Y_i - \\hat{Y}_i)\n\n\n\nVisualization 3: R-squared as a Proportion\n\n# Calculate sum of squares\nSST &lt;- sum((skill - mean_y)^2)\nSSR &lt;- sum((data$predicted - mean_y)^2)\nSSE &lt;- sum(data$residual^2)\nr_squared &lt;- SSR / SST\n\n# Create bar chart data\nss_data &lt;- data.frame(\n  Component = c(\"Total (SST)\", \"Explained (SSR)\", \"Unexplained (SSE)\"),\n  Value = c(SST, SSR, SSE),\n  Color = c(\"purple\", \"green\", \"red\")\n)\n\np1 &lt;- ggplot(ss_data, aes(x = Component, y = Value, fill = Component)) +\n  geom_bar(stat = \"identity\", alpha = 0.7, color = \"black\") +\n  geom_text(aes(label = round(Value, 2)), vjust = -0.5, size = 5, fontface = \"bold\") +\n  scale_fill_manual(values = c(\"green\", \"red\", \"purple\")) +\n  labs(\n    title = \"Sum of Squares Breakdown\",\n    y = \"Sum of Squares\",\n    x = \"\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"))\n\n# Create proportion visualization\nprop_data &lt;- data.frame(\n  Component = c(\"Explained\\n(SSR)\", \"Unexplained\\n(SSE)\"),\n  Proportion = c(SSR/SST, SSE/SST),\n  Percentage = c(SSR/SST * 100, SSE/SST * 100)\n)\n\np2 &lt;- ggplot(prop_data, aes(x = \"\", y = Proportion, fill = Component)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\", linewidth = 2) +\n  coord_polar(theta = \"y\") +\n  scale_fill_manual(values = c(\"green3\", \"red3\")) +\n  geom_text(aes(label = paste0(round(Percentage, 1), \"%\")), \n            position = position_stack(vjust = 0.5), \n            size = 6, fontface = \"bold\", color = \"white\") +\n  labs(title = paste0(\"R² = \", round(r_squared, 3))) +\n  theme_void(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n        legend.position = \"bottom\",\n        legend.title = element_blank())\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\nR-squared Formulas:\nR^2 = \\frac{SSR}{SST} = \\frac{25.20}{34} = 0.74\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{8.80}{34} = 0.74\nInterpretation: Our model explains 74% of the variation in skill ratings!\n\n\nVisualization 4: R² as Correlation Between Deviations\nThis shows the geometric interpretation of R²: how well predicted deviations from the mean match actual deviations.\n\n# Calculate deviations\ndata$actual_dev &lt;- skill - mean_y\ndata$predicted_dev &lt;- data$predicted - mean_y\n\n# Side-by-side comparison\np1 &lt;- ggplot(data, aes(x = Practice)) +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.8) +\n  geom_segment(aes(y = 0, yend = actual_dev, xend = Practice), \n               color = \"purple\", linewidth = 2, alpha = 0.7,\n               arrow = arrow(length = unit(0.15, \"inches\"))) +\n  geom_point(aes(y = actual_dev), size = 4, color = \"purple\") +\n  labs(\n    title = \"Actual Deviations from Mean\",\n    subtitle = expression(Y[i] - bar(Y)),\n    x = \"Practice Hours\",\n    y = \"Deviation from Mean\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(data, aes(x = Practice)) +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.8) +\n  geom_segment(aes(y = 0, yend = predicted_dev, xend = Practice), \n               color = \"green\", linewidth = 2, alpha = 0.7,\n               arrow = arrow(length = unit(0.15, \"inches\"))) +\n  geom_point(aes(y = predicted_dev), size = 4, color = \"green\") +\n  labs(\n    title = \"Predicted Deviations from Mean\",\n    subtitle = expression(hat(Y)[i] - bar(Y)),\n    x = \"Practice Hours\",\n    y = \"Deviation from Mean\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(plot.title = element_text(face = \"bold\"))\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nVisualization 5: Scatterplot of Deviations (R² Interpretation)\n\nggplot(data, aes(x = predicted_dev, y = actual_dev)) +\n  geom_abline(slope = 1, intercept = 0, \n              linetype = \"dashed\", color = \"gray50\", linewidth = 1) +\n  geom_point(size = 5, color = \"darkblue\", alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\", \n              fill = \"lightblue\", alpha = 0.3) +\n  annotate(\"text\", x = -3, y = 2.5, \n           label = paste0(\"If perfect fit:\\nall points on this line\\n(R² = 1)\"), \n           color = \"gray40\", size = 3.5, hjust = 0) +\n  annotate(\"text\", x = 1.5, y = -3, \n           label = paste0(\"Correlation = \", round(sqrt(r_squared), 3), \n                         \"\\nR² = \", round(r_squared, 3)), \n           color = \"blue\", size = 5, fontface = \"bold\") +\n  labs(\n    title = \"R² Measures How Well Predicted Deviations Match Actual Deviations\",\n    subtitle = \"Each point compares model's prediction to reality (both relative to mean)\",\n    x = expression(paste(\"Predicted Deviation: \", hat(Y)[i] - bar(Y))),\n    y = expression(paste(\"Actual Deviation: \", Y[i] - bar(Y)))\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 13))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKey Insight:\nR² is literally the square of the correlation between:\n\nWhat the model predicts (relative to mean)\nWhat actually happened (relative to mean)\n\nIf R^2 = 1: all points fall exactly on the diagonal (perfect predictions)\nIf R^2 = 0: no relationship between predicted and actual deviations\n\n\nSummary Table\n\n# Create summary table\nsummary_stats &lt;- data.frame(\n  Statistic = c(\"Sample Size (n)\", \n                \"Mean Practice (X̄)\", \n                \"Mean Skill (Ȳ)\",\n                \"Correlation (r)\",\n                \"Intercept (β₀)\",\n                \"Slope (β₁)\",\n                \"R-squared\",\n                \"SST (Total)\",\n                \"SSR (Explained)\",\n                \"SSE (Unexplained)\"),\n  Value = c(6,\n            round(mean_x, 2),\n            round(mean_y, 2),\n            round(cor(practice, skill), 3),\n            round(coef_model[1], 2),\n            round(coef_model[2], 2),\n            round(r_squared, 3),\n            round(SST, 2),\n            round(SSR, 2),\n            round(SSE, 2))\n)\n\nknitr::kable(summary_stats, align = c(\"l\", \"r\"))\n\n\n\n\nStatistic\nValue\n\n\n\n\nSample Size (n)\n6.000\n\n\nMean Practice (X̄)\n3.500\n\n\nMean Skill (Ȳ)\n7.000\n\n\nCorrelation (r)\n0.861\n\n\nIntercept (β₀)\n2.800\n\n\nSlope (β₁)\n1.200\n\n\nR-squared\n0.741\n\n\nSST (Total)\n34.000\n\n\nSSR (Explained)\n25.200\n\n\nSSE (Unexplained)\n8.800\n\n\n\n\n\n\n\nKey Takeaways\nOLS Regression:\n\nFinds the line that minimizes the sum of squared residuals\nProduces unbiased estimates with minimum variance (BLUE)\n\nR-squared (0.74) means:\n\n74% of variation in skill is explained by practice hours\nThe correlation between predicted and actual deviations is \\sqrt{0.74} = 0.86\nSSR is 74% of SST; SSE is 26% of SST\n\nGeometric Interpretation:\n\nTotal variation = distance of each point from the mean\nModel captures 74% of these distances through the regression line\nRemaining 26% is unexplained (residuals)\n\nPractical Implication:\nEach additional hour of practice increases expected skill by 1.2 points, and this relationship explains most (but not all) of the variation we observe!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#example-2.-voter-participation-and-economic-prosperity",
    "href": "correg_en.html#example-2.-voter-participation-and-economic-prosperity",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.45 Example 2. Voter Participation and Economic Prosperity",
    "text": "9.45 Example 2. Voter Participation and Economic Prosperity\nAnalysis of the relationship between economic prosperity and voter turnout in Amsterdam districts based on data from the 2022 local elections.\n\nData\nThe sample includes five representative districts:\n\n\n\nDistrict\nIncome (€000s)\nTurnout (%)\n\n\n\n\nNoord\n50\n60\n\n\nZuid\n45\n56\n\n\nCentrum\n56\n70\n\n\nWest\n40\n50\n\n\nOost\n60\n75\n\n\n\n\n# Load libraries\nlibrary(tidyverse)\n\n# Create dataset\ndata &lt;- data.frame(\n  district = c(\"Noord\", \"Zuid\", \"Centrum\", \"West\", \"Oost\"),\n  income = c(50, 45, 56, 40, 60),\n  turnout = c(60, 56, 70, 50, 75)\n)\n\n# View data\ndata\n\n  district income turnout\n1    Noord     50      60\n2     Zuid     45      56\n3  Centrum     56      70\n4     West     40      50\n5     Oost     60      75\n\n\n\n\nPart 1: Descriptive Statistics\n\n# Statistics for income\ncat(\"INCOME (€000s):\\n\")\n\nINCOME (€000s):\n\ncat(\"Mean:\", mean(data$income), \"\\n\")\n\nMean: 50.2 \n\ncat(\"Median:\", median(data$income), \"\\n\")\n\nMedian: 50 \n\ncat(\"Standard deviation:\", round(sd(data$income), 2), \"\\n\")\n\nStandard deviation: 8.07 \n\ncat(\"Range:\", min(data$income), \"-\", max(data$income), \"\\n\\n\")\n\nRange: 40 - 60 \n\n# Statistics for turnout\ncat(\"TURNOUT (%):\\n\")\n\nTURNOUT (%):\n\ncat(\"Mean:\", mean(data$turnout), \"\\n\")\n\nMean: 62.2 \n\ncat(\"Median:\", median(data$turnout), \"\\n\")\n\nMedian: 60 \n\ncat(\"Standard deviation:\", round(sd(data$turnout), 2), \"\\n\")\n\nStandard deviation: 10.21 \n\ncat(\"Range:\", min(data$turnout), \"-\", max(data$turnout))\n\nRange: 50 - 75\n\n\n\n\nPart 2: Correlation Analysis\n\n# Pearson correlation test\ncorrelation &lt;- cor.test(data$income, data$turnout)\n\ncat(\"Correlation coefficient (r):\", round(correlation$estimate, 3), \"\\n\")\n\nCorrelation coefficient (r): 0.994 \n\ncat(\"P-value:\", round(correlation$p.value, 3), \"\\n\")\n\nP-value: 0.001 \n\n# Interpretation\nif (correlation$p.value &lt; 0.05) {\n  cat(\"Result is statistically significant (p &lt; 0.05)\")\n} else {\n  cat(\"Result is not statistically significant (p ≥ 0.05)\")\n}\n\nResult is statistically significant (p &lt; 0.05)\n\n\n\n\nPart 3: Linear Regression Model\n\n# Fit the model\nmodel &lt;- lm(turnout ~ income, data = data)\n\n# Basic model information\nsummary(model)\n\n\nCall:\nlm(formula = turnout ~ income, data = data)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \nincome       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n# R-squared - how well the model explains the data\ncat(\"\\nThe model explains\", round(summary(model)$r.squared * 100, 1), \"% of the variance in turnout\")\n\n\nThe model explains 98.9 % of the variance in turnout\n\n\n\n\nPart 4: Visualization\n\n# Scatter plot with regression line\nggplot(data, aes(x = income, y = turnout)) +\n  geom_point(size = 4, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\", alpha = 0.3) +\n  geom_text(aes(label = district), vjust = -1, size = 4) +\n  labs(\n    title = \"Income vs Voter Turnout\",\n    subtitle = paste(\"r =\", round(correlation$estimate, 3)),\n    x = \"Average Income (€000s)\",\n    y = \"Voter Turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nPart 5: Interpreting Results\n\n# Extract coefficients\ncoefficients &lt;- coef(model)\nintercept &lt;- round(coefficients[1], 1)\nslope &lt;- round(coefficients[2], 2)\n\ncat(\"REGRESSION EQUATION:\\n\")\n\nREGRESSION EQUATION:\n\ncat(\"Turnout =\", intercept, \"+\", slope, \"× Income\\n\\n\")\n\nTurnout = -0.9 + 1.26 × Income\n\ncat(\"INTERPRETATION:\\n\")\n\nINTERPRETATION:\n\ncat(\"• An increase of €1000 in income increases turnout by\", slope, \"percentage points\\n\")\n\n• An increase of €1000 in income increases turnout by 1.26 percentage points\n\ncat(\"• The correlation is\", ifelse(correlation$estimate &gt; 0, \"positive\", \"negative\"), \n    \"and\", ifelse(abs(correlation$estimate) &gt; 0.7, \"strong\", \n               ifelse(abs(correlation$estimate) &gt; 0.5, \"moderate\", \"weak\")))\n\n• The correlation is positive and strong\n\n\n\n\nConclusions\nKey findings: - There is a strong positive correlation (r = 0.994) between income and voter turnout - The model explains 98.9% of the variance in the data - Districts with higher incomes have higher voter turnout\nPractical application: Results suggest that efforts to increase voter turnout should particularly focus on lower-income districts.\nLimitations and Caveats:\n⚠️ Critical limitations:\n\nVery small sample (n=5) severely limits generalizability\nLow statistical power - risk of Type II errors\nLack of control for confounding variables (age, education, population density)\nPossible spurious correlation - additional control variables needed\n\nRecommendations for future research:\n\nExpand sample to all Amsterdam districts\nInclude demographic and socioeconomic variables\nAnalyze longitudinal data from multiple election cycles",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#example-3.-descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "href": "correg_en.html#example-3.-descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.46 Example 3. Descriptive Statistics and OLS Example - Income and Voter Turnout",
    "text": "9.46 Example 3. Descriptive Statistics and OLS Example - Income and Voter Turnout\nBackground\nIn preparation for the 2024 municipal elections, the Amsterdam Electoral Commission conducted research on voter participation patterns across different city neighborhoods. A key question emerged:\nDoes economic prosperity of a neighborhood correlate with civic engagement, specifically voter turnout?\nData Collection\nSample: 5 representative neighborhoods in Amsterdam\nTime Period: Data from the 2022 municipal elections\nVariables:\n\nIncome: Average annual household income per capita (thousands €)\nTurnout: Percentage of registered voters who voted in the election\n\n\nInitial R Output for Reference\n\n# Data\nincome &lt;- c(50, 45, 56, 40, 60)  # thousands €\nturnout &lt;- c(60, 56, 70, 50, 75) # %\n\n# Full model check\nmodel &lt;- lm(turnout ~ income)\nsummary(model)\n\n\nCall:\nlm(formula = turnout ~ income)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \nincome       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n\n\n\nDispersion Measures\nMeans:\n\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n} = \\frac{50 + 45 + 56 + 40 + 60}{5} = \\frac{251}{5} = 50.2\n\\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n} = \\frac{60 + 56 + 70 + 50 + 75}{5} = \\frac{311}{5} = 62.2\n\n# Verification\nmean(income)  # 50.2\n\n[1] 50.2\n\nmean(turnout) # 62.2\n\n[1] 62.2\n\n\nVariances:\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X: (-0.2, -5.2, 5.8, -10.2, 9.8)\ns^2_X = \\frac{0.04 + 27.04 + 33.64 + 104.04 + 96.04}{4} = \\frac{260.8}{4} = 65.2\nDeviations for Y: (-2.2, -6.2, 7.8, -12.2, 12.8)\ns^2_Y = \\frac{4.84 + 38.44 + 60.84 + 148.84 + 163.84}{4} = \\frac{416.8}{4} = 104.2\n\n# Verification\nvar(income)  # 65.2\n\n[1] 65.2\n\nvar(turnout) # 104.2\n\n[1] 104.2\n\n\n\n\nCovariance and Correlation\nCovariance:\ns_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-0.2 \\times -2.2) = 0.44 (-5.2 \\times -6.2) = 32.24 (5.8 \\times 7.8) = 45.24 (-10.2 \\times -12.2) = 124.44 (9.8 \\times 12.8) = 125.44\ns_{XY} = \\frac{327.8}{4} = 81.95\n\n# Verification\ncov(income, turnout) # 81.95\n\n[1] 81.95\n\n\nCorrelation:\nr_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{81.95}{\\sqrt{65.2}\\sqrt{104.2}} = 0.994\n\n# Verification\ncor(income, turnout) # 0.994\n\n[1] 0.9942402\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{81.95}{65.2} = 1.2571429\nIntercept:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\nStep by step:\n\n1.2571429 \\times 50.2 = 63.1085714\n\\hat{\\beta_0} = 62.2 - 63.1085714 = -0.9085714\n\n\n# Verification\ncoef(model)  # Exact coefficients from R\n\n(Intercept)      income \n -0.8964724   1.2569018 \n\n\n\n\nDetailed Decomposition of Variance and R-squared\nStep 1: Calculate predicted values (\\hat{Y}):\n \\hat{Y} = -0.9085714 + 1.2571429X\nThe predicted values \\hat{Y} for each X value:\nFor X = 50:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (50)\n \\hat{Y} = -0.9085714 + 62.857145 \\hat{Y} = 61.9485736\nFor X = 45:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (45)\n \\hat{Y} = -0.9085714 + 56.5714305 \\hat{Y} = 55.6535591\nFor X = 56:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (56)\n \\hat{Y} = -0.9085714 + 70.4200024 \\hat{Y} = 69.5114310\nFor X = 40:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (40)\n \\hat{Y} = -0.9085714 + 50.2657160 \\hat{Y} = 49.3571446\nFor X = 60:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (60)\n \\hat{Y} = -0.9085714 + 75.4285740 \\hat{Y} = 74.5200026\n\n# Verification of predicted values\ny_hat &lt;- -0.9085714 + 1.2571429 * income\ndata.frame(\n  X = income,\n  Y = turnout,\n  Y_hat = y_hat,\n  row.names = 1:5\n)\n\n   X  Y    Y_hat\n1 50 60 61.94857\n2 45 56 55.66286\n3 56 70 69.49143\n4 40 50 49.37714\n5 60 75 74.52000\n\n\nStep 2: Calculate SST (Total Sum of Squares)\nSST = \\sum(Y_i - \\bar{Y})^2 \\text{ where } \\bar{Y} = 62.2\n(60 - 62.2)^2 = (-2.2)^2 = 4.84 (56 - 62.2)^2 = (-6.2)^2 = 38.44 (70 - 62.2)^2 = (7.8)^2 = 60.84 (50 - 62.2)^2 = (-12.2)^2 = 148.84 (75 - 62.2)^2 = (12.8)^2 = 163.84\nSST = 4.84 + 38.44 + 60.84 + 148.84 + 163.84 = 416.8\nStep 3: Calculate SSR (Regression Sum of Squares)\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\n(61.9485736 - 62.2)^2 = (-0.2514264)^2 = 0.0632151 (55.6535591 - 62.2)^2 = (-6.5464409)^2 = 42.8558689 (69.5114310 - 62.2)^2 = (7.3114310)^2 = 53.4570178 (49.3571446 - 62.2)^2 = (-12.8428554)^2 = 164.9389370 (74.5200026 - 62.2)^2 = (12.3200026)^2 = 151.7824640\nSSR = 413.0975028\nStep 4: Calculate SSE (Error Sum of Squares)\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n(60 - 61.9485736)^2 = (-1.9485736)^2 = 3.7969384 (56 - 55.6535591)^2 = (0.3464409)^2 = 0.1200212 (70 - 69.5114310)^2 = (0.4885690)^2 = 0.2387198 (50 - 49.3571446)^2 = (0.6428554)^2 = 0.4132631 (75 - 74.5200026)^2 = (0.4799974)^2 = 0.2303975\nSSE = 4.7024972\nStep 5: Verify decomposition\nSST = SSR + SSE 416.8 = 413.0975028 + 4.7024972\nStep 6: Calculate R-squared\nR^2 = \\frac{SSR}{SST} = \\frac{413.0975028}{416.8} = 0.9916\n\n# Verification\nsummary(model)$r.squared  # Should match our calculation\n\n[1] 0.9885135\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\ndf &lt;- data.frame(income = income, turnout = turnout)\n\nggplot(df, aes(x = income, y = turnout)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Voter Turnout vs Income per Capita\",\n    x = \"Income per Capita (thousands €)\",\n    y = \"Voter Turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    text = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\nInterpretation\nThe analysis shows:\n\nA very strong positive correlation (r = 0.994) between income and voter turnout\nThe regression equation \\hat{Y} = -0.9085714 + 1.2571429X indicates that:\n\nFor each €1,000 increase in income, turnout increases by about 1.26 percentage points\nThe intercept (-0.9086) has little practical meaning as income is never zero\n\nThe R-squared of 0.9916 indicates that 99.16% of the variance in turnout is explained by income",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#example-4.-anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "href": "correg_en.html#example-4.-anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.47 Example 4. Anxiety Levels and Cognitive Performance: A Laboratory Study",
    "text": "9.47 Example 4. Anxiety Levels and Cognitive Performance: A Laboratory Study\n\nData and Context\nIn a psychology experiment, researchers measured the relationship between anxiety levels (measured by galvanic skin response, GSR) and cognitive performance (score on a working memory task).\n\n# Data\nanxiety &lt;- c(2.1, 3.4, 4.2, 5.1, 5.8, 6.4, 7.2, 8.0)  # GSR readings\nperformance &lt;- c(92, 88, 84, 78, 74, 70, 65, 62)      # Working memory scores\n\n# Initial model check\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8993 -0.6660  0.2162  0.6106  1.5262 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 105.3248     1.3189   79.86 0.00000000026 ***\nanxiety      -5.4407     0.2359  -23.06 0.00000043549 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 6 degrees of freedom\nMultiple R-squared:  0.9888,    Adjusted R-squared:  0.987 \nF-statistic: 531.9 on 1 and 6 DF,  p-value: 0.0000004355\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{2.1 + 3.4 + 4.2 + 5.1 + 5.8 + 6.4 + 7.2 + 8.0}{8} = \\frac{42.2}{8} = 5.275\n\\bar{Y} = \\frac{92 + 88 + 84 + 78 + 74 + 70 + 65 + 62}{8} = \\frac{613}{8} = 76.625\n\n# Verification\nmean(anxiety)\n\n[1] 5.275\n\nmean(performance)\n\n[1] 76.625\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(2.1 - 5.275) = -3.175\n(3.4 - 5.275) = -1.875\n(4.2 - 5.275) = -1.075\n(5.1 - 5.275) = -0.175\n(5.8 - 5.275) = 0.525\n(6.4 - 5.275) = 1.125\n(7.2 - 5.275) = 1.925\n(8.0 - 5.275) = 2.725\n\nSquared deviations:\n10.08063 + 3.51563 + 1.15563 + 0.03063 + 0.27563 + 1.26563 + 3.70563 +\n7.42563 = 27.45500\ns^2_X = \\frac{27.45500}{7} = 3.922143\nSimilarly for Y: Deviations:\n15.375, 11.375, 7.375, 1.375, -2.625, -6.625, -11.625, -14.625\ns^2_Y = \\frac{236.875 + 129.391 + 54.391 + 1.891 + 6.891 + 43.891 + 135.141 + 213.891}{7} = \\frac{822.362}{7} = 117.4803\n\n# Verification\nvar(anxiety)\n\n[1] 3.922143\n\nvar(performance)\n\n[1] 117.4107\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-3.175 × 15.375) = -48.815625\n(-1.875 × 11.375) = -21.328125\n(-1.075 × 7.375) = -7.928125\n(-0.175 × 1.375) = -0.240625\n(0.525 × -2.625) = -1.378125\n(1.125 × -6.625) = -7.453125\n(1.925 × -11.625) = -22.378125\n(2.725 × -14.625) = -39.853125\nSum = -149.375\ns_{XY} = \\frac{-149.375}{7} = -21.33929\n\n# Verification\ncov(anxiety, performance)\n\n[1] -21.33929\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-21.33929}{\\sqrt{3.922143}\\sqrt{117.4803}} = -0.9932\n\n# Verification\ncor(anxiety, performance)\n\n[1] -0.9944073\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-21.33929}{3.922143} = -5.4407\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-5.4407 × 5.275 = -28.6997\n\\hat{\\beta_0} = 76.625 - (-28.6997) = 105.3247\n\n\n# Verification\ncoef(model)\n\n(Intercept)     anxiety \n 105.324804   -5.440721 \n\n\n\n\n4. R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}): \\hat{Y} = 105.3247 - 5.4407X\n\n# Predicted values\ny_hat &lt;- 105.3247 - 5.4407 * anxiety\ndata.frame(\n  Anxiety = anxiety,\n  Performance = performance,\n  Predicted = y_hat,\n  row.names = 1:8\n)\n\n  Anxiety Performance Predicted\n1     2.1          92  93.89923\n2     3.4          88  86.82632\n3     4.2          84  82.47376\n4     5.1          78  77.57713\n5     5.8          74  73.76864\n6     6.4          70  70.50422\n7     7.2          65  66.15166\n8     8.0          62  61.79910\n\n\nStep 2: Sum of Squares\nSST = \\sum(Y_i - \\bar{Y})^2 = 822.362\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 816.3094\nSSE = \\sum(Y_i - \\hat{Y}_i)^2 = 6.0526\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{816.3094}{822.362} = 0.9926\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.9888459\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\n\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Cognitive Performance vs. Anxiety Levels\",\n    x = \"Anxiety (GSR)\",\n    y = \"Performance Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.993) between anxiety and cognitive performance\nFor each unit increase in GSR (anxiety), performance decreases by approximately 5.44 points\nThe model explains 99.26% of the variance in performance scores\nThe relationship appears to be strongly linear, suggesting a reliable anxiety-performance relationship\nThe high intercept (105.32) represents the theoretical maximum performance at zero anxiety\n\n\n\nStudy Limitations\n\nSmall sample size (n=8)\nPossible other confounding variables\nLimited range of anxiety levels\nCross-sectional rather than longitudinal data",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#example-5.-anxiety-vs.-performance-correlation-and-regression-analysis",
    "href": "correg_en.html#example-5.-anxiety-vs.-performance-correlation-and-regression-analysis",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.48 Example 5. Anxiety vs. Performance: Correlation and Regression Analysis",
    "text": "9.48 Example 5. Anxiety vs. Performance: Correlation and Regression Analysis\nIn this tutorial, we’ll explore the relationship between test anxiety levels and exam performance among university students. Research suggests that while a small amount of anxiety can be motivating, excessive anxiety typically impairs performance through reduced concentration, working memory interference, and physical symptoms (Yerkes-Dodson law). We’ll analyze data from 8 students to understand this relationship mathematically.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#data-presentation",
    "href": "correg_en.html#data-presentation",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.49 Data Presentation",
    "text": "9.49 Data Presentation\n\nThe Dataset\nWe collected data from 8 students, measuring:\n\nX: Test anxiety score (1-10 scale, where 1 = very low, 10 = very high)\nY: Exam performance (percentage score)\n\n\n# Our data\nanxiety &lt;- c(2.5, 3.2, 4.1, 4.8, 5.6, 6.3, 7.0, 7.9)  # Anxiety scores\nperformance &lt;- c(80, 85, 78, 82, 77, 74, 68, 72)      # Exam scores (%)\n\n# Create a data frame for easy viewing\ndata &lt;- data.frame(\n  Student = 1:8,\n  Anxiety = anxiety,\n  Performance = performance\n)\nprint(data)\n\n  Student Anxiety Performance\n1       1     2.5          80\n2       2     3.2          85\n3       3     4.1          78\n4       4     4.8          82\n5       5     5.6          77\n6       6     6.3          74\n7       7     7.0          68\n8       8     7.9          72\n\n\n\n\nInitial Visualization\nLet’s first visualize our data to get an intuitive sense of the relationship:\n\nlibrary(ggplot2)\n\n# Create scatterplot\nggplot(data, aes(x = Anxiety, y = Performance)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  labs(\n    title = \"Test Anxiety vs. Exam Performance\",\n    x = \"Test Anxiety Score\",\n    y = \"Exam Performance (%)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(2, 8, 1)) +\n  scale_y_continuous(breaks = seq(65, 90, 5))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-statistics",
    "href": "correg_en.html#summary-statistics",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.50 Summary Statistics",
    "text": "9.50 Summary Statistics\n\nCalculating the Means\nThe mean is the average value, calculated by summing all observations and dividing by the number of observations.\nMean of Anxiety Scores (X): \\bar{X} = \\frac{\\sum_{i=1}^{n} X_i}{n} = \\frac{2.5 + 3.2 + 4.1 + 4.8 + 5.6 + 6.3 + 7.0 + 7.9}{8}\n\\bar{X} = \\frac{41.4}{8} = 5.175\nMean of Performance Scores (Y): \\bar{Y} = \\frac{\\sum_{i=1}^{n} Y_i}{n} = \\frac{80 + 85 + 78 + 82 + 77 + 74 + 68 + 72}{8}\n\\bar{Y} = \\frac{616}{8} = 77\n\n# Verify our calculations\nmean_x &lt;- mean(anxiety)\nmean_y &lt;- mean(performance)\ncat(\"Mean Anxiety:\", mean_x, \"\\n\")\n\nMean Anxiety: 5.175 \n\ncat(\"Mean Performance:\", mean_y, \"\\n\")\n\nMean Performance: 77 \n\n\n\n\nCalculating Variance and Standard Deviation\nVariance measures how spread out the data is from the mean. We use the sample variance formula (dividing by n-1).\nVariance of X:\nFirst, calculate deviations from the mean (X_i - \\bar{X}):\n\n\n\nStudent\nX_i\nX_i - \\bar{X}\n(X_i - \\bar{X})^2\n\n\n\n\n1\n2.5\n2.5 - 5.175 = -2.675\n(-2.675)^2 = 7.1556\n\n\n2\n3.2\n3.2 - 5.175 = -1.975\n(-1.975)^2 = 3.9006\n\n\n3\n4.1\n4.1 - 5.175 = -1.075\n(-1.075)^2 = 1.1556\n\n\n4\n4.8\n4.8 - 5.175 = -0.375\n(-0.375)^2 = 0.1406\n\n\n5\n5.6\n5.6 - 5.175 = 0.425\n(0.425)^2 = 0.1806\n\n\n6\n6.3\n6.3 - 5.175 = 1.125\n(1.125)^2 = 1.2656\n\n\n7\n7.0\n7.0 - 5.175 = 1.825\n(1.825)^2 = 3.3306\n\n\n8\n7.9\n7.9 - 5.175 = 2.725\n(2.725)^2 = 7.4256\n\n\nSum\n\n\n24.555\n\n\n\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1} = \\frac{24.555}{7} = 3.5079\ns_X = \\sqrt{3.5079} = 1.8730\nVariance of Y:\n\n\n\nStudent\nY_i\nY_i - \\bar{Y}\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n80\n80 - 77 = 3\n(3)^2 = 9\n\n\n2\n85\n85 - 77 = 8\n(8)^2 = 64\n\n\n3\n78\n78 - 77 = 1\n(1)^2 = 1\n\n\n4\n82\n82 - 77 = 5\n(5)^2 = 25\n\n\n5\n77\n77 - 77 = 0\n(0)^2 = 0\n\n\n6\n74\n74 - 77 = -3\n(-3)^2 = 9\n\n\n7\n68\n68 - 77 = -9\n(-9)^2 = 81\n\n\n8\n72\n72 - 77 = -5\n(-5)^2 = 25\n\n\nSum\n\n\n214\n\n\n\ns^2_Y = \\frac{\\sum(Y_i - \\bar{Y})^2}{n-1} = \\frac{214}{7} = 30.5714\ns_Y = \\sqrt{30.5714} = 5.5291\n\n# Verify variance and standard deviation\ncat(\"Variance of Anxiety:\", var(anxiety), \"\\n\")\n\nVariance of Anxiety: 3.507857 \n\ncat(\"SD of Anxiety:\", sd(anxiety), \"\\n\")\n\nSD of Anxiety: 1.872927 \n\ncat(\"Variance of Performance:\", var(performance), \"\\n\")\n\nVariance of Performance: 30.57143 \n\ncat(\"SD of Performance:\", sd(performance), \"\\n\")\n\nSD of Performance: 5.529144",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#covariance-and-correlation-2",
    "href": "correg_en.html#covariance-and-correlation-2",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.51 Covariance and Correlation",
    "text": "9.51 Covariance and Correlation\n\nCalculating Covariance\nCovariance measures how two variables vary together. A negative covariance indicates that as one variable increases, the other tends to decrease.\ns_{XY} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nLet’s calculate the products for each observation:\n\n\n\n\n\n\n\n\n\nStudent\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n\n\n\n\n1\n-2.675\n3\n(-2.675) \\times (3) = -8.025\n\n\n2\n-1.975\n8\n(-1.975) \\times (8) = -15.800\n\n\n3\n-1.075\n1\n(-1.075) \\times (1) = -1.075\n\n\n4\n-0.375\n5\n(-0.375) \\times (5) = -1.875\n\n\n5\n0.425\n0\n(0.425) \\times (0) = 0\n\n\n6\n1.125\n-3\n(1.125) \\times (-3) = -3.375\n\n\n7\n1.825\n-9\n(1.825) \\times (-9) = -16.425\n\n\n8\n2.725\n-5\n(2.725) \\times (-5) = -13.625\n\n\nSum\n\n\n-60.2\n\n\n\ns_{XY} = \\frac{-60.2}{7} = -8.6\n\n\nCalculating Pearson Correlation Coefficient\nThe correlation coefficient standardizes the covariance to a scale from -1 to +1:\nr = \\frac{s_{XY}}{s_X \\cdot s_Y} = \\frac{-8.6}{1.8730 \\times 5.5291} = \\frac{-8.6}{10.3560} = -0.831\nThis gives us a correlation of -0.831, indicating a strong negative relationship between anxiety and performance.\n\n# Verify correlation\nactual_cor &lt;- cor(anxiety, performance)\ncat(\"Pearson correlation coefficient:\", actual_cor, \"\\n\")\n\nPearson correlation coefficient: -0.8304618",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#simple-ols-regression",
    "href": "correg_en.html#simple-ols-regression",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.52 Simple OLS Regression",
    "text": "9.52 Simple OLS Regression\n\nThe Problem of Modeling Relationships\nWhen we observe a relationship between two variables, we want to find a mathematical model that:\n\nDescribes the relationship\nAllows us to make predictions\nQuantifies the strength of the relationship\n\nThe simplest model is a straight line: Y = \\beta_0 + \\beta_1 X + \\epsilon\nWhere:\n\nY is the outcome variable (performance)\nX is the predictor variable (anxiety)\n\\beta_0 is the intercept (performance when anxiety = 0)\n\\beta_1 is the slope (change in performance per unit change in anxiety)\n\\epsilon is the error term (unexplained variation)\n\n\n\nThe Idea of Sum of Squared Errors (SSE)\n\nWhy Do We Need a Criterion?\nImagine trying to draw a line through our data points. There are infinitely many lines we could draw! Some would go through the middle of the points, others might be too high or too low, too steep or too flat. We need a systematic way to determine which line is “best.”\n\n# Visualize multiple possible lines\nlibrary(ggplot2)\n\n# Create different possible lines\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  # Bad line 1: Too steep\n  geom_abline(intercept = 100, slope = -5, color = \"gray\", linetype = \"dashed\", alpha = 0.5) +\n  # Bad line 2: Too flat\n  geom_abline(intercept = 78, slope = -0.5, color = \"gray\", linetype = \"dashed\", alpha = 0.5) +\n  # Bad line 3: Wrong direction\n  geom_abline(intercept = 65, slope = 2, color = \"gray\", linetype = \"dashed\", alpha = 0.5) +\n  # Good line (we'll calculate this properly)\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Which Line Best Fits Our Data?\",\n    subtitle = \"Gray dashed lines show poor fits, red line shows the optimal fit\",\n    x = \"Test Anxiety Score\",\n    y = \"Exam Performance (%)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWhat Are Errors (Residuals)?\nFor any line we draw, each data point will have an error or residual - the vertical distance from the point to the line. This represents how “wrong” our prediction is for that point.\n\nPositive error: The actual value is above the predicted value (we underestimated)\nNegative error: The actual value is below the predicted value (we overestimated)\n\n\n# Visualize errors for the regression line\nmodel &lt;- lm(performance ~ anxiety)\npredicted &lt;- predict(model)\n\nggplot(data.frame(anxiety, performance, predicted), aes(x = anxiety, y = performance)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_segment(aes(xend = anxiety, yend = predicted), \n               color = \"gray50\", linetype = \"dashed\", alpha = 0.7) +\n  # Add labels for a few errors\n  geom_text(aes(x = 3.2, y = 86, label = \"Error = +4.9\"), size = 3, color = \"gray40\") +\n  geom_text(aes(x = 7.0, y = 69, label = \"Error = -1.4\"), size = 3, color = \"gray40\") +\n  labs(\n    title = \"Visualizing Errors (Residuals) in Regression\",\n    subtitle = \"Dashed lines show the vertical distance from each point to the regression line\",\n    x = \"Test Anxiety Score\",\n    y = \"Exam Performance (%)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWhy Square the Errors?\nSimply adding up the errors won’t work because positive and negative errors cancel out. We could use absolute values, but squaring has several advantages:\n\nMathematical convenience: Squared functions are differentiable, making it easier to find the minimum using calculus\nPenalizes large errors more: A few large errors are worse than many small errors\n\nError of 4: squared = 16\nTwo errors of 2: squared = 4 + 4 = 8\nFour errors of 1: squared = 1 + 1 + 1 + 1 = 4\n\nCreates a smooth, bowl-shaped function: This guarantees a unique minimum\n\n\n\nThe SSE Formula\nThe Sum of Squared Errors is: SSE = \\sum_{i=1}^{n}(Y_i - \\hat{Y_i})^2 = \\sum_{i=1}^{n}(Y_i - (\\beta_0 + \\beta_1 X_i))^2\n\n\\min_{\\beta}\\ \\sum_{i=1}^n\\bigl(Y_i-\\hat{Y}_i(\\beta)\\bigr)^2,\n\\quad\\text{where } \\hat{Y}_i(\\beta)=\\beta_0+\\beta_1X_i.\n\nThis formula:\n\nTakes each actual value (Y_i)\nSubtracts the predicted value (\\hat{Y}_i(\\beta)=\\beta_0+\\beta_1X_i)\nSquares the difference\nAdds them all up\n\n\n\nFinding the Best Line\nThe “best” line is the one that minimizes SSE. Using calculus (taking derivatives with respect to \\beta_0 and \\beta_1 and setting them to zero), we get the OLS formulas.\n\n\n\nOLS Estimators\nThe Ordinary Least Squares (OLS) method finds the values of \\beta_0 and \\beta_1 that minimize SSE:\nSlope estimator: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\nIntercept estimator: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\n\nCalculating the OLS Parameters\nUsing our calculated values:\n\ns_{XY} = -8.6\ns^2_X = 3.5079\n\\bar{X} = 5.175\n\\bar{Y} = 77\n\nStep 1: Calculate the slope \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-8.6}{3.5079} = -2.451\nStep 2: Calculate the intercept \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} = 77 - (-2.451 \\times 5.175) = 77 + 12.684 = 89.684\n\n# Verify with R\nmodel &lt;- lm(performance ~ anxiety)\ncoef(model)\n\n(Intercept)     anxiety \n  89.687233   -2.451639 \n\n\nOur regression equation is: \\hat{Y} = 89.684 - 2.451X\nThis means:\n\nWhen anxiety = 0, predicted performance = 89.68%\nFor each 1-point increase in anxiety, performance decreases by 2.45%",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#plotting-the-model",
    "href": "correg_en.html#plotting-the-model",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.53 Plotting the Model",
    "text": "9.53 Plotting the Model\n\n# Create comprehensive plot\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1) +\n  geom_text(aes(label = paste0(\"(\", round(anxiety, 1), \", \", performance, \")\")),\n            vjust = -1, size = 3) +\n  annotate(\"text\", x = 3, y = 70, \n           label = paste0(\"ŷ = \", round(coef(model)[1], 2), \" - \", \n                         abs(round(coef(model)[2], 2)), \"x\"),\n           size = 5, color = \"red\") +\n  labs(\n    title = \"Regression Line: Performance vs. Anxiety\",\n    subtitle = \"Higher anxiety is associated with lower exam performance\",\n    x = \"Test Anxiety Score\",\n    y = \"Exam Performance (%)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(2, 8, 1)) +\n  scale_y_continuous(breaks = seq(65, 90, 5))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#model-evaluation",
    "href": "correg_en.html#model-evaluation",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.54 Model Evaluation",
    "text": "9.54 Model Evaluation\n\nVariance Decomposition\nThe total variation in Y can be decomposed into two parts:\nSST = SSE + SSR\nWhere:\n\nSST (Total Sum of Squares): Total variation in Y\nSSE (Sum of Squared Errors): Unexplained variation\n\nSSR (Sum of Squares Regression): Explained variation\n\n\n\n\n\n\n\nTip\n\n\n\nImagine you’re trying to understand why people have different salaries at a company. Some people make $40,000, others make $80,000, and some make $120,000. There’s variation in salaries - they’re not all the same.\n\nThe Total Variation (SST)\nThis is simply asking: “How spread out are all the salaries from the average salary?”\nIf the average salary is $70,000, then SST measures how far each person’s salary differs from $70,000, squares those differences (to make them all positive), and adds them up. It’s the total amount of variation we’re trying to understand.\n\n\nThe Explained Variation (SSR)\nNow suppose we build a model that predicts salary based on years of experience. Our model might say: - 2 years experience → predicts $50,000 - 5 years experience → predicts $70,000\n- 10 years experience → predicts $100,000\nSSR measures how much these predictions vary from the average. It’s the variation that our model successfully “explains” through the relationship with experience. It’s like saying “this much of the salary differences between people is because they have different amounts of experience.”\n\n\nThe Unexplained Variation (SSE)\nThis is what’s left over - the part our model can’t explain.\nMaybe two people both have 5 years of experience, but one makes $65,000 and another makes $75,000. Our model predicted $70,000 for both. These differences from our predictions (the errors) represent variation due to other factors we haven’t captured - maybe education, performance, negotiation skills, or just random luck.\n\n\n9.55 The Key Insight\nThe beautiful thing is that these three always relate as: Total Variation = Explained Variation + Unexplained Variation\nIt’s like having a pie chart of “why salaries differ”: - One slice is “differences explained by experience” (SSR) - The other slice is “differences due to other stuff” (SSE) - Together they make the whole pie (SST)\n\n\n9.56 Why This Matters\nThis decomposition lets us calculate R-squared (R²), which is simply: R^2 = \\frac{SSR}{SST} = \\frac{\\text{Explained Variation}}{\\text{Total Variation}}\nIf R² = 0.70, it means our model explains 70% of why the Y values differ from each other. The remaining 30% is due to factors we haven’t captured or random noise.\nThink of it like solving a mystery: SST is the total mystery to solve, SSR is how much of the mystery you’ve solved, and SSE is what remains unsolved!\n\n\n\nLet’s calculate each:\nStep 1: Calculate predicted values\nUsing \\hat{Y} = 89.684 - 2.451X:\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i = 89.684 - 2.451X_i\n\n\n\n\n1\n2.5\n80\n89.684 - 2.451(2.5) = 83.556\n\n\n2\n3.2\n85\n89.684 - 2.451(3.2) = 81.841\n\n\n3\n4.1\n78\n89.684 - 2.451(4.1) = 79.635\n\n\n4\n4.8\n82\n89.684 - 2.451(4.8) = 77.919\n\n\n5\n5.6\n77\n89.684 - 2.451(5.6) = 75.968\n\n\n6\n6.3\n74\n89.684 - 2.451(6.3) = 74.253\n\n\n7\n7.0\n68\n89.684 - 2.451(7.0) = 72.527\n\n\n8\n7.9\n72\n89.684 - 2.451(7.9) = 70.321\n\n\n\nStep 2: Calculate sum of squares\nSST (Total variation): SST = \\sum(Y_i - \\bar{Y})^2\nFrom our earlier variance calculation: SST = (n-1) \\times s^2_Y = 7 \\times 30.5714 = 214\nSSE (Unexplained variation): SSE = \\sum(Y_i - \\hat{Y}_i)^2\n\n\n\n\n\n\n\n\n\n\ni\nY_i\n\\hat{Y}_i\nY_i - \\hat{Y}_i\n(Y_i - \\hat{Y}_i)^2\n\n\n\n\n1\n80\n83.556\n-3.556\n12.645\n\n\n2\n85\n81.841\n3.159\n9.979\n\n\n3\n78\n79.635\n-1.635\n2.673\n\n\n4\n82\n77.919\n4.081\n16.655\n\n\n5\n77\n75.968\n1.032\n1.065\n\n\n6\n74\n74.253\n-0.253\n0.064\n\n\n7\n68\n72.527\n-4.527\n20.494\n\n\n8\n72\n70.321\n1.679\n2.819\n\n\nSum\n\n\n\n66.394\n\n\n\nSSR (Explained variation): SSR = SST - SSE = 214 - 66.394 = 147.606\n\n# Verify calculations\nanova_table &lt;- anova(model)\ncat(\"SSR (Regression):\", anova_table$`Sum Sq`[1], \"\\n\")\n\nSSR (Regression): 147.5887 \n\ncat(\"SSE (Residual):\", anova_table$`Sum Sq`[2], \"\\n\")\n\nSSE (Residual): 66.41132 \n\ncat(\"SST (Total):\", sum(anova_table$`Sum Sq`), \"\\n\")\n\nSST (Total): 214 \n\n\n\n\nR-squared (Coefficient of Determination)\nR-squared tells us what proportion of the total variation in Y is explained by our model:\nR^2 = \\frac{SSR}{SST} = \\frac{147.606}{214} = 0.690\nThis means our model explains 69.0% of the variation in exam performance.\nAlternative formula using correlation: R^2 = r^2 = (-0.831)^2 = 0.691\n\n# Verify R-squared\ncat(\"R-squared:\", summary(model)$r.squared, \"\\n\")\n\nR-squared: 0.6896667 \n\ncat(\"Correlation squared:\", cor(anxiety, performance)^2, \"\\n\")\n\nCorrelation squared: 0.6896667",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-key-insight",
    "href": "correg_en.html#the-key-insight",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.55 The Key Insight",
    "text": "9.55 The Key Insight\nThe beautiful thing is that these three always relate as: Total Variation = Explained Variation + Unexplained Variation\nIt’s like having a pie chart of “why salaries differ”: - One slice is “differences explained by experience” (SSR) - The other slice is “differences due to other stuff” (SSE) - Together they make the whole pie (SST)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#why-this-matters",
    "href": "correg_en.html#why-this-matters",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.56 Why This Matters",
    "text": "9.56 Why This Matters\nThis decomposition lets us calculate R-squared (R²), which is simply: R^2 = \\frac{SSR}{SST} = \\frac{\\text{Explained Variation}}{\\text{Total Variation}}\nIf R² = 0.70, it means our model explains 70% of why the Y values differ from each other. The remaining 30% is due to factors we haven’t captured or random noise.\nThink of it like solving a mystery: SST is the total mystery to solve, SSR is how much of the mystery you’ve solved, and SSE is what remains unsolved!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#effect-size",
    "href": "correg_en.html#effect-size",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.57 Effect Size",
    "text": "9.57 Effect Size\nThe slope coefficient \\hat{\\beta_1} = -2.451 is our effect size. It tells us:\n\nMagnitude: Each 1-point increase in anxiety is associated with a 2.45% decrease in performance\nPractical significance: A student moving from low anxiety (3) to high anxiety (7) would see an expected performance decrease of 2.451 \\times 4 = 9.80\\%\n\nStandardized effect size (correlation coefficient): The correlation r = -0.831 indicates a strong negative relationship.\nCohen’s guidelines for interpreting correlation:\n\nSmall effect: |r| = 0.10\nMedium effect: |r| = 0.30\nLarge effect: |r| = 0.50\n\nOur |r| = 0.831 represents a large effect size.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#confidence-intervals-and-statistical-significance",
    "href": "correg_en.html#confidence-intervals-and-statistical-significance",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.58 Confidence Intervals and Statistical Significance",
    "text": "9.58 Confidence Intervals and Statistical Significance\n\nStandard Error of the Regression\nFirst, we need the standard error of the residuals:\ns_e = \\sqrt{\\frac{SSE}{n-2}} = \\sqrt{\\frac{66.394}{6}} = \\sqrt{11.066} = 3.327\n\n\nStandard Error of the Slope\nThe standard error of \\hat{\\beta_1} is:\nSE(\\hat{\\beta_1}) = \\frac{s_e}{\\sqrt{\\sum(X_i - \\bar{X})^2}} = \\frac{3.327}{\\sqrt{24.555}} = \\frac{3.327}{4.955} = 0.671\n\n\n95% Confidence Interval for the Slope\nIn plain English: A confidence interval gives us a range of plausible values for our true slope. If we repeated this study many times, 95% of the intervals we calculate would contain the true slope value.\nThe formula uses a critical value (approximately 2.45 for 6 degrees of freedom):\nCI = \\hat{\\beta_1} \\pm (critical\\_value \\times SE(\\hat{\\beta_1})) CI = -2.451 \\pm (2.45 \\times 0.671) CI = -2.451 \\pm 1.644 CI = [-4.095, -0.807]\nInterpretation: We are 95% confident that the true change in performance per unit change in anxiety is between -4.10% and -0.81%.\n\n\nStatistical Significance\nTo test if the relationship is statistically significant (i.e., not due to chance), we calculate a t-statistic:\nt = \\frac{\\hat{\\beta_1}}{SE(\\hat{\\beta_1})} = \\frac{-2.451}{0.671} = -3.653\nIn plain English: This t-value tells us how many standard errors our slope is away from zero. An absolute value of 3.65 is quite large (typically, values beyond ±2.45 are considered significant for our sample size), providing strong evidence of a real negative relationship between anxiety and performance.\n\n# Verify calculations with R\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.526 -2.116  0.400  2.050  4.081 \n\nCoefficients:\n            Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)  89.6872     3.6682  24.450 0.000000308 ***\nanxiety      -2.4516     0.6714  -3.652      0.0107 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.327 on 6 degrees of freedom\nMultiple R-squared:  0.6897,    Adjusted R-squared:  0.6379 \nF-statistic: 13.33 on 1 and 6 DF,  p-value: 0.01069\n\nconfint(model)\n\n                2.5 %     97.5 %\n(Intercept) 80.711582 98.6628838\nanxiety     -4.094474 -0.8088048",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-and-interpretation",
    "href": "correg_en.html#summary-and-interpretation",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.59 Summary and Interpretation",
    "text": "9.59 Summary and Interpretation\n\nWhat We’ve Learned\n\nThere is a negative relationship between test anxiety and exam performance (r = -0.831)\nThe relationship is moderately strong - anxiety explains 69.0% of the variation in performance\nThe effect is substantial - each 1-point increase in anxiety is associated with about a 2.5% decrease in performance\nThe relationship is statistically significant - very unlikely to be due to chance\n\n\n\nPractical Implications\nOur analysis suggests that high test anxiety impairs performance, possibly through:\n\nCognitive interference (worrying thoughts compete for working memory)\nPhysical symptoms (sweating, rapid heartbeat) that distract from the task\nNegative self-talk reducing confidence and motivation\nTest-taking behaviors (rushing, second-guessing answers)\n\n\n\nRecommendations Based on Findings\nGiven the strong negative relationship, interventions might include:\n\nTeaching anxiety management techniques (deep breathing, progressive muscle relaxation)\nCognitive restructuring to address catastrophic thinking\nStudy skills training to increase preparation confidence\nPractice tests to reduce fear of the unknown\n\n\n\nLimitations of Our Analysis\n\nSmall sample size (n = 8) limits generalizability\nLinear assumption - the relationship might be curved (optimal anxiety might exist)\nOther variables not considered (preparation time, ability, sleep, etc.)\nCausation vs. correlation - we cannot prove anxiety causes poor performance\nSelf-reported anxiety - subjective measures may not reflect physiological arousal\n\n\n\nComplete R Code\n\n# Complete analysis code\nanxiety &lt;- c(2.5, 3.2, 4.1, 4.8, 5.6, 6.3, 7.0, 7.9)\nperformance &lt;- c(80, 85, 78, 82, 77, 74, 68, 72)\n\n# Descriptive statistics\nmean(anxiety); sd(anxiety)\nmean(performance); sd(performance)\n\n# Correlation\ncor(anxiety, performance)\n\n# Regression\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\nconfint(model)\n\n# Visualization\nlibrary(ggplot2)\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_minimal()\n\n# Diagnostics\nplot(model)\n\n# ANOVA table\nanova(model)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#example-6.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "href": "correg_en.html#example-6.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.60 Example 6. District Magnitude and Electoral Disproportionality: A Comparative Analysis",
    "text": "9.60 Example 6. District Magnitude and Electoral Disproportionality: A Comparative Analysis\n\nData Generating Process\nLet’s set up a DGP where:\n\\begin{aligned}\n& Y_{\\text{Gallagher}} = 12 - 0.8X_{\\text{DM}} + \\varepsilon \\\\\n& \\varepsilon \\sim \\mathcal{N}(0, 1) \\\\\n& X_{\\text{DM}} \\in \\{3, 5, 7, 10, 12, 15\\}\n\\end{aligned}\n\n# DGP\nmagnitude &lt;- c(3, 5, 7, 10, 12, 15)\nepsilon &lt;- rnorm(6, mean = 0, sd = 1)\ngallagher &lt;- 12 - 0.8 * magnitude + epsilon\n\n# Round (sampled from the DGP) Gallagher indices to one decimal place\ngallagher &lt;- round(c(9.0, 7.8, 9.2, 4.1, 2.5, 1.7), 1)\n\n# Show data\ndata.frame(\n  District_Magnitude = magnitude,\n  Gallagher_Index = gallagher\n)\n\n  District_Magnitude Gallagher_Index\n1                  3             9.0\n2                  5             7.8\n3                  7             9.2\n4                 10             4.1\n5                 12             2.5\n6                 15             1.7\n\n# Initial model check\nmodel &lt;- lm(gallagher ~ magnitude)\nsummary(model)\n\n\nCall:\nlm(formula = gallagher ~ magnitude)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.6516 -0.4628  2.3260 -0.6908 -0.9020  0.3813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7349     1.3034   9.003 0.000843 ***\nmagnitude    -0.6944     0.1359  -5.110 0.006934 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.368 on 4 degrees of freedom\nMultiple R-squared:  0.8672,    Adjusted R-squared:  0.834 \nF-statistic: 26.11 on 1 and 4 DF,  p-value: 0.006934\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{3 + 5 + 7 + 10 + 12 + 15}{6} = \\frac{52}{6} = 8.6667\n\\bar{Y} = \\frac{9.0 + 7.8 + 9.2 + 4.1 + 2.5 + 1.7}{6} = \\frac{34.3}{6} = 5.7167\n\n# Verification\nmean(magnitude)\n\n[1] 8.666667\n\nmean(gallagher)\n\n[1] 5.716667\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(3 - 8.6667) = -5.6667\n(5 - 8.6667) = -3.6667\n(7 - 8.6667) = -1.6667\n(10 - 8.6667) = 1.3333\n(12 - 8.6667) = 3.3333\n(15 - 8.6667) = 6.3333\n\nSquared deviations:\n\n32.1115 + 13.4445 + 2.7779 + 1.7777 + 11.1109 + 40.1107 = 101.3332\n\ns^2_X = \\frac{101.3332}{5} = 20.2666\nFor Y: Deviations: 3.2833, 2.0833, 3.4833, -1.6167, -3.2167, -4.0167\ns^2_Y = \\frac{56.3483}{5} = 11.2697\n\n# Verification\nvar(magnitude)\n\n[1] 20.26667\n\nvar(gallagher)\n\n[1] 11.26967\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n\n(-5.6667 × 3.2833) = -18.6057\n(-3.6667 × 2.0833) = -7.6387\n(-1.6667 × 3.4833) = -5.8056\n(1.3333 × -1.6167) = -2.1556\n(3.3333 × -3.2167) = -10.7223\n(6.3333 × -4.0167) = -25.4391\n\nSum = -70.3670\ns_{XY} = \\frac{-70.3670}{5} = -14.0734\n\n# Verification\ncov(magnitude, gallagher)\n\n[1] -14.07333\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-14.0734}{\\sqrt{20.2666}\\sqrt{11.2697}} = -0.9279\n\n# Verification\ncor(magnitude, gallagher)\n\n[1] -0.9312157\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-14.0734}{20.2666} = -0.6944\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-0.6944 × 8.6667 = -6.0181\n\\hat{\\beta_0} = 5.7167 - (-6.0181) = 11.7348\n\n\n# Verification\ncoef(model)\n\n(Intercept)   magnitude \n 11.7348684  -0.6944079 \n\n\n\n\nR-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = 11.7348 - 0.6944X\n\n# Predicted values\ny_hat &lt;- 11.7348 - 0.6944 * magnitude\ndata.frame(\n  Magnitude = magnitude,\n  Gallagher = gallagher,\n  Predicted = y_hat,\n  row.names = 1:6\n)\n\n  Magnitude Gallagher Predicted\n1         3       9.0    9.6516\n2         5       7.8    8.2628\n3         7       9.2    6.8740\n4        10       4.1    4.7908\n5        12       2.5    3.4020\n6        15       1.7    1.3188\n\n\nStep 2: Sum of Squares SST = \\sum(Y_i - \\bar{Y})^2 = 56.3483 SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 48.5271 SSE = \\sum(Y_i - \\hat{Y}_i)^2 = 7.8212\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{48.5271}{56.3483} = 0.8612\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.8671626\n\n\n\n\nVisualization - True vs. Estimated Parameters\n\nTrue DGP: Y = 12 - 0.8X + ε\nEstimated Model: Y = 11.7348 - 0.6944X\n\n\nlibrary(ggplot2)\n\n# Create data frame with original data\ndf &lt;- data.frame(\n  magnitude = magnitude,\n  gallagher = gallagher\n)\n\n# Create sequence for smooth lines\nx_seq &lt;- seq(min(magnitude), max(magnitude), length.out = 100)\n\n# Calculate predicted values for both lines\ntrue_dgp &lt;- 12 - 0.8 * x_seq\nestimated &lt;- 11.7348 - 0.6944 * x_seq\n\n# Combine into a data frame for plotting\nlines_df &lt;- data.frame(\n  magnitude = rep(x_seq, 2),\n  value = c(true_dgp, estimated),\n  Model = rep(c(\"True DGP\", \"Estimated\"), each = length(x_seq))\n)\n\n# Create plot\nggplot() +\n  geom_line(data = lines_df, \n            aes(x = magnitude, y = value, color = Model, linetype = Model),\n            size = 1) +\n  geom_point(data = df, \n             aes(x = magnitude, y = gallagher),\n             color = \"black\", \n             size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  labs(\n    title = \"True DGP vs. Estimated Regression Line\",\n    subtitle = \"Black points show observed data with random noise\",\n    x = \"District Magnitude\",\n    y = \"Gallagher Index\",\n    caption = \"True DGP: Y = 12 - 0.8X + ε\\nEstimated: Y = 11.73 - 0.69X\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.caption = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\n\nObservations about Model Fit\n\nSlope Comparison\n\nTrue slope: -0.8\nEstimated slope: -0.69\nThe estimated slope is reasonably close to the true parameter\n\nIntercept Comparison\n\nTrue intercept: 12\nEstimated intercept: 11.73\nThe estimated intercept very closely approximates the true value\n\nVisual Patterns\n\nThe lines are nearly parallel, showing good slope recovery\nPoints scatter around both lines due to the random error term (ε)\nThe small sample size (n=6) leads to some imprecision in estimation\nThe estimated line (blue) provides a good approximation of the true DGP (red dashed)\n\nImpact of Random Error\n\nThe scatter of points around the true DGP line reflects the N(0,1) error term\nThis noise leads to the slight differences in estimated parameters\nWith a larger sample, we would expect even closer convergence to true parameters\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.93) between district magnitude and electoral disproportionality\nFor each unit increase in district magnitude, the Gallagher index decreases by approximately 0.69 points\nThe model explains 86.12% of the variance in disproportionality\nThe relationship appears strongly linear with moderate scatter\nThe intercept (11.73) represents the expected disproportionality in a hypothetical single-member district system\n\n\n\nStudy Context\n\nData represents simulated observations from a DGP with moderate noise\nSample shows how increasing district magnitude tends to reduce disproportionality\nRandom component reflects other institutional and political factors affecting disproportionality\n\n\n\nLimitations\n\nSmall sample size (n=6)\nSimulated rather than real-world data\nAssumes linear relationship\nDoes not account for other institutional features",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-d-ols-regression-in-matrix-form---manual-calculations",
    "href": "correg_en.html#appendix-d-ols-regression-in-matrix-form---manual-calculations",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.61 Appendix D: OLS Regression in Matrix Form - Manual Calculations (*)",
    "text": "9.61 Appendix D: OLS Regression in Matrix Form - Manual Calculations (*)\n\nA.1 Essential Linear Algebra Concepts\nBefore diving into OLS regression, we need to understand some fundamental linear algebra concepts.\n\nA.1.1 Matrix Transpose\nThe transpose of a matrix \\mathbf{A}, denoted \\mathbf{A}^T, is obtained by swapping rows and columns:\n\n\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\implies \\mathbf{A}^T = \\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 4 & 6 \\end{bmatrix}\n\nProperties:\n\n(A^T)^T = A\n(AB)^T = B^T A^T\n(A + B)^T = A^T + B^T\n\n\n\nA.1.2 Matrix Multiplication\nFor matrices \\mathbf{A} (of size m \\times n) and \\mathbf{B} (of size n \\times p), the product \\mathbf{AB} is an m \\times p matrix where element (i,j) is:\n\n(\\mathbf{AB})_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}\n\nExample:\n\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 1(5)+2(7) & 1(6)+2(8) \\\\ 3(5)+4(7) & 3(6)+4(8) \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n\n\n\nA.1.3 Identity Matrix\nThe identity matrix \\mathbf{I}_n is an n \\times n matrix with 1’s on the diagonal and 0’s elsewhere:\n\n\\mathbf{I}_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\nProperty: \\mathbf{AI} = \\mathbf{IA} = \\mathbf{A}\n\n\nA.1.4 Determinant\nThe determinant is a scalar value that characterizes certain properties of a square matrix. It indicates whether a matrix is invertible.\nFor a 2 \\times 2 matrix:\n\n\\det\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = ad - bc\n\nExample:\n\n\\det\\begin{bmatrix} 3 & 8 \\\\ 4 & 6 \\end{bmatrix} = 3(6) - 8(4) = 18 - 32 = -14\n\nFor a 3 \\times 3 matrix (cofactor expansion along first row):\n\n\\det\\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} = a\\begin{vmatrix} e & f \\\\ h & i \\end{vmatrix} - b\\begin{vmatrix} d & f \\\\ g & i \\end{vmatrix} + c\\begin{vmatrix} d & e \\\\ g & h \\end{vmatrix}\n\nProperties:\n\nIf \\det(\\mathbf{A}) = 0, the matrix is singular (not invertible)\nIf \\det(\\mathbf{A}) \\neq 0, the matrix is non-singular (invertible)\n\\det(\\mathbf{AB}) = \\det(\\mathbf{A})\\det(\\mathbf{B})\n\\det(\\mathbf{A}^T) = \\det(\\mathbf{A})\n\n\n\nA.1.5 Matrix Inverse\nThe inverse of matrix \\mathbf{A}, denoted \\mathbf{A}^{-1}, satisfies:\n\n\\mathbf{AA}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\n\nFor a 2 \\times 2 matrix:\n\n\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad-bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\nFor larger matrices, the inverse can be computed as:\n\n\\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})}\\text{adj}(\\mathbf{A})\n\nwhere \\text{adj}(\\mathbf{A}) is the adjugate (transpose of the cofactor matrix).\nProperties:\n\n(A^{-1})^{-1} = A\n(AB)^{-1} = B^{-1}A^{-1}\n(A^T)^{-1} = (A^{-1})^T\n\n\n\nA.1.6 Matrix Rank\nThe rank of a matrix is the maximum number of linearly independent rows (or columns). A matrix \\mathbf{X} of size n \\times p has:\n\nFull column rank if \\text{rank}(\\mathbf{X}) = p (all columns are independent)\nRank deficiency if \\text{rank}(\\mathbf{X}) &lt; p (multicollinearity exists)\n\nFull column rank is required for \\mathbf{X}^T\\mathbf{X} to be invertible in regression.\n\n\nA.1.7 Quadratic Forms\nA quadratic form is an expression of the type:\n\n\\mathbf{x}^T\\mathbf{A}\\mathbf{x} = \\sum_{i=1}^n\\sum_{j=1}^n a_{ij}x_ix_j\n\nFor a vector \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} and matrix \\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}:\n\n\\mathbf{x}^T\\mathbf{A}\\mathbf{x} = a_{11}x_1^2 + (a_{12}+a_{21})x_1x_2 + a_{22}x_2^2\n\nThis is crucial for understanding the OLS objective function.\n\n\n\nA.2 OLS Minimization Problem and Derivation\n\nA.2.1 The Objective Function\nIn matrix notation, the linear regression model is:\n\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\nwhere:\n\n\\mathbf{y} is an n \\times 1 vector of observed responses\n\\mathbf{X} is an n \\times (p+1) design matrix of predictors (including intercept)\n\\boldsymbol{\\beta} is a (p+1) \\times 1 vector of regression coefficients\n\\boldsymbol{\\epsilon} is an n \\times 1 vector of errors\n\nThe sum of squared residuals (SSR) is:\n\nS(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2\n\nIn matrix form, this becomes:\n\nS(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n\n\n\nA.2.2 Expanding the Objective Function\nLet’s expand the expression:\n\nS(\\boldsymbol{\\beta}) = (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n\n\n= \\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\nNote that \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} is a scalar, so it equals its transpose:\n\n\\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} = (\\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta})^T = \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y}\n\nTherefore:\n\nS(\\boldsymbol{\\beta}) = \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\n\n\nA.2.3 Taking the Derivative\nTo minimize S(\\boldsymbol{\\beta}), we take the derivative with respect to \\boldsymbol{\\beta} and set it equal to zero.\nMatrix calculus rules needed:\n\n\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}(\\mathbf{a}^T\\boldsymbol{\\beta}) = \\mathbf{a}\n\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}(\\boldsymbol{\\beta}^T\\mathbf{A}\\boldsymbol{\\beta}) = 2\\mathbf{A}\\boldsymbol{\\beta} (when \\mathbf{A} is symmetric)\n\nApplying these rules:\n\n\\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\n\n\nA.2.4 Setting the Derivative to Zero (Normal Equations)\nSet \\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = \\mathbf{0}:\n\n-2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0}\n\nSimplify:\n\n\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}\n\nThese are called the normal equations.\n\n\nA.2.5 Solving for \\hat{\\boldsymbol{\\beta}}\nAssuming \\mathbf{X}^T\\mathbf{X} is invertible (full rank assumption), multiply both sides by (\\mathbf{X}^T\\mathbf{X})^{-1}:\n\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n\nThis is the OLS estimator.\n\n\nA.2.6 Verifying This is a Minimum\nTo confirm this is a minimum (not maximum or saddle point), we check the second derivative:\n\n\\frac{\\partial^2 S}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T} = 2\\mathbf{X}^T\\mathbf{X}\n\nThis is the Hessian matrix. Since \\mathbf{X}^T\\mathbf{X} is positive definite (when \\mathbf{X} has full column rank), the Hessian is positive definite, confirming we have a minimum.\n\n\n\nA.3 Case 1: Simple Linear Regression (One Predictor)\n\nA.3.1 Setting Up the Problem\nConsider the following dataset with n = 5 observations:\n\n\n\nObservation\nx\ny\n\n\n\n\n1\n1\n2\n\n\n2\n2\n4\n\n\n3\n3\n5\n\n\n4\n4\n4\n\n\n5\n5\n5\n\n\n\nOur model is: y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\n\nA.3.2 Constructing the Design Matrix\nThe design matrix \\mathbf{X} includes a column of ones for the intercept:\n\n\\mathbf{X} = \\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 4 \\\\\n1 & 5\n\\end{bmatrix}, \\quad\n\\mathbf{y} = \\begin{bmatrix}\n2 \\\\\n4 \\\\\n5 \\\\\n4 \\\\\n5\n\\end{bmatrix}\n\n\n\nA.3.3 Step 1: Calculate \\mathbf{X}^T\\mathbf{X}\n\n\\mathbf{X}^T = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5\n\\end{bmatrix}\n\n\n\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 4 \\\\\n1 & 5\n\\end{bmatrix}\n\nElement-by-element calculation:\n\n(1,1) element: 1(1) + 1(1) + 1(1) + 1(1) + 1(1) = 5\n(1,2) element: 1(1) + 1(2) + 1(3) + 1(4) + 1(5) = 15\n(2,1) element: 1(1) + 2(1) + 3(1) + 4(1) + 5(1) = 15\n(2,2) element: 1(1) + 2(2) + 3(3) + 4(4) + 5(5) = 1 + 4 + 9 + 16 + 25 = 55\n\n\n\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n5 & 15 \\\\\n15 & 55\n\\end{bmatrix}\n\n\n\nA.3.4 Step 2: Calculate (\\mathbf{X}^T\\mathbf{X})^{-1}\nFor a 2 \\times 2 matrix \\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}, the inverse is:\n\n\\mathbf{A}^{-1} = \\frac{1}{ad - bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\nCalculate the determinant:\n\n\\det(\\mathbf{X}^T\\mathbf{X}) = 5(55) - 15(15) = 275 - 225 = 50\n\nCalculate the inverse:\n\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\frac{1}{50}\\begin{bmatrix}\n55 & -15 \\\\\n-15 & 5\n\\end{bmatrix} = \\begin{bmatrix}\n1.1 & -0.3 \\\\\n-0.3 & 0.1\n\\end{bmatrix}\n\n\n\nA.3.5 Step 3: Calculate \\mathbf{X}^T\\mathbf{y}\n\n\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\n2 \\\\\n4 \\\\\n5 \\\\\n4 \\\\\n5\n\\end{bmatrix}\n\nElement-by-element calculation:\n\nFirst element: 1(2) + 1(4) + 1(5) + 1(4) + 1(5) = 20\nSecond element: 1(2) + 2(4) + 3(5) + 4(4) + 5(5) = 2 + 8 + 15 + 16 + 25 = 66\n\n\n\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n20 \\\\\n66\n\\end{bmatrix}\n\n\n\nA.3.6 Step 4: Calculate \\hat{\\boldsymbol{\\beta}}\n\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n1.1 & -0.3 \\\\\n-0.3 & 0.1\n\\end{bmatrix}\n\\begin{bmatrix}\n20 \\\\\n66\n\\end{bmatrix}\n\nElement-by-element calculation:\n\n\\hat{\\beta}_0: 1.1(20) + (-0.3)(66) = 22 - 19.8 = 2.2\n\\hat{\\beta}_1: (-0.3)(20) + 0.1(66) = -6 + 6.6 = 0.6\n\n\n\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix}\n2.2 \\\\\n0.6\n\\end{bmatrix}\n\nResult: The fitted regression line is \\hat{y} = 2.2 + 0.6x\n\n\n\nA.4 Case 2: Multiple Linear Regression (Two Predictors)\n\nA.4.1 Setting Up the Problem\nConsider a dataset with n = 5 observations and two predictors:\n\n\n\nObservation\nx_1\nx_2\ny\n\n\n\n\n1\n1\n3\n3\n\n\n2\n2\n2\n5\n\n\n3\n3\n5\n7\n\n\n4\n4\n4\n9\n\n\n5\n5\n7\n10\n\n\n\nOur model is: y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i\n\n\nA.4.2 Constructing the Design Matrix\n\n\\mathbf{X} = \\begin{bmatrix}\n1 & 1 & 3 \\\\\n1 & 2 & 2 \\\\\n1 & 3 & 5 \\\\\n1 & 4 & 4 \\\\\n1 & 5 & 7\n\\end{bmatrix}, \\quad\n\\mathbf{y} = \\begin{bmatrix}\n3 \\\\\n5 \\\\\n7 \\\\\n9 \\\\\n10\n\\end{bmatrix}\n\n\n\nA.4.3 Step 1: Calculate \\mathbf{X}^T\\mathbf{X}\n\n\\mathbf{X}^T = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5 \\\\\n3 & 2 & 5 & 4 & 7\n\\end{bmatrix}\n\n\n\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5 \\\\\n3 & 2 & 5 & 4 & 7\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 1 & 3 \\\\\n1 & 2 & 2 \\\\\n1 & 3 & 5 \\\\\n1 & 4 & 4 \\\\\n1 & 5 & 7\n\\end{bmatrix}\n\nElement-by-element calculation:\nRow 1:\n\n(1,1): 1 + 1 + 1 + 1 + 1 = 5\n(1,2): 1 + 2 + 3 + 4 + 5 = 15\n(1,3): 3 + 2 + 5 + 4 + 7 = 21\n\nRow 2:\n\n(2,1): 1 + 2 + 3 + 4 + 5 = 15\n(2,2): 1 + 4 + 9 + 16 + 25 = 55\n(2,3): 3 + 4 + 15 + 16 + 35 = 73\n\nRow 3:\n\n(3,1): 3 + 2 + 5 + 4 + 7 = 21\n(3,2): 3 + 4 + 15 + 16 + 35 = 73\n(3,3): 9 + 4 + 25 + 16 + 49 = 103\n\n\n\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n5 & 15 & 21 \\\\\n15 & 55 & 73 \\\\\n21 & 73 & 103\n\\end{bmatrix}\n\n\n\nA.4.4 Step 2: Calculate (\\mathbf{X}^T\\mathbf{X})^{-1}\nFor a 3 \\times 3 matrix, we use: \\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})}\\text{adj}(\\mathbf{A})\nStep 2a: Calculate the determinant using cofactor expansion along the first row:\n\n\\det(\\mathbf{X}^T\\mathbf{X}) = 5\\begin{vmatrix}55 & 73 \\\\ 73 & 103\\end{vmatrix} - 15\\begin{vmatrix}15 & 73 \\\\ 21 & 103\\end{vmatrix} + 21\\begin{vmatrix}15 & 55 \\\\ 21 & 73\\end{vmatrix}\n\nCalculate each 2 \\times 2 determinant:\n\n\\begin{vmatrix}55 & 73 \\\\ 73 & 103\\end{vmatrix} = 55(103) - 73(73) = 5665 - 5329 = 336\n\\begin{vmatrix}15 & 73 \\\\ 21 & 103\\end{vmatrix} = 15(103) - 73(21) = 1545 - 1533 = 12\n\\begin{vmatrix}15 & 55 \\\\ 21 & 73\\end{vmatrix} = 15(73) - 55(21) = 1095 - 1155 = -60\n\n\n\\det(\\mathbf{X}^T\\mathbf{X}) = 5(336) - 15(12) + 21(-60) = 1680 - 180 - 1260 = 240\n\nStep 2b: Calculate the cofactor matrix\nThe cofactor C_{ij} is (-1)^{i+j} times the determinant of the matrix obtained by deleting row i and column j.\nC_{11} = (+1)\\begin{vmatrix}55 & 73 \\\\ 73 & 103\\end{vmatrix} = 336\nC_{12} = (-1)\\begin{vmatrix}15 & 73 \\\\ 21 & 103\\end{vmatrix} = -12\nC_{13} = (+1)\\begin{vmatrix}15 & 55 \\\\ 21 & 73\\end{vmatrix} = -60\nC_{21} = (-1)\\begin{vmatrix}15 & 21 \\\\ 73 & 103\\end{vmatrix} = -(15 \\times 103 - 21 \\times 73) = -(1545 - 1533) = -12\nC_{22} = (+1)\\begin{vmatrix}5 & 21 \\\\ 21 & 103\\end{vmatrix} = 5(103) - 21(21) = 515 - 441 = 74\nC_{23} = (-1)\\begin{vmatrix}5 & 15 \\\\ 21 & 73\\end{vmatrix} = -(5 \\times 73 - 15 \\times 21) = -(365 - 315) = -50\nC_{31} = (+1)\\begin{vmatrix}15 & 21 \\\\ 55 & 73\\end{vmatrix} = 15(73) - 21(55) = 1095 - 1155 = -60\nC_{32} = (-1)\\begin{vmatrix}5 & 21 \\\\ 15 & 73\\end{vmatrix} = -(5 \\times 73 - 21 \\times 15) = -(365 - 315) = -50\nC_{33} = (+1)\\begin{vmatrix}5 & 15 \\\\ 15 & 55\\end{vmatrix} = 5(55) - 15(15) = 275 - 225 = 50\nCofactor matrix:\n\n\\mathbf{C} = \\begin{bmatrix}\n336 & -12 & -60 \\\\\n-12 & 74 & -50 \\\\\n-60 & -50 & 50\n\\end{bmatrix}\n\nStep 2c: Calculate the adjugate (transpose of cofactor matrix):\n\n\\text{adj}(\\mathbf{X}^T\\mathbf{X}) = \\mathbf{C}^T = \\begin{bmatrix}\n336 & -12 & -60 \\\\\n-12 & 74 & -50 \\\\\n-60 & -50 & 50\n\\end{bmatrix}\n\nStep 2d: Calculate the inverse:\n\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\frac{1}{240}\\begin{bmatrix}\n336 & -12 & -60 \\\\\n-12 & 74 & -50 \\\\\n-60 & -50 & 50\n\\end{bmatrix} = \\begin{bmatrix}\n1.4 & -0.05 & -0.25 \\\\\n-0.05 & 0.308\\overline{3} & -0.208\\overline{3} \\\\\n-0.25 & -0.208\\overline{3} & 0.208\\overline{3}\n\\end{bmatrix}\n\nFor simplicity in calculations, we’ll use:\n\n(\\mathbf{X}^T\\mathbf{X})^{-1} \\approx \\begin{bmatrix}\n1.400 & -0.050 & -0.250 \\\\\n-0.050 & 0.308 & -0.208 \\\\\n-0.250 & -0.208 & 0.208\n\\end{bmatrix}\n\n\n\nA.4.5 Step 3: Calculate \\mathbf{X}^T\\mathbf{y}\n\n\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5 \\\\\n3 & 2 & 5 & 4 & 7\n\\end{bmatrix}\n\\begin{bmatrix}\n3 \\\\\n5 \\\\\n7 \\\\\n9 \\\\\n10\n\\end{bmatrix}\n\nElement-by-element calculation:\n\nFirst: 3 + 5 + 7 + 9 + 10 = 34\nSecond: 3 + 10 + 21 + 36 + 50 = 120\nThird: 9 + 10 + 35 + 36 + 70 = 160\n\n\n\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n34 \\\\\n120 \\\\\n160\n\\end{bmatrix}\n\n\n\nA.4.6 Step 4: Calculate \\hat{\\boldsymbol{\\beta}}\n\n\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix}\n1.400 & -0.050 & -0.250 \\\\\n-0.050 & 0.308 & -0.208 \\\\\n-0.250 & -0.208 & 0.208\n\\end{bmatrix}\n\\begin{bmatrix}\n34 \\\\\n120 \\\\\n160\n\\end{bmatrix}\n\nElement-by-element calculation:\n\n\\hat{\\beta}_0 = 1.400(34) - 0.050(120) - 0.250(160) = 47.6 - 6.0 - 40.0 = 1.6\n\\hat{\\beta}_1 = -0.050(34) + 0.308(120) - 0.208(160) = -1.7 + 37.0 - 33.3 = 2.0\n\\hat{\\beta}_2 = -0.250(34) - 0.208(120) + 0.208(160) = -8.5 - 25.0 + 33.3 = -0.2\n\n\n\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix}\n1.6 \\\\\n2.0 \\\\\n-0.2\n\\end{bmatrix}\n\nResult: The fitted regression is \\hat{y} = 1.6 + 2.0x_1 - 0.2x_2\n(Note: With exact fractions, the result would be slightly different, but this illustrates the manual calculation process.)\n\n\n\nA.5 The Gauss-Markov Theorem\n\nA.5.1 Statement of the Theorem\nUnder the classical linear regression assumptions, the OLS estimator \\hat{\\boldsymbol{\\beta}} is the Best Linear Unbiased Estimator (BLUE) of \\boldsymbol{\\beta}.\n“Best” means it has the minimum variance among all linear unbiased estimators.\n\n\nA.5.2 Required Assumptions\n\nLinearity: \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\nStrict Exogeneity: E[\\boldsymbol{\\epsilon}|\\mathbf{X}] = \\mathbf{0}\nNo Perfect Multicollinearity: \\mathbf{X} has full column rank, i.e., \\text{rank}(\\mathbf{X}) = p+1\nSpherical Errors: \\text{Var}(\\boldsymbol{\\epsilon}|\\mathbf{X}) = \\sigma^2\\mathbf{I}_n\n\nHomoscedasticity: \\text{Var}(\\epsilon_i) = \\sigma^2 for all i\nNo autocorrelation: \\text{Cov}(\\epsilon_i, \\epsilon_j) = 0 for i \\neq j\n\n\n\n\nA.5.3 Proof that OLS is Unbiased\nStarting with: \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\nSubstitute \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}:\n\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon})\n\n\n= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}\n\n\n= \\mathbf{I}\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}\n\n\n= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}\n\nTaking expectations:\n\nE[\\hat{\\boldsymbol{\\beta}}|\\mathbf{X}] = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T E[\\boldsymbol{\\epsilon}|\\mathbf{X}]\n\nSince E[\\boldsymbol{\\epsilon}|\\mathbf{X}] = \\mathbf{0}:\n\nE[\\hat{\\boldsymbol{\\beta}}|\\mathbf{X}] = \\boldsymbol{\\beta}\n\nTherefore, OLS is unbiased.\n\n\nA.5.4 Variance of the OLS Estimator\nFrom \\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}:\n\n\\text{Var}(\\hat{\\boldsymbol{\\beta}}|\\mathbf{X}) = \\text{Var}[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}|\\mathbf{X}]\n\nSince (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T is non-random conditional on \\mathbf{X}:\n\n= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\text{Var}(\\boldsymbol{\\epsilon}|\\mathbf{X}) \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\nWith spherical errors \\text{Var}(\\boldsymbol{\\epsilon}|\\mathbf{X}) = \\sigma^2\\mathbf{I}:\n\n= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T (\\sigma^2\\mathbf{I}) \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\n\n= \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\n\n= \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\n\nThis is the variance-covariance matrix of \\hat{\\boldsymbol{\\beta}}.\n\n\nA.5.5 Example: Variance Calculation for Simple Regression\nFrom Case 1, we had:\n\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\begin{bmatrix}\n1.1 & -0.3 \\\\\n-0.3 & 0.1\n\\end{bmatrix}\n\nWe estimate \\sigma^2 using the residual sum of squares:\n\n\\hat{\\sigma}^2 = \\frac{\\text{RSS}}{n - (p + 1)} = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n - 2}\n\nFor our example, the fitted values are: \\hat{y}_i = 2.2 + 0.6x_i\n\n\n\n\n\n\n\n\n\n\ni\ny_i\n\\hat{y}_i\ny_i - \\hat{y}_i\n(y_i - \\hat{y}_i)^2\n\n\n\n\n1\n2\n2.8\n-0.8\n0.64\n\n\n2\n4\n3.4\n0.6\n0.36\n\n\n3\n5\n4.0\n1.0\n1.00\n\n\n4\n4\n4.6\n-0.6\n0.36\n\n\n5\n5\n5.2\n-0.2\n0.04\n\n\n\n\n\\text{RSS} = 0.64 + 0.36 + 1.00 + 0.36 + 0.04 = 2.4\n\n\n\\hat{\\sigma}^2 = \\frac{2.4}{5 - 2} = \\frac{2.4}{3} = 0.8\n\nThe variance-covariance matrix of \\hat{\\boldsymbol{\\beta}} is:\n\n\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = 0.8 \\begin{bmatrix}\n1.1 & -0.3 \\\\\n-0.3 & 0.1\n\\end{bmatrix} = \\begin{bmatrix}\n0.88 & -0.24 \\\\\n-0.24 & 0.08\n\\end{bmatrix}\n\nStandard errors:\n\n\\text{SE}(\\hat{\\beta}_0) = \\sqrt{0.88} = 0.938\n\\text{SE}(\\hat{\\beta}_1) = \\sqrt{0.08} = 0.283\n\nThese standard errors are used for hypothesis testing and confidence intervals.\n\n\nA.5.6 Why OLS is “Best” (Sketch of Proof)\nThe Gauss-Markov theorem states that among all linear unbiased estimators, OLS has the minimum variance.\nConsider any other linear unbiased estimator:\n\n\\tilde{\\boldsymbol{\\beta}} = \\mathbf{C}\\mathbf{y}\n\nwhere \\mathbf{C} is some matrix. For unbiasedness:\n\nE[\\tilde{\\boldsymbol{\\beta}}] = \\mathbf{C}E[\\mathbf{y}] = \\mathbf{C}\\mathbf{X}\\boldsymbol{\\beta} = \\boldsymbol{\\beta}\n\nThis requires \\mathbf{C}\\mathbf{X} = \\mathbf{I}.\nWe can write \\mathbf{C} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D} for some matrix \\mathbf{D} where \\mathbf{D}\\mathbf{X} = \\mathbf{0}.\nThe variance of \\tilde{\\boldsymbol{\\beta}} is:\n\n\\text{Var}(\\tilde{\\boldsymbol{\\beta}}) = \\sigma^2\\mathbf{C}\\mathbf{C}^T\n\n\n= \\sigma^2[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D}][(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D}]^T\n\n\n= \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} + \\sigma^2\\mathbf{D}\\mathbf{D}^T\n\nSince \\mathbf{D}\\mathbf{D}^T is positive semi-definite:\n\n\\text{Var}(\\tilde{\\boldsymbol{\\beta}}) - \\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2\\mathbf{D}\\mathbf{D}^T \\geq \\mathbf{0}\n\nTherefore, OLS has the smallest variance among all linear unbiased estimators.\n\n\n\nA.6 Key Takeaways\n\nMatrix notation provides an elegant and scalable framework for regression that extends naturally from simple to multiple regression.\nThe OLS minimization derives from setting \\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = \\mathbf{0}, yielding the normal equations \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}.\nThe OLS solution \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} requires that \\mathbf{X}^T\\mathbf{X} be invertible (no perfect multicollinearity).\nUnder the Gauss-Markov assumptions, OLS is BLUE: Best (minimum variance), Linear, Unbiased Estimator.\nThe variance of OLS estimates is \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}, which can be used for inference.\nKey linear algebra tools include matrix transpose, multiplication, determinant, inverse, and rank—all essential for understanding OLS.\nAssumption violations (e.g., heteroscedasticity, autocorrelation) don’t make OLS biased, but they invalidate the “best” property and standard inference procedures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_pl.html",
    "href": "correg_pl.html",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "",
    "text": "10.1 Wprowadzenie\nRóżnica między korelacją (correlation) a przyczynowością/kausalnością (causation) to jedno z podstawowych wyzwań w analizie statystycznej. Korelacja mierzy statystyczny związek między zmiennymi, natomiast przyczynowość oznacza bezpośredni wpływ jednej zmiennej na drugą.\nZależności statystyczne stanowią fundament podejmowania decyzji opartych na danych w wielu dyscyplinach — od ekonomii i zdrowia publicznego po psychologię i nauki o środowisku. Zrozumienie, kiedy związek wskazuje jedynie na asocjację (association), a kiedy na prawdziwą kausalność (genuine causality), jest kluczowe dla poprawnych wniosków i skutecznych rekomendacji politycznych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kowariancja-covariance",
    "href": "correg_pl.html#kowariancja-covariance",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.2 Kowariancja (Covariance)",
    "text": "10.2 Kowariancja (Covariance)\nKowariancja (covariance) mierzy, w jaki sposób dwie zmienne współzmieniają się, wskazując zarówno kierunek, jak i siłę ich liniowego związku.\nWzór (z próby):\n\n\\operatorname{cov}(X,Y)\n= \\frac{\\sum_{i=1}^{n} (x_i - \\bar x)(y_i - \\bar y)}{n - 1}.\n\nGdzie:\n\nx_i i y_i to poszczególne obserwacje,\n\\bar{x} i \\bar{y} to średnie odpowiednio zmiennych X i Y,\nn to liczba obserwacji,\ndzielimy przez (n-1), ponieważ liczymy kowariancję z próby (tzw. poprawka Bessela; Bessel’s correction).\n\n\nObliczenia ręczne krok po kroku (Step-by-Step Manual Calculation Process)\nPrzykład 1: Godziny nauki a wyniki testu\nDane:\n\nGodziny nauki (X): 2, 4, 6, 8, 10\nWyniki testu (Y): 65, 70, 80, 85, 95\n\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nOblicz średnie\n\\bar{x}=\\frac{2+4+6+8+10}{5}=6 godz.\n\n\n\n\n\\bar{y}=\\frac{65+70+80+85+95}{5}=79 pkt\n\n\n2\nOdchylenia od średnich\n(x_i-\\bar{x}): -4, -2, 0, 2, 4\n\n\n\n\n(y_i-\\bar{y}): -14, -9, 1, 6, 16\n\n\n3\nIloczyny odchyleń\n(x_i-\\bar{x})(y_i-\\bar{y}): 56, 18, 0, 12, 64\n\n\n4\nSuma iloczynów\n\\sum = 56+18+0+12+64=150\n\n\n5\nPodziel przez (n-1)\n\\operatorname{cov}(X,Y)=\\frac{150}{5-1}=\\frac{150}{4}=37.5\n\n\n\nWeryfikacja w R (R Verification):\n\n# Definicja danych\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Kowariancja\ncov_manual &lt;- cov(study_hours, test_scores)\nprint(paste(\"Covariance:\", round(cov_manual, 2)))\n\n[1] \"Covariance: 37.5\"\n\n# Weryfikacja krok po kroku\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\nproducts &lt;- x_dev * y_dev\ncov_calculated &lt;- sum(products) / (length(study_hours) - 1)\n\n# Tabela obliczeń\ndata.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_deviation = x_dev,\n  Y_deviation = y_dev,\n  Product = products\n)\n\n   X  Y X_deviation Y_deviation Product\n1  2 65          -4         -14      56\n2  4 70          -2          -9      18\n3  6 80           0           1       0\n4  8 85           2           6      12\n5 10 95           4          16      64\n\n\nInterpretacja: Dodatnia kowariancja (37.5) wskazuje, że wraz ze wzrostem liczby godzin nauki rosną także wyniki testu — zmienne mają tendencję do wspólnego wzrostu.\n\n\nZadanie ćwiczeniowe z rozwiązaniem (Practice Problem with Solution)\nPolicz ręcznie kowariancję dla:\n\nTemperatura (°F): 32, 50, 68, 86, 95\nSprzedaż lodów ($): 100, 200, 400, 600, 800\n\nRozwiązanie:\n\n\n\n\n\n\n\nKrok\nObliczenie\n\n\n\n\n1. Średnie\n\\bar{x}=\\frac{32+50+68+86+95}{5}=66.2^{\\circ}\\mathrm{F}\n\n\n\n\\bar{y}=\\frac{100+200+400+600+800}{5}=420\n\n\n2. Odchylenia\nX: -34.2, -16.2, 1.8, 19.8, 28.8\n\n\n\nY: -320, -220, -20, 180, 380\n\n\n3. Iloczyny\n10944, 3564, -36, 3564, 10944\n\n\n4. Suma\n28980\n\n\n5. Kowariancja\n\\frac{28980}{4}=7245\n\n\n\n\n# Weryfikacja zadania\ntemp &lt;- c(32, 50, 68, 86, 95)\nsales &lt;- c(100, 200, 400, 600, 800)\nprint(paste(\"Covariance for practice problem:\", round(cov(temp, sales), 2)))\n\n[1] \"Covariance for practice problem: 7245\"",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#współczynnik-korelacji-correlation-coefficient",
    "href": "correg_pl.html#współczynnik-korelacji-correlation-coefficient",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.3 Współczynnik korelacji (Correlation Coefficient)",
    "text": "10.3 Współczynnik korelacji (Correlation Coefficient)\nWspółczynnik korelacji (correlation coefficient) standaryzuje kowariancję, usuwając zależność od skali i przyjmując wartości od -1 do +1.\n\nWskazówki interpretacyjne (Interpretation Guidelines)\n\n\n\n\n\n\n\n\n\nWartość korelacji\nSiła\nInterpretacja\nPrzykład\n\n\n\n\n±0.90 do ±1.00\nBardzo silna\nNiemal doskonały związek\nWzrost rodziców i dzieci\n\n\n±0.70 do ±0.89\nSilna\nZmienne silnie powiązane\nCzas nauki i oceny\n\n\n±0.50 do ±0.69\nUmiarkowana\nUmiarkowany związek\nĆwiczenia a spadek masy\n\n\n±0.30 do ±0.49\nSłaba\nSłaby związek\nRozmiar buta a umiejętność czytania\n\n\n±0.00 do ±0.29\nBardzo słaba/brak\nZnikomy lub brak związku\nMiesiąc urodzenia a inteligencja\n\n\n\n\n\nWizualizacja typów zależności korelacyjnych (Types of Correlations Visualization)\n\n# Generowanie przykładowych danych dla różnych wzorców korelacji\nn &lt;- 100\n\n# Dodatnia zależność liniowa\nset.seed(123)\nyears_education &lt;- rnorm(n, 14, 3)\nannual_income &lt;- 15000 + 3500 * years_education + rnorm(n, 0, 10000)\npos_cor &lt;- round(cor(years_education, annual_income), 3)\n\n# Ujemna zależność liniowa\nscreen_time &lt;- runif(n, 1, 8)\nsleep_hours &lt;- 9 - 0.5 * screen_time + rnorm(n, 0, 0.5)\nneg_cor &lt;- round(cor(screen_time, sleep_hours), 3)\n\n# Brak korelacji\nx_random &lt;- rnorm(n, 50, 10)\ny_random &lt;- rnorm(n, 50, 10)\nno_cor &lt;- round(cor(x_random, y_random), 3)\n\n# Nieliniowa zależność (kwadratowa)\nhours_studied &lt;- runif(n, 0, 12)\ntest_score &lt;- -2 * (hours_studied - 6)^2 + 90 + rnorm(n, 0, 5)\nnonlin_cor &lt;- round(cor(hours_studied, test_score), 3)\n\n# Ramki danych z wartościami korelacji\npositive_data &lt;- data.frame(\n  x = years_education, \n  y = annual_income,\n  label = paste0(\"Dodatnia liniowa (r = \", pos_cor, \")\")\n)\n\nnegative_data &lt;- data.frame(\n  x = screen_time, \n  y = sleep_hours,\n  label = paste0(\"Ujemna liniowa (r = \", neg_cor, \")\")\n)\n\nno_corr_data &lt;- data.frame(\n  x = x_random, \n  y = y_random,\n  label = paste0(\"Brak korelacji (r = \", no_cor, \")\")\n)\n\nnonlinear_data &lt;- data.frame(\n  x = hours_studied, \n  y = test_score,\n  label = paste0(\"Nieliniowa (Pearson r = \", nonlin_cor, \")\")\n)\n\n# Połączenie danych\nall_data &lt;- rbind(positive_data, negative_data, no_corr_data, nonlinear_data)\n\n# Wykres fasetowy\nggplot(all_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE, alpha = 0.3) +\n  facet_wrap(~ label, scales = \"free\", ncol = 2) +\n  labs(\n    title = \"Różne typy korelacji\",\n    subtitle = \"Linia regresji liniowej (na czerwono) z pasmem ufności\",\n    x = \"\", \n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, color = \"gray50\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#korelacja-pearsona-pearson-correlation",
    "href": "correg_pl.html#korelacja-pearsona-pearson-correlation",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.4 Korelacja Pearsona (Pearson Correlation)",
    "text": "10.4 Korelacja Pearsona (Pearson Correlation)\nWzór:\n\nr\n= \\frac{\\operatorname{cov}(X,Y)}{s_X\\, s_Y}\n= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}\n{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2}\\,\\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}}.\n\n\nPełny przykład obliczeń ręcznych (Complete Manual Calculation Example)\nNa danych o godzinach nauki:\n\nGodziny nauki (X): 2, 4, 6, 8, 10\nWyniki testu (Y): 65, 70, 80, 85, 95\n\nSzczegółowe kroki:\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nKowariancja\nZ powyżej: \\operatorname{cov}(X,Y) = 37.5\n\n\n2\nKwadraty odchyleń\n\n\n\n\nDla X\n(x_i-\\bar{x})^2: 16, 4, 0, 4, 16\n\n\n\n\nSuma = 40\n\n\n\nDla Y\n(y_i-\\bar{y})^2: 196, 81, 1, 36, 256\n\n\n\n\nSuma = 570\n\n\n3\nOdchylenia standardowe (standard deviations)\n\n\n\n\ns_X\ns_X=\\sqrt{\\frac{40}{4}}=\\sqrt{10}=3.162\n\n\n\ns_Y\ns_Y=\\sqrt{\\frac{570}{4}}=\\sqrt{142.5}=11.937\n\n\n4\nKorelacja\nr=\\frac{37.5}{3.162 \\times 11.937}\n\n\n\n\nr=\\frac{37.5}{37.73}\\approx 0.994\n\n\n\n\n# Weryfikacja obliczeń\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Współczynnik korelacji Pearsona\nr_value &lt;- cor(study_hours, test_scores, method = \"pearson\")\nprint(paste(\"Pearson correlation coefficient:\", round(r_value, 3)))\n\n[1] \"Pearson correlation coefficient: 0.993\"\n\n# Obliczenia szczegółowe\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\n\n# Tabela obliczeń\ncalc_table &lt;- data.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_dev = x_dev,\n  Y_dev = y_dev,\n  X_dev_sq = x_dev^2,\n  Y_dev_sq = y_dev^2,\n  Product = x_dev * y_dev\n)\nprint(calc_table)\n\n   X  Y X_dev Y_dev X_dev_sq Y_dev_sq Product\n1  2 65    -4   -14       16      196      56\n2  4 70    -2    -9        4       81      18\n3  6 80     0     1        0        1       0\n4  8 85     2     6        4       36      12\n5 10 95     4    16       16      256      64\n\n# Statystyki podsumowujące\ncat(\"\\nSummary:\")\n\n\nSummary:\n\ncat(\"\\nSum of (X-mean)^2:\", sum(x_dev^2))\n\n\nSum of (X-mean)^2: 40\n\ncat(\"\\nSum of (Y-mean)^2:\", sum(y_dev^2))\n\n\nSum of (Y-mean)^2: 570\n\ncat(\"\\nSum of products:\", sum(x_dev * y_dev))\n\n\nSum of products: 150\n\ncat(\"\\nStandard deviation of X:\", sd(study_hours))\n\n\nStandard deviation of X: 3.162278\n\ncat(\"\\nStandard deviation of Y:\", sd(test_scores))\n\n\nStandard deviation of Y: 11.93734\n\n# Przedział ufności i p-value\ncor_test &lt;- cor.test(study_hours, test_scores, method = \"pearson\")\nprint(cor_test)\n\n\n    Pearson's product-moment correlation\n\ndata:  study_hours and test_scores\nt = 15, df = 3, p-value = 0.0006431\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8994446 0.9995859\nsample estimates:\n      cor \n0.9933993 \n\n\nInterpretacja: r \\approx 0.994 wskazuje na niemal doskonały dodatni liniowy związek między godzinami nauki a wynikiem testu. Wartość p &lt; 0.05 sugeruje statystyczną istotność tej zależności.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#korelacja-rang-spearmana-spearman-rank-correlation",
    "href": "correg_pl.html#korelacja-rang-spearmana-spearman-rank-correlation",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.5 Korelacja rang Spearmana (Spearman Rank Correlation)",
    "text": "10.5 Korelacja rang Spearmana (Spearman Rank Correlation)\nKorelacja Spearmana mierzy monotoniczne zależności, używając rang zamiast surowych wartości.\nWzór:\n\n\\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2 - 1)},\n\ngdzie d_i to różnica rang dla obserwacji i.\n\nPełny przykład obliczeń ręcznych (Complete Manual Example)\nDane: Wyniki z matematyki i angielskiego\n\n\n\nUczeń\nMatematyka\nAngielski\n\n\n\n\nA\n78\n82\n\n\nB\n85\n90\n\n\nC\n88\n88\n\n\nD\n92\n94\n\n\nE\n95\n96\n\n\n\nRangowanie i obliczenia:\n\n\n\n\n\n\n\n\n\n\n\n\nUczeń\nWynik z mat.\nRanga mat.\nWynik z ang.\nRanga ang.\nd = \\text{ranga mat.} - \\text{ranga ang.}\nd^2\n\n\n\n\nA\n78\n1\n82\n1\n0\n0\n\n\nB\n85\n2\n90\n3\n-1\n1\n\n\nC\n88\n3\n88\n2\n1\n1\n\n\nD\n92\n4\n94\n4\n0\n0\n\n\nE\n95\n5\n96\n5\n0\n0\n\n\n\n\n\n\n\nSuma:\n2\n\n\n\nObliczenie:\n\n\\rho = 1 - \\frac{6 \\times 2}{5(25-1)} = 1 - \\frac{12}{120} = 0.9.\n\n\n# Dane\nmath_scores &lt;- c(78, 85, 88, 92, 95)\nenglish_scores &lt;- c(82, 90, 88, 94, 96)\n\n# Rangi\nrank_table &lt;- data.frame(\n  Student = LETTERS[1:5],\n  Math_Score = math_scores,\n  Math_Rank = rank(math_scores),\n  English_Score = english_scores,\n  English_Rank = rank(english_scores)\n)\n\nrank_table$d &lt;- rank_table$Math_Rank - rank_table$English_Rank\nrank_table$d_squared &lt;- rank_table$d^2\n\nprint(rank_table)\n\n  Student Math_Score Math_Rank English_Score English_Rank  d d_squared\n1       A         78         1            82            1  0         0\n2       B         85         2            90            3 -1         1\n3       C         88         3            88            2  1         1\n4       D         92         4            94            4  0         0\n5       E         95         5            96            5  0         0\n\ncat(\"\\nSum of d^2:\", sum(rank_table$d_squared))\n\n\nSum of d^2: 2\n\n# Korelacja Spearmana\nrho &lt;- cor(math_scores, english_scores, method = \"spearman\")\ncat(\"\\nSpearman correlation (R):\", round(rho, 3))\n\n\nSpearman correlation (R): 0.9\n\n# Obliczenie ręczne\nn &lt;- length(math_scores)\nrho_manual &lt;- 1 - (6 * sum(rank_table$d_squared)) / (n * (n^2 - 1))\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.9",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#tabele-krzyżowe-cross-tabulation-i-dane-kategoryczne",
    "href": "correg_pl.html#tabele-krzyżowe-cross-tabulation-i-dane-kategoryczne",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.6 Tabele krzyżowe (Cross-tabulation) i dane kategoryczne",
    "text": "10.6 Tabele krzyżowe (Cross-tabulation) i dane kategoryczne\nTabela krzyżowa (cross-tabulation, contingency table) pokazuje zależności między zmiennymi kategorycznymi.\n\n# Bardziej realistyczne dane przykładowe\nset.seed(123)\nn_total &lt;- 120\n\n# Poziom edukacji a zatrudnienie\neducation &lt;- factor(\n  c(rep(\"High School\", 40), \n    rep(\"College\", 50), \n    rep(\"Graduate\", 30)),\n  levels = c(\"High School\", \"College\", \"Graduate\")\n)\n\n# Status zatrudnienia z prawdopodobieństwami zależnymi od edukacji\nemployment &lt;- factor(\n  c(\n    # High School: 75% employed\n    sample(c(rep(\"Employed\", 30), rep(\"Unemployed\", 10)), 40),\n    # College: 90% employed  \n    sample(c(rep(\"Employed\", 45), rep(\"Unemployed\", 5)), 50),\n    # Graduate: 95% employed\n    sample(c(rep(\"Employed\", 28), rep(\"Unemployed\", 2)), 30)\n  ),\n  levels = c(\"Employed\", \"Unemployed\")\n)\n\n# Tabela kontyngencji\ncontingency_table &lt;- table(education, employment)\nprint(\"Contingency Table:\")\n\n[1] \"Contingency Table:\"\n\nprint(contingency_table)\n\n             employment\neducation     Employed Unemployed\n  High School       30         10\n  College           45          5\n  Graduate          28          2\n\n# Procenty w wierszach\nprint(\"\\nRow Percentages (% within each education level):\")\n\n[1] \"\\nRow Percentages (% within each education level):\"\n\nprint(round(prop.table(contingency_table, 1) * 100, 1))\n\n             employment\neducation     Employed Unemployed\n  High School     75.0       25.0\n  College         90.0       10.0\n  Graduate        93.3        6.7\n\n# Test niezależności chi-kwadrat (Chi-square test)\nchi_test &lt;- chisq.test(contingency_table)\n\nWarning in chisq.test(contingency_table): Chi-squared approximation may be\nincorrect\n\nprint(chi_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 5.9623, df = 2, p-value = 0.05073",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#ćwiczenia-praktyczne-z-rozwiązaniami-practical-exercises-with-solutions",
    "href": "correg_pl.html#ćwiczenia-praktyczne-z-rozwiązaniami-practical-exercises-with-solutions",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.7 Ćwiczenia praktyczne z rozwiązaniami (Practical Exercises with Solutions)",
    "text": "10.7 Ćwiczenia praktyczne z rozwiązaniami (Practical Exercises with Solutions)\n\nĆwiczenie 1: Ręczne obliczenie korelacji Pearsona (Calculate Pearson Correlation Manually)\nDane:\n\nWzrost (cale): 66, 68, 70, 72, 74\nWaga (funty): 140, 155, 170, 185, 200\n\nRozwiązanie:\n\n# Dane\nheight &lt;- c(66, 68, 70, 72, 74)\nweight &lt;- c(140, 155, 170, 185, 200)\n\n# Krok 1: Średnie\nh_mean &lt;- mean(height)  # 70\nw_mean &lt;- mean(weight)  # 170\n\n# Krok 2: Odchylenia i iloczyny\ncalculation_df &lt;- data.frame(\n  Height = height,\n  Weight = weight,\n  H_dev = height - h_mean,\n  W_dev = weight - w_mean,\n  H_dev_sq = (height - h_mean)^2,\n  W_dev_sq = (weight - w_mean)^2,\n  Product = (height - h_mean) * (weight - w_mean)\n)\n\nprint(calculation_df)\n\n  Height Weight H_dev W_dev H_dev_sq W_dev_sq Product\n1     66    140    -4   -30       16      900     120\n2     68    155    -2   -15        4      225      30\n3     70    170     0     0        0        0       0\n4     72    185     2    15        4      225      30\n5     74    200     4    30       16      900     120\n\n# Krok 3: Korelacja\ncov_hw &lt;- sum(calculation_df$Product) / (length(height) - 1)\nsd_h &lt;- sqrt(sum(calculation_df$H_dev_sq) / (length(height) - 1))\nsd_w &lt;- sqrt(sum(calculation_df$W_dev_sq) / (length(weight) - 1))\nr_manual &lt;- cov_hw / (sd_h * sd_w)\n\ncat(\"\\nManual calculation:\")\n\n\nManual calculation:\n\ncat(\"\\nCovariance:\", round(cov_hw, 2))\n\n\nCovariance: 75\n\ncat(\"\\nSD Height:\", round(sd_h, 2))\n\n\nSD Height: 3.16\n\ncat(\"\\nSD Weight:\", round(sd_w, 2))\n\n\nSD Weight: 23.72\n\ncat(\"\\nCorrelation:\", round(r_manual, 3))\n\n\nCorrelation: 1\n\n# Weryfikacja w R\ncat(\"\\n\\nR calculation:\", round(cor(height, weight), 3))\n\n\n\nR calculation: 1\n\n\n\n\nĆwiczenie 2: Ręczne obliczenie korelacji Spearmana (Calculate Spearman Correlation Manually)\nDane:\n\nRangi uczniów z matematyki: 1, 3, 2, 5, 4\nRangi uczniów z nauk ścisłych (science): 2, 4, 1, 5, 3\n\nRozwiązanie:\n\n# Rangi (już zrankowane)\nmath_rank &lt;- c(1, 3, 2, 5, 4)\nscience_rank &lt;- c(2, 4, 1, 5, 3)\n\n# Różnice\nd &lt;- math_rank - science_rank\nd_squared &lt;- d^2\n\n# Tabela\nrank_df &lt;- data.frame(\n  Student = 1:5,\n  Math_Rank = math_rank,\n  Science_Rank = science_rank,\n  d = d,\n  d_squared = d_squared\n)\n\nprint(rank_df)\n\n  Student Math_Rank Science_Rank  d d_squared\n1       1         1            2 -1         1\n2       2         3            4 -1         1\n3       3         2            1  1         1\n4       4         5            5  0         0\n5       5         4            3  1         1\n\n# Korelacja Spearmana\nn &lt;- length(math_rank)\nsum_d_sq &lt;- sum(d_squared)\nrho_manual &lt;- 1 - (6 * sum_d_sq) / (n * (n^2 - 1))\n\ncat(\"\\nSum of d^2:\", sum_d_sq)\n\n\nSum of d^2: 4\n\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.8\n\ncat(\"\\nSpearman correlation (R):\", round(cor(math_rank, science_rank, method = \"spearman\"), 3))\n\n\nSpearman correlation (R): 0.8\n\n\n\n\nĆwiczenie 3: Interpretacja wyników (Interpretation Practice)\nZinterpretuj następujące wartości korelacji:\n\nr = 0.85 między godzinami treningu a wynikiem sprawdzianu sprawności\nOdpowiedź: Silny dodatni związek. Wraz ze wzrostem liczby godzin treningu wyniki istotnie rosną.\nr = -0.72 między temperaturą na zewnątrz a kosztami ogrzewania\nOdpowiedź: Silny ujemny związek. Wraz ze wzrostem temperatury koszty ogrzewania wyraźnie maleją.\nr = 0.12 między rozmiarem buta a inteligencją\nOdpowiedź: Bardzo słaby/brak istotnego związku. Zmienne są praktycznie niezależne.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#najważniejsze-rzeczy-do-zapamiętania-important-points-to-remember",
    "href": "correg_pl.html#najważniejsze-rzeczy-do-zapamiętania-important-points-to-remember",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.8 Najważniejsze rzeczy do zapamiętania (Important Points to Remember)",
    "text": "10.8 Najważniejsze rzeczy do zapamiętania (Important Points to Remember)\n\nKorelacja mierzy siłę związku: Wartości od -1 do +1.\nKorelacja ≠ przyczynowość (Correlation ≠ Causation): Wysoka korelacja nie dowodzi wpływu jednej zmiennej na drugą.\nDobierz właściwą metodę:\n\nPearson: Związki liniowe dla danych ciągłych.\nSpearman: Związki monotoniczne lub dane rangowe.\n\nSprawdź założenia:\n\nPearson: liniowość i (w praktyce) rozkład zbliżony do normalnego.\nSpearman: wymagana jedynie monotoniczność.\n\nUwaga na obserwacje odstające (outliers): Mogą silnie wpływać na korelację Pearsona.\nZawsze wizualizuj dane: Wykresy pomagają ocenić kształt zależności.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#podsumowanie-drzewko-decyzyjne-do-analizy-korelacji-summary-decision-tree-for-correlation-analysis",
    "href": "correg_pl.html#podsumowanie-drzewko-decyzyjne-do-analizy-korelacji-summary-decision-tree-for-correlation-analysis",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.9 Podsumowanie: drzewko decyzyjne do analizy korelacji (Summary: Decision Tree for Correlation Analysis)",
    "text": "10.9 Podsumowanie: drzewko decyzyjne do analizy korelacji (Summary: Decision Tree for Correlation Analysis)\n\n\n\nWYBÓR WŁAŚCIWEJ MIARY KORELACJI:\n\nCzy dane są liczbowe (numeryczne)?\n├─ TAK → Czy związek jest liniowy?\n│   ├─ TAK → Użyj korelacji PEARSONA\n│   └─ NIE → Czy związek jest monotoniczny?\n│       ├─ TAK → Użyj korelacji SPEARMANA\n│       └─ NIE → Rozważ metody nieliniowe\n└─ NIE → Czy dane są porządkowe (rangi)?\n    ├─ TAK → Użyj korelacji SPEARMANA\n    └─ NIE → Użyj TABEL KRZYŻOWYCH dla danych kategorycznych\n\n\n\nŚciąga (Quick Reference Card)\n\n\n\n\n\n\n\n\n\nMiara\nKiedy używać (Use When)\nWzór (Formula)\nZakres (Range)\n\n\n\n\nKowariancja (Covariance)\nWstępne badanie związku\n\\displaystyle \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n-\\infty do +\\infty\n\n\nPearson r\nZwiązki liniowe, dane ciągłe\n\\displaystyle \\frac{\\operatorname{cov}(X,Y)}{s_X s_Y}\n-1 do +1\n\n\nSpearman \\rho\nZwiązki monotoniczne, rangi\n\\displaystyle 1-\\frac{6\\sum d_i^2}{n(n^2-1)}\n-1 do +1\n\n\nTabele krzyżowe (Cross-tabs)\nZmienne kategoryczne\nZliczenia częstości\nn/d",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#analiza-regresji-ols-ordinary-least-squares-przewodnik-na-start-a-quick-start-guide",
    "href": "correg_pl.html#analiza-regresji-ols-ordinary-least-squares-przewodnik-na-start-a-quick-start-guide",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.10 Analiza regresji OLS (Ordinary Least Squares): przewodnik na start (A Quick-start Guide)",
    "text": "10.10 Analiza regresji OLS (Ordinary Least Squares): przewodnik na start (A Quick-start Guide)\n\n\n\n\n\n\nAnaliza regresji OLS: przewodnik na start\n\n\n\n\nWprowadzenie: czym jest analiza regresji?\nAnaliza regresji (regression analysis) pomaga zrozumieć i mierzyć zależności między obserwowalnymi wielkościami. To zestaw narzędzi matematycznych do identyfikowania wzorców w danych, które umożliwiają prognozowanie (prediction).\nRozważ pytania badawcze:\n\nJak czas nauki wpływa na wynik testu?\nJak doświadczenie wpływa na wynagrodzenie?\nJak wydatki na reklamę oddziałują na sprzedaż?\n\nRegresja dostarcza systematycznych metod, by na te pytania odpowiadać na podstawie realnych danych.\n\n\nPunkt wyjścia: prosty przykład\nZacznijmy od konkretu. Zebrano dane o 20 studentach z Twojej klasy:\n\n\n\nStudent\nStudy Hours\nExam Score\n\n\n\n\nAlex\n2\n68\n\n\nBeth\n4\n74\n\n\nCarlos\n6\n85\n\n\nDiana\n8\n91\n\n\n…\n…\n…\n\n\n\nPo narysowaniu wykresu punktowego (scatter plot) chcesz znaleźć prostą, która najlepiej opisuje związek między godzinami nauki a wynikiem.\nAle co znaczy „najlepiej”? Właśnie to odkryjemy.\n\n\nDlaczego prawdziwe dane nie układają się w idealną linię\nZanim przejdziemy do rachunków, zrozummy, dlaczego punkty zwykle nie leżą na jednej prostej.\n\nModele deterministyczne vs. stochastyczne\nModele deterministyczne (deterministic models) opisują związki bez niepewności. Przykład z fizyki:\n\\text{Distance} = \\text{Speed} \\times \\text{Time}\nJedziesz dokładnie 60 mph przez 2 godziny → zawsze 120 mil. Zero odchyleń.\nModele stochastyczne (stochastic models) uznają, że w danych naturalnie występuje losowość. Ogólna postać to:\nY = f(X) + \\epsilon\nGdzie:\n\nY — wielkość, którą prognozujemy (np. wynik testu),\nf(X) — wzorzec systematyczny (jak godziny nauki typowo wpływają na wyniki),\n\\epsilon — „reszta”/szum: wszystko, czego nie mierzymy.\n\nW naszym przykładzie dwoje studentów może uczyć się po 5 godzin, a jednak dostać różne oceny, bo:\n\njedno lepiej spało,\njedno ma talent do testów,\njedno miało hałas na sali,\npytania trafiły bardziej/mniej pod ich przygotowanie.\n\nTa losowość jest naturalna — tym zajmuje się \\epsilon.\n\n\n\nProsty model regresji liniowej\nZależność między godzinami nauki a wynikiem zapisujemy jako:\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nRozszyfrujmy:\n\nY_i — wynik testu studenta i,\nX_i — godziny nauki studenta i,\n\\beta_0 — wyraz wolny (intercept, „poziom bazowy” przy 0 godzin),\n\\beta_1 — nachylenie (slope, przyrost punktów na godzinę),\n\\epsilon_i — „wszystko inne” wpływające na wynik i.\n\nWażne: Prawdziwych wartości \\beta_0, \\beta_1 nie znamy. Szacujemy je z danych i oznaczamy „z daszkiem”: \\hat{\\beta}_0, \\hat{\\beta}_1.\n\n\nReszty: jak bardzo mylimy się w przewidywaniach?\nPo dopasowaniu prostej możemy przewidzieć wyniki. Dla każdej obserwacji:\n\nWartość rzeczywista (y_i): faktyczny wynik,\nWartość przewidziana (\\hat{y}_i): co „mówi” nasza prosta,\nReszta (e_i): różnica = Rzeczywista − Przewidziana.\n\nPrzykład:\nDiana: 8 h nauki, wynik 91\nLinia przewiduje: 88\nReszta: 91 − 88 = +3 (zaniżyliśmy)\n\nEric: 5 h nauki, wynik 70\nLinia przewiduje: 79\nReszta: 70 − 79 = −9 (zawyżyliśmy)\n\n\nKluczowy pomysł: dlaczego kwadratujemy reszty?\nZałóżmy reszty czterech studentów:\n\nA: +5\nB: −5\nC: +3\nD: −3\n\nSuma: (+5) + (-5) + (+3) + (-3) = 0.\nTo nie znaczy, że przewidywania są idealne — błędy się znoszą.\nRozwiązanie: sumujemy kwadraty reszt:\n\n(+5)^2 = 25\n(-5)^2 = 25\n(+3)^2 = 9\n(-3)^2 = 9\nSuma kwadratów błędów = 68\n\nDlaczego to działa:\n\nBrak znoszenia znaków, bo kwadraty są dodatnie,\nDuże błędy ważą mocniej (10 punktów to 4× więcej niż 5 punktów),\nWygoda matematyczna: funkcje kwadratowe są gładkie i różniczkowalne.\n\n\n\nMetoda zwykłych najmniejszych kwadratów (OLS)\nOLS wybiera taką prostą, która minimalizuje sumę kwadratów reszt (SSE — Sum of Squared Errors):\n\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\nCzyli:\n\\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n \\big(y_i - (\\beta_0 + \\beta_1 x_i)\\big)^2\n„Po ludzku”: Znajdź takie \\beta_0 i \\beta_1, by łączny błąd (w kwadracie) przewidywań był jak najmniejszy.\n\n\nRozwiązanie matematyczne\nMinimalizujemy SSE rachunkiem różniczkowym. Warunki pierwszego rzędu:\n\\frac{\\partial SSE}{\\partial \\beta_0} = -2\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i) = 0\n\\frac{\\partial SSE}{\\partial \\beta_1} = -2\\sum_{i=1}^n x_i(y_i - \\beta_0 - \\beta_1x_i) = 0\nRozwiązanie układu:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\nWnioski:\n\nNachylenie zależy od tego, jak współzmieniają się X i Y (kowariancja) względem zmienności samego X (wariancja),\nLinia przechodzi przez punkt średnich (\\bar{x}, \\bar{y}).\n\n\n\nSkąd wiemy, że linia jest „dobra”? Rozkład zmienności\nRozbijamy całkowitą zmienność wyników:\nCałkowita suma kwadratów (SST — Total Sum of Squares)\n„Jak bardzo ogólnie różnią się wyniki?”\nSST = \\sum_{i=1}^n (y_i - \\bar{y})^2\nSuma kwadratów regresji (SSR — Regression Sum of Squares)\n„Ile zmienności wyjaśnia nasza linia?”\nSSR = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\nSuma kwadratów błędów (SSE — Error Sum of Squares)\n„Ile nie wyjaśniamy?”\nSSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\nTożsamość wariancyjna:\nSST = SSR + SSE \\quad\\Rightarrow\\quad \\text{Całkowita} = \\text{Wyjaśniona} + \\text{Niewyjaśniona}\n\n\nR^2: ocena dopasowania\nWspółczynnik determinacji (R^2) mówi, jaki odsetek zmienności wyjaśnia model:\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nInterpretacja:\n\nR^2 = 0.75: „Godziny nauki wyjaśniają 75% zróżnicowania wyników”,\nR^2 = 0.30: „Model wyjaśnia 30% tego, czym różnią się wyniki”,\nR^2 = 1.00: perfekcyjne dopasowanie (w realu prawie nigdy),\nR^2 = 0.00: nie lepiej niż zgadywanie średniej.\n\nUwaga kontekstowa: w naukach społecznych 0.30 bywa świetne; w inżynierii oczekuje się bardzo wysokich R^2.\n\n\nInterpretacja wyników\nZałóżmy, że oszacowaliśmy \\hat{\\beta}_0 = 60 i \\hat{\\beta}_1 = 4.\nNachylenie (\\hat{\\beta}_1 = 4):\n\n„Każda dodatkowa godzina nauki wiąże się średnio z +4 punktami”,\nTo efekt przeciętny, nie obietnica dla konkretnej osoby.\n\nWyraz wolny (\\hat{\\beta}_0 = 60):\n\n„Przy 0 godzinach nauki przewidujemy 60 punktów”,\nCzęsto to tylko kotwica matematyczna — może nie mieć sensu praktycznego.\n\nRównanie predykcji:\n\\widehat{\\text{Wynik}} = 60 + 4 \\times \\text{Godziny nauki}\n5 godzin → 60 + 4 \\cdot 5 = 80 punktów.\n\n\nWielkość efektu i istotność praktyczna\nIstotność statystyczna mówi, czy efekt istnieje; istotność praktyczna — czy ma znaczenie. Potrzebujemy obu.\n\nSurowa wielkość efektu\nTo po prostu nachylenie \\hat{\\beta}_1.\nPrzykład: \\hat{\\beta}_1 = 4 pkt/godz.\nCzy to „dużo”? Zależy od:\n\nSkali wyniku (4/100 = 4% vs 4/500 = 0,8%),\nKosztu interwencji (czy 1h nauki warta 4 pkt?),\nProgów decyzyjnych (czy 4 pkt zmienia ocenę?).\n\n\n\nStandaryzowana wielkość efektu\nUłatwia porównania między badaniami:\n\\beta_{\\text{std}} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y}\ngdzie s_X i s_Y to odchylenia standardowe X i Y.\nPrzykład: jeśli s_X = 2.5 h, s_Y = 12 pkt i \\hat{\\beta}_1 = 4:\n\\beta_{\\text{std}} = 4 \\cdot \\frac{2.5}{12} = 0.83\n„Przy wzroście X o 1 SD, Y rośnie o 0.83 SD”.\n\n\nWskazówki Cohena\nDla standaryzowanych współczynników:\n\nmały: |\\beta| \\approx 0.10 (~1% wariancji),\nśredni: |\\beta| \\approx 0.30 (~9%),\nduży: |\\beta| \\approx 0.50 (~25%).\n\nDla R^2:\n\nmały: R^2 \\approx 0.02,\nśredni: R^2 \\approx 0.13,\nduży: R^2 \\approx 0.26.\n\nUwaga: to ogólne progi — normy różnią się między dziedzinami.\n\n\nPrzedziały ufności dla wielkości efektu\nDla surowego współczynnika:\nCI = \\hat{\\beta}_1 \\pm t_{\\text{critical}} \\cdot SE(\\hat{\\beta}_1)\nJeśli 95% CI = [3.2, 4.8], mówimy: „Z 95% pewnością prawdziwy efekt mieści się między 3.2 a 4.8 pkt/godz.”\n\n\nOcena istotności praktycznej\nWeź pod uwagę:\n\nMinimalnie istotną różnicę (MID),\nIle trzeba zmienić X, by osiągnąć sensowną zmianę Y,\nKoszt-efektywność:\n\n\\text{Efektywność} = \\frac{\\text{Wielkość efektu}}{\\text{Koszt interwencji}}\nPrzykład:\nEfekt: 4 pkt/godz.\nPróg zaliczenia: 70; średnia: 68 → 30 min nauki może zmienić niezal na zal → istotne praktycznie.\n\n\n\nNiepewność\nSzacunki pochodzą z próby, nie z całej populacji → niepewność.\n\nSkąd niepewność?\n\nMasz 20 studentów, nie wszystkich,\nPróba może być nietypowa,\nPomiary nie są doskonałe (raportowanie godzin nauki).\n\n\n\nPrzedziały ufności\nZamiast „efekt to dokładnie 4”, mówimy:\n\n„Szacujemy 4”,\n„95% CI: [3.2, 4.8]“.\n\nZnaczenie: w wielu powtórzeniach 95% takich przedziałów zawiera prawdziwą wartość.\n\n\nTestowanie istnienia zależności\nPytamy: „Gdyby prawdziwie nie było związku, jak mało prawdopodobny byłby obserwowany wzorzec?”\n\np = 0.03: przy braku efektu tylko 3% szans na tak silny wzorzec „z przypadku”,\np = 0.40: wzorzec „mógłby się łatwo zdarzyć” bez efektu.\n\nReguła kciuka: p &lt; 0.05 → „statystycznie istotne”.\n\n\n\nGdy coś idzie źle: diagnostyka modelu\nSzybkie wizualizacje:\n\nWykres punktowy: czy związek jest mniej więcej liniowy?\nReszty vs. przewidywania: powinien być losowy obłok,\nOutliery: punkty bardzo odległe?\n\nSygnały ostrzegawcze:\n\nWzorzec w resztach → brak liniowości lub zmienna pominięta,\nRozszerzający się wachlarz reszt → heteroskedastyczność,\nWpływowe obserwacje (influential) ciągną prostą,\nPominięte zmienne → obciążenia (bias).\n\n\n\nZałożenia: kiedy OLS działa dobrze\n\nLiniowość (linearity) — zależność w przybliżeniu prosta,\nNiezależność (independence) — obserwacje od siebie niezależne,\nStała wariancja (homoskedastyczność) — rozrzut reszt podobny w całym zakresie,\nBrak doskonałej współliniowości (no perfect multicollinearity) — w regresji wielorakiej predyktory nie są liniowo zależne „na 100%“,\nLosowy dobór próby (random sampling) — dane reprezentatywne.\n\n\n\nPodsumowanie\nCo robi OLS:\n\nDopasowuje prostą minimalizującą SSE,\nSzacuje, o ile średnio zmienia się Y przy zmianie X o 1,\nPodaje R^2 — ile zmienności wyjaśniamy,\nKwantyfikuje niepewność (SE, CI, p-values).\n\nKroki praktyczne:\n\nWykres danych — czy linia ma sens?\nUruchom OLS → \\hat{\\beta}_0, \\hat{\\beta}_1,\nSprawdź R^2,\nOblicz wielkości efektu (surową i standaryzowaną),\nOceń istotność praktyczną,\nSprawdź przedziały ufności,\nObejrzyj reszty,\nDecyduj, łącząc istotność statystyczną i praktyczną.\n\n\n\nKluczowe interpretacje / Key interpretations\nDomyślny model: regresja OLS Y=\\beta_0+\\beta_1 X+\\varepsilon (lub wieloraka: Y=\\beta_0+\\beta_1 X_1+\\cdots+\\beta_p X_p+\\varepsilon).\n\nNachylenie / Slope (\\beta_1) PL: Przy wzroście X o 1 jednostkę (ceteris paribus), przeciętna wartość Y zmienia się o \\beta_1 jednostek. ENG: When X increases by 1 unit (ceteris paribus), the expected value of Y changes by \\beta_1 units.\nStandaryzowane nachylenie / Standardized slope \\big(\\beta_{1}^{(\\mathrm{std})}\\big) Definicja:\n\n\\beta_{1}^{(\\mathrm{std})} \\;=\\; \\beta_1 \\cdot \\frac{s_X}{s_Y},\n\ngdzie s_X i s_Y to odchylenia standardowe X i Y. PL: Przy wzroście X o 1 odchylenie standardowe (SD), przeciętna wartość Y zmienia się o \\beta_{1}^{(\\mathrm{std})} odchyleń standardowych Y. ENG: For a 1 standard deviation (SD) increase in X, the expected value of Y changes by \\beta_{1}^{(\\mathrm{std})} SDs of Y. Uwaga/Note: W regresji prostej \\beta_{1}^{(\\mathrm{std})} = r (Pearson). / In simple regression, \\beta_{1}^{(\\mathrm{std})} = r (Pearson).\nWspółczynnik determinacji / R^2 Definicja:\n\nR^2 \\;=\\; 1 - \\frac{SS_{\\mathrm{res}}}{SS_{\\mathrm{tot}}}.\n\nPL: Model wyjaśnia 100\\times R^2% zmienności Y względem modelu tylko z wyrazem wolnym (in-sample). ENG: The model explains 100\\times R^2% of the variance in Y relative to the intercept-only model (in-sample). W wielu zmiennych rozważ: \\text{adjusted } R^2. / With multiple predictors consider: adjusted R^2.\nWartość p / P-value Formalnie/Formally:\n\np \\;=\\; \\Pr\\!\\big(\\,|T|\\ge |t_{\\mathrm{obs}}| \\mid H_0\\,\\big),\n\ngdzie T ma rozkład t przy H_0. PL: Zakładając prawdziwość H_0 i spełnione założenia modelu, prawdopodobieństwo uzyskania co najmniej tak ekstremalnej statystyki jak obserwowana wynosi p. ENG: Assuming H_0 and the model assumptions hold, p is the probability of observing a test statistic at least as extreme as the one obtained.\nPrzedział ufności / Confidence interval (np. dla \\beta_1) Konstrukcja/Construction:\n\n\\hat{\\beta}_1 \\;\\pm\\; t_{1-\\alpha/2,\\ \\mathrm{df}} \\cdot \\mathrm{SE}\\!\\left(\\hat{\\beta}_1\\right).\n\nPL (ściśle): W długiej serii powtórzeń 95% tak skonstruowanych przedziałów zawiera prawdziwą wartość \\beta_1; dla naszych danych oszacowanie mieści się w [\\text{lower},\\ \\text{upper}]. ENG (strict): Over many repetitions, 95% of such intervals would contain the true \\beta_1; for our data, the estimate lies within [\\text{lower},\\ \\text{upper}]. PL (skrót dydaktyczny): „Jesteśmy 95% pewni, że \\beta_1 leży w [\\text{lower},\\ \\text{upper}].” ENG (teaching shorthand): “We are 95% confident that \\beta_1 lies in [\\text{lower},\\ \\text{upper}].”\n\n\nNajczęstsze nieporozumienia / Common pitfalls\n\nPL: p nie jest prawdopodobieństwem, że H_0 jest prawdziwa. ENG: p is not the probability that H_0 is true.\nPL: 95% CI nie zawiera 95% obserwacji (od tego jest przedział predykcji). ENG: A 95% CI does not contain 95% of observations (that’s a prediction interval).\nPL/ENG: Wysokie R^2 ≠ przyczynowość / High R^2 ≠ causality. Zawsze sprawdzaj/Always check diagnozy reszt, skalę efektu, i dopasowanie poza próbą.\n\nPamiętaj:\n\nAsocjacja ≠ przyczynowość,\nIstotność statystyczna ≠ istotność praktyczna,\n„Każdy model jest błędny, niektóre są użyteczne”,\nZawsze wizualizuj dane i reszty,\nDecyzje opieraj na wielkości efektu i niepewności.\n\nOLS dostarcza uporządkowany, matematyczny sposób znajdowania wzorców w danych. Nie daje doskonałych prognoz, ale zapewnia najlepszą liniową aproksymację wraz z uczciwą oceną jej jakości i niepewności.\n\n\n\n10.11 Ręczne obliczenia OLS krok po kroku\nBadaczka chce zbadać zależność między godzinami nauki a wynikiem testu (6 studentów):\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nCelem jest wyznaczyć \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X metodą OLS.\n\nKrok 1: Średnie\nDla X:\n\\bar{X} = \\frac{1+2+3+4+5+6}{6} = \\frac{21}{6} = 3.5\nDla Y:\n\\bar{Y} = \\frac{65+70+75+85+88+95}{6} = \\frac{478}{6} = 79.67\n\n\nKrok 2: Odchylenia od średnich\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n−2.5\n−14.67\n\n\nB\n2\n70\n−1.5\n−9.67\n\n\nC\n3\n75\n−0.5\n−4.67\n\n\nD\n4\n85\n0.5\n5.33\n\n\nE\n5\n88\n1.5\n8.33\n\n\nF\n6\n95\n2.5\n15.33\n\n\n\n\n\nKrok 3: Iloczyny i kwadraty\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n36.68\n6.25\n\n\nB\n14.51\n2.25\n\n\nC\n2.34\n0.25\n\n\nD\n2.67\n0.25\n\n\nE\n12.50\n2.25\n\n\nF\n38.33\n6.25\n\n\nSuma\n107.03\n17.50\n\n\n\n\n\nKrok 4: Nachylenie \\hat{\\beta}_1\n\\hat{\\beta}_1 = \\frac{107.03}{17.50} = 6.12\nInterpretacja: +6.12 punktu za każdą dodatkową godzinę nauki.\n\n\nKrok 5: Wyraz wolny \\hat{\\beta}_0\n\\hat{\\beta}_0 = 79.67 - 6.12 \\cdot 3.5 = 58.25\nInterpretacja: przy 0 godzinach przewidujemy 58.25.\n\n\nKrok 6: Równanie regresji\n\\hat{Y} = 58.25 + 6.12 X\n\n\nKrok 7: Predykcje i reszty\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nReszta e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n64.37\n0.63\n\n\nB\n2\n70\n70.49\n−0.49\n\n\nC\n3\n75\n76.61\n−1.61\n\n\nD\n4\n85\n82.73\n2.27\n\n\nE\n5\n88\n88.85\n−0.85\n\n\nF\n6\n95\n94.97\n0.03\n\n\n\nKontrola: suma reszt ≈ 0 ✓\n\n\nKrok 8: Sumy kwadratów\nSST — całkowita:\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n215.21\n\n\nB\n70\n93.51\n\n\nC\n75\n21.81\n\n\nD\n85\n28.41\n\n\nE\n88\n69.39\n\n\nF\n95\n235.01\n\n\nSuma\n\nSST = 663.34\n\n\n\nSSR — wyjaśniona:\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n234.09\n\n\nB\n70.49\n84.27\n\n\nC\n76.61\n9.36\n\n\nD\n82.73\n9.36\n\n\nE\n88.85\n84.27\n\n\nF\n94.97\n234.09\n\n\nSuma\n\nSSR = 655.44\n\n\n\nSSE — błąd:\n\n\n\nStudent\ne_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n−0.49\n0.24\n\n\nC\n−1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n−0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSuma\n\nSSE = 9.10\n\n\n\nWeryfikacja: SST \\approx SSR + SSE\n663.34 \\approx 655.44 + 9.10 = 664.54 (drobne różnice zaokrągleń).\n\n\nKrok 9: R^2\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternatywnie:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 0.986\nInterpretacja: ~98.8% zmienności wyników wyjaśniają godziny nauki — bardzo silny związek.\n\n\nKrok 10: Wielkości efektu\nSurowa: 6.12 pkt/godz.\nStandaryzowana:\n\ns_X = \\sqrt{17.50/5} = 1.87,\ns_Y = \\sqrt{663.34/5} = 11.52,\n\\beta_{\\text{std}} = 6.12 \\cdot (1.87/11.52) = 0.99 → bardzo duży efekt (wg Cohena).\n\n\n\nKrok 11: Istotność praktyczna\n\nSkala: 6.12% na 100-punktowej skali / godz.,\nProgi: zmiana oceny (10 pkt) ≈ 1.63 h,\nKoszt-efekt: korzystny — sensowna inwestycja czasu.\n\n\n\nPodsumowanie wyników\n\nRównanie: \\hat{Y} = 58.25 + 6.12 X\nNachylenie: 6.12 pkt/godz.\nWyraz wolny: 58.25 pkt\nR^2: 0.988\n\\beta_{\\text{std}}: 0.99\n\nW praktyce: każda godzina nauki to ≈ +6 pkt; dopasowanie znakomite; efekt istotny statystycznie i praktycznie.\n\n\nKontrola wyniku\nSprawdź, że linia przechodzi przez (\\bar{X}, \\bar{Y}):\n58.25 + 6.12 \\cdot 3.5 = 79.67 = \\bar{Y} ✓\n\n\n\n10.12 Kod R do weryfikacji obliczeń\n\n# Krok 1: Dane\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)      # X\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95) # Y\nn &lt;- length(study_hours)\n\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Krok 2: Średnie\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Krok 3: Odchylenia od średnich\ndata$x_dev &lt;- data$X - x_bar\ndata$y_dev &lt;- data$Y - y_bar\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Krok 4: Iloczyny i kwadraty\ndata$xy_product &lt;- data$x_dev * data$y_dev\ndata$x_dev_sq &lt;- data$x_dev^2\n\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - X̄)(Yi - Ȳ):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - X̄)(Yi - Ȳ): 107 \n\ncat(\"Sum of (Xi - X̄)²:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - X̄)²: 17.5 \n\n# Krok 5: Nachylenie (beta_1)\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (β₁) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (β₁) calculated manually: 6.11 \n\n# Krok 6: Wyraz wolny (beta_0)\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (β₀) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (β₀) calculated manually: 58.27 \n\n# Krok 7: Porównanie z lm()\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Krok 8: Predykcje i reszty\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X\ndata$residual &lt;- data$Y - data$Y_hat\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ≈ 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ≈ 0): 0 \n\n# Krok 9: Sumy kwadratów\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Krok 10: R-kwadrat\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R² (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR² (Method 1: SSR/SST): 0.9863 \n\ncat(\"R² (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR² (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R² (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR² (from lm function): 0.9863 \n\n# Krok 11: Wielkości efektu\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\nsd_x &lt;- sd(data$X)\nsd_y &lt;- sd(data$Y)\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Korelacja (dla regresji prostej |r| = sqrt(R^2))\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R²:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R²: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n\n\n# Krok 12: Wizualizacja\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5,\n     xlim = c(0, 7), ylim = c(60, 100))\n\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Punkt średnich\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Reszty jako odcinki pionowe\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# Równanie na wykresie\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R² = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\nAnaliza regresji OLS - godziny nauki vs. wynik testu\n\n\n\n# Podsumowanie końcowe\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )\n\n\n\n\n10.13 Jak uruchomić kod\n\nSkopiuj cały blok kodu,\nWklej do RStudio,\nUruchom chunk po chunk lub cały dokument,\nPorównaj wyniki z obliczeniami ręcznymi.\n\nCo zobaczysz:\n\nNachylenie: 6.12,\nWyraz wolny: 58.25,\nR^2: ≈ 0.988,\nEfekt standaryzowany: ≈ 0.99,\nWykres z punktami, linią regresji i resztami.\n\nTo potwierdza poprawność obliczeń manualnych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#ręczne-obliczenia-ols-krok-po-kroku",
    "href": "correg_pl.html#ręczne-obliczenia-ols-krok-po-kroku",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.11 Ręczne obliczenia OLS krok po kroku",
    "text": "10.11 Ręczne obliczenia OLS krok po kroku\nBadaczka chce zbadać zależność między godzinami nauki a wynikiem testu (6 studentów):\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nCelem jest wyznaczyć \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X metodą OLS.\n\nKrok 1: Średnie\nDla X:\n\\bar{X} = \\frac{1+2+3+4+5+6}{6} = \\frac{21}{6} = 3.5\nDla Y:\n\\bar{Y} = \\frac{65+70+75+85+88+95}{6} = \\frac{478}{6} = 79.67\n\n\nKrok 2: Odchylenia od średnich\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n−2.5\n−14.67\n\n\nB\n2\n70\n−1.5\n−9.67\n\n\nC\n3\n75\n−0.5\n−4.67\n\n\nD\n4\n85\n0.5\n5.33\n\n\nE\n5\n88\n1.5\n8.33\n\n\nF\n6\n95\n2.5\n15.33\n\n\n\n\n\nKrok 3: Iloczyny i kwadraty\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n36.68\n6.25\n\n\nB\n14.51\n2.25\n\n\nC\n2.34\n0.25\n\n\nD\n2.67\n0.25\n\n\nE\n12.50\n2.25\n\n\nF\n38.33\n6.25\n\n\nSuma\n107.03\n17.50\n\n\n\n\n\nKrok 4: Nachylenie \\hat{\\beta}_1\n\\hat{\\beta}_1 = \\frac{107.03}{17.50} = 6.12\nInterpretacja: +6.12 punktu za każdą dodatkową godzinę nauki.\n\n\nKrok 5: Wyraz wolny \\hat{\\beta}_0\n\\hat{\\beta}_0 = 79.67 - 6.12 \\cdot 3.5 = 58.25\nInterpretacja: przy 0 godzinach przewidujemy 58.25.\n\n\nKrok 6: Równanie regresji\n\\hat{Y} = 58.25 + 6.12 X\n\n\nKrok 7: Predykcje i reszty\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nReszta e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n64.37\n0.63\n\n\nB\n2\n70\n70.49\n−0.49\n\n\nC\n3\n75\n76.61\n−1.61\n\n\nD\n4\n85\n82.73\n2.27\n\n\nE\n5\n88\n88.85\n−0.85\n\n\nF\n6\n95\n94.97\n0.03\n\n\n\nKontrola: suma reszt ≈ 0 ✓\n\n\nKrok 8: Sumy kwadratów\nSST — całkowita:\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n215.21\n\n\nB\n70\n93.51\n\n\nC\n75\n21.81\n\n\nD\n85\n28.41\n\n\nE\n88\n69.39\n\n\nF\n95\n235.01\n\n\nSuma\n\nSST = 663.34\n\n\n\nSSR — wyjaśniona:\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n234.09\n\n\nB\n70.49\n84.27\n\n\nC\n76.61\n9.36\n\n\nD\n82.73\n9.36\n\n\nE\n88.85\n84.27\n\n\nF\n94.97\n234.09\n\n\nSuma\n\nSSR = 655.44\n\n\n\nSSE — błąd:\n\n\n\nStudent\ne_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n−0.49\n0.24\n\n\nC\n−1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n−0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSuma\n\nSSE = 9.10\n\n\n\nWeryfikacja: SST \\approx SSR + SSE\n663.34 \\approx 655.44 + 9.10 = 664.54 (drobne różnice zaokrągleń).\n\n\nKrok 9: R^2\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternatywnie:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 0.986\nInterpretacja: ~98.8% zmienności wyników wyjaśniają godziny nauki — bardzo silny związek.\n\n\nKrok 10: Wielkości efektu\nSurowa: 6.12 pkt/godz.\nStandaryzowana:\n\ns_X = \\sqrt{17.50/5} = 1.87,\ns_Y = \\sqrt{663.34/5} = 11.52,\n\\beta_{\\text{std}} = 6.12 \\cdot (1.87/11.52) = 0.99 → bardzo duży efekt (wg Cohena).\n\n\n\nKrok 11: Istotność praktyczna\n\nSkala: 6.12% na 100-punktowej skali / godz.,\nProgi: zmiana oceny (10 pkt) ≈ 1.63 h,\nKoszt-efekt: korzystny — sensowna inwestycja czasu.\n\n\n\nPodsumowanie wyników\n\nRównanie: \\hat{Y} = 58.25 + 6.12 X\nNachylenie: 6.12 pkt/godz.\nWyraz wolny: 58.25 pkt\nR^2: 0.988\n\\beta_{\\text{std}}: 0.99\n\nW praktyce: każda godzina nauki to ≈ +6 pkt; dopasowanie znakomite; efekt istotny statystycznie i praktycznie.\n\n\nKontrola wyniku\nSprawdź, że linia przechodzi przez (\\bar{X}, \\bar{Y}):\n58.25 + 6.12 \\cdot 3.5 = 79.67 = \\bar{Y} ✓",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kod-r-do-weryfikacji-obliczeń",
    "href": "correg_pl.html#kod-r-do-weryfikacji-obliczeń",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.12 Kod R do weryfikacji obliczeń",
    "text": "10.12 Kod R do weryfikacji obliczeń\n\n# Krok 1: Dane\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)      # X\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95) # Y\nn &lt;- length(study_hours)\n\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Krok 2: Średnie\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Krok 3: Odchylenia od średnich\ndata$x_dev &lt;- data$X - x_bar\ndata$y_dev &lt;- data$Y - y_bar\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Krok 4: Iloczyny i kwadraty\ndata$xy_product &lt;- data$x_dev * data$y_dev\ndata$x_dev_sq &lt;- data$x_dev^2\n\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - X̄)(Yi - Ȳ):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - X̄)(Yi - Ȳ): 107 \n\ncat(\"Sum of (Xi - X̄)²:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - X̄)²: 17.5 \n\n# Krok 5: Nachylenie (beta_1)\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (β₁) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (β₁) calculated manually: 6.11 \n\n# Krok 6: Wyraz wolny (beta_0)\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (β₀) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (β₀) calculated manually: 58.27 \n\n# Krok 7: Porównanie z lm()\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Krok 8: Predykcje i reszty\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X\ndata$residual &lt;- data$Y - data$Y_hat\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ≈ 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ≈ 0): 0 \n\n# Krok 9: Sumy kwadratów\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Krok 10: R-kwadrat\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R² (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR² (Method 1: SSR/SST): 0.9863 \n\ncat(\"R² (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR² (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R² (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR² (from lm function): 0.9863 \n\n# Krok 11: Wielkości efektu\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\nsd_x &lt;- sd(data$X)\nsd_y &lt;- sd(data$Y)\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Korelacja (dla regresji prostej |r| = sqrt(R^2))\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R²:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R²: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n\n\n# Krok 12: Wizualizacja\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5,\n     xlim = c(0, 7), ylim = c(60, 100))\n\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Punkt średnich\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Reszty jako odcinki pionowe\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# Równanie na wykresie\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R² = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\nAnaliza regresji OLS - godziny nauki vs. wynik testu\n\n\n\n# Podsumowanie końcowe\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#jak-uruchomić-kod",
    "href": "correg_pl.html#jak-uruchomić-kod",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.13 Jak uruchomić kod",
    "text": "10.13 Jak uruchomić kod\n\nSkopiuj cały blok kodu,\nWklej do RStudio,\nUruchom chunk po chunk lub cały dokument,\nPorównaj wyniki z obliczeniami ręcznymi.\n\nCo zobaczysz:\n\nNachylenie: 6.12,\nWyraz wolny: 58.25,\nR^2: ≈ 0.988,\nEfekt standaryzowany: ≈ 0.99,\nWykres z punktami, linią regresji i resztami.\n\nTo potwierdza poprawność obliczeń manualnych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#appendix-a-obliczanie-kowariancji-korelacji-pearsona-i-spearmana-oraz-modelowanie-ols---przykład-wprowadzający",
    "href": "correg_pl.html#appendix-a-obliczanie-kowariancji-korelacji-pearsona-i-spearmana-oraz-modelowanie-ols---przykład-wprowadzający",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.14 Appendix A: Obliczanie Kowariancji, Korelacji Pearsona i Spearmana, oraz modelowanie OLS - przykład wprowadzający",
    "text": "10.14 Appendix A: Obliczanie Kowariancji, Korelacji Pearsona i Spearmana, oraz modelowanie OLS - przykład wprowadzający\nStudentka politologii bada związek między wielkością okręgu wyborczego (DM) a wskaźnikiem dysproporcjonalności Gallaghera (GH) w wyborach parlamentarnych w 10 losowo wybranych demokracjach.\nDane dotyczące wielkości okręgu wyborczego (\\text{DM}) i indeksu Gallaghera:\n\n\n\n\\text{DM} (X)\nGallagher (Y)\n\n\n\n\n2\n18,2\n\n\n3\n16,7\n\n\n4\n15,8\n\n\n5\n15,3\n\n\n6\n15,0\n\n\n7\n14,8\n\n\n8\n14,7\n\n\n9\n14,6\n\n\n10\n14,55\n\n\n11\n14,52\n\n\n\n\nKrok 1: Obliczanie Podstawowych Statystyk\nObliczanie średnich:\nDla \\text{DM} (X): \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\nSzczegółowe obliczenia:\n2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 65 \\bar{x} = \\frac{65}{10} = 6,5\nDla indeksu Gallaghera (Y): \\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\nSzczegółowe obliczenia:\n18,2 + 16,7 + 15,8 + 15,3 + 15,0 + 14,8 + 14,7 + 14,6 + 14,55 + 14,52 = 154,17 \\bar{y} = \\frac{154,17}{10} = 15,417\n\n\nKrok 2: Szczegółowe Obliczenia Kowariancji\nPełna tabela robocza ze wszystkimi obliczeniami:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18,2\n-4,5\n2,783\n-12,5235\n20,25\n7,7451\n\n\n2\n3\n16,7\n-3,5\n1,283\n-4,4905\n12,25\n1,6461\n\n\n3\n4\n15,8\n-2,5\n0,383\n-0,9575\n6,25\n0,1467\n\n\n4\n5\n15,3\n-1,5\n-0,117\n0,1755\n2,25\n0,0137\n\n\n5\n6\n15,0\n-0,5\n-0,417\n0,2085\n0,25\n0,1739\n\n\n6\n7\n14,8\n0,5\n-0,617\n-0,3085\n0,25\n0,3807\n\n\n7\n8\n14,7\n1,5\n-0,717\n-1,0755\n2,25\n0,5141\n\n\n8\n9\n14,6\n2,5\n-0,817\n-2,0425\n6,25\n0,6675\n\n\n9\n10\n14,55\n3,5\n-0,867\n-3,0345\n12,25\n0,7517\n\n\n10\n11\n14,52\n4,5\n-0,897\n-4,0365\n20,25\n0,8047\n\n\nSuma\n65\n154,17\n0\n0\n-28,085\n82,5\n12,8442\n\n\n\nObliczanie kowariancji: \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\text{Cov}(X,Y) = \\frac{-28,085}{9} = -3,120556\n\n\nKrok 3: Obliczanie Odchylenia Standardowego\nDla \\text{DM} (X): \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}}\n\\sigma_x = \\sqrt{\\frac{82,5}{9}} = \\sqrt{9,1667} = 3,026582\nDla Gallaghera (Y): \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}}\n\\sigma_y = \\sqrt{\\frac{12,8442}{9}} = \\sqrt{1,4271} = 1,194612\n\n\nKrok 4: Obliczanie Korelacji Pearsona\nr = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\nr = \\frac{-3,120556}{3,026582 \\times 1,194612} = \\frac{-3,120556}{3,615752} = -0,863044\n\n\nKrok 5: Obliczanie Korelacji Rangowej Spearmana\nPełna tabela rangowa ze wszystkimi obliczeniami:\n\n\n\ni\nX_i\nY_i\nRanga X_i\nRanga Y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n18,2\n1\n10\n-9\n81\n\n\n2\n3\n16,7\n2\n9\n-7\n49\n\n\n3\n4\n15,8\n3\n8\n-5\n25\n\n\n4\n5\n15,3\n4\n7\n-3\n9\n\n\n5\n6\n15,0\n5\n6\n-1\n1\n\n\n6\n7\n14,8\n6\n5\n1\n1\n\n\n7\n8\n14,7\n7\n4\n3\n9\n\n\n8\n9\n14,6\n8\n3\n5\n25\n\n\n9\n10\n14,55\n9\n2\n7\n49\n\n\n10\n11\n14,52\n10\n1\n9\n81\n\n\nSuma\n\n\n\n\n\n330\n\n\n\nObliczanie korelacji Spearmana: \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n\\rho = 1 - \\frac{6 \\times 330}{10(100 - 1)} = 1 - \\frac{1980}{990} = 1 - 2 = -1\n\n\nKrok 6: Weryfikacja w R\n\n# Tworzenie wektorów\nDM &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nGH &lt;- c(18.2, 16.7, 15.8, 15.3, 15.0, 14.8, 14.7, 14.6, 14.55, 14.52)\n\n# Obliczanie kowariancji\ncov(DM, GH)\n\n[1] -3.120556\n\n# Obliczanie korelacji\ncor(DM, GH, method = \"pearson\")\n\n[1] -0.8627742\n\ncor(DM, GH, method = \"spearman\")\n\n[1] -1\n\n\n\n\nKrok 7: Podstawowa Wizualizacja\n\nlibrary(ggplot2)\n\n# Tworzenie ramki danych\ndata &lt;- data.frame(DM = DM, GH = GH)\n\n# Tworzenie wykresu rozrzutu\nggplot(data, aes(x = DM, y = GH)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Wielkość Okręgu vs Indeks Gallaghera\",\n    x = \"Wielkość Okręgu (DM)\",\n    y = \"Indeks Gallaghera (GH)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nEstymacja OLS i Miary Dopasowania Modelu\n\n\nKrok 1: Obliczanie Estymatorów OLS\nKorzystając z wcześniej obliczonych wartości:\n\n\\sum(X_i - \\bar{X})(Y_i - \\bar{Y}) = -28,085\n\\sum(X_i - \\bar{X})^2 = 82,5\n\\bar{X} = 6,5\n\\bar{Y} = 15,417\n\nObliczanie nachylenia (\\hat{\\beta_1}):\n\\hat{\\beta_1} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\hat{\\beta_1} = -28,085 ÷ 82,5 = -0,3404\nObliczanie wyrazu wolnego (\\hat{\\beta_0}): \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 15,417 - (-0,3404 × 6,5)\n   = 15,417 + 2,2126\n   = 17,6296\nZatem równanie regresji OLS ma postać: \\hat{Y} = 17,6296 - 0,3404X\n\n\nKrok 2: Obliczanie Wartości Dopasowanych i Reszt\nPełna tabela ze wszystkimi obliczeniami:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i\ne_i = Y_i - \\hat{Y}_i\ne_i^2\n(Y_i - \\bar{Y})^2\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18,2\n16,9488\n1,2512\n1,5655\n7,7451\n2,3404\n\n\n2\n3\n16,7\n16,6084\n0,0916\n0,0084\n1,6461\n1,4241\n\n\n3\n4\n15,8\n16,2680\n-0,4680\n0,2190\n0,1467\n0,7225\n\n\n4\n5\n15,3\n15,9276\n-0,6276\n0,3939\n0,0137\n0,2601\n\n\n5\n6\n15,0\n15,5872\n-0,5872\n0,3448\n0,1739\n0,0289\n\n\n6\n7\n14,8\n15,2468\n-0,4468\n0,1996\n0,3807\n0,0290\n\n\n7\n8\n14,7\n14,9064\n-0,2064\n0,0426\n0,5141\n0,2610\n\n\n8\n9\n14,6\n14,5660\n0,0340\n0,0012\n0,6675\n0,7241\n\n\n9\n10\n14,55\n14,2256\n0,3244\n0,1052\n0,7517\n1,4184\n\n\n10\n11\n14,52\n13,8852\n0,6348\n0,4030\n0,8047\n2,3439\n\n\nSuma\n65\n154,17\n154,17\n0\n3,2832\n12,8442\n9,5524\n\n\n\nObliczenia dla wartości dopasowanych:\nDla X = 2:\nŶ = 17,6296 + (-0,3404 × 2) = 16,9488\n\nDla X = 3:\nŶ = 17,6296 + (-0,3404 × 3) = 16,6084\n\n[... kontynuacja dla wszystkich wartości]\n\n\nKrok 3: Obliczanie Miar Dopasowania\nSuma kwadratów reszt (SSE): SSE = \\sum e_i^2\nSSE = 3,2832\nCałkowita suma kwadratów (SST): SST = \\sum(Y_i - \\bar{Y})^2\nSST = 12,8442\nSuma kwadratów regresji (SSR): SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\nSSR = 9,5524\nWeryfikacja dekompozycji: SST = SSR + SSE\n12,8442 = 9,5524 + 3,2832 (w granicach błędu zaokrąglenia)\nObliczanie współczynnika determinacji R-kwadrat: R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nR² = 9,5524 ÷ 12,8442\n   = 0,7438\n\n\nKrok 4: Weryfikacja w R\n\n# Dopasowanie modelu liniowego\nmodel &lt;- lm(GH ~ DM, data = data)\n\n# Podsumowanie statystyk\nsummary(model)\n\n\nCall:\nlm(formula = GH ~ DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62764 -0.46274 -0.08615  0.26624  1.25109 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept) 17.62976    0.50121  35.174 0.000000000467 ***\nDM          -0.34042    0.07053  -4.827        0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6406 on 8 degrees of freedom\nMultiple R-squared:  0.7444,    Adjusted R-squared:  0.7124 \nF-statistic:  23.3 on 1 and 8 DF,  p-value: 0.00131\n\n# Ręczne obliczenie R-kwadrat\nSST &lt;- sum((GH - mean(GH))^2)\nSSE &lt;- sum(residuals(model)^2)\nSSR &lt;- SST - SSE\nR2_manual &lt;- SSR/SST\nR2_manual\n\n[1] 0.7443793\n\n\n\n\nKrok 5: Analiza Reszt\n\n# Tworzenie wykresów reszt\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\nKrok 6: Wykres Wartości Przewidywanych vs Rzeczywistych\n\n# Tworzenie wykresu wartości przewidywanych vs rzeczywistych\nggplot(data.frame(\n  Rzeczywiste = GH,\n  Przewidywane = fitted(model)\n), aes(x = Przewidywane, y = Rzeczywiste)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Wartości Przewidywane vs Rzeczywiste\",\n    x = \"Przewidywany Indeks Gallaghera\",\n    y = \"Rzeczywisty Indeks Gallaghera\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nModele z Transformacją Logarytmiczną\n\n\nKrok 1: Transformacja Danych\nNajpierw obliczamy logarytmy naturalne zmiennych:\n\n\n\ni\nX_i\nY_i\n\\ln(X_i)\n\\ln(Y_i)\n\n\n\n\n1\n2\n18,2\n0,6931\n2,9014\n\n\n2\n3\n16,7\n1,0986\n2,8154\n\n\n3\n4\n15,8\n1,3863\n2,7600\n\n\n4\n5\n15,3\n1,6094\n2,7278\n\n\n5\n6\n15,0\n1,7918\n2,7081\n\n\n6\n7\n14,8\n1,9459\n2,6946\n\n\n7\n8\n14,7\n2,0794\n2,6878\n\n\n8\n9\n14,6\n2,1972\n2,6810\n\n\n9\n10\n14,55\n2,3026\n2,6777\n\n\n10\n11\n14,52\n2,3979\n2,6757\n\n\n\n\n\nKrok 2: Porównanie Różnych Specyfikacji Modelu\nSzacujemy trzy alternatywne specyfikacje:\n\nModel log-liniowy: \\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nModel liniowo-logarytmiczny: Y_i = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\nModel log-log: \\ln(Y_i) = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\n\n\n# Tworzenie zmiennych transformowanych\ndata$log_DM &lt;- log(data$DM)\ndata$log_GH &lt;- log(data$GH)\n\n# Dopasowanie modeli\nmodel_linear &lt;- lm(GH ~ DM, data = data)\nmodel_loglinear &lt;- lm(log_GH ~ DM, data = data)\nmodel_linearlog &lt;- lm(GH ~ log_DM, data = data)\nmodel_loglog &lt;- lm(log_GH ~ log_DM, data = data)\n\n# Porównanie wartości R-kwadrat\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Liniowy\", \"Log-liniowy\", \"Liniowo-logarytmiczny\", \"Log-log\"),\n  R_kwadrat = c(\n    summary(model_linear)$r.squared,\n    summary(model_loglinear)$r.squared,\n    summary(model_linearlog)$r.squared,\n    summary(model_loglog)$r.squared\n  )\n)\n\n# Wyświetlenie porównania\nmodels_comparison\n\n                  Model R_kwadrat\n1               Liniowy 0.7443793\n2           Log-liniowy 0.7670346\n3 Liniowo-logarytmiczny 0.9141560\n4               Log-log 0.9288088\n\n\n\n\nKrok 3: Porównanie Wizualne\n\n# Tworzenie wykresów dla każdego modelu\np1 &lt;- ggplot(data, aes(x = DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Liniowy\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Log-liniowy\") +\n  theme_minimal()\n\np3 &lt;- ggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Liniowo-logarytmiczny\") +\n  theme_minimal()\n\np4 &lt;- ggplot(data, aes(x = log_DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Log-log\") +\n  theme_minimal()\n\n# Układanie wykresów w siatkę\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nKrok 4: Analiza Reszt dla Najlepszego Modelu\nNa podstawie wartości R-kwadrat, analiza reszt dla najlepiej dopasowanego modelu:\n\n# Wykresy reszt dla najlepszego modelu\npar(mfrow = c(2, 2))\nplot(model_linearlog)\n\n\n\n\n\n\n\n\n\n\nKrok 5: Interpretacja Najlepszego Modelu\nWspółczynniki modelu liniowo-logarytmicznego:\n\nsummary(model_linearlog)\n\n\nCall:\nlm(formula = GH ~ log_DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40702 -0.30207 -0.04907  0.22905  0.60549 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)  19.0223     0.4079   46.64 0.0000000000494 ***\nlog_DM       -2.0599     0.2232   -9.23 0.0000153880425 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3712 on 8 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9034 \nF-statistic: 85.19 on 1 and 8 DF,  p-value: 0.00001539\n\n\nInterpretacja:\n\n\\hat{\\beta_0} reprezentuje oczekiwany Indeks Gallaghera, gdy ln(DM) = 0 (czyli gdy DM = 1)\n\\hat{\\beta_1} reprezentuje zmianę Indeksu Gallaghera związaną z jednostkowym wzrostem ln(DM)\n\n\n\nKrok 6: Predykcje Modelu\n\n# Tworzenie wykresu predykcji dla najlepszego modelu\nggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Model Liniowo-logarytmiczny: Indeks Gallaghera vs ln(Wielkość Okręgu)\",\n    x = \"ln(Wielkość Okręgu)\",\n    y = \"Indeks Gallaghera\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nKrok 7: Analiza Elastyczności\nDla modelu log-log współczynniki bezpośrednio reprezentują elastyczności. Obliczenie średniej elastyczności dla modelu liniowo-logarytmicznego:\n\n# Obliczenie elastyczności przy wartościach średnich\nmean_DM &lt;- mean(data$DM)\nmean_GH &lt;- mean(data$GH)\nbeta1 &lt;- coef(model_linearlog)[2]\nelastycznosc &lt;- beta1 * (1/mean_GH)\nelastycznosc\n\n    log_DM \n-0.1336136 \n\n\nWartość ta reprezentuje procentową zmianę Indeksu Gallaghera przy jednoprocentowej zmianie Wielkości Okręgu.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#appendix-b.-prosty-model-regresji-liniowej-mnkols---przykłady-różne",
    "href": "correg_pl.html#appendix-b.-prosty-model-regresji-liniowej-mnkols---przykłady-różne",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.15 Appendix B. Prosty Model Regresji Liniowej (MNK/OLS) - przykłady różne",
    "text": "10.15 Appendix B. Prosty Model Regresji Liniowej (MNK/OLS) - przykłady różne",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przykład-0.-czas-nauki-vs.-ocena-przewodnik-po-mnk-i-r-kwadrat",
    "href": "correg_pl.html#przykład-0.-czas-nauki-vs.-ocena-przewodnik-po-mnk-i-r-kwadrat",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.16 Przykład 0. Czas Nauki vs. Ocena: Przewodnik po MNK i R-kwadrat",
    "text": "10.16 Przykład 0. Czas Nauki vs. Ocena: Przewodnik po MNK i R-kwadrat\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\n\n# Nasz prosty zbiór danych\npractice &lt;- c(1, 2, 3, 4, 5, 6)\nskill &lt;- c(3, 7, 5, 8, 10, 9)\n\n# Tworzenie ramki danych\ndata &lt;- data.frame(\n  Student = 1:6,\n  Practice = practice,\n  Skill = skill\n)\n\nprint(data)\n\n  Student Practice Skill\n1       1        1     3\n2       2        2     7\n3       3        3     5\n4       4        4     8\n5       5        5    10\n6       6        6     9\n\n\n\nObliczenia Ręczne\n\nKrok 1: Obliczenie Średnich\nŚrednia to wartość przeciętna, obliczana przez zsumowanie wszystkich obserwacji i podzielenie przez ich liczbę.\nŚrednia Godzin Ćwiczeń (X̄):\n\\bar{X} = \\frac{\\sum_{i=1}^{n} X_i}{n} = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5\nŚrednia Ocen Umiejętności (Ȳ):\n\\bar{Y} = \\frac{\\sum_{i=1}^{n} Y_i}{n} = \\frac{3 + 7 + 5 + 8 + 10 + 9}{6} = \\frac{42}{6} = 7\n\n# Obliczenie średnich\nmean_x &lt;- mean(practice)\nmean_y &lt;- mean(skill)\n\ncat(\"Średnia Godzin Ćwiczeń:\", mean_x, \"\\n\")\n\nŚrednia Godzin Ćwiczeń: 3.5 \n\ncat(\"Średnia Oceny Umiejętności:\", mean_y, \"\\n\")\n\nŚrednia Oceny Umiejętności: 7 \n\n\n\n\nKrok 2: Obliczenie Wariancji i Odchylenia Standardowego\nWariancja mierzy, jak bardzo dane są rozproszone wokół średniej. Używamy wzoru na wariancję próbkową (dzielenie przez n-1).\nWariancja X (Godziny Ćwiczeń):\nNajpierw obliczamy odchylenia od średniej (X_i - \\bar{X}):\n\n\n\nStudent\nX_i\nX_i - \\bar{X}\n(X_i - \\bar{X})^2\n\n\n\n\n1\n1\n1 - 3.5 = -2.5\n(-2.5)^2 = 6.25\n\n\n2\n2\n2 - 3.5 = -1.5\n(-1.5)^2 = 2.25\n\n\n3\n3\n3 - 3.5 = -0.5\n(-0.5)^2 = 0.25\n\n\n4\n4\n4 - 3.5 = 0.5\n(0.5)^2 = 0.25\n\n\n5\n5\n5 - 3.5 = 1.5\n(1.5)^2 = 2.25\n\n\n6\n6\n6 - 3.5 = 2.5\n(2.5)^2 = 6.25\n\n\nSuma\n\n\n17.5\n\n\n\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1} = \\frac{17.5}{5} = 3.5\ns_X = \\sqrt{3.5} = 1.871\nWariancja Y (Oceny Umiejętności):\n\n\n\nStudent\nY_i\nY_i - \\bar{Y}\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n3\n3 - 7 = -4\n(-4)^2 = 16\n\n\n2\n7\n7 - 7 = 0\n(0)^2 = 0\n\n\n3\n5\n5 - 7 = -2\n(-2)^2 = 4\n\n\n4\n8\n8 - 7 = 1\n(1)^2 = 1\n\n\n5\n10\n10 - 7 = 3\n(3)^2 = 9\n\n\n6\n9\n9 - 7 = 2\n(2)^2 = 4\n\n\nSuma\n\n\n34\n\n\n\ns^2_Y = \\frac{\\sum(Y_i - \\bar{Y})^2}{n-1} = \\frac{34}{5} = 6.8\ns_Y = \\sqrt{6.8} = 2.608\n\n# Weryfikacja wariancji i odchylenia standardowego\ncat(\"Wariancja Ćwiczeń:\", var(practice), \"\\n\")\n\nWariancja Ćwiczeń: 3.5 \n\ncat(\"Odch. Stand. Ćwiczeń:\", sd(practice), \"\\n\")\n\nOdch. Stand. Ćwiczeń: 1.870829 \n\ncat(\"Wariancja Umiejętności:\", var(skill), \"\\n\")\n\nWariancja Umiejętności: 6.8 \n\ncat(\"Odch. Stand. Umiejętności:\", sd(skill), \"\\n\")\n\nOdch. Stand. Umiejętności: 2.607681 \n\n\n\n\nKrok 3: Obliczenie Kowariancji\nKowariancja mierzy, jak dwie zmienne zmieniają się wspólnie. Dodatnia kowariancja wskazuje, że gdy jedna zmienna rośnie, druga również ma tendencję do wzrostu.\ns_{XY} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nObliczmy iloczyny dla każdej obserwacji:\n\n\n\n\n\n\n\n\n\nStudent\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n\n\n\n\n1\n-2.5\n-4\n(-2.5) \\times (-4) = 10.0\n\n\n2\n-1.5\n0\n(-1.5) \\times (0) = 0.0\n\n\n3\n-0.5\n-2\n(-0.5) \\times (-2) = 1.0\n\n\n4\n0.5\n1\n(0.5) \\times (1) = 0.5\n\n\n5\n1.5\n3\n(1.5) \\times (3) = 4.5\n\n\n6\n2.5\n2\n(2.5) \\times (2) = 5.0\n\n\nSuma\n\n\n21.0\n\n\n\ns_{XY} = \\frac{21.0}{5} = 4.2\n\n# Weryfikacja kowariancji\ncat(\"Kowariancja:\", cov(practice, skill), \"\\n\")\n\nKowariancja: 4.2 \n\n\n\n\nKrok 4: Obliczenie Współczynnika Korelacji Pearsona\nWspółczynnik korelacji standaryzuje kowariancję do skali od -1 do +1:\nr = \\frac{s_{XY}}{s_X \\cdot s_Y} = \\frac{4.2}{1.871 \\times 2.608} = \\frac{4.2}{4.879} = 0.861\nOtrzymujemy korelację wynoszącą 0.861, co wskazuje na silny dodatni związek między godzinami ćwiczeń a oceną umiejętności.\n\n# Weryfikacja korelacji\ncat(\"Korelacja:\", cor(practice, skill), \"\\n\")\n\nKorelacja: 0.8609161 \n\n\n\n\nKrok 5: Obliczenie Współczynników Regresji MNK\nMetoda Najmniejszych Kwadratów (MNK) znajduje wartości \\beta_0 i \\beta_1, które minimalizują sumę kwadratów błędów.\nEstymator nachylenia:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\nUżywając naszych obliczonych wartości:\n\\hat{\\beta_1} = \\frac{4.2}{3.5} = 1.2\nEstymator wyrazu wolnego:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 7 - (1.2 \\times 3.5) = 7 - 4.2 = 2.8\nNasze równanie regresji:\n\\hat{Y} = 2.8 + 1.2X\nTo oznacza:\n\nGdy godziny ćwiczeń = 0, przewidywana umiejętność = 2.8\nKażda dodatkowa godzina ćwiczeń zwiększa ocenę umiejętności o 1.2 punktu\n\n\n# Dopasowanie modelu\nmodel &lt;- lm(skill ~ practice)\ncoef_model &lt;- coef(model)\n\ncat(\"Wyraz wolny (β₀):\", coef_model[1], \"\\n\")\n\nWyraz wolny (β₀): 2.8 \n\ncat(\"Nachylenie (β₁):\", coef_model[2], \"\\n\")\n\nNachylenie (β₁): 1.2 \n\n# Obliczenie przewidywań\ndata$predicted &lt;- predict(model)\ndata$residual &lt;- residuals(model)\n\n\n\nKrok 6: Obliczenie Wartości Przewidywanych i Reszt\nUżywając \\hat{Y} = 2.8 + 1.2X:\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 2.8 + 1.2X_i\nReszta (Y_i - \\hat{Y}_i)\n\n\n\n\n1\n1\n3\n2.8 + 1.2(1) = 4.0\n3 - 4.0 = -1.0\n\n\n2\n2\n7\n2.8 + 1.2(2) = 5.2\n7 - 5.2 = 1.8\n\n\n3\n3\n5\n2.8 + 1.2(3) = 6.4\n5 - 6.4 = -1.4\n\n\n4\n4\n8\n2.8 + 1.2(4) = 7.6\n8 - 7.6 = 0.4\n\n\n5\n5\n10\n2.8 + 1.2(5) = 8.8\n10 - 8.8 = 1.2\n\n\n6\n6\n9\n2.8 + 1.2(6) = 10.0\n9 - 10.0 = -1.0\n\n\n\n\n\nKrok 7: Obliczenie Sum Kwadratów\nSST (Całkowita Suma Kwadratów) - Całkowita zmienność Y:\nSST = \\sum(Y_i - \\bar{Y})^2\nZ naszych wcześniejszych obliczeń wariancji:\nSST = (n-1) \\times s^2_Y = 5 \\times 6.8 = 34\nSSE (Suma Kwadratów Błędów) - Zmienność niewyjaśniona:\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n\n\n\n\n\n\n\n\n\n\nStudent\nY_i\n\\hat{Y}_i\n(Y_i - \\hat{Y}_i)\n(Y_i - \\hat{Y}_i)^2\n\n\n\n\n1\n3\n4.0\n-1.0\n1.00\n\n\n2\n7\n5.2\n1.8\n3.24\n\n\n3\n5\n6.4\n-1.4\n1.96\n\n\n4\n8\n7.6\n0.4\n0.16\n\n\n5\n10\n8.8\n1.2\n1.44\n\n\n6\n9\n10.0\n-1.0\n1.00\n\n\nSuma\n\n\n\n8.80\n\n\n\nSSE = 8.80\nSSR (Suma Kwadratów Regresji) - Zmienność wyjaśniona:\nSSR = SST - SSE = 34 - 8.80 = 25.20\n\n\nKrok 8: Obliczenie R-kwadrat\nR-kwadrat informuje nas, jaka część całkowitej zmienności Y jest wyjaśniona przez nasz model:\nR^2 = \\frac{SSR}{SST} = \\frac{25.20}{34} = 0.741\nAlternatywny wzór:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{8.80}{34} = 1 - 0.259 = 0.741\nLub po prostu:\nR^2 = r^2 = (0.861)^2 = 0.741\nTo oznacza, że nasz model wyjaśnia 74.1% zmienności ocen umiejętności!\n\n# Weryfikacja sum kwadratów i R-kwadrat\nSST &lt;- sum((skill - mean_y)^2)\nSSR &lt;- sum((data$predicted - mean_y)^2)\nSSE &lt;- sum(data$residual^2)\nr_squared &lt;- SSR / SST\n\ncat(\"SST (Całkowita):\", SST, \"\\n\")\n\nSST (Całkowita): 34 \n\ncat(\"SSR (Wyjaśniona):\", SSR, \"\\n\")\n\nSSR (Wyjaśniona): 25.2 \n\ncat(\"SSE (Niewyjaśniona):\", SSE, \"\\n\")\n\nSSE (Niewyjaśniona): 8.8 \n\ncat(\"R-kwadrat:\", r_squared, \"\\n\")\n\nR-kwadrat: 0.7411765 \n\ncat(\"R-kwadrat (z korelacji):\", cor(practice, skill)^2, \"\\n\")\n\nR-kwadrat (z korelacji): 0.7411765 \n\n\n\n\n\nWizualizacja 1: Linia Najlepszego Dopasowania MNK\nTen wykres pokazuje, jak MNK minimalizuje sumę kwadratów reszt (pionowe odległości od punktów do linii).\n\nggplot(data, aes(x = Practice, y = Skill)) +\n  geom_hline(yintercept = mean_y, linetype = \"dashed\", \n             color = \"gray50\", linewidth = 0.8, alpha = 0.7) +\n  geom_segment(aes(xend = Practice, yend = predicted), \n               color = \"red\", linetype = \"dotted\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1.2) +\n  geom_point(size = 5, color = \"darkblue\") +\n  geom_point(aes(y = predicted), size = 3, color = \"blue\", shape = 17) +\n  annotate(\"text\", x = 1.5, y = mean_y + 0.3, \n           label = paste0(\"Średnia (ȳ = \", mean_y, \")\"), \n           color = \"gray30\", size = 4) +\n  annotate(\"text\", x = 5, y = 4, \n           label = \"Reszty (błędy)\\nMNK minimalizuje Σ(reszty²)\", \n           color = \"red\", size = 3.5, hjust = 0) +\n  annotate(\"text\", x = 2, y = 10.5, \n           label = paste0(\"ŷ = \", round(coef_model[1], 1), \n                         \" + \", round(coef_model[2], 1), \"x\"), \n           color = \"blue\", size = 5, fontface = \"bold\") +\n  labs(\n    title = \"Regresja MNK: Minimalizacja Kwadratów Reszt\",\n    subtitle = \"Niebieskie trójkąty to wartości przewidywane; czerwone linie pokazują reszty\",\n    x = \"Godziny Ćwiczeń\",\n    y = \"Ocena Umiejętności\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKluczowa Obserwacja: MNK znajduje unikalną linię, która czyni sumę kwadratów czerwonych odległości jak najmniejszą!\n\n\nWizualizacja 2: Dekompozycja Wariancji (SST = SSR + SSE)\nTo pokazuje, jak całkowita zmienność jest dzielona na składniki wyjaśniony i niewyjaśniony.\n\n# Tworzenie wykresu dekompozycji\nggplot(data, aes(x = Practice)) +\n  # Całkowite odchylenie (SST)\n  geom_segment(aes(y = mean_y, yend = Skill, xend = Practice), \n               color = \"purple\", linewidth = 1.2, alpha = 0.6,\n               arrow = arrow(length = unit(0.15, \"inches\"))) +\n  \n  # Odchylenie wyjaśnione (SSR)\n  geom_segment(aes(y = mean_y, yend = predicted, xend = Practice), \n               color = \"green\", linewidth = 1.5, alpha = 0.8) +\n  \n  # Odchylenie niewyjaśnione (SSE)\n  geom_segment(aes(y = predicted, yend = Skill, xend = Practice), \n               color = \"red\", linewidth = 1.2, alpha = 0.8,\n               linetype = \"dashed\") +\n  \n  # Linia średniej\n  geom_hline(yintercept = mean_y, linetype = \"solid\", \n             color = \"gray40\", linewidth = 1) +\n  \n  # Linia regresji\n  geom_smooth(aes(y = Skill), method = \"lm\", se = FALSE, \n              color = \"blue\", linewidth = 1) +\n  \n  # Punkty\n  geom_point(aes(y = Skill), size = 5, color = \"darkblue\") +\n  geom_point(aes(y = predicted), size = 3, color = \"blue\", shape = 15) +\n  \n  # Adnotacje\n  annotate(\"text\", x = 6.5, y = mean_y, \n           label = \"Średnia\", color = \"gray40\", size = 4, hjust = 0) +\n  annotate(\"text\", x = 6.5, y = 9.5, \n           label = \"Linia Regresji\", color = \"blue\", size = 4, hjust = 0) +\n  \n  # Legenda\n  annotate(\"segment\", x = 0.5, xend = 0.5, y = 2, yend = 3.5,\n           color = \"purple\", linewidth = 1.2, \n           arrow = arrow(length = unit(0.12, \"inches\"))) +\n  annotate(\"text\", x = 0.7, y = 2.75, \n           label = \"Całkowite (SST)\", color = \"purple\", \n           size = 3.5, hjust = 0, fontface = \"bold\") +\n  \n  annotate(\"segment\", x = 0.5, xend = 0.5, y = 4.5, yend = 5.5,\n           color = \"green\", linewidth = 1.5) +\n  annotate(\"text\", x = 0.7, y = 5, \n           label = \"Wyjaśnione (SSR)\", color = \"green\", \n           size = 3.5, hjust = 0, fontface = \"bold\") +\n  \n  annotate(\"segment\", x = 0.5, xend = 0.5, y = 6.5, yend = 7.5,\n           color = \"red\", linewidth = 1.2, linetype = \"dashed\") +\n  annotate(\"text\", x = 0.7, y = 7, \n           label = \"Niewyjaśnione (SSE)\", color = \"red\", \n           size = 3.5, hjust = 0, fontface = \"bold\") +\n  \n  labs(\n    title = \"Dekompozycja Wariancji: SST = SSR + SSE\",\n    subtitle = \"Fioletowe = odchylenie całkowite | Zielone = wyjaśnione przez model | Czerwone = błąd resztowy\",\n    x = \"Godziny Ćwiczeń\",\n    y = \"Ocena Umiejętności\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 14)) +\n  coord_cartesian(xlim = c(0.3, 7.5), ylim = c(2, 11))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nDekompozycja Matematyczna:\nDla każdej obserwacji: (Y_i - \\bar{Y})^2 = (\\hat{Y}_i - \\bar{Y})^2 + (Y_i - \\hat{Y}_i)^2\n\nFioletowe strzałki: Całkowite odchylenie od średniej = (Y_i - \\bar{Y})\nZielone słupki: Wyjaśnienie modelu = (\\hat{Y}_i - \\bar{Y})\nCzerwone przerywane: Co zostaje = (Y_i - \\hat{Y}_i)\n\n\n\nWizualizacja 3: R-kwadrat jako Proporcja\n\n# Obliczenie sum kwadratów\nSST &lt;- sum((skill - mean_y)^2)\nSSR &lt;- sum((data$predicted - mean_y)^2)\nSSE &lt;- sum(data$residual^2)\nr_squared &lt;- SSR / SST\n\n# Dane do wykresu słupkowego\nss_data &lt;- data.frame(\n  Component = c(\"Całkowita (SST)\", \"Wyjaśniona (SSR)\", \"Niewyjaśniona (SSE)\"),\n  Value = c(SST, SSR, SSE),\n  Color = c(\"purple\", \"green\", \"red\")\n)\n\np1 &lt;- ggplot(ss_data, aes(x = Component, y = Value, fill = Component)) +\n  geom_bar(stat = \"identity\", alpha = 0.7, color = \"black\") +\n  geom_text(aes(label = round(Value, 2)), vjust = -0.5, size = 5, fontface = \"bold\") +\n  scale_fill_manual(values = c(\"green\", \"red\", \"purple\")) +\n  labs(\n    title = \"Rozkład Sum Kwadratów\",\n    y = \"Suma Kwadratów\",\n    x = \"\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"))\n\n# Wizualizacja proporcji\nprop_data &lt;- data.frame(\n  Component = c(\"Wyjaśniona\\n(SSR)\", \"Niewyjaśniona\\n(SSE)\"),\n  Proportion = c(SSR/SST, SSE/SST),\n  Percentage = c(SSR/SST * 100, SSE/SST * 100)\n)\n\np2 &lt;- ggplot(prop_data, aes(x = \"\", y = Proportion, fill = Component)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\", linewidth = 2) +\n  coord_polar(theta = \"y\") +\n  scale_fill_manual(values = c(\"green3\", \"red3\")) +\n  geom_text(aes(label = paste0(round(Percentage, 1), \"%\")), \n            position = position_stack(vjust = 0.5), \n            size = 6, fontface = \"bold\", color = \"white\") +\n  labs(title = paste0(\"R² = \", round(r_squared, 3))) +\n  theme_void(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n        legend.position = \"bottom\",\n        legend.title = element_blank())\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\nWzory na R-kwadrat:\nR^2 = \\frac{SSR}{SST} = \\frac{25.20}{34} = 0.74\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{8.80}{34} = 0.74\nInterpretacja: Nasz model wyjaśnia 74% zmienności ocen umiejętności!\n\n\nWizualizacja 4: R² jako Korelacja między Odchyleniami\nTo pokazuje geometryczną interpretację R²: jak dobrze przewidywane odchylenia od średniej pasują do rzeczywistych odchyleń.\n\n# Obliczenie odchyleń\ndata$actual_dev &lt;- skill - mean_y\ndata$predicted_dev &lt;- data$predicted - mean_y\n\n# Porównanie obok siebie\np1 &lt;- ggplot(data, aes(x = Practice)) +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.8) +\n  geom_segment(aes(y = 0, yend = actual_dev, xend = Practice), \n               color = \"purple\", linewidth = 2, alpha = 0.7,\n               arrow = arrow(length = unit(0.15, \"inches\"))) +\n  geom_point(aes(y = actual_dev), size = 4, color = \"purple\") +\n  labs(\n    title = \"Rzeczywiste Odchylenia od Średniej\",\n    subtitle = expression(Y[i] - bar(Y)),\n    x = \"Godziny Ćwiczeń\",\n    y = \"Odchylenie od Średniej\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(data, aes(x = Practice)) +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.8) +\n  geom_segment(aes(y = 0, yend = predicted_dev, xend = Practice), \n               color = \"green\", linewidth = 2, alpha = 0.7,\n               arrow = arrow(length = unit(0.15, \"inches\"))) +\n  geom_point(aes(y = predicted_dev), size = 4, color = \"green\") +\n  labs(\n    title = \"Przewidywane Odchylenia od Średniej\",\n    subtitle = expression(hat(Y)[i] - bar(Y)),\n    x = \"Godziny Ćwiczeń\",\n    y = \"Odchylenie od Średniej\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(plot.title = element_text(face = \"bold\"))\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nWizualizacja 5: Wykres Rozrzutu Odchyleń (Interpretacja R²)\n\nggplot(data, aes(x = predicted_dev, y = actual_dev)) +\n  geom_abline(slope = 1, intercept = 0, \n              linetype = \"dashed\", color = \"gray50\", linewidth = 1) +\n  geom_point(size = 5, color = \"darkblue\", alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\", \n              fill = \"lightblue\", alpha = 0.3) +\n  annotate(\"text\", x = -3, y = 2.5, \n           label = paste0(\"Gdyby dopasowanie idealne:\\nwszystkie punkty na tej linii\\n(R² = 1)\"), \n           color = \"gray40\", size = 3.5, hjust = 0) +\n  annotate(\"text\", x = 1.5, y = -3, \n           label = paste0(\"Korelacja = \", round(sqrt(r_squared), 3), \n                         \"\\nR² = \", round(r_squared, 3)), \n           color = \"blue\", size = 5, fontface = \"bold\") +\n  labs(\n    title = \"R² Mierzy, Jak Dobrze Przewidywane Odchylenia Pasują do Rzeczywistych\",\n    subtitle = \"Każdy punkt porównuje przewidywanie modelu z rzeczywistością (obie względem średniej)\",\n    x = expression(paste(\"Odchylenie Przewidywane: \", hat(Y)[i] - bar(Y))),\n    y = expression(paste(\"Odchylenie Rzeczywiste: \", Y[i] - bar(Y)))\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 13))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKluczowa Obserwacja:\nR² to dosłownie kwadrat korelacji między:\n\nTym, co przewiduje model (względem średniej)\nTym, co rzeczywiście się zdarzyło (względem średniej)\n\nJeśli R^2 = 1: wszystkie punkty leżą dokładnie na przekątnej (idealne przewidywania)\nJeśli R^2 = 0: brak związku między przewidywanymi a rzeczywistymi odchyleniami\n\n\nTabela Podsumowująca\n\n# Tworzenie tabeli podsumowującej\nsummary_stats &lt;- data.frame(\n  Statystyka = c(\"Liczebność próby (n)\", \n                \"Średnia Ćwiczeń (X̄)\", \n                \"Średnia Umiejętności (Ȳ)\",\n                \"Korelacja (r)\",\n                \"Wyraz wolny (β₀)\",\n                \"Nachylenie (β₁)\",\n                \"R-kwadrat\",\n                \"SST (Całkowita)\",\n                \"SSR (Wyjaśniona)\",\n                \"SSE (Niewyjaśniona)\"),\n  Wartość = c(6,\n            round(mean_x, 2),\n            round(mean_y, 2),\n            round(cor(practice, skill), 3),\n            round(coef_model[1], 2),\n            round(coef_model[2], 2),\n            round(r_squared, 3),\n            round(SST, 2),\n            round(SSR, 2),\n            round(SSE, 2))\n)\n\nknitr::kable(summary_stats, align = c(\"l\", \"r\"))\n\n\n\n\nStatystyka\nWartość\n\n\n\n\nLiczebność próby (n)\n6.000\n\n\nŚrednia Ćwiczeń (X̄)\n3.500\n\n\nŚrednia Umiejętności (Ȳ)\n7.000\n\n\nKorelacja (r)\n0.861\n\n\nWyraz wolny (β₀)\n2.800\n\n\nNachylenie (β₁)\n1.200\n\n\nR-kwadrat\n0.741\n\n\nSST (Całkowita)\n34.000\n\n\nSSR (Wyjaśniona)\n25.200\n\n\nSSE (Niewyjaśniona)\n8.800\n\n\n\n\n\n\n\nKluczowe Wnioski\nRegresja MNK:\n\nZnajduje linię minimalizującą sumę kwadratów reszt\nProdukuje nieobciążone estymatory o minimalnej wariancji (BLUE)\n\nR-kwadrat (0.74) oznacza:\n\n74% zmienności umiejętności jest wyjaśnione przez godziny ćwiczeń\nKorelacja między przewidywanymi a rzeczywistymi odchyleniami wynosi \\sqrt{0.74} = 0.86\nSSR stanowi 74% SST; SSE stanowi 26% SST\n\nInterpretacja Geometryczna:\n\nCałkowita zmienność = odległość każdego punktu od średniej\nModel uchwytuje 74% tych odległości przez linię regresji\nPozostałe 26% jest niewyjaśnione (reszty)\n\nImplikacja Praktyczna:\nKażda dodatkowa godzina ćwiczeń zwiększa oczekiwaną umiejętność o 1.2 punktu, a ten związek wyjaśnia większość (ale nie całość) obserwowanej zmienności!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przykład-1.-analiza-związku-między-dobrobytem-ekonomicznym-a-frekwencją-wyborczą",
    "href": "correg_pl.html#przykład-1.-analiza-związku-między-dobrobytem-ekonomicznym-a-frekwencją-wyborczą",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.17 Przykład 1. Analiza związku między dobrobytem ekonomicznym a frekwencją wyborczą",
    "text": "10.17 Przykład 1. Analiza związku między dobrobytem ekonomicznym a frekwencją wyborczą\nAnaliza związku między dobrobytem ekonomicznym a frekwencją wyborczą w dzielnicach Amsterdamu na podstawie danych z wyborów samorządowych 2022.\n\nDane\nPróba obejmuje pięć reprezentatywnych dzielnic:\n\n\n\nDzielnica\nDochód (tys. €)\nFrekwencja (%)\n\n\n\n\nNoord\n50\n60\n\n\nZuid\n45\n56\n\n\nCentrum\n56\n70\n\n\nWest\n40\n50\n\n\nOost\n60\n75\n\n\n\n\n# Wczytanie bibliotek\nlibrary(tidyverse)\n\n# Utworzenie zbioru danych\ndane &lt;- data.frame(\n  dzielnica = c(\"Noord\", \"Zuid\", \"Centrum\", \"West\", \"Oost\"),\n  dochod = c(50, 45, 56, 40, 60),\n  frekwencja = c(60, 56, 70, 50, 75)\n)\n\n# Podgląd danych\ndane\n\n  dzielnica dochod frekwencja\n1     Noord     50         60\n2      Zuid     45         56\n3   Centrum     56         70\n4      West     40         50\n5      Oost     60         75\n\n\n\n\nCzęść 1: Statystyki opisowe\n\n# Statystyki dla dochodu\ncat(\"DOCHÓD (tys. €):\\n\")\n\nDOCHÓD (tys. €):\n\ncat(\"Średnia:\", mean(dane$dochod), \"\\n\")\n\nŚrednia: 50.2 \n\ncat(\"Mediana:\", median(dane$dochod), \"\\n\")\n\nMediana: 50 \n\ncat(\"Odchylenie standardowe:\", round(sd(dane$dochod), 2), \"\\n\")\n\nOdchylenie standardowe: 8.07 \n\ncat(\"Zakres:\", min(dane$dochod), \"-\", max(dane$dochod), \"\\n\\n\")\n\nZakres: 40 - 60 \n\n# Statystyki dla frekwencji\ncat(\"FREKWENCJA (%):\\n\")\n\nFREKWENCJA (%):\n\ncat(\"Średnia:\", mean(dane$frekwencja), \"\\n\")\n\nŚrednia: 62.2 \n\ncat(\"Mediana:\", median(dane$frekwencja), \"\\n\")\n\nMediana: 60 \n\ncat(\"Odchylenie standardowe:\", round(sd(dane$frekwencja), 2), \"\\n\")\n\nOdchylenie standardowe: 10.21 \n\ncat(\"Zakres:\", min(dane$frekwencja), \"-\", max(dane$frekwencja))\n\nZakres: 50 - 75\n\n\n\n\nCzęść 2: Analiza korelacji\n\n# Test korelacji Pearsona\nkorelacja &lt;- cor.test(dane$dochod, dane$frekwencja)\n\ncat(\"Współczynnik korelacji (r):\", round(korelacja$estimate, 3), \"\\n\")\n\nWspółczynnik korelacji (r): 0.994 \n\ncat(\"P-value:\", round(korelacja$p.value, 3), \"\\n\")\n\nP-value: 0.001 \n\n# Interpretacja\nif (korelacja$p.value &lt; 0.05) {\n  cat(\"Wynik jest statystycznie istotny (p &lt; 0.05)\")\n} else {\n  cat(\"Wynik nie jest statystycznie istotny (p ≥ 0.05)\")\n}\n\nWynik jest statystycznie istotny (p &lt; 0.05)\n\n\n\n\nCzęść 3: Model regresji liniowej\n\n# Dopasowanie modelu\nmodel &lt;- lm(frekwencja ~ dochod, data = dane)\n\n# Podstawowe informacje o modelu\nsummary(model)\n\n\nCall:\nlm(formula = frekwencja ~ dochod, data = dane)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \ndochod       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n# R-squared - jak dobrze model wyjaśnia dane\ncat(\"\\nModel wyjaśnia\", round(summary(model)$r.squared * 100, 1), \"% zmienności frekwencji\")\n\n\nModel wyjaśnia 98.9 % zmienności frekwencji\n\n\n\n\nCzęść 4: Wizualizacja\n\n# Wykres rozrzutu z linią regresji\nggplot(dane, aes(x = dochod, y = frekwencja)) +\n  geom_point(size = 4, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\", alpha = 0.3) +\n  geom_text(aes(label = dzielnica), vjust = -1, size = 4) +\n  labs(\n    title = \"Dochód vs Frekwencja wyborcza\",\n    subtitle = paste(\"r =\", round(korelacja$estimate, 3)),\n    x = \"Średni dochód (tys. €)\",\n    y = \"Frekwencja wyborcza (%)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCzęść 5: Interpretacja wyników\n\n# Pobranie współczynników\nwspolczynniki &lt;- coef(model)\nprzeciecie &lt;- round(wspolczynniki[1], 1)\nnachylenie &lt;- round(wspolczynniki[2], 2)\n\ncat(\"RÓWNANIE REGRESJI:\\n\")\n\nRÓWNANIE REGRESJI:\n\ncat(\"Frekwencja =\", przeciecie, \"+\", nachylenie, \"× Dochód\\n\\n\")\n\nFrekwencja = -0.9 + 1.26 × Dochód\n\ncat(\"INTERPRETACJA:\\n\")\n\nINTERPRETACJA:\n\ncat(\"• Wzrost dochodu o 1000€ zwiększa frekwencję o\", nachylenie, \"punktów procentowych\\n\")\n\n• Wzrost dochodu o 1000€ zwiększa frekwencję o 1.26 punktów procentowych\n\ncat(\"• Korelacja jest\", ifelse(korelacja$estimate &gt; 0, \"dodatnia\", \"ujemna\"), \n    \"i\", ifelse(abs(korelacja$estimate) &gt; 0.7, \"silna\", \n               ifelse(abs(korelacja$estimate) &gt; 0.5, \"umiarkowana\", \"słaba\")))\n\n• Korelacja jest dodatnia i silna\n\n\n\n\nWnioski\nGłówne ustalenia:\n\nIstnieje silna dodatnia korelacja (r = 0.994) między dochodem a frekwencją wyborczą\nModel wyjaśnia 98.9% zmienności w danych\nDzielnice z wyższymi dochodami mają wyższą frekwencję wyborczą\n\nWażne ograniczenie:\n⚠️ Mała próba (n=5) oznacza, że wyniki należy traktować ostrożnie i nie można ich generalizować na całą populację bez dodatkowych badań.\nPraktyczne zastosowanie:\nWyniki sugerują, że działania mające na celu zwiększenie frekwencji wyborczej powinny szczególnie koncentrować się na dzielnicach o niższych dochodach.\nOgraniczenia i zastrzeżenia:\n⚠️ Krytyczne ograniczenia:\n\nBardzo mała próba (n=5) znacznie ogranicza możliwość generalizacji\nNiska moc statystyczna - ryzyko błędów II rodzaju\nBrak kontroli zmiennych zakłócających (wiek, wykształcenie, gęstość zaludnienia)\nMożliwa korelacja pozorna - potrzebne dodatkowe zmienne kontrolne\n\nRekomendacje dla przyszłych badań:\n\nZwiększenie próby do wszystkich dzielnic Amsterdamu\nWłączenie zmiennych demograficznych i socjoekonomicznych\nAnaliza danych longitudinalnych z kilku cykli wyborczych",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przykład-2.-związek-między-wielkością-okręgu-a-dysproporcjonalnością-wyborczą-1",
    "href": "correg_pl.html#przykład-2.-związek-między-wielkością-okręgu-a-dysproporcjonalnością-wyborczą-1",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.18 Przykład 2. Związek Między Wielkością Okręgu a Dysproporcjonalnością Wyborczą (1)",
    "text": "10.18 Przykład 2. Związek Między Wielkością Okręgu a Dysproporcjonalnością Wyborczą (1)\nTa analiza bada związek między wielkością okręgu wyborczego (DM) a wskaźnikiem dysproporcjonalności Loosemore-Hanby (LH) w wyborach parlamentarnych w 6 krajach. Indeks Loosemore-Hanby mierzy dysproporcjonalność wyborczą, gdzie wyższe wartości wskazują na większą dysproporcjonalność między głosami a mandatami.\n\nDane\n\n\nWarning: package 'knitr' was built under R version 4.4.3\n\n\n\nWielkość Okręgu i Indeks LH według Kraju\n\n\nCountry\nDM\nLH\n\n\n\n\nA\n3\n15.50\n\n\nB\n4\n14.25\n\n\nC\n5\n13.50\n\n\nD\n6\n13.50\n\n\nE\n7\n13.00\n\n\nF\n8\n12.75\n\n\n\n\n\n\n\nKrok 1: Obliczenie wariancji i odchylenia standardowego dla DM i LH\n\nObliczenia dla Wielkości Okręgu (DM)\nNajpierw obliczam średnią wartości DM:\n\\bar{x}_{DM} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\frac{3 + 4 + 5 + 6 + 7 + 8}{6} = \\frac{33}{6} = 5.5\nNastępnie obliczam wariancję używając formuły z korektą Bessela:\n\\sigma^2_{DM} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\n\n\nKraj\nDM (x_i)\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\n\n\n\n\nA\n3\n3 - 5.5 = -2.5\n(-2.5)^2 = 6.25\n\n\nB\n4\n4 - 5.5 = -1.5\n(-1.5)^2 = 2.25\n\n\nC\n5\n5 - 5.5 = -0.5\n(-0.5)^2 = 0.25\n\n\nD\n6\n6 - 5.5 = 0.5\n(0.5)^2 = 0.25\n\n\nE\n7\n7 - 5.5 = 1.5\n(1.5)^2 = 2.25\n\n\nF\n8\n8 - 5.5 = 2.5\n(2.5)^2 = 6.25\n\n\nSuma\n\n\n17.5\n\n\n\n\\sigma^2_{DM} = \\frac{17.5}{6-1} = \\frac{17.5}{5} = 3.5\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji:\n\\sigma_{DM} = \\sqrt{\\sigma^2_{DM}} = \\sqrt{3.5} = 1.871\n\n\nObliczenia dla Indeksu LH\nNajpierw obliczam średnią wartości LH:\n\\bar{x}_{LH} = \\frac{15.5 + 14.25 + 13.5 + 13.5 + 13 + 12.75}{6} = \\frac{82.5}{6} = 13.75\nNastępnie obliczam wariancję:\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n15.5\n15.5 - 13.75 = 1.75\n(1.75)^2 = 3.0625\n\n\nB\n14.25\n14.25 - 13.75 = 0.5\n(0.5)^2 = 0.25\n\n\nC\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nD\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nE\n13\n13 - 13.75 = -0.75\n(-0.75)^2 = 0.5625\n\n\nF\n12.75\n12.75 - 13.75 = -1\n(-1)^2 = 1\n\n\nSuma\n\n\n5\n\n\n\n\\sigma^2_{LH} = \\frac{5}{6-1} = \\frac{5}{5} = 1\nOdchylenie standardowe wynosi:\n\\sigma_{LH} = \\sqrt{\\sigma^2_{LH}} = \\sqrt{1} = 1\nPodsumowanie Zadania 1:\n\nWariancja DM (z korektą Bessela): 3.5\nOdchylenie Standardowe DM: 1.871\nWariancja LH (z korektą Bessela): 1\nOdchylenie Standardowe LH: 1\n\n\n\n\nKrok 2: Obliczenie kowariancji między DM i LH dla tej próby danych\nKowariancja jest obliczana przy użyciu formuły z korektą Bessela:\nCov(DM, LH) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\nA\n3\n15.5\n-2.5\n1.75\n(-2.5)(1.75) = -4.375\n\n\nB\n4\n14.25\n-1.5\n0.5\n(-1.5)(0.5) = -0.75\n\n\nC\n5\n13.5\n-0.5\n-0.25\n(-0.5)(-0.25) = 0.125\n\n\nD\n6\n13.5\n0.5\n-0.25\n(0.5)(-0.25) = -0.125\n\n\nE\n7\n13\n1.5\n-0.75\n(1.5)(-0.75) = -1.125\n\n\nF\n8\n12.75\n2.5\n-1\n(2.5)(-1) = -2.5\n\n\nSuma\n\n\n\n\n-8.75\n\n\n\nCov(DM, LH) = \\frac{-8.75}{5} = -1.75\nKowariancja między DM i LH: -1.75\nUjemna kowariancja wskazuje na odwrotną zależność: gdy wielkość okręgu wzrasta, indeks dysproporcjonalności LH ma tendencję do spadku.\n\n\nKrok 3: Obliczenie współczynnika korelacji liniowej Pearsona między DM i LH\nWspółczynnik korelacji Pearsona obliczany jest przy użyciu formuły:\nr = \\frac{Cov(DM, LH)}{\\sigma_{DM} \\cdot \\sigma_{LH}}\nMamy już obliczone:\n\nCov(DM, LH) = -1.75\n\\sigma_{DM} = 1.871\n\\sigma_{LH} = 1\n\nr = \\frac{-1.75}{1.871 \\cdot 1} = \\frac{-1.75}{1.871} = -0.935\nWspółczynnik korelacji Pearsona: -0.935\n\nInterpretacja:\nWspółczynnik korelacji -0.935 wskazuje:\n\nKierunek: Znak ujemny pokazuje odwrotną zależność między wielkością okręgu a indeksem LH.\nSiła: Wartość bezwzględna 0.935 wskazuje na bardzo silną korelację (blisko -1).\nInterpretacja praktyczna: Ponieważ wyższe wartości indeksu LH wskazują na większą dysproporcjonalność, ta silna ujemna korelacja sugeruje, że gdy wielkość okręgu wzrasta, dysproporcjonalność wyborcza ma tendencję do znacznego spadku. Innymi słowy, systemy wyborcze z większymi okręgami (więcej przedstawicieli wybieranych z jednego okręgu) zwykle dają bardziej proporcjonalne wyniki (niższa dysproporcjonalność).\n\nOdkrycie to jest zgodne z teorią nauk politycznych, która sugeruje, że większe okręgi zapewniają więcej możliwości mniejszym partiom, aby uzyskać reprezentację, co prowadzi do wyników wyborczych, które lepiej odzwierciedlają rozkład głosów między partiami.\n\n\n\nKrok 4: Skonstruowanie modelu regresji liniowej prostej i obliczenie R-kwadrat\nFormuła dla regresji liniowej prostej:\nLH = \\beta_0 + \\beta_1 \\cdot DM + \\varepsilon\nGdzie:\n\n\\beta_0 to wyraz wolny (przecięcie)\n\\beta_1 to współczynnik nachylenia dla DM\n\nFormuła do obliczenia \\beta_1 to:\n\\beta_1 = \\frac{Cov(DM, LH)}{\\sigma^2_{DM}} = \\frac{-1.75}{3.5} = -0.5\nAby obliczyć \\beta_0, używam:\n\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x} = 13.75 - (-0.5) \\cdot 5.5 = 13.75 + 2.75 = 16.5\nZatem równanie regresji to:\nLH = 16.5 - 0.5 \\cdot DM\n\nObliczanie wartości przewidywanych i błędów\nUżywając naszego równania regresji, obliczam przewidywane wartości LH:\n\\hat{LH} = 16.5 - 0.5 \\cdot DM\n\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i = 16.5 - 0.5x_i)\nBłąd (y_i - \\hat{y}_i)\nBłąd bezwzględny (|y_i - \\hat{y}_i|)\nBłąd kwadratowy ([y_i - \\hat{y}_i]^2)\n\n\n\n\nA\n3\n15.5\n16.5 - 0.5(3) = 16.5 - 1.5 = 15\n15.5 - 15 = 0.5\n|0.5| = 0.5\n(0.5)^2 = 0.25\n\n\nB\n4\n14.25\n16.5 - 0.5(4) = 16.5 - 2 = 14.5\n14.25 - 14.5 = -0.25\n|-0.25| = 0.25\n(-0.25)^2 = 0.0625\n\n\nC\n5\n13.5\n16.5 - 0.5(5) = 16.5 - 2.5 = 14\n13.5 - 14 = -0.5\n|-0.5| = 0.5\n(-0.5)^2 = 0.25\n\n\nD\n6\n13.5\n16.5 - 0.5(6) = 16.5 - 3 = 13.5\n13.5 - 13.5 = 0\n|0| = 0\n(0)^2 = 0\n\n\nE\n7\n13\n16.5 - 0.5(7) = 16.5 - 3.5 = 13\n13 - 13 = 0\n|0| = 0\n(0)^2 = 0\n\n\nF\n8\n12.75\n16.5 - 0.5(8) = 16.5 - 4 = 12.5\n12.75 - 12.5 = 0.25\n|0.25| = 0.25\n(0.25)^2 = 0.0625\n\n\nSuma\n\n\n\n\n1.5\n0.625\n\n\n\n\n\nObliczanie R-kwadrat\n\nSST (Całkowita suma kwadratów)\n\nSST = \\sum_{i=1}^{n}(y_i - \\bar{y})^2\nObliczyliśmy już te odchylenia w Kroku 1:\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n15.5\n1.75\n3.0625\n\n\nB\n14.25\n0.5\n0.25\n\n\nC\n13.5\n-0.25\n0.0625\n\n\nD\n13.5\n-0.25\n0.0625\n\n\nE\n13\n-0.75\n0.5625\n\n\nF\n12.75\n-1\n1\n\n\nSuma\n\n\n5\n\n\n\nSST = 5\n\nSSR (Suma kwadratów regresji)\n\nSSR = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2\n\n\n\n\n\n\n\n\n\nKraj\nPrzewidywane LH (\\hat{y}_i)\n\\hat{y}_i - \\bar{y}\n(\\hat{y}_i - \\bar{y})^2\n\n\n\n\nA\n15\n15 - 13.75 = 1.25\n(1.25)^2 = 1.5625\n\n\nB\n14.5\n14.5 - 13.75 = 0.75\n(0.75)^2 = 0.5625\n\n\nC\n14\n14 - 13.75 = 0.25\n(0.25)^2 = 0.0625\n\n\nD\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nE\n13\n13 - 13.75 = -0.75\n(-0.75)^2 = 0.5625\n\n\nF\n12.5\n12.5 - 13.75 = -1.25\n(-1.25)^2 = 1.5625\n\n\nSuma\n\n\n4.375\n\n\n\nSSR = 4.375\n\nSSE (Suma kwadratów błędów)\n\nSSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\nZ tabeli powyżej, suma kwadratów błędów wynosi:\nSSE = 0.625\n\nWeryfikacja\n\nMożemy zweryfikować nasze obliczenia sprawdzając, czy SST = SSR + SSE:\n5 = 4.375 + 0.625 = 5 \\checkmark\n\nObliczanie R-kwadrat\n\nR^2 = \\frac{SSR}{SST} = \\frac{4.375}{5} = 0.875\n\n\nObliczanie RMSE (Root Mean Square Error)\nRMSE jest obliczane przy użyciu formuły:\nRMSE = \\sqrt{\\frac{SSE}{n}} = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}}\nUżywając naszego obliczonego SSE:\nRMSE = \\sqrt{\\frac{0.625}{6}} = \\sqrt{0.104} \\approx 0.323\n\n\nObliczanie MAE (Mean Absolute Error)\nMAE jest obliczane przy użyciu formuły:\nMAE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}\nUżywając sum z tabeli:\nMAE = \\frac{1.5}{6} = 0.25\nModel regresji: LH = 16.5 - 0.5 \\cdot DM\nR-kwadrat: 0.875\nRMSE: 0.323\nMAE: 0.25\n\n\nInterpretacja:\n\nRównanie regresji: Dla każdego wzrostu jednostkowego wielkości okręgu, indeks dysproporcjonalności LH jest oczekiwany spadek o 0.5 jednostki. Wyraz wolny (16.5) reprezentuje oczekiwany indeks LH, gdy wielkość okręgu wynosi zero (choć nie ma to praktycznego znaczenia, ponieważ wielkość okręgu nie może wynosić zero).\nR-kwadrat: 0.875 wskazuje, że około 87.5% wariancji w dysproporcjonalności wyborczej (indeks LH) może być wyjaśnione przez wielkość okręgu. Jest to wysoka wartość, sugerująca, że wielkość okręgu jest rzeczywiście silnym predyktorem dysproporcjonalności wyborczej.\nRMSE i MAE: Niskie wartości RMSE (0.323) i MAE (0.25) wskazują, że model dobrze dopasowuje się do danych, z małymi błędami predykcji.\nImplikacje polityczne: Odkrycia sugerują, że zwiększanie wielkości okręgu mogłoby być skuteczną strategią reformy wyborczej dla krajów dążących do zmniejszenia dysproporcjonalności między głosami a mandatami. Jednak korzyści marginalne wydają się zmniejszać wraz ze wzrostem wielkości okręgu, jak widać w wzorcu danych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przykład-3.-analiza-związku-między-wielkością-okręgu-a-wskaźnikiem-dysproporcjonalności-wyborczej-2",
    "href": "correg_pl.html#przykład-3.-analiza-związku-między-wielkością-okręgu-a-wskaźnikiem-dysproporcjonalności-wyborczej-2",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.19 Przykład 3. Analiza związku między wielkością okręgu a wskaźnikiem dysproporcjonalności wyborczej (2)",
    "text": "10.19 Przykład 3. Analiza związku między wielkością okręgu a wskaźnikiem dysproporcjonalności wyborczej (2)\nTa analiza bada związek między wielkością okręgu (DM) a wskaźnikiem dysproporcjonalności Loosemore-Hanby (LH) w wyborach parlamentarnych w 6 krajach. Wskaźnik Loosemore-Hanby mierzy dysproporcjonalność wyborczą, przy czym wyższe wartości wskazują na większą dysproporcjonalność między głosami a mandatami.\n\nDane\n\n\n\nWielkość okręgu i wskaźnik LH według kraju\n\n\nKraj\nDM\nLH\n\n\n\n\nA\n4\n12\n\n\nB\n10\n8\n\n\nC\n3\n15\n\n\nD\n8\n10\n\n\nE\n7\n6\n\n\nF\n4\n13\n\n\n\n\n\n\n\nKrok 1: Obliczenie wariancji i odchylenia standardowego dla DM i LH\n\nObliczenia dla wielkości okręgu (DM)\nNajpierw obliczę średnią wartości DM:\n\\bar{x}_{DM} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\frac{4 + 10 + 3 + 8 + 7 + 4}{6} = \\frac{36}{6} = 6\nNastępnie obliczę wariancję z korektą Bessela, korzystając z wzoru:\ns^2_{DM} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\n\n\nKraj\nDM (x_i)\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\n\n\n\n\nA\n4\n4 - 6 = -2\n(-2)^2 = 4\n\n\nB\n10\n10 - 6 = 4\n(4)^2 = 16\n\n\nC\n3\n3 - 6 = -3\n(-3)^2 = 9\n\n\nD\n8\n8 - 6 = 2\n(2)^2 = 4\n\n\nE\n7\n7 - 6 = 1\n(1)^2 = 1\n\n\nF\n4\n4 - 6 = -2\n(-2)^2 = 4\n\n\nSuma\n\n\n38\n\n\n\ns^2_{DM} = \\frac{38}{6-1} = \\frac{38}{5} = 7.6\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji:\ns_{DM} = \\sqrt{s^2_{DM}} = \\sqrt{7.6} = 2.757\n\n\nObliczenia dla wskaźnika LH\nNajpierw obliczę średnią wartości LH:\n\\bar{y}_{LH} = \\frac{12 + 8 + 15 + 10 + 6 + 13}{6} = \\frac{64}{6} = 10.667\nNastępnie obliczę wariancję z korektą Bessela:\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n12\n12 - 10.667 = 1.333\n(1.333)^2 = 1.777\n\n\nB\n8\n8 - 10.667 = -2.667\n(-2.667)^2 = 7.113\n\n\nC\n15\n15 - 10.667 = 4.333\n(4.333)^2 = 18.775\n\n\nD\n10\n10 - 10.667 = -0.667\n(-0.667)^2 = 0.445\n\n\nE\n6\n6 - 10.667 = -4.667\n(-4.667)^2 = 21.781\n\n\nF\n13\n13 - 10.667 = 2.333\n(2.333)^2 = 5.443\n\n\nSuma\n\n\n55.334\n\n\n\ns^2_{LH} = \\frac{55.334}{6-1} = \\frac{55.334}{5} = 11.067\nOdchylenie standardowe to:\ns_{LH} = \\sqrt{s^2_{LH}} = \\sqrt{11.067} = 3.327\nPodsumowanie kroku 1:\n\nWariancja DM (z korektą Bessela): 7.6\nOdchylenie standardowe DM: 2.757\nWariancja LH (z korektą Bessela): 11.067\nOdchylenie standardowe LH: 3.327\n\n\n\n\nKrok 2: Obliczenie kowariancji między DM a LH dla tej próby danych\nKowariancja jest obliczana przy użyciu wzoru z korektą Bessela:\nCov(DM, LH) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\nA\n4\n12\n-2\n1.333\n(-2)(1.333) = -2.666\n\n\nB\n10\n8\n4\n-2.667\n(4)(-2.667) = -10.668\n\n\nC\n3\n15\n-3\n4.333\n(-3)(4.333) = -12.999\n\n\nD\n8\n10\n2\n-0.667\n(2)(-0.667) = -1.334\n\n\nE\n7\n6\n1\n-4.667\n(1)(-4.667) = -4.667\n\n\nF\n4\n13\n-2\n2.333\n(-2)(2.333) = -4.666\n\n\nSuma\n\n\n\n\n-37\n\n\n\nCov(DM, LH) = \\frac{-37}{6-1} = \\frac{-37}{5} = -7.4\nKowariancja między DM a LH: -7.4\nUjemna kowariancja wskazuje na odwrotną zależność: wraz ze wzrostem wielkości okręgu wskaźnik dysproporcjonalności LH ma tendencję do spadku.\n\n\nKrok 3: Obliczenie współczynnika korelacji liniowej Pearsona między DM a LH\nWspółczynnik korelacji Pearsona oblicza się przy użyciu wzoru:\nr = \\frac{Cov(DM, LH)}{s_{DM} \\cdot s_{LH}}\nMamy już obliczone:\n\nCov(DM, LH) = -7.4\ns_{DM} = 2.757\ns_{LH} = 3.327\n\nr = \\frac{-7.4}{2.757 \\cdot 3.327} = \\frac{-7.4}{9.172} = -0.807\nWspółczynnik korelacji Pearsona: -0.807\n\nInterpretacja:\nWspółczynnik korelacji -0.807 wskazuje:\n\nKierunek: Ujemny znak pokazuje odwrotną zależność między wielkością okręgu a wskaźnikiem LH.\nSiła: Wartość bezwzględna 0.807 wskazuje na silną korelację (blisko -1).\nInterpretacja praktyczna: Ponieważ wyższe wartości wskaźnika LH wskazują na większą dysproporcjonalność, ta silna ujemna korelacja sugeruje, że wraz ze wzrostem wielkości okręgu, dysproporcjonalność wyborcza ma tendencję do znacznego spadku. Innymi słowy, systemy wyborcze z większymi okręgami wyborczymi (więcej przedstawicieli wybieranych w okręgu) mają tendencję do generowania bardziej proporcjonalnych wyników (mniejsza dysproporcjonalność).\n\nUstalenie to jest zgodne z teorią nauk politycznych, która sugeruje, że większe okręgi wyborcze zapewniają mniejszym partiom więcej możliwości uzyskania reprezentacji, co prowadzi do wyników wyborczych, które lepiej odzwierciedlają rozkład głosów między partiami.\n\n\n\nKrok 4: Skonstruowanie prostego modelu regresji liniowej i obliczenie R-kwadrat\nUżyję wzoru na prostą regresję liniową:\nLH = \\beta_0 + \\beta_1 \\cdot DM + \\varepsilon\nGdzie:\n\n\\beta_0 to wyraz wolny\n\\beta_1 to współczynnik nachylenia dla DM\n\nWzór na obliczenie \\beta_1 z korektą Bessela to:\n\\beta_1 = \\frac{Cov(DM, LH)}{s^2_{DM}} = \\frac{-7.4}{7.6} = -0.974\nAby obliczyć \\beta_0, użyję:\n\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x} = 10.667 - (-0.974) \\cdot 6 = 10.667 + 5.844 = 16.511\nZatem równanie regresji to:\nLH = 16.511 - 0.974 \\cdot DM\n\nObliczanie R-kwadrat\nAby właściwie obliczyć R-kwadrat, muszę obliczyć następujące sumy kwadratów:\n\nSST (Całkowita suma kwadratów): Mierzy całkowitą zmienność zmiennej zależnej (LH)\nSSR (Suma kwadratów regresji): Mierzy zmienność wyjaśnioną przez model regresji\nSSE (Suma kwadratów błędów): Mierzy niewyjaśnioną zmienność modelu\n\nNajpierw obliczę przewidywane wartości LH, używając naszego równania regresji:\n\\hat{LH} = 16.511 - 0.974 \\cdot DM\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i = 16.511 - 0.974x_i)\n\n\n\n\nA\n4\n12\n16.511 - 0.974(4) = 16.511 - 3.896 = 12.615\n\n\nB\n10\n8\n16.511 - 0.974(10) = 16.511 - 9.74 = 6.771\n\n\nC\n3\n15\n16.511 - 0.974(3) = 16.511 - 2.922 = 13.589\n\n\nD\n8\n10\n16.511 - 0.974(8) = 16.511 - 7.792 = 8.719\n\n\nE\n7\n6\n16.511 - 0.974(7) = 16.511 - 6.818 = 9.693\n\n\nF\n4\n13\n16.511 - 0.974(4) = 16.511 - 3.896 = 12.615\n\n\n\nTeraz, obliczę każdą sumę kwadratów:\n\nSST (Całkowita suma kwadratów)\n\nSST = \\sum_{i=1}^{n}(y_i - \\bar{y})^2\nJuż obliczyliśmy te odchylenia w Kroku 1:\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n12\n1.333\n1.777\n\n\nB\n8\n-2.667\n7.113\n\n\nC\n15\n4.333\n18.775\n\n\nD\n10\n-0.667\n0.445\n\n\nE\n6\n-4.667\n21.781\n\n\nF\n13\n2.333\n5.443\n\n\nSuma\n\n\n55.334\n\n\n\nSST = 55.334\n\nSSR (Suma kwadratów regresji)\n\nSSR = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2\n\n\n\n\n\n\n\n\n\nKraj\nPrzewidywane LH (\\hat{y}_i)\n\\hat{y}_i - \\bar{y}\n(\\hat{y}_i - \\bar{y})^2\n\n\n\n\nA\n12.615\n12.615 - 10.667 = 1.948\n(1.948)^2 = 3.795\n\n\nB\n6.771\n6.771 - 10.667 = -3.896\n(-3.896)^2 = 15.178\n\n\nC\n13.589\n13.589 - 10.667 = 2.922\n(2.922)^2 = 8.538\n\n\nD\n8.719\n8.719 - 10.667 = -1.948\n(-1.948)^2 = 3.795\n\n\nE\n9.693\n9.693 - 10.667 = -0.974\n(-0.974)^2 = 0.949\n\n\nF\n12.615\n12.615 - 10.667 = 1.948\n(1.948)^2 = 3.795\n\n\nSuma\n\n\n36.05\n\n\n\nSSR = 36.05\n\nSSE (Suma kwadratów błędów)\n\nSSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i)\ny_i - \\hat{y}_i\n(y_i - \\hat{y}_i)^2\n\n\n\n\nA\n12\n12.615\n12 - 12.615 = -0.615\n(-0.615)^2 = 0.378\n\n\nB\n8\n6.771\n8 - 6.771 = 1.229\n(1.229)^2 = 1.510\n\n\nC\n15\n13.589\n15 - 13.589 = 1.411\n(1.411)^2 = 1.991\n\n\nD\n10\n8.719\n10 - 8.719 = 1.281\n(1.281)^2 = 1.641\n\n\nE\n6\n9.693\n6 - 9.693 = -3.693\n(-3.693)^2 = 13.638\n\n\nF\n13\n12.615\n13 - 12.615 = 0.385\n(0.385)^2 = 0.148\n\n\nSuma\n\n\n\n19.306\n\n\n\nSSE = 19.306\n\nWeryfikacja\n\nMożemy zweryfikować nasze obliczenia, sprawdzając czy SST = SSR + SSE:\n55.334 \\approx 36.05 + 19.306 = 55.356\nNiewielka różnica (0.022) wynika z zaokrągleń w obliczeniach.\n\nObliczanie R-kwadrat\n\nR^2 = \\frac{SSR}{SST} = \\frac{36.05}{55.334} = 0.652\nAlternatywnie, możemy też obliczyć:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{19.306}{55.334} = 1 - 0.349 = 0.651\nDrobna różnica wynika z zaokrągleń.\n\n\nObliczanie RMSE (Pierwiastek średniego błędu kwadratowego)\nObliczanie RMSE (Pierwiastek średniego błędu kwadratowego)\nRMSE oblicza się przy użyciu wzoru:\nRMSE = \\sqrt{\\frac{SSE}{n}} = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}}\nKorzystając z naszej obliczonej SSE:\nRMSE = \\sqrt{\\frac{19.306}{6}} = \\sqrt{3.218} = 1.794\nKorekta Bessela (dzielenie przez n-1 zamiast n) stosuje się do estymacji wariancji próby, ale nie jest standardowo stosowana przy obliczaniu RMSE, gdyż RMSE jest miarą błędu predykcji, a nie estymatorem parametru populacji.\n\n\nObliczanie MAE (Średni błąd bezwzględny)\nMAE oblicza się przy użyciu wzoru:\nMAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i)\n|y_i - \\hat{y}_i|\n\n\n\n\nA\n12\n12.615\n|12 - 12.615| = 0.615\n\n\nB\n8\n6.771\n|8 - 6.771| = 1.229\n\n\nC\n15\n13.589\n|15 - 13.589| = 1.411\n\n\nD\n10\n8.719\n|10 - 8.719| = 1.281\n\n\nE\n6\n9.693\n|6 - 9.693| = 3.693\n\n\nF\n13\n12.615\n|13 - 12.615| = 0.385\n\n\nSuma\n\n\n8.614\n\n\n\nMAE = \\frac{8.614}{6} = 1.436\nModel regresji: LH = 16.511 - 0.974 \\cdot DM\nR-kwadrat: 0.651\nRMSE: 1.794\nMAE: 1.436\n\n\nInterpretacja:\n\nRównanie regresji: Dla każdego jednostkowego wzrostu wielkości okręgu, wskaźnik dysproporcjonalności LH zmniejsza się o 0.974 jednostki. Wyraz wolny (16.511) reprezentuje oczekiwany wskaźnik LH, gdy wielkość okręgu wynosi zero (choć nie ma to praktycznego znaczenia, ponieważ wielkość okręgu nie może wynosić zero).\nR-kwadrat: 0.651 wskazuje, że około 65.1% wariancji dysproporcjonalności wyborczej (wskaźnik LH) może być wyjaśnione przez wielkość okręgu. Jest to dość wysoka wartość, sugerująca, że wielkość okręgu jest rzeczywiście silnym predyktorem dysproporcjonalności wyborczej, choć mniejszym niż w poprzednim zestawie danych.\nRMSE: Wartość 1.794 informuje nas o przeciętnym błędzie prognozy modelu. Jest to miara dokładności przewidywań modelu wyrażona w jednostkach zmiennej zależnej (LH).\nMAE: Wartość 1.436 informuje nas o przeciętnym bezwzględnym błędzie prognozy modelu. W porównaniu z RMSE, MAE jest mniej czuły na wartości odstające, co potwierdza, że niektóre obserwacje (np. dla kraju E) mają stosunkowo duży błąd predykcji.\nImplikacje polityczne: Wyniki sugerują, że zwiększenie wielkości okręgu mogłoby być skuteczną strategią reform wyborczych dla krajów starających się zmniejszyć dysproporcjonalność między głosami a mandatami. Jednakże, korzyści marginalne wydają się zmniejszać wraz ze wzrostem wielkości okręgu, jak widać we wzorcu danych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przykład-4.-lęk-vs.-wyniki-egzaminu-analiza-korelacji-i-regresji",
    "href": "correg_pl.html#przykład-4.-lęk-vs.-wyniki-egzaminu-analiza-korelacji-i-regresji",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.20 Przykład 4. Lęk vs. Wyniki Egzaminu: Analiza Korelacji i Regresji",
    "text": "10.20 Przykład 4. Lęk vs. Wyniki Egzaminu: Analiza Korelacji i Regresji\nW tym tutorialu zbadamy związek między poziomem lęku przed egzaminem a wynikami egzaminacyjnymi wśród studentów uniwersytetu. Badania sugerują, że podczas gdy niewielka ilość lęku może być motywująca, nadmierny lęk zazwyczaj pogarsza wyniki poprzez zmniejszoną koncentrację, zakłócenia pamięci roboczej i objawy fizyczne (Yerkes-Dodson law). Przeanalizujemy dane od 8 studentów, aby zrozumieć ten związek matematycznie.\n\nPrezentacja Danych\n\nZbiór Danych\nZebraliśmy dane od 8 studentów, mierząc:\n\nX: Wynik lęku przed testem (skala 1-10, gdzie 1 = bardzo niski, 10 = bardzo wysoki)\nY: Wyniki egzaminu (wynik procentowy)\n\n\n# Nasze dane\nlęk &lt;- c(2.5, 3.2, 4.1, 4.8, 5.6, 6.3, 7.0, 7.9)      # Wyniki lęku\nwyniki &lt;- c(80, 85, 78, 82, 77, 74, 68, 72)            # Wyniki egzaminu (%)\n\n# Stworzenie ramki danych dla łatwego przeglądu\ndane &lt;- data.frame(\n  Student = 1:8,\n  Lęk = lęk,\n  Wyniki = wyniki\n)\nprint(dane)\n\n  Student Lęk Wyniki\n1       1 2.5     80\n2       2 3.2     85\n3       3 4.1     78\n4       4 4.8     82\n5       5 5.6     77\n6       6 6.3     74\n7       7 7.0     68\n8       8 7.9     72\n\n\n\n\nWstępna Wizualizacja\nNajpierw zwizualizujmy nasze dane, aby uzyskać intuicyjne zrozumienie związku:\n\nlibrary(ggplot2)\n\n# Stworzenie wykresu punktowego\nggplot(dane, aes(x = Lęk, y = Wyniki)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  labs(\n    title = \"Lęk przed Testem vs. Wyniki Egzaminu\",\n    x = \"Wynik Lęku przed Testem\",\n    y = \"Wyniki Egzaminu (%)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(2, 8, 1)) +\n  scale_y_continuous(breaks = seq(65, 90, 5))\n\n\n\n\n\n\n\n\n\n\n\nStatystyki Opisowe\n\nObliczanie Średnich\nŚrednia to wartość przeciętna, obliczana przez zsumowanie wszystkich obserwacji i podzielenie przez liczbę obserwacji.\nŚrednia Wyników Lęku (X): \\bar{X} = \\frac{\\sum_{i=1}^{n} X_i}{n} = \\frac{2,5 + 3,2 + 4,1 + 4,8 + 5,6 + 6,3 + 7,0 + 7,9}{8}\n\\bar{X} = \\frac{41,4}{8} = 5,175\nŚrednia Wyników Egzaminu (Y): \\bar{Y} = \\frac{\\sum_{i=1}^{n} Y_i}{n} = \\frac{80 + 85 + 78 + 82 + 77 + 74 + 68 + 72}{8}\n\\bar{Y} = \\frac{616}{8} = 77\n\n# Weryfikacja naszych obliczeń\nśrednia_x &lt;- mean(lęk)\nśrednia_y &lt;- mean(wyniki)\ncat(\"Średni Lęk:\", średnia_x, \"\\n\")\n\nŚredni Lęk: 5.175 \n\ncat(\"Średnie Wyniki:\", średnia_y, \"\\n\")\n\nŚrednie Wyniki: 77 \n\n\n\n\nObliczanie Wariancji i Odchylenia Standardowego\nWariancja mierzy, jak bardzo rozproszone są dane od średniej. Używamy wzoru na wariancję próbkową (dzieląc przez n-1).\nWariancja X:\nNajpierw obliczamy odchylenia od średniej (X_i - \\bar{X}):\n\n\n\nStudent\nX_i\nX_i - \\bar{X}\n(X_i - \\bar{X})^2\n\n\n\n\n1\n2,5\n2,5 - 5,175 = -2,675\n(-2,675)^2 = 7,1556\n\n\n2\n3,2\n3,2 - 5,175 = -1,975\n(-1,975)^2 = 3,9006\n\n\n3\n4,1\n4,1 - 5,175 = -1,075\n(-1,075)^2 = 1,1556\n\n\n4\n4,8\n4,8 - 5,175 = -0,375\n(-0,375)^2 = 0,1406\n\n\n5\n5,6\n5,6 - 5,175 = 0,425\n(0,425)^2 = 0,1806\n\n\n6\n6,3\n6,3 - 5,175 = 1,125\n(1,125)^2 = 1,2656\n\n\n7\n7,0\n7,0 - 5,175 = 1,825\n(1,825)^2 = 3,3306\n\n\n8\n7,9\n7,9 - 5,175 = 2,725\n(2,725)^2 = 7,4256\n\n\nSuma\n\n\n24,555\n\n\n\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1} = \\frac{24,555}{7} = 3,5079\ns_X = \\sqrt{3,5079} = 1,8730\nWariancja Y:\n\n\n\nStudent\nY_i\nY_i - \\bar{Y}\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n80\n80 - 77 = 3\n(3)^2 = 9\n\n\n2\n85\n85 - 77 = 8\n(8)^2 = 64\n\n\n3\n78\n78 - 77 = 1\n(1)^2 = 1\n\n\n4\n82\n82 - 77 = 5\n(5)^2 = 25\n\n\n5\n77\n77 - 77 = 0\n(0)^2 = 0\n\n\n6\n74\n74 - 77 = -3\n(-3)^2 = 9\n\n\n7\n68\n68 - 77 = -9\n(-9)^2 = 81\n\n\n8\n72\n72 - 77 = -5\n(-5)^2 = 25\n\n\nSuma\n\n\n214\n\n\n\ns^2_Y = \\frac{\\sum(Y_i - \\bar{Y})^2}{n-1} = \\frac{214}{7} = 30,5714\ns_Y = \\sqrt{30,5714} = 5,5291\n\n# Weryfikacja wariancji i odchylenia standardowego\ncat(\"Wariancja Lęku:\", var(lęk), \"\\n\")\n\nWariancja Lęku: 3.507857 \n\ncat(\"Odch. Stand. Lęku:\", sd(lęk), \"\\n\")\n\nOdch. Stand. Lęku: 1.872927 \n\ncat(\"Wariancja Wyników:\", var(wyniki), \"\\n\")\n\nWariancja Wyników: 30.57143 \n\ncat(\"Odch. Stand. Wyników:\", sd(wyniki), \"\\n\")\n\nOdch. Stand. Wyników: 5.529144 \n\n\n\n\n\nKowariancja i Korelacja\n\nObliczanie Kowariancji\nKowariancja mierzy, jak dwie zmienne zmieniają się razem. Ujemna kowariancja wskazuje, że gdy jedna zmienna wzrasta, druga ma tendencję do spadku.\ns_{XY} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nObliczmy iloczyny dla każdej obserwacji:\n\n\n\n\n\n\n\n\n\nStudent\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n\n\n\n\n1\n-2,675\n3\n(-2,675) \\times (3) = -8,025\n\n\n2\n-1,975\n8\n(-1,975) \\times (8) = -15,800\n\n\n3\n-1,075\n1\n(-1,075) \\times (1) = -1,075\n\n\n4\n-0,375\n5\n(-0,375) \\times (5) = -1,875\n\n\n5\n0,425\n0\n(0,425) \\times (0) = 0\n\n\n6\n1,125\n-3\n(1,125) \\times (-3) = -3,375\n\n\n7\n1,825\n-9\n(1,825) \\times (-9) = -16,425\n\n\n8\n2,725\n-5\n(2,725) \\times (-5) = -13,625\n\n\nSuma\n\n\n-60,2\n\n\n\ns_{XY} = \\frac{-60,2}{7} = -8,6\n\n\nObliczanie Współczynnika Korelacji Pearsona\nWspółczynnik korelacji standaryzuje kowariancję do skali od -1 do +1:\nr = \\frac{s_{XY}}{s_X \\cdot s_Y} = \\frac{-8,6}{1,8730 \\times 5,5291} = \\frac{-8,6}{10,3560} = -0,831\nTo daje nam korelację -0,831, wskazującą na silny ujemny związek między lękiem a wynikami.\n\n# Weryfikacja korelacji\nrzeczywista_kor &lt;- cor(lęk, wyniki)\ncat(\"Współczynnik korelacji Pearsona:\", rzeczywista_kor, \"\\n\")\n\nWspółczynnik korelacji Pearsona: -0.8304618 \n\n\n\n\n\nProsta Regresja MNK\n\nProblem Modelowania Związków\nKiedy obserwujemy związek między dwiema zmiennymi, chcemy znaleźć model matematyczny, który:\n\nOpisuje związek\nPozwala nam dokonywać prognoz\nKwantyfikuje siłę związku\n\nNajprostszym modelem jest linia prosta: Y = \\beta_0 + \\beta_1 X + \\epsilon\nGdzie:\n\nY to zmienna wynikowa (wyniki)\nX to zmienna predykcyjna (lęk)\n\\beta_0 to punkt przecięcia (wyniki gdy lęk = 0)\n\\beta_1 to nachylenie (zmiana wyników na jednostkę zmiany lęku)\n\\epsilon to składnik błędu (niewyjaśniona zmienność)\n\n\n\nIdea Sumy Kwadratów Błędów (SSE)\nWyobraź sobie próbę narysowania linii przez nasze punkty danych. Jest nieskończenie wiele linii, które moglibyśmy narysować! Niektóre przeszłyby przez środek punktów, inne mogłyby być za wysokie lub za niskie, za strome lub za płaskie. Potrzebujemy systematycznego sposobu określenia, która linia jest “najlepsza”.\n\nCzym są Błędy (Reszty)?\nDla każdej linii, którą narysujemy, każdy punkt danych będzie miał błąd lub resztę - pionową odległość od punktu do linii. To reprezentuje, jak “błędna” jest nasza prognoza dla tego punktu.\n\nBłąd dodatni: Rzeczywista wartość jest powyżej przewidywanej wartości (niedoszacowaliśmy)\nBłąd ujemny: Rzeczywista wartość jest poniżej przewidywanej wartości (przeszacowaliśmy)\n\n\n\nDlaczego Podnosimy Błędy do Kwadratu?\nProste dodawanie błędów nie zadziała, ponieważ błędy dodatnie i ujemne się znoszą. Moglibyśmy użyć wartości bezwzględnych, ale podnoszenie do kwadratu ma kilka zalet:\n\nWygoda matematyczna: Funkcje kwadratowe są różniczkowalne, co ułatwia znalezienie minimum za pomocą rachunku różniczkowego\nPenalizuje większe błędy bardziej: Kilka dużych błędów jest gorsze niż wiele małych błędów\nTworzy gładką, miskowatą funkcję: To gwarantuje unikalne minimum\n\n\n\nWzór SSE\nSuma Kwadratów Błędów to: SSE = \\sum_{i=1}^{n}(Y_i - \\hat{Y_i})^2 = \\sum_{i=1}^{n}(Y_i - (\\beta_0 + \\beta_1 X_i))^2\n\n\nZnajdowanie Najlepszej Linii\n“Najlepsza” linia to ta, która minimalizuje SSE. Używając rachunku różniczkowego (biorąc pochodne względem \\beta_0 i \\beta_1 i przyrównując je do zera), otrzymujemy wzory MNK.\n\n\n\nEstymatory MNK\nMetoda Najmniejszych Kwadratów (MNK) znajduje wartości \\beta_0 i \\beta_1, które minimalizują SSE:\nEstymator nachylenia: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\nEstymator punktu przecięcia: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\n\nObliczanie Parametrów MNK\nUżywając naszych obliczonych wartości:\n\ns_{XY} = -8,6\ns^2_X = 3,5079\n\\bar{X} = 5,175\n\\bar{Y} = 77\n\nKrok 1: Oblicz nachylenie \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-8,6}{3,5079} = -2,451\nKrok 2: Oblicz punkt przecięcia \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} = 77 - (-2,451 \\times 5,175) = 77 + 12,684 = 89,684\n\n# Weryfikacja z R\nmodel &lt;- lm(wyniki ~ lęk)\ncoef(model)\n\n(Intercept)         lęk \n  89.687233   -2.451639 \n\n\nNasze równanie regresji to: \\hat{Y} = 89,684 - 2,451X\nTo oznacza:\n\nGdy lęk = 0, przewidywane wyniki = 89,68%\nZa każdy wzrost lęku o 1 punkt, wyniki spadają o 2,45%\n\n\n\n\nWykres Modelu\n\nlibrary(ggplot2)\n\n# Stworzenie kompleksowego wykresu\nggplot(data.frame(lęk, wyniki), aes(x = lęk, y = wyniki)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1) +\n  geom_text(aes(label = paste0(\"(\", round(lęk, 1), \", \", wyniki, \")\")),\n            vjust = -1, size = 3) +\n  annotate(\"text\", x = 3, y = 70, \n           label = paste0(\"ŷ = \", round(coef(model)[1], 2), \" - \", \n                         abs(round(coef(model)[2], 2)), \"x\"),\n           size = 5, color = \"red\") +\n  labs(\n    title = \"Linia Regresji: Wyniki vs. Lęk\",\n    subtitle = \"Wyższy lęk jest związany z niższymi wynikami egzaminu\",\n    x = \"Wynik Lęku przed Testem\",\n    y = \"Wyniki Egzaminu (%)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(2, 8, 1)) +\n  scale_y_continuous(breaks = seq(65, 90, 5))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nOcena Modelu\n\nDekompozycja Wariancji\nCałkowita zmienność w Y może być rozłożona na dwie części:\nSST = SSE + SSR\nGdzie:\n\nSST (Sum of Squares Total): Całkowita zmienność w Y\nSSE (Sum of Squared Errors): Niewyjaśniona zmienność\n\nSSR (Sum of Squares Regression): Wyjaśniona zmienność\n\n\n\n\n\n\n\nTip\n\n\n\nWyobraź sobie, że chcesz zrozumieć, dlaczego pensje w firmie się różnią. Jedni zarabiają 40 000, inni 80 000, a jeszcze inni 120 000. Mamy więc zmienność wynagrodzeń — nie są takie same.\n\nCałkowita zmienność (SST)\nTo pytanie: „Jak bardzo wszystkie pensje są rozproszone wokół średniej pensji?” Jeśli średnia to 70 000, to SST mierzy, o ile każda pensja różni się od 70 000, podnosi te różnice do kwadratu (żeby były dodatnie) i sumuje. To całkowita ilość zmienności, którą próbujemy wyjaśnić.\n\n\nZmienność wyjaśniona (SSR)\nZałóżmy, że budujemy model przewidujący pensję na podstawie lat doświadczenia. Model może mówić:\n\n2 lata doświadczenia → przewiduje 50 000\n5 lat doświadczenia → przewiduje 70 000\n10 lat doświadczenia → przewiduje 100 000\n\nSSR mierzy, jak bardzo te przewidywania różnią się od średniej. To ta część zmienności, którą model „wyjaśnia” relacją z doświadczeniem. Innymi słowy: „tę część różnic w pensjach da się przypisać różnym poziomom doświadczenia”.\n\n\nZmienność niewyjaśniona (SSE)\nTo to, co zostaje — część, której model nie tłumaczy. Może być tak, że dwie osoby mają po 5 lat doświadczenia, ale jedna zarabia 65 000, a druga 75 000. Model obu przewidział 70 000. Te różnice względem przewidywań (błędy) reprezentują zmienność wynikającą z innych czynników — np. edukacja, wyniki pracy, umiejętności negocjacyjne albo po prostu losowość.\n\n\n10.21 Kluczowa intuicja\nPiękne jest to, że te trzy wielkości zawsze spełniają zależność: Całkowita zmienność = Zmienność wyjaśniona + Zmienność niewyjaśniona czyli SST = SSR + SSE.\nMożesz myśleć o tym jak o „wykresie kołowym powodów, dlaczego pensje się różnią”:\n\nJeden kawałek to „różnice wyjaśnione doświadczeniem” (SSR),\nDrugi kawałek to „różnice z innych powodów” (SSE),\nRazem dają całość (SST).\n\n\n\n10.22 Dlaczego to ważne\nTo rozbicie pozwala policzyć współczynnik determinacji R^2:\n\nR^2 = \\frac{\\mathrm{SSR}}{\\mathrm{SST}} = \\frac{\\text{Zmienność wyjaśniona}}{\\text{Całkowita zmienność}}.\n\nJeśli R^2 = 0{,}70, to model wyjaśnia 70% tego, dlaczego wartości Y (tu: pensje) różnią się między sobą. Pozostałe 30% to czynniki nieuwzględnione lub szum losowy.\nPomyśl o tym jak o rozwiązywaniu zagadki: SST to cała zagadka do rozwiązania, SSR to część już rozwiązana, a SSE to to, co wciąż pozostaje do wyjaśnienia!\n\n\n\nObliczmy każdą:\nKrok 1: Oblicz przewidywane wartości\nUżywając \\hat{Y} = 89,684 - 2,451X:\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i = 89,684 - 2,451X_i\n\n\n\n\n1\n2,5\n80\n89,684 - 2,451(2,5) = 83,556\n\n\n2\n3,2\n85\n89,684 - 2,451(3,2) = 81,841\n\n\n3\n4,1\n78\n89,684 - 2,451(4,1) = 79,635\n\n\n4\n4,8\n82\n89,684 - 2,451(4,8) = 77,919\n\n\n5\n5,6\n77\n89,684 - 2,451(5,6) = 75,968\n\n\n6\n6,3\n74\n89,684 - 2,451(6,3) = 74,253\n\n\n7\n7,0\n68\n89,684 - 2,451(7,0) = 72,527\n\n\n8\n7,9\n72\n89,684 - 2,451(7,9) = 70,321\n\n\n\nKrok 2: Oblicz sumy kwadratów\nSST (Całkowita zmienność): SST = \\sum(Y_i - \\bar{Y})^2\nZ naszych wcześniejszych obliczeń wariancji: SST = (n-1) \\times s^2_Y = 7 \\times 30,5714 = 214\nSSE (Niewyjaśniona zmienność): SSE = \\sum(Y_i - \\hat{Y}_i)^2\n\n\n\n\n\n\n\n\n\n\ni\nY_i\n\\hat{Y}_i\nY_i - \\hat{Y}_i\n(Y_i - \\hat{Y}_i)^2\n\n\n\n\n1\n80\n83,556\n-3,556\n12,645\n\n\n2\n85\n81,841\n3,159\n9,979\n\n\n3\n78\n79,635\n-1,635\n2,673\n\n\n4\n82\n77,919\n4,081\n16,655\n\n\n5\n77\n75,968\n1,032\n1,065\n\n\n6\n74\n74,253\n-0,253\n0,064\n\n\n7\n68\n72,527\n-4,527\n20,494\n\n\n8\n72\n70,321\n1,679\n2,819\n\n\nSuma\n\n\n\n66,394\n\n\n\nSSR (Wyjaśniona zmienność): SSR = SST - SSE = 214 - 66,394 = 147,606\n\n# Weryfikacja obliczeń\ntabela_anova &lt;- anova(model)\ncat(\"SSR (Regresja):\", tabela_anova$`Sum Sq`[1], \"\\n\")\n\nSSR (Regresja): 147.5887 \n\ncat(\"SSE (Reszty):\", tabela_anova$`Sum Sq`[2], \"\\n\")\n\nSSE (Reszty): 66.41132 \n\ncat(\"SST (Całkowita):\", sum(tabela_anova$`Sum Sq`), \"\\n\")\n\nSST (Całkowita): 214 \n\n\n\n\nR-kwadrat (Współczynnik Determinacji)\nR-kwadrat mówi nam, jaka część całkowitej zmienności w Y jest wyjaśniona przez nasz model:\nR^2 = \\frac{SSR}{SST} = \\frac{147,606}{214} = 0,690\nTo oznacza, że nasz model wyjaśnia 69,0% zmienności w wynikach egzaminu.\nAlternatywny wzór używający korelacji: R^2 = r^2 = (-0,831)^2 = 0,691\n\n# Weryfikacja R-kwadrat\ncat(\"R-kwadrat:\", summary(model)$r.squared, \"\\n\")\n\nR-kwadrat: 0.6896667 \n\ncat(\"Korelacja do kwadratu:\", cor(lęk, wyniki)^2, \"\\n\")\n\nKorelacja do kwadratu: 0.6896667 \n\n\n\n\n\nWielkość Efektu\nWspółczynnik nachylenia \\hat{\\beta_1} = -2,451 to nasza wielkość efektu. Mówi nam:\n\nWielkość: Każdy wzrost lęku o 1 punkt jest związany z 2,45% spadkiem wyników\nZnaczenie praktyczne: Student przechodzący od niskiego lęku (3) do wysokiego lęku (7) doświadczyłby oczekiwanego spadku wyników o 2,451 \\times 4 = 9,80\\%\n\nStandaryzowana wielkość efektu (współczynnik korelacji): Korelacja r = -0,831 wskazuje na silny ujemny związek.\nWytyczne Cohena dla interpretacji korelacji:\n\nMały efekt: |r| = 0,10\nŚredni efekt: |r| = 0,30\nDuży efekt: |r| = 0,50\n\nNasze |r| = 0,831 reprezentuje dużą wielkość efektu.\n\n\nPrzedziały Ufności i Istotność Statystyczna\n\nBłąd Standardowy Regresji\nNajpierw potrzebujemy błędu standardowego reszt:\ns_e = \\sqrt{\\frac{SSE}{n-2}} = \\sqrt{\\frac{66,394}{6}} = \\sqrt{11,066} = 3,327\n\n\nBłąd Standardowy Nachylenia\nBłąd standardowy \\hat{\\beta_1} to:\nBE(\\hat{\\beta_1}) = \\frac{s_e}{\\sqrt{\\sum(X_i - \\bar{X})^2}} = \\frac{3,327}{\\sqrt{24,555}} = \\frac{3,327}{4,955} = 0,671\n\n\n95% Przedział Ufności dla Nachylenia\nMówiąc prościej: Przedział ufności daje nam zakres prawdopodobnych wartości dla naszego prawdziwego nachylenia. Gdybyśmy powtórzyli to badanie wiele razy, 95% obliczonych przez nas przedziałów zawierałoby prawdziwą wartość nachylenia.\nWzór używa wartości krytycznej (około 2,45 dla 6 stopni swobody):\nPU = \\hat{\\beta_1} \\pm (wartość\\_krytyczna \\times BE(\\hat{\\beta_1})) PU = -2,451 \\pm (2,45 \\times 0,671) PU = -2,451 \\pm 1,644 PU = [-4,095, -0,807]\nInterpretacja: Jesteśmy w 95% pewni, że prawdziwa zmiana wyników na jednostkę zmiany lęku mieści się między -4,10% a -0,81%.\n\n\nIstotność Statystyczna\nAby sprawdzić, czy związek jest statystycznie istotny (tj. nie jest przypadkowy), obliczamy statystykę t:\nt = \\frac{\\hat{\\beta_1}}{BE(\\hat{\\beta_1})} = \\frac{-2,451}{0,671} = -3,653\nMówiąc prościej: Ta wartość t mówi nam, o ile błędów standardowych nasze nachylenie odbiega od zera. Wartość bezwzględna 3,65 jest dość duża (zazwyczaj wartości przekraczające ±2,45 są uważane za istotne dla naszej wielkości próby), dostarczając silnego dowodu na rzeczywisty ujemny związek między lękiem a wynikami.\n\n# Weryfikacja obliczeń z R\nsummary(model)\n\n\nCall:\nlm(formula = wyniki ~ lęk)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.526 -2.116  0.400  2.050  4.081 \n\nCoefficients:\n            Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)  89.6872     3.6682  24.450 0.000000308 ***\nlęk          -2.4516     0.6714  -3.652      0.0107 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.327 on 6 degrees of freedom\nMultiple R-squared:  0.6897,    Adjusted R-squared:  0.6379 \nF-statistic: 13.33 on 1 and 6 DF,  p-value: 0.01069\n\nconfint(model)\n\n                2.5 %     97.5 %\n(Intercept) 80.711582 98.6628838\nlęk         -4.094474 -0.8088048\n\n\n\n\n\nPodsumowanie i Interpretacja\n\nCzego się Nauczyliśmy\n\nIstnieje ujemny związek między lękiem przed testem a wynikami egzaminu (r = -0,831)\nZwiązek jest umiarkowanie silny - lęk wyjaśnia 69,0% zmienności w wynikach\nEfekt jest znaczący - każdy wzrost lęku o 1 punkt jest związany z około 2,5% spadkiem wyników\nZwiązek jest statystycznie istotny - bardzo mało prawdopodobne, żeby był przypadkowy\n\n\n\nImplikacje Praktyczne\nNasza analiza sugeruje, że wysoki lęk przed testem pogarsza wyniki, prawdopodobnie poprzez:\n\nZakłócenia poznawcze (niepokojące myśli konkurują o pamięć roboczą)\nObjawy fizyczne (pocenie się, szybkie bicie serca), które odwracają uwagę od zadania\nNegatywny monolog wewnętrzny zmniejszający pewność siebie i motywację\nZachowania podczas testu (pośpiech, podważanie odpowiedzi)\n\n\n\nRekomendacje na Podstawie Ustaleń\nBiorąc pod uwagę silny ujemny związek, interwencje mogłyby obejmować:\n\nNauczanie technik zarządzania lękiem (głębokie oddychanie, progresywne rozluźnianie mięśni)\nRestrukturyzację poznawczą w celu radzenia sobie z katastroficznym myśleniem\nTrening umiejętności nauki w celu zwiększenia pewności siebie w przygotowaniu\nTesty próbne w celu zmniejszenia strachu przed nieznanym\n\n\n\nOgraniczenia Naszej Analizy\n\nMała wielkość próby (n = 8) ogranicza możliwość uogólnienia\nZałożenie liniowości - związek może być krzywoliniowy (może istnieć optymalny lęk)\nInne zmienne nieuwzględnione (czas przygotowania, zdolności, sen, itp.)\nPrzyczynowość vs. korelacja - nie możemy udowodnić, że lęk powoduje słabe wyniki\nSamoopisowy lęk - subiektywne miary mogą nie odzwierciedlać pobudzenia fizjologicznego\n\n\n\n\nDodatek: Kompletny Kod R\n\n# Kompletny kod analizy\nlęk &lt;- c(2.5, 3.2, 4.1, 4.8, 5.6, 6.3, 7.0, 7.9)\nwyniki &lt;- c(80, 85, 78, 82, 77, 74, 68, 72)\n\n# Statystyki opisowe\nmean(lęk); sd(lęk)\n\n[1] 5.175\n\n\n[1] 1.872927\n\nmean(wyniki); sd(wyniki)\n\n[1] 77\n\n\n[1] 5.529144\n\n# Korelacja\ncor(lęk, wyniki)\n\n[1] -0.8304618\n\n# Regresja\nmodel &lt;- lm(wyniki ~ lęk)\nsummary(model)\n\n\nCall:\nlm(formula = wyniki ~ lęk)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.526 -2.116  0.400  2.050  4.081 \n\nCoefficients:\n            Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)  89.6872     3.6682  24.450 0.000000308 ***\nlęk          -2.4516     0.6714  -3.652      0.0107 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.327 on 6 degrees of freedom\nMultiple R-squared:  0.6897,    Adjusted R-squared:  0.6379 \nF-statistic: 13.33 on 1 and 6 DF,  p-value: 0.01069\n\nconfint(model)\n\n                2.5 %     97.5 %\n(Intercept) 80.711582 98.6628838\nlęk         -4.094474 -0.8088048\n\n# Wizualizacja\nlibrary(ggplot2)\nggplot(data.frame(lęk, wyniki), aes(x = lęk, y = wyniki)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Diagnostyka\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Tabela ANOVA\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: wyniki\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nlęk        1 147.589 147.589  13.334 0.01069 *\nResiduals  6  66.411  11.069                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kluczowa-intuicja",
    "href": "correg_pl.html#kluczowa-intuicja",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.21 Kluczowa intuicja",
    "text": "10.21 Kluczowa intuicja\nPiękne jest to, że te trzy wielkości zawsze spełniają zależność: Całkowita zmienność = Zmienność wyjaśniona + Zmienność niewyjaśniona czyli SST = SSR + SSE.\nMożesz myśleć o tym jak o „wykresie kołowym powodów, dlaczego pensje się różnią”:\n\nJeden kawałek to „różnice wyjaśnione doświadczeniem” (SSR),\nDrugi kawałek to „różnice z innych powodów” (SSE),\nRazem dają całość (SST).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#dlaczego-to-ważne",
    "href": "correg_pl.html#dlaczego-to-ważne",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.22 Dlaczego to ważne",
    "text": "10.22 Dlaczego to ważne\nTo rozbicie pozwala policzyć współczynnik determinacji R^2:\n\nR^2 = \\frac{\\mathrm{SSR}}{\\mathrm{SST}} = \\frac{\\text{Zmienność wyjaśniona}}{\\text{Całkowita zmienność}}.\n\nJeśli R^2 = 0{,}70, to model wyjaśnia 70% tego, dlaczego wartości Y (tu: pensje) różnią się między sobą. Pozostałe 30% to czynniki nieuwzględnione lub szum losowy.\nPomyśl o tym jak o rozwiązywaniu zagadki: SST to cała zagadka do rozwiązania, SSR to część już rozwiązana, a SSE to to, co wciąż pozostaje do wyjaśnienia!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przykład-5.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis-en",
    "href": "correg_pl.html#przykład-5.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis-en",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.23 Przykład 5. District Magnitude and Electoral Disproportionality: A Comparative Analysis (*) [EN]",
    "text": "10.23 Przykład 5. District Magnitude and Electoral Disproportionality: A Comparative Analysis (*) [EN]\n\nData Generating Process\nLet’s set up a DGP where:\n\\begin{aligned}\n& Y_{\\text{Gallagher}} = 12 - 0.8X_{\\text{DM}} + \\varepsilon \\\\\n& \\varepsilon \\sim \\mathcal{N}(0, 1) \\\\\n& X_{\\text{DM}} \\in \\{3, 5, 7, 10, 12, 15\\}\n\\end{aligned}\n\n# DGP\nmagnitude &lt;- c(3, 5, 7, 10, 12, 15)\nepsilon &lt;- rnorm(6, mean = 0, sd = 1)\ngallagher &lt;- 12 - 0.8 * magnitude + epsilon\n\n# Round (sampled from the DGP) Gallagher indices to one decimal place\ngallagher &lt;- round(c(9.0, 7.8, 9.2, 4.1, 2.5, 1.7), 1)\n\n# Show data\ndata.frame(\n  District_Magnitude = magnitude,\n  Gallagher_Index = gallagher\n)\n\n  District_Magnitude Gallagher_Index\n1                  3             9.0\n2                  5             7.8\n3                  7             9.2\n4                 10             4.1\n5                 12             2.5\n6                 15             1.7\n\n# Initial model check\nmodel &lt;- lm(gallagher ~ magnitude)\nsummary(model)\n\n\nCall:\nlm(formula = gallagher ~ magnitude)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.6516 -0.4628  2.3260 -0.6908 -0.9020  0.3813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7349     1.3034   9.003 0.000843 ***\nmagnitude    -0.6944     0.1359  -5.110 0.006934 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.368 on 4 degrees of freedom\nMultiple R-squared:  0.8672,    Adjusted R-squared:  0.834 \nF-statistic: 26.11 on 1 and 4 DF,  p-value: 0.006934\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{3 + 5 + 7 + 10 + 12 + 15}{6} = \\frac{52}{6} = 8.6667\n\\bar{Y} = \\frac{9.0 + 7.8 + 9.2 + 4.1 + 2.5 + 1.7}{6} = \\frac{34.3}{6} = 5.7167\n\n# Verification\nmean(magnitude)\n\n[1] 8.666667\n\nmean(gallagher)\n\n[1] 5.716667\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(3 - 8.6667) = -5.6667\n(5 - 8.6667) = -3.6667\n(7 - 8.6667) = -1.6667\n(10 - 8.6667) = 1.3333\n(12 - 8.6667) = 3.3333\n(15 - 8.6667) = 6.3333\n\nSquared deviations:\n\n32.1115 + 13.4445 + 2.7779 + 1.7777 + 11.1109 + 40.1107 = 101.3332\n\ns^2_X = \\frac{101.3332}{5} = 20.2666\nFor Y: Deviations: 3.2833, 2.0833, 3.4833, -1.6167, -3.2167, -4.0167\ns^2_Y = \\frac{56.3483}{5} = 11.2697\n\n# Verification\nvar(magnitude)\n\n[1] 20.26667\n\nvar(gallagher)\n\n[1] 11.26967\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n\n(-5.6667 × 3.2833) = -18.6057\n(-3.6667 × 2.0833) = -7.6387\n(-1.6667 × 3.4833) = -5.8056\n(1.3333 × -1.6167) = -2.1556\n(3.3333 × -3.2167) = -10.7223\n(6.3333 × -4.0167) = -25.4391\n\nSum = -70.3670\ns_{XY} = \\frac{-70.3670}{5} = -14.0734\n\n# Verification\ncov(magnitude, gallagher)\n\n[1] -14.07333\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-14.0734}{\\sqrt{20.2666}\\sqrt{11.2697}} = -0.9279\n\n# Verification\ncor(magnitude, gallagher)\n\n[1] -0.9312157\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-14.0734}{20.2666} = -0.6944\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-0.6944 × 8.6667 = -6.0181\n\\hat{\\beta_0} = 5.7167 - (-6.0181) = 11.7348\n\n\n# Verification\ncoef(model)\n\n(Intercept)   magnitude \n 11.7348684  -0.6944079 \n\n\n\n\nR-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = 11.7348 - 0.6944X\n\n# Predicted values\ny_hat &lt;- 11.7348 - 0.6944 * magnitude\ndata.frame(\n  Magnitude = magnitude,\n  Gallagher = gallagher,\n  Predicted = y_hat,\n  row.names = 1:6\n)\n\n  Magnitude Gallagher Predicted\n1         3       9.0    9.6516\n2         5       7.8    8.2628\n3         7       9.2    6.8740\n4        10       4.1    4.7908\n5        12       2.5    3.4020\n6        15       1.7    1.3188\n\n\nStep 2: Sum of Squares SST = \\sum(Y_i - \\bar{Y})^2 = 56.3483 SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 48.5271 SSE = \\sum(Y_i - \\hat{Y}_i)^2 = 7.8212\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{48.5271}{56.3483} = 0.8612\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.8671626\n\n\n\n\nVisualization - True vs. Estimated Parameters\n\nTrue DGP: Y = 12 - 0.8X + ε\nEstimated Model: Y = 11.7348 - 0.6944X\n\n\nlibrary(ggplot2)\n\n# Create data frame with original data\ndf &lt;- data.frame(\n  magnitude = magnitude,\n  gallagher = gallagher\n)\n\n# Create sequence for smooth lines\nx_seq &lt;- seq(min(magnitude), max(magnitude), length.out = 100)\n\n# Calculate predicted values for both lines\ntrue_dgp &lt;- 12 - 0.8 * x_seq\nestimated &lt;- 11.7348 - 0.6944 * x_seq\n\n# Combine into a data frame for plotting\nlines_df &lt;- data.frame(\n  magnitude = rep(x_seq, 2),\n  value = c(true_dgp, estimated),\n  Model = rep(c(\"True DGP\", \"Estimated\"), each = length(x_seq))\n)\n\n# Create plot\nggplot() +\n  geom_line(data = lines_df, \n            aes(x = magnitude, y = value, color = Model, linetype = Model),\n            size = 1) +\n  geom_point(data = df, \n             aes(x = magnitude, y = gallagher),\n             color = \"black\", \n             size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  labs(\n    title = \"True DGP vs. Estimated Regression Line\",\n    subtitle = \"Black points show observed data with random noise\",\n    x = \"District Magnitude\",\n    y = \"Gallagher Index\",\n    caption = \"True DGP: Y = 12 - 0.8X + ε\\nEstimated: Y = 11.73 - 0.69X\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.caption = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\n\nObservations about Model Fit\n\nSlope Comparison\n\nTrue slope: -0.8\nEstimated slope: -0.69\nThe estimated slope is reasonably close to the true parameter\n\nIntercept Comparison\n\nTrue intercept: 12\nEstimated intercept: 11.73\nThe estimated intercept very closely approximates the true value\n\nVisual Patterns\n\nThe lines are nearly parallel, showing good slope recovery\nPoints scatter around both lines due to the random error term (ε)\nThe small sample size (n=6) leads to some imprecision in estimation\nThe estimated line (blue) provides a good approximation of the true DGP (red dashed)\n\nImpact of Random Error\n\nThe scatter of points around the true DGP line reflects the N(0,1) error term\nThis noise leads to the slight differences in estimated parameters\nWith a larger sample, we would expect even closer convergence to true parameters\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.93) between district magnitude and electoral disproportionality\nFor each unit increase in district magnitude, the Gallagher index decreases by approximately 0.69 points\nThe model explains 86.12% of the variance in disproportionality\nThe relationship appears strongly linear with moderate scatter\nThe intercept (11.73) represents the expected disproportionality in a hypothetical single-member district system\n\n\n\nStudy Context\n\nData represents simulated observations from a DGP with moderate noise\nSample shows how increasing district magnitude tends to reduce disproportionality\nRandom component reflects other institutional and political factors affecting disproportionality\n\n\n\nLimitations\n\nSmall sample size (n=6)\nSimulated rather than real-world data\nAssumes linear relationship\nDoes not account for other institutional features",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "inference_en.html",
    "href": "inference_en.html",
    "title": "13  Introduction to Statistical Inference: The Logic of Statistical Hypothesis Testing",
    "section": "",
    "text": "(…)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Statistical Inference: The Logic of Statistical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "inference_pl.html",
    "href": "inference_pl.html",
    "title": "14  Wprowadzenie do Wnioskowania Statystycznego: Logika Testowania Hipotez Statystycznych",
    "section": "",
    "text": "(…)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do Wnioskowania Statystycznego: Logika Testowania Hipotez Statystycznych</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\n\n\n\nBlair, G., Coppock, A., & Humphreys, M. (2023). Research design in the social sciences: declaration, diagnosis, and redesign. Princeton University Press. https://book.declaredesign.org/\nBryman, A., 2016. Social research methods. Oxford University Press.\nBueno de Mesquita, Ethan and Anthony Fowler. 2021. Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis. Princeton University Press.\nCausality for Machine Learning. https://ff13.fastforwardlabs.com/\nCetinkaya-Rundel, M., Diez, D.M. and Barr, C.D., 2019 (4th ed.). OpenIntro Statistics: an Open-source Textbook: https://www.openintro.org/book/os/\nClaude [Large language model], 2024. https://www.anthropic.com\nConcepts and Computation: An Introduction to Political Methodology. https://pos3713.github.io/notes/\nHannay, K. (2019). Introduction to statistics and data science. http://khannay.com/StatsBook/\nIsmay, C. and Kim, A.Y., 2019. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. https://moderndive.com/index.html\nNavarro, D.J. and Foxcroft, D.R. (2019). Learning statistics with Jamovi: a tutorial for psychology students and other beginners. (Version 0.70). DOI: 10.24384/hgc3-7p15\nRemler, D.K. and Van Ryzin, G.G., 2014. Research methods in practice: Strategies for description and causation. Sage Publications.\nSanchez, G., Marzban, E. (2020) All Models Are Wrong: Concepts of Statistical Learning. https://allmodelsarewrong.github.io\nSchneider, W. J. (2023). Psycheval: A psychological evaluation toolkit. https://github.com/wjschne/psycheval\nTimbers, T., Campbell, T., & Lee, M. (2022). Data science: A first introduction. Chapman and Hall/CRC. https://datasciencebook.ca/",
    "crumbs": [
      "References"
    ]
  }
]