[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics: An Introduction (PL: Wprowadzenie do Statystyki)",
    "section": "",
    "text": "Preface\n\n\n\n\n\n\nImportant\n\n\n\nThis is a preliminary (unfinished) draft of a Quarto class notes on Statistics. Please do not cite or reproduce its contents, as it may contain errors!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Foundations of Statistics and Demography",
    "section": "",
    "text": "1.1 Introduction\nStatistics is a way to learn about the world from data. It teaches how to collect data wisely, spot patterns, estimate population parameters, and make predictions—stating how wrong we might be.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#introduction",
    "href": "chapter1.html#introduction",
    "title": "1  Foundations of Statistics and Demography",
    "section": "",
    "text": "Statistics is the science of learning from data under uncertainty.\n\n\n\n\n\n\n\n\nNote\n\n\n\nStatistics is the science of collecting, organizing, analyzing, interpreting, and presenting data. It encompasses both the methods for working with data and the theoretical foundations that justify these methods.\nBut statistics is more than just numbers and formulas—it’s a way of thinking about uncertainty and variation in the world around us.\n\n\n\nWhat is Data?\nData: Information collected during research – this includes survey responses, experimental results, economic indicators, social media content, or any other measurable observations.\nA data distribution describes how values spread across possible outcomes (what values and how often a variable takes). Distributions tell us what values are common, what values are rare, and what patterns exist in our data.\n\n\nDemography is the scientific study of human populations, focusing on their size, structure, distribution, and changes over time. It’s essentially the statistical analysis of people - who they are, where they live, how many there are, and how these characteristics evolve.\n\nStatistics and demography are interconnected disciplines that provide powerful tools for understanding populations, their characteristics, and the patterns that emerge from data.\n\n\n\n\n\n\nRounding and Scientific Notation\n\n\n\nMain Rule: Unless otherwise specified, round the decimal parts of decimal numbers to at least 2 significant figures. In statistics, we often work with long decimal parts and very small numbers — don’t round excessively in intermediate steps, round at the end of calculations.\n\nRounding in Statistical Context\nThe decimal part consists of digits after the decimal point. In statistics, it’s particularly important to maintain appropriate precision:\nDescriptive statistics:\n\nMean: \\bar{x} = 15.847693... \\rightarrow 15.85\nStandard deviation: s = 2.7488... \\rightarrow 2.75\nCorrelation coefficient: r = 0.78432... \\rightarrow 0.78\n\nVery small numbers (p-values, probabilities):\n\np = 0.000347... \\rightarrow 0.00035 or 3.5 \\times 10^{-4}\nP(X &gt; 2) = 0.0000891... \\rightarrow 0.000089 or 8.9 \\times 10^{-5}\n\n\n\nSignificant Figures in Decimal Parts\nIn the decimal part, significant figures are all digits except leading zeros:\n\n.78432 has 5 significant figures → round to .78 (2 s.f.)\n.000347 has 3 significant figures → round to .00035 (2 s.f.)\n.050600 has 4 significant figures → round to .051 (2 s.f.)\n\n\n\nRounding Rules in Statistics\n\nRound only the decimal part to at least 2 significant figures\nThe integer part remains unchanged\nIn long calculations keep 3-4 digits in the decimal part until the final step\nNEVER round to zero - small values have interpretive significance\nFor very small numbers use scientific notation when it improves readability\nP-values often require greater precision — keep 2-3 significant figures\n\n\n\nScientific Notation in Statistics\nIn statistics, we often encounter very small numbers. Use scientific notation when it improves readability:\nP-values and probabilities:\n\np = 0.000347 = 3.47 \\times 10^{-4} (better: 3.5 \\times 10^{-4})\nP(Z &gt; 3.5) = 0.000233 = 2.33 \\times 10^{-4}\n\nLarge numbers (rare in basic statistics):\n\nN = 1\\,234\\,567 = 1.23 \\times 10^6\n\nWhen in doubt: Better to keep an extra digit than to round too aggressively\n\n\n\n\n\n\n\n\n\n\nWhat is Statistics For in Social and Political Science?\n\n\n\nStatistics is essential in social and political science for several key purposes:\nUnderstanding Social Phenomena: Measuring inequality, poverty, unemployment, political participation; describing demographic patterns and social trends; quantifying attitudes, beliefs, and behaviors in populations.\nTesting Theories: Political scientists theorize about democracy, voting behavior, conflict, and institutions. Sociologists develop theories about social mobility, inequality, and group dynamics. Statistics allows us to test whether these theories match reality.\nCausal Inference: Social scientists want to answer “why” questions—Does education increase income? Do democracies go to war less often? Does social media affect political polarization? Statistics helps separate causation from mere correlation.\nPolicy Evaluation: Assessing whether interventions work—Does a job training program reduce unemployment? Did election reform increase voter turnout? Are anti-poverty programs effective? Statistics provides tools to evaluate what works and what doesn’t.\nPublic Opinion Research: Election polls and forecasting; measuring public support for policies; understanding how opinions vary across demographic groups; tracking attitude changes over time.\nMaking Generalizations: We can’t survey everyone, so we sample and use statistics to make inferences about entire populations. A poll of 1,000 people can tell us about a nation of millions (with known uncertainty).\nDealing with Complexity: Human societies are messy—many factors influence outcomes simultaneously. Statistics helps us control for confounding variables, isolate specific effects, and make sense of multivariate relationships.\nThe Uniqueness of Social Sciences: Unlike natural sciences, social sciences study human behavior, which is highly variable and context-dependent. Statistics provides the tools to find patterns and draw conclusions despite this inherent uncertainty.\n\n\n\n\nWhen working with data, statisticians use two different approaches: exploration and confirmation/verification (inferential statistics). First, we examine the data to understand its characteristics and identify patterns. Then, we use formal methods to test specific hypotheses and draw conclusions.\n\n\n\n\n\n\n\nEDA vs. Inferential Statistics\n\n\n\nStatistics can be viewed as two complementary phases:\n\nExploratory Data Analysis (EDA): combines descriptive statistics and visualization methods to explore data, uncover patterns, check assumptions, and generate hypotheses.\n\nInferential Statistics: uses probability models to test hypotheses and draw conclusions that generalize beyond the observed data.\n\n\n\n\n\n\n\n\n\nPercent vs Percentage Points (pp)\n\n\n\nWhen news reports say “unemployment decreased by 2,” do they mean 2 percentage points (pp) or 2 percent?\nThese are not the same:\n\n2 pp (absolute change): e.g., 10% → 8% (−2 pp).\n2% (relative change): multiply the old rate by 0.98; e.g., 10% → 9.8% (−0.2 pp).\n\nAlways ask:\n\nWhat is the baseline (earlier rate)?\nIs the change absolute (pp) or relative (%)?\nCould this be sampling error / random variation?\nHow was unemployment measured (survey vs. administrative), when, and who’s included?\n\nRule of thumb\n\nUse percentage points (pp) when comparing rates directly (unemployment, turnout).\nUse percent (%) for relative changes (proportional to the starting value).\n\nTiny lookup table\n\n\n\n\n\n\n\n\nStarting rate\n“Down 2%” (relative)\n“Down 2 pp” (absolute)\n\n\n\n\n6%\n6% × 0.98 = 5.88% (−0.12 pp)\n4%\n\n\n8%\n8% × 0.98 = 7.84% (−0.16 pp)\n6%\n\n\n10%\n10% × 0.98 = 9.8% (−0.2 pp)\n8%\n\n\n\nUwaga (PL): 2% ≠ 2 punkty procentowe (pp).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#exploratory-data-analysis-eda",
    "href": "chapter1.html#exploratory-data-analysis-eda",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.2 Exploratory Data Analysis (EDA)",
    "text": "1.2 Exploratory Data Analysis (EDA)\nWhat is EDA? Exploratory Data Analysis is the initial step where we examine data systematically to understand its structure and characteristics. This phase does not involve formal hypothesis testing—it focuses on discovering what the data contains.\nWhy do we do EDA?\n\nFind interesting patterns you didn’t expect\nSpot mistakes or unusual values in your data\nGet ideas about what questions to ask\nUnderstand what your data looks like before doing formal tests (many statistical methods have specific requirements about the data to work properly. EDA helps check whether our data meets these requirements - e.g. 1) some tests require data to have a normal distribution (bell-shaped), 2) we need to verify that the relationship between variables is actually linear, or 3) check homogeneity of variance and find outliers)\n\n\n\n\n\n\n\nThe EDA Approach\n\n\n\nWhen conducting EDA, we begin without predetermined hypotheses. Instead, we examine data from multiple perspectives to discover patterns and generate questions for further investigation.\n\n\n\nSimple Tools for Exploring Data\n1. Summary Numbers (Descriptive Statistics)\nThese are basic calculations that describe your data:\nFinding the “Typical” Value:\n\nArithmetic Mean (Average): Add up all values and divide by how many you have. Example: If 5 students scored 70, 80, 85, 90, and 100 on a test, the average is 85.\nMedian (Middle): The value in the middle when you line up all numbers from smallest to largest. In our test example, the median is also 85.\nMode (Most Common): The value that appears most often. If ten families have 1, 2, 2, 2, 2, 3, 3, 3, 4, and 5 children, the mode is 2 children.\n\nUnderstanding Spread:\n\nRange: Just subtract the smallest number from the biggest. If students’ ages go from 18 to 24, the range is 6 years.\nStandard Deviation: Shows how spread out your data is from the average. A small standard deviation means most values are close to the average; a large one means they’re more spread out.\n\n2. Visual Exploration\nGraphical methods help reveal patterns that numerical summaries alone might not show:\n\nPopulation Pyramids: Show how many people are in each age group, split by males and females. Helps you see if a population is young or old.\nBox Plots: Show the middle of your data and help spot unusual values\nScatter Plots: Display relationships between two variables (such as hours studied versus test scores)\nTime (Series) Graphs: Show how something changes over time (like temperature throughout the year)\nHistograms: A histogram is a graphical representation of data that shows the frequency distribution of a dataset. It consists of adjacent bars (with no gaps between them) where each bar represents a range of values (called a bin or class interval), and the height of the bar shows how many data points (what proportion of data points) fall within that range. Histograms are used to visualize the shape, spread, and central tendency of numerical data.\n\n\n\n\nhttps://commons.wikimedia.org/wiki/File:%C5%81%C3%B3d%C5%BA_population_pyramid.svg\n\n\n3. Looking for Connections/Associations:\n\nDo two variables move together? (When one goes up, does the other go up too?)\nCan you draw a line (regression line) that roughly fits your data points?\nDo you see any clear patterns or trends?\n\n\n\n\n\n\n\nUsing the Same Techniques for Different Purposes\n\n\n\nMany statistical techniques serve both exploratory and confirmatory functions:\nExploring: We calculate correlations or fit regression lines to understand what relationships exist in the data. The focus is on discovering patterns.\nConfirming: We apply statistical tests to determine whether observed patterns are statistically significant or could have occurred by chance. The focus is on formal hypothesis testing.\nThe same technique can serve different purposes depending on the research phase.\n\n\n4. Good Questions to Ask While Exploring:\n\nWhat does the shape of my data look like?\nAre there any weird or unusual values?\nDo I see any patterns?\nIs any data missing?\nDo different groups show different patterns?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#inferential-statistics",
    "href": "chapter1.html#inferential-statistics",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.3 Inferential Statistics",
    "text": "1.3 Inferential Statistics\nAfter exploring, you might want to make formal conclusions. Inferential statistics helps you do this.\nThe Basic Idea: You have data from some people (a sample), but you want to know about everyone (the population). Inferential statistics helps you make educated guesses about the bigger group based on your smaller group.\n\n\n\n\n\n\nNote\n\n\n\nA random sample requires that each member has a known, non-zero chance of being selected, not necessarily an equal chance.\nWhen every member has an equal chance of selection, that’s specifically called a simple random sample - which is the most basic type.\n\n\n\n\n\n\n\n\nA Soup-Tasting Analogy\n\n\n\n\nConsider a chef preparing soup for 100 people who needs to assess its flavor without consuming the entire batch:\nPopulation: The entire pot of soup (100 servings)\nSample: A single spoonful for tasting\nPopulation Parameter: The true average saltiness of the complete pot (unknown)\nSample Statistic: The saltiness level detected in the spoonful (observable, a point estimate)\nStatistical Inference: Using the spoonful’s characteristics to draw conclusions about the entire pot\n\nKey points\n1. Random sampling is essential. The cook should stir thoroughly or sample from random locations. Skimming only the surface can miss seasonings that settled to the bottom, introducing systematic bias.\n2. Sample size drives precision. A larger ladle — or more spoonfuls (larger n) — reduces random error and gives a more stable estimate of the “average taste,” though cost and time limit how much you can increase n.\n3. Uncertainty is unavoidable. Even with proper sampling, a single spoonful may not perfectly represent the whole pot; there is always random variability.\n4. Systematic bias undermines inference. If someone secretly adds salt only where you sample, conclusions about the whole pot will be distorted — a classic case of sampling bias.\n5. One sample is limited. A single taste can tell you the average saltiness, but not how much it varies across the pot. To assess variability, you need multiple independent samples.\nNote: Increasing n improves precision (less noise) but does not remove bias; eliminating bias requires fixing the sampling design.\nThis analogy captures the essence of statistical reasoning: using carefully selected samples to learn about larger populations while explicitly acknowledging and quantifying the inherent uncertainty in this process.\n\n\n\n\n\nStatistical Thinking\n\n\n\n\n\n\n\nKey concepts (at a glance)\n\n\n\nPipeline: Research question → Estimand (population quantity) → Parameter (true, unknown value) → Estimator (sample rule/statistic; random) → Estimate (the single number from your data)\nWhat we want to know:\n\nEstimand — the population quantity we aim to learn (the formal target), not the sentence itself.\nExample: “Population mean age at first birth in Poland in 2023.”\nParameter (\\theta) — the true but unknown value of that estimand in the population (fixed, not random).\nExample: The true mean \\mu (e.g., \\mu=29.4 years).\n\n\nHow we estimate (3 steps):\n\nSample statistic — any function of the sample (a rule), e.g.\n\\displaystyle \\bar{X}=\\frac{1}{n}\\sum_{i=1}^n X_i\nEstimator — that statistic chosen to estimate a specific parameter (depends on a random sample, so it’s random).\nExample: Use \\bar{X} as an estimator of \\mu.\nEstimate (\\hat\\theta) — the numerical result after applying the estimator to your observed data (x_1,\\dots,x_n).\nExample: \\hat\\mu=\\bar{x}=29.1 years.\n\n\nAnalogy:\nStatistic = tool → Estimator = tool chosen for a goal → Estimate = the finished output (your concrete result)\n\nCommon estimators\n\n\n\n\n\n\n\n\n\nTarget parameter (goal)\nEstimator (statistic)\nFormula\nNote\n\n\n\n\nPopulation mean \\mu\nSample mean\n\\bar X=\\frac{1}{n}\\sum_{i=1}^n X_i\nUnbiased estimator. The estimator \\bar X is a random variable; a specific calculated value (e.g., \\bar x = 5.2) is called an estimate.\n\n\nPopulation proportion p\nSample proportion\n\\hat p=\\frac{K}{n} where K=\\sum_{i=1}^n Y_i for Y_i\\in\\{0,1\\}\nEquivalent to \\bar Y when encoding outcomes as 0/1. Here K counts the number of successes in n trials.\n\n\nPopulation variance \\sigma^2\nSample variance\ns^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2\nThe n-1 divisor (Bessel’s correction) makes this unbiased for \\sigma^2. Using n would give a biased estimator.\n\n\n\n\n\nEvery estimator is a statistic, but not every statistic is an estimator — until you tie it to a target (an estimand), it’s “just” a statistic.\n\n\n\n\n\n\n\nhttps://allmodelsarewrong.github.io/mse.html\n\n\n\n\n\n\n\n\nQuality Criteria: Bias, Variance, MSE, Efficiency (*)\n\n\n\nHow do we assess if an estimator (“method”) is good?\n\nBias — does our method give true results “on average”?\nImagine we want to know the average height of adult Poles (true value: 172 cm). We draw 100 different samples of 500 people each and calculate the mean for each one.\nUnbiased estimator: Those 100 means will differ (169 cm, 173 cm, 171 cm…), but their average will be close to 172 cm. Sometimes we overestimate, sometimes underestimate, but there’s no systematic error.\nBiased estimator: If we accidentally always excluded people over 180 cm, all our 100 means would be too low (e.g., oscillating around 168 cm). That’s systematic bias.\n\n\n\nVariance — how much do results differ between samples?\nWe have two methods for estimating the same parameter. Both give good results “on average,” but:\n\nMethod A: from 10 samples we get: 171, 172, 173, 171, 172, 173, 172, 171, 173, 172 cm\nMethod B: from 10 samples we get: 165, 179, 168, 176, 171, 174, 169, 175, 167, 176 cm\n\nMethod A has lower variance — results are more concentrated, predictable. In practice, you prefer Method A because you can be more confident in a single result.\nKey principle: Larger sample = lower variance. With a sample of 100 people, the mean will “jump around” more than with a sample of 1,000 people.\n\n\n\nMean Squared Error (MSE) — what matters more: unbiasedness or stability?\nSometimes we face a dilemma:\n\nEstimator A: Unbiased (average 172 cm), but very unstable (results from 160 to 184 cm)\nEstimator B: Slightly biased (average 171 cm instead of 172 cm), but very stable (results from 169 to 173 cm)\n\nMSE says: Estimator B is better — a small systematic underestimation of 1 cm is less problematic than the huge spread of results in Estimator A.\n\n\n\nEfficiency — which unbiased estimator to choose?\nYou have data on incomes of 500 people. You want to know the “typical” income. Two options:\n\nArithmetic mean: typically gives results in the range 4,800–5,200 PLN\nMedian: gives results in the range 4,500–5,500 PLN\n\nIf both methods are unbiased, choose the one with smaller spread (the mean is more efficient for normally distributed data).\n\n\n\n\nExample of Statistical Thinking\nYour university is considering keeping the library open 24/7. The administration needs to know: What proportion of students support this change?\n\n\n\n\n\n\nNote\n\n\n\nIdeal world: Ask all 20,000 students → Get the exact answer (\\theta parameter)\nReal world: Survey 100 students → Get an estimate (\\hat{\\theta}) with uncertainty\n\n\n\n\n\n\n\n\nBias vs. Random Error\n\n\n\nStatistical (prediction) error can be decomposed into two main components: bias (systematic error) and random error (unpredictable variation).\nBias is like a miscalibrated scale that consistently reads 2 kg too high—every measurement is wrong in the same direction. It’s systematic error.\nRandom error is the unpredictable variation in your observations, like:\n\nA dart player aiming at the bullseye—each throw lands in a slightly different spot due to hand tremor, air currents, tiny muscle variations\nMeasuring someone’s height multiple times and getting 174.8 cm, 175.0 cm, 175.3 cm—small fluctuations from posture changes, breathing, how you read the scale, and natural body variations\nA weather model that’s sometimes 2°C too high, sometimes 1°C too low, sometimes spot on\nOpinion polls showing 52%, 49%, 51% support across different surveys—each random sample gives slightly different results, but they cluster around the true value\n\nRandom error is measured by variance—the average squared deviation of observations from their mean. It quantifies how much your data points (predictions) scatter.\nRandom error is like asking 5 friends to estimate how many jellybeans are in a jar—they’ll all give different answers just due to chance, but those differences scatter randomly around the truth rather than all being wrong in the same direction.\nPolling example: Bias is like polling only at the gym at 6am—you’ll always get more health-conscious, early-rising, employed people and always miss night-shift workers, people with young kids, etc. The poll is broken in a predictable way. Or: only counting responses from people who actually answer unknown phone calls—you’ll systematically miss everyone (especially younger people) who screens their calls.\nKey difference: Averaging more observations reduces random error but never fixes bias. You can’t average your way out of a miscalibrated scale—or a biased sampling method!\n\n\n\n\nTwo Approaches to the Same Data\nImagine you survey 100 random students and find that 60 support the 24/7 library hours.\n\n\n❌ Without Statistical Thinking\n“60 out of 100 students said yes.”\nConclusion: “Exactly 60% of all students support it.”\nDecision: “Since it’s over 50%, we have clear majority support.”\nProblem: Ignores that a different sample might give 55% or 65%\n\n✅ With Statistical Thinking\n“60 out of 100 students said yes.”\nConclusion: “We estimate 60% support, with a margin of error of ±10 pp.”\nDecision: “True support is likely between 50% and 70%—we need more data to be certain of majority support.”\nAdvantage: Acknowledges uncertainty and informs better decisions\n\n\nHow sample size affects precision:\n\n\n\n\n\n\n\n\n\n\nSample Size\nObserved Result\nMargin of Error\n(95%) Range of Plausible Values\nInterpretation\n\n\n\n\nn = 100\n60%\n±10 pp\n50% to 70%\nUncertain about majority\n\n\nn = 400\n60%\n±5 pp\n55% to 65%\nLikely majority support\n\n\nn = 1,000\n60%\n±3 pp\n57% to 63%\nClear majority support\n\n\nn = 1,600\n60%\n±2.5 pp\n57.5% to 62.5%\nStrong majority support\n\n\nn = 10,000\n60%\n±1 pp\n59% to 61%\nVery precise estimate\n\n\n\nThe Diminishing Returns Principle: Notice that quadrupling the sample size from 100 to 400 cuts the margin of error in half, but increasing from 1,600 to 10,000 (a 6.25× increase) only reduces it by 1.5 percentage points. To halve your margin of error, you must quadruple your sample size.\nThis is why most polls stop around 1,000–1,500 respondents—the gains in precision beyond that point rarely justify the additional cost and effort.\n\n\n\nSample Size and Uncertainty (Random Error)\nSuppose we take a random sample of n=1000 voters and observe \\hat p = 0.55 (e.g., 55% support for a candidate in upcoming elections—550 out of 1,000 respondents). Then:\n\nOur best single-number estimate (point estimate) of the population proportion is \\hat p = 0.55.\nA typical “range of plausible values” (at the 95\\% confidence level) around \\hat p can be approximated by \\hat p \\pm \\text{Margin of Error}, i.e., \n  \\hat p \\;\\pm\\; 2\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\n  \\;=\\;\n  0.55 \\;\\pm\\; 2\\sqrt{\\frac{0.55\\cdot 0.45}{1000}}\n  \\approx\n  0.55 \\pm 0.031,\n giving roughly (interval estimate) 52\\% to 58\\% (approximately \\pm 3.1 percentage points).\n\n\n\nNote: The factor of 2 is a convenient rounding of 1.96, the critical value from the standard normal distribution for 95% confidence.\n\n\nThe width of this interval shrinks predictably with sample size: \n  \\text{Margin of Error} \\;\\propto\\; \\frac{1}{\\sqrt{n}}.\n For example, increasing n from 1,000 to 4,000 cuts the margin of error approximately in half (from \\pm 3.1\\% to \\pm 1.6\\%).\n\n\n\n\n\n\n\n\nNote\n\n\n\nFundamental Principle: Statistics does not eliminate uncertainty—it helps us measure, manage, and communicate it effectively.\n\n\n\n\n\n\n\n\n\nHistorical Example: the 1936 Literary Digest Poll\n\n\n\nIn 1936, Literary Digest ran one of the largest opinion polls ever — about 2.4 million mailed responses — yet it completely misjudged the U.S. presidential election.\n\n\n\nCandidate\nPrediction\nActual result\nError\n\n\n\n\nLandon\n57%\n36.5%\n≈20 pp\n\n\nRoosevelt\n43%\n60.8%\n≈18 pp\n\n\n\n\nWhat went wrong?\nEven with millions of responses, the poll was badly biased — not random, but systematic.\n\n\n\nSystematic vs. Random Error\nImagine a bathroom scale that adds +2.3 kg to everyone’s weight:\n\nRandom error (no bias): Each time you step on, your balance shifts a little. Readings jump around your true weight — say 68.0–68.5 kg. Averaging them gives the right result (≈68 kg). More readings reduce the scatter.\nSystematic error (bias): The scale’s zero point is wrong. Every reading shows +2.3 kg too much. Weigh yourself once: 70.3 kg. Weigh yourself 1,000 times: still ~70.3 kg — precisely wrong.\n\nThat was Literary Digest’s problem: a miscalibrated “instrument” for measuring public opinion. Millions of biased responses only produced false confidence.\n\n\n\nWhere did the bias come from?\nTwo biases both worked in favor of Alf Landon:\n\nCoverage (selection) bias — who could be contacted\n\nThe poll used telephone books, car registration lists, and magazine subscribers.\nDuring the Great Depression, these lists mostly included wealthier Americans, who leaned Republican.\nResult: systematic underrepresentation of poorer, pro-Roosevelt voters.\n\nNonresponse bias — who chose to reply\n\nOnly about one in four people (≈24%) who were contacted returned their ballot.\nThose who responded were more politically active and more likely to oppose Roosevelt.\n\n\nTogether, these created a huge systematic bias that no large sample could fix.\n\n\n\nWhy sample size couldn’t save the poll\nTaking 2.4 million responses from a biased list is like weighing an entire country on a faulty scale.\n\nThe maximum possible (worst case scenario) margin of error (for the 95\\% confidence level) for a given sample size (if it had been a true random sample) would have been: \\text{MoE}_{95\\%} \\approx 1.96\\sqrt{\\frac{0.25}{2{,}400{,}000}} \\approx \\pm 0.06 \\text{ percentage points} — tiny.\nThat formula only captures random error, not bias.\nThe real error was about ±18–20 percentage points — hundreds of times larger.\n\nLesson: Precision without representativeness is useless. A huge biased sample can be worse than a small, carefully chosen one.\n\n\n\nModern Polling: Smaller but Smarter\nThe Literary Digest disaster transformed polling practice:\n\nProbability sampling: every voter has a known, non-zero chance of selection.\nWeighting: adjust for groups that reply too often or too rarely.\nTotal survey error mindset: consider coverage, nonresponse, measurement, and processing errors — not just sampling error.\n\nBottom line: How you sample matters far more than how many you sample.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#understanding-randomness",
    "href": "chapter1.html#understanding-randomness",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.4 Understanding Randomness",
    "text": "1.4 Understanding Randomness\nA random experiment is any process whose result cannot be predicted with certainty, such as tossing a coin or rolling a die.\nAn outcome is a single possible result of that experiment—for example, getting “heads” or rolling a “5”.\nSample space is the set of all possible outcomes of a random experiment. It is typically denoted by the symbol S or Ω (omega).\nAn event is a set of one or more outcomes that we’re interested in; it could be a simple event (like rolling exactly a 3) or a compound event (like rolling an even number, which includes the outcomes 2, 4, and 6).\n\nProbability is a way of measuring how likely something is to happen. It’s a number between 0 and 1 (or 0% and 100%) that represents the chance of an event occurring.\n\nA probability distribution is a mathematical function/rule that describes the likelihood of different possible outcomes in a random experiment.\nIf something has a probability of 0, it’s impossible - it will never happen. If something has a probability of 1, it’s certain - it will definitely happen. Most things fall somewhere in between.\nFor example, when you flip a fair coin, there’s a 0.5 (or 50%) probability it will land on heads, because there are two equally likely outcomes and heads is one of them.\nProbability helps us make sense of uncertainty and randomness in the world.\n\n\nIn statistics, randomness is an orderly way to describe uncertainty. While each individual outcome is unpredictable, stable patterns (more formally, empirical distributions of outcomes converge to probability distributions) emerge over many repetitions.\n\nExample: Flip a fair coin:\n\nSingle flip: Completely unpredictable—you can’t know if it’ll be heads or tails\n100 flips: You’ll get close to 50% heads (maybe 48 or 53)\n10,000 flips: Almost certainly very close to 50% heads (perhaps 49.8%)\n\nThe same applies to dice: you can’t predict your next roll, but roll 600 times and each number (1-6) will appear close to 100 times. This predictable long-run behavior from unpredictable individual events is the essence of statistical randomness.\n\nTypes of Randomness\nEpistemic vs. Ontological Randomness:\n\nEpistemic randomness (due to incomplete knowledge): We treat an outcome as random because not all determinants are observed or conditions are not controlled. The system itself is deterministic—it follows fixed rules—but we lack the information needed to predict the outcome.\n\nCoin toss: The trajectory of the coin is governed entirely by classical mechanics. If we knew the exact initial position, force, angular momentum, air resistance, and surface properties, we could theoretically predict whether it lands on heads or tails. The “randomness” exists only because we cannot measure these conditions with sufficient precision.\nPoll responses: An individual’s answer to a survey question is determined by their beliefs, experiences, and context, but we don’t have access to this complete psychological state, so we model it as random.\nMeasurement error: Limited instrument precision means the “true” value exists, but we observe it with uncertainty.\n\nOntological randomness (intrinsic indeterminacy): Even complete knowledge of all conditions does not remove outcome uncertainty. The randomness is fundamental to the nature of reality itself, not just a gap in our knowledge.\n\nRadioactive decay: The exact moment when a particular atom will decay is fundamentally unpredictable, even in principle. Quantum mechanics tells us only the probability distribution, not the precise timing.\nQuantum measurements: The outcome of measuring a quantum particle’s position or spin is inherently probabilistic, not determined by hidden variables we simply haven’t discovered yet.\n\n\n\n\n\n\n\n\nThe Coin Toss Paradox\n\n\n\nWhile we treat coin tosses as producing 50-50 random outcomes, research by mathematician Persi Diaconis has shown that with a mechanical coin-flipping machine that precisely controls initial conditions, you can reliably bias the outcome toward a chosen side. This confirms that coin tosses are epistemically, not ontologically, random—the apparent randomness comes from our inability to control and measure conditions, not from any fundamental indeterminacy in physics.\n\n\n\n\nRelated Concepts\nRandomness vs. Haphazardness: Statistical randomness has mathematical structure and follows probability laws—it’s orderly uncertainty. Haphazardness suggests complete disorder without underlying patterns or rules.\nDeterministic Chaos: The middle ground between perfect predictability and randomness. Chaos refers to deterministic systems (following fixed, known rules) that exhibit extreme sensitivity to initial conditions, making long-term prediction impossible in practice.\nThink of chaos like a pinball machine, with the butterfly effect:\n\nYou know all the rules perfectly—the physics of collisions, friction, gravity\nThe system is completely deterministic: release the ball from exactly the same spot with exactly the same force, and you’ll get exactly the same result every time\nBut: A difference of 0.01 millimeters in starting position leads to the ball hitting different bumpers, which compounds with each collision until the final outcome is completely different\nThis is the butterfly effect: tiny perturbations in initial conditions grow exponentially over time\n\nClassic examples of deterministic chaos:\n\nWeather systems: Edward Lorenz discovered that atmospheric models are so sensitive that a butterfly flapping its wings in Brazil could theoretically alter whether a tornado forms in Texas weeks later. This is why weather forecasts are reliable for days but not months.\nPlanetary orbits: While stable on human timescales, the solar system’s dynamics are chaotic over millions of years. We cannot predict the exact position of planets in the distant future, even though we know the gravitational laws perfectly.\nDouble pendulum: Release it from a slightly different angle, and after a few swings, the motion becomes completely different.\n\nChaos vs. Epistemic Randomness—A Critical Distinction:\nBoth involve unpredictability due to limited knowledge, but they differ in a crucial way:\n\n\n\n\n\n\n\n\nAspect\nEpistemic Randomness\nDeterministic Chaos\n\n\n\n\nRules known?\nOften yes\nYes, completely\n\n\nCurrent state known?\nNo (or imprecisely)\nNo (or imprecisely)\n\n\nWhat causes unpredictability?\nMissing information about the current state\nExponential amplification of tiny measurement errors\n\n\nCan perfect info help?\nYes—learning the state eliminates uncertainty\nOnly in the short term—errors accumulate again\n\n\n\nExample to clarify:\n\nEpistemic randomness (card face-down): The card is already the 7 of hearts. It’s not changing or evolving. You just don’t know which card it is yet. Flip it over, and the uncertainty vanishes completely and permanently.\nChaos (weather in 3 weeks): Even if you measure current atmospheric conditions to extraordinary precision, tiny errors (measurement at 6 decimal places instead of 20) compound over time. You might predict well for 5 days, but by week 3, your forecast is useless—not because you don’t know the physics, but because the system amplifies microscopic uncertainties.\n\n\n\n\n\n\n\nKey Insight\n\n\n\nChaos is deterministic yet unpredictable. Epistemic randomness is deterministic yet unknown. Ontological randomness is fundamentally indeterministic. Statistical practice treats all three as “random,” but understanding the source of unpredictability helps us know when more information could help (epistemic), when it helps temporarily but not long-term (chaos), and when it cannot help at all (ontological).\n\n\nEntropy: A measure of disorder or uncertainty in a system. High entropy means high unpredictability or many possible microstates; low entropy means high order and low uncertainty. In information theory and statistics, entropy quantifies the amount of uncertainty in a probability distribution—more spread out distributions have higher entropy.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#populations-and-samples",
    "href": "chapter1.html#populations-and-samples",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.5 Populations and Samples",
    "text": "1.5 Populations and Samples\nUnderstanding the distinction between populations and samples is crucial for proper statistical analysis.\n\nPopulation\nA population is the complete set of individuals, objects, or measurements about which we wish to draw conclusions. The key word here is “complete”—a population includes every single member of the group we’re studying.\nExamples of Populations in Demography:\n\nAll residents of India as of January 1, 2024: This includes every person living in India on that specific date—approximately 1.4 billion people.\nAll births in Sweden during 2023: Every baby born within Swedish borders during that calendar year—roughly 100,000 births.\nAll households in Tokyo: Every residential unit where people live, cook, and sleep separately from others—about 7 million households.\nAll deaths from COVID-19 worldwide in 2020: Every death where COVID-19 was listed as a cause—several million deaths.\n\nPopulations can be:\nFinite: Having a countable number of members (all current U.S. citizens, all Polish municipalities in 2024)\nInfinite: Theoretical or uncountably large (all possible future births, all possible coin tosses or dice flips)\nFixed: Defined at a specific point in time (all residents on census day)\nDynamic: Changing over time (the population of a city that experiences births, deaths, and migration daily)\n\n\nSample\nA sample is a subset of the population that is actually observed or measured. We study samples because examining entire populations is often impossible, impractical, or unnecessary.\nWhy We Use Samples:\nPractical Impossibility: Imagine testing every person in China for a disease. By the time you finished testing 1.4 billion people, the disease situation would have changed completely, and some people tested early would need retesting.\nCost Considerations: The 2020 U.S. Census cost approximately $16 billion. Conducting such complete enumerations frequently would be prohibitively expensive. A well-designed sample survey can provide accurate estimates at a fraction of the cost.\nTime Constraints: Policy makers often need information quickly. A sample survey of 10,000 people can be completed in weeks, while a census takes years to plan, execute, and process.\nDestructive Measurement: Some measurements destroy what’s being measured. Testing the lifespan of light bulbs or the breaking point of materials requires using samples.\nGreater Accuracy: Surprisingly, samples can sometimes be more accurate than complete enumerations. With a sample, you can afford better training for interviewers, more careful data collection, and more thorough quality checks.\nExample of Sample vs. Population:\nLet’s say we want to know the average household size in New York City:\n\nPopulation: All 3.2 million households in NYC\nCensus approach: Attempt to contact every household (expensive, time-consuming, some will be missed)\nSample approach: Randomly select 5,000 households, carefully measure their sizes, and use this to estimate the average for all households\nResult: The sample might find an average of 2.43 people per household with a margin of error of ±0.05, meaning we’re confident the true population average is between 2.38 and 2.48\n\n\n\n\n\n\n\n\nOverview of Sampling Methods\n\n\n\nSampling involves selecting a subset of the population to estimate its characteristics. The sampling frame (list from which we sample) should ideally contain each member exactly once. Frame problems: undercoverage, overcoverage, duplication, and clustering.\n\nProbability Sampling (Statistical Inference Possible)\n\nSimple Random Sampling (SRS): Every possible sample of size n has equal probability of selection (sampling without replacement). Gold standard of probability methods.\n\nFormal definition: Each of the \\binom{N}{n} possible samples has probability \\frac{1}{\\binom{N}{n}}.\nInclusion probability for a unit:\n\nQuestion: In how many samples does a specific person (e.g., student John) appear?\nIf John is already in the sample (that’s fixed), we need to select n-1 more people from the remaining N-1 people (everyone except John).\nNumber of samples containing John: \\binom{N-1}{n-1}\nProbability:\n\n\nP(\\text{John in sample}) = \\frac{\\text{samples with John}}{\\text{all samples}} = \\frac{\\binom{N-1}{n-1}}{\\binom{N}{n}} = \\frac{n}{N}\n\nNumerical example: N=5 people {A,B,C,D,E}, we sample n=3. All samples: \\binom{5}{3}=10. Samples with person A: {ABC, ABD, ABE, ACD, ACE, ADE} = \\binom{4}{2}=6 samples. Probability: 6/10 = 3/5 = n/N ✓\n\nSystematic Sampling: Selection of every k-th element, where k = N/n. Simple to implement, but beware of hidden periodicity in the frame (e.g., list ordered by patterns).\nSystematic Sampling: Selection of every k-th element, where k = N/n (sampling interval).\n\nHow it works: Randomly select a starting point r from \\{1, 2, ..., k\\}, then select: r, r+k, r+2k, r+3k, ...\nExample: N=1000, n=100, so k=10. If r=7, we select: 7, 17, 27, 37, …, 997.\nAdvantages: Very simple, ensures even coverage of the population.\nPeriodicity problem: If the list has a pattern repeating every k elements, the sample can be severely biased.\n\nExample (bad): Apartment list: 101, 102, 103, 104 (corner), 201, 202, 203, 204 (corner), … If k=4, we might sample only corner apartments!\nExample (bad): Daily production data with 7-day cycle. If k=7, we might sample only Mondays.\nExample (good): Alphabetical list of surnames - usually no periodicity.\n\n\nCluster Sampling: Selection of entire groups (clusters) instead of individual units. Cost-effective for geographically dispersed populations (e.g., sampling schools instead of students), but typically less precise than SRS (design effect: DEFF = Variance(cluster)/Variance(SRS)). Can be single- or multi-stage.\n\n\n\nNon-Probability Sampling (Limited Statistical Inference)\n\nConvenience Sampling: Selection based on ease of access (e.g., passersby in city center). Useful in pilot/exploratory studies, but likely serious selection bias.\nPurposive/Judgmental Sampling: Deliberate selection of typical, extreme, or information-rich cases. Valuable in qualitative research and studying rare populations.\nQuota Sampling: Matching population proportions (e.g., 50% women), but without random selection. Quick and inexpensive, but hidden selection bias and no ability to calculate sampling error.\nSnowball Sampling: Participants recruit others from their networks. Essential for hard-to-reach populations (drug users, undocumented immigrants), but biased toward well-connected individuals.\n\nFundamental Principle: Probability sampling enables valid statistical inference and calculation of sampling error; non-probability methods may be necessary for practical or ethical reasons, but limit the ability to generalize results to the entire population.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#superpopulation-and-data-generating-process-dgp",
    "href": "chapter1.html#superpopulation-and-data-generating-process-dgp",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.6 Superpopulation and Data Generating Process (DGP) (*)",
    "text": "1.6 Superpopulation and Data Generating Process (DGP) (*)\n\n\nSuperpopulation\nA superpopulation is a theoretical infinite population from which your finite population is considered to be one random sample.\nThink of it in three levels:\n\nSuperpopulation: An infinite collection of possible values (theoretical)\nFinite population: The actual population you could theoretically census (e.g., all 50 US states, all 10,000 firms in an industry)\nSample: The subset you actually observe (e.g., 30 states, 500 firms)\n\nWhy do we need this concept?\nConsider the 50 US states. You might measure unemployment rate for all 50 states—a complete census, no sampling needed. But you still want to:\n\nTest if unemployment is related to education levels\nPredict next year’s unemployment rates\nDetermine if differences between states are “statistically significant”\n\nWithout the superpopulation concept, you’re stuck—you have all the data, so what’s left to infer? The answer: treat this year’s 50 values as one draw from an infinite superpopulation of possible values that could occur under similar conditions.\nMathematical representation:\n\nFinite population value: Y_i (state i’s unemployment rate)\nSuperpopulation model: Y_i = \\mu + \\epsilon_i where \\epsilon_i \\sim (0, \\sigma^2)\nThe 50 observed values are one realization of this process\n\n\n\n\nData Generating Process: The True Recipe\nThe Data Generating Process (DGP) is the actual mechanism that creates your data—including all factors, relationships, and random elements.\nAn intuitive example: Suppose student test scores are truly generated by:\n\\text{Score}_i = 50 + 2(\\text{StudyHours}_i) + 3(\\text{SleepHours}_i) - 5(\\text{Stress}_i) + 1.5(\\text{Breakfast}_i) + \\epsilon_i\nThis is the TRUE DGP. But you don’t know this! You might estimate:\n\\text{Score}_i = \\alpha + \\beta(\\text{StudyHours}_i) + u_i\nYour model is simpler than reality. You’re missing variables (sleep, stress, breakfast), so your estimates might be biased. The u_i term captures everything you missed.\nKey insight: We never know the true DGP. Our statistical models are always approximations, trying to capture the most important parts of the unknown, complex truth.\n\n\n\nTwo Approaches to Statistical Inference\nWhen analyzing data, especially from surveys or samples, we can take two philosophical approaches:\n\n1. Design-Based Inference\n\nPhilosophy: The population values are fixed numbers. Randomness comes ONLY from which units we happened to sample.\nFocus: How we selected the sample (simple random, stratified, cluster sampling, etc.)\nExample: The mean income of California counties is a fixed number. We sample 10 counties. Our uncertainty comes from which 10 we randomly selected.\nNo models needed: We don’t assume anything about the population values’ distribution\n\n\n\n2. Model-Based Inference\n\nPhilosophy: The population values themselves are realizations from some probability model (superpopulation)\nFocus: The statistical model generating the population values\nExample: Each California county’s income is drawn from: Y_i = \\mu + \\epsilon_i where \\epsilon_i \\sim N(0, \\sigma^2)\nModels required: We make assumptions about how the data were generated\n\nWhich is better?\n\nLarge populations, good random samples: Design-based works well\nSmall populations (like 50 states): Model-based often necessary\nComplete enumeration: Only model-based allows inference\nModern practice: Often combines both approaches\n\n\n\n\n\nPractical Example: Analyzing State Education Spending\nSuppose you collect education spending per pupil for all 50 US states.\nWithout superpopulation thinking:\n\nYou have all 50 values—that’s it\nThe mean is the mean, no uncertainty\nYou can’t test hypotheses or make predictions\n\nWith superpopulation thinking:\n\nThis year’s 50 values are one realization from a superpopulation\nModel: \\text{Spending}_i = \\mu + \\beta(\\text{StateIncome}_i) + \\epsilon_i\nNow you can:\n\nTest if spending relates to state income (\\beta \\neq 0?)\nPredict next year’s values\nCalculate confidence intervals\n\n\nThe key insight: Even with complete data, the superpopulation framework enables statistical inference by treating observed values as one possible outcome from an underlying stochastic process.\n\n\n\nSummary\n\nSuperpopulation: Treats your finite population as one draw from an infinite possibility space—essential when your finite population is small or completely observed\nDGP: The true (unknown) process creating your data—your models try to approximate it",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#understanding-data-data-distributions-and-data-typologies",
    "href": "chapter1.html#understanding-data-data-distributions-and-data-typologies",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.7 Understanding Data, Data Distributions, and Data Typologies",
    "text": "1.7 Understanding Data, Data Distributions, and Data Typologies\nWhat is Data?\nData is a collection of facts, observations, or measurements that we gather to answer questions or understand phenomena. In statistics and data analysis, data represents information in a structured format that can be analyzed.\nData Points\nA data point is a single observation or measurement in a dataset. For example, if we measure the height of 5 students, each individual height measurement is a data point.\nVariables\nA variable is a characteristic or attribute that can take different values across observations. Variables can be:\n\nCategorical (e.g., color, gender, country)\nNumerical (e.g., age, temperature, income)\n\nData Distribution\nData distribution describes what values a variable takes and how often each value occurs in the dataset. Understanding distribution helps us see patterns, central tendencies, and variability in our data.\nFrequency Distribution Tables\nA frequency distribution table organizes data by showing each unique value (or range of values) and the number of times it appears:\n\n\n\nValue\nFrequency\nRelative Frequency\n\n\n\n\nA\n15\n0.30 (30%)\n\n\nB\n25\n0.50 (50%)\n\n\nC\n10\n0.20 (20%)\n\n\nTotal\n50\n1.00 (100%)\n\n\n\nThis table allows us to quickly see which values are most common and understand the overall distribution pattern.\n\n\n\n\n\n\n\nUnderstanding Different Types of Data Structures (Data Sets) and Their Formats\n\n\n\n\nCross-sectional Data\nObservations for variables (columns in a database) collected at a single point in time across multiple entities/individuals:\n\n\n\nIndividual\nAge\nIncome\nEducation\n\n\n\n\n1\n25\n50000\nBachelor’s\n\n\n2\n35\n75000\nMaster’s\n\n\n3\n45\n90000\nPhD\n\n\n\n\n\nTime Series Data\nObservations of a single entity tracked over multiple time points:\n\n\n\nYear\nGDP (in billions)\nUnemployment Rate\n\n\n\n\n2018\n20,580\n3.9%\n\n\n2019\n21,433\n3.7%\n\n\n2020\n20,933\n8.1%\n\n\n\n\n\nPanel Data (Longitudinal Data)\nObservations of multiple entities tracked over time:\n\n\n\nCountry\nYear\nGDP per capita\nLife Expectancy\n\n\n\n\nUSA\n2018\n62,794\n78.7\n\n\nUSA\n2019\n65,118\n78.8\n\n\nCanada\n2018\n46,194\n81.9\n\n\nCanada\n2019\n46,194\n82.0\n\n\n\n\n\nTime-series Cross-sectional (TSCS) Data\nA special case of panel data where:\n\nNumber of time points &gt; Number of entities\nSimilar structure to panel data but with emphasis on temporal depth\nCommon in political science and economics research\n\n\n\n\nData Formats\n\nWide Format\nEach row represents an entity; columns represent variables/time points:\n\n\n\nCountry\nGDP_2018\nGDP_2019\nLE_2018\nLE_2019\n\n\n\n\nUSA\n62,794\n65,118\n78.7\n78.8\n\n\nCanada\n46,194\n46,194\n81.9\n82.0\n\n\n\n\n\nLong Format\nEach row represents a unique entity-time-variable combination:\n\n\n\nCountry\nYear\nVariable\nValue\n\n\n\n\nUSA\n2018\nGDP per capita\n62,794\n\n\nUSA\n2019\nGDP per capita\n65,118\n\n\nUSA\n2018\nLife Expectancy\n78.7\n\n\nUSA\n2019\nLife Expectancy\n78.8\n\n\nCanada\n2018\nGDP per capita\n46,194\n\n\nCanada\n2019\nGDP per capita\n46,194\n\n\nCanada\n2018\nLife Expectancy\n81.9\n\n\nCanada\n2019\nLife Expectancy\n82.0\n\n\n\nNote: Long format is generally preferred for:\n\nData manipulation in R and Python\nStatistical analysis\nData visualization\n\n\n\n\n\n\n\nUnderstanding data types and distributions is fundamental to choosing appropriate analyses and interpreting results correctly.\n\n\nTypes of Data\nData consists of collected observations or measurements. The type of data determines what mathematical operations (e.g. multiplication) are meaningful and what statistical methods apply.\n\nQuantitative Data\nContinuous Data can take any value within a range:\nExamples:\n\nAge: Can be 25.5 years, 25.51 years, 25.514 years (precision limited only by measurement)\nBody Mass Index: 23.7 kg/m²\nFertility Rate: 1.73 children per woman\nPopulation Density: 4,521.3 people per km²\nVoter turnout: 60%\n\nProperties:\n\nCan perform all arithmetic operations\nCan calculate means, standard deviations\n\nDiscrete Data can only take specific values:\nExamples:\n\nNumber of Children: 0, 1, 2, 3… (can’t have 2.5 children)\nNumber of Marriages: 0, 1, 2, 3…\nHousehold Size: 1, 2, 3, 4… people\nNumber of Doctor Visits: 0, 1, 2, 3… per year\nElectoral District Magnitude: 1, 2, 3, …\n\n\n\nQualitative/Categorical Data\nNominal Data represents categories with no inherent order:\nExamples:\n\nCountry of Birth: USA, China, India, Brazil…\nReligion: Christian, Muslim, Hindu, Buddhist, None…\nMarital Status: Single, Married, Divorced, Widowed\nCause of Death: Heart disease, Cancer, Accident, Stroke…\nBlood Type: A, B, AB, O\n\nWhat We Can Do:\n\nCount frequencies\nCalculate proportions\nFind mode\n\nWhat We Cannot Do:\n\nCalculate mean (average religion makes no sense)\nOrder categories meaningfully\nCompute distances between categories\n\nOrdinal Data represents ordered categories:\nExamples:\n\nEducation Level: None &lt; Primary &lt; Secondary &lt; Tertiary\nSocioeconomic Status: Low &lt; Middle &lt; High\nSelf-Rated Health: Poor &lt; Fair &lt; Good &lt; Excellent\nAgreement Scale: Strongly Disagree &lt; Disagree &lt; Neutral &lt; Agree &lt; Strongly Agree\n\nThe Challenge: Intervals between categories aren’t necessarily equal. The “distance” from Poor to Fair health may not equal the distance from Good to Excellent.\n\n\nFrequency, Relative Frequency, and Density\nWhen we analyze data, we’re often interested in how many times each value (or range of values) appears. This leads us to three related concepts:\n(Absolute) Frequency is simply the count of how many times a particular value or category occurs in your data. If 15 students scored between 70-80 points on an exam, the frequency for that range is 15.\nRelative frequency expresses frequency as a proportion or percentage of the total. It answers the question: “What fraction of all observations fall into this category?” Relative frequency is calculated as:\n\\text{Relative Frequency} = \\frac{\\text{Frequency}}{\\text{Total Number of Observations}}\nIf 15 out of 100 students scored 70-80 points, the relative frequency is 15/100 = 0.15 or 15%. Relative frequencies always sum to 1 (or 100%), making them useful for comparing distributions with different sample sizes.\n\n\n\n\n\n\nTip\n\n\n\nThe probability of an event is a number between 0 and 1; the larger the probability, the more likely an event is to occur.\n\n\nDensity (probability per unit length) measures how concentrated observations are per unit of measurement. When grouping continuous data (like time or unemployment rate) into intervals of different widths, we need density to ensure fair comparison—wider intervals naturally contain more observations simply because they’re wider, not because values are more concentrated there. Density is calculated as:\n\\text{Density} = \\frac{\\text{Relative Frequency}}{\\text{Interval Width}}\nThis standardization allows fair comparison between intervals—wider intervals don’t appear artificially more important just because they’re wider.\n\nDensity is particularly important for continuous variables because it ensures that the total area under the distribution equals 1, which allows us to interpret areas as probabilities.\n\nCumulative frequency tells us how many observations fall at or below a certain value.\nInstead of asking “how many observations are in this category?”, cumulative frequency answers “how many observations are in this category or any category below it?” It’s calculated by adding up all frequencies from the lowest value up to and including the current value.\nSimilarly, cumulative relative frequency expresses this as a proportion of the total, answering “what percentage of observations fall at or below this value?” For example, if the cumulative relative frequency at score 70 is 0.40, this means 40% of students scored 70 or below.\n\n\nDistribution Tables\nA frequency distribution table organizes data by showing how observations are distributed across different values or intervals. Here’s an example with exam scores:\n\n\n\n\n\n\n\n\n\n\n\nScore Range\nFrequency\nRelative Frequency\nCumulative Frequency\nCumulative Relative Frequency\nDensity\n\n\n\n\n0-50\n10\n0.10\n10\n0.10\n0.002\n\n\n50-70\n30\n0.30\n40\n0.40\n0.015\n\n\n70-90\n45\n0.45\n85\n0.85\n0.0225\n\n\n90-100\n15\n0.15\n100\n1.00\n0.015\n\n\nTotal\n100\n1.00\n-\n-\n-\n\n\n\nThis table reveals that most students scored in the 70-90 range, while very few scored below 50 or above 90. The cumulative columns show us that 40% of students scored below 70, and 85% scored below 90. Such tables are invaluable for getting a quick overview of your data before conducting more complex analyses.\n\n\nVisualizing Distributions: Histograms\nA histogram is a graphical representation of a frequency distribution. It displays data using bars where:\n\nThe x-axis shows the values or intervals (bins)\nThe y-axis can show frequency, relative frequency, or density\nThe height of each bar represents the count, proportion, or density for that interval\nBars touch each other (no gaps) for continuous variables\n\nChoosing bin widths: The number and width of bins significantly affects how your histogram looks. Too few bins hide important patterns, while too many bins create “noise” and make patterns hard to see.\n\nIn statistics, noise is unwanted random variation that obscures the pattern we’re trying to find. Think of it like static on a radio—it makes the music (the “signal”) harder to hear. In data, noise comes from measurement errors, random fluctuations, or the inherent variability in what we’re studying. Noise is random variation in data that hides the real patterns we want to see, similar to how background noise makes conversation difficult to hear.\n\nSeveral approaches help determine appropriate bin widths (*):\n\nSturges’ rule: Use k = 1 + \\log_2(n) bins, where n is the sample size. This works well for roughly symmetric distributions.\nSquare root rule: Use k = \\sqrt{n} bins. A simple, reasonable default for many situations.\n\nIn R, you can specify bins in several ways:\n\n# Generate exam scores data\nset.seed(123)  # For reproducibility\nexam_scores &lt;- c(\n  rnorm(80, mean = 75, sd = 12),  # Most students cluster around 75\n  runif(15, 50, 65),               # Some lower performers\n  runif(5, 85, 95)                 # A few high achievers\n)\n\n# Keep scores within valid range (0-100)\nexam_scores &lt;- pmin(pmax(exam_scores, 0), 100)\n\n# Round to whole numbers\nexam_scores &lt;- round(exam_scores)\n\n# Specify number of bins\nhist(exam_scores, breaks = 10)\n\n\n\n\n\n\n\n# Specify exact break points\nhist(exam_scores, breaks = seq(0, 100, by = 10))\n\n\n\n\n\n\n\n# Let R choose automatically (uses Sturges' rule by default)\nhist(exam_scores)\n\n\n\n\n\n\n\n\nThe best approach is often to experiment with different bin widths to find what best reveals your data’s pattern. Start with a default, then try fewer and more bins to see how the story changes.\nDefining bin boundaries: When creating bins for a frequency table, you must decide how to handle values that fall exactly on the boundaries. For example, if you have bins 0-10 and 10-20, which bin does the value 10 belong to?\nThe solution is to use interval notation to specify whether each boundary is included or excluded:\n\nClosed interval [a, b] includes both endpoints: a \\leq x \\leq b\nOpen interval (a, b) excludes both endpoints: a &lt; x &lt; b\nHalf-open interval [a, b) includes the left endpoint but excludes the right: a \\leq x &lt; b\nHalf-open interval (a, b] excludes the left endpoint but includes the right: a &lt; x \\leq b\n\nStandard convention: Most statistical software, including R, uses left-closed, right-open intervals [a, b) for all bins except the last one, which is fully closed [a, b]. This means:\n\nThe value at the lower boundary is included in the bin\nThe value at the upper boundary belongs to the next bin\nThe very last bin includes both boundaries to capture the maximum value\n\nFor example, with bins 0-20, 20-40, 40-60, 60-80, 80-100:\n\n\n\nScore Range\nInterval Notation\nValues Included\n\n\n\n\n0-20\n[0, 20)\n0 ≤ score &lt; 20\n\n\n20-40\n[20, 40)\n20 ≤ score &lt; 40\n\n\n40-60\n[40, 60)\n40 ≤ score &lt; 60\n\n\n60-80\n[60, 80)\n60 ≤ score &lt; 80\n\n\n80-100\n[80, 100]\n80 ≤ score ≤ 100\n\n\n\nThis convention ensures that:\n\nEvery value is counted exactly once (no double-counting)\nNo values fall through the cracks\nThe bins partition the entire range completely\n\nWhen presenting frequency tables in reports, you can simply write “0-20, 20-40, …” and note that bins are left-closed, right-open, or explicitly show the interval notation if precision is important.\nFrequency histogram shows the raw counts:\n\n# R code example\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     main = \"Distribution of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Frequency\",\n     col = \"lightblue\")\n\n\n\n\n\n\n\n\nRelative frequency histogram shows proportions (useful when comparing groups of different sizes):\n\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     freq = FALSE,  # This creates relative frequency/density\n     main = \"Distribution of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Relative Frequency\",\n     col = \"lightgreen\")\n\n\n\n\n\n\n\n\nDensity histogram adjusts for interval width and is used with density curves:\n\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     freq = FALSE,  # Creates density scale\n     main = \"Distribution of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Density\",\n     col = \"lightcoral\")\n\n\n\n\n\n\n\n\n\n\nDensity Curves\nA density curve is a smooth line that approximates/models the shape of a distribution. Unlike histograms that show actual data in discrete bins, density curves show the overall pattern as a continuous function. The area under the entire curve always equals 1, and the area under any portion of the curve represents the proportion of observations in that range.\n\n# Adding a density curve to a histogram\nhist(exam_scores, \n     freq = FALSE,\n     main = \"Exam Scores with Density Curve\",\n     xlab = \"Score\",\n     ylab = \"Density\",\n     col = \"lightblue\",\n     border = \"white\")\nlines(density(exam_scores), \n      col = \"darkred\", \n      lwd = 2)\n\n\n\n\n\n\n\n\nDensity curves are particularly useful for:\n\nIdentifying the shape of the distribution (symmetric, skewed, bimodal)\nComparing multiple distributions on the same plot\nUnderstanding the theoretical (true) distribution underlying your data\n\n\n\n\n\n\n\nTip\n\n\n\nIn statistics, a percentile indicates the relative position of a data point within a dataset by showing the percentage of observations that fall at or below that value. For example, if a student scores at the 90th percentile on a test, their score is equal to or higher than 90% of all other scores.\nQuartiles are special percentiles that divide data into four equal parts: the first quartile (Q1, 25th percentile), second quartile (Q2, 50th percentile, also the median), and third quartile (Q3, 75th percentile). If Q1 = 65 points, then 25% of students scored 65 or below.\nMore generally, quantiles are values that divide data into equal-sized groups—percentiles divide into 100 parts, quartiles into 4 parts, deciles into 10 parts, and so on.\n\n\n\n\nVisualizing Cumulative Frequency (*)\nCumulative frequency plots, also called ogives (pronounced “oh-jive”), display how frequencies accumulate across values. These plots use lines rather than bars and always increase from left to right, eventually reaching the total number of observations (for cumulative frequency) or 1.0 (for cumulative relative frequency).\nCumulative frequency plots are excellent for:\n\nFinding percentiles and quartiles visually\nDetermining what proportion of data falls below or above a certain value\nComparing distributions of different groups\n\n\n# Creating cumulative frequency data\nscore_breaks &lt;- seq(0, 100, by = 10)\nfreq_counts &lt;- hist(exam_scores, breaks = score_breaks, plot = FALSE)$counts\ncumulative_freq &lt;- cumsum(freq_counts)\n\n# Plotting cumulative frequency\nplot(score_breaks[-1], cumulative_freq,\n     type = \"b\",  # both points and lines\n     main = \"Cumulative Frequency of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Cumulative Frequency\",\n     col = \"darkblue\",\n     lwd = 2,\n     pch = 19)\ngrid()\n\n\n\n\n\n\n\n\nFor cumulative relative frequency (which is more commonly used):\n\n# Cumulative relative frequency\ncumulative_rel_freq &lt;- cumulative_freq / length(exam_scores)\n\nplot(score_breaks[-1], cumulative_rel_freq,\n     type = \"b\",\n     main = \"Cumulative Relative Frequency of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Cumulative Relative Frequency\",\n     col = \"darkred\",\n     lwd = 2,\n     pch = 19,\n     ylim = c(0, 1))\ngrid()\nabline(h = c(0.25, 0.5, 0.75), lty = 2, col = \"gray\")  # Quartile lines\n\n\n\n\n\n\n\n\nThe cumulative relative frequency curve makes it easy to read percentiles. For example, if you draw a horizontal line at 0.75 and see where it intersects the curve, the corresponding x-value is the 75th percentile—the score below which 75% of students fall.\n\n\n\nDiscrete vs. Continuous Distributions\nThe type of variable you’re analyzing determines how you visualize its distribution:\nDiscrete distributions apply to variables that can only take specific, countable values. Examples include number of children in a family (0, 1, 2, 3…), number of customer complaints per day, or responses on a 5-point Likert scale.\nFor discrete data, we typically use:\n\nBar charts (with gaps between bars) rather than histograms\nFrequency or relative frequency on the y-axis\nEach distinct value gets its own bar\n\n\n# Example: Number of children per family\nchildren &lt;- c(0, 1, 2, 2, 1, 3, 0, 2, 1, 4, 2, 1, 0, 2, 3)\nbarplot(table(children),\n        main = \"Distribution of Number of Children\",\n        xlab = \"Number of Children\",\n        ylab = \"Frequency\",\n        col = \"skyblue\")\n\n\n\n\n\n\n\n\nContinuous distributions apply to variables that can take any value within a range. Examples include temperature, response time, height, or turnout percentage.\nFor continuous data, we use:\n\nHistograms (with touching bars) that group data into intervals\nDensity curves to show the smooth pattern\nDensity on the y-axis when using density curves\n\n\n# Generate response time data (in seconds)\nset.seed(456)  # For reproducibility\nresponse_time &lt;- rgamma(200, shape = 2, scale = 1.5)\n\n# Example: Response time distribution\nhist(response_time, \n     breaks = 15,\n     freq = FALSE,\n     main = \"Distribution of Response Time\",\n     xlab = \"Response Time (seconds)\",\n     ylab = \"Density\",\n     col = \"lightgreen\",\n     border = \"white\")\nlines(density(response_time), \n      col = \"darkgreen\", \n      lwd = 2)\n\n\n\n\n\n\n\n\nThe key difference is that discrete distributions show probability at specific points, while continuous distributions show probability density across ranges. For continuous variables, the probability of any exact value is essentially zero—instead, we talk about the probability of falling within an interval.\nUnderstanding whether your variable is discrete or continuous guides your choice of visualization and statistical methods, ensuring your analysis accurately represents the nature of your data.\n\n\nDescribing Distributions\nShape Characteristics:\nSymmetry vs. Skewness:\n\nSymmetric: Mirror image around center (example: heights in homogeneous population)\nRight-skewed (positive skew): Long tail to right (example: income, wealth)\nLeft-skewed (negative skew): Long tail to left (example: age at death in developed countries)\n\nExample of Skewness Impact:\nIncome distribution in the U.S.:\n\nMedian household income: ~$70,000\nMean household income: ~$100,000\nMean &gt; Median indicates right skew\nA few very high incomes pull the mean up\n\n\nModality:\n\nUnimodal: One peak (example: test scores)\nBimodal: Two peaks (example: height when mixing males and females)\nMultimodal: Multiple peaks (example: age distribution in a college town—peaks at college age and middle age)\n\n\n\n\n\n\n\n\n\n\nImportant Probability Distributions:\nNormal (Gaussian) Distribution:\n\nBell-shaped, symmetric\nCharacterized by mean (\\mu) and standard deviation (\\sigma)\nAbout 68% of values within \\mu \\pm \\sigma\nAbout 95% within \\mu \\pm 2\\sigma\nAbout 99.7% within \\mu \\pm 3\\sigma\n\nDemographic Applications:\n\nHeights within homogeneous populations\nMeasurement errors\nSampling distributions of means (Central Limit Theorem)\n\nBinomial Distribution:\n\nNumber of successes in n independent trials\nEach trial has probability p of success\nMean = np, Variance = np(1-p)\n\nExample: Number of male births out of 100 births (p \\approx 0.512)\nPoisson Distribution:\n\nCount of events in fixed time/space\nMean = Variance = \\lambda\nGood for rare events\n\nDemographic Applications:\n\nNumber of deaths per day in small town\nNumber of births per hour in hospital\nNumber of accidents at intersection per month\n\n\n\n\nVisualizing Frequency Distributions (*)\nHistogram: For continuous data, shows frequency with bar heights.\n\nX-axis: Value ranges (bins)\nY-axis: Frequency or density\nNo gaps between bars (continuous data)\nBin width affects appearance\n\nBar Chart: For categorical data, shows frequency with separated bars.\n\nX-axis: Categories\nY-axis: Frequency\nGaps between bars (discrete categories)\nOrder may or may not matter\n\nCumulative Distribution Function (CDF): Shows proportion of values ≤ each point of data.\n\nAlways increases (or stays flat)\nStarts at 0, ends at 1\nSteep slopes indicate common values\nFlat areas indicate rare values\n\nBox Plot (Box-and-Whisker Plot): A visual summary that displays the distribution’s key statistics using five key values.\nThe Five-Number Summary:\n\nMinimum: Leftmost whisker end (excluding outliers)\nQ1 (First Quartile): Left edge of the box (25th percentile)\nMedian (Q2): Line inside the box (50th percentile)\n\nQ3 (Third Quartile): Right edge of the box (75th percentile)\nMaximum: Rightmost whisker end (excluding outliers)\n\nWhat It Reveals:\n\nSkewness: If median line is off-center in the box, or whiskers are unequal\nSpread: Wider boxes and longer whiskers indicate more variability\nOutliers: Immediately visible as separate points\nSymmetry: Equal whisker lengths and centered median suggest normal distribution\n\nQuick Interpretation:\n\nNarrow box = consistent data\nLong whiskers = wide range of values\n\nMany outliers = potential data quality issues or interesting extreme cases\nMedian closer to Q1 = right-skewed data (tail extends right)\nMedian closer to Q3 = left-skewed data (tail extends left)\n\nBox plots are especially useful for comparing multiple groups side-by-side!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#variables-and-measurement-scales",
    "href": "chapter1.html#variables-and-measurement-scales",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.8 Variables and Measurement Scales",
    "text": "1.8 Variables and Measurement Scales\n\nA variable is any characteristic that can take different values across units of observation.\n\n\nMeasurement: Transforming Concepts into Numbers\n\nThe Political World is Full of Data\nPolitical science has evolved from a primarily theoretical discipline to one that increasingly relies on empirical evidence. Whether we’re studying:\n\nElection outcomes: Why do people vote the way they do?\nPublic opinion: What shapes attitudes toward immigration or climate policy?\nInternational relations: What factors predict conflict between nations?\nPolicy effectiveness: Did a new education policy actually improve outcomes?\n\nWe need systematic ways to analyze data and draw conclusions that go beyond anecdotes and personal impressions.\n\nConsider this question: “Does democracy lead to economic growth?”\n\nYour intuition might suggest yes—democratic countries tend to be wealthier. But is this causation or correlation? Are there exceptions? How confident can we be in our conclusions?\nStatistics provides the tools to move from hunches to evidence-based answers, helping us distinguish between what seems true and what actually is true.\n\n\nThe Challenge of Measurement in Social Sciences\nIn social sciences, we often struggle with the fact that key concepts do not translate directly into numbers:\n\nHow do we measure “democracy”?\nWhat number captures “political ideology”?\nHow do we quantify “institutional strength”?\nHow do we measure “political participation”?\n\n\n\n\n\n\n\n\n🔍 Correlation ≠ Causation: Understanding Spurious Relationships\n\n\n\n\nThe Fundamental Distinction\nCorrelation measures how two variables move together:\n\nPositive: Both increase together (study hours ↑, grades ↑)\nNegative: One increases while other decreases (TV hours ↑, grades ↓)\nMeasured by correlation coefficient: r \\in [-1, 1]\n\nCausation means one variable directly influences another:\n\nX \\rightarrow Y: Changes in X directly cause changes in Y\nRequires: (1) correlation, (2) temporal precedence, (3) no alternative explanations\n\n\n\nThe Danger: Spurious Correlation\nA spurious correlation occurs when two variables appear related but are actually both influenced by a third variable (a confounder).\nClassic Example:\n\nObserved: Ice cream sales correlate with drowning deaths\nSpurious conclusion: Ice cream causes drowning (❌)\nReality: Summer weather (confounder) causes both:\nSummer → More ice cream sales\nSummer → More swimming → More drownings\n\nMathematical representation:\n\nObserved correlation: \\text{Cor}(X,Y) \\neq 0\nBut the true model: X = \\alpha Z + \\epsilon_1 and Y = \\beta Z + \\epsilon_2\nWhere Z is the confounding variable causing both\n\n\n\nConfounding: The Hidden Influence\nA confounding variable (confounder):\n\nAffects both the presumed cause and effect\nCreates an illusion of direct causation 3. Must be controlled for valid causal inference\n\nResearch Example:\n\nObserved: Coffee consumption correlates with heart disease\nPotential confounder: Smoking (coffee drinkers more likely to smoke)\nTrue relationships:\nSmoking → Heart disease (causal)\nSmoking → Coffee consumption (association)\nCoffee → Heart disease (spurious without controlling for smoking)\n\n\n\nHow to Identify Causal Relationships\n\nRandomized Controlled Trials (RCTs): Random assignment breaks confounding\nNatural Experiments: External events create “as-if” random variation\nStatistical Control: Include confounders in regression models\nInstrumental Variables: Find variables affecting X but not Y directly\n\n\n\nKey Takeaway\nFinding correlation is easy. Establishing causation is hard. Always ask: “What else could explain this relationship?”\nRemember: The most dangerous phrase in empirical research is “our data shows that X causes Y” when all you’ve measured is correlation.\n\n\n\n\n\n\n\n\n\n\n📊 Quick Test: Correlation or Causation?\n\n\n\n\n\nFor each scenario, identify whether the relationship is likely causal or spurious:\n\nCities with more churches have more crime\n\nAnswer: Spurious (confounder: population size)\n\nSmoking leads to lung cancer\n\nAnswer: Causal (established through multiple study designs)\n\nStudents with more books at home get better grades\n\nAnswer: Likely spurious (confounders: parental education, income)\n\nCountries with higher chocolate consumption have more Nobel laureates\n\nAnswer: Spurious (confounder: wealth/development level)\n\n\n\n\n\n\n\n\n\nTypes of Variables\nQuantitative Variables represent amounts or quantities and can be:\nContinuous Variables: Can take any value within a range, limited only by measurement precision.\n\nAge (22.5 years, 22.51 years, 22.514 years…)\nIncome ($45,234.67)\nHeight (175.3 cm)\nPopulation density (432.7 people per square kilometer)\n\nDiscrete Variables: Can only take specific values, usually counts.\n\nNumber of children in a family (0, 1, 2, 3…)\nNumber of marriages (0, 1, 2…)\nNumber of rooms in a dwelling (1, 2, 3…)\nNumber of migrants entering a country per year\n\nQualitative Variables represent categories or qualities and can be:\nNominal Variables: Categories with no inherent order.\n\nCountry of birth (USA, Mexico, Canada…)\nReligion (Christian, Muslim, Hindu, Buddhist…)\nBlood type (A, B, AB, O)\nCause of death (heart disease, cancer, accident…)\n\nOrdinal Variables: Categories with a meaningful order but unequal intervals.\n\nEducation level (no schooling, primary, secondary, tertiary)\nSatisfaction with healthcare (very dissatisfied, dissatisfied, neutral, satisfied, very satisfied)\nSocioeconomic status (low, middle, high)\nSelf-rated health (poor, fair, good, excellent)\n\n\n\n\nMeasurement Scales\nUnderstanding measurement scales is crucial because they determine which statistical methods are appropriate:\nNominal Scale: Categories only—we can count frequencies but cannot order or perform arithmetic. Example: We can say 45% of residents were born locally, but we cannot calculate an “average birthplace.”\nOrdinal Scale: Order matters but differences between values are not necessarily equal. Example: The difference between “poor” and “fair” health may not equal the difference between “good” and “excellent” health.\nInterval Scale: Equal intervals between values but no true zero point. Example: Temperature in Celsius—the difference between 20°C and 30°C equals the difference between 30°C and 40°C, but 0°C doesn’t mean “no temperature.”\nRatio Scale: Equal intervals with a true zero point, allowing all mathematical operations. Example: Income—$40,000 is twice as much as $20,000, and $0 means no income.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#parameters-statistics-estimands-estimators-and-estimates",
    "href": "chapter1.html#parameters-statistics-estimands-estimators-and-estimates",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.9 Parameters, Statistics, Estimands, Estimators, and Estimates",
    "text": "1.9 Parameters, Statistics, Estimands, Estimators, and Estimates\nStatistical inference is the process of learning unknown features of a population from finite samples. This section introduces five core ideas.\n\nQuick comparison (summary table)\n\n\n\n\n\n\n\n\n\n\nTerm\nWhat is it?\nRandom?\nTypical notation\nExample\n\n\n\n\nEstimand\nPrecisely defined target quantity\nNo\nwords (specification)\n“Median household income in CA on 2024-01-01.”\n\n\nParameter\nThe true population value of that quantity\nNo*\n\\theta,\\ \\mu,\\ p,\\ \\beta\nTrue mean age at first birth in France (2023)\n\n\nEstimator\nA rule/formula mapping data to an estimate\n—\n\\hat\\theta = g(X_1,\\dots,X_n)\n\\bar X, \\hat p = X/n, OLS \\hat\\beta\n\n\nStatistic\nAny function of the sample (includes estimators)\nYes\n\\bar X,\\ s^2,\\ r\nSample mean from n=500 births\n\n\nEstimate\nThe numerical value obtained from the estimator\nNo\na number\n\\hat p = 0.433\n\n\n\n*Fixed for the population/time frame you define; it can differ across places/times.\n\n\n\nParameter\nA parameter is a numerical characteristic of a population—fixed but unknown.\n\nCommon parameters: \\mu (mean), \\sigma^2 (variance), p (proportion), \\beta (regression effect), \\lambda (rate).\n\nExample. The true mean age at first birth for all women in France, 2023, is a parameter \\mu. We do not know it without full population data.\n\n\n\n\n\n\nNote\n\n\n\nNotation. A common convention is Greek letters for population parameters and Roman letters for sample statistics. Consistency matters more than the specific symbols chosen.\n\n\n\n\n\nStatistic\nA statistic is any function of sample data. Statistics vary from sample to sample.\n\nExamples: \\bar x (sample mean), s^2 (sample variance), \\hat p (sample proportion), r (sample correlation), b (sample regression slope).\n\nExample. From a random sample of 500 births, \\bar x = 30.9 years; a different sample might give 31.4.\n\n\n\nEstimand\nThe estimand is the target quantity—specified clearly enough that two researchers would compute the same number from the same full population.\n\nWell-specified estimands\n\n“Median household income in California on 2024-01-01.”\n“Male–female difference in life expectancy for births in Sweden, 2023.”\n“Proportion of 25–34 year-olds in urban areas with tertiary education.”\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhy precise definitions matter. “Unemployment rate” is ambiguous unless you specify (i) who counts as unemployed, (ii) age range, (iii) geography, (iv) time window. Different definitions lead to different parameters (e.g., U-1 … U-6 in the U.S.).\n\n\n\n\n\nEstimator\nAn estimator is the rule that turns data into an estimate.\n\nCommon estimators\n\n\\hat\\mu=\\bar X=\\frac{1}{n}\\sum_{i=1}^n X_i\n\n\n\\hat p=\\frac{X}{n}\\quad\\text{(with $X$ successes)}\n\n\ns^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhy n-1? Bessel’s correction makes s^2 unbiased for the population variance when the mean is estimated from the same data.\n\n\n\n\n\nJudging estimators: bias, variance, MSE, efficiency\nBias — is the estimator centered on the truth? If the same study were repeated many times, an unbiased estimator would average to the true value. A biased estimator would systematically miss it (too high or too low).\nVariance — how much do estimates differ across samples? Even without bias, repeated samples will not give exactly the same number. Lower variance means more stable results from sample to sample.\nMean Squared Error (MSE) — overall accuracy in one measure. MSE combines both components: \n\\mathrm{MSE}(\\hat\\theta)=\\mathrm{Var}(\\hat\\theta)+\\big(\\mathrm{Bias}(\\hat\\theta)\\big)^2.\n Lower MSE is better. An estimator with a small bias but much lower variance can have a lower MSE than an unbiased but highly variable one.\nEfficiency — comparative precision among estimators. Among unbiased estimators that target the same parameter with the same data, the more efficient estimator has the smaller variance. When small bias is allowed, compare using MSE instead.\n\n\n\n\n\n\nSources of precision (common cases)\n\n\n\n\nSample mean (simple random sample): \n\\operatorname{Var}(\\bar X)=\\frac{\\sigma^2}{n},\\qquad\n\\mathrm{SE}(\\bar X)=\\frac{\\sigma}{\\sqrt{n}}.\n Larger n reduces SE at the rate 1/\\sqrt{n}.\nSample proportion: \n\\operatorname{Var}(\\hat p)=\\frac{p(1-p)}{n},\\qquad\n\\mathrm{SE}(\\hat p)=\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}.\n\nDesign effects: clustering, stratification, and weights can change variance. Match your SE method to the sampling design.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nPractical guidance\n\nDefine the estimand precisely (population, time, unit, and definition).\nSelect an estimator that directly targets that estimand.\nAmong unbiased options, prefer lower variance (greater efficiency).\nWhen bias–variance trade-offs are relevant, compare MSE.\nReport the estimate and its uncertainty (SE or CI), and state key assumptions.\n\n\n\n\n\n\nEstimate\nAn estimate is the numerical value obtained after applying the estimator to the data.\nWorked example\n\nEstimand: Approval share among all U.S. adults today.\nParameter: p (unknown true approval).\nEstimator: \\hat p = X/n.\nSample: n=1{,}500, approvals X=650.\nEstimate: \\hat p = 650/1500 = 0.433 (43.3%).\n\n\n\n\nCommon confusions and clarifications\n\nParameter vs statistic: Population quantity vs sample-derived quantity.\nEstimator vs estimate: Procedure vs numerical result.\nTime index: Parameters can change over time (e.g., Q2 vs Q3).\nDefinition first: Specify the estimand before choosing the estimator.\n\n\n\n\n\n\n\n\nUnderstanding Different Types of Unpredictability\n\n\n\nNot all uncertainty is the same. Understanding different sources of unpredictability helps us choose appropriate statistical methods and interpret results correctly.\n\n\n\n\n\n\n\n\n\nConcept\nWhat is it?\nSource of unpredictability\nExample\n\n\n\n\nRandomness\nIndividual outcomes are uncertain, but the probability distribution is known or modeled.\nFluctuations across realizations; lack of information about a specific outcome.\nDice roll, coin toss, polling sample\n\n\nChaos\nDeterministic dynamics highly sensitive to initial conditions (butterfly effect).\nTiny initial differences grow rapidly → large trajectory divergences.\nWeather forecasting, double pendulum, population dynamics\n\n\nEntropy\nA measure of uncertainty/dispersion (information-theoretic or thermodynamic).\nLarger when outcomes are more evenly distributed (less predictive information).\nShannon entropy in data compression\n\n\n“Haphazardness” (colloquial)\nA felt lack of order without an explicit model; a mixture of mechanisms.\nNo structured description or stable rules; overlapping processes.\nTraffic patterns, social media trends\n\n\nQuantum randomness\nA single outcome is not determined; only the distribution is specified (Born rule).\nFundamental (ontological) indeterminacy of individual measurements.\nElectron spin measurement, photon polarization\n\n\n\n\nKey Distinctions for Statistical Practice\nDeterministic chaos ≠ statistical randomness: A chaotic system is fully deterministic yet practically unpredictable due to extreme sensitivity to initial conditions. Statistical randomness, by contrast, models uncertainty via probability distributions where individual outcomes are genuinely uncertain.\nWhy this matters: In statistics, we typically model phenomena as random processes, assuming we can specify probability distributions even when individual outcomes are unpredictable. This assumption underlies most statistical inference.\n\n\nQuantum Mechanics and Fundamental Randomness\nIn the Copenhagen interpretation, randomness is fundamental (ontological): a single outcome cannot be predicted, but the probability distribution is given by the Born rule.\nThis represents true randomness at the most basic level of nature.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#statistical-error-and-uncertainty",
    "href": "chapter1.html#statistical-error-and-uncertainty",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.10 Statistical Error and Uncertainty",
    "text": "1.10 Statistical Error and Uncertainty\n\nIntroduction: Why Uncertainty Matters\nNo measurement or estimate is perfect. Understanding different types of error is crucial for interpreting results and improving study design.\n\n\n\n\n\n\nThe Central Challenge\n\n\n\nEvery time we use a sample to learn about a population, we introduce uncertainty. The key is to:\n\nQuantify this uncertainty honestly\nDistinguish between different sources of error\nCommunicate results transparently\n\n\n\n\n\n\nTypes of Error\n\nRandom Error\nRandom error represents unpredictable fluctuations that vary from observation to observation without a consistent pattern. These errors arise from various sources of natural variability in the data collection and measurement process.\n\n\n\n\n\n\nKey Characteristics\n\n\n\n\nUnpredictable Direction: Sometimes too high, sometimes too low\nNo Consistent Pattern: Varies randomly across observations\nAverages to Zero: Over many measurements, positive and negative errors cancel out\nQuantifiable: Can be estimated and reduced through appropriate methods\n\n\n\nRandom error encompasses several subtypes:\n\nSampling Error\nSampling error is the most common type of random error—it arises because we observe a sample rather than the entire population. Different random samples from the same population will yield different estimates purely by chance.\nKey properties:\n\nDecreases with sample size: \\propto 1/\\sqrt{n}\nQuantifiable using probability theory\nInevitable when working with samples\n\nExample: Internet Access Survey\nImagine surveying 100 random households about internet access:\n\n\n\n\n\n\n\n\n\nThe variation around the true value (red line) represents sampling error. With larger samples, estimates would cluster more tightly.\n\n\nMeasurement Error\nMeasurement error is random variation in the measurement process itself—even when measuring the same thing repeatedly.\nExamples:\n\nSlight variations when reading a thermometer due to parallax\nRandom fluctuations in electronic instruments\nInconsistencies in human judgment when coding qualitative data\n\nUnlike sampling error (which comes from who/what we observe), measurement error comes from how we observe.\n\n\nOther Sources of Random Error\n\nProcessing error: Random mistakes in data entry, coding, or computation\nModel specification error: When the true relationship is more complex than assumed\nTemporal variation: Natural day-to-day fluctuations in the phenomenon being measured\n\n\n\n\nSystematic Error (Bias)\nSystematic error represents consistent deviation in a particular direction. Unlike random error, it doesn’t average out with repeated sampling or measurement—it persists and pushes results consistently away from the truth.\n\nSelection BiasMeasurement BiasResponse BiasNon-response BiasSurvivorship BiasObserver/Interviewer Bias\n\n\nSampling method systematically excludes certain groups.\nExample: Phone surveys during business hours underrepresent employed people.\n\n\nMeasurement instrument consistently over/under-measures.\nExample: Scales that always read 2 pounds heavy; survey questions that lead respondents toward particular answers.\n\n\nRespondents systematically misreport.\nExample: People underreport alcohol consumption, overreport voting, or give socially desirable answers.\n\n\nNon-responders differ systematically from responders.\nExample: Very sick and very healthy people less likely to respond to health surveys, leaving only those with moderate health.\n\n\nOnly observing “survivors” of some process.\nExample: During WWII, the military analyzed returning bombers to determine where to add armor. Planes showed the most damage on wings and tail sections. Abraham Wald realized the flaw: they should armor where there weren’t bullet holes—the engine and cockpit. Planes hit in those areas never made it back to be analyzed. They were only studying the survivors.\n\n\nObservers or interviewers systematically influence results.\nExample: Interviewers unconsciously prompting certain responses or recording observations that confirm their expectations.\n\n\n\n\n\nThe Bias-Variance Decomposition\nMathematically, total error (Mean Squared Error) decomposes into:\n\\mathrm{MSE}(\\hat\\theta) = \\underbrace{\\mathrm{Var}(\\hat\\theta)}_{\\text{random error}} + \\underbrace{\\big(\\mathrm{Bias}(\\hat\\theta)\\big)^2}_{\\text{systematic error}}\n\n\n\n\n\n\nCritical Insight\n\n\n\n\nA large biased sample gives a precisely wrong answer.\n\n\nIncrease n → reduces random error (specifically sampling error)\nImprove study design → reduces systematic error\nBetter instruments → reduces measurement error\n\n\n\n\n\n\nDifferent combinations of bias and variance in estimation\n\n\nIntuitive analogy: Think of trying to hit a bullseye:\n\nRandom error = scattered shots around a target (sometimes left, sometimes right, sometimes high, sometimes low)\nSystematic error = consistently hitting the same wrong spot (all shots clustered, but away from the bullseye)\nIdeal = shots tightly clustered at the bullseye center\n\n\n\n\n\nQuantifying Uncertainty\n\nStandard Error\nThe standard error (SE) quantifies how much an estimate varies across different possible samples. It measures sampling error specifically.\n\n\nFor a Proportion:\nSE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\nFor a Mean:\nSE(\\bar{x}) = \\frac{s}{\\sqrt{n}}\n\nFor a Difference:\nSE(\\bar{x}_1 - \\bar{x}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\n\n\n\n\n\n\n\nWhat SE Tells Us\n\n\n\nStandard error quantifies sampling error only. It does not account for systematic errors (bias), measurement error, or other sources of uncertainty.\n\n\n\n\nMargin of Error\nThe margin of error (MOE) represents the expected maximum difference between sample estimate and true parameter.\n\\text{MOE} = \\text{Critical Value} \\times \\text{Standard Error}\n\n\n\n\n\n\nUnderstanding the Critical Value\n\n\n\n\n\nFor 95% confidence, we use 1.96 (often simplified to 2). This ensures that ~95% of intervals constructed this way will contain the true parameter.\n\n90% confidence: z = 1.645\n95% confidence: z = 1.96\n99% confidence: z = 2.576\n\n\n\n\n\n\nConfidence Intervals\nA confidence interval provides a range of plausible values:\n\\text{CI} = \\text{Estimate} \\pm (\\text{Critical Value} \\times \\text{Standard Error})\n\n\n\n\n\n\nImportant Limitation\n\n\n\nConfidence intervals quantify sampling uncertainty but assume no systematic error. A perfectly precise estimate (narrow CI) can still be biased if the study design is flawed.\n\n\n\n\n\n\nPractical Application: Opinion Polling\n\n\n\n\n\n\nCase Study: Political Polls\n\n\n\nWhen a poll reports “Candidate A: 52%, Candidate B: 48%”, this is incomplete without uncertainty quantification.\n\n\n\nThe Golden Rule of Polling\nWith ~1,000 randomly selected respondents:\n\nMargin of error: ±3 percentage points (95% confidence)\nInterpretation: A reported 52% means true support likely between 49% and 55%\nWhat this covers: Only random sampling error—assumes no systematic bias\n\n\n\n\n\n\n\nCritical Distinction\n\n\n\nThe ±3% margin of error quantifies sampling uncertainty only. It does not account for:\n\nCoverage bias (who’s excluded from the sampling frame)\nNon-response bias (who refuses to participate)\nResponse bias (people misreporting their true views)\nTiming effects (opinions changing between poll and election)\n\n\n\n\n\nSample Size and Precision\n\n\n\nSample Size\nMargin of Error (95%)\nUse Case\n\n\n\n\nn = 100\n± 10 pp\nBroad direction only\n\n\nn = 400\n± 5 pp\nGeneral trends\n\n\nn = 1,000\n± 3 pp\nStandard polls\n\n\nn = 2,500\n± 2 pp\nHigh precision\n\n\nn = 10,000\n± 1 pp\nVery high precision\n\n\n\n\n\n\n\n\n\nLaw of Diminishing Returns\n\n\n\nTo halve the margin of error, you need four times the sample size because \\text{MOE} \\propto 1/\\sqrt{n}\nThis applies only to sampling error. Doubling your sample size from 1,000 to 2,000 won’t fix systematic problems like biased question wording or unrepresentative sampling methods.\n\n\n\n\nWhat Quality Polls Should Report\nA transparent poll discloses:\n\nField dates: When was data collected?\nPopulation and sampling method: Who was surveyed and how were they selected?\nSample size: How many people responded?\nResponse rate: What proportion of contacted people participated?\nWeighting procedures: How was the sample adjusted to match population characteristics?\nMargin of sampling error: Quantification of sampling uncertainty\nQuestion wording: Exact text of questions asked\n\n\n\n\n\n\n\nThe Reporting Gap\n\n\n\nMost news reports mention only the topline numbers and occasionally the margin of error. They rarely discuss potential systematic biases, which can be much larger than sampling error.\n\n\n\n\n\n\nVisualization: Sampling Variability\nThe following simulation demonstrates how confidence intervals behave across repeated sampling:\n\n\nShow simulation code\nlibrary(ggplot2)\nset.seed(42)\n\n# Parameters\nn_polls      &lt;- 20\nn_people     &lt;- 100\ntrue_support &lt;- 0.50\n\n# Simulate independent polls\nsupport &lt;- rbinom(n_polls, n_people, true_support) / n_people\n\n# Calculate standard errors and margins of error\nse   &lt;- sqrt(support * (1 - support) / n_people)\nmoe  &lt;- 2 * se  # Simplified multiplier for clarity\n\n# Create confidence intervals\nlower &lt;- pmax(0, support - moe)\nupper &lt;- pmin(1, support + moe)\n\n# Check coverage\ncovers &lt;- (lower &lt;= true_support) & (upper &gt;= true_support)\nn_cover &lt;- sum(covers)\n\nresults &lt;- data.frame(\n  poll = seq_len(n_polls),\n  support, se, moe, lower, upper, covers\n)\n\n# Create visualization\nggplot(results, aes(x = poll, y = support, color = covers)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                width = 0.3, alpha = 0.8, size = 1) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = true_support, \n             linetype = \"dashed\", \n             color = \"black\",\n             alpha = 0.7) +\n  scale_color_manual(\n    values = c(\"TRUE\" = \"forestgreen\", \"FALSE\" = \"darkorange\"),\n    labels = c(\"TRUE\" = \"Covers truth\", \"FALSE\" = \"Misses truth\"),\n    name   = NULL\n  ) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0, 1)) +\n  labs(\n    title    = \"Sampling Variability in 20 Independent Polls\",\n    subtitle = paste0(\n      \"Each poll: n = \", n_people, \" | True value = \",\n      scales::percent(true_support),\n      \" | Coverage: \", n_cover, \"/\", n_polls,\n      \" (\", round(100 * n_cover / n_polls), \"%)\"\n    ),\n    x = \"Poll Number\",\n    y = \"Estimated Support\",\n    caption = \"Error bars show approximate 95% confidence intervals\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Observation\n\n\n\nMost intervals capture the true value, but some “miss” purely due to sampling randomness. This is expected and quantifiable—it’s the nature of random sampling error.\nImportant: This simulation assumes no systematic bias. In real polling, systematic errors (non-response bias, coverage problems, question wording effects) can shift all estimates in the same direction, making them consistently wrong even with large samples.\n\n\n\n\n\nCommon Misconceptions\n\n\n\n\n\n\nMisconception #1: Margin of Error Covers All Uncertainty\n\n\n\n❌ Myth: “The true value is definitely within the margin of error”\n✅ Reality:\n\nWith 95% confidence, there’s still a 5% chance the true value falls outside the interval due to sampling randomness alone\nMore importantly, margin of error only covers sampling error, not systematic biases\nReal polls often have larger errors from non-response bias, question wording, or coverage problems than from sampling error\n\n\n\n\n\n\n\n\n\nMisconception #2: Larger Samples Fix Everything\n\n\n\n❌ Myth: “If we just survey more people, we’ll eliminate all error”\n✅ Reality:\n\nLarger samples reduce random error (particularly sampling error): more precise estimates\nLarger samples do NOT reduce systematic error: bias remains unchanged\nA poll of 10,000 people with 70% response rate and biased sampling frame will give a precisely wrong answer\nBetter to have 1,000 well-selected respondents than 10,000 poorly selected ones\n\n\n\n\n\n\n\n\n\nMisconception #3: Random = Careless\n\n\n\n❌ Myth: “Random error means someone made mistakes”\n✅ Reality:\n\nRandom error is inherent in sampling and measurement—it’s not a mistake\nEven with perfect methodology, different random samples yield different results\nRandom errors are predictable in aggregate even though unpredictable individually\nThe term “random” refers to the pattern (no systematic direction), not to carelessness\n\n\n\n\n\n\n\n\n\nMisconception #4: Confidence Intervals are Guarantees\n\n\n\n❌ Myth: “95% confidence means there’s a 95% chance the true value is in this specific interval”\n✅ Reality:\n\nThe true value is fixed (but unknown)—it either is or isn’t in the interval\n“95% confidence” means: if we repeated this process many times, about 95% of the intervals we construct would contain the true value\nEach specific interval either captures the truth or doesn’t—we just don’t know which\n\n\n\n\n\n\n\n\n\nMisconception #5: Bias Can Be Calculated Like Random Error\n\n\n\n❌ Myth: “We can calculate the bias just like we calculate standard error”\n✅ Reality:\n\nRandom error is quantifiable using probability theory because we know the sampling process\nSystematic error is usually unknown and unknowable without external validation\nYou can’t use the sample itself to detect bias—you need independent information about the population\nThis is why comparing polls to election results is valuable: it reveals biases that weren’t quantifiable beforehand\n\n\n\n\n\n\nReal-World Example: Polling Failures\n\n\n\n\n\n\nCase Study: When Polls Mislead\n\n\n\nConsider a scenario where 20 polls all show Candidate A leading by 3-5 points, with margins of error around ±3%. The polls seem consistent, but Candidate B wins.\nWhat happened?\n\nNot sampling error: All polls agreed—unlikely if only random variation\nLikely systematic error:\n\nNon-response bias: Certain voters consistently refused to participate\nSocial desirability bias: Some voters misreported their true preference\nTurnout modeling error: Wrong assumptions about who would actually vote\nCoverage bias: Sampling frame (e.g., phone lists) systematically excluded certain groups\n\n\nThe lesson: Consistency among polls doesn’t guarantee accuracy. All polls can share the same systematic biases, giving false confidence in wrong estimates.\n\n\n\n\n\nKey Takeaways\n\n\n\n\n\n\nEssential Points\n\n\n\nUnderstanding Error Types:\n\nRandom error is unpredictable variation that averages to zero\n\nSampling error: From observing a sample, not the whole population\nMeasurement error: From imperfect measurement instruments or processes\nReduced by: larger samples, better instruments, more measurements\n\nSystematic error (bias) is consistent deviation in one direction\n\nSelection bias, measurement bias, response bias, non-response bias, etc.\nReduced by: better study design, not larger samples\n\n\nQuantifying Uncertainty:\n\nStandard error measures typical sampling variability (one type of random error)\nMargin of error ≈ 2 × SE gives a range for 95% confidence about sampling uncertainty\nSample size and sampling error precision follow: \\text{SE} \\propto 1/\\sqrt{n}\n\nQuadrupling sample size halves sampling error\nDiminishing returns as n increases\n\nConfidence intervals provide plausible ranges but assume no systematic bias\n\nCritical Insights:\n\nA precisely wrong answer (large biased sample) is often worse than an imprecisely right answer (small unbiased sample)\nAlways consider both sampling error AND potential systematic biases—published margins of error typically ignore the latter\nTransparency matters: Report methodology, response rates, and potential biases, not just point estimates and margins of error\nValidation is essential: Compare estimates to known values whenever possible to detect systematic errors\n\n\n\n\n\n\n\n\n\nThe Practitioner’s Priority\n\n\n\nWhen designing studies:\nFirst: Minimize systematic error through careful design\n\nRepresentative sampling methods\nHigh response rates\nUnbiased measurement tools\nProper question wording\n\nThen: Optimize sample size to achieve acceptable precision\n\nLarger samples help only after bias is addressed\nBalance cost vs. precision improvement\nRemember diminishing returns\n\nFinally: Report uncertainty honestly\n\nState assumptions clearly\nAcknowledge potential biases\nDon’t let precise estimates create false confidence",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#sampling-and-sampling-methods",
    "href": "chapter1.html#sampling-and-sampling-methods",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.11 Sampling and Sampling Methods (*)",
    "text": "1.11 Sampling and Sampling Methods (*)\nSampling is the process of selecting a subset of individuals from a population to estimate characteristics of the whole population. The way we sample profoundly affects what we can conclude from our data.\n\nThe Sampling Frame\nBefore discussing methods, we must understand the sampling frame—the list or device from which we draw our sample. The frame should ideally include every population member exactly once.\nCommon Sampling Frames:\n\nElectoral rolls (for adult citizens)\nTelephone directories (increasingly problematic due to mobile phones and unlisted numbers)\nAddress lists from postal services\nBirth registrations (for newborns)\nSchool enrollment lists (for children)\nTax records (for income earners)\nSatellite imagery (for dwellings in remote areas)\n\nFrame Problems:\n\nUndercoverage: Frame missing population members (homeless individuals not on address lists)\nOvercoverage: Frame includes non-population members (deceased people still on voter rolls)\nDuplication: Same unit appears multiple times (people with multiple phone numbers)\nClustering: Multiple population members per frame unit (multiple families at one address)\n\n\n\nProbability Sampling Methods\nProbability sampling gives every population member a known, non-zero probability of selection. This allows us to make statistical inferences about the population.\n\nSimple Random Sampling (SRS)\nEvery possible sample of size n has equal probability of selection. It’s the gold standard for statistical theory but often impractical for large populations.\nHow It Works:\n\nNumber every unit in the population from 1 to N\nUse random numbers to select n units\nEach unit has probability n/N of selection\n\nExample: To sample 50 students from a school of 1,000:\n\nAssign each student a number from 1 to 1,000\nGenerate 50 random numbers between 1 and 1,000\nSelect students with those numbers\n\nAdvantages:\n\nStatistically optimal\nEasy to analyze\nNo need for additional information about population\n\nDisadvantages:\n\nRequires complete sampling frame\nCan be expensive (selected units might be far apart)\nMay not represent important subgroups well by chance\n\n\n\nSystematic Sampling\nSelect every kth element from an ordered sampling frame, where k = N/n (the sampling interval).\nHow It Works:\n\nCalculate sampling interval k = N/n\nRandomly select starting point between 1 and k\nSelect every kth unit thereafter\n\nExample: To sample 100 houses from 5,000 on a street listing:\n\nk = 5,000/100 = 50\nRandom start: 23\nSample houses: 23, 73, 123, 173, 223…\n\nAdvantages:\n\nSimple to implement in field\nSpreads sample throughout population\n\nDisadvantages:\n\nCan introduce bias if there’s periodicity in the frame\n\nHidden Periodicity Example: Sampling every 10th apartment in buildings where corner apartments (numbers ending in 0) are all larger. This would bias our estimate of average apartment size.\n\n\nStratified Sampling\nDivide population into homogeneous subgroups (strata) before sampling. Sample independently within each stratum.\nHow It Works:\n\nDivide population into non-overlapping strata\nSample independently from each stratum\nCombine results with appropriate weights\n\nExample: Studying income in a city with distinct neighborhoods:\n\nStratum 1: High-income neighborhood (10% of population) - sample 100\nStratum 2: Middle-income neighborhood (60% of population) - sample 600\nStratum 3: Low-income neighborhood (30% of population) - sample 300\n\nTypes of Allocation:\nProportional: Sample size in each stratum proportional to stratum size\n\nIf stratum has 20% of population, it gets 20% of sample\n\nOptimal (Neyman): Larger samples from more variable strata\n\nIf income varies more in high-income areas, sample more there\n\nEqual: Same sample size per stratum regardless of population size\n\nUseful when comparing strata is primary goal\n\nAdvantages:\n\nEnsures representation of all subgroups\nCan increase precision substantially\nAllows different sampling methods per stratum\nProvides estimates for each stratum\n\nDisadvantages:\n\nRequires information to create strata\nCan be complex to analyze\n\n\n\nCluster Sampling\nSelect groups (clusters) rather than individuals. Often used when population is naturally grouped or when creating a complete frame is difficult.\nSingle-Stage Cluster Sampling:\n\nDivide population into clusters\nRandomly select some clusters\nInclude all units from selected clusters\n\nTwo-Stage Cluster Sampling:\n\nRandomly select clusters (Primary Sampling Units)\nWithin selected clusters, randomly select individuals (Secondary Sampling Units)\n\nExample: Surveying rural households in a large country:\n\nStage 1: Randomly select 50 villages from 1,000 villages\nStage 2: Within each selected village, randomly select 20 households\nTotal sample: 50 × 20 = 1,000 households\n\nMulti-Stage Example: National health survey:\n\nStage 1: Select states\nStage 2: Select counties within selected states\nStage 3: Select census blocks within selected counties\nStage 4: Select households within selected blocks\nStage 5: Select one adult within selected households\n\nAdvantages:\n\nDoesn’t require complete population list\nReduces travel costs (units clustered geographically)\nCan use different methods at different stages\nNatural for hierarchical populations\n\nDisadvantages:\n\nLess statistically efficient than SRS\nComplex variance estimation\nLarger samples needed for same precision\n\nDesign Effect: Cluster sampling typically requires larger samples than SRS. The design effect (DEFF) quantifies this:\n\\text{DEFF} = \\frac{\\text{Variance(cluster sample)}}{\\text{Variance(SRS)}}\nIf DEFF = 2, you need twice the sample size to achieve the same precision as SRS.\n\n\n\nNon-Probability Sampling Methods\nNon-probability sampling doesn’t guarantee known selection probabilities. While limiting statistical inference, these methods may be necessary or useful in certain situations.\n\nConvenience Sampling\nSelection based purely on ease of access. No attempt at representation.\nExamples:\n\nSurveying students in your class about study habits\nInterviewing people at a shopping mall about consumer preferences\nOnline polls where anyone can participate\nMedical studies using volunteers who respond to advertisements\n\nWhen It Might Be Acceptable:\n\nPilot studies to test survey instruments\nExploratory research to identify issues\nWhen studying processes believed to be universal\n\nMajor Problems:\n\nNo basis for inference to population\nSevere selection bias likely\nResults may be completely misleading\n\nReal Example: Literary Digest’s 1936 U.S. presidential poll surveyed 2.4 million people (huge sample!) but used telephone directories and club memberships as frames during the Depression, dramatically overrepresenting wealthy voters and incorrectly predicting Landon would defeat Roosevelt.\n\n\nPurposive (Judgmental) Sampling\nDeliberate selection of specific cases based on researcher judgment about what’s “typical” or “interesting.”\nExamples:\n\nSelecting “typical” villages to represent rural areas\nChoosing specific age groups for a developmental study\nSelecting extreme cases to understand range of variation\nPicking information-rich cases for in-depth study\n\nTypes of Purposive Sampling:\nTypical Case: Choose average or normal examples\n\nStudying “typical” American suburbs\n\nExtreme/Deviant Case: Choose unusual examples\n\nStudying villages with unusually low infant mortality to understand success factors\n\nMaximum Variation: Deliberately pick diverse cases\n\nSelecting diverse schools (urban/rural, rich/poor, large/small) for education research\n\nCritical Case: Choose cases that will be definitive\n\n“If it doesn’t work here, it won’t work anywhere”\n\nWhen It’s Useful:\n\nQualitative research focusing on depth over breadth\nWhen studying rare populations\nResource constraints limit sample size severely\nExploratory phases of research\n\nProblems:\n\nEntirely dependent on researcher judgment\nNo statistical inference possible\nDifferent researchers might select different “typical” cases\n\n\n\nQuota Sampling\nSelection to match population proportions on key characteristics. Like stratified sampling but without random selection within groups.\nHow Quota Sampling Works:\n\nIdentify key characteristics (age, sex, race, education)\nDetermine population proportions for these characteristics\nSet quotas for each combination\nInterviewers fill quotas using convenience methods\n\nDetailed Example: Political poll with quotas:\nPopulation proportions:\n\nMale 18-34: 15%\nMale 35-54: 20%\nMale 55+: 15%\nFemale 18-34: 16%\nFemale 35-54: 19%\nFemale 55+: 15%\n\nFor a sample of 1,000:\n\nInterview 150 males aged 18-34\nInterview 200 males aged 35-54\nAnd so on…\n\nInterviewers might stand on street corners approaching people who appear to fit needed categories until quotas are filled.\nWhy It’s Popular in Market Research:\n\nFaster than probability sampling\nCheaper (no callbacks for specific individuals)\nEnsures demographic representation\nNo sampling frame needed\n\nWhy It’s Problematic for Statistical Inference:\nHidden Selection Bias: Interviewers approach people who look approachable, speak the language well, aren’t in a hurry—systematically excluding certain types within each quota cell.\nExample of Bias: An interviewer filling a quota for “women 18-34” might approach women at a shopping mall on Tuesday afternoon, systematically missing:\n\nWomen who work during weekdays\nWomen who can’t afford to shop at malls\nWomen with young children who avoid malls\nWomen who shop online\n\nEven though the final sample has the “right” proportion of young women, they’re not representative of all young women.\nNo Measure of Sampling Error: Without selection probabilities, we can’t calculate standard errors or confidence intervals.\nHistorical Cautionary Tale: Quota sampling was standard in polling until the 1948 U.S. presidential election, when polls using quota sampling incorrectly predicted Dewey would defeat Truman. The failure led to adoption of probability sampling in polling.\n\n\nSnowball Sampling\nParticipants recruit additional subjects from their acquaintances. The sample grows like a rolling snowball.\nHow It Works:\n\nIdentify initial participants (seeds)\nAsk them to refer others with required characteristics\nAsk new participants for further referrals\nContinue until sample size reached or referrals exhausted\n\nExample: Studying undocumented immigrants:\n\nStart with 5 immigrants you can identify\nEach refers 3 others they know\nThose 15 each refer 2-3 others\nContinue until you have 100+ participants\n\nWhen It’s Valuable:\nHidden Populations: Groups without sampling frames\n\nDrug users\nHomeless individuals\nPeople with rare diseases\nMembers of underground movements\n\nSocially Connected Populations: When relationships matter\n\nStudying social network effects\nResearching community transmission of diseases\nUnderstanding information diffusion\n\nTrust-Dependent Research: When referrals increase participation\n\nSensitive topics where trust is essential\nClosed communities suspicious of outsiders\n\nMajor Limitations:\n\nSamples biased toward cooperative, well-connected individuals\nIsolated members of population missed entirely\nStatistical inference generally impossible\nCan reinforce social divisions (chains rarely cross social boundaries)\n\nAdvanced Version - Respondent-Driven Sampling (RDS):\nAttempts to make snowball sampling more rigorous by:\n\nTracking who recruited whom\nLimiting number of referrals per person\nWeighting based on network size\nUsing mathematical models to adjust for bias\n\nStill controversial whether RDS truly allows valid inference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#probability-concepts-for-statistical-analysis",
    "href": "chapter1.html#probability-concepts-for-statistical-analysis",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.12 Probability Concepts for Statistical Analysis",
    "text": "1.12 Probability Concepts for Statistical Analysis\nWhile this is primarily a statistics course, understanding basic probability is essential for statistical inference.\n\nBasic Probability\nProbability quantifies uncertainty on a scale from 0 (impossible) to 1 (certain).\nClassical Probability: P(\\text{event}) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total possible outcomes}}\nExample: Probability a randomly selected person is female \\approx 0.5\nEmpirical Probability: Based on observed frequencies\nExample: In a village, 423 of 1,000 residents are female, so P(\\text{female}) \\approx 0.423\n\n\nConditional Probability\nConditional Probability is the probability of event A given that event B has occurred: P(A|B)\nDemographic Example: Probability of dying within a year given current age:\n\nP(\\text{death within year} | \\text{age 30}) \\approx 0.001\nP(\\text{death within year} | \\text{age 80}) \\approx 0.05\n\nThese conditional probabilities form the basis of life tables.\n\n\nIndependence\nEvents A and B are independent if P(A|B) = P(A).\nTesting Independence in Demographic Data:\nAre education and fertility independent?\n\nP(\\text{3+ children}) = 0.3 overall\nP(\\text{3+ children} | \\text{college degree}) = 0.15\nDifferent probabilities indicate dependence\n\n\n\nLaw of Large Numbers\nAs sample size increases, sample statistics converge to population parameters.\nDemonstration: Estimating sex ratio at birth:\n\n10 births: 7 males (70% - very unstable)\n100 births: 53 males (53% - getting closer to ~51.2%)\n1,000 births: 515 males (51.5% - quite close)\n10,000 births: 5,118 males (51.18% - very close)\n\n\n\nVisualizing the Law of Large Numbers: Coin Flips\nLet’s see this in action with coin flips. A fair coin has a 50% chance of landing heads, but individual flips are unpredictable.\n\n# Simulate coin flips and show convergence\nset.seed(42)\nn_flips &lt;- 1000\nflips &lt;- rbinom(n_flips, 1, 0.5)  # 1 = heads, 0 = tails\n\n# Calculate cumulative proportion of heads\ncumulative_prop &lt;- cumsum(flips) / seq_along(flips)\n\n# Create data frame for plotting\nlln_data &lt;- data.frame(\n  flip_number = 1:n_flips,\n  cumulative_proportion = cumulative_prop\n)\n\n# Plot the convergence\nggplot(lln_data, aes(x = flip_number, y = cumulative_proportion)) +\n  geom_line(color = \"steelblue\", alpha = 0.7) +\n  geom_hline(yintercept = 0.5, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_hline(yintercept = c(0.45, 0.55), color = \"red\", linetype = \"dotted\", alpha = 0.7) +\n  labs(\n    title = \"Law of Large Numbers: Coin Flip Proportions Converge to 0.5\",\n    x = \"Number of coin flips\",\n    y = \"Cumulative proportion of heads\",\n    caption = \"Red dashed line = true probability (0.5)\\nDotted lines = ±5% range\"\n  ) +\n  scale_y_continuous(limits = c(0.3, 0.7), breaks = seq(0.3, 0.7, 0.1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhat this shows:\n\nEarly flips show wild variation (first 10 flips might be 70% or 30% heads)\nAs we add more flips, the proportion stabilizes around 50%\nThe “noise” of individual outcomes averages out over time\n\n\n\nThe Mathematical Statement\nLet A denote an event of interest (e.g., “heads on a coin flip”, “vote for party X”, “sum of dice equals 7”). If P(A) = p and we observe n independent trials with the same distribution (i.i.d.), then the sample frequency of A:\n\\hat{p}_n = \\frac{\\text{number of occurrences of } A}{n}\nconverges to p as n increases.\n\n\nExamples in Different Contexts\nDice example: The event “sum = 7” with two dice has probability 6/36 ≈ 16.7\\%, while “sum = 4” has 3/36 ≈ 8.3\\%. Over many throws, a sum of 7 appears about twice as often as a sum of 4.\nElection polling: If population support for a party equals p, then under random sampling of size n, the observed frequency \\hat{p}_n will approach p as n grows (assuming random sampling and independence).\nQuality control: If 2% of products are defective, then in large batches, approximately 2% will be found defective (assuming independent production).\n\n\nWhy This Matters for Statistics\nBottom line: Randomness underpins statistical inference by turning uncertainty in individual outcomes into predictable distributions for estimates. The Law of Large Numbers guarantees that the “noise” of individual outcomes averages out, allowing us to:\n\nPredict long-run frequencies\nQuantify uncertainty (margins of error)\n\nDraw reliable inferences from samples\nMake probabilistic statements about populations\n\nThis principle works in surveys, experiments, and even quantum phenomena (in the frequentist interpretation).\n\n\n\nCentral Limit Theorem (CLT)\nThe Central Limit Theorem states that the distribution of sample means approaches a normal distribution as sample size increases, regardless of the shape of the original population distribution. This holds true even for highly skewed or non-normal populations.\n\nKey Insights\n\nSample Size Threshold: Sample sizes of n ≥ 30 are typically sufficient for the CLT to apply\nStandard Error: The standard deviation of sample means equals σ/√n, where σ is the population standard deviation\nStatistical Foundation: We can make inferences about population parameters using normal distribution properties, even when the underlying data is non-normal\n\n\n\nWhy This Matters in Practice\nConsider income data, which is typically right-skewed with a long tail of high earners. While individual incomes don’t follow a normal distribution, something remarkable happens when we repeatedly take samples and calculate their means:\nWhat “normally distributed sample means” actually means:\n\nIf you take many different groups of 30+ people and calculate each group’s average income\nThese group averages will form a bell-shaped pattern when plotted\nMost group averages will cluster near the true population mean\nThe probability of getting a group average far from the population mean becomes predictable\n\nThis predictable pattern (normal distribution) allows us to:\n\nCalculate confidence intervals using normal distribution properties\nPerform statistical hypothesis tests\nMake predictions about sample means with known probability\n\nConcrete Example: Imagine a city where individual incomes range from $20,000 to $10,000,000, heavily skewed right. If you:\n\nRandomly select 100 people and calculate their mean income: maybe $75,000\nRepeat this 1000 times (1000 different groups of 100 people)\nPlot these 1000 group means: they’ll form a bell curve centered around the true population mean\nAbout 95% of these group means will fall within a predictable range\nThis happens even though individual incomes are extremely skewed!\n\n\n\nMathematical Foundation\nFor a population with mean μ and finite variance σ²:\n\nSampling distribution of the mean: \\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n}) as n \\to \\infty\nStandard error of the mean: SE_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\nStandardized sample mean: Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1) for large n\n\n\n\nKey Takeaways\n\nUniversal Application: The CLT applies to any distribution with finite variance\nConvergence to Normality: The approximation to normal distribution improves as sample size increases\nFoundation for Inference: Most parametric statistical tests rely on the CLT\nSample Size Considerations: While n ≥ 30 is a common guideline, highly skewed distributions may require larger samples for accurate approximation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#statistical-significance-a-quick-start-guide",
    "href": "chapter1.html#statistical-significance-a-quick-start-guide",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.13 Statistical Significance: A Quick Start Guide",
    "text": "1.13 Statistical Significance: A Quick Start Guide\nImagine you flip a coin 10 times and get 8 heads. Is the coin biased, or did you just get lucky? This is the core question statistical significance (statistical inference) helps us answer.\nStatistical significance tells us whether patterns in our data likely reflect something real or could have happened by pure chance.\nStatistical significance is a measure (p-value) of how confident we can be that patterns observed in our sample are not due to chance alone. When a result is statistically significant (typically p-value &lt; 0.05), it means the probability of obtaining such data in the absence of a real effect is very low.\n\nThe Courtroom Analogy\nStatistical hypothesis testing works like a criminal trial:\n\nNull Hypothesis (H_0): The defendant is innocent (no effect exists)\nAlternative Hypothesis (H_1): The defendant is guilty (an effect exists)\nThe Evidence: Your data and test results\nThe Verdict: “Guilty” (reject H_0) or “Not Guilty” (fail to reject H_0)\n\nCrucial distinction: “Not guilty” ≠ “Innocent”\n\nA “not guilty” verdict means insufficient evidence to convict\nSimilarly, “not statistically significant” means insufficient evidence for an effect, NOT proof of no effect\n\n\n\nStart with Skepticism (Presumption of Innocence)\nIn statistics, we always start by assuming nothing special is happening:\n\nNull Hypothesis (H_0): “There’s no effect”\n\nThe coin is fair\nThe new drug doesn’t work\nStudy time doesn’t affect grades\n\nAlternative Hypothesis (H_1): “There IS an effect”\n\nThe coin is biased\nThe drug works\nMore study time improves grades\n\n\nKey principle: We maintain the null hypothesis (innocence) unless our data provides strong evidence against it—“beyond a reasonable doubt” in legal terms, or “p &lt; 0.05” in statistical terms.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-p-value-your-surprise-meter",
    "href": "chapter1.html#the-p-value-your-surprise-meter",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.14 The p-value: Your “Surprise Meter”",
    "text": "1.14 The p-value: Your “Surprise Meter”\nThe p-value answers one specific question:\n\n“If nothing special were happening (null hypothesis is true), how surprising would our results be?”\nA p-value is the probability of observing the study’s results, or more extreme results, if the null hypothesis (a statement of no effect or no difference) is true.\n\n\nThree Ways to Think About p-values\n\n1. The Surprise Scale\n\np &lt; 0.01: Very surprising! (Strong evidence against H_0)\np &lt; 0.05: Pretty surprising (Moderate evidence against H_0)\np &gt; 0.05: Not that surprising (Insufficient evidence against H_0)\n\n\n\n2. Concrete Example: The Suspicious Coin\nYou flip a coin 10 times and get 8 heads. What’s the p-value?\nThe calculation: If the coin were fair, the probability of getting 8 or more heads is: p = P(≥8 \\text{ heads in 10 flips}) \\approx 0.055 \\approx 5.5\\%\nP(X \\geq 8) = \\sum_{k=8}^{10} \\binom{10}{k} 0,5^{10} = \\frac{56}{1024} \\approx 0,0547\nInterpretation: There’s a 5.5% chance of getting results this extreme with a fair coin. That’s somewhat unusual but not shocking.\n\n\n3. The Formal Definition\nA p-value is the probability of getting results at least as extreme as what you observed, assuming the null hypothesis is true.\n\n\n\n\n\n\nWarning\n\n\n\nCommon Mistake: The p-value is NOT the probability that the null hypothesis is true! It assumes the null is true and tells you how unusual your data would be in that world.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-prosecutor-fallacy-a-warning",
    "href": "chapter1.html#the-prosecutor-fallacy-a-warning",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.15 The Prosecutor Fallacy: A Warning",
    "text": "1.15 The Prosecutor Fallacy: A Warning\nI can see why the example might be challenging for beginners! Here’s a revised version that builds up the intuition more gradually without requiring knowledge of Bayes theorem or significance levels:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-prosecutor-fallacy-a-warning-1",
    "href": "chapter1.html#the-prosecutor-fallacy-a-warning-1",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.16 The Prosecutor Fallacy: A Warning",
    "text": "1.16 The Prosecutor Fallacy: A Warning\n\nThe Fallacy Explained\nImagine this courtroom scenario:\nProsecutor: “If the defendant were innocent, there’s only a 1% chance we’d find his DNA at the crime scene. We found his DNA. Therefore, there’s a 99% chance he’s guilty!”\nThis is WRONG! The prosecutor confused:\n\nP(Evidence | Innocent) = 0.01 ← What we know\nP(Innocent | Evidence) = ? ← What we want to know (but can’t get from the p-value alone!)\n\n\nWhen we get p = 0.01, it’s tempting to think:\n❌ WRONG: “There’s only a 1% chance the null hypothesis is true”\n❌ WRONG: “There’s a 99% chance our treatment works”\n✅ CORRECT: “If the null hypothesis were true, there’s only a 1% chance we’d see data this extreme”\n\n\nWhy This Matters: A Simple Medical Testing Example\nImagine a rare disease test that’s 99% accurate:\n\nIf you have the disease, the test is positive 99% of the time\nIf you don’t have the disease, the test is negative 99% of the time (so 1% false positive rate)\n\nHere’s the key: Suppose only 1 in 1000 people actually have this disease.\nNow let’s test 10,000 people:\n\n10 people have the disease → 10 test positive (rounded)\n9,990 people don’t have the disease → about 100 test positive by mistake (1% of 9,990)\nTotal positive tests: 110\n\nIf you test positive, what’s the chance you actually have the disease?\n\nOnly 10 out of 110 positive tests are real\nThat’s about 9%, not 99%!\n\n\n\nThe Research Analogy\nThe same thing happens in research:\n\nWhen we test many hypotheses (like testing many potential drugs)\nMost don’t work (like most people don’t have the rare disease)\nEven with “significant” results (like a positive test), most findings might be false positives\n\n\n\n\n\n\n\nImportant\n\n\n\nA p-value tells you how surprising your data would be IF the null hypothesis were true. It doesn’t tell you the probability that the null hypothesis IS true.\nThink of it like this: The probability of the ground being wet IF it rained is very different from the probability it rained IF the ground is wet—the ground could be wet from a sprinkler!\n\nRemember: A p-value tells you P(Data | Null is true), not P(Null is true | Data). These are as different as P(Wet ground | Rain) and P(Rain | Wet ground)—the ground could be wet from a sprinkler!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#introduction-to-regression-analysis-modeling-relationships-between-variables",
    "href": "chapter1.html#introduction-to-regression-analysis-modeling-relationships-between-variables",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.17 Introduction to Regression Analysis: Modeling Relationships Between Variables",
    "text": "1.17 Introduction to Regression Analysis: Modeling Relationships Between Variables\nBefore we begin our discussion of regression analysis, we need to understand what we mean by a model in scientific inquiry. A model is a simplified, abstract representation of a real-world phenomenon or system. Models deliberately omit details to focus on the essential relationships we are trying to understand. They are not meant to capture every aspect of reality—which would be impossibly complex—but rather to serve as tools that help us identify patterns, make predictions, test hypotheses, and communicate our ideas clearly. The statistician George Box captured this idea perfectly when he noted that “all models are wrong, but some are useful.” In other words, while we know our models don’t perfectly represent reality, they can still provide valuable insights into the phenomena we study.\nRegression analysis is a fundamental statistical method for modeling the relationship between variables. Specifically, it helps us understand how one or more independent variables (also called predictors or explanatory variables) are related to a dependent variable (the outcome or response variable we want to explain or predict). The goal of regression analysis is to quantify these relationships and, when appropriate, to predict values of the dependent variable based on the independent variables.\nIn its simplest form, called simple linear regression, we model the relationship between a single independent variable X and a dependent variable Y using the equation:\nY = \\beta_0 + \\beta_1 X + \\varepsilon\nwhere \\beta_0 represents the intercept, \\beta_1 represents the slope (showing how much Y changes for each unit change in X), and \\varepsilon represents the error term—the part of Y that our model cannot explain.\n\nOne of the most powerful tools in statistical analysis is regression analysis—a method for understanding and quantifying relationships between variables.\nThe core idea is simple: How does one thing relate to another, and can we use that relationship to make predictions?\n\nThe One-Sentence Summary: Regression helps us understand how things relate to each other in a messy, complicated world where everything affects everything else.\n\n\nWhat is Regression Analysis?\nImagine you’re curious about the relationship between education and income. You notice that people with more education tend to earn more money, but you want to understand this relationship more precisely:\n\nHow much does each additional year of education increase income, on average?\nHow strong is this relationship?\nAre there other factors we should consider?\nCan we predict someone’s likely income if we know their education level?\n\nRegression analysis provides systematic answers to these questions. It’s like finding the “best-fitting story” that describes how variables relate to each other.\n\n\nVariables and Variation\nA variable is any characteristic that can take different values across units of observation. In political science:\n\nUnits of analysis: Countries, individuals, elections, policies, years\nVariables: GDP, voting preference, democracy score, conflict occurrence\n\n\n💡 In Plain English: A variable is anything that changes. If everyone voted the same way, “voting preference” wouldn’t be a variable—it would be a constant. We study variables because we want to understand why things differ.\n\n\n\n\n\n\n\n\nNote\n\n\n\nConsider a typical pre-election news headline: “Candidate Smith’s approval rating reaches 68%.” Your immediate inference likely suggests favorable electoral prospects for Smith—not guaranteed victory, but a strong position. You naturally understand that higher approval ratings tend to predict better electoral performance, even though the relationship is not perfect.\nThis intuitive assessment exemplifies the core logic of regression analysis. You used one piece of information (approval rating) to make a prediction about another outcome (electoral success). Moreover, you recognized both the relationship between these variables and the uncertainty inherent in your prediction.\nWhile such informal reasoning serves us well in daily life, it has important limitations. How much better are Smith’s chances at 68% approval compared to 58%? What happens when we need to consider multiple factors simultaneously—approval ratings, economic conditions, and incumbency status? How confident should we be in our predictions?\nRegression analysis provides a systematic framework for addressing these questions. It transforms our intuitive understanding of relationships into precise mathematical models that can be tested and refined. Through regression analysis, researchers can:\n\nGenerate precise predictions: Move beyond general assessments to specific numerical estimates—for instance, predicting not just that Smith will “probably win,” but estimating the expected vote share and range of likely outcomes.\nIdentify which factors matter most: Determine the relative importance of different variables—perhaps discovering that economic conditions influence elections more strongly than approval ratings.\nQuantify uncertainty in predictions: Explicitly measure how confident we should be in our predictions, distinguishing between near-certain outcomes and educated guesses.\nTest theoretical propositions with empirical data: Evaluate whether our beliefs about cause-and-effect relationships hold up when examined systematically across many observations.\n\n\nIn essence, regression analysis systematizes the pattern recognition we perform intuitively, providing tools to make our predictions more accurate, our comparisons more meaningful, and our conclusions more reliable.\n\n\n\nThe Fundamental Model\nA model represents an object, person, or system in an informative way. Models divide into physical representations (such as architectural models) and abstract representations (such as mathematical equations describing atmospheric dynamics).\nThe core of statistical thinking can be expressed as:\nY = f(X) + \\text{error}\nThis equation states that our outcome (Y) equals some function of our predictors (X), plus unpredictable variation.\nComponents:\n\nY = Dependent variable (the phenomenon we seek to explain)\nX = Independent variable(s) (explanatory factors)\nf() = The functional relationship (often assumed linear)\nerror (\\epsilon) = Unexplained variation\n\n\n💡 What This Really Means: Think of it like a recipe. Your grade in a class (Y) depends on study hours (X), but not perfectly. Two students studying 10 hours might get different grades because of test anxiety, prior knowledge, or just luck (the error term). Regression finds the average relationship.\n\nThis model provides the foundation for all statistical analysis—from simple correlations to complex machine learning algorithms.\nRegression helps answer fundamental questions such as:\n\nHow much does education increase political participation?\nWhat factors predict electoral success?\nDo democratic institutions promote economic growth?\n\n\n\n\n\n\nThe Basic Idea: Drawing the Best Line Through Points\n\nSimple Linear Regression\nLet’s start with the simplest case: the relationship between two variables. Suppose we plot education (years of schooling) on the x-axis and annual income on the y-axis for 100 people. We’d see a cloud of points, and regression finds the straight line that best represents the pattern in these points.\nWhat makes a line “best”? The regression line minimizes the total squared vertical distances from all points to the line. Think of it as finding the line that makes the smallest total prediction error.\nThe equation of this line is: Y = a + bX + \\text{error}\nOr in our example: \\text{Income} = a + b \\times \\text{Education} + \\text{error}\nWhere:\n\na (intercept) = predicted income with zero education\nb (slope) = change in income per additional year of education\nerror (e) = difference between actual and predicted income\n\nInterpreting the Results:\nIf our analysis finds: \\text{Income} = 15,000 + 4,000 \\times \\text{Education}\nThis tells us:\n\nSomeone with 0 years of education is predicted to earn $15,000\nEach additional year of education is associated with $4,000 more income\nSomeone with 12 years of education is predicted to earn: $15,000 + (4,000 ) = $63,000\nSomeone with 16 years (bachelor’s degree) is predicted to earn: $15,000 + (4,000 ) = $79,000\n\n\n\n\nUnderstanding Relationships vs. Proving Causation\nA crucial distinction: regression shows association, not necessarily causation. Our education-income regression shows they’re related, but doesn’t prove education causes higher income. Other explanations are possible:\n\nReverse causation: Maybe wealthier families can afford more education for their children\nCommon cause: Perhaps intelligence or motivation affects both education and income\nCoincidence: In small samples, patterns can appear by chance\n\nExample of Spurious Correlation: A regression might show that ice cream sales strongly predict drowning deaths. Does ice cream cause drowning? No! Both increase in summer (the common cause, confounding variable).\n\n\n\nMultiple Regression: Controlling for Other Factors\nReal life is complicated—many factors influence outcomes simultaneously. Multiple regression lets us examine one relationship while “controlling for” or “holding constant” other variables.\n\nThe Power of Statistical Control\nReturning to education and income, we might wonder: Is the education effect just because educated people tend to be from wealthier families, or live in cities? Multiple regression can separate these effects:\n\\text{Income} = a + b_1 \\times \\text{Education} + b_2 \\times \\text{Age} + b_3 \\times \\text{Urban} + b_4 \\times \\text{Parent Income} + \\text{error}\nNow b_1 represents the education effect after accounting for age, location, and family background. If b_1 = 3,000, it means: “Comparing people of the same age, location, and family background, each additional year of education is associated with $3,000 more income.”\nDemographic Example: Fertility and Women’s Education\nResearchers studying fertility might find: \\text{Children} = 4.5 - 0.3 \\times \\text{Education}\nThis suggests each year of women’s education is associated with 0.3 fewer children. But is education the cause, or are educated women different in other ways? Adding controls:\n\\text{Children} = a - 0.15 \\times \\text{Education} - 0.2 \\times \\text{Urban} + 0.1 \\times \\text{Husband Education} - 0.4 \\times \\text{Contraceptive Access}\nNow we see education’s association is weaker (-0.15 instead of -0.3) after accounting for urban residence and contraceptive access. This suggests part of education’s apparent effect operates through these other pathways.\n\n\n\nTypes of Variables in Regression\n\nOutcome (Dependent) Variable\nThis is what we’re trying to understand or predict:\n\nIncome in our first example\nNumber of children in our fertility example\nLife expectancy in health studies\nMigration probability in population studies\n\n\n\nPredictor (Independent) Variables\nThese are factors we think might influence the outcome:\n\nQuantitative: Age, years of education, income, distance\nQualitative (categorical): Gender, race, marital status, region\nBinary (Dummy): Urban/rural, employed/unemployed, married/unmarried\n\nHandling Categorical Variables: We can’t directly put “religion” into an equation. Instead, we create binary variables:\n\nChristian = 1 if Christian, 0 otherwise\nMuslim = 1 if Muslim, 0 otherwise\nHindu = 1 if Hindu, 0 otherwise\n(One category becomes the reference group)\n\n\n\n\nDifferent Types of Regression for Different Outcomes\nThe basic regression idea adapts to many situations:\n\nLinear Regression\nFor continuous outcomes (income, height, blood pressure): Y = a + b_1X_1 + b_2X_2 + … + \\text{error}\n\n\nLogistic Regression\nFor binary outcomes (died/survived, migrated/stayed, married/unmarried):\nInstead of predicting the outcome directly, we predict the probability: \\log\\left(\\frac{p}{1-p}\\right) = a + b_1X_1 + b_2X_2 + …\nWhere p is the probability of the event occurring.\nExample: Predicting migration probability based on age, education, and marital status. The model might find young, educated, unmarried people have 40% probability of migrating, while older, less educated, married people have only 5% probability.\n\n\nPoisson Regression\nFor count outcomes (number of children, number of doctor visits): \\log(\\text{expected count}) = a + b_1X_1 + b_2X_2 + …\nExample: Modeling number of children based on women’s characteristics. Useful because it ensures predictions are never negative (can’t have -0.5 children!).\n\n\nSurvival (Cox model)/Hazard Regression\nWhat it’s for: Predicting when something will happen, not just if it will happen.\nThe challenge: Imagine you’re studying how long marriages last. You follow 1,000 couples for 10 years, but by the end of your study:\n\n400 couples divorced (you know exactly when)\n600 couples are still married (you don’t know if/when they’ll divorce)\n\nRegular regression can’t handle this “incomplete story” problem—those 600 ongoing marriages contain valuable information, but we don’t know their endpoints yet.\nHow Cox models help: Instead of trying to predict the exact timing, they focus on relative risk—who’s more likely to experience the event sooner. Think of it like asking “At any given moment, who’s at higher risk?” rather than “Exactly when will this happen?”\nReal-world applications:\n\nMedical research: Who responds to treatment faster?\nBusiness: Which customers cancel subscriptions sooner?\nSocial science: What factors make life events happen earlier/later?\n\n\n\n\n\nInterpreting Regression Results\n\nCoefficients\nThe coefficient tells us the expected change in outcome for a one-unit increase in the predictor, holding other variables constant.\nExamples of Interpretation:\nLinear regression for income:\n\n“Each additional year of education is associated with $3,500 higher annual income, controlling for age and experience”\n\nLogistic regression for infant mortality:\n\n“Each additional prenatal visit is associated with 15% lower odds of infant death, controlling for mother’s age and education”\n\nMultiple regression for life expectancy:\n\n“Each $1,000 increase in per-capita GDP is associated with 0.4 years longer life expectancy, after controlling for education and healthcare access”\n\n\n\nStatistical Significance\nThe regression also tests whether relationships could be due to chance:\n\np-value &lt; 0.05: Relationship unlikely due to chance (statistically significant)\np-value &gt; 0.05: Relationship could plausibly be random variation\n\n\nBut remember: Statistical significance ≠ practical importance. With large samples, tiny effects become “significant.”\n\n\n\nConfidence Intervals for Coefficients\nJust as we have confidence intervals for means or proportions, we have them for regression coefficients:\n“The effect of education on income is $3,500 per year, 95% CI: [$2,800, $4,200]”\nThis means we’re 95% confident the true effect is between $2,800 and $4,200.\n\n\nR-squared: How Well Does the Model Fit?\nR^2 (R-squared) measures the proportion of variation in the outcome explained by the predictors:\n\nR^2 = 0: Predictors explain nothing\nR^2 = 1: Predictors explain everything\nR^2 = 0.3: Predictors explain 30% of variation\n\nExample: A model of income with only education might have R^2 = 0.15 (education explains 15% of income variation). Adding age, experience, and location might increase R^2 to 0.35 (together they explain 35%).\n\n\n\n\n\n\nAssumptions and Limitations\n\n\n\nRegression makes assumptions that may not hold:\n\nExogeneity (No Hidden Relationships)\nThe most fundamental assumption: predictors must not be correlated with errors. In simple terms, there shouldn’t be hidden factors that affect both your predictors and outcome.\nExample: If studying education’s effect on income but omitting “ability,” your results are biased - ability affects both education level and income. This assumption is written as: E[\\varepsilon | X] = 0\nWhy it matters: Without it, all your coefficients are wrong, even with millions of observations!\n\n\nLinearity\nAssumes straight-line relationships. But what if education’s effect on income is stronger at higher levels? We can add polynomial terms: \\text{Income} = a + b_1 \\times \\text{Education} + b_2 \\times \\text{Education}^2\n\n\nIndependence\nAssumes observations are independent. But family members might be similar, repeated measures on the same person are related, and neighbors might influence each other. Special methods handle these dependencies.\n\n\nHomoscedasticity\nAssumes error variance is constant. But prediction errors might be larger for high-income people than low-income people. Diagnostic plots help detect this.\n\n\nNormality\nAssumes errors follow normal distribution. Important for small samples and hypothesis tests, less critical for large samples.\nNote: The first assumption (exogeneity) is about getting the right answer. The others are mostly about precision and statistical inference. Violating exogeneity means your model is fundamentally wrong; violating the others means your confidence intervals and p-values might be off.\n\n\n\n\n\n\n\n\n\nCommon Statistical Pitfalls\n\n\n\n\nEndogeneity (omitted variable bias): Forgetting about hidden factors that affect both X and Y, violating the fundamental exogeneity assumption. Example: Studying education→income without accounting for ability.\nSimultaneity/Reverse causality: When X and Y determine each other at the same time. Simple regression assumes one-way causation, but reality is often bidirectional. Example: Price affects demand AND demand affects price simultaneously.\nConfounding: Failing to account for variables that affect both predictor and outcome, leading to spurious relationships. Example: Ice cream sales correlate with drownings (both caused by summer).\nSelection bias: Non-random samples that systematically exclude certain groups, making results ungeneralizable. Example: Surveying only smartphone users about internet usage.\nEcological fallacy: Assuming group-level patterns apply to individuals. Example: Rich countries have lower birth rates ≠ rich people have fewer children.\nP-hacking (data dredging): Testing multiple hypotheses until finding significance, or tweaking analysis until p &lt; 0.05. With 20 tests, you expect 1 false positive by chance alone!\nOverfitting: Building a model too complex for your data - perfect on training data, useless for prediction. Remember: With enough parameters, you can fit an elephant.\nSurvivorship bias: Analyzing only “survivors” while ignoring failures. Example: Studying successful companies while ignoring those that went bankrupt.\nOvergeneralization: Extending findings beyond the studied population, time period, or context. Example: Results from US college students ≠ universal human behavior.\n\nRemember: The first three are forms of endogeneity - they violate E[\\varepsilon|X]=0 and make your coefficients fundamentally wrong. The others make results misleading or non-representative.\n\n\n\n\n\n\nApplications in Demography\n\nFertility Analysis\nUnderstanding what factors influence fertility decisions: \\text{Children} = f(\\text{Education, Income, Urban, Religion, Contraception, …})\nHelps identify policy levers for countries concerned about high or low fertility.\nPolicy levers are the tools and methods that governments and organizations use to influence events and achieve specific goals by affecting behavior and outcomes.\n\n\nMortality Modeling\nPredicting life expectancy or mortality risk: \\text{Mortality Risk} = f(\\text{Age, Sex, Smoking, Education, Healthcare Access, …})\nUsed by insurance companies, public health officials, and researchers.\n\n\nMigration Prediction\nUnderstanding who migrates and why: P(\\text{Migration}) = f(\\text{Age, Education, Employment, Family Ties, Distance, …})\nHelps predict population flows and plan for demographic change.\n\n\nMarriage and Divorce\nAnalyzing union formation and dissolution: P(\\text{Divorce}) = f(\\text{Age at Marriage, Education Match, Income, Children, Duration, …})\nInforms social policy and support services.\n\n\n\nCommon Pitfalls and How to Avoid Them\n\nOverfitting\nIncluding too many predictors can make the model fit perfectly in your sample but fail with new data. Like memorizing exam answers instead of understanding concepts.\nSolution: Use simpler models, cross-validation, or reserve some data for testing.\n\n\nMulticollinearity\nWhen predictors are highly correlated (e.g., years of education and degree level), the model can’t separate their effects.\nSolution: Choose one variable or combine them into an index.\n\n\nOmitted Variable Bias\nLeaving out important variables can make other effects appear stronger or weaker than they really are.\nExample: The relationship between ice cream sales and crime rates disappears when you control for temperature.\n\n\nExtrapolation\nUsing the model outside the range of observed data.\nExample: If your data includes education from 0-20 years, don’t predict income for someone with 30 years of education.\n\n\n\nMaking Regression Intuitive\nThink of regression as a sophisticated averaging technique:\n\nSimple average: “The average income is $50,000”\nConditional average: “The average income for college graduates is $70,000”\nRegression: “The average income for 35-year-old college graduates in urban areas is $78,000”\n\nEach added variable makes our prediction more specific and (hopefully) more accurate.\n\n\nRegression in Practice: A Complete Example\nResearch Question: What factors influence age at first birth?\nData: Survey of 1,000 women who have had at least one child\nVariables:\n\nOutcome: Age at first birth (years)\nPredictors: Education (years), Urban (0/1), Income (thousands), Religious (0/1)\n\nSimple Regression Result: \\text{Age at First Birth} = 18 + 0.8 \\times \\text{Education}\nInterpretation: Each year of education associated with 0.8 years later first birth.\nMultiple Regression Result: \\text{Age at First Birth} = 16 + 0.5 \\times \\text{Education} + 2 \\times \\text{Urban} + 0.03 \\times \\text{Income} - 1.5 \\times \\text{Religious}\nInterpretation:\n\nEducation effect reduced but still positive (0.5 years per education year)\nUrban women have first births 2 years later\nEach $1,000 income associated with 0.03 years (11 days) later\nReligious women have first births 1.5 years earlier\nR^2 = 0.42 (model explains 42% of variation)\n\nThis richer model helps us understand that education’s effect partly operates through urban residence and income.\n\n\n\n\n\n\nWarning\n\n\n\nRegression is a gateway to advanced statistical modeling. Once you understand the basic concept—using variables to predict outcomes and quantifying relationships—you can explore:\n\nInteraction effects: When one variable’s effect depends on another\nNon-linear relationships: Curves, thresholds, and complex patterns\nMultilevel models: Accounting for grouped data (students in schools, people in neighborhoods)\nTime series regression: Analyzing change over time\nMachine learning extensions: Random forests, neural networks, and more\n\nThe key insight remains: We’re trying to understand how things relate to each other in a systematic, quantifiable way.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#data-quality-and-sources",
    "href": "chapter1.html#data-quality-and-sources",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.18 Data Quality and Sources",
    "text": "1.18 Data Quality and Sources\nNo analysis is better than the data it’s based on. Understanding data quality issues is crucial for demographic and social research.\n\nDimensions of Data Quality\nAccuracy: How close are measurements to true values?\nExample: Age reporting often shows “heaping” at round numbers (30, 40, 50) because people round their ages.\nCompleteness: What proportion of the population is covered?\nExample: Birth registration completeness varies widely:\n\nDeveloped countries: &gt;99%\nSome developing countries: &lt;50%\n\nTimeliness: How current is the data?\nExample: Census conducted every 10 years becomes increasingly outdated, especially in rapidly changing areas.\nConsistency: Are definitions and methods stable over time and space?\nExample: Definition of “urban” varies by country, making international comparisons difficult.\nAccessibility: Can researchers and policy makers actually use the data?\n\n\nCommon Data Sources in Demography\nCensus: Complete enumeration of population\nAdvantages:\n\nComplete coverage (in theory)\nSmall area data available\nBaseline for other estimates\n\nDisadvantages:\n\nExpensive and infrequent\nSome populations hard to count\nLimited variables collected\n\nSample Surveys: Detailed data from population subset\nExamples:\n\nDemographic and Health Surveys (DHS)\nAmerican Community Survey (ACS)\nLabour Force Surveys\n\nAdvantages:\n\nCan collect detailed information\nMore frequent than census\nCan focus on specific topics\n\nDisadvantages:\n\nSampling error present\nSmall areas not represented\nResponse burden may reduce quality\n\nAdministrative Records: Data collected for non-statistical purposes\nExamples:\n\nTax records\nSchool enrollment\nHealth insurance claims\nMobile phone data\n\nAdvantages:\n\nAlready collected (no additional burden)\nOften complete for covered population\nContinuously updated\n\nDisadvantages:\n\nCoverage may be selective\nDefinitions may not match research needs\nAccess often restricted\n\n\n\nData Quality Issues Specific to Demography\nAge Heaping: Tendency to report ages ending in 0 or 5\nDetection: Calculate Whipple’s Index or Myers’ Index\nImpact: Affects age-specific rates and projections\nDigit Preference: Reporting certain final digits more than others\nExample: Birth weights often reported as 3,000g, 3,500g rather than precise values\nRecall Bias: Difficulty remembering past events accurately\nExample: “How many times did you visit a doctor last year?” Often underreported for frequent visitors, overreported for rare visitors.\nProxy Reporting: Information provided by someone else\nChallenge: Household head reporting for all members may not know everyone’s exact age or education",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#ethical-considerations-in-statistical-demographics",
    "href": "chapter1.html#ethical-considerations-in-statistical-demographics",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.19 Ethical Considerations in Statistical Demographics",
    "text": "1.19 Ethical Considerations in Statistical Demographics\nStatistics isn’t just about numbers—it involves real people and has real consequences.\n\nInformed Consent\nParticipants should understand:\n\nPurpose of data collection\nHow data will be used\nRisks and benefits\nTheir right to refuse or withdraw\n\nChallenge in Demographics: Census participation is often mandatory, raising ethical questions about consent.\n\n\nConfidentiality and Privacy\nStatistical Disclosure Control: Protecting individual identity in published data\nMethods include:\n\nSuppressing small cells (e.g., “&lt;5” instead of “2”)\nGeographic aggregation\n\nExample: In a table of occupation by age by sex for a small town, there might be only one female doctor aged 60-65, making her identifiable.\n\n\nRepresentation and Fairness\nWho’s Counted?: Decisions about who to include affect representation\n\nPrisoners: Where are they counted—prison location or home address?\nHomeless: How to ensure coverage?\nUndocumented immigrants: Include or exclude?\n\nDifferential Privacy: Mathematical framework for privacy protection while maintaining statistical utility\nTrade-off: More privacy protection = less accurate statistics\n\n\nMisuse of Statistics\nCherry-Picking: Selecting only favorable results\nExample: Reporting decline in teen pregnancy from peak year rather than showing full trend\nP-Hacking: Manipulating analysis to achieve statistical significance\nEcological Fallacy: Inferring individual relationships from group data\nExample: Counties with more immigrants have higher average incomes ≠ immigrants have higher incomes\n\n\nResponsible Reporting\nUncertainty Communication: Always report confidence intervals or margins of error\nContext Provision: Include relevant comparison groups and historical trends\nLimitation Acknowledgment: Clearly state what data can and cannot show",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#common-misconceptions-in-statistics",
    "href": "chapter1.html#common-misconceptions-in-statistics",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.20 Common Misconceptions in Statistics",
    "text": "1.20 Common Misconceptions in Statistics\nUnderstanding what statistics is NOT is as important as understanding what it is.\n\nMisconception 1: “Statistics Can Prove Anything”\nReality: Statistics can only provide evidence, never absolute proof. And proper statistics, honestly applied, constrains conclusions significantly.\nExample: A study finds correlation between ice cream sales and drowning deaths. Statistics doesn’t “prove” ice cream causes drowning—both are related to summer weather.\n\n\nMisconception 2: “Larger Samples Are Always Better”\nReality: Beyond a certain point, larger samples add little precision but may add bias.\nExample: Online survey with 1 million responses may be less accurate than probability sample of 1,000 due to self-selection bias.\nDiminishing Returns:\n\nn = 100: Margin of error \\approx 10 pp.\nn = 1,000: Margin of error \\approx 3.2 pp.\nn = 10,000: Margin of error \\approx 1 pp.\nn = 100,000: Margin of error \\approx 0.32 pp.\n\nThe jump from 10,000 to 100,000 barely improves precision but costs 10\\times more.\n\n\nMisconception 3: “Statistical Significance = Practical Importance”\nReality: With large samples, tiny differences become “statistically significant” even if meaningless.\nExample: Study of 100,000 people finds men are 0.1 cm taller on average (p &lt; 0.001). Statistically significant but practically irrelevant.\n\n\nMisconception 4: “Correlation Implies Causation”\nReality: Correlation is necessary but not sufficient for causation.\nClassic Examples:\n\nCities with more churches have more crime (both correlate with population size)\nCountries with more TV sets have longer life expectancy (both correlate with development)\n\n\n\nMisconception 5: “Random Means Haphazard”\nReality: Statistical randomness is carefully controlled and systematic.\nExample: Random sampling requires careful procedure, not just grabbing whoever is convenient.\n\n\nMisconception 6: “Average Represents Everyone”\nReality: Averages can be misleading when distributions are skewed or multimodal.\nExample: Average income of bar patrons is $50,000. Bill Gates walks in. Now average is $1 million. Nobody’s actual income changed.\n\n\nMisconception 7: “Past Patterns Guarantee Future Results”\nReality: Extrapolation assumes conditions remain constant.\nExample: Linear population growth projection from 1950-2000 would badly overestimate 2050 population because it misses fertility decline.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#applications-in-demography-1",
    "href": "chapter1.html#applications-in-demography-1",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.21 Applications in Demography",
    "text": "1.21 Applications in Demography\nThese statistical foundations enable sophisticated demographic analyses. Let’s explore key applications.\n\nPopulation Estimation and Projection\nIntercensal Estimates: Estimating population between censuses\nComponents Method: P(t+1) = P(t) + B - D + I - E\nWhere:\n\nP(t) = Population at time t\nB = Births\nD = Deaths\nI = Immigration\nE = Emigration\n\nEach component estimated from different sources with different error structures.\nPopulation Projections: Forecasting future population\nCohort Component Method:\n\nProject survival rates by age\nProject fertility rates\nProject migration rates\nApply to base population\nAggregate results\n\nUncertainty increases with projection horizon.\n\n\nDemographic Rate Calculation\nCrude Rates: Events per 1,000 population\n\\text{Crude Birth Rate} = \\frac{\\text{Births}}{\\text{Mid-year Population}} \\times 1,000\nAge-Specific Rates: Control for age structure\n\\text{Age-Specific Fertility Rate} = \\frac{\\text{Births to women aged } x}{\\text{Women aged } x} \\times 1,000\nStandardization: Compare populations with different structures\nDirect Standardization: Apply population’s rates to standard age structure Indirect Standardization: Apply standard rates to population’s age structure\n\n\nLife Table Analysis\nLife tables summarize mortality experience of a population.\nKey Columns:\n\nq_x: Probability of dying between age x and x+1\nl_x: Number surviving to age x (from 100,000 births)\nd_x: Deaths between age x and x+1\nL_x: Person-years lived between age x and x+1\ne_x: Life expectancy at age x\n\nExample Interpretation: If q_{65} = 0.015, then 1.5% of 65-year-olds die before reaching 66. If e_{65} = 18.5, then 65-year-olds average 18.5 more years of life.\n\n\nFertility Analysis\nTotal Fertility Rate (TFR): Average children per woman given current age-specific rates\n\\text{TFR} = \\sum (\\text{ASFR} \\times \\text{age interval width})\nExample: If each 5-year age group from 15-49 has ASFR = 20 per 1,000: \\text{TFR} = 7 \\text{ age groups} \\times \\frac{20}{1,000} \\times 5 \\text{ years} = 0.7 \\text{ children per woman}\nThis very low TFR indicates below-replacement fertility.\n\n\nMigration Analysis\nNet Migration Rate: \\text{NMR} = \\frac{\\text{Immigrants} - \\text{Emigrants}}{\\text{Population}} \\times 1,000\nMigration Effectiveness Index: \\text{MEI} = \\frac{|\\text{In} - \\text{Out}|}{\\text{In} + \\text{Out}}\n\nValues near 0: High turnover, little net change\nValues near 1: Mostly one-way flow\n\n\n\nPopulation Health Metrics\nDisability-Adjusted Life Years (DALYs): Years of healthy life lost\nDALY = Years of Life Lost (YLL) + Years Lived with Disability (YLD)\nHealthy Life Expectancy: Expected years in good health\nCombines mortality and morbidity information.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#software-and-tools",
    "href": "chapter1.html#software-and-tools",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.22 Software and Tools",
    "text": "1.22 Software and Tools\nModern demographic and social statistics relies heavily on computational tools.\n\nStatistical Software Packages\nR: Free, open-source, extensive demographic packages\n\nPackages: demography, popReconstruct, bayesPop\nAdvantages: Reproducible research, cutting-edge methods\nDisadvantages: Steep learning curve\n\nStata: Widely used in social sciences\n\nStrengths: Survey data analysis, panel data\nCommon in: Economics, epidemiology\n\nSPSS: User-friendly interface\n\nStrengths: Point-and-click interface\nCommon in: Social sciences, market research\n\nPython: General programming language with statistical libraries\n\nLibraries: pandas, numpy, scipy, statsmodels\nAdvantages: Integration with other applications",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#conclusion",
    "href": "chapter1.html#conclusion",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.23 Conclusion",
    "text": "1.23 Conclusion\n\nKey Terms Summary\nStatistics: The science of collecting, organizing, analyzing, interpreting, and presenting data to understand phenomena and support decision-making\nDescriptive Statistics: Methods for summarizing and presenting data in meaningful ways without extending conclusions beyond the observed data\nInferential Statistics: Techniques for drawing conclusions about populations from samples, including estimation and hypothesis testing\nPopulation: The complete set of individuals, objects, or measurements about which conclusions are to be drawn\nSample: A subset of the population that is actually observed or measured to make inferences about the population\nSuperpopulation: A theoretical infinite population from which observed finite populations are considered to be samples\nParameter: A numerical characteristic of a population (usually unknown and denoted by Greek letters)\nStatistic: A numerical characteristic calculated from sample data (known and denoted by Roman letters)\nEstimator: A rule or formula for calculating estimates of population parameters from sample data\nEstimand: The specific population parameter targeted for estimation\nEstimate: The numerical value produced by applying an estimator to observed data\nRandom Error (Sampling Error): Unpredictable variation arising from the sampling process that decreases with larger samples\nSystematic Error (Bias): Consistent deviation from true values that cannot be reduced by increasing sample size\nSampling: The process of selecting a subset of units from a population for measurement\nSampling Frame: The list or device from which a sample is drawn, ideally containing all population members\nProbability Sampling: Sampling methods where every population member has a known, non-zero probability of selection\nSimple Random Sampling: Every possible sample of size n has equal probability of selection\nSystematic Sampling: Selection of every kth element from an ordered sampling frame\nStratified Sampling: Division of population into homogeneous subgroups before sampling within each\nCluster Sampling: Selection of groups (clusters) rather than individuals\nNon-probability Sampling: Sampling methods without guaranteed known selection probabilities\nConvenience Sampling: Selection based purely on ease of access\nPurposive Sampling: Deliberate selection based on researcher judgment\nQuota Sampling: Selection to match population proportions on key characteristics without random selection\nSnowball Sampling: Participants recruit additional subjects from their acquaintances\nStandard Error: The standard deviation of the sampling distribution of a statistic\nMargin of Error: Maximum expected difference between estimate and parameter at specified confidence\nConfidence Interval: Range of plausible values for a parameter at specified confidence level\nConfidence Level: Probability that the confidence interval method produces intervals containing the parameter\nData: Collected observations or measurements\nQuantitative Data: Numerical measurements (continuous or discrete)\nQualitative Data: Categorical information (nominal or ordinal)\nData Distribution: Description of how values spread across possible outcomes\nFrequency Distribution: Summary showing how often each value occurs in data\nAbsolute Frequency: Count of observations for each value\nRelative Frequency: Proportion of observations in each category\nCumulative Frequency: Running total of frequencies up to each value",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-a-visualizations-for-statistics-demography",
    "href": "chapter1.html#appendix-a-visualizations-for-statistics-demography",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.24 Appendix A: Visualizations for Statistics & Demography",
    "text": "1.24 Appendix A: Visualizations for Statistics & Demography\n\n## ============================================\n## Visualizations for Statistics & Demography\n## Chapter 1: Foundations\n## ============================================\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(gridExtra)\nlibrary(scales)\nlibrary(patchwork)  # for combining plots\n\n# Set theme for all plots\ntheme_set(theme_minimal(base_size = 12))\n\n# Color palette for consistency\ncolors &lt;- c(\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#6A994E\")\n\n\n# ==================================================\n# 1. POPULATION vs SAMPLE VISUALIZATION\n# ==================================================\n\n# Create a population and sample visualization\nset.seed(123)\n\n# Generate population data (e.g., ages of 10,000 people)\npopulation &lt;- data.frame(\n  id = 1:10000,\n  age = round(rnorm(10000, mean = 40, sd = 15))\n)\npopulation$age[population$age &lt; 0] &lt;- 0\npopulation$age[population$age &gt; 100] &lt;- 100\n\n# Take a random sample\nsample_size &lt;- 500\nsample_data &lt;- population[sample(nrow(population), sample_size), ]\n\n# Create visualization\np1 &lt;- ggplot(population, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = colors[1], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = mean(population$age), \n             color = colors[2], linetype = \"dashed\", size = 1.2) +\n  labs(title = \"Population Distribution (N = 10,000)\",\n       subtitle = paste(\"Population mean (μ) =\", round(mean(population$age), 2), \"years\"),\n       x = \"Age (years)\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(sample_data, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = colors[3], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = mean(sample_data$age), \n             color = colors[4], linetype = \"dashed\", size = 1.2) +\n  labs(title = paste(\"Sample Distribution (n =\", sample_size, \")\"),\n       subtitle = paste(\"Sample mean (x̄) =\", round(mean(sample_data$age), 2), \"years\"),\n       x = \"Age (years)\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Combine plots\npopulation_sample_plot &lt;- p1 / p2\nprint(population_sample_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 2. TYPES OF DATA DISTRIBUTIONS\n# ==================================================\n\n# Generate different distribution types\nset.seed(456)\nn &lt;- 5000\n\n# Normal distribution\nnormal_data &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Right-skewed distribution (income-like)\nright_skewed &lt;- rgamma(n, shape = 2, scale = 15)\n\n# Left-skewed distribution (age at death in developed country)\nleft_skewed &lt;- 90 - rgamma(n, shape = 3, scale = 5)\nleft_skewed[left_skewed &lt; 0] &lt;- 0\n\n# Bimodal distribution (e.g., height of mixed male/female population)\nn2  &lt;- 20000\nnf &lt;- n2 %/% 2; nm &lt;- n2 - nf\nbimodal &lt;- c(rnorm(nf, mean = 164, sd = 5),\n             rnorm(nm, mean = 182, sd = 5))\n\n\n# Create data frame\ndistributions_df &lt;- data.frame(\n  Normal = normal_data,\n  `Right Skewed` = right_skewed,\n  `Left Skewed` = left_skewed,\n  Bimodal = bimodal\n) %&gt;%\n  pivot_longer(everything(), names_to = \"Distribution\", values_to = \"Value\")\n\n# Plot distributions\ndistributions_plot &lt;- ggplot(distributions_df, aes(x = Value, fill = Distribution)) +\n  geom_histogram(bins = 30, alpha = 0.7, color = \"white\") +\n  facet_wrap(~Distribution, scales = \"free\", nrow = 2) +\n  scale_fill_manual(values = colors[1:4]) +\n  labs(title = \"Types of Data Distributions\",\n       subtitle = \"Common patterns in demographic data\",\n       x = \"Value\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"none\")\n\nprint(distributions_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 3. NORMAL DISTRIBUTION WITH 68-95-99.7 RULE\n# ==================================================\n\n# Generate normal distribution data\nset.seed(789)\nmean_val &lt;- 100\nsd_val &lt;- 15\nx &lt;- seq(mean_val - 4*sd_val, mean_val + 4*sd_val, length.out = 1000)\ny &lt;- dnorm(x, mean = mean_val, sd = sd_val)\ndf_norm &lt;- data.frame(x = x, y = y)\n\n# Create the plot\nnormal_plot &lt;- ggplot(df_norm, aes(x = x, y = y)) +\n  # Fill areas under the curve\n  geom_area(data = subset(df_norm, x &gt;= mean_val - sd_val & x &lt;= mean_val + sd_val),\n            aes(x = x, y = y), fill = colors[1], alpha = 0.3) +\n  geom_area(data = subset(df_norm, x &gt;= mean_val - 2*sd_val & x &lt;= mean_val + 2*sd_val),\n            aes(x = x, y = y), fill = colors[2], alpha = 0.2) +\n  geom_area(data = subset(df_norm, x &gt;= mean_val - 3*sd_val & x &lt;= mean_val + 3*sd_val),\n            aes(x = x, y = y), fill = colors[3], alpha = 0.1) +\n  # Add the curve\n  geom_line(size = 1.5, color = \"black\") +\n  # Add vertical lines for standard deviations\n  geom_vline(xintercept = mean_val, linetype = \"solid\", size = 1, color = \"black\") +\n  geom_vline(xintercept = c(mean_val - sd_val, mean_val + sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[1]) +\n  geom_vline(xintercept = c(mean_val - 2*sd_val, mean_val + 2*sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[2]) +\n  geom_vline(xintercept = c(mean_val - 3*sd_val, mean_val + 3*sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[3]) +\n  # Add labels\n  annotate(\"text\", x = mean_val, y = max(y) * 0.5, label = \"68%\", \n           size = 5, fontface = \"bold\", color = colors[1]) +\n  annotate(\"text\", x = mean_val, y = max(y) * 0.3, label = \"95%\", \n           size = 5, fontface = \"bold\", color = colors[2]) +\n  annotate(\"text\", x = mean_val, y = max(y) * 0.1, label = \"99.7%\", \n           size = 5, fontface = \"bold\", color = colors[3]) +\n  # Labels\n  scale_x_continuous(breaks = c(mean_val - 3*sd_val, mean_val - 2*sd_val, \n                                mean_val - sd_val, mean_val, \n                                mean_val + sd_val, mean_val + 2*sd_val, \n                                mean_val + 3*sd_val),\n                     labels = c(\"μ-3σ\", \"μ-2σ\", \"μ-σ\", \"μ\", \"μ+σ\", \"μ+2σ\", \"μ+3σ\")) +\n  labs(title = \"Normal Distribution: The 68-95-99.7 Rule\",\n       subtitle = \"Proportion of data within standard deviations from the mean\",\n       x = \"Value\", y = \"Probability Density\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\nprint(normal_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 4. SIMPLE LINEAR REGRESSION\n# ==================================================\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(scales)\n\n# Define color palette (this was missing in original code)\ncolors &lt;- c(\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#592E83\")\n\n# Generate data for regression example (Education vs Income)\nset.seed(2024)\nn_reg &lt;- 200\neducation &lt;- round(rnorm(n_reg, mean = 14, sd = 3))\neducation[education &lt; 8] &lt;- 8\neducation[education &gt; 22] &lt;- 22\n\n# Create income with linear relationship plus noise\nincome &lt;- 15000 + 4000 * education + rnorm(n_reg, mean = 0, sd = 8000)\nincome[income &lt; 10000] &lt;- 10000\n\nreg_data &lt;- data.frame(education = education, income = income)\n\n# Fit linear model\nlm_model &lt;- lm(income ~ education, data = reg_data)\n\n# Create subset of data for residual lines\nsubset_indices &lt;- sample(nrow(reg_data), 20)\nsubset_data &lt;- reg_data[subset_indices, ]\nsubset_data$predicted &lt;- predict(lm_model, newdata = subset_data)\n\n# Create regression plot\nregression_plot &lt;- ggplot(reg_data, aes(x = education, y = income)) +\n  # Add points\n  geom_point(alpha = 0.6, size = 2, color = colors[1]) +\n  \n  # Add regression line with confidence interval\n  geom_smooth(method = \"lm\", se = TRUE, color = colors[2], fill = colors[2], alpha = 0.2) +\n  \n  # Add residual lines for a subset of points to show the concept\n  geom_segment(data = subset_data,\n               aes(x = education, xend = education, \n                   y = income, yend = predicted),\n               color = colors[4], alpha = 0.5, linetype = \"dotted\") +\n  \n  # Add equation to plot (adjusted position based on data range)\n  annotate(\"text\", x = min(reg_data$education) + 1, y = max(reg_data$income) * 0.9, \n           label = paste(\"Income = $\", format(round(coef(lm_model)[1]), big.mark = \",\"), \n                        \" + $\", format(round(coef(lm_model)[2]), big.mark = \",\"), \" × Education\",\n                        \"\\nR² = \", round(summary(lm_model)$r.squared, 3), sep = \"\"),\n           hjust = 0, size = 4, fontface = \"italic\") +\n  \n  # Labels and formatting\n  scale_y_continuous(labels = dollar_format()) +\n  labs(title = \"Simple Linear Regression: Education and Income\",\n       subtitle = \"Each year of education associated with higher income\",\n       x = \"Years of Education\", \n       y = \"Annual Income\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\nprint(regression_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 5. SAMPLING ERROR AND SAMPLE SIZE\n# ==================================================\n\n# Show how standard error decreases with sample size\nset.seed(111)\nsample_sizes &lt;- c(10, 25, 50, 100, 250, 500, 1000, 2500, 5000)\nn_simulations &lt;- 1000\n\n# True population parameters\ntrue_mean &lt;- 50\ntrue_sd &lt;- 10\n\n# Run simulations for each sample size\nse_results &lt;- data.frame()\nfor (n in sample_sizes) {\n  sample_means &lt;- replicate(n_simulations, mean(rnorm(n, true_mean, true_sd)))\n  se_results &lt;- rbind(se_results, \n                      data.frame(n = n, \n                                se_empirical = sd(sample_means),\n                                se_theoretical = true_sd / sqrt(n)))\n}\n\n# Create the plot\nse_plot &lt;- ggplot(se_results, aes(x = n)) +\n  geom_line(aes(y = se_empirical, color = \"Empirical SE\"), size = 1.5) +\n  geom_point(aes(y = se_empirical, color = \"Empirical SE\"), size = 3) +\n  geom_line(aes(y = se_theoretical, color = \"Theoretical SE\"), \n            size = 1.5, linetype = \"dashed\") +\n  scale_x_log10(breaks = sample_sizes) +\n  scale_color_manual(values = c(\"Empirical SE\" = colors[1], \n                               \"Theoretical SE\" = colors[2])) +\n  labs(title = \"Standard Error Decreases with Sample Size\",\n       subtitle = \"The precision of estimates improves with larger samples\",\n       x = \"Sample Size (log scale)\", \n       y = \"Standard Error\",\n       color = \"\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"top\")\n\nprint(se_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 6. CONFIDENCE INTERVALS VISUALIZATION\n# ==================================================\n\n# Simulate multiple samples and their confidence intervals\nset.seed(999)\nn_samples &lt;- 20\nsample_size_ci &lt;- 100\ntrue_mean_ci &lt;- 50\ntrue_sd_ci &lt;- 10\n\n# Generate samples and calculate CIs\nci_data &lt;- data.frame()\nfor (i in 1:n_samples) {\n  sample_i &lt;- rnorm(sample_size_ci, true_mean_ci, true_sd_ci)\n  mean_i &lt;- mean(sample_i)\n  se_i &lt;- sd(sample_i) / sqrt(sample_size_ci)\n  ci_lower &lt;- mean_i - 1.96 * se_i\n  ci_upper &lt;- mean_i + 1.96 * se_i\n  contains_true &lt;- (true_mean_ci &gt;= ci_lower) & (true_mean_ci &lt;= ci_upper)\n  \n  ci_data &lt;- rbind(ci_data,\n                   data.frame(sample = i, mean = mean_i, \n                             lower = ci_lower, upper = ci_upper,\n                             contains = contains_true))\n}\n\n# Create CI plot\nci_plot &lt;- ggplot(ci_data, aes(x = sample, y = mean)) +\n  geom_hline(yintercept = true_mean_ci, color = \"red\", \n             linetype = \"dashed\", size = 1) +\n  geom_errorbar(aes(ymin = lower, ymax = upper, color = contains), \n                width = 0.3, size = 0.8) +\n  geom_point(aes(color = contains), size = 2) +\n  scale_color_manual(values = c(\"TRUE\" = colors[1], \"FALSE\" = colors[4]),\n                    labels = c(\"Misses true value\", \"Contains true value\")) +\n  coord_flip() +\n  labs(title = \"95% Confidence Intervals from 20 Different Samples\",\n       subtitle = paste(\"True population mean = \", true_mean_ci, \n                       \" (red dashed line)\", sep = \"\"),\n       x = \"Sample Number\", \n       y = \"Sample Mean with 95% CI\",\n       color = \"\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"bottom\")\n\nprint(ci_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 7. SAMPLING DISTRIBUTIONS (CENTRAL LIMIT THEOREM)\n# ==================================================\n\n# ---- Setup ----\nlibrary(tidyverse)\nlibrary(ggplot2)\ntheme_set(theme_minimal(base_size = 13))\nset.seed(2025)\n\n# Skewed population (Gamma); change if you want another DGP\nNpop &lt;- 100000\npopulation &lt;- rgamma(Npop, shape = 2, scale = 10)  # skewed right\nmu    &lt;- mean(population)\nsigma &lt;- sd(population)\n\n# ---- CLT: sampling distribution of the mean ----\nsample_sizes &lt;- c(1, 5, 10, 30, 100)\nB &lt;- 2000  # resamples per n\n\nclt_df &lt;- purrr::map_dfr(sample_sizes, \\(n) {\n  tibble(n = n,\n         mean = replicate(B, mean(sample(population, n, replace = TRUE))))\n})\n\n# Normal overlays: N(mu, sigma/sqrt(n))\nclt_range &lt;- clt_df |&gt;\n  group_by(n) |&gt;\n  summarise(min_x = min(mean), max_x = max(mean), .groups = \"drop\")\n\nnormal_df &lt;- clt_range |&gt;\n  rowwise() |&gt;\n  mutate(x = list(seq(min_x, max_x, length.out = 200))) |&gt;\n  unnest(x) |&gt;\n  mutate(density = dnorm(x, mean = mu, sd = sigma / sqrt(n)))\n\nclt_plot &lt;- ggplot(clt_df, aes(mean)) +\n  geom_histogram(aes(y = after_stat(density), fill = factor(n)),\n                 bins = 30, alpha = 0.6, color = \"white\") +\n  geom_line(data = normal_df, aes(x, density), linewidth = 0.8) +\n  geom_vline(xintercept = mu, linetype = \"dashed\") +\n  facet_wrap(~ n, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"CLT: Sampling distribution of the mean → Normal(μ, σ/√n)\",\n    subtitle = sprintf(\"Skewed population: Gamma(shape=2, scale=10).  μ≈%.2f, σ≈%.2f; B=%d resamples each.\", mu, sigma, B),\n    x = \"Sample mean\", y = \"Density\"\n  ) +\n  guides(fill = \"none\")\n\nclt_plot\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 8. TYPES OF SAMPLING ERROR\n# ==================================================\n\n# Create data to show random vs systematic error\nset.seed(321)\nn_measurements &lt;- 100\ntrue_value &lt;- 50\n\n# Random error only\nrandom_error &lt;- rnorm(n_measurements, mean = true_value, sd = 5)\n\n# Systematic error (bias) only\nsystematic_error &lt;- rep(true_value + 10, n_measurements) + rnorm(n_measurements, 0, 0.5)\n\n# Both errors\nboth_errors &lt;- rnorm(n_measurements, mean = true_value + 10, sd = 5)\n\nerror_data &lt;- data.frame(\n  measurement = 1:n_measurements,\n  `Random Error Only` = random_error,\n  `Systematic Error Only` = systematic_error,\n  `Both Errors` = both_errors\n) %&gt;%\n  pivot_longer(-measurement, names_to = \"Error_Type\", values_to = \"Value\")\n\n# Create error visualization\nerror_plot &lt;- ggplot(error_data, aes(x = measurement, y = Value, color = Error_Type)) +\n  geom_hline(yintercept = true_value, linetype = \"dashed\", size = 1, color = \"black\") +\n  geom_point(alpha = 0.6, size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 1.2) +\n  facet_wrap(~Error_Type, nrow = 1) +\n  scale_color_manual(values = colors[1:3]) +\n  labs(title = \"Random Error vs Systematic Error (Bias)\",\n       subtitle = paste(\"True value = \", true_value, \" (black dashed line)\", sep = \"\"),\n       x = \"Measurement Number\", \n       y = \"Measured Value\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"none\")\n\nprint(error_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 9. DEMOGRAPHIC PYRAMID\n# ==================================================\n\n# Create age pyramid data\nset.seed(777)\nage_groups &lt;- c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \n               \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \n               \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80+\")\n\n# Create data for a developing country pattern\nmale_pop &lt;- c(12, 11.5, 11, 10.5, 10, 9.5, 9, 8.5, 8, 7.5, 7, \n             6, 5, 4, 3, 2, 1.5)\nfemale_pop &lt;- c(11.8, 11.3, 10.8, 10.3, 9.8, 9.3, 8.8, 8.3, 7.8, \n               7.3, 6.8, 5.8, 4.8, 3.8, 2.8, 2.2, 2)\n\npyramid_data &lt;- data.frame(\n  Age = factor(rep(age_groups, 2), levels = rev(age_groups)),\n  Population = c(-male_pop, female_pop),  # Negative for males\n  Sex = c(rep(\"Male\", length(male_pop)), rep(\"Female\", length(female_pop)))\n)\n\n# Create population pyramid\npyramid_plot &lt;- ggplot(pyramid_data, aes(x = Age, y = Population, fill = Sex)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  scale_y_continuous(labels = function(x) paste0(abs(x), \"%\")) +\n  scale_fill_manual(values = c(\"Male\" = colors[1], \"Female\" = colors[3])) +\n  coord_flip() +\n  labs(title = \"Population Pyramid\",\n       subtitle = \"Age and sex distribution (typical developing country pattern)\",\n       x = \"Age Group\", \n       y = \"Percentage of Population\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"top\")\n\nprint(pyramid_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 10. REGRESSION RESIDUALS AND DIAGNOSTICS\n# ==================================================\n\n# Use the previous regression model for diagnostics\nreg_diagnostics &lt;- data.frame(\n  fitted = fitted(lm_model),\n  residuals = residuals(lm_model),\n  standardized_residuals = rstandard(lm_model),\n  education = reg_data$education,\n  income = reg_data$income\n)\n\n# Create diagnostic plots\n# 1. Residuals vs Fitted\np_resid_fitted &lt;- ggplot(reg_diagnostics, aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.5, color = colors[1]) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = colors[2], size = 0.8) +\n  labs(title = \"Residuals vs Fitted Values\",\n       subtitle = \"Check for homoscedasticity\",\n       x = \"Fitted Values\", y = \"Residuals\")\n\n# 2. Q-Q plot\np_qq &lt;- ggplot(reg_diagnostics, aes(sample = standardized_residuals)) +\n  stat_qq(color = colors[1]) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Normal Q-Q Plot\",\n       subtitle = \"Check for normality of residuals\",\n       x = \"Theoretical Quantiles\", y = \"Standardized Residuals\")\n\n# 3. Histogram of residuals\np_hist_resid &lt;- ggplot(reg_diagnostics, aes(x = residuals)) +\n  geom_histogram(bins = 30, fill = colors[3], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Distribution of Residuals\",\n       subtitle = \"Should be approximately normal\",\n       x = \"Residuals\", y = \"Frequency\")\n\n# 4. Residuals vs Predictor\np_resid_x &lt;- ggplot(reg_diagnostics, aes(x = education, y = residuals)) +\n  geom_point(alpha = 0.5, color = colors[4]) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = colors[2], size = 0.8) +\n  labs(title = \"Residuals vs Predictor\",\n       subtitle = \"Check for patterns\",\n       x = \"Education (years)\", y = \"Residuals\")\n\n# Combine diagnostic plots\ndiagnostic_plots &lt;- (p_resid_fitted + p_qq) / (p_hist_resid + p_resid_x)\nprint(diagnostic_plots)\n\n\n\n\n\n\n\n# ==================================================\n# 11. SAVE ALL PLOTS (Optional)\n# ==================================================\n\n# Uncomment to save plots as high-resolution images\n# ggsave(\"population_sample.png\", population_sample_plot, width = 10, height = 8, dpi = 300)\n# ggsave(\"distributions.png\", distributions_plot, width = 12, height = 8, dpi = 300)\n# ggsave(\"normal_distribution.png\", normal_plot, width = 10, height = 6, dpi = 300)\n# ggsave(\"regression.png\", regression_plot, width = 10, height = 7, dpi = 300)\n# ggsave(\"standard_error.png\", se_plot, width = 10, height = 6, dpi = 300)\n# ggsave(\"confidence_intervals.png\", ci_plot, width = 10, height = 8, dpi = 300)\n# ggsave(\"central_limit_theorem.png\", clt_plot, width = 14, height = 5, dpi = 300)\n# ggsave(\"error_types.png\", error_plot, width = 14, height = 5, dpi = 300)\n# ggsave(\"population_pyramid.png\", pyramid_plot, width = 8, height = 8, dpi = 300)\n# ggsave(\"regression_diagnostics.png\", diagnostic_plots, width = 12, height = 10, dpi = 300)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-b-central-limit-theorem-clt",
    "href": "chapter1.html#appendix-b-central-limit-theorem-clt",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.25 Appendix B: Central Limit Theorem (CLT)",
    "text": "1.25 Appendix B: Central Limit Theorem (CLT)\nThe Central Limit Theorem states that the distribution of sample means approaches a normal distribution as sample size increases, regardless of the shape of the original population distribution.\n\nKey Insights\n\nSample Size Threshold: Sample sizes of n ≥ 30 are typically sufficient for the CLT to apply\nStandard Error: The standard deviation of sample means equals σ/√n, where σ is the population standard deviation\nStatistical Foundation: We can make inferences about population parameters using normal distribution properties",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#visual-demonstration-step-by-step-progression",
    "href": "chapter1.html#visual-demonstration-step-by-step-progression",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.26 Visual Demonstration: Step-by-Step Progression",
    "text": "1.26 Visual Demonstration: Step-by-Step Progression\nThe most effective approach to understanding CLT is to observe the systematic transformation of the distribution as the number of dice increases. Beginning with 1 die (uniform distribution), we can observe how increasing the sample size gradually transforms the distribution into a normal distribution.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n\nThe Progressive Transformation\n\n# Sample sizes to demonstrate\nsample_sizes &lt;- c(1, 2, 5, 10, 30, 50)\nnum_simulations &lt;- 10000\n\n# Simulate for each sample size\nall_data &lt;- data.frame()\n\nfor (n in sample_sizes) {\n  means &lt;- replicate(num_simulations, {\n    dice &lt;- sample(1:6, n, replace = TRUE)\n    mean(dice)\n  })\n  \n  temp_df &lt;- data.frame(\n    mean = means,\n    n = n,\n    label = paste(n, ifelse(n == 1, \"die\", \"dice\"))\n  )\n  all_data &lt;- rbind(all_data, temp_df)\n}\n\n# Create ordered factor\nall_data$label &lt;- factor(all_data$label, \n                         levels = paste(sample_sizes, \n                                       ifelse(sample_sizes == 1, \"die\", \"dice\")))\n\n# Plot the progression\nggplot(all_data, aes(x = mean)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 bins = 40, fill = \"#3b82f6\", color = \"white\", alpha = 0.7) +\n  facet_wrap(~label, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"Central Limit Theorem: Step-by-Step Progression\",\n    subtitle = sprintf(\"Each panel shows %s simulations demonstrating the convergence to normality\", \n                      format(num_simulations, big.mark = \",\")),\n    x = \"Mean Value\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    strip.text = element_text(face = \"bold\", size = 12),\n    strip.background = element_rect(fill = \"#f0f0f0\", color = NA)\n  )\n\n\n\n\n\n\n\n\n\nAnalysis of Progressive Stages:\n\n1 die: Uniform (discrete) distribution - all values 1 to 6 equally probable\n2 dice: Triangular tendency - central values more frequent\n5 dice: Emergent bell-shaped pattern - observable clustering around 3.5\n10 dice: Distinctly normal - narrow Gaussian curve forming\n30 dice: Normal distribution - practical demonstration of CLT\n50 dice: Near-ideal normal distribution - strong concentration around mean\n\nThe distribution exhibits decreasing variability and increasingly pronounced bell-shaped characteristics as n increases.\n\n\n\nComparative Analysis\nA cleaner comparison of key developmental stages:\n\nkey_sizes &lt;- all_data %&gt;%\n  filter(n %in% c(1, 2, 5, 10, 30))\n\nggplot(key_sizes, aes(x = mean)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 bins = 40, fill = \"#3b82f6\", color = \"white\", alpha = 0.7) +\n  facet_wrap(~label, scales = \"free_x\", nrow = 1) +\n  labs(\n    title = \"CLT Evolution: From Uniform to Normal\",\n    x = \"Mean Value\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    strip.text = element_text(face = \"bold\", size = 11),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\nSuperimposed Distributions\nAn alternative visualization method displaying all distributions simultaneously:\n\ncomparison_data &lt;- all_data %&gt;%\n  filter(n %in% c(1, 5, 10, 30))\n\nggplot(comparison_data, aes(x = mean, fill = label, color = label)) +\n  geom_density(alpha = 0.3, linewidth = 1.2) +\n  scale_fill_manual(values = c(\"#991b1b\", \"#ea580c\", \"#ca8a04\", \"#16a34a\")) +\n  scale_color_manual(values = c(\"#991b1b\", \"#ea580c\", \"#ca8a04\", \"#16a34a\")) +\n  labs(\n    title = \"CLT Progression: Superimposed Distributions\",\n    subtitle = \"Systematic narrowing and convergence to normal form\",\n    x = \"Mean Value\",\n    y = \"Density\",\n    fill = \"Sample Size\",\n    color = \"Sample Size\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nKey Observation: As sample size increases, the distribution exhibits:\n\nIncreased symmetry (bell-shaped form)\nGreater concentration around the population mean (3.5)\nImproved conformity to the normal distribution\n\n\n\nStandard Error Convergence\nThe dispersion (standard deviation) decreases according to the relationship SE = σ/√n:\n\nvariance_data &lt;- all_data %&gt;%\n  group_by(n, label) %&gt;%\n  summarise(\n    observed_sd = sd(mean),\n    theoretical_se = sqrt(35/12) / sqrt(n),\n    .groups = \"drop\"\n  )\n\nggplot(variance_data, aes(x = n)) +\n  geom_line(aes(y = observed_sd, color = \"Observed SD\"), \n            linewidth = 1.5) +\n  geom_point(aes(y = observed_sd, color = \"Observed SD\"), \n             size = 4) +\n  geom_line(aes(y = theoretical_se, color = \"Theoretical SE\"), \n            linewidth = 1.5, linetype = \"dashed\") +\n  geom_point(aes(y = theoretical_se, color = \"Theoretical SE\"), \n             size = 4) +\n  scale_color_manual(values = c(\"Observed SD\" = \"#3b82f6\", \n                                \"Theoretical SE\" = \"#ef4444\")) +\n  scale_x_continuous(breaks = sample_sizes) +\n  labs(\n    title = \"Standard Error Decreases as Sample Size Increases\",\n    subtitle = \"Following the SE = σ/√n relationship\",\n    x = \"Sample Size (n)\",\n    y = \"Standard Deviation / Standard Error\",\n    color = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    legend.position = \"top\",\n    legend.text = element_text(size = 11)\n  )\n\n\n\n\n\n\n\n\n\n\nNumerical Summary\n\nsummary_stats &lt;- all_data %&gt;%\n  group_by(label) %&gt;%\n  summarise(\n    n = first(n),\n    Observed_Mean = round(mean(mean), 3),\n    Observed_SD = round(sd(mean), 3),\n    Theoretical_Mean = 3.5,\n    Theoretical_SE = round(sqrt(35/12) / sqrt(first(n)), 3),\n    Range = paste0(\"[\", round(min(mean), 2), \", \", round(max(mean), 2), \"]\")\n  ) %&gt;%\n  select(-label)\n\nknitr::kable(summary_stats, \n             caption = \"Observed vs Theoretical Values Across Sample Sizes\")\n\n\nObserved vs Theoretical Values Across Sample Sizes\n\n\n\n\n\n\n\n\n\n\nn\nObserved_Mean\nObserved_SD\nTheoretical_Mean\nTheoretical_SE\nRange\n\n\n\n\n1\n3.470\n1.716\n3.5\n1.708\n[1, 6]\n\n\n2\n3.503\n1.213\n3.5\n1.208\n[1, 6]\n\n\n5\n3.494\n0.764\n3.5\n0.764\n[1, 6]\n\n\n10\n3.507\n0.537\n3.5\n0.540\n[1.7, 5.4]\n\n\n30\n3.500\n0.311\n3.5\n0.312\n[2.27, 4.63]\n\n\n50\n3.498\n0.239\n3.5\n0.242\n[2.68, 4.3]\n\n\n\n\n\nObservations:\n\nThe population mean remains constant at 3.5 (independent of sample size)\nThe standard error exhibits systematic decline as n increases (SE ∝ 1/√n)\nThe range narrows considerably with increasing sample size",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#mathematical-foundation-1",
    "href": "chapter1.html#mathematical-foundation-1",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.27 Mathematical Foundation",
    "text": "1.27 Mathematical Foundation\nFor a population with mean μ and finite variance σ²:\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\text{ as } n \\to \\infty\nStandard error of the mean:\nSE_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\nFor a fair die: μ = 3.5, σ² = 35/12 ≈ 2.917",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#key-takeaways-2",
    "href": "chapter1.html#key-takeaways-2",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.28 Key Takeaways",
    "text": "1.28 Key Takeaways\n\nInitial Condition: A single die exhibits a uniform (discrete) distribution\nProgressive Transformation: As the number of observations increases, the distribution shape systematically evolves\nConvergence to Normality: At n=30, a distinct normal distribution is observable\nVariance Reduction: The distribution demonstrates increasing concentration around the expected value\nUniversality: The theorem applies to any population distribution with finite variance",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#practical-significance",
    "href": "chapter1.html#practical-significance",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.29 Practical Significance",
    "text": "1.29 Practical Significance\nThis distributional transformation enables:\n\nApplication of normal distribution tables and properties for statistical inference\nConstruction of confidence intervals with specified confidence levels\nExecution of hypothesis tests (t-tests, z-tests)\nFormulation of predictions about sample means with known probability\n\nEssential Property of CLT: Although individual die rolls follow a uniform distribution, the distribution of means from multiple dice converges asymptotically to a normal distribution in a predictable manner consistent with mathematical theory, providing the foundation for classical statistical inference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-c-standard-errors-and-margins-of-error-means-proportions-variance-and-covariance",
    "href": "chapter1.html#appendix-c-standard-errors-and-margins-of-error-means-proportions-variance-and-covariance",
    "title": "1  Foundations of Statistics and Demography",
    "section": "1.30 Appendix C: Standard Errors and Margins of Error: Means, Proportions, Variance, and Covariance",
    "text": "1.30 Appendix C: Standard Errors and Margins of Error: Means, Proportions, Variance, and Covariance\n\nKey Insight: A Proportion IS a Mean\nA proportion is simply the mean of a binary (0/1) variable. If you code “success” as 1 and “failure” as 0, then:\n\\hat{p} = \\bar{x} = \\frac{\\sum x_i}{n}\nFor example, if 6 out of 10 people support a policy (coded as 1=support, 0=don’t support):\n\nProportion: \\hat{p} = 0.6\nMean: \\bar{x} = \\frac{1+1+1+1+1+1+0+0+0+0}{10} = 0.6\n\nThey’re identical! The special formulas for proportions are just the general formulas applied to binary data.\n\n\nThe Universal Formula for Means\nBoth proportions and continuous means use the same fundamental formula for standard error:\nSE = \\frac{SD}{\\sqrt{n}}\nThe Margin of Error (for 95% confidence) is then:\nMoE = 1.96 \\times SE = 1.96 \\times \\frac{SD}{\\sqrt{n}}\n\n\nCalculating SE and MoE for Proportions\nFor a sample proportion \\hat{p}, the standard deviation is derived from the binomial distribution:\nSD = \\sqrt{p(1-p)}\nTherefore:\nSE_p = \\sqrt{\\frac{p(1-p)}{n}}\nMoE_p = 1.96\\sqrt{\\frac{p(1-p)}{n}}\nExample: Political Poll\nIf 60% of voters support a candidate (p = 0.6) with n = 400:\n\nSD = \\sqrt{0.6 \\times 0.4} = \\sqrt{0.24} = 0.490\nSE = \\frac{0.490}{\\sqrt{400}} = \\frac{0.490}{20} = 0.0245 (or 2.45%)\nMoE = 1.96 \\times 0.0245 = 0.048 (or ±4.8%)\n\n\n\nCalculating SE and MoE for Typical Means\nFor a continuous variable like height, weight, or test scores:\nSE_{\\bar{x}} = \\frac{SD}{\\sqrt{n}}\nMoE_{\\bar{x}} = 1.96 \\times \\frac{SD}{\\sqrt{n}}\nExample: Mean Height\nIf measuring height with SD = 10 cm and n = 100:\n\nSE = \\frac{10}{\\sqrt{100}} = \\frac{10}{10} = 1.0 cm\nMoE = 1.96 \\times 1.0 = ±1.96 cm\n\n\n\nWhy Proportions Often Require Larger Samples\nThe perception that proportions need larger samples arises from several factors:\n\n1. Maximum Variance at p = 0.5\nThe variance p(1-p) is maximized when p = 0.5, giving:\nSD_{max} = \\sqrt{0.5 \\times 0.5} = 0.5\nThis means on a 0-1 scale, the standard deviation can be quite large relative to the range. For “maximum uncertainty” scenarios (p = 0.5):\nn = \\left(\\frac{1.96 \\times 0.5}{MoE}\\right)^2 = \\frac{0.9604}{MoE^2}\nSample size requirements for different margins of error (at p = 0.5):\n\n\n\nDesired MoE\nRequired n\n\n\n\n\n±1% (0.01)\n9,604\n\n\n±2% (0.02)\n2,401\n\n\n±3% (0.03)\n1,068\n\n\n±5% (0.05)\n385\n\n\n\n\n\n2. Context of Precision\nThe desired precision differs by context:\n\nProportions: Political polls typically want ±3-4 percentage points\nHeight: ±0.5 cm might suffice (only 5% of a 10 cm SD)\nTest scores: ±2 points might be acceptable (depends on scale)\n\nThese represent different levels of relative precision.\n\n\n3. Scale Matters\nFor a proportion measured as ±0.02 (2 percentage points):\n\nThis is 2% of the full 0-1 scale\nRelatively speaking, this is very precise\n\nFor height measured as ±2 cm with SD = 10 cm:\n\nThis is only 20% of one standard deviation\nLess stringent requirement\n\n\n\n4. Rare Events\nWhen estimating rare proportions (e.g., p = 0.01), you need enough sample to actually observe the events:\n\nFor p = 0.01 with n = 100, you expect only 1 success\nNeed n \\approx 1,500 for ±0.5% precision\n\n\n\n\nMargin of Error and Sample Size for Variance\nVariance estimation is more complex because sample variance does not follow a normal distribution - it follows a scaled chi-squared distribution (for normally distributed data).\n\n\nStandard Error of Variance\nFor a normally distributed variable, the standard error of the sample variance s^2 is:\nSE(s^2) = s^2\\sqrt{\\frac{2}{n-1}}\nExample: Height Variance\nIf height has s^2 = 100 cm² (so s = 10 cm) with n = 101:\n\nSE(s^2) = 100\\sqrt{\\frac{2}{100}} = 100 \\times 0.1414 = 14.14 cm²\nMoE = 1.96 \\times 14.14 = ±27.7 cm²\n\nImportant considerations:\n\nConfidence intervals are asymmetric: Because chi-squared distribution is skewed, exact CIs should use chi-squared quantiles, not the ±1.96 approach\nNormality assumption matters: The formula assumes underlying normality\nLarger samples needed: Note SE \\propto 1/\\sqrt{n-1} means variance estimates converge slowly\nSample size for given precision: To get MoE = 0.1 × s^2 (10% precision):\n\nn \\approx 1 + 2\\left(\\frac{1.96}{0.1}\\right)^2 = 769\nThis is much larger than for means!\n\n\nStandard Error of Standard Deviation\nFor the standard deviation itself (using delta method):\nSE(s) \\approx \\frac{s}{\\sqrt{2(n-1)}}\nThis is approximately half the coefficient of variation of the variance.\n\n\n\nMargin of Error and Sample Size for Covariance\nCovariance estimation is even more complex because it depends on the joint distribution of two variables.\n\nStandard Error of Covariance\nFor two variables X and Y from a bivariate normal distribution:\nSE(Cov(X,Y)) \\approx \\sqrt{\\frac{1}{n}\\left[\\sigma_X^2\\sigma_Y^2 + \\sigma_{XY}^2\\right]}\nWhere:\n\n\\sigma_X^2, \\sigma_Y^2 are the population variances\n\\sigma_{XY} is the population covariance\n\nIn practice, these are estimated from the sample.\nExample: Covariance of Height and Weight\nSuppose:\n\ns_X = 10 cm (height SD)\ns_Y = 15 kg (weight SD)\n\ns_{XY} = 80 cm·kg (sample covariance)\nn = 100\n\nSE(s_{XY}) \\approx \\sqrt{\\frac{1}{100}[(10^2)(15^2) + 80^2]} = \\sqrt{\\frac{1}{100}[22,500 + 6,400]}\n= \\sqrt{289} = 17.0 \\text{ cm·kg}\nMoE = 1.96 \\times 17.0 = ±33.3 \\text{ cm·kg}\n\n\nStandard Error of Correlation\nFor the correlation coefficient r (Pearson’s), when the true correlation \\rho is not zero:\nSE(r) \\approx \\frac{(1-r^2)}{\\sqrt{n}}\nFor r = 0.5 and n = 100:\nSE(r) = \\frac{1-0.25}{\\sqrt{100}} = \\frac{0.75}{10} = 0.075\nMoE = 1.96 \\times 0.075 = ±0.147\nImportant notes for variance/covariance:\n\nNon-normal sampling distributions: These statistics don’t follow normal distributions, especially for small samples\nFisher’s z-transformation: For correlation, CIs are typically computed using Fisher’s z-transform for better coverage\nBootstrap methods recommended: For complex scenarios, bootstrap confidence intervals often perform better\nLarger samples required: Variance and covariance require substantially larger samples than means for equivalent precision\n\n\n\n\nComparative Example: Sample Size Requirements\nTo achieve 10% relative precision (MoE = 10% of the estimate):\nFor a Mean:\n\nNeed: n = \\left(\\frac{1.96 \\times CV}{0.10}\\right)^2 where CV = SD/\\mu\nIf CV = 0.5: n = 96\n\nFor a Proportion at p = 0.5:\n\nNeed: n = \\left(\\frac{1.96 \\times 0.5}{0.05}\\right)^2 = 384\n(Here 10% relative = ±0.05 absolute on 0-1 scale)\n\nFor a Variance:\n\nNeed: n \\approx 1 + 2\\left(\\frac{1.96}{0.10}\\right)^2 = 769\n\nVariance requires approximately 8× the sample size of a mean for equivalent relative precision!\n\n\nKey Takeaways\nFor Means (including proportions):\n\nProportions ARE means - just means of 0/1 data\nStandard errors decrease as 1/\\sqrt{n}\nNormal approximation works well for moderate samples\nSymmetric confidence intervals appropriate\n\nFor Variance and Covariance:\n\nStandard errors decrease as 1/\\sqrt{n} but with larger constants\nSampling distributions are skewed (chi-squared for variance)\nRequire substantially larger samples for equivalent precision\nAsymmetric confidence intervals often needed\nBootstrap or exact methods recommended over simple ±1.96 approach\n\nWhy proportions seem to need larger samples:\n\nThe scale of measurement (0-1 vs. unbounded)\nThe relative precision desired (±3% is stringent on 0-1 scale)\nThe maximum SD for proportions (0.5) being large relative to range\nContextual standards in polling and surveys\n\nWhen comparing equivalent relative precision, sample size requirements for means and proportions are comparable!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html",
    "href": "rozdzial1.html",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "",
    "text": "2.1 Wprowadzenie\nStatystyka jest sposobem poznawania świata na podstawie danych. Uczy nas, jak mądrze zbierać dane, dostrzegać wzorce, szacować parametry (cechy) populacyjne i dokonywać prognoz — określając, jak bardzo możemy się mylić.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wprowadzenie",
    "href": "rozdzial1.html#wprowadzenie",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "",
    "text": "Statystyka to nauka o uczeniu się z danych (the science of learning from data) w warunkach niepewności.\n\n\n\n\n\n\n\n\nNote\n\n\n\nStatystyka to nauka o zbieraniu, organizowaniu, analizowaniu, interpretowaniu i prezentowaniu danych. Obejmuje zarówno metody pracy z danymi, jak i teoretyczne podstawy uzasadniające te metody.\nAle statystyka to coś więcej niż tylko liczby i wzory — to sposób myślenia o niepewności i zmienności w otaczającym nas świecie.\n\n\n\nCzym są dane?\nDane: Informacje zebrane podczas badania – obejmują odpowiedzi z ankiet, wyniki eksperymentów, wskaźniki ekonomiczne, treści z mediów społecznościowych lub wszelkie inne mierzalne obserwacje.\nRozkład danych (data distribution) opisuje, jak wartości rozkładają się między możliwymi wynikami (jakie wartości przyjmuje zmienna i jak często). Rozkłady mówią nam, które wartości są powszechne, które są rzadkie i jakie wzorce istnieją w naszych danych.\n\n\nDemografia to nauka zajmująca się badaniem ludności, koncentrująca się na jej wielkości, strukturze, rozmieszczeniu i zmianach zachodzących w czasie. To zasadniczo analiza statystyczna populacji - kim są ludzie, gdzie mieszkają, ilu ich jest i jak te charakterystyki ewoluują.\n\nStatystyka i demografia to powiązane ze sobą dyscypliny, które dostarczają narzędzi do zrozumienia populacji, ich charakterystyk i wzorców wyłaniających się z danych.\n\n\n\n\n\n\nZaokrąglenia i notacja naukowa\n\n\n\nZasada główna: O ile nie podano inaczej, części ułamkowe liczb dziesiętnych zaokrąglaj do co najmniej 2 cyfr znaczących. W statystyce często pracujemy z długimi częściami ułamkowymi i bardzo małymi liczbami — w obliczeniach, nie zaokrąglaj nadmiernie w krokach pośrednich, zaokrąglaj na końcu obliczeń.\n\nZaokrąglanie w kontekście statystycznym\nCzęść ułamkowa to cyfry po przecinku dziesiętnym. W statystyce szczególnie ważne jest zachowanie odpowiedniej precyzji:\nStatystyki opisowe:\n\nŚrednia: \\bar{x} = 15.847693... \\rightarrow 15.85\nOdchylenie standardowe: s = 2.7488... \\rightarrow 2.75\nWspółczynnik korelacji: r = 0.78432... \\rightarrow 0.78\n\nBardzo małe liczby (p-wartości, prawdopodobieństwa):\n\np = 0.000347... \\rightarrow 0.00035 lub 3.5 \\times 10^{-4}\nP(X &gt; 2) = 0.0000891... \\rightarrow 0.000089 lub 8.9 \\times 10^{-5}\n\n\n\nCyfry znaczące w części ułamkowej\nW części ułamkowej cyfry znaczące to wszystkie cyfry oprócz zer wiodących:\n\n.78432 ma 5 cyfr znaczących → zaokrąglamy do .78 (2 c.z.)\n.000347 ma 3 cyfry znaczące → zaokrąglamy do .00035 (2 c.z.)\n.050600 ma 4 cyfry znaczące → zaokrąglamy do .051 (2 c.z.)\n\n\n\nZasady zaokrąglania w statystyce\n\nZaokrąglaj tylko część ułamkową do co najmniej 2 cyfr znaczących\nCzęść całkowita pozostaje niezmieniona\nW długich obliczeniach zachowuj 3-4 cyfry w części ułamkowej do ostatniego kroku\nNIGDY nie zaokrąglaj do zera - małe wartości mają znaczenie interpretacyjne\nDla bardzo małych liczb używaj notacji naukowej gdy to ułatwia odczyt\nP-wartości często wymagają większej precyzji — zachowaj 2-3 cyfry znaczące\n\n\n\nNotacja naukowa w statystyce\nW statystyce często spotykamy bardzo małe liczby. Używaj notacji naukowej gdy ułatwia to odczyt:\nP-wartości i prawdopodobieństwa:\n\np = 0.000347 = 3.47 \\times 10^{-4} (lepiej: 3.5 \\times 10^{-4})\nP(Z &gt; 3.5) = 0.000233 = 2.33 \\times 10^{-4}\n\nDuże liczby (rzadko w podstawowej statystyce):\n\nN = 1\\,234\\,567 = 1.23 \\times 10^6\n\nWątpliwości: Lepiej zachować dodatkową cyfrę niż zaokrąglić zbyt mocno\n\n\n\n\n\n\n\n\n\n\nPo Co Statystyka w Naukach Społecznych i Politologii lub SM?\n\n\n\nStatystyka jest niezbędna w naukach społecznych i politologii z kilku kluczowych powodów:\nRozumienie Zjawisk Społecznych: Mierzenie nierówności, ubóstwa, bezrobocia, uczestnictwa politycznego; opisywanie wzorców demograficznych i trendów społecznych; kwantyfikowanie postaw, przekonań i zachowań w populacjach.\nTestowanie Teorii: Politolodzy tworzą teorie na temat demokracji, zachowań wyborczych, konfliktów i instytucji. Socjolodzy rozwijają teorie dotyczące mobilności społecznej, nierówności i dynamiki grupowej. Statystyka pozwala nam testować, czy te teorie odpowiadają rzeczywistości.\nWnioskowanie Przyczynowe (Causal Inference): Naukowcy społeczni chcą odpowiadać na pytania “dlaczego”—Czy wykształcenie zwiększa dochody? Czy demokracje rzadziej prowadzą wojny? Czy media społecznościowe wpływają na polaryzację polityczną? Statystyka pomaga odróżnić przyczynowość od zwykłej korelacji.\nEwaluacja Polityk (Policy): Ocena, czy interwencje (programy, polityki publiczne) działają—Czy program szkolenia zawodowego zmniejsza bezrobocie? Czy reforma wyborcza zwiększyła frekwencję? Czy programy walki z ubóstwem są skuteczne? Statystyka dostarcza narzędzi do oceny tego, co działa, a co nie.\nBadania Opinii Publicznej: Sondaże wyborcze i prognozy; mierzenie poparcia społecznego dla polityk; zrozumienie, jak opinie różnią się w grupach demograficznych; śledzenie zmian postaw w czasie.\nDokonywanie Uogólnień: Nie możemy przepytać wszystkich, więc pobieramy próbę (sample) i używamy statystyki do wnioskowania o całych populacjach. Ankieta wśród 1000 osób może nam powiedzieć coś o narodzie liczącym miliony (z oszacowaną niepewnością).\nRadzenie Sobie ze Złożonością: Społeczności ludzkie są skomplikowane—wiele czynników wzajemnie się warunkuje. Statystyka pomaga nam kontrolować zmienne zakłócające (confounding variables), izolować konkretne efekty (reguła ceteris paribus) i rozumieć wielowymiarowe zależności.\nUnikalność Nauk Społecznych: W przeciwieństwie do nauk przyrodniczych, nauki społeczne badają ludzkie zachowania, które są bardzo zmienne i zależne od kontekstu. Statystyka dostarcza narzędzi do znajdowania wzorców i wyciągania wniosków pomimo tej niepewności.\n\n\n\n\nPracując z danymi, statystycy stosują dwa różne podejścia: eksplorację i konfirmację/weryfikację (wnioskowanie statystyczne). Najpierw badamy dane, aby zrozumieć ich charakterystykę i zidentyfikować wzorce. Następnie używamy formalnych metod do testowania konkretnych hipotez i wyciągania wniosków.\n\n\n\n\n\n\n\nEDA vs. statystyka inferencyjna\n\n\n\nStatystykę można rozumieć jako dwa uzupełniające się etapy:\n\nEksploracyjna analiza danych (EDA): łączy metody statystyki opisowej oraz metody wizualizacji (wykresy, tabele, przekształcenia) w celu zbadania danych, wykrycia wzorców, sprawdzenia założeń i wygenerowania hipotez.\n\nStatystyka inferencyjna: wykorzystuje modele probabilistyczne do testowania hipotez i formułowania wniosków uogólnialnych poza badaną próbą.\n\n\n\n\n\n\n\n\n\nProcent vs punkty procentowe (pp)\n\n\n\nGdy w mediach słyszysz, że „bezrobocie spadło o 2”, czy chodzi o 2 punkty procentowe (pp), czy 2 procent?\nTo nie to samo:\n\n2 pp (zmiana absolutna): np. 10% → 8% (−2 pp).\n2% (zmiana względna): mnożymy starą stopę przez 0,98; np. 10% → 9,8% (−0,2 pp).\n\nZawsze pytaj:\n\nJaka jest wartość bazowa (wcześniejsza stopa)?\nCzy to zmiana absolutna (pp), czy względna (%)?\nCzy różnica może wynikać z błędu losowego / błędu próby?\nJak mierzono bezrobocie (badanie ankietowe vs dane administracyjne), kiedy i kogo uwzględniono?\n\nProsta zasada\n\nUżywaj punktów procentowych (pp), gdy porównujesz stopy/procenty wprost (bezrobocie, frekwencja).\nUżywaj procentów (%) dla zmian względnych (względem wartości wyjściowej).\n\nMała ściąga\n\n\n\n\n\n\n\n\nStopa początkowa\n„Spadek o 2%” (względny)\n„Spadek o 2 pp” (absolutny)\n\n\n\n\n6%\n6% × 0,98 = 5,88% (−0,12 pp)\n4%\n\n\n8%\n8% × 0,98 = 7,84% (−0,16 pp)\n6%\n\n\n10%\n10% × 0,98 = 9,8% (−0,2 pp)\n8%\n\n\n\nUwaga: 2% ≠ 2 punkty procentowe (pp).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#eksploracyjna-analiza-danych-eda---exploratory-data-analysis",
    "href": "rozdzial1.html#eksploracyjna-analiza-danych-eda---exploratory-data-analysis",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.2 Eksploracyjna Analiza Danych (EDA - Exploratory Data Analysis)",
    "text": "2.2 Eksploracyjna Analiza Danych (EDA - Exploratory Data Analysis)\nCzym jest EDA? Eksploracyjna Analiza Danych to początkowy etap, w którym systematycznie badamy dane, aby zrozumieć ich strukturę i charakterystykę. Ta faza nie obejmuje formalnego testowania hipotez statystycznych—koncentruje się na odkrywaniu tego, co dane zawierają.\nPo co przeprowadzamy EDA?\n\nWykrycie nieoczekiwanych wzorców i zależności\nIdentyfikacja wartości odstających (outliers) i problemów z jakością danych\nSprawdzenie założeń do późniejszego modelowania (wiele metod statystycznych ma określone wymagania dotyczące danych, aby działały prawidłowo. EDA pomaga sprawdzić, czy nasze dane spełniają te wymagania; np. normalność rozkładu, liniowość, “outliers”, jednorodność wariancji)\nGenerowanie hipotez wartych przetestowania\nZrozumienie struktury i charakterystyki zbioru danych\n\n\n\n\n\n\n\nPodejście EDA\n\n\n\nPrzeprowadzając EDA, zaczynamy bez z góry określonych hipotez. Zamiast tego badamy dane z wielu perspektyw, aby odkryć wzorce i wygenerować pytania do dalszych badań.\n\n\n\nNarzędzia do Eksploracji Danych\n1. Statystyki Opisowe (Descriptive Statistics)\nSą to podstawowe obliczenia, które opisują nasze dane:\nMiary Tendencji Centralnej - gdzie znajduje się centrum (średnia, “wartość typowa/oczekiwana”) danych?\n\nŚrednia arytmetyczna (Mean): Suma wszystkich wartości podzielona przez ich liczbę. Przykład: Jeśli 5 studentów uzyskało na teście 70, 80, 85, 90 i 100 punktów, średnia wynosi 85.\nMediana (Median): Wartość środkowa, gdy ustawimy wszystkie liczby od najmniejszej do największej. W naszym przykładzie mediana również wynosi 85.\nModa (Mode): Wartość występująca najczęściej. Jeśli dziesięć rodzin ma 1, 2, 2, 2, 2, 3, 3, 3, 4 i 5 dzieci, modą są 2 dzieci.\n\nMiary Zmienności (Measures of Variability) - jak bardzo rozproszone są dane?\n\nRozstęp (Range): Różnica między największą a najmniejszą wartością. Jeśli wiek studentów wynosi od 18 do 24 lat, rozstęp to 6 lat.\nOdchylenie Standardowe (Standard Deviation): Pokazuje, jak bardzo dane są rozproszone wokół średniej. Małe odchylenie standardowe oznacza, że większość wartości jest blisko średniej; duże oznacza większe rozproszenie.\n\n2. Wizualizacja Danych\nMetody graficzne pomagają ujawnić wzorce, których same podsumowania numeryczne mogą nie pokazać:\n\nPiramidy Wieku (Population Pyramids): Pokazują rozkład wieku i płci w populacji\nWykresy Pudełkowe (Box Plots): Pokazują środek danych i pomagają wykryć wartości nietypowe\nWykresy Rozrzutu (Scatter Plots): Pokazują związki między dwiema zmiennymi (np. godziny nauki a wyniki testów)\nWykresy Szeregów Czasowych (Time Series Graphs): Pokazują zmiany w czasie (np. temperatura w ciągu roku)\nHistogramy (Histograms): Histogram to graficzna reprezentacja danych, która pokazuje rozkład częstości zbioru danych. Składa się z przylegających do siebie słupków (bez przerw między nimi), gdzie każdy słupek reprezentuje przedział wartości (nazywany przedziałem klasowym), a wysokość słupka pokazuje, jaka część danych mieści się w tym przedziale. Histogramy służą do wizualizacji kształtu, rozrzutu i tendencji centralnej danych liczbowych.\n\n\n\n\nhttps://commons.wikimedia.org/wiki/File:%C5%81%C3%B3d%C5%BA_population_pyramid.svg\n\n\n3. Poszukiwanie Zależności:\n\nCzy dwie zmienne zmieniają się razem? (Kiedy jedna rośnie, czy druga też rośnie?)\nCzy można dopasować linię (linię regresji) do danych?\nCzy widoczne są jakieś wyraźne wzorce lub trendy?\n\n\n\n\n\n\n\n\nNote\n\n\n\nWiele technik statystycznych służy zarówno celom eksploracyjnym, jak i konfirmacyjnym/weryfikacyjnym:\nEksploracja: Obliczamy korelacje (correlations) lub dopasowujemy linie regresji (regression lines), aby zrozumieć, jakie zależności istnieją w danych. Koncentrujemy się na odkrywaniu wzorców.\nKonfirmacja: Stosujemy testy statystyczne, aby określić, czy zaobserwowane wzorce są istotne statystycznie, czy mogły wystąpić przypadkowo. Koncentrujemy się na formalnym testowaniu hipotez.\nTa sama technika może służyć różnym celom w zależności od fazy badania.\n\n\n4. Ważne Pytania do Zadania Podczas Eksploracji:\n\nJaki jest kształt rozkładu danych?\nCzy są wartości odstające?\nCzy widać jakieś wzorce?\nCzy brakuje jakichś danych?\nCzy różne grupy wykazują różne wzorce?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wnioskowanie-statystyczne-inferential-statistics",
    "href": "rozdzial1.html#wnioskowanie-statystyczne-inferential-statistics",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.3 Wnioskowanie Statystyczne (Inferential Statistics)",
    "text": "2.3 Wnioskowanie Statystyczne (Inferential Statistics)\nPo zbadaniu danych możemy chcieć wyciągnąć formalne wnioski. Wnioskowanie statystyczne (inferential statistics) nam to umożliwia.\nPodstawowa Idea: Mamy dane z pewnej grupy osób (próba, sample), ale chcemy wiedzieć coś o wszystkich (populacja, population). Wnioskowanie statystyczne pomaga nam wyciągać wnioski o większej grupie na podstawie mniejszej grupy.\n\n\n\n\n\n\nNote\n\n\n\nPróba losowa wymaga, aby każdy element populacji miał znane, niezerowe prawdopodobieństwo zostania wybranym, niekoniecznie równe.\nGdy każdy element ma równe prawdopodobieństwo wyboru, nazywamy to konkretnie prostą próbą losową - jest to najbardziej podstawowy typ.\n\n\n\n\n\n\n\n\nWnioskowanie z próby o cechach populacji: Analogia „próbowania zupy”\n\n\n\n\nRozważmy kucharza przygotowującego zupę dla 100 osób, który musi ocenić jej smak bez konsumowania całego garnka:\nPopulacja: Cały garnek zupy (100 porcji)\nPróba: Jedna łyżka do spróbowania\nParametr populacji: Prawdziwy średni poziom słoności całego garnka (nieznany)\nStatystyka z próby: Poziom słoności wykryty w łyżce (“estymacja punktowa”)\nWnioskowanie statystyczne: Używanie charakterystyk łyżki do wyciągania wniosków o całym garnku\n\nWażne\n1. Próbkowanie losowe jest kluczowe. Przed pobraniem próbki zupę trzeba dobrze zamieszać albo pobierać z losowych miejsc. Nabieranie tylko z powierzchni może pominąć przyprawy, które opadły na dno, co wprowadza błąd systematyczny (bias).\n2. Wielkość próby decyduje o precyzji. Większa łyżka albo więcej łyżek (większe n) daje mniejszy błąd losowy i stabilniejszy szacunek „średniego smaku”, choć koszty i czas ograniczają, jak bardzo można zwiększać próbę.\n3. Niepewność jest nieusuwalna. Nawet przy poprawnym próbkowaniu pojedyncza łyżka może nie odzwierciedlać idealnie całego garnka; zawsze istnieje losowa zmienność.\n4. Błąd systematyczny podważa wnioskowanie. Jeśli sól dosypano tylko tam, skąd zwykle nabierasz próbkę, wnioski o całym garnku będą zafałszowane — to przykład stronniczości próbkowania.\n5. Jedna próbka ma ograniczoną wartość. Jednorazowy test może powiedzieć, że „średnio jest słona”, ale nie pokaże rozpiętości smaków w garnku. Aby ocenić zmienność, trzeba pobrać wiele niezależnych próbek.\nUwaga: zwiększanie liczebności próby poprawia precyzję (mniej szumu), ale nie usuwa błędu systematycznego; ten wymaga poprawy schematu próbkowania.\n\nTa analogia chwyta istotę rozumowania statystycznego: używanie starannie wybranych prób do poznawania większych populacji przy jednoczesnym jawnym uznawaniu i kwantyfikacji nieodłącznej niepewności w tym procesie.\n\n\n\n\n\nMyślenie Statystyczne\n\n\n\n\n\n\n\nKluczowe pojęcia (w skrócie)\n\n\n\nSchemat: Pytanie badawcze → Estymanda (co mierzymy w populacji) → Parametr (prawdziwa, nieznana wartość) → Estymator (reguła z próby) → Estymata/oszacowanie (konkretna liczba z Twoich danych)\nCo chcemy poznać:\n\nEstymanda — wielkość w populacji, którą chcemy poznać (formalny cel), a nie samo zdanie-pytanie.\nPrzykład: „Średni wiek przy pierwszym porodzie w Polsce w 2023 r.”\nParametr (\\theta) — prawdziwa, ale nieznana wartość estymandy w populacji (stała, nie losowa).\nPrzykład: Rzeczywista średnia \\mu = 29{,}4 roku życia.\n\n\nJak to szacujemy (3 kroki):\n\nStatystyka z próby — dowolna funkcja danych z próby (reguła), np.\n\\displaystyle \\bar{X}=\\frac{1}{n}\\sum_{i=1}^n X_i\nEstymator — ta statystyka wybrana do oszacowania konkretnego parametru (z definicji zależy od losowej próby, więc jest losowa).\nPrzykład: Używamy \\bar{X} jako estymatora \\mu.\nEstymata / oszacowanie (\\hat\\theta) — konkretna liczba po zastosowaniu estymatora do Twoich danych (x_1,\\dots,x_n).\nPrzykład: \\hat\\mu = \\bar{x} = 29{,}1 roku.\n\n\nAnalogia:\nStatystyka = narzędzie → Estymator = narzędzie wybrane do celu → Estymata = efekt pracy narzędzia (konkretny wynik)\n\nPopularne estymatory\n\n\n\n\n\n\n\n\n\nParametr populacji (cel)\nEstymator (statystyka)\nWzór\nUwaga\n\n\n\n\nŚrednia populacji \\mu\nŚrednia z próby\n\\bar X=\\frac{1}{n}\\sum_{i=1}^n X_i\nEstymator nieobciążony. Estymator \\bar X jest zmienną losową; konkretna wyliczona wartość (np. \\bar x = 5{,}2) nazywa się oszacowaniem.\n\n\nProporcja/frakcja w populacji p\nProporcja/frakcja z próby\n\\hat p=\\frac{K}{n}, gdzie K=\\sum_{i=1}^n Y_i dla Y_i\\in\\{0,1\\}\nRównoważne \\bar Y przy kodowaniu wyników jako 0/1. Tutaj K zlicza liczbę sukcesów w n próbach.\n\n\nWariancja populacji \\sigma^2\nWariancja z próby\ns^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2\nDzielnik n-1 (korekta Bessela) czyni ten estymator nieobciążonym dla \\sigma^2. Użycie n dałoby estymator obciążony.\n\n\n\n\n\nKażdy estymator jest statystyką, ale nie każda statystyka jest estymatorem — dopóki nie przypiszesz jej konkretnego celu (estymandy), jest „po prostu” statystyką.\n\n\n\n\n\n\n\nhttps://allmodelsarewrong.github.io/mse.html\n\n\n\n\n\n\n\n\nKryteria jakości: błąd, wariancja, MSE, efektywność (*)\n\n\n\nJak ocenić, czy estymator (metoda) jest dobry?\n\nBłąd systematyczny (bias) — czy nasza metoda daje prawdziwe wyniki „średnio”?\nWyobraź sobie, że chcemy poznać średni wzrost dorosłych Polaków (prawdziwa wartość: 172 cm). Pobieramy 100 różnych prób po 500 osób każda i dla każdej liczymy średnią.\nEstymator nieobciążony: Te 100 średnich będzie się różnić (169 cm, 173 cm, 171 cm…), ale ich średnia będzie bliska 172 cm. Czasem przeszacowujemy, czasem niedoszacowujemy, ale nie ma systematycznego błędu.\nEstymator obciążony: Gdybyśmy przypadkowo zawsze pomijali osoby powyżej 180 cm, wszystkie nasze 100 średnich byłyby za niskie (np. oscylowały wokół 168 cm). To błąd systematyczny.\n\n\n\nWariancja — jak bardzo różnią się wyniki między próbami?\nMamy dwie metody szacowania tego samego parametru. Obie „średnio” dają dobry wynik, ale:\n\nMetoda A: z 10 prób otrzymujemy: 171, 172, 173, 171, 172, 173, 172, 171, 173, 172 cm\nMetoda B: z 10 prób otrzymujemy: 165, 179, 168, 176, 171, 174, 169, 175, 167, 176 cm\n\nMetoda A ma mniejszą wariancję — wyniki są bardziej skupione, przewidywalne. W praktyce wolisz metodę A, bo możesz być bardziej pewien pojedynczego wyniku.\nKluczowa zasada: Większa próba = mniejsza wariancja. Z próby 100 osób średnia będzie bardziej “skakać” niż z próby 1000 osób.\n\n\n\nŚredni błąd kwadratowy (MSE) — co jest ważniejsze: brak obciążenia czy stabilność?\nCzasem mamy dylemat:\n\nEstymator A: Nieobciążony (średnio 172 cm), ale bardzo niestabilny (wyniki od 160 do 184 cm)\nEstymator B: Lekko obciążony (średnio 171 cm zamiast 172 cm), ale bardzo stabilny (wyniki od 169 do 173 cm)\n\nMSE mówi: Estymator B jest lepszy — niewielkie systematyczne przeszacowanie o 1 cm jest mniej problematyczne niż ogromny rozrzut wyników w estymatorze A.\n\n\n\nEfektywność — który nieobciążony estymator wybrać?\nMasz dane o dochodach 500 osób. Chcesz poznać „typowy” dochód. Dwie możliwości:\n\nŚrednia arytmetyczna: zazwyczaj daje wyniki w zakresie 4800–5200 zł\nMediana: daje wyniki w zakresie 4500–5500 zł\n\nJeśli obie metody są nieobciążone, wybierz tę o mniejszym rozrzucie (średnia jest bardziej efektywna dla danych z rozkładu normalnego).\n\n\n\n\nPrzykład Myślenia Statystycznego\nWładze uniwersytetu rozważają udostępnienie biblioteki całodobowo. Administracja potrzebuje odpowiedzi na pytanie: Jaka część studentów popiera tę zmianę?\n\n\n\n\n\n\nNote\n\n\n\nSytuacja idealna: Zapytanie wszystkich 20 000 studentów → Uzyskanie dokładnej odpowiedzi (parametr \\theta)\nSytuacja rzeczywista: Ankietowanie 100 studentów → Uzyskanie oszacowania (\\hat{\\theta}) z niepewnością\n\n\n\n\n\n\n\n\nObciążenie vs. Błąd Losowy\n\n\n\nBłąd statystyczny można rozłożyć na dwa główne komponenty: obciążenie (błąd systematyczny) i błąd losowy (nieprzewidywalna zmienność).\nObciążenie jest jak nieprawidłowo skalibrowana waga, która konsekwentnie pokazuje o 2 kg za dużo—każdy pomiar jest błędny w tym samym kierunku. To błąd systematyczny.\nBłąd losowy to nieprzewidywalna zmienność w obserwacjach, jak:\n\nGracz w rzutki celujący w środek tarczy—każdy rzut ląduje w nieco innym miejscu z powodu drżenia ręki, prądów powietrza, drobnych różnic w ruchu mięśni\nMierzenie wzrostu osoby kilka razy i otrzymywanie 174,8 cm, 175,0 cm, 175,3 cm—małe fluktuacje wynikające ze zmiany postawy, oddychania, sposobu odczytu skali i naturalnych wahań ciała\nModel pogody, który czasem przewiduje o 2°C za dużo, czasem o 1°C za mało, czasem trafnie\nSondaże opinii publicznej pokazujące 52%, 49%, 51% poparcia w różnych badaniach—każda losowa próba daje nieco inne wyniki, ale skupiają się wokół prawdziwej wartości\n\nBłąd losowy mierzymy wariancją — średnią kwadratów odchyleń obserwacji od średniej. Pokazuje ona, jak duży jest rozrzut wyników (np. prognoz) wokół średniej.\nObrazowe porównanie: Wyobraź sobie, że prosisz pięcioro znajomych, by oszacowali, ile cukierków jest w słoiku. Każdy poda inną liczbę — to efekt przypadku — ale odpowiedzi będą się wahać wokół wartości prawdziwej, a nie wszystkie odchylą się w tę samą stronę.\nBłąd systematyczny (bias) w sondażach: to nieprzypadkowe odchylenie wyników, gdy sposób zbierania danych faworyzuje jedne grupy, a pomija inne.\n\nAnkietowanie wyłącznie na siłowni o 6:00 rano sprawi, że konsekwentnie przeszacujesz udział osób dbających o zdrowie i wcześnie wstających, a zaniżysz udział pracujących na nocne zmiany czy rodziców małych dzieci. Sondaż jest „zepsuty” w przewidywalny sposób.\nZliczanie tylko odpowiedzi osób odbierających połączenia z nieznanych numerów spowoduje, że systematycznie pominiesz tych — zwłaszcza młodszych — którzy filtrują połączenia.\n\nKrótko: wariancja opisuje rozrzut (błąd losowy), a bias — przesunięcie w określoną stronę (błąd systematyczny).\nKluczowa różnica: Uśrednianie większej liczby obserwacji zmniejsza błąd losowy, ale nigdy nie naprawia obciążenia. Nie można wyeliminować błędu systematycznego przez uśrednianie—ani nieprawidłowo skalibrowanej wagi, ani stronniczej metody próbkowania!\n\n\n\n\nDwa Podejścia do Tych Samych Danych\nZałóżmy, że przeprowadzono ankietę wśród 100 losowo wybranych studentów i stwierdzono, że 60 z nich popiera całodobowe otwarcie biblioteki.\n\n\n❌ Bez Myślenia Statystycznego\n“60 ze 100 studentów odpowiedziało twierdząco.”\nWniosek: “Dokładnie 60% wszystkich studentów popiera zmianę.”\nDecyzja: “Ponieważ przekracza to 50%, mamy wyraźne poparcie większości.”\nProblem: Ignorowanie faktu, że inna próba mogłaby dać wynik 55% lub 65%\n\n✅ Z Zastosowaniem Myślenia Statystycznego\n“60 ze 100 studentów odpowiedziało twierdząco.”\nWniosek: “Szacujemy poparcie na poziomie 60% z marginesem błędu ±10 pp.”\nDecyzja: “Prawdziwe poparcie prawdopodobnie mieści się między 50% a 70% — potrzebujemy większej próby dla pewności większościowego poparcia.”\nPrzewaga: Uznanie niepewności prowadzi do lepszych decyzji\n\n\nJak wielkość próby wpływa na precyzję:\n\n\n\n\n\n\n\n\n\n\nWielkość próby\nObserwowany wynik\nBłąd losowy oszacowania\n(95%) “przedział wiarygodnych wartości”\nInterpretacja\n\n\n\n\nn = 100\n60%\n±10 p.p.\n50% do 70%\nNiepewność co do większości\n\n\nn = 400\n60%\n±5 p.p.\n55% do 65%\nPrawdopodobna większość\n\n\nn = 1000\n60%\n±3 p.p.\n57% do 63%\nWyraźna większość\n\n\nn = 1600\n60%\n±2,5 p.p.\n57,5% do 62,5%\nSilna większość\n\n\nn = 10 000\n60%\n±1 p.p.\n59% do 61%\nBardzo precyzyjne oszacowanie\n\n\n\nZasada malejących korzyści: Zauważ, że czterokrotne zwiększenie próby ze 100 do 400 zmniejsza błąd oszacowania o połowę, ale zwiększenie z 1600 do 10 000 (wzrost 6,25-krotny) redukuje go tylko o 1,5 punktu procentowego. Aby zmniejszyć błąd oszacowania o połowę, należy zwiększyć wielkość próby czterokrotnie.\nDlatego większość sondaży zatrzymuje się na około 1000–1500 respondentach—dalszy wzrost precyzji rzadko uzasadnia dodatkowe koszty i nakład pracy.\n\n\n\nWielkość próby a niepewność (błąd losowy)\nZałóżmy, że pobieramy próbę losową o liczebności n=1000 wyborców i obserwujemy \\hat p = 0,55 (np. 55% poparcia dla kandydata w nadchodzących wyborach—550 na 1000 respondentów). Wówczas:\n\nNaszym najlepszym punktowym oszacowaniem (estymacja punktowa) proporcji w populacji jest \\hat p = 0,55.\nTypowy „przedział wiarygodnych wartości” (przedział ufności dla poziomu ufności 95\\%) wokół \\hat p można aproksymować jako \\hat p \\pm \\text{Błąd losowy (Margin of Error, margines błędu)}, czyli: \n  \\hat p \\;\\pm\\; 2\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\n  \\;=\\;\n  0,55 \\;\\pm\\; 2\\sqrt{\\frac{0,55\\cdot 0,45}{1000}}\n  \\approx\n  0,55 \\pm 0,031,\n co daje w przybliżeniu przedział (estymacja przedziałowa) od 52\\% do 58\\% (około \\pm 3,1 punktu procentowego).\n\n\n\nUwaga: Współczynnik 2 jest wygodnym zaokrągleniem wartości 1,96, czyli tzw. wartości krytycznej z rozkładu normalnego standardowego dla poziomu ufności 95%.\n\n\nSzerokość tego przedziału maleje w przewidywalny sposób wraz z wielkością próby: \n  \\text{Błąd losowy oszacowania} \\;\\propto\\; \\frac{1}{\\sqrt{n}}.\n Na przykład, zwiększenie n z 1000 do 4000 zmniejsza błąd oszacowania mniej więcej o połowę (z \\pm 3,1\\% do \\pm 1,6\\%).\n\n\n\n\n\n\n\n\nNote\n\n\n\nPodstawowa zasada: Statystyka nie eliminuje niepewności — pomaga nam ją mierzyć, zarządzać nią i skutecznie komunikować.\n\n\n\n\n\n\n\n\n\nHistoryczny przykład: sondaż Literary Digest z 1936 roku\n\n\n\nW 1936 roku magazyn Literary Digest przeprowadził jeden z największych sondaży w historii — wysłał miliony ankiet i zebrał około 2,4 miliona odpowiedzi. Mimo ogromnej liczby uczestników, przewidywania okazały się całkowicie błędne.\n\n\n\nKandydat\nPrognoza\nWynik rzeczywisty\nBłąd\n\n\n\n\nLandon\n57%\n36,5%\n≈20 p.p.\n\n\nRoosevelt\n43%\n60,8%\n≈18 p.p.\n\n\n\n\nCo poszło nie tak?\nOgromna liczba odpowiedzi nie pomogła, bo sondaż był obciążony systematycznym błędem, a nie błędem losowym.\n\n\n\nBłąd systematyczny a błąd losowy\nWyobraź sobie wagę łazienkową, która pokazuje zawsze +2,3 kg za dużo:\n\nBłąd losowy (bez stronniczości): za każdym razem stajesz trochę inaczej, więc waga pokazuje np. 68,0–68,5 kg. Średnia z wielu pomiarów da prawidłowy wynik (≈68 kg). Im więcej pomiarów, tym mniejsze wahania.\nBłąd systematyczny (stronniczość): waga jest źle wyzerowana i zawsze dodaje 2,3 kg. Nieważne, czy zważysz się raz, czy tysiąc razy — zawsze będzie ok. 70,3 kg, czyli dokładnie błędny wynik.\n\nTak właśnie było z Literary Digest: ich „instrument pomiarowy” — sposób zbierania opinii — był źle skalibrowany. Miliony błędnych odpowiedzi dały tylko fałszywe poczucie pewności.\n\n\n\nSkąd wziął się błąd?\nDwa różne źródła stronniczości działały w tym samym kierunku — na korzyść Alfa Landona:\n\nBłąd pokrycia (doboru) — kogo w ogóle można było objąć próbą\n\nWykorzystano listy: abonentów telefonów, właścicieli samochodów i prenumeratorów magazynu.\nW czasie Wielkiego Kryzysu te grupy były zamożniejsze niż przeciętny wyborca.\nSkutek: systematyczne niedoszacowanie wyborców o niższych dochodach, popierających Roosevelta.\n\nBłąd braku odpowiedzi (nonresponse bias) — kto zdecydował się odesłać ankietę\n\nOdpowiedziało tylko ok. co czwarte zaproszenie (≈24%).\nOsoby bardziej zaangażowane politycznie — częściej przeciwnicy Roosevelta — chętniej odpowiadały.\n\n\nOba błędy działały w tym samym kierunku, tworząc ogromne zniekształcenie, którego żadna wielkość próby nie mogła naprawić.\n\n\n\nDlaczego wielkość próby nie poprawiła oszacowania\nZebranie 2,4 miliona odpowiedzi z błędnej listy to jak zważyć cały kraj na wadze z błędną kalibracją.\n\nGdyby była to losowa próba, maksymalny teoretyczny margines błędu (zakładając 95% poziom ufności) wyniósłby: \\text{MoE}_{95\\%} \\approx 1.96\\sqrt{\\frac{0.25}{2{,}400{,}000}} \\approx \\pm 0.06 \\text{ percentage points} — malutki.\nWzór ten opisuje tylko błąd losowy, a nie stronniczość.\nRzeczywisty błąd sięgnął 18–20 punktów procentowych — kilkaset razy więcej.\n\nWniosek: Dokładność bez reprezentatywności nic nie znaczy. Ogromna, ale błędna próba może być gorsza niż mała, dobrze dobrana.\n\n\n\nWspółczesne sondaże: mniejsze, ale “mądrzejsze”\nProblem Literary Digest na zawsze zmienił metody badań opinii:\n\nDobór losowy (probability sampling): każdy wyborca ma znane, niezerowe prawdopodobieństwo znalezienia się w próbie.\nWażenie i kalibracja: koryguje nad- lub niedoreprezentację niektórych grup.\nPodejście „total survey error”: uwzględnia błędy pokrycia, braku odpowiedzi, pomiaru i przetwarzania, a nie tylko błąd losowy.\n\nSedno: liczy się nie to, ile osób zbadamy, lecz kogo i jak.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zrozumieć-losowość",
    "href": "rozdzial1.html#zrozumieć-losowość",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.4 Zrozumieć losowość",
    "text": "2.4 Zrozumieć losowość\nEksperyment losowy (random experiment) to dowolny proces, którego wyniku nie można przewidzieć z pewnością, na przykład rzut monetą lub kostką do gry.\nWynik (outcome) to pojedynczy możliwy rezultat tego eksperymentu — na przykład wypadnięcie „orła” lub wyrzucenie „5”.\nPrzestrzeń próbkowa (lub przestrzeń zdarzeń elementarnych) to zbiór wszystkich możliwych wyników eksperymentu losowego. Zazwyczaj oznaczana jest symbolem S lub Ω (omega).\nZdarzenie (event) to zbiór jednego lub więcej wyników, którymi jesteśmy zainteresowani; może to być zdarzenie elementarne (jak wyrzucenie dokładnie 3) lub zdarzenie złożone (jak wyrzucenie liczby parzystej, które obejmuje wyniki 2, 4 i 6).\n\nPrawdopodobieństwo (probability) to sposób mierzenia, jak prawdopodobne jest zajście czegoś. Jest to liczba między 0 a 1 (lub 0% a 100%), która reprezentuje szansę wystąpienia zdarzenia.\n\nRozkład prawdopodobieństwa to funkcja/reguła matematyczna opisująca prawdopodobieństwo wystąpienia różnych możliwych wyników w eksperymencie losowym.\nJeśli coś ma prawdopodobieństwo 0, jest niemożliwe — nigdy się nie wydarzy. Jeśli coś ma prawdopodobieństwo 1, jest pewne — na pewno się wydarzy. Większość rzeczy mieści się gdzieś pomiędzy.\nNa przykład, gdy rzucasz uczciwą monetą, prawdopodobieństwo wypadnięcia orła wynosi 0,5 (czyli 50%), ponieważ są dwa równie prawdopodobne wyniki, a orzeł jest jednym z nich.\nPrawdopodobieństwo pomaga nam nadać sens niepewności i losowości w świecie.\n\n\nW statystyce losowość (randomness) to uporządkowany sposób opisywania niepewności. Chociaż każdy pojedynczy wynik jest nieprzewidywalny, stabilne wzorce (mówiąc formalniej: rozkłady empiryczne wyników zbiegają do rozkładów prawdopodobieństwa) pojawiają się po wielu powtórzeniach.\n\nPrzykład: Rzut uczciwą monetą:\n\nPojedynczy rzut: Całkowicie nieprzewidywalny — nie wiesz, czy wypadnie orzeł czy reszka\n100 rzutów: Otrzymasz wynik bliski 50% orłów (może 48 lub 53)\n10 000 rzutów: Prawie na pewno bardzo blisko 50% orłów (być może 49,8%)\n\nTo samo dotyczy kostki: nie możesz przewidzieć następnego rzutu, ale rzuć 600 razy, a każda liczba (1-6) pojawi się około 100 razy. Ta przewidywalna długoterminowa regularność wynikająca z nieprzewidywalnych pojedynczych zdarzeń to esencja statystycznej losowości.\n\nRodzaje losowości\nLosowość epistemiczna a ontologiczna:\n\nLosowość epistemiczna (epistemic randomness) (wynikająca z niepełnej wiedzy): Traktujemy wynik jako losowy, ponieważ nie wszystkie czynniki determinujące są obserwowane lub warunki nie są kontrolowane. Sam system jest deterministyczny — podlega stałym regułom — ale brakuje nam informacji potrzebnych do przewidzenia wyniku.\n\nRzut monetą: Trajektoria monety jest całkowicie rządzona mechaniką klasyczną. Gdybyśmy znali dokładną pozycję początkową, siłę, moment pędu, opór powietrza i właściwości powierzchni, moglibyśmy teoretycznie przewidzieć, czy moneta wyląduje na orle czy reszce. „Losowość” istnieje tylko dlatego, że nie możemy zmierzyć tych warunków z wystarczającą precyzją.\nOdpowiedzi w sondażu: Odpowiedź danej osoby na pytanie ankietowe jest zdeterminowana przez jej przekonania, doświadczenia i kontekst, ale nie mamy dostępu do tego pełnego stanu psychologicznego, więc modelujemy to jako proces losowy.\nBłąd pomiaru: Ograniczona precyzja instrumentu oznacza, że „prawdziwa” wartość istnieje, ale obserwujemy ją z niepewnością.\n\nLosowość ontologiczna (ontological randomness) (wewnętrzna nieokreśloność): Nawet pełna znajomość wszystkich warunków nie usuwa niepewności co do wyniku. Losowość jest fundamentalna dla samej natury rzeczywistości, a nie tylko luką w naszej wiedzy.\n\nRozpad promieniotwórczy: Dokładny moment, w którym dany atom ulegnie rozpadowi, jest fundamentalnie nieprzewidywalny. Mechanika kwantowa podaje nam tylko rozkład prawdopodobieństwa, a nie dokładny czas.\nPomiary kwantowe: Wynik pomiaru pozycji lub spinu cząstki kwantowej jest z natury probabilistyczny, nie jest określony przez ukryte zmienne, których po prostu jeszcze nie odkryliśmy.\n\n\n\n\n\n\n\n\nParadoks rzutu monetą\n\n\n\nChociaż traktujemy rzuty monetą jako dające losowe wyniki 50-50, badania matematyka Persi Diaconisa wykazały, że przy użyciu mechanicznej maszyny do rzucania monetą, która precyzyjnie kontroluje warunki początkowe, można w sposób powtarzalny przechylić wynik w stronę wybranej strony. To potwierdza, że rzuty monetą są losowe epistemicznie, a nie ontologicznie — pozorna losowość wynika z naszej niezdolności do kontrolowania i mierzenia warunków, a nie z jakiejkolwiek fundamentalnej nieokreśloności w fizyce.\n\n\n\n\nPojęcia pokrewne\nLosowość a przypadkowość: Statystyczna losowość ma strukturę matematyczną i podlega prawom prawdopodobieństwa — jest to uporządkowana niepewność. Przypadkowość sugeruje kompletny nieład bez leżących u podstaw wzorców czy reguł.\nChaos deterministyczny (deterministic chaos): Punkt pośredni między doskonałą przewidywalnością a losowością. Chaos odnosi się do systemów deterministycznych (podlegających stałym, znanym regułom), które wykazują ekstremalną wrażliwość na warunki początkowe (sensitivity to initial conditions), co czyni przewidywanie długoterminowe niemożliwym w praktyce.\nPomyśl o chaosie jak o automacie do gry w flipera (pinball machine), z efektem motyla:\n\nZnasz wszystkie reguły doskonale — fizykę zderzeń, tarcie, grawitację\nSystem jest całkowicie deterministyczny: wypuść kulkę dokładnie z tego samego miejsca z dokładnie tą samą siłą, a otrzymasz dokładnie ten sam wynik za każdym razem\nAle: różnica 0,01 milimetra w pozycji startowej sprawia, że kulka uderza w inne odbijaki, co kumuluje się z każdym zderzeniem, aż finalny wynik jest zupełnie inny\nTo jest efekt motyla (butterfly effect): maleńkie zaburzenia w warunkach początkowych rosną wykładniczo w czasie\n\nKlasyczne przykłady chaosu deterministycznego:\n\nSystemy pogodowe: Edward Lorenz odkrył, że modele atmosferyczne są tak wrażliwe, że motyl trzepoczący skrzydłami w Brazylii mógłby teoretycznie zmienić to, czy tornado powstanie w Teksasie tygodnie później. Dlatego prognozy pogody są wiarygodne na dni, ale nie na miesiące.\nOrbity planet: Choć stabilne w skali ludzkiego życia, dynamika Układu Słonecznego jest chaotyczna w skali milionów lat. Nie możemy przewidzieć dokładnej pozycji planet w odległej przyszłości, mimo że znamy prawa grawitacji doskonale.\nPodwójne wahadło: Wypuść je pod nieznacznie innym kątem, a po kilku wahnięciach ruch staje się zupełnie inny.\n\nChaos a losowość epistemiczna — kluczowe rozróżnienie:\nOba wiążą się z nieprzewidywalnością wynikającą z ograniczonej wiedzy, ale różnią się w istotny sposób:\n\n\n\n\n\n\n\n\nAspekt\nLosowość epistemiczna\nChaos deterministyczny\n\n\n\n\nReguły znane?\nCzęsto tak\nTak, całkowicie\n\n\nStan obecny znany?\nNie (lub niedokładnie)\nNie (lub niedokładnie)\n\n\nCo powoduje nieprzewidywalność?\nBrakująca informacja o obecnym stanie\nWykładnicze wzmocnienie drobnych błędów pomiaru\n\n\nCzy doskonała informacja pomoże?\nTak — poznanie stanu eliminuje niepewność\nJedynie krótkoterminowo — błędy narastają ponownie\n\n\n\nPrzykład dla wyjaśnienia:\n\nLosowość epistemiczna (zakryta karta): Karta to już siódemka kier. Nie zmienia się ani nie ewoluuje. Po prostu nie wiesz jeszcze, która to karta. Odwróć ją, a niepewność znika całkowicie i na stałe.\nChaos (pogoda za 3 tygodnie): Nawet jeśli zmierzysz obecne warunki atmosferyczne z niezwykłą precyzją, drobne błędy (pomiar do 6 miejsc dziesiętnych zamiast 20) kumulują się w czasie. Możesz dobrze przewidywać przez 5 dni, ale w 3. tygodniu twoja prognoza jest bezużyteczna.\n\n\n\n\n\n\n\nIntuicja\n\n\n\nChaos jest deterministyczny, ale nieprzewidywalny. Losowość epistemiczna jest deterministyczna, ale nieznana. Losowość ontologiczna jest fundamentalnie niezdeterminowana. Praktyka statystyczna traktuje wszystkie trzy jako „losowe”, ale zrozumienie źródła nieprzewidywalności pomaga nam wiedzieć, kiedy więcej informacji może pomóc (epistemiczna), kiedy pomaga tymczasowo, ale nie długoterminowo (chaos), i kiedy nie może pomóc wcale (ontologiczna).\n\n\nEntropia (entropy): Miara nieuporządkowania lub niepewności w systemie. Wysoka entropia oznacza wysoką nieprzewidywalność lub wiele możliwych mikrostanów; niska entropia oznacza wysoki porządek i niską niepewność. W teorii informacji i statystyce entropia kwantyfikuje ilość niepewności w rozkładzie prawdopodobieństwa — bardziej rozproszone rozkłady mają wyższą entropię.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#populacje-i-próby",
    "href": "rozdzial1.html#populacje-i-próby",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.5 Populacje i próby",
    "text": "2.5 Populacje i próby\nZrozumienie rozróżnienia między populacjami a próbami jest kluczowe dla właściwej analizy statystycznej.\n\nPopulacja (Population)\nPopulacja to kompletny zbiór jednostek, obiektów lub pomiarów, o których chcemy wyciągnąć wnioski. Kluczowe słowo to „kompletny” — populacja obejmuje każdego pojedynczego członka grupy, którą badamy.\nPrzykłady populacji w demografii:\n\nWszyscy mieszkańcy Polski na dzień 1 stycznia 2024: Obejmuje każdą osobę mieszkającą w Polsce w tym konkretnym dniu — około 38 milionów osób.\nWszystkie urodzenia w Szwecji w 2023 roku: Każde dziecko urodzone w granicach Szwecji w tym roku kalendarzowym — około 100 000 urodzeń.\nWszystkie gospodarstwa domowe w Tokio: Każda jednostka mieszkalna, gdzie ludzie mieszkają, gotują i śpią — około 7 milionów gospodarstw.\nWszystkie zgony z powodu COVID-19 na świecie w 2020 roku: Każdy zgon, gdzie COVID-19 został wymieniony jako przyczyna — kilka milionów zgonów.\n\nPopulacje mogą być:\nSkończone (Finite): Mające policzalną liczbę członków (wszyscy obecni obywatele Polski, wszytkie gminy w Polsce w 2024 r.)\nNieskończone (Infinite): Teoretyczne lub niepoliczalnie duże (wszystkie możliwe przyszłe urodzenia, wszystkie możliwe rzuty monetą)\nStałe (Fixed): Zdefiniowane w określonym punkcie czasu (wszyscy mieszkańcy w dniu spisu)\nDynamiczne (Dynamic): Zmieniające się w czasie (populacja miasta doświadczająca urodzeń, zgonów i migracji codziennie)\n\n\nPróba (Sample)\nPróba to podzbiór populacji, który jest faktycznie obserwowany lub mierzony. Badamy próby, ponieważ badanie całych populacji jest często niemożliwe, niepraktyczne lub niepotrzebne.\nDlaczego używamy prób:\nPraktyczna niemożliwość: Wyobraź sobie testowanie każdej osoby w Chinach na obecność pewnej choroby. Zanim skończyłbyś testować 1,4 miliarda ludzi, sytuacja chorobowa całkowicie by się zmieniła, a niektórzy ludzie testowani wcześnie wymagaliby ponownego testowania.\nWzględy kosztowe: Amerykański spis powszechny z 2020 roku kosztował około 16 miliardów dolarów. Przeprowadzanie tak kompletnych wyliczeń często byłoby zbyt kosztowne.\nOgraniczenia czasowe: Decydenci często potrzebują informacji szybko. Badanie ankietowe 10 000 osób można ukończyć w ciągu tygodni, podczas gdy spis wymaga lat planowania, wykonania i przetwarzania.\nPomiar destrukcyjny: Niektóre pomiary niszczą to, co jest mierzone. Testowanie żywotności żarówek wymaga użycia prób.\nWiększa dokładność: Co zaskakujące, próby mogą czasem być dokładniejsze niż badania pełne. Z próbą można pozwolić sobie na lepsze szkolenie ankieterów, bardziej staranne zbieranie danych i dokładniejsze kontrole jakości.\nPrzykład próby vs. populacja:\nPowiedzmy, że chcemy poznać średnią wielkość gospodarstwa domowego w Warszawie:\n\nPopulacja: Wszystkie 800 000 gospodarstw domowych w Warszawie\nPodejście spisowe: Próba skontaktowania się z każdym gospodarstwem (drogie, czasochłonne, niektóre zostaną pominięte)\nPodejście próbkowe: Losowo wybrać 5000 gospodarstw, dokładnie zmierzyć ich wielkości i użyć tego do oszacowania średniej dla wszystkich gospodarstw\nWynik: Próba może znaleźć średnią 2,43 osób na gospodarstwo z marginesem błędu ±0,05, co oznacza, że jesteśmy pewni, że prawdziwa średnia populacji mieści się między 2,38 a 2,48\n\n\n\n\n\n\n\n\nPrzegląd Metod Doboru Próby\n\n\n\nDobór próby polega na wyborze podzbioru populacji w celu oszacowania jej charakterystyk. Operat losowania (lista, z której losujemy) powinien idealnie zawierać każdego członka dokładnie raz. Problemy operatu: niedobór pokrycia, nadmiar pokrycia, duplikacja i grupowanie.\n\nDobór Probabilistyczny (Możliwe Wnioskowanie Statystyczne)\n\nProsty Dobór Losowy (SRS): Każda możliwa próba o rozmiarze n ma równe prawdopodobieństwo wyboru (losowanie bez zwracania). Złoty standard metod probabilistycznych.\n\nDefinicja formalna: Każda z \\binom{N}{n} możliwych prób ma prawdopodobieństwo \\frac{1}{\\binom{N}{n}}.\nPrawdopodobieństwo włączenia jednostki:\n\nPytanie: W ilu próbach znajduje się konkretna osoba (np. student Jan)?\nJeśli Jan jest już w próbie (to ustalone), musimy dobrać jeszcze n-1 osób z pozostałych N-1 osób (wszyscy oprócz Jana).\nLiczba prób zawierających Jana: \\binom{N-1}{n-1}\nPrawdopodobieństwo:\n\n\nP(\\text{Jan w próbie}) = \\frac{\\text{próby z Janem}}{\\text{wszystkie próby}} = \\frac{\\binom{N-1}{n-1}}{\\binom{N}{n}} = \\frac{n}{N}\n\nPrzykład liczbowy: N=5 osób {A,B,C,D,E}, losujemy n=3. Wszystkie próby: \\binom{5}{3}=10. Próby z osobą A: {ABC, ABD, ABE, ACD, ACE, ADE} = \\binom{4}{2}=6 prób. Prawdopodobieństwo: 6/10 = 3/5 = n/N ✓\n\nDobór Systematyczny: Wybór co k-tego elementu, gdzie k = N/n (interwał próbkowania).\n\nJak to działa: Losujemy punkt startowy r z \\{1, 2, ..., k\\}, następnie wybieramy: r, r+k, r+2k, r+3k, ...\nPrzykład: N=1000, n=100, więc k=10. Jeśli r=7, to wybieramy: 7, 17, 27, 37, …, 997.\nZalety: Bardzo prosty, zapewnia równomierne pokrycie populacji.\nProblem periodyczności: Jeśli lista ma wzorzec powtarzający się co k elementów, próba może być silnie obciążona.\n\nPrzykład (źle): Lista mieszkań: 101, 102, 103, 104 (narożne), 201, 202, 203, 204 (narożne), … Jeśli k=4, możemy wylosować tylko mieszkania narożne!\nPrzykład (źle): Dane produkcyjne dzienne z 7-dniowym cyklem. Jeśli k=7, możemy wylosować tylko poniedziałki.\nPrzykład (dobrze): Lista alfabetyczna nazwisk - zwykle brak periodyczności.\n\n\nDobór Warstwowy: Podział populacji na jednorodne warstwy (np. płeć, region), niezależne losowanie w każdej warstwie. Zapewnia reprezentację podgrup i może znacznie zwiększyć precyzję. Typy alokacji: proporcjonalna, optymalna (Neymana) lub równa.\nDobór Klastrowy (Skupieniowy): Wybór całych grup (klastrów) zamiast pojedynczych jednostek. Efektywny kosztowo dla populacji rozproszonych geograficznie (np. losowanie szkół zamiast uczniów), ale zazwyczaj mniej precyzyjny niż SRS (efekt schematu: DEFF = Wariancja(klaster)/Wariancja(SRS)). Może być jedno- lub wielostopniowy.\n\n\n\nDobór Nieprobabilistyczny (Ograniczone Wnioskowanie Statystyczne)\n\nDobór Dogodny: Wybór według łatwości dostępu (np. przechodnie w centrum miasta). Przydatny w badaniach pilotażowych/eksploracyjnych, ale prawdopodobne poważne obciążenie selekcji.\nDobór Celowy/Ekspercki: Świadomy wybór przypadków typowych, ekstremalnych lub bogatych informacyjnie. Wartościowy w badaniach jakościowych i badaniu rzadkich populacji.\nDobór Kwotowy: Dopasowanie proporcji populacji (np. 50% kobiet), ale bez losowego wyboru. Szybki i tani, ale ukryte obciążenie selekcji i brak możliwości obliczenia błędu próbkowania.\nDobór Kuli Śnieżnej: Uczestnicy rekrutują innych ze swoich sieci. Niezbędny dla trudno dostępnych populacji (osoby używające narkotyków, nielegalni imigranci), ale obciążony w stronę dobrze połączonych jednostek.\n\nPodstawowa Zasada: Dobór probabilistyczny umożliwia prawidłowe wnioskowanie statystyczne i obliczenie błędu próbkowania; metody nieprobabilistyczne mogą być konieczne ze względów praktycznych lub etycznych, ale ograniczają możliwość uogólnienia wyników na całą populację.\n\n\n\n\n\n\n\n\n\n\nSuperpopulacja i Proces Generowania Danych (DGP) (*)\n\n\n\n\n\nSuperpopulacja (Superpopulation)\nSuperpopulacja to teoretyczna nieskończona populacja, z której twoja skończona populacja jest traktowana jako jedna losowa próba.\nPomyśl o tym w trzech poziomach:\n\nSuperpopulacja: Nieskończony zbiór możliwych wartości (teoretyczny)\nPopulacja skończona (finite population): Rzeczywista populacja, którą teoretycznie możesz spisać (np. wszystkie 50 stanów USA, wszystkie 10 000 firm w branży)\nPróba (sample): Podzbiór, który faktycznie obserwujesz (np. 30 stanów, 500 firm)\n\nDlaczego potrzebujemy tego pojęcia?\nRozważmy 50 stanów USA. Możesz zmierzyć stopę bezrobocia dla wszystkich 50 stanów — pełny spis, bez próbkowania. Ale nadal chcesz:\n\nSprawdzić, czy bezrobocie jest powiązane z poziomem wykształcenia\nPrzewidzieć przyszłoroczne stopy bezrobocia\nOkreślić, czy różnice między stanami są „istotne statystycznie”\n\nBez koncepcji superpopulacji utkniesz — masz wszystkie dane, więc co pozostaje do wnioskowania? Odpowiedź: traktuj tegoroczne 50 wartości jako jedno losowanie z nieskończonej superpopulacji możliwych wartości, które mogłyby wystąpić w podobnych warunkach.\nReprezentacja matematyczna:\n\nWartość populacji skończonej: Y_i (stopa bezrobocia stanu i)\nModel superpopulacji: Y_i = \\mu + \\epsilon_i gdzie \\epsilon_i \\sim (0, \\sigma^2)\n50 zaobserwowanych wartości to jedna realizacja tego procesu\n\n\n\n\nProces Generowania Danych (Data Generating Process): Prawdziwa Recepta\nProces Generowania Danych (DGP) to rzeczywisty mechanizm, który tworzy twoje dane — włączając wszystkie czynniki, relacje i elementy losowe.\nIntuicyjny przykład: Załóżmy, że wyniki testów uczniów są naprawdę generowane przez:\n\\text{Wynik}_i = 50 + 2(\\text{GodzinyNauki}_i) + 3(\\text{GodzinySnu}_i) - 5(\\text{Stres}_i) + 1.5(\\text{Śniadanie}_i) + \\epsilon_i\nTo jest PRAWDZIWY DGP. Ale ty tego nie wiesz! Możesz estymować:\n\\text{Wynik}_i = \\alpha + \\beta(\\text{GodzinyNauki}_i) + u_i\nTwój model jest prostszy niż rzeczywistość. Brakuje ci zmiennych (sen, stres, śniadanie), więc twoje oszacowania mogą być obciążone (biased). Składnik u_i zawiera wszystko, co pominąłeś.\nIntuicja: Nigdy nie znamy prawdziwego DGP. Nasze modele statystyczne są zawsze przybliżeniami, próbującymi uchwycić najważniejsze części nieznanej, złożonej prawdy.\n\n\n\nDwa Podejścia do Wnioskowania Statystycznego\nAnalizując dane, szczególnie z badań czy prób, możemy przyjąć dwa filozoficzne podejścia:\n\n1. Wnioskowanie Oparte na Schemacie (Design-Based Inference)\n\nFilozofia: Wartości populacji są stałymi liczbami. Losowość pochodzi TYLKO z tego, które jednostki wylosowaliśmy.\nSkupienie: Jak wybraliśmy próbę (losowanie proste, warstwowe, gniazdowe itp.)\nPrzykład: Średni dochód hrabstw Kalifornii jest stałą liczbą. Losujemy 10 hrabstw. Nasza niepewność wynika z tego, które 10 losowo wybraliśmy.\nBez modeli: Nie zakładamy nic o rozkładzie wartości populacji\n\n\n\n2. Wnioskowanie Oparte na Modelu (Model-Based Inference)\n\nFilozofia: Same wartości populacji są realizacjami z pewnego modelu probabilistycznego (superpopulacji)\nSkupienie: Model statystyczny generujący wartości populacji\nPrzykład: Dochód każdego hrabstwa Kalifornii jest losowany z: Y_i = \\mu + \\epsilon_i gdzie \\epsilon_i \\sim N(0, \\sigma^2)\nWymagane modele: Przyjmujemy założenia o tym, jak dane zostały wygenerowane\n\nKtóre jest lepsze?\n\nDuże populacje, dobre próby losowe: Podejście oparte na schemacie działa dobrze\nMałe populacje (jak 50 stanów): Często konieczne podejście modelowe\nPełne spisanie: Tylko podejście modelowe umożliwia wnioskowanie\nWspółczesna praktyka: Często łączy oba podejścia\n\n\n\n\n\nPraktyczny Przykład: Analiza Wydatków Stanowych na Edukację\nZałóżmy, że zbierasz wydatki na edukację per uczeń dla wszystkich 50 stanów USA.\nBez myślenia superpopulacyjnego:\n\nMasz wszystkie 50 wartości — to wszystko\nŚrednia to średnia, bez niepewności\nNie możesz testować hipotez ani tworzyć prognoz\n\nZ myśleniem superpopulacyjnym:\n\nTegoroczne 50 wartości to jedna realizacja z superpopulacji\nModel: \\text{Wydatki}_i = \\mu + \\beta(\\text{DochódStanu}_i) + \\epsilon_i\nTeraz możesz:\n\nTestować, czy wydatki są powiązane z dochodem stanu (\\beta \\neq 0?)\nPrzewidywać przyszłoroczne wartości\nObliczać przedziały ufności\n\n\nIntuicja: Nawet z kompletnymi danymi, ramy superpopulacji umożliwiają wnioskowanie statystyczne poprzez traktowanie obserwowanych wartości jako jednego możliwego wyniku z podstawowego procesu stochastycznego.\n\n\n\nPodsumowanie\n\nSuperpopulacja: Traktuje twoją populację skończoną jako jedno losowanie z nieskończonej przestrzeni możliwości — niezbędne, gdy twoja populacja skończona jest mała lub całkowicie obserwowana\nDGP: Prawdziwy (nieznany) proces tworzący twoje dane — twoje modele próbują go przybliżyć",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#dane-rozkład-danych-rozkład-cechyzmiennej-typologie-danych-zmiennych",
    "href": "rozdzial1.html#dane-rozkład-danych-rozkład-cechyzmiennej-typologie-danych-zmiennych",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.6 Dane, rozkład danych (rozkład cechy/zmiennej), typologie danych (zmiennych)",
    "text": "2.6 Dane, rozkład danych (rozkład cechy/zmiennej), typologie danych (zmiennych)\nCzym są dane?\nDane to zbiór faktów, obserwacji lub pomiarów, które gromadzimy, aby odpowiedzieć na pytania lub zrozumieć zjawiska. W statystyce i analizie danych, dane reprezentują informacje w ustrukturyzowanym formacie, który można analizować.\nPunkty danych\nPunkt danych to pojedyncza obserwacja lub pomiar w zbiorze danych. Na przykład, jeśli zmierzymy wzrost 5 uczniów, każdy pojedynczy pomiar wzrostu jest punktem danych.\nZmienne\nZmienna to cecha lub atrybut, który może przyjmować różne wartości w obserwacjach. Zmienne mogą być:\n\nKategoryczne (np. kolor, płeć, kraj)\nNumeryczne (np. wiek, temperatura, dochód)\n\nRozkład danych\nRozkład danych opisuje, jakie wartości przyjmuje zmienna i jak często każda wartość występuje w zbiorze danych. Zrozumienie rozkładu pomaga nam dostrzec wzorce, tendencje centralne i zmienność w naszych danych.\nTabele rozkładu częstości\nTabela rozkładu częstości organizuje dane, pokazując każdą unikalną wartość (lub zakres wartości) oraz liczbę wystąpień:\n\n\n\nWartość\nCzęstość\nCzęstość względna\n\n\n\n\nA\n15\n0,30 (30%)\n\n\nB\n25\n0,50 (50%)\n\n\nC\n10\n0,20 (20%)\n\n\nSuma\n50\n1,00 (100%)\n\n\n\nTa tabela pozwala nam szybko zobaczyć, które wartości są najczęstsze i zrozumieć ogólny wzorzec rozkładu.\n\n\n\n\n\n\n\nRodzaje i Formaty Zbiorów Danych\n\n\n\n\nDane Przekrojowe\nObserwacje na zmiennych (kolumny w bazie danych) zebrane w jednym punkcie czasowym dla wielu podmiotów:\n\n\n\nOsoba\nWiek\nDochód\nWykształcenie\n\n\n\n\n1\n25\n5000\nLicencjat\n\n\n2\n35\n7500\nMagister\n\n\n3\n45\n9000\nDoktorat\n\n\n\n\n\nSzeregi Czasowe\nObserwacje jednego podmiotu w kolejnych punktach czasowych:\n\n\n\nRok\nPKB (w mld)\nStopa Bezrobocia\n\n\n\n\n2018\n20.580\n3,9%\n\n\n2019\n21.433\n3,7%\n\n\n2020\n20.933\n8,1%\n\n\n\n\n\nDane Panelowe (Longitudinalne)\nObserwacje wielu podmiotów w czasie:\n\n\n\nKraj\nRok\nPKB per capita\nDługość życia\n\n\n\n\nPolska\n2018\n32.794\n76,7\n\n\nPolska\n2019\n35.118\n76,8\n\n\nNiemcy\n2018\n46.194\n81,9\n\n\nNiemcy\n2019\n46.194\n82,0\n\n\n\n\n\nDane Przekrojowo-Czasowe (TSCS)\nSzczególny przypadek danych panelowych gdzie:\n\nLiczba punktów czasowych &gt; liczba podmiotów\nStruktura podobna do danych panelowych\nCzęsto stosowane w ekonomii i politologii\n\n\n\n\nFormaty Danych\n\nFormat Szeroki\nKażdy wiersz to podmiot; kolumny to zmienne/punkty czasowe:\n\n\n\nKraj\nPKB_2018\nPKB_2019\nDŻ_2018\nDŻ_2019\n\n\n\n\nPolska\n32.794\n35.118\n76,7\n76,8\n\n\nNiemcy\n46.194\n46.194\n81,9\n82,0\n\n\n\n\n\nFormat Długi\nKażdy wiersz to unikalna kombinacja podmiot-czas-zmienna:\n\n\n\nKraj\nRok\nZmienna\nWartość\n\n\n\n\nPolska\n2018\nPKB per capita\n32.794\n\n\nPolska\n2019\nPKB per capita\n35.118\n\n\nPolska\n2018\nDługość życia\n76,7\n\n\nPolska\n2019\nDługość życia\n76,8\n\n\nNiemcy\n2018\nPKB per capita\n46.194\n\n\nNiemcy\n2019\nPKB per capita\n46.194\n\n\nNiemcy\n2018\nDługość życia\n81,9\n\n\nNiemcy\n2019\nDługość życia\n82,0\n\n\n\nUwaga: Format długi jest zazwyczaj preferowany do:\n\nManipulacji danymi w R i Pythonie\nAnaliz statystycznych\nWizualizacji danych\n\n\n\n\n\n\n\nZrozumienie typów danych i rozkładów jest fundamentalne dla wyboru odpowiednich analiz i poprawnej interpretacji wyników.\n\n\nTypy danych\nDane składają się z zebranych obserwacji lub pomiarów. Typ danych określa, jakie operacje matematyczne są wykonalne i jakie metody statystyczne mają zastosowanie.\n\nDane ilościowe\nDane ciągłe mogą przyjmować dowolną wartość w przedziale:\nPrzykłady:\n\nWiek: Może wynosić 25,5 lat, 25,51 lat, 25,514 lat (precyzja ograniczona tylko dokładnością narzędzia pomiarowego)\nWskaźnik masy ciała: 23,7 kg/m²\nWspółczynnik dzietności: 1,73 dzieci na kobietę\nGęstość zaludnienia: 4521,3 osoby na km²\nFrekwencja wyborcza: 60%\n\nWłaściwości:\n\nMożna wykonywać wszystkie operacje arytmetyczne\nMożna obliczać średnie, odchylenia standardowe\n\nDane dyskretne mogą przyjmować tylko określone wartości:\nPrzykłady:\n\nLiczba dzieci: 0, 1, 2, 3… (nie można mieć 2,5 dziecka)\nLiczba małżeństw: 0, 1, 2, 3…\nWielkość gospodarstwa domowego: 1, 2, 3, 4… osób\nLiczba wizyt u lekarza: 0, 1, 2, 3… rocznie\nWielkość okręgu wyborczego: 1, 2, 3, …\n\n\n\nDane jakościowe/kategorialne\nDane nominalne reprezentują kategorie bez naturalnego porządku:\nPrzykłady:\n\nKraj urodzenia: USA, Chiny, Indie, Brazylia…\nReligia: Chrześcijaństwo, Islam, Hinduizm, Buddyzm, Brak…\nStan cywilny: Kawaler/Panna, Żonaty/Mężatka, Rozwiedziony/a, Wdowiec/Wdowa\nPrzyczyna śmierci: Choroby serca, Rak, Wypadek, Udar…\nGrupa krwi: A, B, AB, 0\n\nCo możemy zrobić:\n\nLiczyć częstości\nObliczać proporcje\nZnaleźć dominantę\n\nCzego nie możemy zrobić:\n\nObliczać średniej (średnia religia nie ma sensu)\nPorządkować kategorii\nObliczać odległości między kategoriami\n\nDane porządkowe reprezentują uporządkowane kategorie:\nPrzykłady:\n\nPoziom wykształcenia: Brak &lt; Podstawowe &lt; Średnie &lt; Wyższe\nStatus społeczno-ekonomiczny: Niski &lt; Średni &lt; Wysoki\nSamoocena zdrowia: Zły &lt; Przeciętny &lt; Dobry &lt; Doskonały\nSkala zgody: Zdecydowanie się nie zgadzam &lt; Nie zgadzam się &lt; Neutralny &lt; Zgadzam się &lt; Zdecydowanie się zgadzam\n\nUwaga: Interwały między kategoriami niekoniecznie są równe. „Odległość” od Złego do Przeciętnego zdrowia może nie równać się odległości od Dobrego do Doskonałego.\n\n\nCzęstość, Częstość Względna i Gęstość\nAnalizując dane, często interesuje nas, ile razy pojawia się każda wartość (lub przedział wartości). Prowadzi nas to do trzech powiązanych pojęć:\nCzęstość (bezwzględna) (frequency) to po prostu liczba wystąpień danej wartości lub kategorii w naszych danych. Jeśli 15 studentów uzyskało wyniki między 70-80 punktów na egzaminie, częstość dla tego przedziału wynosi 15.\nCzęstość względna (relative frequency) wyraża częstość jako proporcję lub procent całości. Odpowiada na pytanie: “Jaka część wszystkich obserwacji należy do tej kategorii?” Częstość względna obliczana jest jako:\n\\text{Częstość względna} = \\frac{\\text{Częstość}}{\\text{Całkowita liczba obserwacji}}\nJeśli 15 ze 100 studentów uzyskało 70-80 punktów, częstość względna wynosi 15/100 = 0,15 lub 15%. Częstości względne zawsze sumują się do 1 (lub 100%), co czyni je użytecznymi do porównywania rozkładów o różnych liczebnościach próby.\n\n\n\n\n\n\nTip\n\n\n\nPrawdopodobieństwo zdarzenia to liczba z przedziału od 0 do 1; im większe prawdopodobieństwo, tym bardziej prawdopodobne jest wystąpienie zdarzenia.\n\n\nGęstość (prawdopodobieństwo na jednostkę długości) mierzy, jak bardzo obserwacje są skoncentrowane na jednostkę pomiaru. Kiedy grupujemy dane ciągłe (takie jak czas lub stopa bezrobocia) w przedziały o różnych szerokościach, potrzebujemy gęstości, aby zapewnić uczciwe porównanie—szersze przedziały naturalnie zawierają więcej obserwacji po prostu dlatego, że są szersze, a nie dlatego, że wartości są tam bardziej skoncentrowane. Gęstość oblicza się jako:\n\\text{Gęstość} = \\frac{\\text{Częstość względna}}{\\text{Szerokość przedziału}}\nTa standaryzacja pozwala na uczciwe porównanie między przedziałami—szersze przedziały nie wydają się sztucznie ważniejsze tylko dlatego, że są szersze.\n\nGęstość jest szczególnie ważna dla zmiennych ciągłych, ponieważ zapewnia, że całkowite pole pod rozkładem równa się 1, co pozwala nam interpretować pola jako prawdopodobieństwa.\n\nCzęstość skumulowana (cumulative frequency) mówi nam, ile obserwacji znajduje się na danym poziomie lub poniżej niego.\nZamiast pytać “ile obserwacji jest w tej kategorii?”, częstość skumulowana odpowiada na pytanie “ile obserwacji jest w tej kategorii lub w kategoriach poniżej?”. Obliczana jest przez sumowanie wszystkich częstości od najniższej wartości do bieżącej wartości włącznie.\nPodobnie, częstość względna skumulowana (cumulative relative frequency) wyraża to jako proporcję całości, odpowiadając na pytanie “jaki procent obserwacji znajduje się na tym poziomie lub poniżej?”. Na przykład, jeśli częstość względna skumulowana dla wyniku 70 wynosi 0,40, oznacza to, że 40% studentów uzyskało wynik 70 lub niższy.\n\n\nTablice Rozkładu (szereg rozdzielczy danych)\nTablica rozkładu częstości (frequency distribution table) organizuje dane, pokazując jak obserwacje rozkładają się między różnymi wartościami lub przedziałami. Oto przykład z wynikami egzaminów:\n\n\n\n\n\n\n\n\n\n\n\nPrzedział wyników\nCzęstość\nCzęstość względna\nCzęstość skumulowana\nCzęstość względna skumulowana\nGęstość\n\n\n\n\n0-50\n10\n0,10\n10\n0,10\n0,002\n\n\n50-70\n30\n0,30\n40\n0,40\n0,015\n\n\n70-90\n45\n0,45\n85\n0,85\n0,0225\n\n\n90-100\n15\n0,15\n100\n1,00\n0,015\n\n\nSuma\n100\n1,00\n-\n-\n-\n\n\n\nTa tablica pokazuje, że większość studentów uzyskała wyniki w przedziale 70-90, podczas gdy bardzo niewielu uzyskało wyniki poniżej 50 lub powyżej 90. Kolumny skumulowane pokazują nam, że 40% studentów uzyskało wyniki poniżej 70, a 85% poniżej 90.\nTakie tablice są użyteczne dla szybkiego przeglądu danych przed przeprowadzeniem bardziej złożonych analiz.\n\n\nWizualizacja Rozkładów: Histogramy\nHistogram to graficzna reprezentacja rozkładu częstości. Wyświetla dane używając słupków, gdzie:\n\nOś x pokazuje wartości lub przedziały (klasy, bins)\nOś y może pokazywać częstość, częstość względną lub gęstość\nWysokość każdego słupka reprezentuje liczbę, proporcję lub gęstość dla danego przedziału\nSłupki stykają się ze sobą (brak przerw) dla zmiennych ciągłych\n\nWybór szerokości klas: Liczba i szerokość klas znacząco wpływa na wygląd histogramu. Zbyt mało klas ukrywa ważne wzorce, podczas gdy zbyt wiele klas tworzy “szum” i utrudnia dostrzeżenie wzorców.\n\nW statystyce szum (noise) to niepożądana losowa zmienność, która przesłania wzorzec, który staramy się znaleźć. Można to porównać do trzasków w radiu — utrudniają one słyszenie muzyki (“sygnału”). W danych szum pochodzi z błędów pomiarowych, losowych fluktuacji lub naturalnej zmienności badanego zjawiska. Szum to losowa zmienność w danych, która ukrywa prawdziwe wzorce, które chcemy dostrzec, podobnie jak hałas w tle utrudnia usłyszenie rozmowy.\n\nKilka metod pomaga określić odpowiednie szerokości klas (*):\n\nReguła Sturgesa (Sturges’ rule): Użyj k = 1 + \\log_2(n) klas, gdzie n to liczebność próby. Działa dobrze dla w przybliżeniu symetrycznych rozkładów.\nReguła pierwiastka kwadratowego (square root rule): Użyj k = \\sqrt{n} klas. Proste, domyślne ustawienie działające w wielu przypadkach wystarczająco dobrze.\n\nW R możesz określić klasy na kilka sposobów:\n\n# Generate exam scores data\nset.seed(123)  # For reproducibility\nexam_scores &lt;- c(\n  rnorm(80, mean = 75, sd = 12),  # Most students cluster around 75\n  runif(15, 50, 65),               # Some lower performers\n  runif(5, 85, 95)                 # A few high achievers\n)\n\n# Keep scores within valid range (0-100)\nexam_scores &lt;- pmin(pmax(exam_scores, 0), 100)\n\n# Round to whole numbers\nexam_scores &lt;- round(exam_scores)\n\n# Określenie liczby klas\nhist(exam_scores, breaks = 10)\n\n\n\n\n\n\n\n# Określenie dokładnych punktów podziału\nhist(exam_scores, breaks = seq(0, 100, by = 10))\n\n\n\n\n\n\n\n# Pozwól R wybrać automatycznie (domyślnie używa reguły Sturgesa)\nhist(exam_scores)\n\n\n\n\n\n\n\n\nNajlepszym podejściem jest często eksperymentowanie z różnymi szerokościami klas, aby znaleźć to, co najlepiej ujawnia wzorzec w danych. Zacznij od ustawienia domyślnego, następnie spróbuj mniej i więcej klas, aby zobaczyć, jak zmienia się obraz.\nDefiniowanie granic klas: Tworząc klasy dla tablicy częstości, musisz zdecydować, jak obsługiwać wartości, które dokładnie przypadają na granice przedziałów klasowych. Na przykład, jeśli masz klasy 0-10 i 10-20, do której klasy należy wartość 10?\nRozwiązaniem jest użycie notacji przedziałowej (interval notation), aby określić, czy każda granica jest włączona czy wyłączona:\n\nPrzedział domknięty (closed interval) [a, b] zawiera oba końce: a \\leq x \\leq b\nPrzedział otwarty (open interval) (a, b) wyklucza oba końce: a &lt; x &lt; b\nPrzedział lewostronnie domknięty (half-open interval) [a, b) zawiera lewy koniec, ale wyklucza prawy: a \\leq x &lt; b\nPrzedział prawostronnie domknięty (half-open interval) (a, b] wyklucza lewy koniec, ale zawiera prawy: a &lt; x \\leq b\n\nStandardowa konwencja: Większość oprogramowania statystycznego, włączając R, używa przedziałów lewostronnie domkniętych [a, b) dla wszystkich klas oprócz ostatniej, która jest w pełni domknięta [a, b]. Oznacza to:\n\nWartość na dolnej granicy jest włączona do klasy\nWartość na górnej granicy należy do następnej klasy\nSama ostatnia klasa zawiera obie granice, aby uchwycić wartość maksymalną\n\nNa przykład, dla klas 0-20, 20-40, 40-60, 60-80, 80-100:\n\n\n\nPrzedział wyników\nNotacja przedziałowa\nZawarte wartości\n\n\n\n\n0-20\n[0, 20)\n0 ≤ wynik &lt; 20\n\n\n20-40\n[20, 40)\n20 ≤ wynik &lt; 40\n\n\n40-60\n[40, 60)\n40 ≤ wynik &lt; 60\n\n\n60-80\n[60, 80)\n60 ≤ wynik &lt; 80\n\n\n80-100\n[80, 100]\n80 ≤ wynik ≤ 100\n\n\n\nTa konwencja zapewnia, że:\n\nKażda wartość jest liczona dokładnie raz (bez podwójnego liczenia)\nŻadne wartości nie przepadają\nKlasy w pełni pokrywają cały zakres\n\nPrzedstawiając tablice częstości w raportach, możesz po prostu napisać “0-20, 20-40, …” i zaznaczyć, że klasy są lewostronnie domknięte, lub jawnie pokazać notację przedziałową, jeśli precyzja jest ważna.\nHistogram częstości pokazuje surowe liczebności:\n\n# Przykład kodu R\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     main = \"Rozkład wyników egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Częstość\",\n     col = \"lightblue\")\n\n\n\n\n\n\n\n\nHistogram częstości względnej pokazuje proporcje (użyteczne przy porównywaniu grup o różnych liczebnościach):\n\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     freq = FALSE,  # Tworzy histogram częstości względnej/gęstości\n     main = \"Rozkład wyników egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Częstość względna\",\n     col = \"lightgreen\")\n\n\n\n\n\n\n\n\nHistogram gęstości dostosowuje się do szerokości przedziałów i jest używany z krzywymi gęstości:\n\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     freq = FALSE,  # Tworzy skalę gęstości\n     main = \"Rozkład wyników egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Gęstość\",\n     col = \"lightcoral\")\n\n\n\n\n\n\n\n\n\n\nKrzywe Gęstości\nKrzywa gęstości (density curve) to wygładzona linia, która przybliża/modeluje kształt rozkładu. W przeciwieństwie do histogramów, które pokazują rzeczywiste dane w dyskretnych klasach, krzywe gęstości pokazują ogólny wzorzec jako funkcję ciągłą. Pole pod całą krzywą zawsze równa się 1, a pole pod dowolną częścią krzywej reprezentuje proporcję obserwacji w tym zakresie.\n\n# Dodawanie krzywej gęstości do histogramu\nhist(exam_scores, \n     freq = FALSE,\n     main = \"Wyniki egzaminacyjne z krzywą gęstości\",\n     xlab = \"Wynik\",\n     ylab = \"Gęstość\",\n     col = \"lightblue\",\n     border = \"white\")\nlines(density(exam_scores), \n      col = \"darkred\", \n      lwd = 2)\n\n\n\n\n\n\n\n\nKrzywe gęstości są szczególnie użyteczne do:\n\nIdentyfikacji kształtu rozkładu (symetryczny, skośny, bimodalny)\nPorównywania wielu rozkładów na tym samym wykresie\nZrozumienia teoretycznego (“prawdziwego”) rozkładu leżącego u podstaw danych\n\n\n\n\n\n\n\nTip\n\n\n\nW statystyce percentyl (percentile) wskazuje względną pozycję punktu danych w zbiorze, pokazując procent obserwacji, które znajdują się na tym poziomie lub poniżej. Na przykład, jeśli student uzyskał wynik na 90. percentylu w teście, jego wynik jest równy lub wyższy niż 90% wszystkich innych wyników.\nKwartyle (quartiles) to specjalne percentyle, które dzielą dane na cztery równe części: pierwszy kwartyl (Q1, 25. percentyl), drugi kwartyl (Q2, 50. percentyl, czyli mediana), i trzeci kwartyl (Q3, 75. percentyl). Jeśli Q1 = 65 punktów, oznacza to, że 25% studentów uzyskało 65 punktów lub mniej.\nBardziej ogólnie, kwantyle (quantiles) to wartości, które dzielą dane na grupy o równej liczebności — percentyle dzielą na 100 części, kwartyle na 4 części, decyle (deciles) na 10 części, itp.\n\n\n\n\nWizualizacja Częstości Skumulowanej (*)\nWykresy częstości skumulowanej, zwane także ogiwami (ogives, wymawiane “oh-dżajw”), pokazują jak częstości kumulują się w zakresie wartości. Te wykresy używają linii zamiast słupków i zawsze rosną od lewej do prawej, ostatecznie osiągając całkowitą liczbę obserwacji (dla częstości skumulowanej) lub 1,0 (dla częstości względnej skumulowanej).\nWykresy częstości skumulowanej są wykorzytywane do:\n\nWizualnego odnajdywania percentyli i kwartyli\nOkreślania, jaka proporcja danych znajduje się poniżej lub powyżej określonej wartości\nPorównywania rozkładów różnych grup\n\n\n# Tworzenie danych częstości skumulowanej\nscore_breaks &lt;- seq(0, 100, by = 10)\nfreq_counts &lt;- hist(exam_scores, breaks = score_breaks, plot = FALSE)$counts\ncumulative_freq &lt;- cumsum(freq_counts)\n\n# Wykres częstości skumulowanej\nplot(score_breaks[-1], cumulative_freq,\n     type = \"b\",  # zarówno punkty, jak i linie\n     main = \"Częstość skumulowana wyników egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Częstość skumulowana\",\n     col = \"darkblue\",\n     lwd = 2,\n     pch = 19)\ngrid()\n\n\n\n\n\n\n\n\nDla częstości względnej skumulowanej (która jest częściej używana):\n\n# Częstość względna skumulowana\ncumulative_rel_freq &lt;- cumulative_freq / length(exam_scores)\n\nplot(score_breaks[-1], cumulative_rel_freq,\n     type = \"b\",\n     main = \"Częstość względna skumulowana wyników egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Częstość względna skumulowana\",\n     col = \"darkred\",\n     lwd = 2,\n     pch = 19,\n     ylim = c(0, 1))\ngrid()\nabline(h = c(0.25, 0.5, 0.75), lty = 2, col = \"gray\")  # Linie kwartyli\n\n\n\n\n\n\n\n\nKrzywa częstości względnej skumulowanej ułatwia odczytywanie percentyli. Na przykład, jeśli narysujesz linię poziomą na 0,75 i zobaczysz, gdzie przecina krzywą, odpowiadająca wartość x to 75. percentyl — wynik, poniżej którego znajduje się 75% studentów.\n\n\n\nRozkłady Dyskretne a Ciągłe\nTyp zmiennej, którą analizujesz, określa sposób wizualizacji jej rozkładu:\nRozkłady dyskretne (discrete distributions) stosują się do zmiennych, które mogą przyjmować tylko określone, policzalne wartości. Przykłady obejmują liczbę dzieci w rodzinie (0, 1, 2, 3…), liczbę skarg klientów dziennie lub odpowiedzi na 5-stopniowej skali Likerta.\nDla danych dyskretnych zazwyczaj używamy:\n\nWykresów słupkowych (z przerwami między słupkami) zamiast histogramów\nCzęstości lub częstości względnej na osi y\nKażda odrębna wartość otrzymuje własny słupek\n\n\n# Przykład: Liczba dzieci w rodzinie\nchildren &lt;- c(0, 1, 2, 2, 1, 3, 0, 2, 1, 4, 2, 1, 0, 2, 3)\nbarplot(table(children),\n        main = \"Rozkład liczby dzieci\",\n        xlab = \"Liczba dzieci\",\n        ylab = \"Częstość\",\n        col = \"skyblue\")\n\n\n\n\n\n\n\n\nRozkłady ciągłe (continuous distributions) stosują się do zmiennych, które mogą przyjmować dowolną wartość w zakresie. Przykłady obejmują temperaturę, czas reakcji, wzrost lub procent frekwencji.\nDla danych ciągłych używamy:\n\nHistogramów (ze stykającymi się słupkami), które grupują dane w przedziały\nKrzywych gęstości, aby pokazać wygładzony wzorzec\nGęstości na osi y przy używaniu krzywych gęstości\n\n\n# Generate response time data (in seconds)\nset.seed(456)  # For reproducibility\nresponse_time &lt;- rgamma(200, shape = 2, scale = 1.5)\n\n# Przykład: Rozkład czasu reakcji\nhist(response_time, \n     breaks = 15,\n     freq = FALSE,\n     main = \"Rozkład czasu reakcji\",\n     xlab = \"Czas reakcji (sekundy)\",\n     ylab = \"Gęstość\",\n     col = \"lightgreen\",\n     border = \"white\")\nlines(density(response_time), \n      col = \"darkgreen\", \n      lwd = 2)\n\n\n\n\n\n\n\n\nKluczowa różnica polega na tym, że rozkłady dyskretne pokazują prawdopodobieństwo w konkretnych punktach, podczas gdy rozkłady ciągłe pokazują gęstość prawdopodobieństwa w zakresach. Dla zmiennych ciągłych prawdopodobieństwo jakiejkolwiek dokładnej wartości jest w zasadzie równe zeru — zamiast tego mówimy o prawdopodobieństwie znalezienia się w przedziale.\nZrozumienie, czy twoja zmienna jest dyskretna czy ciągła, kieruje wyborem wizualizacji i metod statystycznych, zapewniając, że twoja analiza dokładnie reprezentuje naturę twoich danych.\n\n\nOpisywanie rozkładów (*)\nCharakterystyki kształtu:\nSymetria vs. Skośność:\n\nSymetryczny: Lustrzane odbicie wokół środka (przykład: wzrost w jednorodnej populacji)\nPrawostronnie skośny (skośność dodatnia): Długi ogon po prawej stronie (przykład: dochód, bogactwo)\nLewostronnie skośny (skośność ujemna): Długi ogon po lewej stronie (przykład: liczba lat życia w krajach rozwiniętych)\n\nPrzykład wpływu skośności:\nRozkład dochodu w USA:\n\nMediana dochodu gospodarstwa domowego: ~70 000 USD\nŚredni dochód gospodarstwa domowego: ~100 000 USD\nŚrednia &gt; Mediana wskazuje na skośność prawostronną\nKilka bardzo wysokich dochodów podnosi średnią\n\n\nModalność:\n\nJednomodalny: Jeden szczyt (przykład: wyniki testów)\nDwumodalny: Dwa szczyty (przykład: wzrost przy mieszaniu mężczyzn i kobiet)\nWielomodalny: Wiele szczytów (przykład: rozkład wieku w mieście uniwersyteckim — szczyty w wieku studenckim i średnim wieku)\n\n\n\n\n\n\n\n\n\n\nWażne rozkłady prawdopodobieństwa:\nRozkład normalny (Gaussa):\n\nKształt dzwonu, symetryczny\nCharakteryzowany przez średnią (\\mu) i odchylenie standardowe (\\sigma)\nOkoło 68% wartości w granicach \\mu \\pm \\sigma\nOkoło 95% w granicach \\mu \\pm 2\\sigma\nOkoło 99,7% w granicach \\mu \\pm 3\\sigma\n\nZastosowania demograficzne:\n\nWzrost w jednorodnych populacjach\nBłędy pomiarowe\nRozkłady próbkowania średnich (Centralne Twierdzenie Graniczne)\n\nRozkład dwumianowy:\n\nLiczba sukcesów w n niezależnych próbach\nKażda próba ma prawdopodobieństwo p sukcesu\nŚrednia = np, Wariancja = np(1-p)\n\nPrzykład: Liczba urodzeń chłopców na 100 urodzeń (p \\approx 0,512)\nRozkład Poissona:\n\nLiczba zdarzeń w stałym czasie/przestrzeni\nŚrednia = Wariancja = \\lambda\nDobry dla rzadkich zdarzeń\n\nZastosowania demograficzne:\n\nLiczba zgonów dziennie w małym mieście\nLiczba urodzeń na godzinę w szpitalu\nLiczba wypadków na skrzyżowaniu miesięcznie\n\n\n\n\nWizualizacja rozkładów częstości (*)\nHistogram: Dla danych ciągłych, pokazuje częstość wysokościami słupków.\n\nOś X: Zakresy wartości (przedziały)\nOś Y: Częstość lub gęstość\nBrak przerw między słupkami (dane ciągłe)\nSzerokość przedziału wpływa na wygląd\n\nWykres słupkowy: Dla danych kategorycznych, pokazuje częstość z oddzielonymi słupkami.\n\nOś X: Kategorie\nOś Y: Częstość\nPrzerwy między słupkami (dyskretne kategorie)\nKolejność może mieć znaczenie lub nie\n\nDystrybuanta (Funkcja Rozkładu Skumulowanego): Pokazuje proporcję wartości ≤ każdego punktu danych. - Zawsze rośnie (lub pozostaje płaska) - Zaczyna się od 0, kończy na 1 - Strome nachylenia wskazują na częste wartości - Płaskie obszary wskazują na rzadkie wartości\nWykres Pudełkowy (Wykres Skrzynkowy): Wizualne podsumowanie, które przedstawia kluczowe statystyki rozkładu przy użyciu pięciu kluczowych wartości.\nPodsumowanie Pięciu Liczb:\n\nMinimum: Koniec lewego wąsa (z wyłączeniem wartości odstających)\nQ1 (Pierwszy Kwartyl): Lewa krawędź pudełka (25. percentyl)\nMediana (Q2): Linia wewnątrz pudełka (50. percentyl)\n\nQ3 (Trzeci Kwartyl): Prawa krawędź pudełka (75. percentyl)\nMaksimum: Koniec prawego wąsa (z wyłączeniem wartości odstających)\n\nCo Pokazuje:\n\nSkośność: Jeśli linia mediany jest przesunięta w pudełku lub wąsy są nierówne\nRozrzut: Szersze pudełka i dłuższe wąsy wskazują na większą zmienność\nWartości odstające: Natychmiast widoczne jako oddzielne punkty\nSymetria: Równe długości wąsów i wyśrodkowana mediana sugerują rozkład normalny\n\nSzybka Interpretacja:\n\nWąskie pudełko = spójne dane\nDługie wąsy = szeroki zakres wartości\n\nWiele wartości odstających = potencjalne problemy z jakością danych lub interesujące przypadki skrajne\nMediana bliżej Q1 = dane skośne prawostronnie (ogon rozciąga się w prawo)\nMediana bliżej Q3 = dane skośne lewostronnie (ogon rozciąga się w lewo)\n\nWykresy pudełkowe są szczególnie użyteczne do porównywania wielu grup obok siebie!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zmienne-i-skale-pomiarowe",
    "href": "rozdzial1.html#zmienne-i-skale-pomiarowe",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.7 Zmienne i skale pomiarowe",
    "text": "2.7 Zmienne i skale pomiarowe\n\nZmienna to każda charakterystyka, która może przyjmować różne wartości dla różnych jednostek obserwacji.\n\n\nPomiar: przekształcanie pojęć w liczby\n\nŚwiat polityki jest pełen danych\nPolitologia ewoluowała z dyscypliny głównie teoretycznej do takiej, która coraz bardziej opiera się na dowodach empirycznych. Niezależnie od tego, czy badamy:\n\nWyniki wyborów: Dlaczego ludzie głosują tak, jak głosują?\nOpinię publiczną: Co kształtuje postawy wobec imigracji lub polityki klimatycznej?\nStosunki międzynarodowe: Jakie czynniki przewidują konflikt między narodami/państwami?\nSkuteczność polityk: Czy nowa polityka edukacyjna rzeczywiście poprawiła wyniki uczniów?\n\nPotrzebujemy systematycznych sposobów analizowania danych i wyciągania wniosków, które wykraczają poza anegdoty i osobiste wrażenia.\n\nRozważ to pytanie: “Czy demokracja prowadzi do wzrostu gospodarczego?”\n\nTwoja intuicja może sugerować, że tak - kraje demokratyczne są zazwyczaj bogatsze. Ale czy to przyczynowość, czy korelacja? Czy są wyjątki? Jak pewni możemy być naszych wniosków?\nStatystyka dostarcza narzędzi do przejścia od przeczuć do odpowiedzi opartych na dowodach, pomagając nam rozróżnić między tym, co wydaje się prawdziwe, a tym, co rzeczywiście jest prawdziwe.\n\n\nPomiar w naukach społecznych\nW naukach społecznych często zmagamy się z tym, że kluczowe pojęcia nie przekładają się wprost na liczby:\n\nJak zmierzyć „demokrację”?\nJaka liczba oddaje „ideologię polityczną”?\nJak ilościowo ująć „siłę instytucji”?\nJak zmierzyć „partycypację polityczną”?\n\n\n\n\n\n\n\n\n🔍 Korelacja ≠ Przyczynowość: Zrozumienie Związków Pozornych (spurious correlation)\n\n\n\n\nFundamentalne Rozróżnienie\nKorelacja (correlation) mierzy, jak dwie zmienne poruszają się razem:\n\nDodatnia: Obie rosną razem (godziny nauki ↑, oceny ↑)\nUjemna: Jedna rośnie, gdy druga maleje (godziny TV ↑, oceny ↓)\nMierzona współczynnikiem korelacji: r \\in [-1, 1]\n\nPrzyczynowość (causation) oznacza, że jedna zmienna bezpośrednio wpływa na drugą:\n\nX \\rightarrow Y: Zmiany w X bezpośrednio powodują zmiany w Y\nWymaga: (1) korelacji, (2) poprzedzania czasowego, (3) braku alternatywnych wyjaśnień\n\n\n\nZagrożenie: Korelacja Pozorna\nKorelacja pozorna (spurious correlation) występuje, gdy dwie zmienne wydają się powiązane, ale w rzeczywistości obie są pod wpływem trzeciej zmiennej (czynnika zakłócającego/confoundera).\nKlasyczny przykład:\n\nObserwacja: Sprzedaż lodów koreluje z liczbą utonięć\nPozorny wniosek: Lody powodują utonięcia (❌)\nRzeczywistość: Letnia pogoda (czynnik zakłócający) powoduje oba zjawiska:\nLato → Więcej sprzedanych lodów\nLato → Więcej pływania → Więcej utonięć\n\nReprezentacja matematyczna:\n\nObserwowana korelacja: \\text{Cor}(X,Y) \\neq 0\nAle prawdziwy model: X = \\alpha Z + \\epsilon_1 oraz Y = \\beta Z + \\epsilon_2\nGdzie Z to zmienna zakłócająca powodująca oba zjawiska\n\n\n\nCzynniki Zakłócające (Confounding): Ukryty Wpływ\nZmienna zakłócająca (confounding variable/confounder):\n\nWpływa zarówno na domniemaną przyczynę, jak i skutek\nTworzy iluzję bezpośredniej przyczynowości\nMusi być kontrolowana dla ważnego wnioskowania przyczynowego\n\nPrzykład badawczy:\n\nObserwacja: Spożycie kawy koreluje z chorobami serca\nPotencjalny czynnik zakłócający: Palenie (osoby pijące kawę częściej palą)\nPrawdziwe relacje:\nPalenie → Choroby serca (przyczynowa)\nPalenie → Spożycie kawy (związek)\nKawa → Choroby serca (pozorna bez kontroli palenia)\n\n\n\nJak Identyfikować Związki Przyczynowe\n\nRandomizowane badania kontrolowane (RCTs): Losowy przydział przerywa wpływ czynników zakłócających\nEksperymenty naturalne (natural experiments): Zdarzenia zewnętrzne tworzą „jakby” losową zmienność\nKontrola statystyczna: Włączenie czynników zakłócających do modeli regresji\nZmienne instrumentalne (instrumental variables): Znalezienie zmiennych wpływających na X, ale nie bezpośrednio na Y\n\n\n\nKluczowy Wniosek\nZnalezienie korelacji jest łatwe. Ustalenie przyczynowości jest trudne. Zawsze pytaj: „Co jeszcze mogłoby wyjaśniać ten związek?”\nPamiętaj: Najbardziej niebezpieczne zdanie w badaniach empirycznych to „nasze dane pokazują, że X powoduje Y”, gdy tak naprawdę zmierzyłeś tylko korelację.\n\n\n\n\n\n\n\n\n\n\n📊 Szybki Test: Korelacja czy Przyczynowość?\n\n\n\n\n\nDla każdego scenariusza określ, czy związek jest prawdopodobnie przyczynowy czy pozorny:\n\nMiasta z większą liczbą kościołów mają więcej przestępstw\n\nOdpowiedź: Pozorny (czynnik zakłócający: wielkość populacji)\n\nPalenie prowadzi do raka płuc\n\nOdpowiedź: Przyczynowy (ustalony poprzez wiele projektów badawczych)\n\nUczniowie z większą liczbą książek w domu mają lepsze oceny\n\nOdpowiedź: Prawdopodobnie pozorny (czynniki zakłócające: wykształcenie rodziców, dochód)\n\nKraje z wyższym spożyciem czekolady mają więcej laureatów Nobla\n\nOdpowiedź: Pozorny (czynnik zakłócający: poziom zamożności/rozwoju)\n\n\n\n\n\n\n\n\n\nTypy zmiennych\nZmienne ilościowe (Quantitative Variables) reprezentują ilości lub wielkości i mogą być:\nZmienne ciągłe (Continuous Variables): Mogą przyjmować dowolną wartość w przedziale, ograniczoną tylko precyzją pomiaru.\n\nWiek (22,5 lat, 22,51 lat, 22,514 lat…)\nDochód (45 234,67 zł)\nWzrost (175,3 cm)\nGęstość zaludnienia (432,7 osób na kilometr kwadratowy)\n\nZmienne dyskretne (Discrete Variables): Mogą przyjmować tylko określone wartości, zazwyczaj liczenia.\n\nLiczba dzieci w rodzinie (0, 1, 2, 3…)\nLiczba małżeństw (0, 1, 2…)\nLiczba pokoi w mieszkaniu (1, 2, 3…)\nLiczba migrantów wjeżdżających do kraju rocznie\n\nZmienne jakościowe (Qualitative Variables) reprezentują kategorie lub cechy i mogą być:\nZmienne nominalne (Nominal Variables): Kategorie bez naturalnego porządku.\n\nKraj urodzenia (Polska, Meksyk, Kanada…)\nReligia (Chrześcijaństwo, Islam, Hinduizm, Buddyzm…)\nGrupa krwi (A, B, AB, 0)\nPrzyczyna śmierci (choroby serca, nowotwory, wypadek…)\n\nZmienne porządkowe (Ordinal Variables): Kategorie ze znaczącym porządkiem, ale nierównymi interwałami.\n\nPoziom wykształcenia (brak wykształcenia, podstawowe, średnie, wyższe)\nZadowolenie z opieki zdrowotnej (bardzo niezadowolony, niezadowolony, neutralny, zadowolony, bardzo zadowolony)\nStatus społeczno-ekonomiczny (niski, średni, wysoki)\nSamoocena stanu zdrowia (zły, przeciętny, dobry, doskonały)\n\n\n\n\nSkale pomiarowe\nZrozumienie skal pomiarowych jest kluczowe, ponieważ determinują, które metody statystyczne są odpowiednie:\nSkala nominalna (Nominal Scale): Tylko kategorie — możemy liczyć częstości, ale nie możemy porządkować ani wykonywać operacji arytmetycznych. Przykład: Możemy powiedzieć, że 45% mieszkańców urodziło się lokalnie, ale nie możemy obliczyć „średniego miejsca urodzenia”.\nSkala porządkowa (Ordinal Scale): Kolejność ma znaczenie, ale różnice między wartościami niekoniecznie są równe. Przykład: Różnica między „złym” a „przeciętnym” zdrowiem może nie równać się różnicy między „dobrym” a „doskonałym” zdrowiem.\nSkala interwałowa (Interval Scale): Równe interwały między wartościami, ale brak prawdziwego punktu zerowego. Przykład: Temperatura w stopniach Celsjusza — różnica między 20°C a 30°C równa się różnicy między 30°C a 40°C, ale 0°C nie oznacza „braku temperatury”.\nSkala ilorazowa (Ratio Scale): Równe interwały z prawdziwym punktem zerowym, umożliwiające wszystkie operacje matematyczne. Przykład: Dochód — 40 000 zł to dwa razy więcej niż 20 000 zł, a 0 zł oznacza brak dochodu.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#parametry-statystyki-estymandy-estymatory-i-estymaty",
    "href": "rozdzial1.html#parametry-statystyki-estymandy-estymatory-i-estymaty",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.8 Parametry, statystyki, estymandy, estymatory i estymaty",
    "text": "2.8 Parametry, statystyki, estymandy, estymatory i estymaty\nWnioskowanie statystyczne polega na poznawaniu nieznanych cech populacji na podstawie skończonych prób. Poniżej pięć kluczowych pojęć.\n\nTabela porównawcza (w skrócie)\n\n\n\n\n\n\n\n\n\n\nTermin\nCo to jest?\nLosowe?\nTypowa notacja\nPrzykład\n\n\n\n\nEstymanda\nDokładnie zdefiniowana wielkość docelowa\nNie\nopis słowny (specyfikacja)\n„Mediana dochodu gospodarstw domowych w Kalifornii na 2024-01-01.”\n\n\nParametr\nPrawdziwa wartość tej wielkości w populacji\nNie*\n\\theta,\\ \\mu,\\ p,\\ \\beta\nPrawdziwa średnia wieku przy pierwszym porodzie we Francji (2023)\n\n\nEstymator\nReguła/wzór przekształcająca dane w oszacowanie\n—\n\\hat\\theta = g(X_1,\\dots,X_n)\n\\bar X, \\hat p = X/n, OLS \\hat\\beta\n\n\nStatystyka\nDowolna funkcja próby (w tym estymatory)\nTak\n\\bar X,\\ s^2,\\ r\nŚrednia z próby n=500 urodzeń\n\n\nEstymata\nLiczbowa wartość otrzymana z estymatora (oszacowanie)\nNie\nliczba\n\\hat p = 0.433 (43,3%)\n\n\n\n*Wartość stała dla zdefiniowanej populacji i horyzontu czasu; może się różnić między miejscami/okresami.\n\n\n\nParametr\nParametr to liczbowa cecha populacji — stała, ale dla nas nieznana.\n\nTypowe parametry: \\mu (średnia), \\sigma^2 (wariancja), p (odsetek/proporcja), \\beta (wpływ w regresji), \\lambda (intensywność/tempo).\n\nPrzykład. Prawdziwa średnia wieku przy pierwszym porodzie wszystkich kobiet we Francji w 2023 r. to parametr \\mu. Nie znamy go bez danych o całej populacji.\n\n\n\n\n\n\nNote\n\n\n\nNotacja. Często przyjmujemy greckie litery dla parametrów populacyjnych i łacińskie dla statystyk z próby. Najważniejsza jest konsekwencja.\n\n\n\n\n\nStatystyka\nStatystyka to dowolna funkcja danych z próby. Statystyki różnią się między próbami.\n\nPrzykłady: \\bar x (średnia z próby), s^2 (wariancja z próby), \\hat p (proporcja w próbie), r (korelacja), b (współczynnik regresji w próbie).\n\nPrzykład. W losowej próbie 500 urodzeń otrzymujemy \\bar x = 30{,}9 lat; inna próba może dać 31{,}4.\n\n\n\nEstymanda\nEstymanda (wielkość docelowa) to to, co chcemy oszacować — opisane tak dokładnie, aby dwaj badacze obliczyli tę samą liczbę, mając pełne dane populacyjne.\n\nDobrze zdefiniowane estymandy\n\n„Mediana dochodu gospodarstw domowych w Kalifornii na 2024-01-01.”\n„Różnica długości życia mężczyźni–kobiety dla rocznika urodzeń w Szwecji, 2023.”\n„Odsetek osób w wieku 25–34 mieszkających w miastach, które ukończyły studia wyższe.”\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDlaczego precyzja definicji ma znaczenie. „Stopa bezrobocia” jest niejednoznaczna, jeśli nie określimy: (i) kto jest bezrobotny, (ii) zakresu wieku, (iii) obszaru, (iv) okna czasowego. Różne definicje prowadzą do różnych parametrów (np. U-1 … U-6 w USA).\n\n\n\n\n\nEstymator\nEstymator to reguła, która zamienia dane w estymatę.\n\nTypowe estymatory\n\n\\hat\\mu=\\bar X=\\frac{1}{n}\\sum_{i=1}^n X_i\n\n\n\\hat p=\\frac{X}{n}\\quad\\text{(gdzie $X$ to liczba „sukcesów”)}\n\n\ns^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2\n\n\n\n\n\n\n\n\nNote\n\n\n\nDlaczego n-1? Poprawka Bessela czyni s^2 nieobciążonym estymatorem wariancji populacji, gdy średnią szacujemy na podstawie tej samej próby.\n\n\n\n\n\nJak oceniamy estymatory: błąd, wariancja, MSE, efektywność\nBłąd (bias) — czy estymator jest „wycentrowany” na prawdzie? Gdyby wielokrotnie powtarzać to samo badanie, nieobciążony estymator średnio dawałby prawdziwą wartość. Obciążony systematycznie zaniżałby lub zawyżał wynik.\nWariancja — jak bardzo estymaty różnią się między próbami? Nawet bez obciążenia kolejne próby nie dadzą identycznych liczb. Mniejsza wariancja oznacza większą stabilność między próbami.\nŚredni błąd kwadratowy (MSE) — jedna miara ogólnej trafności. MSE łączy oba składniki: \n\\mathrm{MSE}(\\hat\\theta)=\\mathrm{Var}(\\hat\\theta)+\\big(\\mathrm{Bias}(\\hat\\theta)\\big)^2.\n Mniejszy MSE jest lepszy. Estymator z niewielkim obciążeniem, ale znacznie mniejszą wariancją, może mieć niższy MSE niż estymator nieobciążony, lecz bardzo zmienny.\nEfektywność — porównawcza precyzja estymatorów. Wśród nieobciążonych estymatorów tego samego parametru, opartych na tych samych danych, bardziej efektywny ma mniejszą wariancję. Jeśli dopuszczamy niewielkie obciążenie, porównujemy estymatory za pomocą MSE.\n\n\n\n\n\n\nSkąd bierze się „precyzja” (częste przypadki)\n\n\n\n\nŚrednia z próby (prosty losowy dobór): \n\\operatorname{Var}(\\bar X)=\\frac{\\sigma^2}{n},\\qquad\n\\mathrm{SE}(\\bar X)=\\frac{\\sigma}{\\sqrt{n}}.\n Większe n zmniejsza błąd standardowy w tempie 1/\\sqrt{n}.\nProporcja z próby: \n\\operatorname{Var}(\\hat p)=\\frac{p(1-p)}{n},\\qquad\n\\mathrm{SE}(\\hat p)=\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}.\n\nEfekty planu (design effects): klastrowanie, warstwowanie i wagi zmieniają wariancję. Dobieraj metodę SE do planu doboru próby.\n\n\n\n\n\n\nEstymata (oszacowanie)\nEstymata to liczbowa wartość otrzymana po zastosowaniu estymatora do danych.\nPrzykład (krok po kroku)\n\nEstymanda: Odsetek dorosłych mieszkańców USA wyrażających aprobatę dziś.\nParametr: p (nieznana prawdziwa aprobata).\nEstymator: \\hat p = X/n.\nPróba: n=1{,}500, liczba aprobujących X=650.\nEstymata: \n\\hat p=\\frac{650}{1{,}500}=0.433 \\quad (43{,}3%).\n\n\n\n\n\nCzęste pomyłki i doprecyzowania\n\nParametr vs statystyka: wielkość populacyjna vs wielkość z próby.\nEstymator vs estymata: procedura vs wynik liczbowy.\nIndeks czasu: parametry często zależą od czasu (np. II kw. vs III kw.).\nNajpierw definicja: zanim wybierzesz estymator, precyzyjnie określ estymandę.\n\n\n\n\n\n\n\n\nWyjaśnienie różnych typów nieprzewidywalności\n\n\n\nNie wszystkie rodzaje niepewności są takie same. Zrozumienie różnych źródeł nieprzewidywalności pomaga w wyborze odpowiednich metod statystycznych i prawidłowej interpretacji wyników.\n\n\n\n\n\n\n\n\n\nPojęcie\nCzym jest?\nŹródło nieprzewidywalności\nPrzykład\n\n\n\n\nLosowość (randomness)\nPoszczególne wyniki są niepewne, ale rozkład prawdopodobieństwa jest znany lub modelowany.\nFluktuacje między realizacjami; brak informacji o konkretnym wyniku.\nRzut kostką, rzut monetą, próba sondażowa\n\n\nChaos\nDynamika deterministyczna bardzo wrażliwa na warunki początkowe (efekt motyla).\nNiewielkie różnice początkowe szybko narastają → duże rozbieżności trajektorii.\nPrognoza pogody, podwójne wahadło, dynamika populacyjna\n\n\nEntropia\nMiara niepewności/rozproszenia (teorioinformacyjna lub termodynamiczna).\nWiększa gdy wyniki są bardziej równomiernie rozłożone (mniej informacji predykcyjnej).\nEntropia Shannona w kompresji danych\n\n\n„Przypadkowość” (potoczne)\nOdczuwany brak porządku bez wyraźnego modelu; mieszanka mechanizmów.\nBrak uporządkowanego opisu lub stabilnych reguł; nakładające się procesy.\nWzorce ruchu, trendy w mediach społecznościowych\n\n\nLosowość kwantowa (quantum randomness)\nPojedynczy wynik nie jest zdeterminowany; tylko rozkład jest określony (reguła Borna).\nFundamentalna (ontologiczna) nieokreśloność poszczególnych pomiarów.\nPomiar spinu elektronu, polaryzacja fotonu\n\n\n\n\nKluczowe rozróżnienia dla praktyki statystycznej\nChaos deterministyczny ≠ losowość statystyczna: System chaotyczny jest w pełni deterministyczny, ale praktycznie nieprzewidywalny z powodu ekstremalnej wrażliwości na warunki początkowe. Losowość statystyczna modeluje natomiast niepewność poprzez rozkłady prawdopodobieństwa, gdzie poszczególne wyniki są rzeczywiście niepewne.\nDlaczego to ważne: W statystyce zazwyczaj modelujemy zjawiska jako procesy losowe, zakładając, że możemy określić rozkłady prawdopodobieństwa, nawet gdy poszczególne wyniki są nieprzewidywalne. To założenie stanowi podstawę większości wnioskowań statystycznych.\n\n\nMechanika kwantowa i fundamentalna losowość\nW interpretacji kopenhaskiej losowość jest fundamentalna (ontologiczna): pojedynczy wynik nie może być przewidziany, ale rozkład prawdopodobieństwa jest dany przez regułę Borna.\nTo reprezentuje prawdziwą losowość na najbardziej podstawowym poziomie natury.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#błąd-statystyczny-i-niepewność",
    "href": "rozdzial1.html#błąd-statystyczny-i-niepewność",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.9 Błąd statystyczny i niepewność",
    "text": "2.9 Błąd statystyczny i niepewność\n\nWprowadzenie: Dlaczego niepewność ma znaczenie\nŻaden pomiar ani oszacowanie nie jest doskonałe. Zrozumienie różnych typów błędów jest kluczowe dla interpretacji wyników i poprawy projektu badania.\n\n\n\n\n\n\nCentralne wyzwanie\n\n\n\nZa każdym razem, gdy używamy próby (sample) do poznania populacji (population), wprowadzamy niepewność. Kluczem jest:\n\nUczciwe skwantyfikowanie tej niepewności\nRozróżnienie między różnymi źródłami błędu\nTransparentna komunikacja wyników\n\n\n\n\n\n\nTypy błędów\n\nBłąd losowy (random error)\nBłąd losowy (random error) reprezentuje nieprzewidywalne fluktuacje, które różnią się między obserwacjami bez stałego wzorca. Te błędy wynikają z różnych źródeł naturalnej zmienności w procesie zbierania i pomiaru danych.\n\n\n\n\n\n\nKluczowe cechy\n\n\n\n\nNieprzewidywalny kierunek: Czasami za wysoki, czasami za niski\nBrak stałego wzorca: Zmienia się losowo między obserwacjami\nŚrednio daje zero: Po wielu pomiarach dodatnie i ujemne błędy się znoszą\nMożliwy do skwantyfikowania: Można go oszacować i zredukować odpowiednimi metodami\n\n\n\nBłąd losowy obejmuje kilka podtypów:\n\nBłąd próbkowania (sampling error)\nBłąd próbkowania (sampling error) to najczęstszy typ błędu losowego—pojawia się, ponieważ obserwujemy próbę, a nie całą populację. Różne losowe próby z tej samej populacji dadzą różne oszacowania wyłącznie przez przypadek.\nKluczowe właściwości:\n\nMaleje wraz z wielkością próby: \\propto 1/\\sqrt{n}\nMożliwy do skwantyfikowania za pomocą teorii prawdopodobieństwa\nNieunikniony przy pracy z próbami\n\nPrzykład: Badanie dostępu do internetu\nWyobraźmy sobie ankietę 100 losowo wybranych gospodarstw domowych o dostępie do internetu:\n\n\n\n\n\n\n\n\n\nZmienność wokół prawdziwej wartości (czerwona linia) reprezentuje błąd próbkowania. Przy większych próbach oszacowania przedziałowe byłyby węższe.\n\n\nBłąd pomiaru (measurement error)\nBłąd pomiaru (measurement error) to losowa zmienność w samym procesie pomiaru—nawet przy wielokrotnym pomiarze tej samej rzeczy.\nPrzykłady:\n\nNiewielkie różnice przy odczycie termometru spowodowane paralaksą\nLosowe fluktuacje w przyrządach elektronicznych\nNiespójności w ludzkiej ocenie przy kodowaniu danych jakościowych\n\nW przeciwieństwie do błędu próbkowania (który wynika z tego, kogo/co obserwujemy), błąd pomiaru wynika z tego, jak obserwujemy.\n\n\nInne źródła błędu losowego\n\nBłąd przetwarzania (processing error): Losowe pomyłki we wprowadzaniu danych, kodowaniu lub obliczeniach\nBłąd specyfikacji modelu (model specification error): Gdy prawdziwa zależność jest bardziej złożona niż zakładano\nZmienność czasowa (temporal variation): Naturalne wahania z dnia na dzień w mierzonym zjawisku\n\n\n\n\nBłąd systematyczny (systematic error / bias)\nBłąd systematyczny (systematic error lub bias) reprezentuje stałe odchylenie w określonym kierunku. W przeciwieństwie do błędu losowego, nie zeruje się przy powtarzanym próbkowaniu lub pomiarze—utrzymuje się i konsekwentnie odsuwa wyniki od prawdy.\n\nBłąd selekcji (selection bias)Błąd pomiaru (measurement bias)Błąd odpowiedzi (response bias)Błąd braku odpowiedzi (non-response bias)Błąd przetrwania (survivorship bias)Błąd obserwatora/ankietera (observer/interviewer bias)\n\n\nMetoda doboru próby systematycznie wyklucza pewne grupy.\nPrzykład: Ankiety telefoniczne w godzinach pracy niedostatecznie reprezentują osoby zatrudnione.\n\n\nNarzędzie pomiarowe konsekwentnie zawyża/zaniża pomiar.\nPrzykład: Waga, która zawsze pokazuje 2 funty za dużo; pytania ankietowe, które nakłaniają respondentów do konkretnych odpowiedzi.\n\n\nRespondenci systematycznie fałszywie raportują.\nPrzykład: Ludzie zaniżają spożycie alkoholu, zawyżają uczestnictwo w wyborach lub dają odpowiedzi społecznie pożądane.\n\n\nOsoby nieudzielające odpowiedzi różnią się systematycznie od respondentów.\nPrzykład: Osoby bardzo chore i bardzo zdrowe rzadziej odpowiadają na ankiety zdrowotne, pozostawiając tylko osoby o umiarkowanym zdrowiu.\n\n\nObserwowanie wyłącznie „ocalałych” z danego procesu.\nPrzykład: Podczas II wojny światowej wojsko analizowało powracające bombowce, aby określić, gdzie należy dodać pancerz. Samoloty wykazywały największe uszkodzenia na skrzydłach i sekcjach ogonowych. Abraham Wald dostrzegł błąd: należy opancerzyć miejsca, gdzie nie było dziur po kulach—silnik i kokpit. Samoloty trafione w tych miejscach nigdy nie wracały, aby je przeanalizować. Badano wyłącznie ocalałe.\n\n\nObserwatorzy lub ankieterzy systematycznie wpływają na wyniki.\nPrzykład: Ankieterzy nieświadomie sugerują pewne odpowiedzi lub rejestrują obserwacje potwierdzające ich oczekiwania.\n\n\n\n\n\nDekompozycja obciążenia i wariancji (bias-variance decomposition)\nMatematycznie, całkowity błąd (błąd średniokwadratowy, Mean Squared Error) rozkłada się na:\n\\mathrm{MSE}(\\hat\\theta) = \\underbrace{\\mathrm{Var}(\\hat\\theta)}_{\\text{błąd losowy}} + \\underbrace{\\big(\\mathrm{Bias}(\\hat\\theta)\\big)^2}_{\\text{błąd systematyczny}}\n\n\n\n\n\n\nKluczowy wniosek\n\n\n\n\nDuża obciążona próba daje precyzyjnie błędną odpowiedź.\n\n\nZwiększ n → redukuje błąd losowy (szczególnie błąd próbkowania)\nPopraw projekt badania → redukuje błąd systematyczny\nLepsze narzędzia → redukuje błąd pomiaru\n\n\n\n\n\n\nRóżne kombinacje obciążenia i wariancji w estymacji\n\n\nIntuicyjna analogia: Pomyśl o próbie trafienia w środek tarczy:\n\nBłąd losowy = rozproszone strzały wokół celu (czasami w lewo, czasami w prawo, czasami wysoko, czasami nisko)\nBłąd systematyczny = konsekwentne trafianie w to samo złe miejsce (wszystkie strzały skupione, ale z dala od centrum)\nIdeał = strzały ciasno skupione w centrum tarczy\n\n\n\n\n\nKwantyfikowanie niepewności\n\nBłąd standardowy (standard error)\nBłąd standardowy (standard error, SE) kwantyfikuje, jak bardzo oszacowanie zmienia się między różnymi możliwymi próbami. Mierzy konkretnie błąd próbkowania.\n\n\nDla proporcji:\nSE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\nDla średniej:\nSE(\\bar{x}) = \\frac{s}{\\sqrt{n}}\n\nDla różnicy:\nSE(\\bar{x}_1 - \\bar{x}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\n\n\n\n\n\n\n\nCo mówi nam SE\n\n\n\nBłąd standardowy kwantyfikuje tylko błąd próbkowania. Nie uwzględnia błędów systematycznych (obciążenia), błędów pomiaru ani innych źródeł niepewności.\n\n\n\n\nMargines błędu (margin of error)\nMargines błędu (margin of error, MOE) reprezentuje oczekiwaną maksymalną różnicę między oszacowaniem z próby a prawdziwym parametrem.\n\\text{MOE} = \\text{Wartość krytyczna} \\times \\text{Błąd standardowy}\n\n\n\n\n\n\nWyjaśnienie wartości krytycznej\n\n\n\n\n\nDla 95% ufności używamy 1,96 (często upraszczane do 2). Zapewnia to, że ~95% przedziałów skonstruowanych w ten sposób będzie zawierać prawdziwy parametr.\n\n90% ufności: z = 1,645\n95% ufności: z = 1,96\n99% ufności: z = 2,576\n\n\n\n\n\n\nPrzedziały ufności (confidence intervals)\nPrzedział ufności (confidence interval) dostarcza zakres prawdopodobnych wartości:\n\\text{CI} = \\text{Oszacowanie} \\pm (\\text{Wartość krytyczna} \\times \\text{Błąd standardowy})\n\n\n\n\n\n\nWażne ograniczenie\n\n\n\nPrzedziały ufności kwantyfikują niepewność próbkowania, ale zakładają brak błędu systematycznego. Doskonale precyzyjne oszacowanie (wąski przedział ufności) może nadal być obciążone, jeśli projekt badania jest wadliwy.\n\n\n\n\n\n\nPraktyczne zastosowanie: Sondaże opinii publicznej\n\n\n\n\n\n\nStudium przypadku: Sondaże polityczne\n\n\n\nGdy sondaż raportuje “Kandydat A: 52%, Kandydat B: 48%”, jest to niekompletne bez kwantyfikacji niepewności.\n\n\n\nZłota zasada sondażowania\nPrzy ~1000 losowo wybranych respondentów:\n\nMargines błędu: ±3 punkty procentowe (95% ufności)\nInterpretacja: Raportowane 52% oznacza, że prawdziwe poparcie prawdopodobnie wynosi między 49% a 55%\nCo to obejmuje: Tylko losowy błąd próbkowania—zakłada brak systematycznego obciążenia\n\n\n\n\n\n\n\nKluczowe rozróżnienie\n\n\n\nMargines błędu ±3% kwantyfikuje tylko niepewność próbkowania. Nie uwzględnia:\n\nBłędu pokrycia (coverage bias): kto jest wykluczony z operatu losowania\nBłędu braku odpowiedzi (non-response bias): kto odmawia udziału\nBłędu odpowiedzi (response bias): ludzie nieprawdziwie raportujący swoje poglądy\nEfektów czasowych (timing effects): zmiany opinii między sondażem a wyborem\n\n\n\n\n\nWielkość próby a precyzja\n\n\n\nWielkość próby\nMargines błędu (95%)\nZastosowanie\n\n\n\n\nn = 100\n± 10 pp\nTylko ogólny kierunek\n\n\nn = 400\n± 5 pp\nOgólne trendy\n\n\nn = 1000\n± 3 pp\nStandardowe sondaże\n\n\nn = 2500\n± 2 pp\nWysoka precyzja\n\n\nn = 10000\n± 1 pp\nBardzo wysoka precyzja\n\n\n\n\n\n\n\n\n\nPrawo malejących przychodów\n\n\n\nAby zmniejszyć margines błędu o połowę, potrzeba czterokrotnie większej próby, ponieważ \\text{MOE} \\propto 1/\\sqrt{n}\nTo dotyczy tylko błędu próbkowania. Podwojenie próby z 1000 do 2000 nie naprawi systematycznych problemów, takich jak stronnicze sformułowanie pytań czy niereprezentacyjne metody doboru próby.\n\n\n\n\nCo powinny raportować jakościowe sondaże\nTransparentny sondaż ujawnia:\n\nDaty badania: Kiedy zebrano dane?\nPopulacja i metoda doboru próby: Kto został przebadany i jak zostali wybrani?\nWielkość próby: Ile osób odpowiedziało?\nWskaźnik odpowiedzi (response rate): Jaki odsetek skontaktowanych osób wziął udział?\nProcedury ważenia (weighting procedures): Jak próba została dostosowana do charakterystyk populacji?\nMargines błędu próbkowania: Kwantyfikacja niepewności próbkowania\nBrzmienie pytań: Dokładny tekst zadanych pytań\n\n\n\n\n\n\n\nLuka w raportowaniu\n\n\n\nWiększość doniesień medialnych wspomina tylko liczby wynikowe i czasami margines błędu. Rzadko omawiają potencjalne obciążenia systematyczne, które mogą być znacznie większe niż błąd próbkowania.\n\n\n\n\n\n\nWizualizacja: Zmienność próbkowania\nPoniższa symulacja demonstruje, jak zachowują się przedziały ufności przy powtarzanym próbkowaniu:\n\n\nPokaż kod symulacji\nlibrary(ggplot2)\nset.seed(42)\n\n# Parametry\nn_polls      &lt;- 20\nn_people     &lt;- 100\ntrue_support &lt;- 0.50\n\n# Symulacja niezależnych sondaży\nsupport &lt;- rbinom(n_polls, n_people, true_support) / n_people\n\n# Obliczenie błędów standardowych i marginesów błędu\nse   &lt;- sqrt(support * (1 - support) / n_people)\nmoe  &lt;- 2 * se  # Uproszczony mnożnik dla przejrzystości\n\n# Utworzenie przedziałów ufności\nlower &lt;- pmax(0, support - moe)\nupper &lt;- pmin(1, support + moe)\n\n# Sprawdzenie pokrycia\ncovers &lt;- (lower &lt;= true_support) & (upper &gt;= true_support)\nn_cover &lt;- sum(covers)\n\nresults &lt;- data.frame(\n  poll = seq_len(n_polls),\n  support, se, moe, lower, upper, covers\n)\n\n# Utworzenie wizualizacji\nggplot(results, aes(x = poll, y = support, color = covers)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                width = 0.3, alpha = 0.8, size = 1) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = true_support, \n             linetype = \"dashed\", \n             color = \"black\",\n             alpha = 0.7) +\n  scale_color_manual(\n    values = c(\"TRUE\" = \"forestgreen\", \"FALSE\" = \"darkorange\"),\n    labels = c(\"TRUE\" = \"Obejmuje prawdę\", \"FALSE\" = \"Mija prawdę\"),\n    name   = NULL\n  ) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0, 1)) +\n  labs(\n    title    = \"Zmienność próbkowania w 20 niezależnych sondażach\",\n    subtitle = paste0(\n      \"Każdy sondaż: n = \", n_people, \" | Prawdziwa wartość = \",\n      scales::percent(true_support),\n      \" | Pokrycie: \", n_cover, \"/\", n_polls,\n      \" (\", round(100 * n_cover / n_polls), \"%)\"\n    ),\n    x = \"Numer sondażu\",\n    y = \"Oszacowane poparcie\",\n    caption = \"Słupki błędów pokazują przybliżone 95% przedziały ufności\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKluczowa obserwacja\n\n\n\nWiększość przedziałów obejmuje prawdziwą wartość, ale niektóre “chybiają” wyłącznie z powodu losowości próbkowania. Jest to oczekiwane i możliwe do skwantyfikowania—taka jest natura losowego błędu próbkowania.\nWażne: Ta symulacja zakłada brak systematycznego obciążenia. W rzeczywistych sondażach błędy systematyczne (błąd braku odpowiedzi, problemy z pokryciem, efekty sformułowania pytań) mogą przesunąć wszystkie oszacowania w tym samym kierunku, czyniąc je konsekwentnie błędnymi nawet przy dużych próbach.\n\n\n\n\n\nPowszechne błędne przekonania\n\n\n\n\n\n\nBłędne przekonanie #1: Margines błędu obejmuje całą niepewność\n\n\n\n❌ Mit: “Prawdziwa wartość na pewno znajduje się w marginesie błędu”\n✅ Rzeczywistość:\n\nPrzy 95% ufności nadal istnieje 5% szansa, że prawdziwa wartość znajduje się poza przedziałem wyłącznie z powodu losowości próbkowania\nCo ważniejsze, margines błędu obejmuje tylko błąd próbkowania, nie obciążenia systematyczne\nRzeczywiste sondaże często mają większe błędy z powodu błędu braku odpowiedzi, sformułowania pytań czy problemów z pokryciem niż z błędu próbkowania\n\n\n\n\n\n\n\n\n\nBłędne przekonanie #2: Większe próby naprawiają wszystko\n\n\n\n❌ Mit: “Jeśli tylko przebadamy więcej ludzi, wyeliminujemy wszystkie błędy”\n✅ Rzeczywistość:\n\nWiększe próby redukują błąd losowy (szczególnie błąd próbkowania): bardziej precyzyjne oszacowania\nWiększe próby NIE redukują błędu systematycznego: obciążenie pozostaje niezmienione\nSondaż 10 000 osób z 70% wskaźnikiem odpowiedzi i obciążonym operatem losowania da precyzyjnie błędną odpowiedź\nLepiej mieć 1000 dobrze wybranych respondentów niż 10 000 źle wybranych\n\n\n\n\n\n\n\n\n\nBłędne przekonanie #3: Losowy = niedbały\n\n\n\n❌ Mit: “Błąd losowy oznacza, że doszło do pomyłki”\n✅ Rzeczywistość:\n\nBłąd losowy jest nieodłączny w próbkowaniu i pomiarze—to nie jest pomyłka\nNawet przy doskonałej metodologii różne losowe próby dają różne wyniki\nBłędy losowe są przewidywalne w agregacie, choć nieprzewidywalne indywidualnie\nTermin “losowy” odnosi się do wzorca (brak systematycznego kierunku), a nie do niedbalstwa\n\n\n\n\n\n\n\n\n\nBłędne przekonanie #4: Przedziały ufności są gwarancjami\n\n\n\n❌ Mit: “95% ufności oznacza, że istnieje 95% szansa, że prawdziwa wartość jest w tym konkretnym przedziale”\n✅ Rzeczywistość:\n\nPrawdziwa wartość jest stała (ale nieznana)—albo jest w przedziale, albo nie\n“95% ufności” oznacza: gdybyśmy powtórzyli ten proces wiele razy, około 95% skonstruowanych przedziałów zawierałoby prawdziwą wartość\nKażdy konkretny przedział albo obejmuje prawdę, albo nie—po prostu nie wiemy, co jest prawdą\n\n\n\n\n\n\n\n\n\nBłędne przekonanie #5: Obciążenie można obliczyć jak błąd losowy\n\n\n\n❌ Mit: “Możemy obliczyć obciążenie tak samo jak obliczamy błąd standardowy”\n✅ Rzeczywistość:\n\nBłąd losowy jest możliwy do skwantyfikowania za pomocą teorii prawdopodobieństwa, ponieważ znamy proces próbkowania\nBłąd systematyczny jest zazwyczaj nieznany i niemożliwy do poznania bez zewnętrznej walidacji\nNie można użyć samej próby do wykrycia obciążenia—potrzeba niezależnej informacji o populacji\nDlatego porównanie sondaży z wynikami wyborów jest wartościowe: ujawnia obciążenia, które nie były możliwe do skwantyfikowania wcześniej\n\n\n\n\n\n\nPrzykład z życia: Porażki sondażowe\n\n\n\n\n\n\nStudium przypadku: Gdy sondaże mylą\n\n\n\nRozważmy scenariusz, w którym 20 sondaży pokazuje, że Kandydat A prowadzi o 3-5 punktów, z marginesami błędu około ±3%. Sondaże wydają się spójne, ale wygrywa Kandydat B.\nCo się stało?\n\nNie błąd próbkowania: Wszystkie sondaże się zgadzały—mało prawdopodobne przy samej losowej zmienności\nPrawdopodobnie błąd systematyczny:\n\nBłąd braku odpowiedzi: Pewni wyborcy konsekwentnie odmawiali udziału\nBłąd społecznej pożądaności (social desirability bias): Niektórzy wyborcy nieprawdziwie raportowali swoje preferencje\nBłąd modelowania frekwencji (turnout modeling error): Błędne założenia o tym, kto rzeczywiście będzie głosować\nBłąd pokrycia: Operat losowania (np. listy telefonów) systematycznie wykluczał pewne grupy\n\n\nLekcja: Spójność między sondażami nie gwarantuje trafności. Wszystkie sondaże mogą dzielić te same obciążenia systematyczne, dając fałszywą pewność w błędnych oszacowaniach.\n\n\n\n\n\nKluczowe wnioski\n\n\n\n\n\n\nNajważniejsze punkty\n\n\n\nRozumienie typów błędów:\n\nBłąd losowy to nieprzewidywalna zmienność, która średnio daje zero\n\nBłąd próbkowania: Z obserwowania próby, a nie całej populacji\nBłąd pomiaru: Z niedoskonałych narzędzi lub procesów pomiarowych\nRedukowany przez: większe próby, lepsze narzędzia, więcej pomiarów\n\nBłąd systematyczny (obciążenie) to konsekwentne odchylenie w jednym kierunku\n\nBłąd selekcji, błąd pomiaru, błąd odpowiedzi, błąd braku odpowiedzi, itp.\nRedukowany przez: lepszy projekt badania, nie większe próby\n\n\nKwantyfikowanie niepewności:\n\nBłąd standardowy mierzy typową zmienność próbkowania (jeden typ błędu losowego)\nMargines błędu ≈ 2 × SE daje zakres dla 95% ufności o niepewności próbkowania\nWielkość próby i precyzja błędu próbkowania są związane: \\text{SE} \\propto 1/\\sqrt{n}\n\nPoczwórna próba zmniejsza błąd próbkowania o połowę\nMalejące przychody wraz ze wzrostem n\n\nPrzedziały ufności dostarczają prawdopodobnych zakresów, ale zakładają brak obciążenia systematycznego\n\nKluczowe wnioski:\n\nPrecyzyjnie błędna odpowiedź (duża obciążona próba) jest często gorsza niż nieprecyzyjnie poprawna odpowiedź (mała nieobciążona próba)\nZawsze rozważ zarówno błąd próbkowania ORAZ potencjalne obciążenia systematyczne—publikowane marginesy błędu zazwyczaj ignorują te drugie\nTransparentność ma znaczenie: Raportuj metodologię, wskaźniki odpowiedzi i potencjalne obciążenia, nie tylko oszacowania punktowe i marginesy błędu\nWalidacja jest niezbędna: Porównuj oszacowania ze znanymi wartościami, gdy to możliwe, aby wykryć błędy systematyczne\n\n\n\n\n\n\n\n\n\nPriorytety praktyka\n\n\n\nPrzy projektowaniu badań:\nNajpierw: Minimalizuj błąd systematyczny poprzez staranny projekt\n\nReprezentatywne metody doboru próby\nWysokie wskaźniki odpowiedzi\nNieobciążone narzędzia pomiarowe\nWłaściwe sformułowanie pytań\n\nNastępnie: Optymalizuj wielkość próby, aby osiągnąć akceptowalną precyzję\n\nWiększe próby pomagają tylko po zajęciu się obciążeniem\nRównowaga między kosztem a poprawą precyzji\nPamiętaj o malejących przychodach\n\nNa koniec: Raportuj niepewność uczciwie\n\nJasno określ założenia\nPrzyznaj się do potencjalnych obciążeń\nNie pozwól, aby precyzyjne oszacowania stworzyły fałszywą pewność",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#próbkowanie-i-metody-próbkowania",
    "href": "rozdzial1.html#próbkowanie-i-metody-próbkowania",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.10 Próbkowanie i metody próbkowania (*)",
    "text": "2.10 Próbkowanie i metody próbkowania (*)\nPróbkowanie to proces wyboru podzbioru jednostek z populacji w celu oszacowania charakterystyk całej populacji. Sposób, w jaki próbkujemy, głęboko wpływa na to, co możemy wywnioskować z naszych danych.\n\nOperat losowania (Sampling Frame)\nZanim omówimy metody, musimy zrozumieć operat losowania — listę lub urządzenie, z którego pobieramy naszą próbę. Operat powinien idealnie obejmować każdego członka populacji dokładnie raz.\nPowszechne operaty losowania:\n\nListy wyborcze (dla dorosłych obywateli)\nKsiążki telefoniczne (coraz bardziej problematyczne z powodu telefonów komórkowych i numerów nienotowanych)\nListy adresowe z poczty\nRejestracje urodzeń (dla noworodków)\nListy zapisów do szkół (dla dzieci)\nRejestry podatkowe (dla osób zarabiających)\nZdjęcia satelitarne (dla mieszkań w odległych obszarach)\n\nProblemy z operatami losowania:\n\nNiepełne pokrycie (Undercoverage): Operat pomija członków populacji (bezdomni nieobecni na listach adresowych)\nNadmierne pokrycie (Overcoverage): Operat obejmuje osoby spoza populacji (zmarli nadal na listach wyborców)\nDuplikacja: Ta sama jednostka pojawia się wielokrotnie (osoby z wieloma numerami telefonów)\nGrupowanie (Clustering): Wielu członków populacji na jednostkę operatu (wiele rodzin pod jednym adresem)\n\n\n\nMetody próbkowania probabilistycznego\nPróbkowanie probabilistyczne daje każdemu członkowi populacji znane, niezerowe prawdopodobieństwo selekcji. To pozwala nam dokonywać wnioskowań statystycznych o populacji.\n\nProste losowanie (Simple Random Sampling - SRS)\nKażda możliwa próba o wielkości n ma równe prawdopodobieństwo selekcji. To złoty standard teorii statystycznej, ale często niepraktyczny dla dużych populacji.\nJak to działa:\n\nPonumeruj każdą jednostkę w populacji od 1 do N\nUżyj liczb losowych do wybrania n jednostek\nKażda jednostka ma prawdopodobieństwo n/N selekcji\n\nPrzykład: Aby wybrać próbę 50 uczniów ze szkoły liczącej 1000:\n\nPrzypisz każdemu uczniowi numer od 1 do 1000\nWygeneruj 50 losowych liczb między 1 a 1000\nWybierz uczniów z tymi numerami\n\nZalety:\n\nStatystycznie optymalny\nŁatwy do analizy\nNie wymaga dodatkowych informacji o populacji\n\nWady:\n\nWymaga kompletnego operatu losowania\nMoże być kosztowny (wybrane jednostki mogą być daleko od siebie)\nMoże nie reprezentować dobrze ważnych podgrup przez przypadek\n\n\n\nLosowanie systematyczne (Systematic Sampling)\nWybierz co k-ty element z uporządkowanego operatu losowania, gdzie k = N/n (interwał próbkowania).\nJak to działa:\n\nOblicz interwał próbkowania k = N/n\nLosowo wybierz punkt początkowy między 1 a k\nWybierz co k-tą jednostkę następnie\n\nPrzykład: Aby wybrać próbę 100 domów z 5000 na liście ulic:\n\nk = 5000/100 = 50\nLosowy start: 23\nPróba gospodarstw domowych: 23, 73, 123, 173, 223…\n\nZalety:\n\nProste do wdrożenia w terenie\nRozprzestrzenia próbę w całej populacji\n\nWady:\n\nMoże wprowadzić obciążenie, jeśli jest okresowość w operacie\n\nPrzykład ukrytej okresowości: Próbkowanie co 10. mieszkania w budynkach, gdzie mieszkania narożne (numery kończące się na 0) są wszystkie większe. To zawyżyłoby nasze oszacowanie średniej wielkości mieszkania.\n\n\nLosowanie warstwowe (Stratified Sampling)\nPodziel populację na jednorodne podgrupy (warstwy) przed próbkowaniem. Próbkuj niezależnie w każdej warstwie.\nJak to działa:\n\nPodziel populację na nienachodzące warstwy\nPróbkuj niezależnie z każdej warstwy\nPołącz wyniki z odpowiednimi wagami\n\nPrzykład: Badanie dochodu w mieście z odrębnymi dzielnicami:\n\nWarstwa 1: Dzielnica wysokich dochodów (10% populacji) - próba 100\nWarstwa 2: Dzielnica średnich dochodów (60% populacji) - próba 600\nWarstwa 3: Dzielnica niskich dochodów (30% populacji) - próba 300\n\nTypy alokacji:\nProporcjonalna: Wielkość próby w każdej warstwie proporcjonalna do wielkości warstwy\n\nJeśli warstwa ma 20% populacji, dostaje 20% próby\n\nOptymalna (Neymana): Większe próby z bardziej zmiennych warstw\n\nJeśli dochód bardziej się różni w obszarach wysokich dochodów, próbkuj tam więcej\n\nRówna: Ta sama wielkość próby na warstwę niezależnie od wielkości populacji\n\nPrzydatna, gdy porównywanie warstw jest głównym celem\n\nZalety:\n\nZapewnia reprezentację wszystkich podgrup\nMoże znacznie zwiększyć precyzję\nPozwala na różne metody próbkowania w warstwie\nDostarcza oszacowania dla każdej warstwy\n\nWady:\n\nWymaga informacji do utworzenia warstw\nMoże być trudna do badania\n\n\n\nLosowanie grupowe (Cluster Sampling)\nWybierz grupy (klastry) zamiast jednostek. Często używane, gdy populacja jest naturalnie pogrupowana lub gdy utworzenie kompletnego operatu jest trudne.\nJednostopniowe losowanie grupowe:\n\nPodziel populację na klastry\nLosowo wybierz niektóre klastry\nUwzględnij wszystkie jednostki z wybranych klastrów\n\nDwustopniowe losowanie grupowe:\n\nLosowo wybierz klastry (Pierwotne Jednostki Losowania)\nW wybranych klastrach losowo wybierz jednostki (Wtórne Jednostki Losowania)\n\nPrzykład: Badanie gospodarstw wiejskich w dużym kraju:\n\nEtap 1: Losowo wybierz 50 wsi z 1000 wsi\nEtap 2: W każdej wybranej wsi losowo wybierz 20 gospodarstw\nCałkowita próba: 50 × 20 = 1000 gospodarstw\n\nPrzykład wielostopniowy: Krajowe badanie zdrowotne:\n\nEtap 1: Wybierz województwa\nEtap 2: Wybierz powiaty w wybranych województwach\nEtap 3: Wybierz obwody spisowe w wybranych powiatach\nEtap 4: Wybierz gospodarstwa w wybranych obwodach\nEtap 5: Wybierz jednego dorosłego w wybranych gospodarstwach\n\nZalety:\n\nNie wymaga kompletnej listy populacji\nRedukuje koszty podróży (jednostki zgrupowane geograficznie)\nMoże używać różnych metod na różnych etapach\nNaturalne dla populacji hierarchicznych\n\nWady:\n\nMniej statystycznie efektywne niż SRS\nZłożona estymacja wariancji\nWiększe próby potrzebne dla tej samej precyzji\n\nEfekt projektu (Design Effect): Losowanie grupowe zazwyczaj wymaga większych prób niż SRS. Efekt projektu (DEFF) kwantyfikuje to:\n\\text{DEFF} = \\frac{\\text{Wariancja(próba grupowa)}}{\\text{Wariancja(SRS)}}\nJeśli DEFF = 2, potrzebujesz dwukrotnie większej próby, aby osiągnąć taką samą precyzję jak SRS.\n\n\n\nMetody próbkowania nieprobabilistycznego\nPróbkowanie nieprobabilistyczne nie gwarantuje znanych prawdopodobieństw selekcji. Choć ogranicza wnioskowanie statystyczne, te metody mogą być konieczne lub przydatne w pewnych sytuacjach.\n\nPróbkowanie wygodne (Convenience Sampling)\nSelekcja oparta wyłącznie na łatwości dostępu. Brak próby reprezentacji.\nPrzykłady:\n\nAnkietowanie studentów w twojej klasie o nawykach nauki\nWywiadowanie ludzi w centrum handlowym o preferencjach konsumenckich\nAnkiety online, w których każdy może uczestniczyć\nBadania medyczne używające wolontariuszy, którzy odpowiadają na ogłoszenia\n\nKiedy może być akceptowalne:\n\nBadania pilotażowe do testowania instrumentów ankietowych\nBadania eksploracyjne do identyfikacji problemów\nGdy badane procesy uważa się za uniwersalne\n\nGłówne problemy:\n\nBrak podstaw do wnioskowania o populacji\nPrawdopodobne poważne obciążenie selekcyjne\nWyniki mogą być całkowicie mylące\n\nPrawdziwy przykład: Sondaż prezydencki Literary Digest z 1936 roku ankietował 2,4 miliona osób (ogromna próba!), ale używał książek telefonicznych i członkostwa w klubach jako operatów podczas Wielkiego Kryzysu, dramatycznie nadreprezentując bogatych wyborców i niepoprawnie przewidując, że Landon pokona Roosevelta.\n\n\nPróbkowanie celowe (Purposive/Judgmental Sampling)\nCelowy wybór konkretnych przypadków oparty na osądzie badacza o tym, co jest „typowe” lub „interesujące”.\nPrzykłady:\n\nWybór „typowych” wsi do reprezentowania obszarów wiejskich\nWybór konkretnych grup wiekowych do badania rozwojowego\nWybór skrajnych przypadków do zrozumienia zakresu zmienności\nWybór przypadków bogatych w informacje do dogłębnego badania\n\nTypy próbkowania celowego:\nTypowy przypadek: Wybierz przeciętne lub normalne przykłady\n\nBadanie „typowych” polskich przedmieść\n\nSkrajny/dewiacyjny przypadek: Wybierz niezwykłe przykłady\n\nBadanie wsi z niezwykle niską śmiertelnością niemowląt, aby zrozumieć czynniki sukcesu\n\nMaksymalna zmienność: Celowo wybierz różnorodne przypadki\n\nWybór różnych szkół (miejskich/wiejskich, bogatych/biednych, dużych/małych) do badań edukacyjnych\n\nPrzypadek krytyczny: Wybierz przypadki, które będą definitywne\n\n„Jeśli to nie działa tutaj, nie zadziała nigdzie”\n\nKiedy jest przydatne:\n\nBadania jakościowe skupiające się na głębi nad szerokością\nGdy badane są rzadkie populacje\nOgraniczenia zasobów poważnie limitują wielkość próby\nFazy eksploracyjne badań\n\nProblemy:\n\nCałkowicie zależne od osądu badacza\nNiemożliwe wnioskowanie statystyczne\nRóżni badacze mogą wybrać różne „typowe” przypadki\n\n\n\nPróbkowanie kwotowe (Quota Sampling)\nSelekcja w celu dopasowania proporcji populacji w kluczowych charakterystykach. Jak losowanie warstwowe, ale bez losowej selekcji w grupach.\nJak działa próbkowanie kwotowe:\n\nZidentyfikuj kluczowe charakterystyki (wiek, płeć, rasa, wykształcenie)\nOkreśl proporcje populacji dla tych charakterystyk\nUstaw kwoty dla każdej kombinacji\nAnkieterzy wypełniają kwoty używając metod wygodnych\n\nSzczegółowy przykład: Sondaż polityczny z kwotami:\nProporcje populacji:\n\nMężczyzna 18-34: 15%\nMężczyzna 35-54: 20%\nMężczyzna 55+: 15%\nKobieta 18-34: 16%\nKobieta 35-54: 19%\nKobieta 55+: 15%\n\nDla próby 1000:\n\nWywiad z 150 mężczyznami w wieku 18-34\nWywiad z 200 mężczyznami w wieku 35-54\nI tak dalej…\n\nAnkieterzy mogą stać na rogach ulic, podchodząc do osób, które wydają się pasować do potrzebnych kategorii, aż kwoty zostaną wypełnione.\nDlaczego jest popularne w badaniach rynkowych:\n\nSzybsze niż próbkowanie probabilistyczne\nTańsze (brak ponownych kontaktów dla konkretnych osób)\nZapewnia reprezentację demograficzną\nNie wymaga operatu losowania\n\nDlaczego jest problematyczne dla wnioskowania statystycznego:\nUkryte obciążenie selekcyjne: Ankieterzy podchodzą do osób, które wyglądają na przystępne, dobrze mówią językiem, nie spieszą się — systematycznie wykluczając pewne typy w każdej komórce kwotowej.\nPrzykład obciążenia: Ankieter wypełniający kwotę dla „kobiet 18-34” może podchodzić do kobiet w centrum handlowym we wtorek po południu, systematycznie pomijając:\n\nKobiety pracujące w dni powszednie\nKobiety, których nie stać na zakupy w centrach handlowych\nKobiety z małymi dziećmi, które unikają centrów handlowych\nKobiety robiące zakupy online\n\nMimo że końcowa próba ma „właściwą” proporcję młodych kobiet, nie są one reprezentatywne dla wszystkich młodych kobiet.\nBrak miary błędu próbkowania: Bez prawdopodobieństw selekcji nie możemy obliczyć błędów standardowych ani przedziałów ufności.\nHistoryczna przestroga: Próbkowanie kwotowe było standardem w sondażach do wyborów prezydenckich w USA w 1948 roku, gdy sondaże używające próbkowania kwotowego niepoprawnie przewidziały, że Dewey pokona Trumana. Niepowodzenie doprowadziło do przyjęcia próbkowania probabilistycznego w sondażach.\n\n\nPróbkowanie kuli śnieżnej (Snowball Sampling)\nUczestnicy rekrutują dodatkowych uczestników ze swoich znajomych. Próba rośnie jak tocząca się kula śnieżna.\nJak to działa:\n\nZidentyfikuj początkowych uczestników (nasiona)\nPoproś ich o polecenie innych z wymaganymi charakterystykami\nPoproś nowych uczestników o dalsze polecenia\nKontynuuj, aż osiągnięta zostanie wielkość próby lub wyczerpią się polecenia\n\nPrzykład: Badanie nieudokumentowanych imigrantów:\n\nZacznij od 5 imigrantów, których możesz zidentyfikować\nKażdy poleca 3 innych, których zna\nTych 15 każdy poleca 2-3 innych\nKontynuuj, aż masz 100+ uczestników\n\nKiedy jest wartościowe:\nUkryte populacje: Grupy bez operatów losowania\n\nUżytkownicy narkotyków\nOsoby bezdomne\nOsoby z rzadkimi chorobami\nCzłonkowie ruchów podziemnych\n\nPopulacje połączone społecznie: Gdy relacje mają znaczenie\n\nBadanie efektów sieci społecznych\nBadanie transmisji chorób w społeczności\nZrozumienie dyfuzji informacji\n\nBadania zależne od zaufania: Gdy polecenia zwiększają uczestnictwo\n\nWrażliwe tematy, gdzie zaufanie jest niezbędne\nZamknięte społeczności podejrzliwe wobec obcych\n\nGłówne ograniczenia:\n\nPróby obciążone w kierunku osób współpracujących, dobrze połączonych\nOdizolowani członkowie populacji całkowicie pominięci\nWnioskowanie statystyczne generalnie niemożliwe\nMoże wzmacniać podziały społeczne (łańcuchy rzadko przekraczają granice społeczne)\n\nZaawansowana wersja — Próbkowanie sterowane przez respondentów (Respondent-Driven Sampling - RDS):\nPróbuje uczynić próbkowanie kuli śnieżnej bardziej rygorystycznym poprzez:\n\nŚledzenie, kto zrekrutował kogo\nOgraniczanie liczby poleceń na osobę\nWażenie na podstawie wielkości sieci\nUżywanie modeli matematycznych do korekty obciążenia\n\nNadal kontrowersyjne, czy RDS naprawdę pozwala na ważne wnioskowanie.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#pojęcia-prawdopodobieństwa-w-analizie-statystycznej",
    "href": "rozdzial1.html#pojęcia-prawdopodobieństwa-w-analizie-statystycznej",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.11 Pojęcia prawdopodobieństwa w analizie statystycznej",
    "text": "2.11 Pojęcia prawdopodobieństwa w analizie statystycznej\nChoć to przede wszystkim kurs statystyki, zrozumienie podstawowego prawdopodobieństwa jest niezbędne dla wnioskowania statystycznego.\n\nPodstawowe prawdopodobieństwo\nPrawdopodobieństwo kwantyfikuje niepewność na skali od 0 (niemożliwe) do 1 (pewne).\nPrawdopodobieństwo klasyczne: P(\\text{zdarzenie}) = \\frac{\\text{Liczba korzystnych wyników}}{\\text{Całkowita liczba możliwych wyników}}\nPrzykład: Prawdopodobieństwo, że losowo wybrana osoba jest kobietą \\approx 0,5\nPrawdopodobieństwo empiryczne: Oparte na obserwowanych częstościach\nPrzykład: W wiosce 423 z 1000 mieszkańców to kobiety, więc P(\\text{kobieta}) \\approx 0,423\n\n\nPrawdopodobieństwo warunkowe\nPrawdopodobieństwo warunkowe to prawdopodobieństwo zdarzenia A, przy założeniu że zdarzenie B wystąpiło: P(A|B)\nPrzykład demograficzny: Prawdopodobieństwo śmierci w ciągu roku przy danym wieku:\n\nP(\\text{śmierć w ciągu roku} | \\text{wiek 30}) \\approx 0,001\nP(\\text{śmierć w ciągu roku} | \\text{wiek 80}) \\approx 0,05\n\nTe prawdopodobieństwa warunkowe stanowią podstawę tablic trwania życia.\n\n\nNiezależność\nZdarzenia A i B są niezależne, jeśli P(A|B) = P(A).\nTestowanie niezależności w danych demograficznych:\nCzy wykształcenie i płodność są niezależne?\n\nP(\\text{3+ dzieci}) = 0,3 ogólnie\nP(\\text{3+ dzieci} | \\text{wykształcenie wyższe}) = 0,15\nRóżne prawdopodobieństwa wskazują na zależność\n\n\n\nPrawo wielkich liczb\nGdy wielkość próby wzrasta, statystyki z próby zbiegają się do parametrów populacji.\nDemonstracja: Szacowanie proporcji płci przy urodzeniu:\n\n10 urodzeń: 7 chłopców (70% - bardzo niestabilne)\n100 urodzeń: 53 chłopców (53% - zbliżamy się do ~51,2%)\n1000 urodzeń: 515 chłopców (51,5% - całkiem blisko)\n10 000 urodzeń: 5118 chłopców (51,18% - bardzo blisko)\n\n\n\nWizualizacja Prawa wielkich liczb: rzuty monetą\nZobaczmy to w działaniu na przykładzie rzutów monetą. Uczciwa moneta ma 50% szansy na wypadnięcie orła, ale poszczególne rzuty są nieprzewidywalne.\n\n# Symulacja rzutów monetą i pokazanie zbieżności\nset.seed(42)\nn_flips &lt;- 1000\nflips &lt;- rbinom(n_flips, 1, 0.5)  # 1 = orzeł, 0 = reszka\n\n# Obliczanie skumulowanej proporcji orłów\ncumulative_prop &lt;- cumsum(flips) / seq_along(flips)\n\n# Utworzenie ramki danych do wizualizacji\nlln_data &lt;- data.frame(\n  flip_number = 1:n_flips,\n  cumulative_proportion = cumulative_prop\n)\n\n# Wykres zbieżności\nggplot(lln_data, aes(x = flip_number, y = cumulative_proportion)) +\n  geom_line(color = \"steelblue\", alpha = 0.7) +\n  geom_hline(yintercept = 0.5, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_hline(yintercept = c(0.45, 0.55), color = \"red\", linetype = \"dotted\", alpha = 0.7) +\n  labs(\n    title = \"Prawo wielkich liczb: Proporcje rzutów monetą zbiegają do 0,5\",\n    x = \"Liczba rzutów monetą\",\n    y = \"Skumulowana proporcja orłów\",\n    caption = \"Czerwona linia przerywana = prawdziwe prawdopodobieństwo (0,5)\\nLinie kropkowane = zakres ±5%\"\n  ) +\n  scale_y_continuous(limits = c(0.3, 0.7), breaks = seq(0.3, 0.7, 0.1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCo to pokazuje:\n\nPoczątkowe rzuty wykazują duże wahania (pierwsze 10 rzutów może dać 70% lub 30% orłów)\nW miarę dodawania kolejnych rzutów, proporcja stabilizuje się wokół 50%\n„Szum” poszczególnych wyników się uśrednia w czasie\n\n\n\nSformułowanie matematyczne\nNiech A oznacza zdarzenie nas interesujące (np. „orzeł w rzucie monetą”, „głos na partię X”, „suma kostek równa 7”). Jeśli P(A) = p i obserwujemy n niezależnych prób z tym samym rozkładem (i.i.d.), to częstość próbkowa zdarzenia A:\n\\hat{p}_n = \\frac{\\text{liczba wystąpień zdarzenia } A}{n}\nzbiega do p gdy n rośnie.\n\n\nPrzykłady w różnych kontekstach\nPrzykład z kostkami: Zdarzenie „suma = 7” przy dwóch kostkach ma prawdopodobieństwo 6/36 ≈ 16,7\\%, podczas gdy „suma = 4” ma 3/36 ≈ 8,3\\%. Przy wielu rzutach suma 7 pojawia się około dwa razy częściej niż suma 4.\nSondaże wyborcze: Jeśli poparcie populacyjne dla partii wynosi p, to przy losowym doborze próby o wielkości n obserwowana częstość \\hat{p}_n będzie zbliżać się do p w miarę wzrostu n (zakładając losowy dobór i niezależność prób).\nKontrola jakości: Jeśli 2% produktów jest wadliwych, to w dużych partiach około 2% zostanie uznanych za wadliwe (zakładając niezależną produkcję).\n\n\nDlaczego to ma znaczenie dla statystyki\nWniosek: Losowość stanowi podstawę wnioskowania statystycznego, przekształcając niepewność poszczególnych wyników w przewidywalne rozkłady dla estymatorów. Prawo wielkich liczb gwarantuje, że „szum” poszczególnych wyników się uśrednia, pozwalając nam:\n\nPrzewidywać długookresowe częstości\nKwantyfikować niepewność (marginesy błędu)\nWyciągać rzetelne wnioski z prób\nFormułować probabilistyczne stwierdzenia o populacjach\n\nTa zasada działa w sondażach, eksperymentach, a nawet w zjawiskach kwantowych (w interpretacji częstościowej).\n\n\n\nCentralne Twierdzenie Graniczne (CTG)\nCentralne Twierdzenie Graniczne stwierdza, że rozkład średnich próbkowych zbliża się do rozkładu normalnego wraz ze wzrostem wielkości próby, niezależnie od kształtu pierwotnego rozkładu populacji. Jest to prawdziwe nawet dla wysoce skośnych lub nienormalnych rozkładów populacji.\n\nImplikacje\n\nPróg Wielkości Próby: Wielkość próby n ≥ 30 jest zazwyczaj wystarczająca, aby zastosować CTG\nBłąd Standardowy: Odchylenie standardowe średnich próbkowych wynosi σ/√n, gdzie σ to odchylenie standardowe populacji\nFundament Statystyczny: Możemy dokonywać wnioskowań o parametrach populacji używając właściwości rozkładu normalnego, nawet gdy dane bazowe nie są normalne\n\n\n\nDlaczego To Ma Znaczenie\nRozważmy dane o dochodach, które zazwyczaj są prawostronnie skośne z długim ogonem wysokich zarobków. Podczas gdy indywidualne dochody nie podlegają rozkładowi normalnemu, dzieje się coś niezwykłego, gdy wielokrotnie pobieramy próby i obliczamy ich średnie:\nCo właściwie oznacza “normalnie rozłożone średnie próbkowe”:\n\nJeśli weźmiesz wiele różnych grup 30+ osób i obliczysz średni dochód każdej grupy\nTe średnie grupowe utworzą wzór w kształcie dzwonu po nanieseniu na wykres\nWiększość średnich grupowych skupi się blisko prawdziwej średniej populacji\nPrawdopodobieństwo otrzymania średniej grupowej daleko od średniej populacji staje się przewidywalne\n\nTen przewidywalny wzór (rozkład normalny) pozwala nam:\n\nObliczać przedziały ufności używając właściwości rozkładu normalnego\nPrzeprowadzać testy hipotez statystycznych\nDokonywać przewidywań dotyczących średnich próbkowych ze znanym prawdopodobieństwem\n\nKonkretny Przykład: Wyobraź sobie miasto, w którym indywidualne dochody wahają się od 80 000 zł do 40 000 000 zł, silnie skośne w prawo. Jeśli:\n\nLosowo wybierzesz 100 osób i obliczysz ich średni dochód: powiedzmy 300 000 zł\nPowtórzysz to 1000 razy (1000 różnych grup po 100 osób)\nNaniesieszz na wykres te 1000 średnich grupowych: utworzą krzywą dzwonową wycentrowaną wokół prawdziwej średniej populacji\nOkoło 95% tych średnich grupowych znajdzie się w przewidywalnym zakresie\nDzieje się tak mimo że indywidualne dochody są skrajnie skośne!\n\n\n\nPodstawy Matematyczne\nDla populacji ze średnią μ i skończoną wariancją σ²:\n\nRozkład próbkowy średniej: \\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n}) gdy n \\to \\infty\nBłąd standardowy średniej: SE_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\nStandaryzowana średnia próbkowa: Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1) dla dużych n\n\n\n\nNajważniejsze Wnioski\n\nUniwersalne Zastosowanie: CTG ma zastosowanie do każdego rozkładu ze skończoną wariancją\nZbieżność do Normalności: Aproksymacja do rozkładu normalnego poprawia się wraz ze wzrostem wielkości próby\nFundament Wnioskowania: Większość parametrycznych testów statystycznych opiera się na CTG\nKwestie Wielkości Próby: Chociaż n ≥ 30 jest podstawową wytyczną, wysoce skośne rozkłady mogą wymagać większych próbek dla dokładnej aproksymacji",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#istotność-statystyczna-wprowadzenie",
    "href": "rozdzial1.html#istotność-statystyczna-wprowadzenie",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.12 Istotność Statystyczna: Wprowadzenie",
    "text": "2.12 Istotność Statystyczna: Wprowadzenie\nWyobraź sobie, że rzucasz monetą 10 razy i wypadło 8 orłów. Czy moneta jest fałszywa, czy po prostu miałeś szczęście? To jest kluczowe pytanie, na które pomaga odpowiedzieć istotność statystyczna (wnioskowanie statystyczne).\nIstotność statystyczna to miara (p-value) tego, na ile możemy być pewni, że wzorce obserwowane w naszej próbie nie są dziełem przypadku. Gdy wynik jest statystycznie istotny (zwykle przyjmujemy p-value &lt; 0.05), oznacza to, że prawdopodobieństwo uzyskania takich danych przy braku rzeczywistego efektu jest bardzo niskie.\nIstotność statystyczna pomaga nam rozróżnić między rzeczywistymi zjawiskami a przypadkowymi fluktuacjami w danych. Gdy mówimy, że wynik jest statystycznie istotny, znaczy to, że prawdopodobnie nie powstał przez zwykły zbieg okoliczności.\n\nAnalogia do Sali Sądowej\nTestowanie hipotez statystycznych działa jak proces karny:\n\nHipoteza Zerowa (H_0): Oskarżony jest niewinny (nie ma efektu)\nHipoteza Alternatywna (H_1): Oskarżony jest winny (efekt istnieje)\nDowody: Twoje dane i wyniki testów\nWerdykt: “Winny” (odrzuć H_0) lub “Niewinny” (nie odrzucaj H_0)\n\nKluczowe rozróżnienie: “Niewinny” ≠ “Niewinny”\n\nWerdykt “niewinny” oznacza niewystarczające dowody do skazania\nPodobnie, “brak istotności statystycznej” oznacza niewystarczające dowody na istnienie efektu, NIE dowód braku efektu\n\n\n\nBrak efektu (“Domniemanie niewinności”)\nW statystyce zawsze zaczynamy od założenia, że nic specjalnego się nie dzieje:\n\nHipoteza Zerowa (H_0): “Nie ma efektu”\n\nMoneta jest uczciwa\nNowy lek nie działa\nCzas nauki nie wpływa na wyniki w nauce\n\nHipoteza Alternatywna (H_1): “Efekt ISTNIEJE”\n\nMoneta jest fałszywa\nLek działa\nWięcej nauki poprawia oceny\n\n\nKluczowa zasada: Podtrzymujemy hipotezę zerową (niewinność), chyba że dane dostarczą mocnych dowodów przeciwko niej — “ponad wszelką wątpliwość” w terminologii prawnej, lub “p &lt; 0,05” w terminologii statystycznej.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wartość-p-p-value-twój-miernik-zaskoczenia",
    "href": "rozdzial1.html#wartość-p-p-value-twój-miernik-zaskoczenia",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.13 Wartość p (p-value): Twój “Miernik Zaskoczenia”",
    "text": "2.13 Wartość p (p-value): Twój “Miernik Zaskoczenia”\nWartość p odpowiada na jedno konkretne pytanie:\n\n“Gdyby nic specjalnego się nie działo (hipoteza zerowa jest prawdziwa), jak zaskakujące byłyby nasze wyniki?”\n\n\nWartość p, p-wartość, prawdopodobieństwo testowe (ang. p-value, probability value) – prawdopodobieństwo uzyskania wyników testu co najmniej tak samo skrajnych, jak te zaobserwowane w rzeczywistości (w próbie badawczej), obliczone przy założeniu, że hipoteza zerowa (brak efektu, różnicy, itp.) jest prawdziwa.\n\n\nTrzy Sposoby Myślenia o Wartościach p\n\n1. Skala Zaskoczenia\n\np &lt; 0,01: Bardzo zaskakujące! (Mocne dowody przeciwko H_0)\np &lt; 0,05: Dość zaskakujące (Umiarkowane dowody przeciwko H_0)\np &gt; 0,05: Niezbyt zaskakujące (Niewystarczające dowody przeciwko H_0)\n\n\n\n2. Konkretny Przykład: Podejrzana Moneta\nRzucasz monetą 10 razy i wypadło 8 orłów. Jaka jest wartość p?\nObliczenie: Jeśli moneta byłaby uczciwa, prawdopodobieństwo uzyskania 8 lub więcej orłów wynosi:\np = P(≥8 \\text{ orłów w 10 rzutach}) \\approx 0.055 \\approx 5.5\\%\nP(X \\geq 8) = \\sum_{k=8}^{10} \\binom{10}{k} 0,5^{10} = \\frac{56}{1024} \\approx 0,0547\nInterpretacja: Jest 5,5% szans na uzyskanie tak ekstremalnych wyników z uczciwą monetą. To trochę nietypowe, ale nie jest to skrajnie nieprawdopodobny wynik.\n\n\n3. Formalna Definicja\nWartość p to prawdopodobieństwo uzyskania wyników co najmniej tak ekstremalnych jak zaobserwowane, zakładając że hipoteza zerowa jest prawdziwa.\n\n\n\n\n\n\nWarning\n\n\n\nCzęsty Błąd: Wartość p NIE jest prawdopodobieństwem, że hipoteza zerowa jest prawdziwa! Zakłada ona, że hipoteza zerowa jest prawdziwa i mówi, jak nietypowe byłyby twoje dane w tym świecie (w którym H_0 jest prawdziwa).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#błąd-rozumowania-prokuratorskiego-ostrzeżenie",
    "href": "rozdzial1.html#błąd-rozumowania-prokuratorskiego-ostrzeżenie",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.14 Błąd Rozumowania Prokuratorskiego: Ostrzeżenie",
    "text": "2.14 Błąd Rozumowania Prokuratorskiego: Ostrzeżenie\n\nWyjaśnienie błędu\nWyobraź sobie taką scenę w sądzie:\nProkurator: “Jeśli oskarżony byłby niewinny, istnieje tylko 1% szans, że znaleźlibyśmy jego DNA na miejscu zbrodni. Znaleźliśmy jego DNA. Zatem istnieje 99% pewności, że jest winny!”\nTo BŁĄD! Prokurator pomylił:\n\nP(Dowód | Niewinny) = 0,01 ← To, co wiemy\nP(Niewinny | Dowód) = ? ← To, co chcemy wiedzieć (ale nie możemy tego wywnioskować z samej wartości p!)\n\n\nGdy otrzymujemy p = 0,01, kuszące jest myślenie:\n❌ ŹLE: “Jest tylko 1% szans, że hipoteza zerowa jest prawdziwa”\n❌ ŹLE: “Jest 99% szans, że nasze leczenie działa”\n✅ DOBRZE: “Jeśli hipoteza zerowa byłaby prawdziwa, istnieje tylko 1% szans, że zobaczylibyśmy tak ekstremalne dane”\n\n\nDlaczego to ważne: Prosty przykład testu medycznego\nWyobraź sobie test na rzadką chorobę, który jest dokładny w 99%:\n\nJeśli masz chorobę, test jest pozytywny w 99% przypadków\nJeśli nie masz choroby, test jest negatywny w 99% przypadków (czyli 1% wyników fałszywie pozytywnych)\n\nOto klucz: Załóżmy, że tylko 1 na 1000 osób faktycznie ma tę chorobę.\nPrzetestujmy 10 000 osób:\n\n10 osób ma chorobę → 10 ma pozytywny wynik testu (w zaokrągleniu)\n9 990 osób nie ma choroby → około 100 ma pozytywny wynik przez pomyłkę (1% z 9 990)\nŁącznie pozytywnych testów: 110\n\nJeśli twój test jest pozytywny, jakie jest prawdopodobieństwo, że rzeczywiście masz chorobę?\n\nTylko 10 ze 110 pozytywnych testów to prawdziwe przypadki\nTo około 9%, nie 99%!\n\n\n\nAnalogia do badań naukowych\nTo samo dzieje się w badaniach:\n\nGdy testujemy wiele hipotez (jak testowanie wielu potencjalnych leków)\nWiększość nie działa (jak większość ludzi nie ma rzadkiej choroby)\nNawet przy “istotnych” wynikach (jak pozytywny test), większość odkryć może być fałszywie pozytywna\n\n\n\n\n\n\n\nImportant\n\n\n\nWartość p mówi ci, jak zaskakujące byłyby twoje dane, GDYBY hipoteza zerowa była prawdziwa. Nie mówi ci o prawdopodobieństwie, że hipoteza zerowa JEST prawdziwa.\nPomyśl o tym tak: Prawdopodobieństwo, że ziemia będzie mokra, JEŚLI padało, jest zupełnie inne niż prawdopodobieństwo, że padało, JEŚLI ziemia jest mokra — ziemia mogła być mokra od zraszacza!\nPamiętaj: Wartość p mówi ci P(Dane | Hipoteza zerowa jest prawdziwa), nie P(Hipoteza zerowa jest prawdziwa | Dane). To tak różne jak P(Mokra ziemia | Deszcz) i P(Deszcz | Mokra ziemia) — ziemia może być mokra od zraszacza!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wprowadzenie-do-analizy-regresji-modelowanie-relacji-między-zmiennymi",
    "href": "rozdzial1.html#wprowadzenie-do-analizy-regresji-modelowanie-relacji-między-zmiennymi",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.15 Wprowadzenie do analizy regresji: Modelowanie relacji między zmiennymi",
    "text": "2.15 Wprowadzenie do analizy regresji: Modelowanie relacji między zmiennymi\nZanim rozpoczniemy omawianie analizy regresji, musimy zrozumieć, co rozumiemy przez model w dociekaniach naukowych. Model to uproszczona, abstrakcyjna reprezentacja zjawiska lub systemu ze świata rzeczywistego. Modele celowo pomijają szczegóły, aby skupić się na istotnych relacjach, które staramy się zrozumieć. Nie są one stworzone po to, by uchwycić każdy aspekt rzeczywistości—co byłoby niemożliwie skomplikowane—ale raczej by służyć jako narzędzia pomagające nam identyfikować wzorce, dokonywać predykcji, testować hipotezy oraz jasno komunikować nasze idee. Statystyk George Box doskonale uchwycił tę ideę, zauważając, że „wszystkie modele są błędne, ale niektóre są użyteczne”. Innymi słowy, choć wiemy, że nasze modele nie reprezentują rzeczywistości w sposób doskonały, mogą one wciąż dostarczać cennych spostrzeżeń na temat badanych przez nas zjawisk.\nAnaliza regresji jest fundamentalną metodą statystyczną służącą do modelowania związków między zmiennymi. Konkretnie, pomaga nam zrozumieć, w jaki sposób jedna lub więcej zmiennych niezależnych (nazywanych również predyktorami lub zmiennymi objaśniającymi) jest powiązanych ze zmienną zależną (zmienną wynikową lub zmienną odpowiedzi, którą chcemy wyjaśnić lub przewidzieć). Celem analizy regresji jest skwantyfikowanie tych relacji oraz, gdy jest to stosowne, przewidywanie wartości zmiennej zależnej na podstawie zmiennych niezależnych.\nW swojej najprostszej formie, nazywanej prostą regresją liniową, modelujemy związek między pojedynczą zmienną niezależną X a zmienną zależną Y za pomocą równania:\nY = \\beta_0 + \\beta_1 X + \\varepsilon\ngdzie \\beta_0 reprezentuje wyraz wolny, \\beta_1 reprezentuje nachylenie (pokazujące, o ile zmienia się Y dla każdej jednostki zmiany X), a \\varepsilon reprezentuje składnik losowy—część Y, której nasz model nie potrafi wyjaśnić.\n\nJednym z najważniejszych narzędzi w analizie statystycznej jest analiza regresji — metoda zrozumienia i kwantyfikacji relacji między zmiennymi.\nPodstawowa idea jest prosta: Jak jedna rzecz odnosi się do drugiej i czy możemy użyć tej relacji do dokonywania przewidywań (np. jak liczba lat nauki wpływa na dochody?)?\n\nW jednym zdaniu: Regresja pomaga nam zrozumieć, jak różne zjawiska są ze sobą powiązane w skomplikowanym świecie, gdzie wszystko wpływa na wszystko inne.\n\n\nCzym jest analiza regresji?\nWyobraź sobie, że jesteś ciekawy relacji między wykształceniem a dochodem. Zauważasz, że ludzie z większym wykształceniem zwykle zarabiają więcej pieniędzy, ale chcesz zrozumieć tę relację bardziej precyzyjnie:\n\nO ile średnio każdy dodatkowy rok edukacji zwiększa dochód?\nJak silna jest ta relacja?\nCzy są inne czynniki, które powinniśmy rozważyć?\nCzy możemy przewidzieć prawdopodobny dochód kogoś, jeśli znamy jego poziom wykształcenia?\n\nAnaliza regresji w sposób systematyczny odpowiada na te pytania — szuka najlepiej dopasowanego opisu relacji między zmiennymi.\n\n\nZmienne i Zmienność\nZmienna to każda charakterystyka, która może przyjmować różne wartości dla różnych jednostek obserwacji. W naukach politycznych:\n\nJednostki analizy: Kraje, osoby, wybory, polityki, lata\nZmienne: PKB, preferencje wyborcze, wskaźnik demokracji, wystąpienie konfliktu\n\n\n💡 Mówiąc Prosto: Zmienna to wszystko, co się zmienia. Gdyby wszyscy głosowali tak samo, “preferencje wyborcze” nie byłyby zmienną - byłyby stałą. Badamy zmienne, ponieważ chcemy zrozumieć, dlaczego rzeczy się różnią.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRozważmy typowy nagłówek prasowy przed wyborami: „Poparcie dla kandydata Kowalskiego sięga 68%.” Najprawdopodobniej wyciągniesz wniosek, że Kowalski ma dobre perspektywy wyborcze—nie gwarantowane zwycięstwo, ale silną pozycję. Intuicyjnie rozumiesz, że wyższe poparcie zwykle przekłada się na lepsze wyniki wyborcze, nawet jeśli związek ten nie jest doskonały.\nTa intuicyjna ocena ilustruje istotę analizy regresji. Wykorzystałeś jedną informację (wskaźnik poparcia), aby przewidzieć inny wynik (sukces wyborczy). Co więcej, rozpoznałeś zarówno związek między tymi zmiennymi, jak i niepewność związaną z twoją prognozą.\nChociaż takie nieformalne rozumowanie dobrze nam służy w życiu codziennym, ma istotne ograniczenia. O ile lepsze są szanse Kowalskiego przy 68% poparciu w porównaniu do 58%? Co się dzieje, gdy musimy jednocześnie uwzględnić wiele czynników—poparcie, sytuację gospodarczą i status urzędującego kandydata? Jak pewni powinniśmy być naszych prognoz?\nAnaliza regresji dostarcza systematycznych ram do odpowiedzi na te pytania. Przekształca nasze intuicyjne rozumienie związków w precyzyjne modele matematyczne, które można testować i udoskonalać. Dzięki analizie regresji badacze mogą:\n\nGenerować precyzyjne prognozy: Wyjść poza ogólne oceny ku konkretnym liczbowym szacunkom—na przykład przewidywać nie tylko, że Kowalski „prawdopodobnie wygra”, ale oszacować oczekiwany procent głosów i zakres prawdopodobnych wyników.\nOkreślić, które czynniki są najważniejsze: Ustalić względne znaczenie różnych zmiennych—być może odkrywając, że warunki gospodarcze wpływają na wybory silniej niż wskaźniki poparcia.\nOkreślić ilościowo niepewność prognoz: Dokładnie zmierzyć, jak pewni powinniśmy być naszych przewidywań, rozróżniając między niemal pewnymi wynikami a edukowanymi przypuszczeniami.\nTestować propozycje teoretyczne danymi empirycznymi: Ocenić, czy nasze przekonania o związkach przyczynowo-skutkowych sprawdzają się, gdy testujemy je systematycznie na wielu obserwacjach.\n\n\nW istocie analiza regresji systematyzuje rozpoznawanie wzorców, które wykonujemy intuicyjnie, dostarczając narzędzi do tego, aby nasze prognozy były dokładniejsze, nasze porównania bardziej znaczące, a nasze wnioski bardziej wiarygodne.\n\n\n\nModel Podstawowy\nModel reprezentuje obiekt, osobę lub system w sposób informatywny. Modele dzielą się na reprezentacje fizyczne (takie jak modele architektoniczne) i abstrakcyjne (takie jak równania matematyczne opisujące dynamikę atmosfery).\nRdzeń myślenia statystycznego można wyrazić jako:\nY = f(X) + \\text{błąd}\nTo równanie stwierdza, że nasz wynik (Y) równa się jakiejś funkcji naszych predyktorów (X), plus nieprzewidywalna zmienność.\nSkładniki:\n\nY = Zmienna zależna (zjawisko, które chcemy wyjaśnić)\nX = Zmienna(e) niezależna(e) (czynniki wyjaśniające)\nf() = Związek funkcyjny (często zakładamy liniowy)\nbłąd (\\epsilon) = Niewyjaśniona zmienność\n\n\n💡 Co To Naprawdę Oznacza: Można to porównać do przepisu kulinarnego. Ocena z przedmiotu (Y) zależy od godzin nauki (X), ale nie doskonale. Dwóch studentów uczących się 10 godzin może otrzymać różne oceny z powodu stresu przed egzaminem, wcześniejszej wiedzy czy po prostu szczęścia (składnik błędu). Regresja znajduje średni związek.\n\nTen model stanowi podstawę całej analizy statystycznej - od prostych korelacji po złożone algorytmy uczenia maszynowego.\nRegresja pomaga odpowiedzieć na fundamentalne pytania takie jak:\n\nO ile edukacja zwiększa uczestnictwo polityczne?\nJakie czynniki przewidują sukces wyborczy?\nCzy instytucje demokratyczne promują wzrost gospodarczy?\n\n\n\n\n\n\n\nPodstawowa idea: Rysowanie najlepszej linii przez punkty\n\nProsta regresja liniowa (Simple Linear Regression)\nZacznijmy od najprostszego przypadku: relacji między dwiema zmiennymi. Załóżmy, że rysujemy wykształcenie (lata nauki) na osi x i roczny dochód na osi y dla 100 osób. Zobaczylibyśmy chmurę punktów, a regresja znajduje prostą linię, która najlepiej reprezentuje wzorzec w tych punktach.\nCo czyni linię „najlepszą”? Linia regresji minimalizuje całkowitą sumę kwadratów pionowych odległości od wszystkich punktów do linii. Pomyśl o tym jako o znalezieniu linii, która tworzy najmniejszy całkowity błąd predykcji.\nRównanie tej linii to: Y = a + bX + \\text{błąd}\nLub w naszym przykładzie: \\text{Dochód} = a + b \\times \\text{Wykształcenie} + \\text{błąd}\nGdzie:\n\na (wyraz wolny/intercept) = przewidywany dochód przy zerowym wykształceniu\nb (nachylenie/slope) = zmiana dochodu na każdy dodatkowy rok wykształcenia\nbłąd (e) = różnica między rzeczywistym a przewidywanym dochodem\n\nInterpretacja wyników:\nJeśli nasza analiza znajduje: \\text{Dochód} = 15000 + 4000 \\times \\text{Wykształcenie}\nTo mówi nam:\n\nKtoś z 0 latami wykształcenia przewidywany jest na zarobki 15 000 zł\nKażdy dodatkowy rok wykształcenia jest związany z 4000 zł większym dochodem\nKtoś z 12 latami wykształcenia przewidywany jest na zarobki: 15 000 + (4000 × 12) = 63 000 zł\nKtoś z 16 latami (licencjat) przewidywany jest na zarobki: 15 000 + (4000 × 16) = 79 000 zł\n\n\n\n\nZrozumienie relacji vs. dowodzenie przyczynowości\nKluczowe rozróżnienie: regresja pokazuje związek (association), niekoniecznie przyczynowość (causation). Nasza regresja wykształcenie-dochód pokazuje, że są powiązane, ale nie dowodzi, że wykształcenie powoduje wyższy dochód. Inne wyjaśnienia są możliwe:\n\nOdwrotna przyczynowość: Może bogatsze rodziny mogą sobie pozwolić na więcej edukacji dla swoich dzieci\nWspólna przyczyna: Być może inteligencja lub motywacja wpływa zarówno na wykształcenie, jak i dochód\nZbieg okoliczności: W małych próbach wzorce mogą pojawić się przez przypadek\n\nPrzykład pozornej korelacji: Regresja może pokazać, że sprzedaż lodów silnie przewiduje utopienia. Czy lody powodują utopienia? Nie! Oba wzrastają latem (wspólna przyczyna, confounding variable).\n\n\n\nRegresja wieloraka (Multiple Regression): Kontrolowanie innych czynników\nRzeczywistość jest skomplikowana — wiele czynników wpływa na wyniki jednocześnie. Regresja wieloraka pozwala nam badać jedną relację, jednocześnie „kontrolując” lub „utrzymując na stałym poziomie” inne zmienne.\n\nMoc kontroli statystycznej\nWracając do wykształcenia i dochodu, możemy się zastanawiać: Czy efekt wykształcenia wynika tylko z tego, że wykształceni ludzie są zwykle z bogatszych rodzin lub mieszkają w miastach? Regresja wieloraka może oddzielić te efekty:\n\\text{Dochód} = a + b_1 \\times \\text{Wykształcenie} + b_2 \\times \\text{Wiek} + b_3 \\times \\text{Miasto} + b_4 \\times \\text{Dochód rodziców} + \\text{błąd}\nTeraz b_1 reprezentuje efekt wykształcenia po uwzględnieniu wieku, lokalizacji i pochodzenia rodzinnego. Jeśli b_1 = 3000, oznacza to: „Porównując osoby w tym samym wieku, lokalizacji i pochodzeniu rodzinnym, każdy dodatkowy rok wykształcenia jest związany z 3000 zł większym dochodem.”\nPrzykład demograficzny: Płodność i wykształcenie kobiet\nBadacze badający płodność mogą znaleźć: \\text{Dzieci} = 4,5 - 0,3 \\times \\text{Wykształcenie}\nTo sugeruje, że każdy rok wykształcenia kobiet jest związany z 0,3 mniej dzieci. Ale czy wykształcenie jest przyczyną, czy wykształcone kobiety różnią się w innych aspektach? Dodając kontrole:\n\\text{Dzieci} = a - 0,15 \\times \\text{Wykształcenie} - 0,2 \\times \\text{Miasto} + 0,1 \\times \\text{Wykształcenie męża} - 0,4 \\times \\text{Dostęp do antykoncepcji}\nTeraz widzimy, że związek wykształcenia jest słabszy (-0,15 zamiast -0,3) po uwzględnieniu zamieszkania w mieście i dostępu do antykoncepcji. To sugeruje, że część pozornego efektu wykształcenia działa przez te inne ścieżki.\n\n\n\nTypy zmiennych w regresji\n\nZmienna wynikowa (zależna)\nTo jest to, co próbujemy zrozumieć lub przewidzieć:\n\nDochód w naszym pierwszym przykładzie\nLiczba dzieci w naszym przykładzie płodności\nOczekiwana długość życia w badaniach zdrowotnych\nPrawdopodobieństwo migracji w badaniach populacyjnych\n\n\n\nZmienne predykcyjne (niezależne)\nTo są czynniki, które według nas mogą wpływać na wynik:\n\nIlościowe: Wiek, lata wykształcenia, dochód, odległość\nJakościowe (kategorialne): Płeć, rasa, stan cywilny, region\nBinarne (dummy): Miasto/wieś, zatrudniony/bezrobotny, żonaty/nieżonaty\n\nObsługa zmiennych kategorialnych: Nie możemy bezpośrednio wstawić „religii” do równania. Zamiast tego tworzymy zmienne binarne:\n\nChrześcijanin = 1 jeśli chrześcijanin, 0 w przeciwnym razie\nMuzułmanin = 1 jeśli muzułmanin, 0 w przeciwnym razie\nBuddysta = 1 jeśli buddysta, 0 w przeciwnym razie\n(Jedna kategoria staje się grupą referencyjną)\n\n\n\n\nRóżne typy regresji dla różnych wyników\nPodstawowa idea regresji dostosowuje się do wielu sytuacji:\n\nRegresja liniowa\nDla wyników ilościowych (dochód, wzrost, ciśnienie krwi): Y = a + b_1X_1 + b_2X_2 + … + \\text{błąd}\n\n\nRegresja logistyczna\nDla wyników binarnych (zmarł/przeżył, wyemigrował/został, żonaty/nieżonaty):\nZamiast przewidywać wynik bezpośrednio, przewidujemy prawdopodobieństwo: \\log\\left(\\frac{p}{1-p}\\right) = a + b_1X_1 + b_2X_2 + …\nGdzie p to prawdopodobieństwo wystąpienia zdarzenia.\nPrzykład: Przewidywanie prawdopodobieństwa migracji na podstawie wieku, wykształcenia i stanu cywilnego. Model może stwierdzić, że młodzi, wykształceni, nieżonaci ludzie mają 40% prawdopodobieństwo migracji, podczas gdy starsi, mniej wykształceni, żonaci ludzie mają tylko 5% prawdopodobieństwo.\n\n\nRegresja Poissona\nDla wyników “zliczeniowych”/count data (liczba dzieci, liczba wizyt u lekarza): \\log(\\text{oczekiwana liczba}) = a + b_1X_1 + b_2X_2 + …\nPrzykład: Modelowanie liczby dzieci na podstawie charakterystyk kobiet. Przydatne, ponieważ zapewnia, że przewidywania nigdy nie są ujemne (nie można mieć -0,5 dziecka!).\n\n\nAnaliza przeżycia (model Coxa)/Regresja hazardu\nDo czego służy: Przewidywanie kiedy coś się stanie, nie tylko czy się stanie.\nProblem: Wyobraź sobie, że badasz jak długo trwają małżeństwa. Obserwujesz 1000 par przez 10 lat, ale na koniec badania: - 400 par się rozwiodło (wiesz dokładnie kiedy) - 600 par jest nadal w małżeństwie (nie wiesz czy/kiedy się rozwiodą)\nZwykła regresja nie radzi sobie z tym problemem “niekompletnej historii” — te 600 trwających małżeństw zawiera cenne informacje, ale nie znamy jeszcze ich zakończenia.\nJak pomagają modele Coxa: Zamiast próbować przewidzieć dokładny moment, skupiają się na ryzyku względnym — kto ma większą szansę na wcześniejsze doświadczenie zdarzenia. To jak pytanie “W dowolnym momencie, kto jest bardziej narażony?” zamiast “Dokładnie kiedy to się stanie?”\nZastosowania praktyczne: - Badania medyczne: Kto szybciej reaguje na leczenie? - Biznes: Którzy klienci wcześniej rezygnują z subskrypcji? - Nauki społeczne: Jakie czynniki powodują, że wydarzenia życiowe następują wcześniej/później?\n\n\n\n\nInterpretacja wyników regresji\n\nWspółczynniki\nWspółczynnik mówi nam o oczekiwanej zmianie wyniku przy wzroście predyktora o jedną jednostkę, przy zachowaniu stałości innych zmiennych.\nPrzykłady interpretacji:\nRegresja liniowa dla dochodu:\n\n„Każdy dodatkowy rok wykształcenia jest związany z 3500 zł wyższym rocznym dochodem, kontrolując wiek i doświadczenie”\n\nRegresja logistyczna dla śmiertelności niemowląt:\n\n„Każda dodatkowa wizyta prenatalna jest związana z 15% niższymi szansami śmierci niemowlęcia, kontrolując wiek i wykształcenie matki”\n\nRegresja wieloraka dla oczekiwanej długości życia:\n\n„Każde 1000 USD wzrostu PKB per capita jest związane z 0,4 roku dłuższą oczekiwaną długością życia, po kontroli wykształcenia i dostępu do opieki zdrowotnej”\n\n\n\nIstotność statystyczna\nRegresja testuje również, czy relacje mogą wynikać z przypadku:\n\nwartość p &lt; 0,05: Relacja nieprawdopodobna z powodu przypadku (statystycznie istotna)\nwartość p &gt; 0,05: Relacja może być prawdopodobnie losową zmiennością\n\n\nAle pamiętaj: Istotność statystyczna ≠ praktyczne znaczenie (“praktyczna istotność”). Przy dużych próbach malutkie efekty stają się „istotne”.\n\n\n\nPrzedziały ufności dla współczynników\nTak jak mamy przedziały ufności dla średnich lub propocji, mamy je dla współczynników regresji:\n„Efekt wykształcenia na dochód wynosi 3500 zł rocznie, 95% CI: [2800 zł, 4200 zł]”\nTo oznacza, że jesteśmy 95% pewni, że prawdziwy efekt mieści się między 2800 zł a 4200 zł.\n\n\nR-kwadrat: Jak dobrze model pasuje do danych?\nR^2 (R-kwadrat) mierzy proporcję zmienności wyniku wyjaśnioną przez predyktory:\n\nR^2 = 0: Predyktory nic nie wyjaśniają\nR^2 = 1: Predyktory wyjaśniają wszystko\nR^2 = 0,3: Predyktory wyjaśniają 30% zmienności\n\nPrzykład: Model dochodu z tylko wykształceniem może mieć R^2 = 0,15 (wykształcenie wyjaśnia 15% zmienności dochodu). Dodanie wieku, doświadczenia i lokalizacji może zwiększyć R^2 do 0,35 (razem wyjaśniają 35%).\n\n\n\n\n\n\nZałożenia i ograniczenia\n\n\n\nRegresja opiera się na założeniach, które mogą nie być spełnione:\n\nEgzogeniczność (brak ukrytych zależności)\nNajważniejsze założenie: predyktory nie mogą być skorelowane z błędami. Prościej mówiąc, nie powinny istnieć ukryte czynniki wpływające jednocześnie na zmienne objaśniające i wynik.\nPrzykład: Badając wpływ edukacji na dochód, ale pomijając “zdolności”, otrzymasz obciążone wyniki - zdolności wpływają zarówno na poziom wykształcenia, jak i dochód. To założenie zapisujemy jako: E[\\varepsilon | X] = 0\nDlaczego to kluczowe: Bez tego wszystkie twoje współczynniki są błędne, nawet przy milionach obserwacji!\n\n\nLiniowość\nZakłada związki prostoliniowe. A co jeśli wpływ edukacji na dochód jest silniejszy na wyższych poziomach? Możemy dodać człony wielomianowe: \\text{Dochód} = a + b_1 \\times \\text{Edukacja} + b_2 \\times \\text{Edukacja}^2\n\n\nNiezależność\nZakłada, że obserwacje są niezależne. Ale członkowie rodziny mogą być podobni, powtarzane pomiary tej samej osoby są powiązane, a sąsiedzi mogą na siebie wpływać. Specjalne metody radzą sobie z tymi zależnościami.\n\n\nHomoskedastyczność\nZakłada stałą wariancję błędów. Ale błędy predykcji mogą być większe dla osób o wysokich dochodach niż niskich. Wykresy diagnostyczne pomagają to wykryć.\n\n\nNormalność\nZakłada, że błędy mają rozkład normalny. Ważne dla małych prób i testów hipotez, mniej krytyczne dla dużych prób.\nUwaga: Pierwsze założenie (egzogeniczność) dotyczy otrzymania poprawnej odpowiedzi. Pozostałe dotyczą głównie precyzji i wnioskowania statystycznego. Naruszenie egzogeniczności oznacza, że model jest fundamentalnie błędny; naruszenie pozostałych oznacza, że przedziały ufności i p-wartości mogą być niedokładne.\n\n\n\n\n\n\n\n\n\nCzęste pułapki statystyczne\n\n\n\n\nEndogeniczność (obciążenie pominiętą zmienną): Zapominanie o ukrytych czynnikach wpływających zarówno na X jak i Y, co narusza fundamentalne założenie egzogeniczności. Przykład: Badanie edukacja→dochód bez uwzględnienia zdolności.\nSymultaniczność/Odwrotna przyczynowość: Gdy X i Y określają się wzajemnie w tym samym czasie. Prosta regresja zakłada jednokierunkową przyczynowość, ale rzeczywistość często jest dwukierunkowa. Przykład: Cena wpływa na popyt ORAZ popyt wpływa na cenę jednocześnie.\nZmienne zakłócające (confounding): Nieuwzględnienie zmiennych wpływających zarówno na predyktor jak i wynik, co prowadzi do pozornych zależności. Przykład: Sprzedaż lodów koreluje z utonięciami (oba powodowane przez lato).\nBłąd selekcji: Nielosowe próby systematycznie wykluczające pewne grupy, uniemożliwiające generalizację. Przykład: Badanie użycia internetu tylko wśród posiadaczy smartfonów.\nBłąd ekologiczny: Zakładanie, że wzorce grupowe dotyczą jednostek. Przykład: Bogate kraje mają niższą dzietność ≠ bogaci ludzie mają mniej dzieci.\nP-hacking (drążenie danych): Testowanie wielu hipotez aż do znalezienia istotności, lub modyfikowanie analizy aż p &lt; 0,05. Przy 20 testach spodziewasz się 1 fałszywego wyniku przez przypadek!\nPrzeuczenie (overfitting): Budowanie modelu zbyt złożonego dla twoich danych - idealny na danych treningowych, bezużyteczny do predykcji. Pamiętaj: Z wystarczającą liczbą parametrów możesz dopasować słonia.\nBłąd przetrwania: Analizowanie tylko “ocalałych” ignorując porażki. Przykład: Badanie firm sukcesu pomijając te, które zbankrutowały.\nNadmierna generalizacja: Rozszerzanie wniosków poza badaną populację, okres czasu lub kontekst. Przykład: Wyniki z amerykańskich studentów ≠ uniwersalne zachowanie ludzkie.\n\nPamiętaj: Pierwsze trzy to formy endogeniczności - naruszają E[\\varepsilon|X]=0 i sprawiają, że współczynniki są fundamentalnie błędne. Pozostałe czynią wyniki mylącymi lub niereprezentatywnymi.\n\n\n\n\n\n\nZastosowania w demografii\n\nAnaliza płodności\nZrozumienie, jakie czynniki wpływają na decyzje o płodności: \\text{Dzieci} = f(\\text{Wykształcenie, Dochód, Miasto, Religia, Antykoncepcja, …})\nPomaga zidentyfikować dźwignie polityczne dla krajów zaniepokojonych wysoką lub niską płodnością.\n\n\nModelowanie śmiertelności\nPrzewidywanie oczekiwanej długości życia lub ryzyka śmiertelności: \\text{Ryzyko śmiertelności} = f(\\text{Wiek, Płeć, Palenie, Wykształcenie, Dostęp do opieki zdrowotnej, …})\nUżywane przez firmy ubezpieczeniowe, urzędników zdrowia publicznego i badaczy.\n\n\nPrzewidywanie migracji\nZrozumienie, kto migruje i dlaczego: P(\\text{Migracja}) = f(\\text{Wiek, Wykształcenie, Zatrudnienie, Więzi rodzinne, Odległość, …})\nPomaga przewidywać przepływy populacji i planować zmiany demograficzne.\n\n\nMałżeństwo i rozwód\nAnalizowanie formowania i rozpadu związków: P(\\text{Rozwód}) = f(\\text{Wiek przy małżeństwie, Dopasowanie wykształcenia, Dochód, Dzieci, Czas trwania, …})\nInformuje politykę społeczną i usługi wsparcia.\n\n\n\nPowszechne pułapki i jak ich unikać\n\nPrzeuczenie (Overfitting)\nWłączenie zbyt wielu predyktorów może sprawić, że model idealnie pasuje do twojej próby, ale zawiedzie z nowymi danymi. Jak zapamiętywanie odpowiedzi na egzamin zamiast zrozumienia pojęć.\nRozwiązanie: Użyj prostszych modeli, walidacji krzyżowej lub zarezerwuj niektóre dane do testowania.\n\n\nWspółliniowość (Multicollinearity)\nGdy predyktory są silnie skorelowane (np. lata wykształcenia i poziom stopnia), model nie może oddzielić ich efektów.\nRozwiązanie: Wybierz jedną zmienną lub połącz je w indeks.\n\n\nObciążenie pominiętej zmiennej (Omitted Variable Bias)\nPominięcie ważnych zmiennych może sprawić, że inne efekty wydają się silniejsze lub słabsze niż naprawdę są.\nPrzykład: Relacja między sprzedażą lodów a wskaźnikami przestępczości znika, gdy kontrolujesz temperaturę.\n\n\nEkstrapolacja\nUżywanie modelu poza zakresem obserwowanych danych.\nPrzykład: Jeśli twoje dane obejmują wykształcenie od 0-20 lat, nie przewiduj dochodu dla kogoś z 30 latami wykształcenia.\n\n\n\nIntuicje\nPomyśl o regresji jako o wyrafinowanej technice uśredniania:\n\nProsta średnia: „Średni dochód wynosi 50 000 zł”\nŚrednia warunkowa: „Średni dochód dla absolwentów uczelni wynosi 70 000 zł”\nRegresja: „Średni dochód dla 35-letnich absolwentów uczelni w obszarach miejskich wynosi 78 000 zł”\n\nKażda dodana zmienna czyni nasze przewidywanie bardziej konkretnym i (miejmy nadzieję) dokładniejszym.\n\n\nRegresja w praktyce: Kompletny przykład\nPytanie badawcze: Jakie czynniki wpływają na wiek przy pierwszym porodzie?\nDane: Badanie 1000 kobiet, które miały co najmniej jedno dziecko\nZmienne:\n\nWynik: Wiek przy pierwszym porodzie (lata)\nPredyktory: Wykształcenie (lata), Miasto (0/1), Dochód (tysiące), Religijność (0/1)\n\nWynik prostej regresji: \\text{Wiek przy pierwszym porodzie} = 18 + 0,8 \\times \\text{Wykształcenie}\nInterpretacja: Każdy rok wykształcenia związany z 0,8 roku późniejszym pierwszym porodem.\nWynik regresji wielorakiej: \\text{Wiek przy pierwszym porodzie} = 16 + 0,5 \\times \\text{Wykształcenie} + 2 \\times \\text{Miasto} + 0,03 \\times \\text{Dochód} - 1,5 \\times \\text{Religijność}\nInterpretacja:\n\nEfekt wykształcenia zredukowany, ale nadal dodatni (0,5 roku na rok wykształcenia)\nKobiety miejskie mają pierwsze porody 2 lata później\nKażde 1000 zł dochodu związane z 0,03 roku (11 dni) później\nReligijne kobiety mają pierwsze porody 1,5 roku wcześniej\nR^2 = 0,42 (model wyjaśnia 42% zmienności)\n\nTen bogatszy model pomaga nam zrozumieć, że efekt wykształcenia częściowo działa przez zamieszkanie w mieście i dochód.\n\n\n\n\n\n\nWarning\n\n\n\nRegresja jest bramą do zaawansowanego modelowania statystycznego. Gdy zrozumiesz podstawową koncepcję — używanie zmiennych do przewidywania wyników i kwantyfikowania relacji — możesz eksplorować:\n\nEfekty interakcji: Gdy efekt jednej zmiennej zależy od innej\nRelacje nieliniowe: Krzywe, progi i złożone wzorce\nModele wielopoziomowe: Uwzględnianie zgrupowanych danych (uczniowie w szkołach, ludzie w dzielnicach)\nRegresja szeregów czasowych: Analizowanie zmian w czasie\nRozszerzenia uczenia maszynowego: Lasy losowe, sieci neuronowe i więcej\n\nKluczowy wgląd pozostaje: Próbujemy zrozumieć, jak rzeczy odnoszą się do siebie w systematyczny, kwantyfikowalny sposób.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#jakość-i-źródła-danych",
    "href": "rozdzial1.html#jakość-i-źródła-danych",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.16 Jakość i źródła danych",
    "text": "2.16 Jakość i źródła danych\nŻadna analiza nie jest lepsza niż dane, na których się opiera. Zrozumienie problemów jakości danych jest kluczowe dla badań demograficznych i społecznych.\n\nWymiary jakości danych\nDokładność (Accuracy): Jak blisko pomiarów są prawdziwe wartości?\nPrzykład: Raportowanie wieku często pokazuje „skupianie” na okrągłych liczbach (30, 40, 50), ponieważ ludzie zaokrąglają swój wiek.\nKompletność (Completeness): Jaka proporcja populacji jest objęta?\nPrzykład: Kompletność rejestracji urodzeń różni się znacznie:\n\nKraje rozwinięte: &gt;99%\nNiektóre kraje rozwijające się: &lt;50%\n\nAktualność (Timeliness): Jak aktualne są dane?\nPrzykład: Spis przeprowadzany co 10 lat staje się coraz bardziej nieaktualny, szczególnie w szybko zmieniających się obszarach.\nSpójność (Consistency): Czy definicje i metody są stabilne w czasie i przestrzeni?\nPrzykład: Definicja „miasta” różni się między krajami, utrudniając międzynarodowe porównania.\nDostępność (Accessibility): Czy badacze i decydenci mogą faktycznie używać danych?\n\n\nPowszechne źródła danych w demografii\nSpis powszechny (Census): Kompletne wyliczenie populacji\nZalety:\n\nKompletne pokrycie (w teorii)\nDane dla małych obszarów dostępne\nPunkt odniesienia dla innych oszacowań\n\nWady:\n\nDrogie i rzadkie\nNiektóre populacje trudne do policzenia\nOgraniczone zbierane zmienne\n\nRejestry urzędu stanu cywilnego (Vital Registration): Ciągłe rejestrowanie urodzeń, zgonów, małżeństw\nZalety:\n\nCiągłe i aktualne\nWymóg prawny zapewnia zgodność\nInformacje o medycznej przyczynie śmierci\n\nWady:\n\nPokrycie różni się według poziomu rozwoju\nJakość kodowania przyczyny śmierci się różni\nOpóźniona rejestracja powszechna w niektórych obszarach\n\nBadania próbkowe (Sample Surveys): Szczegółowe dane z podzbioru populacji\nPrzykłady:\n\nBadania demograficzne i zdrowotne (DHS)\nAmerykańskie Badanie Społeczności (ACS)\nBadania Siły Roboczej (np. BAEL GUS)\n\nZalety:\n\nMożna zbierać szczegółowe informacje\nCzęstsze niż spis\nMożna skupić się na konkretnych tematach\n\nWady:\n\nObecny błąd próbkowania\nMałe obszary niereprezentowane\nObciążenie odpowiedzi może zmniejszyć jakość\n\nRejestry administracyjne (Administrative Records): Dane zbierane do celów niestatystycznych\nPrzykłady:\n\nRejestry podatkowe\nZapisy szkolne\nRoszczenia ubezpieczenia zdrowotnego\nDane telefonii komórkowej\n\nZalety:\n\nJuż zebrane (bez dodatkowego obciążenia)\nCzęsto kompletne dla objętej populacji\nCiągle aktualizowane\n\nWady:\n\nPokrycie może być selektywne\nDefinicje mogą nie odpowiadać potrzebom badawczym\nDostęp często ograniczony\n\n\n\nProblemy jakości danych specyficzne dla demografii\nSkupianie wieku (Age Heaping): Tendencja do raportowania wieku kończącego się na 0 lub 5\nWykrywanie: Oblicz Indeks Whipple’a lub Indeks Myersa\nWpływ: Wpływa na wskaźniki specyficzne dla wieku i projekcje\nPreferencja cyfr (Digit Preference): Raportowanie niektórych końcowych cyfr częściej niż innych\nPrzykład: Wagi urodzeniowe często raportowane jako 3000g, 3500g zamiast dokładnych wartości\nObciążenie przypominania (Recall Bias): Trudność dokładnego przypominania przeszłych wydarzeń\nPrzykład: „Ile razy odwiedziłeś lekarza w zeszłym roku?” Często niedoszacowane dla częstych odwiedzających, przeszacowane dla rzadkich odwiedzających.\nRaportowanie przez pełnomocnika (Proxy Reporting): Informacje dostarczane przez kogoś innego\nWyzwanie: Głowa gospodarstwa domowego raportująca za wszystkich członków może nie znać dokładnego wieku lub wykształcenia każdego",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#względy-etyczne-w-demografii-statystycznej",
    "href": "rozdzial1.html#względy-etyczne-w-demografii-statystycznej",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.17 Względy etyczne w demografii statystycznej",
    "text": "2.17 Względy etyczne w demografii statystycznej\nStatystyka to nie tylko liczby — dotyczy prawdziwych ludzi i ma prawdziwe konsekwencje.\n\nŚwiadoma zgoda\nUczestnicy powinni zrozumieć:\n\nCel zbierania danych\nJak dane będą używane\nRyzyka i korzyści\nIch prawo do odmowy lub wycofania się\n\nWyzwanie w demografii: Uczestnictwo w spisie jest często obowiązkowe, co rodzi pytania etyczne o zgodę.\n\n\nPoufność i prywatność\nStatystyczna kontrola ujawniania: Ochrona tożsamości jednostek w opublikowanych danych\nMetody obejmują:\n\nTłumienie małych komórek (np. „&lt;5” zamiast „2”)\nAgregacja geograficzna\n\nPrzykład: W tabeli zawodu według wieku według płci dla małego miasta może być tylko jedna lekarka w wieku 60-65 lat, co czyni ją identyfikowalną.\n\n\nReprezentacja i uczciwość\nKto jest liczony?: Decyzje o tym, kogo uwzględnić, wpływają na reprezentację\n\nWięźniowie: Gdzie są liczeni — lokalizacja więzienia czy adres domowy?\nBezdomni: Jak zapewnić pokrycie?\nNieudokumentowani imigranci: Uwzględnić czy wykluczyć?\n\nPrywatność różnicowa (Differential Privacy): Matematyczna struktura ochrony prywatności przy zachowaniu użyteczności statystycznej\nKompromis: Większa ochrona prywatności = mniej dokładne statystyki\n\n\nNiewłaściwe użycie statystyk\nWybieranie wisienek (Cherry-Picking): Wybieranie tylko korzystnych wyników\nPrzykład: Raportowanie spadku ciąż nastolatek od roku szczytowego zamiast pokazywania pełnego trendu\nP-Hacking: Manipulowanie analizą w celu osiągnięcia istotności statystycznej\nBłąd ekologiczny: Wnioskowanie relacji indywidualnych z danych grupowych\nPrzykład: Powiaty z większą liczbą imigrantów mają wyższe średnie dochody ≠ imigranci mają wyższe dochody\n\n\nOdpowiedzialne raportowanie\nKomunikacja niepewności: Zawsze raportuj przedziały ufności lub marginesy błędu\nDostarczanie kontekstu: Uwzględnij odpowiednie grupy porównawcze i trendy historyczne\nUznanie ograniczeń: Jasno określ, co dane mogą i nie mogą pokazać",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#powszechne-nieporozumienia-w-statystyce",
    "href": "rozdzial1.html#powszechne-nieporozumienia-w-statystyce",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.18 Powszechne nieporozumienia w statystyce",
    "text": "2.18 Powszechne nieporozumienia w statystyce\nZrozumienie, czym statystyka NIE jest, jest równie ważne jak zrozumienie, czym jest.\n\nNieporozumienie 1: „Statystyki mogą udowodnić wszystko”\nRzeczywistość: Statystyki mogą dostarczyć tylko dowodów, nigdy absolutnego dowodu. A właściwa statystyka, uczciwie zastosowana, znacznie ogranicza wnioski.\nPrzykład: Badanie znajduje korelację między sprzedażą lodów a utopieniami. Statystyka nie „dowodzi”, że lody powodują utopienia — oba są związane z letnią pogodą.\n\n\nNieporozumienie 2: „Większe próby są zawsze lepsze”\nRzeczywistość: Poza pewnym punktem większe próby dodają niewiele precyzji, ale mogą dodać obciążenie.\nPrzykład: Ankieta online z 1 milionem odpowiedzi może być mniej dokładna niż próba probabilistyczna 1000 osób z powodu obciążenia samoselekcji.\nMalejące zyski:\n\nn = 100: Margines błędu \\approx 10 pp.\nn = 1000: Margines błędu \\approx 3,2 pp.\nn = 10 000: Margines błędu \\approx 1 pp.\nn = 100 000: Margines błędu \\approx 0,32 pp.\n\nSkok z 10 000 do 100 000 ledwo poprawia precyzję, ale kosztuje 10\\times więcej.\n\n\nNieporozumienie 3: “Istotność statystyczna = Praktyczne znaczenie”\nRzeczywistość: Przy dużych próbach malutkie różnice stają się „statystycznie istotne”, nawet jeśli są bez znaczenia.\nPrzykład: Badanie 100 000 osób stwierdza, że mężczyźni są średnio o 0,1 cm wyżsi (p &lt; 0,001). Statystycznie istotne, ale praktycznie nieistotne.\n\n\nNieporozumienie 4: “Korelacja implikuje przyczynowość”\nRzeczywistość: Korelacja jest konieczna, ale niewystarczająca dla przyczynowości.\nKlasyczne przykłady:\n\nMiasta z większą liczbą kościołów mają więcej przestępstw (oba korelują z wielkością populacji)\nKraje z większą liczbą telewizorów mają dłuższą oczekiwaną długość życia (oba korelują z rozwojem)\n\n\n\nNieporozumienie 5: “Losowy oznacza przypadkowy”\nRzeczywistość: Statystyczna losowość jest starannie kontrolowana i systematyczna.\nPrzykład: Losowe próbkowanie wymaga starannej procedury, a nie tylko chwytania kogokolwiek wygodnego.\n\n\nNieporozumienie 6: “Średnia reprezentuje wszystkich”\nRzeczywistość: Średnie mogą być mylące, gdy rozkłady są skośne lub wielomodalne.\nPrzykład: Średni dochód bywalców baru wynosi 50 000 zł. Bill Gates wchodzi. Teraz średnia wynosi 1 milion zł. Rzeczywisty dochód nikogo się nie zmienił.\n\n\nNieporozumienie 7: “Przeszłe wzorce gwarantują przyszłe wyniki”\nRzeczywistość: Ekstrapolacja zakłada, że warunki pozostają stałe.\nPrzykład: Liniowa projekcja wzrostu populacji z lat 1950-2000 źle przeszacowałaby populację 2050 roku, ponieważ pomija spadek płodności.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zastosowania-w-demografii-1",
    "href": "rozdzial1.html#zastosowania-w-demografii-1",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.19 Zastosowania w demografii",
    "text": "2.19 Zastosowania w demografii\n\nSzacowanie i projekcja populacji\nOszacowania międzyspisowe: Szacowanie populacji między spisami\nMetoda komponentów: P(t+1) = P(t) + B - D + I - E\nGdzie:\n\nP(t) = Populacja w czasie t\nB = Urodzenia\nD = Zgony\nI = Imigracja\nE = Emigracja\n\nKażdy komponent szacowany z różnych źródeł z różnymi strukturami błędów.\nProjekcje populacji: Prognozowanie przyszłej populacji\nMetoda komponentów kohortowych:\n\nPrognozuj wskaźniki przeżycia według wieku\nPrognozuj wskaźniki płodności\nPrognozuj wskaźniki migracji\nZastosuj do populacji bazowej\nZagreguj wyniki\n\nNiepewność wzrasta z horyzontem projekcji.\n\n\nObliczanie wskaźników demograficznych\nWskaźniki surowe (Crude Rates): Zdarzenia na 1000 populacji\n\\text{Surowy współczynnik urodzeń} = \\frac{\\text{Urodzenia}}{\\text{Populacja w połowie roku}} \\times 1000\nWskaźniki specyficzne dla wieku (Age-Specific Fertility Rate): Kontrola struktury wieku\n\\text{Współczynnik płodności specyficzny dla wieku} = \\frac{\\text{Urodzenia kobietom w wieku } x}{\\text{Kobiety w wieku } x} \\times 1000\nStandaryzacja: Porównywanie populacji z różnymi strukturami\nStandaryzacja bezpośrednia: Zastosuj wskaźniki populacji do standardowej struktury wieku Standaryzacja pośrednia: Zastosuj standardowe wskaźniki do struktury wieku populacji\n\n\nAnaliza tablic trwania życia\nTablice życia podsumowują doświadczenie śmiertelności populacji.\nKluczowe kolumny:\n\nq_x: Prawdopodobieństwo śmierci między wiekiem x a x+1\nl_x: Liczba przeżywających do wieku x (ze 100 000 urodzeń)\nd_x: Zgony między wiekiem x a x+1\nL_x: Osobo-lata przeżyte między wiekiem x a x+1\ne_x: Oczekiwana długość życia w wieku x\n\nPrzykład interpretacji: Jeśli q_{65} = 0,015, to 1,5% 65-latków umrze przed osiągnięciem 66 lat. Jeśli e_{65} = 18,5, to 65-latkowie średnio żyją jeszcze 18,5 roku.\n\n\nAnaliza płodności\nWspółczynnik dzietności całkowitej (TFR - Total Fertility Rate): Średnia liczba dzieci na kobietę przy obecnych wskaźnikach płodności specyficznych dla wieku (ASFR - Age-Specific Fertility Rate)\n\\text{TFR} = \\sum (\\text{ASFR} \\times \\text{szerokość przedziału wieku})\nPrzykład: Jeśli każda 5-letnia grupa wiekowa od 15-49 ma ASFR = 20 na 1000: \\text{TFR} = 7 \\text{ grup wiekowych} \\times \\frac{20}{1000} \\times 5 \\text{ lat} = 0,7 \\text{ dzieci na kobietę}\nTen bardzo niski TFR wskazuje na płodność poniżej poziomu zastępowalności.\n\n\nAnaliza migracji\nWspółczynnik migracji netto: \\text{NMR} = \\frac{\\text{Imigranci} - \\text{Emigranci}}{\\text{Populacja}} \\times 1000\nWskaźnik efektywności migracji: \\text{MEI} = \\frac{|\\text{Napływ} - \\text{Odpływ}|}{\\text{Napływ} + \\text{Odpływ}}\n\nWartości blisko 0: Wysoka rotacja, mała zmiana netto\nWartości blisko 1: Głównie przepływ jednokierunkowy\n\n\n\nMetryki zdrowia populacji\nLata życia skorygowane o niepełnosprawność (DALYs): Utracone lata zdrowego życia\nDALY = Utracone lata życia (YLL) + Lata przeżyte z niepełnosprawnością (YLD)\nOczekiwana długość życia w zdrowiu: Oczekiwane lata w dobrym zdrowiu\nŁączy informacje o śmiertelności i chorobowości.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#oprogramowanie-i-narzędzia",
    "href": "rozdzial1.html#oprogramowanie-i-narzędzia",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.20 Oprogramowanie i narzędzia",
    "text": "2.20 Oprogramowanie i narzędzia\nWspółczesna statystyka demograficzna opiera się w dużej mierze na narzędziach obliczeniowych.\n\nPakiety oprogramowania statystycznego\nR: Darmowy, otwarty, rozbudowane pakiety demograficzne\n\nPakiety: demography, popReconstruct, bayesPop\nZalety: Powtarzalne badania, najnowocześniejsze metody\nWady: Stroma krzywa uczenia\n\nStata: Szeroko używany w naukach społecznych\n\nMocne strony: Analiza danych z badań, dane panelowe\nPowszechny w: Ekonomii, epidemiologii\n\nSPSS: Przyjazny interfejs użytkownika\n\nMocne strony: Interfejs wskaż-i-kliknij\nPowszechny w: Naukach społecznych, badaniach rynkowych\n\nPython: Język programowania ogólnego przeznaczenia z bibliotekami statystycznymi\n\nBiblioteki: pandas, numpy, scipy, statsmodels\nZalety: Integracja z innymi aplikacjami",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zakończenie",
    "href": "rozdzial1.html#zakończenie",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.21 Zakończenie",
    "text": "2.21 Zakończenie\n\nPodsumowanie kluczowych terminów\nStatystyka: Nauka o zbieraniu, organizowaniu, analizowaniu, interpretowaniu i prezentowaniu danych w celu zrozumienia zjawisk i wsparcia podejmowania decyzji\nStatystyka opisowa: Metody podsumowywania i prezentowania danych w znaczący sposób bez rozszerzania wniosków poza obserwowane dane\nStatystyka wnioskowania: Techniki wyciągania wniosków o populacjach z prób, w tym estymacja i testowanie hipotez\nPopulacja: Kompletny zbiór jednostek, obiektów lub pomiarów, o których chcemy wyciągnąć wnioski\nPróba: Podzbiór populacji, który jest faktycznie obserwowany lub mierzony w celu dokonania wniosków o populacji\nSuperpopulacja: Teoretyczna nieskończona populacja, z której obserwowane skończone populacje są uważane za próby\nParametr: Liczbowa charakterystyka populacji (zazwyczaj nieznana i oznaczana literami greckimi)\nStatystyka: Liczbowa charakterystyka obliczona z danych z próby (znana i oznaczana literami łacińskimi)\nEstymator: Reguła lub formuła do obliczania oszacowań parametrów populacji z danych z próby\nEstimand: Konkretny parametr populacji będący celem estymacji\nOszacowanie: Wartość liczbowa uzyskana przez zastosowanie estymatora do obserwowanych danych\nBłąd losowy: Nieprzewidywalna zmienność wynikająca z procesu próbkowania, która maleje z większymi próbami\nBłąd systematyczny (Obciążenie): Konsekwentne odchylenie od prawdziwych wartości, którego nie można zmniejszyć przez zwiększenie wielkości próby\nPróbkowanie: Proces wyboru podzbioru jednostek z populacji do pomiaru\nOperat losowania: Lista lub urządzenie, z którego pobierana jest próba, idealnie zawierające wszystkich członków populacji\nPróbkowanie probabilistyczne: Metody próbkowania, w których każdy członek populacji ma znane, niezerowe prawdopodobieństwo selekcji\nProste losowanie: Każda możliwa próba wielkości n ma równe prawdopodobieństwo selekcji\nLosowanie systematyczne: Wybór co k-tego elementu z uporządkowanego operanta losowania\nLosowanie warstwowe: Podział populacji na jednorodne podgrupy przed próbkowaniem w każdej\nLosowanie grupowe: Wybór grup (klastrów) zamiast jednostek\nPróbkowanie nieprobabilistyczne: Metody próbkowania bez gwarantowanych znanych prawdopodobieństw selekcji\nPróbkowanie wygodne: Wybór oparty wyłącznie na łatwości dostępu\nPróbkowanie celowe: Celowy wybór oparty na osądzie badacza\nPróbkowanie kwotowe: Wybór w celu dopasowania proporcji populacji w kluczowych charakterystykach bez losowej selekcji\nPróbkowanie kuli śnieżnej: Uczestnicy rekrutują dodatkowych uczestników ze swoich znajomych\nBłąd standardowy: Odchylenie standardowe rozkładu próbkowania statystyki\nMargines błędu: Maksymalna oczekiwana różnica między oszacowaniem a parametrem przy określonym poziomie ufności\nPrzedział ufności: Zakres prawdopodobnych wartości dla parametru przy określonym poziomie ufności\nPoziom ufności: Prawdopodobieństwo, że metoda przedziału ufności wytworzy przedziały zawierające parametr\nDane: Zebrane obserwacje lub pomiary\nDane ilościowe: Pomiary liczbowe (ciągłe lub dyskretne)\nDane jakościowe: Informacje kategoryczne (nominalne lub porządkowe)\nRozkład danych: Opis tego, jak wartości rozkładają się na możliwe wyniki\nRozkład częstości: Podsumowanie pokazujące, jak często każda wartość występuje w danych\nCzęstość bezwzględna: Liczba obserwacji dla każdej wartości\nCzęstość względna: Proporcja obserwacji w każdej kategorii\nCzęstość skumulowana: Suma bieżąca częstości do każdej wartości",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#załącznik-a-visualizations-for-statistics-demography",
    "href": "rozdzial1.html#załącznik-a-visualizations-for-statistics-demography",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.22 Załącznik A: Visualizations for Statistics & Demography",
    "text": "2.22 Załącznik A: Visualizations for Statistics & Demography\n\n## ============================================\n## Visualizations for Statistics & Demography\n## Chapter 1: Foundations\n## ============================================\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(gridExtra)\nlibrary(scales)\nlibrary(patchwork)  # for combining plots\n\n# Set theme for all plots\ntheme_set(theme_minimal(base_size = 12))\n\n# Color palette for consistency\ncolors &lt;- c(\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#6A994E\")\n\n\n# ==================================================\n# 1. POPULATION vs SAMPLE VISUALIZATION\n# ==================================================\n\n# Create a population and sample visualization\nset.seed(123)\n\n# Generate population data (e.g., ages of 10,000 people)\npopulation &lt;- data.frame(\n  id = 1:10000,\n  age = round(rnorm(10000, mean = 40, sd = 15))\n)\npopulation$age[population$age &lt; 0] &lt;- 0\npopulation$age[population$age &gt; 100] &lt;- 100\n\n# Take a random sample\nsample_size &lt;- 500\nsample_data &lt;- population[sample(nrow(population), sample_size), ]\n\n# Create visualization\np1 &lt;- ggplot(population, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = colors[1], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = mean(population$age), \n             color = colors[2], linetype = \"dashed\", size = 1.2) +\n  labs(title = \"Population Distribution (N = 10,000)\",\n       subtitle = paste(\"Population mean (μ) =\", round(mean(population$age), 2), \"years\"),\n       x = \"Age (years)\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(sample_data, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = colors[3], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = mean(sample_data$age), \n             color = colors[4], linetype = \"dashed\", size = 1.2) +\n  labs(title = paste(\"Sample Distribution (n =\", sample_size, \")\"),\n       subtitle = paste(\"Sample mean (x̄) =\", round(mean(sample_data$age), 2), \"years\"),\n       x = \"Age (years)\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Combine plots\npopulation_sample_plot &lt;- p1 / p2\nprint(population_sample_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 2. TYPES OF DATA DISTRIBUTIONS\n# ==================================================\n\n# Generate different distribution types\nset.seed(456)\nn &lt;- 5000\n\n# Normal distribution\nnormal_data &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Right-skewed distribution (income-like)\nright_skewed &lt;- rgamma(n, shape = 2, scale = 15)\n\n# Left-skewed distribution (age at death in developed country)\nleft_skewed &lt;- 90 - rgamma(n, shape = 3, scale = 5)\nleft_skewed[left_skewed &lt; 0] &lt;- 0\n\n# Bimodal distribution (e.g., height of mixed male/female population)\nn2  &lt;- 20000\nnf &lt;- n2 %/% 2; nm &lt;- n2 - nf\nbimodal &lt;- c(rnorm(nf, mean = 164, sd = 5),\n             rnorm(nm, mean = 182, sd = 5))\n\n\n# Create data frame\ndistributions_df &lt;- data.frame(\n  Normal = normal_data,\n  `Right Skewed` = right_skewed,\n  `Left Skewed` = left_skewed,\n  Bimodal = bimodal\n) %&gt;%\n  pivot_longer(everything(), names_to = \"Distribution\", values_to = \"Value\")\n\n# Plot distributions\ndistributions_plot &lt;- ggplot(distributions_df, aes(x = Value, fill = Distribution)) +\n  geom_histogram(bins = 30, alpha = 0.7, color = \"white\") +\n  facet_wrap(~Distribution, scales = \"free\", nrow = 2) +\n  scale_fill_manual(values = colors[1:4]) +\n  labs(title = \"Types of Data Distributions\",\n       subtitle = \"Common patterns in demographic data\",\n       x = \"Value\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"none\")\n\nprint(distributions_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 3. NORMAL DISTRIBUTION WITH 68-95-99.7 RULE\n# ==================================================\n\n# Generate normal distribution data\nset.seed(789)\nmean_val &lt;- 100\nsd_val &lt;- 15\nx &lt;- seq(mean_val - 4*sd_val, mean_val + 4*sd_val, length.out = 1000)\ny &lt;- dnorm(x, mean = mean_val, sd = sd_val)\ndf_norm &lt;- data.frame(x = x, y = y)\n\n# Create the plot\nnormal_plot &lt;- ggplot(df_norm, aes(x = x, y = y)) +\n  # Fill areas under the curve\n  geom_area(data = subset(df_norm, x &gt;= mean_val - sd_val & x &lt;= mean_val + sd_val),\n            aes(x = x, y = y), fill = colors[1], alpha = 0.3) +\n  geom_area(data = subset(df_norm, x &gt;= mean_val - 2*sd_val & x &lt;= mean_val + 2*sd_val),\n            aes(x = x, y = y), fill = colors[2], alpha = 0.2) +\n  geom_area(data = subset(df_norm, x &gt;= mean_val - 3*sd_val & x &lt;= mean_val + 3*sd_val),\n            aes(x = x, y = y), fill = colors[3], alpha = 0.1) +\n  # Add the curve\n  geom_line(size = 1.5, color = \"black\") +\n  # Add vertical lines for standard deviations\n  geom_vline(xintercept = mean_val, linetype = \"solid\", size = 1, color = \"black\") +\n  geom_vline(xintercept = c(mean_val - sd_val, mean_val + sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[1]) +\n  geom_vline(xintercept = c(mean_val - 2*sd_val, mean_val + 2*sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[2]) +\n  geom_vline(xintercept = c(mean_val - 3*sd_val, mean_val + 3*sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[3]) +\n  # Add labels\n  annotate(\"text\", x = mean_val, y = max(y) * 0.5, label = \"68%\", \n           size = 5, fontface = \"bold\", color = colors[1]) +\n  annotate(\"text\", x = mean_val, y = max(y) * 0.3, label = \"95%\", \n           size = 5, fontface = \"bold\", color = colors[2]) +\n  annotate(\"text\", x = mean_val, y = max(y) * 0.1, label = \"99.7%\", \n           size = 5, fontface = \"bold\", color = colors[3]) +\n  # Labels\n  scale_x_continuous(breaks = c(mean_val - 3*sd_val, mean_val - 2*sd_val, \n                                mean_val - sd_val, mean_val, \n                                mean_val + sd_val, mean_val + 2*sd_val, \n                                mean_val + 3*sd_val),\n                     labels = c(\"μ-3σ\", \"μ-2σ\", \"μ-σ\", \"μ\", \"μ+σ\", \"μ+2σ\", \"μ+3σ\")) +\n  labs(title = \"Normal Distribution: The 68-95-99.7 Rule\",\n       subtitle = \"Proportion of data within standard deviations from the mean\",\n       x = \"Value\", y = \"Probability Density\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\nprint(normal_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 4. SIMPLE LINEAR REGRESSION\n# ==================================================\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(scales)\n\n# Define color palette (this was missing in original code)\ncolors &lt;- c(\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#592E83\")\n\n# Generate data for regression example (Education vs Income)\nset.seed(2024)\nn_reg &lt;- 200\neducation &lt;- round(rnorm(n_reg, mean = 14, sd = 3))\neducation[education &lt; 8] &lt;- 8\neducation[education &gt; 22] &lt;- 22\n\n# Create income with linear relationship plus noise\nincome &lt;- 15000 + 4000 * education + rnorm(n_reg, mean = 0, sd = 8000)\nincome[income &lt; 10000] &lt;- 10000\n\nreg_data &lt;- data.frame(education = education, income = income)\n\n# Fit linear model\nlm_model &lt;- lm(income ~ education, data = reg_data)\n\n# Create subset of data for residual lines\nsubset_indices &lt;- sample(nrow(reg_data), 20)\nsubset_data &lt;- reg_data[subset_indices, ]\nsubset_data$predicted &lt;- predict(lm_model, newdata = subset_data)\n\n# Create regression plot\nregression_plot &lt;- ggplot(reg_data, aes(x = education, y = income)) +\n  # Add points\n  geom_point(alpha = 0.6, size = 2, color = colors[1]) +\n  \n  # Add regression line with confidence interval\n  geom_smooth(method = \"lm\", se = TRUE, color = colors[2], fill = colors[2], alpha = 0.2) +\n  \n  # Add residual lines for a subset of points to show the concept\n  geom_segment(data = subset_data,\n               aes(x = education, xend = education, \n                   y = income, yend = predicted),\n               color = colors[4], alpha = 0.5, linetype = \"dotted\") +\n  \n  # Add equation to plot (adjusted position based on data range)\n  annotate(\"text\", x = min(reg_data$education) + 1, y = max(reg_data$income) * 0.9, \n           label = paste(\"Income = $\", format(round(coef(lm_model)[1]), big.mark = \",\"), \n                        \" + $\", format(round(coef(lm_model)[2]), big.mark = \",\"), \" × Education\",\n                        \"\\nR² = \", round(summary(lm_model)$r.squared, 3), sep = \"\"),\n           hjust = 0, size = 4, fontface = \"italic\") +\n  \n  # Labels and formatting\n  scale_y_continuous(labels = dollar_format()) +\n  labs(title = \"Simple Linear Regression: Education and Income\",\n       subtitle = \"Each year of education associated with higher income\",\n       x = \"Years of Education\", \n       y = \"Annual Income\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\nprint(regression_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 5. SAMPLING ERROR AND SAMPLE SIZE\n# ==================================================\n\n# Show how standard error decreases with sample size\nset.seed(111)\nsample_sizes &lt;- c(10, 25, 50, 100, 250, 500, 1000, 2500, 5000)\nn_simulations &lt;- 1000\n\n# True population parameters\ntrue_mean &lt;- 50\ntrue_sd &lt;- 10\n\n# Run simulations for each sample size\nse_results &lt;- data.frame()\nfor (n in sample_sizes) {\n  sample_means &lt;- replicate(n_simulations, mean(rnorm(n, true_mean, true_sd)))\n  se_results &lt;- rbind(se_results, \n                      data.frame(n = n, \n                                se_empirical = sd(sample_means),\n                                se_theoretical = true_sd / sqrt(n)))\n}\n\n# Create the plot\nse_plot &lt;- ggplot(se_results, aes(x = n)) +\n  geom_line(aes(y = se_empirical, color = \"Empirical SE\"), size = 1.5) +\n  geom_point(aes(y = se_empirical, color = \"Empirical SE\"), size = 3) +\n  geom_line(aes(y = se_theoretical, color = \"Theoretical SE\"), \n            size = 1.5, linetype = \"dashed\") +\n  scale_x_log10(breaks = sample_sizes) +\n  scale_color_manual(values = c(\"Empirical SE\" = colors[1], \n                               \"Theoretical SE\" = colors[2])) +\n  labs(title = \"Standard Error Decreases with Sample Size\",\n       subtitle = \"The precision of estimates improves with larger samples\",\n       x = \"Sample Size (log scale)\", \n       y = \"Standard Error\",\n       color = \"\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"top\")\n\nprint(se_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 6. CONFIDENCE INTERVALS VISUALIZATION\n# ==================================================\n\n# Simulate multiple samples and their confidence intervals\nset.seed(999)\nn_samples &lt;- 20\nsample_size_ci &lt;- 100\ntrue_mean_ci &lt;- 50\ntrue_sd_ci &lt;- 10\n\n# Generate samples and calculate CIs\nci_data &lt;- data.frame()\nfor (i in 1:n_samples) {\n  sample_i &lt;- rnorm(sample_size_ci, true_mean_ci, true_sd_ci)\n  mean_i &lt;- mean(sample_i)\n  se_i &lt;- sd(sample_i) / sqrt(sample_size_ci)\n  ci_lower &lt;- mean_i - 1.96 * se_i\n  ci_upper &lt;- mean_i + 1.96 * se_i\n  contains_true &lt;- (true_mean_ci &gt;= ci_lower) & (true_mean_ci &lt;= ci_upper)\n  \n  ci_data &lt;- rbind(ci_data,\n                   data.frame(sample = i, mean = mean_i, \n                             lower = ci_lower, upper = ci_upper,\n                             contains = contains_true))\n}\n\n# Create CI plot\nci_plot &lt;- ggplot(ci_data, aes(x = sample, y = mean)) +\n  geom_hline(yintercept = true_mean_ci, color = \"red\", \n             linetype = \"dashed\", size = 1) +\n  geom_errorbar(aes(ymin = lower, ymax = upper, color = contains), \n                width = 0.3, size = 0.8) +\n  geom_point(aes(color = contains), size = 2) +\n  scale_color_manual(values = c(\"TRUE\" = colors[1], \"FALSE\" = colors[4]),\n                    labels = c(\"Misses true value\", \"Contains true value\")) +\n  coord_flip() +\n  labs(title = \"95% Confidence Intervals from 20 Different Samples\",\n       subtitle = paste(\"True population mean = \", true_mean_ci, \n                       \" (red dashed line)\", sep = \"\"),\n       x = \"Sample Number\", \n       y = \"Sample Mean with 95% CI\",\n       color = \"\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"bottom\")\n\nprint(ci_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 7. SAMPLING DISTRIBUTIONS (CENTRAL LIMIT THEOREM)\n# ==================================================\n\n# ---- Setup ----\nlibrary(tidyverse)\nlibrary(ggplot2)\ntheme_set(theme_minimal(base_size = 13))\nset.seed(2025)\n\n# Skewed population (Gamma); change if you want another DGP\nNpop &lt;- 100000\npopulation &lt;- rgamma(Npop, shape = 2, scale = 10)  # skewed right\nmu    &lt;- mean(population)\nsigma &lt;- sd(population)\n\n# ---- CLT: sampling distribution of the mean ----\nsample_sizes &lt;- c(1, 5, 10, 30, 100)\nB &lt;- 2000  # resamples per n\n\nclt_df &lt;- purrr::map_dfr(sample_sizes, \\(n) {\n  tibble(n = n,\n         mean = replicate(B, mean(sample(population, n, replace = TRUE))))\n})\n\n# Normal overlays: N(mu, sigma/sqrt(n))\nclt_range &lt;- clt_df |&gt;\n  group_by(n) |&gt;\n  summarise(min_x = min(mean), max_x = max(mean), .groups = \"drop\")\n\nnormal_df &lt;- clt_range |&gt;\n  rowwise() |&gt;\n  mutate(x = list(seq(min_x, max_x, length.out = 200))) |&gt;\n  unnest(x) |&gt;\n  mutate(density = dnorm(x, mean = mu, sd = sigma / sqrt(n)))\n\nclt_plot &lt;- ggplot(clt_df, aes(mean)) +\n  geom_histogram(aes(y = after_stat(density), fill = factor(n)),\n                 bins = 30, alpha = 0.6, color = \"white\") +\n  geom_line(data = normal_df, aes(x, density), linewidth = 0.8) +\n  geom_vline(xintercept = mu, linetype = \"dashed\") +\n  facet_wrap(~ n, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"CLT: Sampling distribution of the mean → Normal(μ, σ/√n)\",\n    subtitle = sprintf(\"Skewed population: Gamma(shape=2, scale=10).  μ≈%.2f, σ≈%.2f; B=%d resamples each.\", mu, sigma, B),\n    x = \"Sample mean\", y = \"Density\"\n  ) +\n  guides(fill = \"none\")\n\nclt_plot\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 8. TYPES OF SAMPLING ERROR\n# ==================================================\n\n# Create data to show random vs systematic error\nset.seed(321)\nn_measurements &lt;- 100\ntrue_value &lt;- 50\n\n# Random error only\nrandom_error &lt;- rnorm(n_measurements, mean = true_value, sd = 5)\n\n# Systematic error (bias) only\nsystematic_error &lt;- rep(true_value + 10, n_measurements) + rnorm(n_measurements, 0, 0.5)\n\n# Both errors\nboth_errors &lt;- rnorm(n_measurements, mean = true_value + 10, sd = 5)\n\nerror_data &lt;- data.frame(\n  measurement = 1:n_measurements,\n  `Random Error Only` = random_error,\n  `Systematic Error Only` = systematic_error,\n  `Both Errors` = both_errors\n) %&gt;%\n  pivot_longer(-measurement, names_to = \"Error_Type\", values_to = \"Value\")\n\n# Create error visualization\nerror_plot &lt;- ggplot(error_data, aes(x = measurement, y = Value, color = Error_Type)) +\n  geom_hline(yintercept = true_value, linetype = \"dashed\", size = 1, color = \"black\") +\n  geom_point(alpha = 0.6, size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 1.2) +\n  facet_wrap(~Error_Type, nrow = 1) +\n  scale_color_manual(values = colors[1:3]) +\n  labs(title = \"Random Error vs Systematic Error (Bias)\",\n       subtitle = paste(\"True value = \", true_value, \" (black dashed line)\", sep = \"\"),\n       x = \"Measurement Number\", \n       y = \"Measured Value\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"none\")\n\nprint(error_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 9. DEMOGRAPHIC PYRAMID\n# ==================================================\n\n# Create age pyramid data\nset.seed(777)\nage_groups &lt;- c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \n               \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \n               \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80+\")\n\n# Create data for a developing country pattern\nmale_pop &lt;- c(12, 11.5, 11, 10.5, 10, 9.5, 9, 8.5, 8, 7.5, 7, \n             6, 5, 4, 3, 2, 1.5)\nfemale_pop &lt;- c(11.8, 11.3, 10.8, 10.3, 9.8, 9.3, 8.8, 8.3, 7.8, \n               7.3, 6.8, 5.8, 4.8, 3.8, 2.8, 2.2, 2)\n\npyramid_data &lt;- data.frame(\n  Age = factor(rep(age_groups, 2), levels = rev(age_groups)),\n  Population = c(-male_pop, female_pop),  # Negative for males\n  Sex = c(rep(\"Male\", length(male_pop)), rep(\"Female\", length(female_pop)))\n)\n\n# Create population pyramid\npyramid_plot &lt;- ggplot(pyramid_data, aes(x = Age, y = Population, fill = Sex)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  scale_y_continuous(labels = function(x) paste0(abs(x), \"%\")) +\n  scale_fill_manual(values = c(\"Male\" = colors[1], \"Female\" = colors[3])) +\n  coord_flip() +\n  labs(title = \"Population Pyramid\",\n       subtitle = \"Age and sex distribution (typical developing country pattern)\",\n       x = \"Age Group\", \n       y = \"Percentage of Population\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"top\")\n\nprint(pyramid_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 10. REGRESSION RESIDUALS AND DIAGNOSTICS\n# ==================================================\n\n# Use the previous regression model for diagnostics\nreg_diagnostics &lt;- data.frame(\n  fitted = fitted(lm_model),\n  residuals = residuals(lm_model),\n  standardized_residuals = rstandard(lm_model),\n  education = reg_data$education,\n  income = reg_data$income\n)\n\n# Create diagnostic plots\n# 1. Residuals vs Fitted\np_resid_fitted &lt;- ggplot(reg_diagnostics, aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.5, color = colors[1]) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = colors[2], size = 0.8) +\n  labs(title = \"Residuals vs Fitted Values\",\n       subtitle = \"Check for homoscedasticity\",\n       x = \"Fitted Values\", y = \"Residuals\")\n\n# 2. Q-Q plot\np_qq &lt;- ggplot(reg_diagnostics, aes(sample = standardized_residuals)) +\n  stat_qq(color = colors[1]) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Normal Q-Q Plot\",\n       subtitle = \"Check for normality of residuals\",\n       x = \"Theoretical Quantiles\", y = \"Standardized Residuals\")\n\n# 3. Histogram of residuals\np_hist_resid &lt;- ggplot(reg_diagnostics, aes(x = residuals)) +\n  geom_histogram(bins = 30, fill = colors[3], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Distribution of Residuals\",\n       subtitle = \"Should be approximately normal\",\n       x = \"Residuals\", y = \"Frequency\")\n\n# 4. Residuals vs Predictor\np_resid_x &lt;- ggplot(reg_diagnostics, aes(x = education, y = residuals)) +\n  geom_point(alpha = 0.5, color = colors[4]) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = colors[2], size = 0.8) +\n  labs(title = \"Residuals vs Predictor\",\n       subtitle = \"Check for patterns\",\n       x = \"Education (years)\", y = \"Residuals\")\n\n# Combine diagnostic plots\ndiagnostic_plots &lt;- (p_resid_fitted + p_qq) / (p_hist_resid + p_resid_x)\nprint(diagnostic_plots)\n\n\n\n\n\n\n\n# ==================================================\n# 11. SAVE ALL PLOTS (Optional)\n# ==================================================\n\n# Uncomment to save plots as high-resolution images\n# ggsave(\"population_sample.png\", population_sample_plot, width = 10, height = 8, dpi = 300)\n# ggsave(\"distributions.png\", distributions_plot, width = 12, height = 8, dpi = 300)\n# ggsave(\"normal_distribution.png\", normal_plot, width = 10, height = 6, dpi = 300)\n# ggsave(\"regression.png\", regression_plot, width = 10, height = 7, dpi = 300)\n# ggsave(\"standard_error.png\", se_plot, width = 10, height = 6, dpi = 300)\n# ggsave(\"confidence_intervals.png\", ci_plot, width = 10, height = 8, dpi = 300)\n# ggsave(\"central_limit_theorem.png\", clt_plot, width = 14, height = 5, dpi = 300)\n# ggsave(\"error_types.png\", error_plot, width = 14, height = 5, dpi = 300)\n# ggsave(\"population_pyramid.png\", pyramid_plot, width = 8, height = 8, dpi = 300)\n# ggsave(\"regression_diagnostics.png\", diagnostic_plots, width = 12, height = 10, dpi = 300)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#załącznik-b-centralne-twierdzenie-graniczne-ctg",
    "href": "rozdzial1.html#załącznik-b-centralne-twierdzenie-graniczne-ctg",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.23 Załącznik B: Centralne Twierdzenie Graniczne (CTG)",
    "text": "2.23 Załącznik B: Centralne Twierdzenie Graniczne (CTG)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#centralne-twierdzenie-graniczne-ctg-1",
    "href": "rozdzial1.html#centralne-twierdzenie-graniczne-ctg-1",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.24 Centralne Twierdzenie Graniczne (CTG)",
    "text": "2.24 Centralne Twierdzenie Graniczne (CTG)\nCentralne Twierdzenie Graniczne stwierdza, że rozkład średnich próbkowych zbliża się do rozkładu normalnego wraz ze wzrostem wielkości próby, niezależnie od kształtu pierwotnego rozkładu populacji.\n\nImplikacje\n\nPróg Wielkości Próby: Wielkość próby n ≥ 30 jest zazwyczaj wystarczająca, aby zastosować CTG\nBłąd Standardowy: Odchylenie standardowe średnich próbkowych wynosi σ/√n, gdzie σ to odchylenie standardowe populacji\nFundament Wnioskowania Statystycznego: Możemy dokonywać wnioskowań o parametrach populacji używając właściwości rozkładu normalnego",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wizualna-demonstracja-progresja-krok-po-kroku",
    "href": "rozdzial1.html#wizualna-demonstracja-progresja-krok-po-kroku",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.25 Wizualna Demonstracja: Progresja Krok po Kroku",
    "text": "2.25 Wizualna Demonstracja: Progresja Krok po Kroku\nNajlepszym sposobem na zrozumienie CTG jest obserwowanie ewolucji rozkładu wraz ze wzrostem liczby kostek. Zaczynając od 1 kostki (rozkład jednostajny), zobaczymy, jak dodawanie kolejnych kostek stopniowo przekształca rozkład w idealną krzywą dzwonową!\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n\nProgresywna Transformacja\n\n# Wielkości próby do demonstracji\nwielkosci_prob &lt;- c(1, 2, 5, 10, 30, 50)\nliczba_symulacji &lt;- 100000\n\n# Symulacja dla każdej wielkości próby\nwszystkie_dane &lt;- data.frame()\n\nfor (n in wielkosci_prob) {\n  srednie &lt;- replicate(liczba_symulacji, {\n    kostki &lt;- sample(1:6, n, replace = TRUE)\n    mean(kostki)\n  })\n  \n  temp_df &lt;- data.frame(\n    srednia = srednie,\n    n = n,\n    etykieta = paste(n, ifelse(n == 1, \"kostka\", \n                               ifelse(n &lt; 5, \"kostki\", \"kostek\")))\n  )\n  wszystkie_dane &lt;- rbind(wszystkie_dane, temp_df)\n}\n\n# Utworzenie uporządkowanego czynnika\nwszystkie_dane$etykieta &lt;- factor(wszystkie_dane$etykieta, \n                                  levels = paste(wielkosci_prob, \n                                                ifelse(wielkosci_prob == 1, \"kostka\",\n                                                      ifelse(wielkosci_prob &lt; 5, \"kostki\", \"kostek\"))))\n\n# Wykres progresji\nggplot(wszystkie_dane, aes(x = srednia)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 bins = 50, fill = \"#3b82f6\", color = \"white\", alpha = 0.7) +\n  facet_wrap(~etykieta, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"Centralne Twierdzenie Graniczne: Progresja Krok po Kroku\",\n    subtitle = sprintf(\"Każdy panel pokazuje %s symulacji rzutu kostkami i obliczenia średniej\", \n                      format(liczba_symulacji, big.mark = \" \")),\n    x = \"Wartość Średnia\",\n    y = \"Gęstość\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    strip.text = element_text(face = \"bold\", size = 12),\n    strip.background = element_rect(fill = \"#f0f0f0\", color = NA)\n  )\n\n\n\n\n\n\n\n\n\nAnaliza Poszczególnych Etapów:\n\n1 kostka: Rozkład jednostajny (równomierny) - wszystkie wartości 1-6 jednakowo prawdopodobne\n2 kostki: Rozkład z tendencją trójkątną - środkowe wartości występują częściej\n5 kostek: Wyłaniający się kształt dzwonowy - obserwowalne skupienie wokół wartości 3,5\n10 kostek: Wyraźnie normalny - formująca się wąska krzywa Gaussa\n30 kostek: Rozkład normalny - praktyczna demonstracja CTG\n50 kostek: Rozkład bliski idealnemu normalnemu - bardzo silna koncentracja\n\nZauważ, jak rozkład charakteryzuje się coraz mniejszą zmiennością i bardziej wyraźnym kształtem dzwonowym wraz ze wzrostem n.\n\n\n\nPorównanie Obok Siebie\nZobaczmy czystsze porównanie kluczowych etapów:\n\nkluczowe_wielkosci &lt;- wszystkie_dane %&gt;%\n  filter(n %in% c(1, 2, 5, 10, 30))\n\nggplot(kluczowe_wielkosci, aes(x = srednia)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 bins = 40, fill = \"#3b82f6\", color = \"white\", alpha = 0.7) +\n  facet_wrap(~etykieta, scales = \"free_x\", nrow = 1) +\n  labs(\n    title = \"Ewolucja CTG: Od Jednostajnego do Normalnego\",\n    x = \"Wartość Średnia\",\n    y = \"Gęstość\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    strip.text = element_text(face = \"bold\", size = 11),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\nNałożone Rozkłady\nInny sposób zobaczenia transformacji - wszystkie rozkłady na jednym wykresie:\n\ndane_porownawcze &lt;- wszystkie_dane %&gt;%\n  filter(n %in% c(1, 5, 10, 30))\n\nggplot(dane_porownawcze, aes(x = srednia, fill = etykieta, color = etykieta)) +\n  geom_density(alpha = 0.3, linewidth = 1.2) +\n  scale_fill_manual(values = c(\"#991b1b\", \"#ea580c\", \"#ca8a04\", \"#16a34a\")) +\n  scale_color_manual(values = c(\"#991b1b\", \"#ea580c\", \"#ca8a04\", \"#16a34a\")) +\n  labs(\n    title = \"Progresja CTG: Nałożone Rozkłady\",\n    subtitle = \"Analiza związku między wielkością próby a zmiennością rozkładu próbkowego\",\n    x = \"Wartość Średnia\",\n    y = \"Gęstość\",\n    fill = \"Wielkość Próby\",\n    color = \"Wielkość Próby\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nKluczowa Obserwacja: Wraz ze wzrostem wielkości próby rozkład charakteryzuje się następującymi właściwościami:\n\nZwiększona symetria (kształt dzwonowy)\nWiększa koncentracja wokół wartości oczekiwanej (3,5)\nLepsza zgodność z rozkładem normalnym\n\n\n\nZbieżność Błędu Standardowego\nRozrzut (odchylenie standardowe) maleje zgodnie ze wzorem SE = σ/√n:\n\ndane_wariancji &lt;- wszystkie_dane %&gt;%\n  group_by(n, etykieta) %&gt;%\n  summarise(\n    obserwowane_sd = sd(srednia),\n    teoretyczne_se = sqrt(35/12) / sqrt(n),\n    .groups = \"drop\"\n  )\n\nggplot(dane_wariancji, aes(x = n)) +\n  geom_line(aes(y = obserwowane_sd, color = \"Obserwowane SD\"), \n            linewidth = 1.5) +\n  geom_point(aes(y = obserwowane_sd, color = \"Obserwowane SD\"), \n             size = 4) +\n  geom_line(aes(y = teoretyczne_se, color = \"Teoretyczne SE\"), \n            linewidth = 1.5, linetype = \"dashed\") +\n  geom_point(aes(y = teoretyczne_se, color = \"Teoretyczne SE\"), \n             size = 4) +\n  scale_color_manual(values = c(\"Obserwowane SD\" = \"#3b82f6\", \n                                \"Teoretyczne SE\" = \"#ef4444\")) +\n  scale_x_continuous(breaks = wielkosci_prob) +\n  labs(\n    title = \"Błąd Standardowy Maleje wraz ze Wzrostem Wielkości Próby\",\n    subtitle = \"Zgodnie ze związkiem SE = σ/√n\",\n    x = \"Wielkość Próby (n)\",\n    y = \"Odchylenie Standardowe / Błąd Standardowy\",\n    color = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    legend.position = \"top\",\n    legend.text = element_text(size = 11)\n  )\n\n\n\n\n\n\n\n\n\n\nPodsumowanie Numeryczne\n\nstatystyki_podsumowanie &lt;- wszystkie_dane %&gt;%\n  group_by(etykieta) %&gt;%\n  summarise(\n    n = first(n),\n    Obserwowana_Srednia = round(mean(srednia), 3),\n    Obserwowane_SD = round(sd(srednia), 3),\n    Teoretyczna_Srednia = 3.5,\n    Teoretyczne_SE = round(sqrt(35/12) / sqrt(first(n)), 3),\n    Zakres = paste0(\"[\", round(min(srednia), 2), \", \", round(max(srednia), 2), \"]\")\n  ) %&gt;%\n  select(-etykieta)\n\nknitr::kable(statystyki_podsumowanie, \n             caption = \"Wartości Obserwowane vs Teoretyczne dla Różnych Wielkości Próby\")\n\n\nWartości Obserwowane vs Teoretyczne dla Różnych Wielkości Próby\n\n\n\n\n\n\n\n\n\n\nn\nObserwowana_Srednia\nObserwowane_SD\nTeoretyczna_Srednia\nTeoretyczne_SE\nZakres\n\n\n\n\n1\n3.495\n1.707\n3.5\n1.708\n[1, 6]\n\n\n2\n3.503\n1.205\n3.5\n1.208\n[1, 6]\n\n\n5\n3.500\n0.765\n3.5\n0.764\n[1, 6]\n\n\n10\n3.499\n0.540\n3.5\n0.540\n[1.3, 5.6]\n\n\n30\n3.501\n0.313\n3.5\n0.312\n[2.17, 4.77]\n\n\n50\n3.501\n0.241\n3.5\n0.242\n[2.36, 4.54]\n\n\n\n\n\nObserwacje:\n\nWartość oczekiwana pozostaje stała na poziomie 3,5 (niezależnie od wielkości próby)\nBłąd standardowy wykazuje systematyczny spadek wraz ze wzrostem n (zgodnie ze związkiem SE ∝ 1/√n)\nRozstęp wartości ulega znacznemu zawężeniu wraz ze wzrostem wielkości próby",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#podstawy-matematyczne-1",
    "href": "rozdzial1.html#podstawy-matematyczne-1",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.26 Podstawy Matematyczne",
    "text": "2.26 Podstawy Matematyczne\nDla populacji ze średnią μ i skończoną wariancją σ²:\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\text{ gdy } n \\to \\infty\nBłąd standardowy średniej:\nSE_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\nDla uczciwej kostki: μ = 3,5, σ² = 35/12 ≈ 2,917",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#najważniejsze-wnioski-1",
    "href": "rozdzial1.html#najważniejsze-wnioski-1",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.27 Najważniejsze Wnioski",
    "text": "2.27 Najważniejsze Wnioski\n\nPunkt Wyjścia: Pojedyncza kostka charakteryzuje się rozkładem jednostajnym (równomiernym)\nStopniowa Transformacja: Wraz ze wzrostem liczby obserwacji kształt rozkładu stopniowo ewoluuje\nKonwergencja do Normalności: Przy n=30 obserwujemy wyraźny rozkład normalny\nRedukcja Zmienności: Rozkład charakteryzuje się coraz większą koncentracją wokół wartości oczekiwanej\nUniwersalność: Twierdzenie ma zastosowanie do każdego rozkładu populacji z skończoną wariancją",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#dlaczego-to-ma-znaczenie-1",
    "href": "rozdzial1.html#dlaczego-to-ma-znaczenie-1",
    "title": "2  Podstawy Statystyki i Demografii",
    "section": "2.28 Dlaczego To Ma Znaczenie",
    "text": "2.28 Dlaczego To Ma Znaczenie\nTa transformacja pozwala nam:\n\nUżywać tablic i właściwości rozkładu normalnego do wnioskowania\nObliczać przedziały ufności ze znanym prawdopodobieństwem\nPrzeprowadzać testy hipotez (testy t, testy z)\nDokonywać przewidywań dotyczących średnich próbkowych\n\nKluczowa właściwość CTG: Mimo że rozkład pojedynczych rzutów kostką jest jednostajny, rozkład średnich z wielu kostek zbliża się do rozkładu normalnego w sposób przewidywalny i zgodny z teorią matematyczną.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "3.1 Foundations in Number Sets\nIn social science research, understanding the nature of our data is crucial for selecting appropriate analysis methods and drawing valid conclusions.\nBefore diving into data types, it’s essential to understand the basic number sets that form the foundation of our understanding of data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#foundations-in-number-sets",
    "href": "chapter2.html#foundations-in-number-sets",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "Basic Number Sets\n\nNatural Numbers (ℕ): The counting numbers {0, 1, 2, 3, …}\nIntegers (ℤ): Includes natural numbers, their negatives, and zero {…, -2, -1, 0, 1, 2, …}\nRational Numbers (ℚ): Numbers that can be expressed as a fraction of two integers\nReal Numbers (ℝ): All numbers on the number line, including rationals and irrationals\n\n\n\nProperties of Sets\n\nCountable Sets: Sets whose elements can be put in a one-to-one correspondence with the natural numbers. For example, the set of integers is countable.\nUncountable Sets: Sets that are not countable. The set of real numbers is uncountable.\nDiscrete Sets: Sets where each element is separated from other elements by a finite gap. The integers form a discrete set.\nDense Sets: Sets where between any two elements, there is always another element of the set. The rational numbers and real numbers are dense sets.\n\n\n\n\n\n\n\nNote\n\n\n\nUnderstanding these set properties is crucial for grasping the nature of different data types in social sciences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#types-of-data-basic-typologies",
    "href": "chapter2.html#types-of-data-basic-typologies",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.2 Types of Data (Basic Typologies)",
    "text": "3.2 Types of Data (Basic Typologies)\nAt a fundamental level, data variables can be classified as either numerical (quantitative) or categorical (qualitative).\n\nNumerical data represents each data point as a number (e.g., items sold, temperature, height, age). It can be discrete (counted) or continuous (measured).\nCategorical data represents each data point as a word or label (e.g., brand names, animal types, colours, countries). It can be ordinal (ordered, like rankings) or nominal (unordered, like names).\n\n\nData Typology 1: Qualitative vs Quantitative\n\n\n\n\n\ngraph TD\n    A[Data] --&gt; B[Qualitative]\n    A --&gt; C[Quantitative]\n    B --&gt; D[Nominal]\n    B --&gt; E[Ordinal]\n    C --&gt; F[Discrete]\n    C --&gt; G[Continuous]\n    \n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#ffe1f5\n    style D fill:#f0f0f0\n    style E fill:#f0f0f0\n    style F fill:#f0f0f0\n    style G fill:#f0f0f0\n\n\n\n\n\n\n\n\nData Typology 2: Stevens’ Levels of Measurement\n\n\n\n\n\ngraph TD\n    A[Levels of Measurement] --&gt; B[Categorical/Qualitative]\n    A --&gt; C[Numerical/Quantitative]\n    \n    B --&gt; D[Nominal]\n    B --&gt; E[Ordinal]\n    C --&gt; F[Interval]\n    C --&gt; G[Ratio]\n    \n    D --&gt; D1[Categories only&lt;br/&gt;e.g., gender, color]\n    E --&gt; E1[Ordered categories&lt;br/&gt;e.g., rankings, grades]\n    F --&gt; F1[Equal intervals, no true zero&lt;br/&gt;e.g., temperature °C]\n    G --&gt; G1[Equal intervals, true zero&lt;br/&gt;e.g., height, weight, age, Kelvins]\n    \n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#ffe1f5\n    style D fill:#f0f0f0\n    style E fill:#f0f0f0\n    style F fill:#f0f0f0\n    style G fill:#f0f0f0\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Difference: Categorical vs. Numerical Scales\n\n\n\nThe fundamental difference between categorical and numerical scales lies not in the number of categories, but in the nature of the intervals. Numerical scales (interval and ratio) have intervals defined by an objective, physical measurement standard. For example:\n\nTemperature: each degree represents the same amount of thermal energy change, regardless of how we feel about it\nDistance: a centimeter between 5cm and 6cm represents exactly the same physical length as a centimeter between 105cm and 106cm\nTime: a second between 10:00:01 and 10:00:02 has the same duration as a second between 15:47:33 and 15:47:34\nWeight: the difference between 50kg and 51kg represents the same amount of mass as the difference between 150kg and 151kg\n\nCategorical scales, including all rating scales, lack this objective standard: even a 1000-point rating scale remains ordinal because the intervals are psychologically defined, not physically measured. When someone rates their agreement as “7” versus “8” on a scale from 1 to 1000, there is no guarantee that the psychological distance between 7 and 8 equals the distance between 47 and 48, or between 847 and 848. The respondent is essentially choosing among ordered categories, not measuring with, e.g., a ruler. Adding more categories doesn’t create equal intervals - it just creates more ordered categories. This is why even a seemingly continuous rating scale with hundreds of values remains fundamentally categorical: it lacks the objective measurement standard that would make the intervals provably equal.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#discrete-vs.-continuous-data-quantitative-data",
    "href": "chapter2.html#discrete-vs.-continuous-data-quantitative-data",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.3 Discrete vs. Continuous Data (Quantitative Data)",
    "text": "3.3 Discrete vs. Continuous Data (Quantitative Data)\nIn data science and statistics, we categorize variables as either discrete or continuous. This distinction shapes how we analyze data and which statistical methods we apply. However, the boundary between these categories is not always clear-cut, and some variables exhibit characteristics of both types. This section explores discrete and continuous data, their differences, and the interesting cases of variables that challenge our intuitive understanding.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDiscrete Data\nDiscrete data can only take on specific, countable values. These values are often (but not always) integers.\n\nCharacteristics of Discrete Data:\n\nCountable set of possible values\nOften represented by integers\nCan be finite or infinite\nNo values exist between two adjacent data points\n\n\n\nExamples:\n\nNumber of students in a class\nNumber of cars sold by a dealership\nNumber of coins in a piggy bank\nDice rolls (1, 2, 3, 4, 5, or 6)\n\n\n\n\nContinuous Data\nContinuous data can take on any value within a given range, including fractional and decimal values. Importantly, continuity is characterized by density: between any two values, there are infinitely many other possible values.\n\nCharacteristics of Continuous Data:\n\nValues from dense sets (rational numbers or real numbers)\nCan be measured to any level of precision (theoretically)\nThere are infinitely many values between any two data points\nTypically represented on a continuous scale\n\n\n\nExamples:\n\nHeight\nWeight\nTemperature\nTime duration\n\n\n\n\n\n\n\nMathematical Note: Density and Continuity\n\n\n\nContinuous data comes from dense sets, where between any two distinct values, there exists another value from the set. The most common examples are:\n\nReal numbers: Uncountable and dense\nRational numbers: Countable but dense\n\nThis density property is what gives continuous data its characteristic “smoothness” and allows us to apply calculus-based statistical methods.\n\n\n\n\n\nThe Discrete-Continuous Spectrum\nIn practice, some variables that are mathematically discrete are often treated as if they are continuous. This dual nature provides flexibility in how these variables can be analyzed and interpreted.\n\nReasons for Treating Discrete Data as Continuous:\n\nFine Granularity (Small Discrete Increments)\n\nWhen a discrete variable has very small increments between possible values, it can approximate continuity.\nExample: Income measured in individual cents. While technically discrete, the tiny increments and vast number of possible values make it behave similarly to a continuous variable.\n\nAnalytical Convenience\n\nContinuous methods often yield reasonable and useful results even for dense discrete variables.\nAssuming continuity allows the use of calculus-based methods and existing statistical tools.\n\nApproximation of Underlying Phenomena\n\nA discrete measurement might represent an underlying continuous process.\nExample: While we measure time in discrete units (seconds, minutes, hours), time itself flows continuously.\n\n\n\n\nExamples of Variables with Dual Discrete-Continuous Nature:\n\nAge\n\nDiscrete: Typically reported in whole years\nContinuous: Can be considered as a continuous variable in many analyses, especially when dealing with large populations or when precision matters\n\nPrice and Income\n\nDiscrete: Prices and incomes are actually measured in discrete units (e.g., cents or smallest currency unit)\nContinuous: In economic models and many analyses, treated as continuous variables due to their dense nature and analytical convenience\n\nTest Scores\n\nDiscrete: Often given as whole numbers or fixed increments\nContinuous: In statistical analyses, may be treated as continuous, especially when the range of possible scores is large\n\n\n\n\n\nConclusion\nThe distinction between discrete and continuous data is not always rigid in practice. Many variables, including those involving money, percentages, or dense measurements, can be viewed through both discrete and continuous lenses. When in doubt, consider both your measurement precision and your analytical goals when deciding how to treat a variable. The choice should be guided by the nature of the data, the goals of the analysis, and the potential implications of the choice. This flexibility, when used thoughtfully, provides powerful tools for researchers to gain insights from their data.\n\n\n\n\n\n\nDiscrete vs. Continuous Numerical Data: A Language-Based Analogies\n\n\n\n\nThe Language Connection\nThink about how you naturally ask questions about quantities:\n\n“How many cookies are in the jar?” (counting)\n“How much water is in the glass?” (measuring)\n\nThis natural language distinction reflects the two fundamental types of numerical data:\n\n\nDiscrete Data = “How Many?” Questions\n\nLike counting whole objects (countable nouns)\nTakes specific values with gaps between them\nExamples:\n\nNumber of pets: 0, 1, 2, 3… (can’t have 2.5 pets)\nDice rolls: 1, 2, 3, 4, 5, 6\nStudents in a class: 20, 21, 22…\n\n\n🤔 Self-Check: Can you find a value between 2 and 3 students? Why not?\n\n\nContinuous Data = “How Much?” Questions\n\nLike measuring quantities (uncountable nouns)\nCan take any value within a range\nExamples:\n\nHeight: 1.7231… meters\nTemperature: 36.8325… °C\nTime: 3.5792… hours\n\n\n🤔 Self-Check: Write down three different values between 1.72 and 1.73 meters\n\n\nQuick Recognition Guide\n\nIf you naturally ask “How many?” → Discrete\nIf you naturally ask “How much?” → Continuous\nIf you can measure it more precisely → Continuous\nIf you can only use whole numbers → Discrete\n\n✍️ Practice: Classify these quantities as discrete or continuous\n\nYour age in years: _____\nYour height: _____\nNumber of songs in a playlist: _____\nVolume of water: _____",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#introduction-to-stevens-data-typology",
    "href": "chapter2.html#introduction-to-stevens-data-typology",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.4 Introduction to Stevens’ Data Typology",
    "text": "3.4 Introduction to Stevens’ Data Typology\nStanley S. Stevens, an American psychologist, introduced a classification system for scales of measurement in his 1946 paper “On the Theory of Scales of Measurement.” This system, known as Stevens’ data typology or levels of measurement, has become fundamental in understanding how different types of data should be analyzed and interpreted.\nStevens proposed four levels of measurement:\n\nNominal\nOrdinal\nInterval\nRatio\n\nEach level has specific properties and allows for different types of statistical operations and analyses.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n\n\n\n\n\nWhy “Levels” of Measurement?\n\n\n\nStanley Stevens used the term “levels of measurement” rather than simply “types” because he conceived of them as existing in a hierarchy, where each level builds upon the previous one with increasing mathematical properties.\nHierarchical Structure:\nStevens organized them from least to most informative: Nominal → Ordinal → Interval → Ratio. Each level includes all properties of the levels below it, plus additional mathematical characteristics.\nMeasurement Theory Perspective:\nStevens was fundamentally concerned with what mathematical operations are meaningful at each level:\n\nNominal: only equality comparisons\nOrdinal: adds greater than/less than\nInterval: adds addition and subtraction\n\nRatio: adds multiplication, division, and meaningful ratios\n\nThe word “level” emphasizes that these aren’t just arbitrary categories, but represent degrees of measurement sophistication. Higher levels allow more powerful statistical analyses and convey more information about the underlying phenomena being measured.\nThis hierarchical framing helped researchers understand not just what kind of data they had, but what they could legitimately do with it statistically.\n\n\n\nNominal Scale\n\nDefinition\nThe nominal scale is the most basic level of measurement. It uses labels or categories to classify data without any quantitative value or order.\n\n\nProperties\n\nCategories are mutually exclusive\nNo inherent order among categories\nNo meaningful arithmetic operations can be performed\n\n\n\nExamples\n\nNationality (Polish, English, …)\nBlood types (A, B, AB, O)\nEye color (Blue, Brown, Green, Hazel)\nBinary variables (“Success” versus “Failure”)\n\n\n\n\nOrdinal Scale\n\nDefinition\nThe ordinal scale categorizes data into ordered categories, but the intervals between categories are not necessarily equal or meaningful.\n\n\nProperties\n\nCategories have a defined order\nDifferences between categories are not quantifiable\nArithmetic operations on the numbers are not meaningful\n\n\n\nExamples\n\nEducation levels (High School, Bachelor’s, Master’s, PhD)\nLikert scales (Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree)\nSocioeconomic status (Low, Medium, High)\n\n\n\n\n\n\n\nLikert Scale Question Types\n\n\n\nLikert scales are a type of rating scale that constitute an ordinal level of measurement, meaning responses can be ranked in order but the distances between points are not necessarily equal.\nAgreement Scale (Strongly Disagree → Strongly Agree)\n\nBest for measuring beliefs and perceptions\nExample: “This website is easy to navigate”\n\nSatisfaction Scale (Very Dissatisfied → Very Satisfied)\n\nIdeal for evaluating experiences and services\nExample: “How satisfied are you with the customer service you received?”\n\nOpinion Scale (Strongly Oppose → Strongly Support)\n\nUsed to gauge positions on proposals or initiatives\nExample: “How do you feel about implementing a hybrid work policy?”\n\nFrequency/Attitude Scale (Never → Always)\n\nMeasures behavioral patterns and consistency\nExample: “I would recommend this product to others”\n\nBest Practices: Use 5-7 point scales, ensure statements are clear and unambiguous, and maintain consistent directionality (positive to negative or vice versa).\n\nThe choice between even (without midpoint) and odd Likert scales is a substantive methodological decision, not just a formatting preference. There’s no consensus in the literature that one approach is universally superior. Your choice should be driven by your theoretical framework and what the midpoint would actually represent in your specific context. What are you measuring, and do you have a theoretical reason to believe neutrality should or shouldn’t exist?\n\nExamples of rating scales\n  \n\n\n\n\n\n\nInterval Scale\n\nDefinition\nThe interval scale has ordered categories with equal intervals between adjacent categories. However, it lacks a true zero point.\n\n\nProperties\n\nEqual intervals between adjacent categories: The difference between any two adjacent values represents the same magnitude of change. For example, the difference between 10°C and 20°C represents the same amount of temperature change as the difference between 80°C and 90°C. This means we can meaningfully add and subtract values (e.g., “it’s 5 degrees warmer today than yesterday”).\nNo true zero point (zero is arbitrary): Zero does not represent the complete absence of the measured property. For example, 0°C does not mean “no temperature” - it’s simply the freezing point of water. This arbitrary zero could be placed elsewhere (as in Fahrenheit, where water freezes at 32°F).\nRatios between values are not meaningful: Because zero is arbitrary, we cannot say that 20°C is “twice as hot” as 10°C. If we converted to Fahrenheit (50°F and 68°F), the ratio would be completely different, showing that the ratio depends on our arbitrary choice of scale.\n\nKey insight: On an interval scale, differences have consistent meaning, but ratios do not.\n\n\nExamples\n\nTemperature in Celsius or Fahrenheit\nCalendar years\npH scale (the difference between pH 4 and 5 represents the same change in hydrogen ion concentration as between pH 6 and 7)\nElevation above sea level\n\n\n\n\nRatio Scale\n\nDefinition\nThe ratio scale is the highest level of measurement. It has all the properties of the interval scale plus a true zero point, making ratios between values meaningful.\n\n\nProperties\n\nAll properties of interval scales\nTrue zero point\nRatios between values are meaningful\n\n\n\nExamples\n\nHeight\nWeight\nAge\nIncome\n\n\n\n\n\n\n\nWhy Some Statistics Work (and Others Don’t) for Interval Scales\n\n\n\n\nKey Idea\nAn interval scale is one where the distances between values are meaningful, but the zero point is arbitrary. For interval scales (e.g., temperature):\n\nAllowed: Addition/subtraction of values and multiplication/division by constants.\nNot allowed: Multiplication/division of values from the scale by each other, as this leads to results without physical interpretation.\n\n\n\nProperties of Interval Scales\n\nEqual intervals represent the same differences:\n\nThe difference between 20°C and 25°C (5°C) represents the same change as between 30°C and 35°C.\nProportions of differences are preserved: 10°C is twice the change of 5°C.\n\nThe zero point is arbitrary:\n\n0°C is the freezing point of water, not the absence of temperature.\nThe same physical state has different values in different scales: 0°C = 32°F.\n\nLinear transformation:\n\nGeneral formula: y = ax + b, where a \\neq 0.\nFor temperature: F = C \\times \\frac{9}{5} + 32.\n\n\n\n\nTheoretical Conclusions\n\nAllowed operations:\n\nAddition/subtraction (preserves differences).\nMultiplication/division by constants (scaling).\nArithmetic means.\nComparing temperature differences.\n\nNot allowed operations:\n\nMultiplying temperatures by each other.\nDividing temperatures by each other.\nGeometric means.\nCoefficient of variation.\n\nPractical implications:\n\nVariance and standard deviation require careful interpretation.\nBetter to use measures based on differences (e.g., MAD - mean absolute deviation).\nWhen comparing variability, it is advisable to standardize the data.\n\n\n\n\nPractical Rule\nIf your calculations involve multiplying values from an interval scale by each other, be particularly cautious in interpreting the results!\n\n\n\n\n\n\nImportance in Research and Analysis\nUnderstanding Stevens’ data typology is crucial for several reasons:\n\nChoosing appropriate statistical tests: The level of measurement determines which statistical analyses are appropriate for a given dataset.\nInterpreting results: The meaning of statistical results depends on the level of measurement of the variables involved.\nDesigning measurement instruments: When creating surveys or other measurement tools, researchers must consider the level of measurement they want to achieve.\nData transformation: Sometimes, data can be transformed from one level to another, but this must be done carefully to avoid misinterpretation.\n\n\n\nControversies and Limitations\nWhile Stevens’ typology is widely used, it has faced some criticisms:\n\nRigidity: Some argue that the typology is too rigid and that many real-world measurements fall between these categories.\nTreatment of ordinal data: There’s ongoing debate about when it’s appropriate to treat ordinal data as interval for certain analyses.\nPsychological scaling: Some psychological constructs (like intelligence) are difficult to categorize definitively within this system.\n\n\n\nConclusion\nStevens’ data typology provides a fundamental framework for understanding different types of data and their properties. By recognizing the level of measurement of their variables, researchers can make informed decisions about data collection, analysis, and interpretation. However, it’s important to remember that while this typology is a useful guide, real-world data often requires nuanced consideration and may not always fit neatly into these categories.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nOrdered values: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nEqual intervals: Each unit change in pH represents a consistent tenfold change in hydrogen ion concentration (logarithmic scale). The interval between pH 4 and pH 5 is equivalent to the interval between pH 7 and pH 8.\nNo true zero: pH 0 does not represent a complete absence of hydrogen ions. Negative pH values and values above 14 are possible in extreme conditions.\nRatios are not meaningful: A pH of 4 is not “twice as acidic” as a pH of 2. Relative acidity is determined by the ratio of hydrogen ion concentrations, not pH values themselves.\n\nThese characteristics align with the interval scale definition: differences between values are meaningful and consistent, but ratios cannot be interpreted.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#common-ordinal-scales-in-behavioural-research",
    "href": "chapter2.html#common-ordinal-scales-in-behavioural-research",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.5 Common Ordinal Scales in Behavioural Research",
    "text": "3.5 Common Ordinal Scales in Behavioural Research\nMany measures in psychology and social sciences are ordinal in nature, even when they appear as numbers. Understanding this distinction is crucial for proper analysis and interpretation. Let’s explore the most common examples.\n\nLikert Scales\nLikert scales are widely used in psychology and social sciences to measure attitudes, opinions, and perceptions. Named after psychologist Rensis Likert, these scales typically consist of statements or questions that respondents rate on a scale, often from “Strongly Disagree” to “Strongly Agree.”\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nWhy Likert Scales are Ordinal Variables\nLikert scales are ordinal because:\n\nOrder without equal intervals: While responses have a clear order (e.g., “Strongly Disagree” &lt; “Disagree” &lt; “Neutral” &lt; “Agree” &lt; “Strongly Agree”), we cannot assume the psychological distance between “Disagree” and “Neutral” equals the distance between “Neutral” and “Agree.”\nSubjective interpretation: Different respondents may interpret the same scale points differently. What feels like “Agree” to one person might be “Strongly Agree” to another.\nLack of true zero point: There’s no absolute zero representing “complete absence” of the attitude being measured.\n\n\n\n\n\n\n\nTip\n\n\n\nThink of it this way: If you’re ranking your favorite films as “Love it,” “Like it,” “Neutral,” “Dislike it,” “Hate it,” you know the order clearly. But can you say the difference between “Love it” and “Like it” is exactly the same as between “Dislike it” and “Hate it”? Probably not.\n\n\n\n\n\nIQ Scores: A Complex Case\nIQ appears to be a continuous, interval-level measure, but it’s fundamentally ordinal. This is one of the most misunderstood aspects of psychological measurement.\n\nHow IQ Scores are Actually Created\nLet’s walk through a concrete example:\n\nCollecting raw scores: 1,000 children take a test with 60 questions. Child A answers 45 correctly, Child B answers 38, Child C answers 52, etc.\nRanking: All 1,000 raw scores are ordered from lowest (say, 12 correct) to highest (say, 58 correct)\nPercentile assignment: Child A (45 correct) is at the 70th percentile—better than 70% of children. Child C (52 correct) is at the 95th percentile.\n\n\n\n\n\n\n\nWhat is a Percentile?\n\n\n\nA percentile tells you what percentage of people scored below you.\nExamples:\n\n50th percentile = you scored better than 50% of people (exactly average)\n70th percentile = you scored better than 70% of people (above average)\n95th percentile = you scored better than 95% of people (well above average)\n10th percentile = you scored better than only 10% of people (below average)\n\nIf you’re at the 80th percentile, it means 80% of people scored lower than you, and 20% scored higher.\n\n\n\nMathematical transformation: These percentiles are converted to IQ scores so that the mean equals 100 and standard deviation equals 15. Child A gets IQ 110, Child C gets IQ 125.\n\nThe fundamental issue: This process forces the scores into a bell curve shape, even if the original raw scores weren’t bell-shaped. Imagine if most children got either 20-30 questions right or 50-55 questions right, with few in between. The transformation would still produce a smooth bell curve of IQ scores, hiding this gap in the actual test performance.\n\n\n\n\n\n\nWhat is a Bell-Shaped (Normal) Distribution?\n\n\n\nA bell-shaped distribution, also called a normal distribution or bell curve, is a symmetrical pattern where:\n\nMost values cluster around the average (the middle of the bell)\nFewer values appear as you move away from the average in either direction\nThe distribution is symmetrical—the left and right sides mirror each other\n\nThink of heights: Most people are around average height (170-180 cm), fewer people are very short (150 cm) or very tall (200 cm), creating a bell shape.\nIn IQ: The scores are transformed so that most people score around 100 (the peak of the bell), fewer people score 85 or 115 (moving down the sides), and very few people score 70 or 130 (at the tails).\nThe key issue: Real test performance might not naturally follow this pattern, but the IQ transformation forces it into one.\n\n\n\nhttps://en.wikipedia.org/wiki/File:IQ_curve.svg\n\n\n\n\n\n\n\n\n\n\nKey Point\n\n\n\nIQ 130 does not mean “twice the intelligence” of IQ 65. IQ points only indicate a person’s position relative to others in the standardization sample, not an absolute quantity of intelligence.\n\n\n\n\nThe Methodological Compromise\nIn research practice, IQ is often treated as an interval scale. This is a pragmatic compromise that allows researchers to use more powerful statistical tools, but it comes with important caveats.\n✅ Treating IQ as interval is acceptable when:\n\nUsing standard statistical tests (correlations, regressions, t-tests) for exploratory or applied research\nComparing groups who took the same IQ test, scored by the same norms (e.g., comparing 8-year-olds who all took WISC-V using 2024 norms—NOT comparing WISC-V scores to Stanford-Binet scores, or 2024 norms to 1990 norms)\nYou explicitly acknowledge this limitation in your interpretation\nYour conclusions focus on patterns rather than precise numerical differences\n\n⚠️ Remember the limitations:\n\nThis treatment works better for scores near the population mean (IQ 85-115) than at the extremes (IQ 70 or IQ 145)\nThe assumption of equal intervals is an approximation, not reality\nResults must be interpreted with appropriate caution\n\n❌ Never:\n\nClaim that equal IQ differences mean equal differences in ability (e.g., saying “the gap between IQ 100 and 115 represents the same cognitive difference as the gap between IQ 85 and 100”)\nUse ratio statements like “twice as intelligent” or “50% smarter”\nSay “Person A with IQ 130 is 15 points smarter than Person B with IQ 115, and Person B is 15 points smarter than Person C with IQ 100, so all three differences are equal”\nInterpret IQ differences as if they were physical measurements like height or weight\n\n\n\n\nOther Psychological Measures\nMany widely-used psychological instruments share IQ’s ordinal nature:\n\nDepression scales (e.g., Beck Depression Inventory): A score of 20 doesn’t mean “twice as depressed” as a score of 10. Someone scoring 30 might be having severe daily symptoms, while someone scoring 15 might have mild occasional symptoms—but the 15-point difference doesn’t represent the same change in depression severity as the difference between scores of 5 and 20.\nAnxiety measures (e.g., State-Trait Anxiety Inventory): If three people score 30, 45, and 60, we know the order of their anxiety levels, but we can’t say the psychological distance between the first two equals the distance between the last two.\nPersonality assessments (e.g., Big Five Inventory): A score of 80 on Extraversion doesn’t mean “twice as extraverted” as a score of 40—it just means more extraverted.\n\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\nThese measures often use summed Likert-type items or other scoring methods that don’t guarantee equal intervals between scores, despite appearing as continuous numbers.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#conclusion-2",
    "href": "chapter2.html#conclusion-2",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nThe ordinal nature of behavioural measures presents both a conceptual challenge and a practical dilemma. While these scales provide valuable information about relative standing and group differences, they don’t support the precise quantitative interpretations we might wish for.\nKey takeaway: Most behavioural measures tell us about order and relative position, not about absolute quantities or equal intervals. This doesn’t make them less useful—it just means we must interpret them appropriately and honestly communicate their limitations to our students, readers, and research consumers.\nPractical summary:\n\nA higher score means more of the characteristic—this we can say confidently\nGroup A scoring higher than Group B—this is a valid conclusion\nThe difference between scores of 100 and 115 being exactly equal to the difference between 115 and 130—this we cannot claim\n\nBy acknowledging these limitations and choosing appropriate analytical methods, we can conduct rigorous research while maintaining scientific integrity.\n\n\n\n\n\n\n\nHow Does the Transformation from Percentiles to IQ Work Exactly (*)?\n\n\n\nThe transformation proceeds in two steps:\nStep 1: From percentile to standard score (z-score)\nA percentile is transformed into a standard score, which tells you how many standard deviations you are from the mean in a normal distribution.\nExamples:\n\n50th percentile (average) → z = 0\n84th percentile → z = +1 (one standard deviation above the mean)\n16th percentile → z = -1 (one standard deviation below the mean)\n98th percentile → z = +2\n2nd percentile → z = -2\n\nWhat is a z-score and why do we use it?\nA standard score (z-score) is a way of expressing where a given value falls relative to the rest of the data, using a common “measure of distance.”\nStandardization formula (z-score):\nz = \\frac{X - \\mu}{\\sigma}\nWhere:\n\nX = raw score (e.g., number of correct answers)\n\\mu = mean in the group\n\\sigma = standard deviation in the group\n\nWhere does this formula come from?\nThe standardization process consists of two operations:\n\n(X - \\mu) — centering: we subtract the mean from each value\n\nAfter this operation, the new mean = 0\nExample: if X = 45, \\mu = 40, then (45 - 40) = 5 points above the mean\n\n/\\sigma — scaling: we divide by the standard deviation\n\nAfter this operation, the new standard deviation = 1\nExample: if \\sigma = 10, then 5 points above the mean = 5/10 = 0.5 standard deviations\n\n\nResult of standardization: We obtain a variable that has \\mu = 0 and \\sigma = 1, regardless of what the original values were!\nIntuitive example:\nImagine three tests:\n\nTest A: \\mu = 50 points, \\sigma = 10 points\nTest B: \\mu = 200 points, \\sigma = 40 points\n\nTest C: \\mu = 15 points, \\sigma = 3 points\n\nA student who scores one standard deviation above the mean on each test will receive:\n\nTest A: 60 points → z = \\frac{60-50}{10} = +1\nTest B: 240 points → z = \\frac{240-200}{40} = +1\nTest C: 18 points → z = \\frac{18-15}{3} = +1\n\nThe raw scores are very different (60, 240, 18), but the z-score is the same (+1), which means the same relative position on each test. This is exactly what allows us to compare scores from different tests!\nStep 2: From standard score to IQ (de-standardization)\nDe-standardization is the reverse of the standardization process. If standardization transforms data to a scale with mean 0 and standard deviation 1, then de-standardization allows us to return to any chosen mean and standard deviation.\nDe-standardization formula:\nX = \\mu + \\sigma \\times z\nWhere does this formula come from?\nThis is an algebraic transformation of the z-score formula. We solve it for X:\n\n\\begin{align}\nz &= \\frac{X - \\mu}{\\sigma} \\\\[0.5em]\nz \\times \\sigma &= X - \\mu \\quad \\text{(multiply both sides by } \\sigma \\text{)} \\\\[0.5em]\nX &= \\mu + \\sigma \\times z \\quad \\text{(add } \\mu \\text{ to both sides)}\n\\end{align}\n\nWhat does this formula mean intuitively?\n“Take the mean (\\mu), and add the appropriate number of standard deviations (\\sigma \\times z) to it”\n\nIf z = +1, you add one standard deviation to the mean: X = \\mu + \\sigma\nIf z = -1, you subtract one standard deviation from the mean: X = \\mu - \\sigma\nIf z = 0, you get exactly the mean: X = \\mu\n\nIQ transformation is de-standardization with \\mu = 100 and \\sigma = 15:\n\\text{IQ} = 100 + 15 \\times z\nWhy 100 and 15 specifically?\nThis is an arbitrary convention established by David Wechsler in the 1930s for the WISC and WAIS scales. Earlier tests (like Stanford-Binet) used different values (\\mu = 100, but \\sigma = 16). Contemporary IQ tests most commonly use the convention \\mu = 100, \\sigma = 15, which makes it easier to compare scores between different tests.\nTheoretically, we could choose any values (e.g., \\mu = 500, \\sigma = 100 as in the SAT), but 100 and 15 have become the standard for intelligence tests.\nComparison: standardization vs. de-standardization (IQ transformation)\n\n\n\n\n\n\n\n\n\n\nOperation\nFormula\nMean\nStandard deviation\nDirection\n\n\n\n\nStandardization\nz = \\frac{X - \\mu}{\\sigma}\n0\n1\nRaw scores → z-score\n\n\nDe-standardization (IQ)\n\\text{IQ} = 100 + 15 \\times z\n100\n15\nz-score → IQ\n\n\n\nLogic of the process:\n\nStandardization creates a “neutral” scale (\\mu = 0, \\sigma = 1) — a common “currency”\nDe-standardization transforms this neutral scale to more convenient numbers (\\mu = 100, \\sigma = 15) — currency exchange at a fixed rate\n\nIt’s like temperature: Celsius → Kelvin (standardization to a “natural” scale) → Fahrenheit (transformation to another scale).\nConcrete calculation example:\nChild C is at the 95th percentile:\n\nThe 95th percentile in a normal distribution corresponds to z \\approx +1.645\n\\text{IQ} = 100 + 15 \\times 1.645 = 100 + 24.675 \\approx 125\n\nChild A is at the 70th percentile:\n\nThe 70th percentile corresponds to z \\approx +0.524\n\\text{IQ} = 100 + 15 \\times 0.524 = 100 + 7.86 \\approx 108\n\nKey observation:\nThis formula can be applied to any data — even if the raw scores don’t have a normal distribution. This is exactly what “forcing” a normal distribution means — the mathematics creates a bell curve regardless of the actual shape of the data. Percentiles are transformed through a theoretical normal distribution, not the actual distribution of raw scores.\nIn R:\n\n# Standardization of a variable\nraw_scores &lt;- c(35, 42, 45, 38, 51)\nz_scores &lt;- scale(raw_scores)  # mean = 0, SD = 1\n\n# De-standardization: back to original parameters\nmean_original &lt;- mean(raw_scores)\nsd_original &lt;- sd(raw_scores)\nback_to_original &lt;- mean_original + sd_original * z_scores\n\n# Transformation of percentiles to IQ (de-standardization with μ=100, σ=15)\npercentiles &lt;- c(0.70, 0.95)  # 70th and 95th percentile\nz_from_percentile &lt;- qnorm(percentiles)  # Step 1: percentile → z\nIQ &lt;- 100 + 15 * z_from_percentile      # Step 2: z → IQ (de-standardization)\n# Result: 108, 125",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#exercise-identifying-measurement-scales",
    "href": "chapter2.html#exercise-identifying-measurement-scales",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.7 Exercise: Identifying Measurement Scales",
    "text": "3.7 Exercise: Identifying Measurement Scales\nFor each of the following variables, determine the most appropriate scale of measurement (Nominal, Ordinal, Interval, or Ratio). Also evaluate whether the variable is discrete or continuous.\n\nGender\nCustomer satisfaction: Poor, Fair, Good, Excellent\nHeight (questionnaire): “I am: very short, short, average, tall, very tall”\nHeight (inches)\nReaction time (milliseconds)\nPostal codes: e.g., 61548, 61761, 62461, 47424, 65233\nAge (years)\nNationality\nStreet addresses\nMilitary ranks\nLeft-Right political self-placement (0-10 scale)\nFamily size: 1 child, 2 children, 3 children, …\nIQ score\nShirt size (S, M, L, XL, …)\nMovie ratings (1 star, 2 stars, 3 stars, 4 stars, 5 stars)\nTemperature (Celsius)\nTemperature (Kelvin)\nBlood types: A, B, AB, O\nIncome categories: low, medium, high\nVoter turnout (percentage)\nPolitical party affiliation\nElectoral district magnitude (number of seats)\nEducational attainment: No degree, High school, Bachelor’s, Master’s, PhD\nNumber of votes received by a candidate\nYear of birth (e.g., 1985, 1990, 2000)\nMarathon finish time (hours:minutes:seconds)\nLikert scale response: Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree\nSocial class: Working class, Middle class, Upper class\nNumber of political parties in parliament\nDistance from home to polling station (kilometers)\n\nRemember to justify your choices for each variable. Pay special attention to ambiguous cases where reasonable scholars might disagree about the appropriate scale.\n\nFor instance: In Stevens’ typology of measurement scales, street addresses are nominal data. This is because:\nThey serve purely as labels/identifiers. They have no inherent ordering (123 Main St isn’t “more than” 23 Oak St). You can’t perform meaningful mathematical operations on them.The only valid operation is testing for equality/inequality (is this the same address or different?)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#appendix-a",
    "href": "chapter2.html#appendix-a",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.8 Appendix A",
    "text": "3.8 Appendix A\n\n\n\n\n\n\nWhy Addition/Subtraction Works for Interval Data, but Multiplication/Division Requires Ratio Data\n\n\n\nInterval scales have equal intervals between values but an arbitrary zero point (e.g., Celsius temperature, calendar dates). Ratio scales have both equal intervals and an absolute zero (e.g., Kelvin temperature, height, weight).\n\nAddition and Subtraction: Valid for Interval Scales\nDifferences maintain consistent proportional relationships under linear transformations. When converting between scales using y = a + bx where b &gt; 0, the additive constant a cancels out:\n(y_2 - y_1) = (a + bx_2) - (a + bx_1) = b(x_2 - x_1)\nExample: Using Celsius to Fahrenheit conversion where F = 1.8C + 32:\n\nAny 10°C difference always converts to 18°F: 18 = 1.8 \\times 10\nTry (20°C - 10°C) \\rightarrow (68°F - 50°F) = 18°F\nTry (100°C - 90°C) \\rightarrow (212°F - 194°F) = 18°F\n\nThe relationship is consistent: a 10-degree Celsius difference always corresponds to an 18-degree Fahrenheit difference, regardless of where on the scale we measure.\n\n\nMultiplication and Division: Require Ratio Scales\nRatios are inconsistent when the zero point is arbitrary. The additive constant a does NOT cancel out in ratios:\n\\frac{y_2}{y_1} = \\frac{a + bx_2}{a + bx_1} \\neq b \\cdot \\frac{x_2}{x_1}\nUnless a = 0 (absolute zero), ratios change unpredictably depending on which values you compare.\nExample: Temperature ratios give inconsistent results:\n\nIs 20°C “twice as hot” as 10°C?\n\nCelsius: 20/10 = 2.0\nFahrenheit: 68/50 = 1.36\nKelvin: 293.15/283.15 = 1.035\n\nWhat about 100°C vs. 90°C?\n\nCelsius: 100/90 = 1.11\nFahrenheit: 212/194 = 1.09\n\n\nThe ratios vary depending on both the scale AND which temperatures we pick. Only with an absolute zero do ratios have consistent physical meaning.\n\n\nImplications for Statistical Measures\nArithmetic mean is valid for interval scales because it uses addition:\n\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\nWhen we transform to scale y, the mean transforms consistently: \\bar{y} = a + b\\bar{x}\nGeometric mean requires ratio scales because it uses multiplication:\nGM = \\sqrt[n]{x_1 \\times x_2 \\times \\cdots \\times x_n}\nThe geometric mean of temperatures in Celsius gives a different result than the geometric mean of the same temperatures in Fahrenheit (after converting back). This makes the geometric mean meaningless for interval data.\nExample: For temperatures 10°C and 20°C:\n\nGeometric mean in Celsius: \\sqrt{10 \\times 20} = 14.14°C \\rightarrow 57.45°F\nGeometric mean in Fahrenheit: \\sqrt{50 \\times 68} = 58.31°F \\rightarrow 14.62°C\n\nThese don’t match! The geometric mean depends on the arbitrary zero point.\n\n\nVariance and Standard Deviation: Valid for Interval Scales\nVariance and standard deviation are acceptable for interval data because they operate on deviations from the mean, which are differences. Critically, variance is translation-invariant: adding a constant to all values doesn’t change the variance because the deviations remain the same.\nUnder linear transformation y = a + bx, variance transforms predictably:\n\\text{Var}(y) = b^2 \\text{Var}(x)\nThe constant a cancels out when computing deviations, just as it does for simple differences.\nExample proof: For temperatures 10°C and 20°C:\nIn Celsius:\n\nMean: \\bar{x} = (10 + 20)/2 = 15°C\nDeviations: (10 - 15) = -5, (20 - 15) = 5\nVariance: \\text{Var}(X) = [(-5)^2 + (5)^2]/2 = 50/2 = 25°C^2\nStandard deviation: \\text{SD}(X) = 5°C\n\nIn Fahrenheit:\n\nConvert: 10°C → 50°F, 20°C → 68°F\nMean: \\bar{y} = (50 + 68)/2 = 59°F\nDeviations: (50 - 59) = -9, (68 - 59) = 9\nVariance: \\text{Var}(Y) = [(-9)^2 + (9)^2]/2 = 162/2 = 81°F^2\nStandard deviation: \\text{SD}(Y) = 9°F\n\nCheck the transformation:\n\nConversion slope: b = 1.8 (from F = 1.8C + 32)\nPredicted variance: 1.8^2 \\times 25 = 3.24 \\times 25 = 81°F^2 ✓\nPredicted SD: 1.8 \\times 5 = 9°F ✓\n\nPerfect match! The variance and standard deviation transform consistently and predictably, making them valid measures of spread for interval data.\nTranslation-invariance demonstration: If we shift all temperatures by +100°C (adding 110°C and 120°C):\n\nNew mean: (110 + 120)/2 = 115°C\nNew deviations: (110 - 115) = -5, (120 - 115) = 5\nNew variance: \\text{Var}(X) = [(-5)^2 + (5)^2]/2 = 25°C^2 (unchanged!)\n\nThe variance remains 25°C² because the spread hasn’t changed, only the location shifted.\nKey principle: Operations based on addition/subtraction and differences work for interval scales because the arbitrary constant a cancels out. Operations involving multiplication/division or ratios require ratio scales because a distorts the results. Variance and SD work because they’re translation-invariant and based on deviations from the mean (differences), not ratios.\n\n\nSummary: Valid Statistical Measures by Measurement Scale\n\n\n\nStatistical Measure\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nMode\n✓\n✓\n✓\n✓\n\n\nMedian\n✗\n✓\n✓\n✓\n\n\nArithmetic Mean\n✗\n✗\n✓\n✓\n\n\nGeometric Mean\n✗\n✗\n✗\n✓\n\n\nVariance & SD\n✗\n✗\n✓\n✓\n\n\nCovariance\n✗\n✗\n✓\n✓\n\n\nPearson Correlation\n✗\n✗\n✓\n✓\n\n\nSpearman Correlation\n✗\n✓\n✓\n✓\n\n\nCoefficient of Variation\n✗\n✗\n✗\n✓\n\n\n\nNotes:\n\nNominal scales (e.g., colors, categories) only support frequency-based measures like mode\nOrdinal scales (e.g., rankings, Likert scales) add median and rank-based correlations\nInterval scales (e.g., Celsius, calendar dates) support all measures based on addition/subtraction\nRatio scales (e.g., height, weight, Kelvin) additionally support measures requiring multiplication/division and meaningful ratios\nCoefficient of variation (\\text{CV} = \\text{SD}/\\text{Mean}) requires a meaningful zero point, so only ratio scales",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html",
    "href": "rozdzial2.html",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "",
    "text": "4.1 Podstawy Zbiorów Liczbowych\nW badaniach z obszaru nauk społecznych zrozumienie natury danych jest kluczowe dla wyboru odpowiednich metod analizy i wyciągania prawidłowych wniosków.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#podstawy-zbiorów-liczbowych",
    "href": "rozdzial2.html#podstawy-zbiorów-liczbowych",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "",
    "text": "Note\n\n\n\nZrozumienie właściwości zbiorów liczbowych jest kluczowe dla uchwycenia natury różnych typów danych w naukach społecznych.\n\n\n\nPodstawowe Zbiory Liczbowe\n\nLiczby Naturalne (ℕ): Liczby naturalne to zbiór liczb służących do liczenia i określania kolejności {0, 1, 2, 3, …}\nLiczby Całkowite (ℤ): Obejmują liczby naturalne, ich przeciwności i zero {…, -2, -1, 0, 1, 2, …}\nLiczby Wymierne (ℚ): Liczby, które można wyrazić jako ułamek dwóch liczb całkowitych\nLiczby Rzeczywiste (ℝ): Wszystkie liczby na osi liczbowej, w tym wymierne i niewymierne\n\n\n\nWłaściwości Zbiorów\n\nZbiory Przeliczalne: Zbiory, których elementy można ustawić w relacji jeden do jednego z liczbami naturalnymi. Na przykład, zbiór liczb całkowitych jest przeliczalny.\nZbiory Nieprzeliczalne: Zbiory, które nie są przeliczalne. Zbiór liczb rzeczywistych jest nieprzeliczalny.\nZbiory Dyskretne: Zbiory, w których każdy element jest oddzielony od innych elementów skończoną przerwą. Liczby całkowite tworzą zbiór dyskretny.\nZbiory Gęste: Zbiory, w których między dowolnymi dwoma elementami zawsze znajduje się inny element zbioru. Liczby wymierne i rzeczywiste są zbiorami gęstymi.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#typy-danych-dwie-podstawowe-typologie",
    "href": "rozdzial2.html#typy-danych-dwie-podstawowe-typologie",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.2 Typy danych (dwie podstawowe typologie)",
    "text": "4.2 Typy danych (dwie podstawowe typologie)\nNa poziomie podstawowym zmienne można podzielić na numeryczne (ilościowe) lub kategorialne (jakościowe).\n\nZmienne numeryczne reprezentują każdą obserwację jako liczbę (np. liczba sprzedanych produktów, temperatura, wzrost, wiek). Mogą być dyskretne (zliczane) lub ciągłe (mierzone).\nZmienne kategorialne reprezentują każdą obserwację jako słowo lub etykietę (np. nazwy marek, typy zwierząt, kolory, nazwy krajów). Mogą być porządkowe (uporządkowane, jak rankingi) lub nominalne (nieuporządkowane, jak nazwy).\n\n\nTypologia danych 1: Jakościowe vs Ilościowe\n\n\n\n\n\ngraph TD\n    A[Dane] --&gt; B[Jakościowe]\n    A --&gt; C[Ilościowe]\n    B --&gt; D[Nominalne]\n    B --&gt; E[Porządkowe]\n    C --&gt; F[Dyskretne]\n    C --&gt; G[Ciągłe]\n    \n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#ffe1f5\n    style D fill:#f0f0f0\n    style E fill:#f0f0f0\n    style F fill:#f0f0f0\n    style G fill:#f0f0f0\n\n\n\n\n\n\n\n\nTypologia danych 2: Poziomy pomiaru Stevensa\n\n\n\n\n\ngraph TD\n    A[Poziomy pomiaru] --&gt; B[Kategorialne/Jakościowe]\n    A --&gt; C[Numeryczne/Ilościowe]\n    \n    B --&gt; D[Nominalne]\n    B --&gt; E[Porządkowe]\n    C --&gt; F[Interwałowe]\n    C --&gt; G[Ilorazowe]\n    \n    D --&gt; D1[Tylko kategorie&lt;br/&gt;np. płeć, kolor]\n    E --&gt; E1[Uporządkowane kategorie&lt;br/&gt;np. rankingi, oceny]\n    F --&gt; F1[Równe przedziały, brak prawdziwego zera&lt;br/&gt;np. temperatura °C]\n    G --&gt; G1[Równe przedziały, prawdziwe zero&lt;br/&gt;np. wzrost, waga, wiek, Kelwiny]\n    \n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#ffe1f5\n    style D fill:#f0f0f0\n    style E fill:#f0f0f0\n    style F fill:#f0f0f0\n    style G fill:#f0f0f0\n\n\n\n\n\n\n\n\n\n\n\n\n\nKluczowa różnica: skale kategorialne (jakościowe) vs. numeryczne\n\n\n\nFundamentalna różnica między skalami kategorialnymi a numerycznymi nie polega na liczbie kategorii, ale na naturze odstępów (intervals). Skale numeryczne (interwałowe i stosunkowe) mają odstępy zdefiniowane przez obiektywny, fizyczny standard pomiaru. Na przykład:\n\nTemperatura: każdy stopień reprezentuje tę samą zmianę energii termicznej, niezależnie od tego, jak to odczuwamy\nOdległość: centymetr między 5cm a 6cm reprezentuje dokładnie tę samą fizyczną długość co centymetr między 105cm a 106cm\nCzas: sekunda między 10:00:01 a 10:00:02 ma ten sam czas trwania co sekunda między 15:47:33 a 15:47:34\nWaga: różnica między 50kg a 51kg reprezentuje tę samą ilość masy co różnica między 150kg a 151kg\n\nSkale kategorialne, w tym wszystkie skale ocen (rating scales), nie posiadają tego obiektywnego standardu: nawet 1000-punktowa skala ocen pozostaje porządkowa (ordinal), ponieważ odstępy są zdefiniowane psychologicznie, a nie fizycznie zmierzone. Kiedy ktoś ocenia swoją zgodność jako “7” versus “8” na skali od 1 do 1000, nie ma gwarancji, że psychologiczna odległość między 7 a 8 jest równa odległości między 47 a 48, lub między 847 a 848. Respondent zasadniczo wybiera spośród uporządkowanych kategorii, a nie mierzy np. linijką. Dodanie większej liczby kategorii nie tworzy równych odstępów - po prostu tworzy więcej uporządkowanych kategorii. Dlatego nawet pozornie ciągła skala ocen z setkami wartości pozostaje fundamentalnie kategorialna: brakuje jej obiektywnego standardu pomiaru, który sprawiłby, że odstępy byłyby udowodnione jako równe.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#dane-dyskretne-a-dane-ciągłe",
    "href": "rozdzial2.html#dane-dyskretne-a-dane-ciągłe",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.3 Dane dyskretne a dane ciągłe",
    "text": "4.3 Dane dyskretne a dane ciągłe\nW analizie danych i statystyce kategoryzujemy zmienne jako dyskretne lub ciągłe. Ten podział kształtuje sposób, w jaki analizujemy dane i które metody statystyczne stosujemy. Jednak granica między tymi kategoriami nie zawsze jest ostra, a niektóre zmienne wykazują cechy obu typów. Niniejszy rozdział omawia dane dyskretne i ciągłe, ich różnice oraz interesujące przypadki zmiennych, które kwestionują nasze intuicyjne rozumienie.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDane dyskretne\nDane dyskretne mogą przyjmować tylko określone, policzalne wartości. Wartości te są często (choć nie zawsze) liczbami całkowitymi.\n\nCharakterystyka danych dyskretnych:\n\nPrzeliczalny zbiór możliwych wartości\nCzęsto reprezentowane przez liczby całkowite\nMogą być skończone lub nieskończone\nBrak wartości pomiędzy dwoma sąsiednimi punktami danych\n\n\n\nPrzykłady:\n\nLiczba studentów w klasie\nLiczba samochodów sprzedanych przez salon\nLiczba monet w skarbonce\nWynik rzutu kostką (1, 2, 3, 4, 5 lub 6)\n\n\n\n\nDane ciągłe\nDane ciągłe mogą przyjmować dowolną wartość w danym zakresie, w tym wartości ułamkowe i dziesiętne. Co ważne, ciągłość charakteryzuje się gęstością: między dowolnymi dwiema wartościami istnieje nieskończenie wiele innych możliwych wartości.\n\nCharakterystyka danych ciągłych:\n\nWartości ze zbiorów gęstych (liczby wymierne lub rzeczywiste)\nMogą być mierzone z dowolną precyzją (teoretycznie)\nMiędzy dowolnymi dwoma punktami danych istnieje nieskończenie wiele wartości\nTypowo reprezentowane na skali ciągłej\n\n\n\nPrzykłady:\n\nWzrost\nWaga\nTemperatura\nCzas trwania\n\n\n\n\n\n\n\nUwaga matematyczna: Gęstość i ciągłość\n\n\n\nDane ciągłe pochodzą ze zbiorów gęstych, w których między dowolnymi dwiema różnymi wartościami istnieje inna wartość ze zbioru. Najczęstsze przykłady to:\n\nLiczby rzeczywiste: nieprzeliczalne i gęste\nLiczby wymierne: przeliczalne, ale gęste\n\nTa właściwość gęstości nadaje danym ciągłym charakterystyczną „gładkość” i pozwala stosować statystyczne metody oparte na rachunku różniczkowym i całkowym.\n\n\n\n\n\nSpektrum dyskretne-ciągłe\nW praktyce niektóre zmienne, które matematycznie są dyskretne, często traktuje się tak, jakby były ciągłe. Ta dualna natura zapewnia elastyczność w analizie i interpretacji tych zmiennych.\n\nPowody traktowania danych dyskretnych jako ciągłych:\n\nMałe przyrosty dyskretne (wysokie zagęszczenie wartości na skali, np. dochód per capita w PLN)\n\nGdy zmienna dyskretna ma bardzo małe przyrosty między możliwymi wartościami, może aproksymować ciągłość.\nPrzykład: Dochód mierzony w groszach. Choć technicznie dyskretny, bardzo małe przyrosty i ogromna liczba możliwych wartości sprawiają, że zachowuje się podobnie do zmiennej ciągłej.\n\nWygoda analityczna\n\nMetody ciągłe często dają rozsądne i użyteczne wyniki nawet dla gęstych zmiennych dyskretnych.\nZałożenie ciągłości pozwala na wykorzystanie metod opartych na rachunku różniczkowym i istniejących narzędzi statystycznych.\n\nAproksymacja zjawisk podstawowych\n\nPomiar dyskretny może reprezentować proces ciągły u podstaw.\nPrzykład: Choć mierzymy czas w jednostkach dyskretnych (sekundy, minuty, godziny), czas sam w sobie płynie w sposób ciągły.\n\n\n\n\nPrzykłady zmiennych o dualnej naturze dyskretno-ciągłej:\n\nWiek\n\nDyskretny: Typowo podawany w pełnych latach\nCiągły: Może być traktowany jako zmienna ciągła w wielu analizach, szczególnie przy dużych populacjach lub gdy liczy się precyzja\n\nCena i dochód\n\nDyskretne: Ceny i dochody są faktycznie mierzone w dyskretnych jednostkach (np. grosze lub najmniejsza jednostka walutowa)\nCiągłe: W modelach ekonomicznych i wielu analizach traktowane jako zmienne ciągłe ze względu na ich gęstą naturę i wygodę analityczną\n\nWyniki testów\n\nDyskretne: Często podawane jako liczby całkowite lub stałe przyrosty\nCiągłe: W analizach statystycznych mogą być traktowane jako ciągłe, szczególnie gdy zakres możliwych wyników jest duży\n\n\n\n\n\nPodsumowanie\nRozróżnienie między danymi dyskretnymi a ciągłymi nie zawsze jest sztywne w praktyce. Wiele zmiennych, w tym te obejmujące pieniądze, procenty czy gęste pomiary, można postrzegać zarówno przez pryzmat dyskretny, jak i ciągły. W razie wątpliwości należy rozważyć zarówno precyzję pomiaru, jak i cele analityczne przy podejmowaniu decyzji, jak traktować zmienną. Wybór powinien być kierowany naturą danych, celami analizy i potencjalnymi implikacjami wyboru. Ta elastyczność, stosowana rozważnie, zapewnia badaczom potężne narzędzia do wydobywania wniosków z danych.\n\n\n\n\n\n\nDane Dyskretne vs. Ciągłe: Analogia Językowa\n\n\n\n\nKluczowe Rozróżnienie Językowe\nW języku polskim mamy precyzyjne rozróżnienie:\n\n“Liczba” → używamy dla rzeczy policzalnych\n“Ilość” → używamy dla rzeczy niepoliczalnych\n\nTo rozróżnienie doskonale odzwierciedla dwa podstawowe typy danych liczbowych:\n\n\nDane Dyskretne = “Liczba czegoś”\n\nUżywamy słowa “liczba” (tak jak mówimy “liczba studentów”)\nWartości są rozdzielone jak pojedyncze elementy\nPrzykłady:\n\nLiczba książek: 0, 1, 2, 3…\nLiczba punktów w teście: 0, 1, 2…\nLiczba mieszkańców: 100, 101, 102…\n\n\n🤔 Czy poprawne jest powiedzenie “ilość studentów” czy “liczba studentów”? (Poprawna forma pomoże Ci rozpoznać typ danych)\n\n\nDane Ciągłe = “Ilość czegoś”\n\nUżywamy słowa “ilość” (tak jak mówimy “ilość wody”)\nWartości płynnie przechodzą jedna w drugą\nPrzykłady:\n\nIlość cieczy: 1,5231… litra\nIlość czasu: 2,3891… godziny\nIlość energii: 5,7123… kWh\n\n\n🤔 Czy mówimy “ilość wody” czy “liczba wody”? (Poprawna forma wskazuje na typ danych)\n\n\nSposób Rozpoznawania\n\nCzy użyłbyś słowa “liczba”? → Dane dyskretne\nCzy użyłbyś słowa “ilość”? → Dane ciągłe\n\n✍️ Ćwiczenie: Uzupełnij poprawnym słowem i określ typ danych\n\n_____ uczniów w klasie (liczba/ilość): typ _____\n_____ deszczu (liczba/ilość): typ _____\n_____ piosenek (liczba/ilość): typ _____\n_____ temperatury (liczba/ilość): typ _____",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "href": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.4 Wprowadzenie do Typologii Danych Stevensa",
    "text": "4.4 Wprowadzenie do Typologii Danych Stevensa\nStanley S. Stevens, amerykański psycholog, wprowadził system klasyfikacji skal pomiarowych w swoim artykule z 1946 roku “On the Theory of Scales of Measurement”. Ten system, znany jako typologia danych Stevensa lub poziomy pomiaru, stał się fundamentalny dla zrozumienia, jak różne typy danych powinny być analizowane i interpretowane.\nStevens zaproponował cztery poziomy pomiaru:\n\nNominalny\nPorządkowy\nInterwałowy\nIlorazowy\n\nKażdy poziom ma specyficzne właściwości i pozwala na różne rodzaje operacji statystycznych i analiz.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n\n\n\n\n\nDlaczego „poziomy” pomiaru?\n\n\n\nStanley Stevens użył terminu „poziomy pomiaru” zamiast po prostu „typy”, ponieważ pojmował je jako istniejące w hierarchii, gdzie każdy poziom opiera się na poprzednim, dodając coraz więcej właściwości matematycznych.\nStruktura hierarchiczna:\nStevens uporządkował je od najmniej do najbardziej informatywnych: Nominalny → Porządkowy → Interwałowy → Stosunkowy. Każdy poziom zawiera wszystkie właściwości poziomów poniżej, plus dodatkowe cechy matematyczne.\nPerspektywa teorii pomiaru:\nStevens był przede wszystkim zainteresowany tym, jakie operacje matematyczne są sensowne na każdym poziomie:\n\nNominalny: tylko porównania równości\nPorządkowy: dodaje relacje większy/mniejszy niż\nInterwałowy: dodaje dodawanie i odejmowanie\nStosunkowy: dodaje mnożenie, dzielenie i sensowne proporcje\n\nSłowo „poziom” podkreśla, że nie są to tylko arbitralne kategorie, ale reprezentują stopnie zaawansowania pomiaru. Wyższe poziomy pozwalają na potężniejsze analizy statystyczne i przekazują więcej informacji o mierzonych zjawiskach.\nTo hierarchiczne ujęcie pomogło badaczom zrozumieć nie tylko jaki rodzaj danych posiadają, ale co mogą z nimi legalnie zrobić statystycznie.\n\n\n\nSkala Nominalna\n\nDefinicja\nSkala nominalna jest najbardziej podstawowym poziomem pomiaru. Używa etykiet lub kategorii do klasyfikacji danych bez żadnej wartości ilościowej ani porządku.\n\n\nWłaściwości\n\nKategorie są wzajemnie wykluczające się\nBrak inherentnego porządku między kategoriami\nNie można wykonywać znaczących operacji arytmetycznych\n\n\n\nPrzykłady\n\nNarodowość (Polak, Niemiec, …)\nGrupy krwi (A, B, AB, O)\nKolor oczu (Niebieskie, Brązowe, Zielone, Piwne)\nZmienne binarne (“Sukces” versus “Niepowodzenie”)\n\n\n\n\nSkala Porządkowa\n\nDefinicja\nSkala porządkowa kategoryzuje dane w uporządkowane kategorie, ale odstępy między kategoriami niekoniecznie są równe lub znaczące.\n\n\nWłaściwości\n\nKategorie mają zdefiniowany porządek\nRóżnice między kategoriami nie są kwantyfikowalne\nOperacje arytmetyczne na liczbach nie są znaczące\n\n\n\nPrzykłady\n\nPoziomy wykształcenia (Szkoła Średnia, Licencjat, Magister, Doktorat)\nSkale Likerta (Zdecydowanie się nie zgadzam, Nie zgadzam się, Neutralnie, Zgadzam się, Zdecydowanie się zgadzam)\nStatus społeczno-ekonomiczny (Niski, Średni, Wysoki)\n\n\n\n\n\n\n\nTypy pytań ze skalą Likerta\n\n\n\nSkala Likerta to rodzaj skali ocen (rating scales), który stanowi porządkowy poziom pomiaru, co oznacza, że odpowiedzi można uszeregować w kolejności, ale odległości między poszczególnymi punktami niekoniecznie są równe.\nSkala zgody (Zdecydowanie się nie zgadzam → Zdecydowanie się zgadzam)\n\nNajlepsza do mierzenia przekonań i percepcji\nPrzykład: “Ta strona internetowa jest łatwa w nawigacji”\n\nSkala satysfakcji (Bardzo niezadowolony → Bardzo zadowolony)\n\nIdealna do oceny doświadczeń i usług\nPrzykład: “Jak ocenia Pan/Pani jakość obsługi klienta?”\n\nSkala opinii (Zdecydowanie się sprzeciwiam → Zdecydowanie popieram)\n\nWykorzystywana do badania stanowisk wobec propozycji lub inicjatyw\nPrzykład: “Jak Pan/Pani ocenia wprowadzenie pracy hybrydowej?”\n\nSkala częstotliwości/postawy (Nigdy → Zawsze)\n\nMierzy wzorce zachowań i konsystencję\nPrzykład: “Poleciłbym/Poleciłabym ten produkt innym”\n\nDobre praktyki: Używaj skal 5-7 punktowych, upewnij się, że stwierdzenia są jasne i jednoznaczne oraz utrzymuj spójny kierunek (od pozytywnego do negatywnego lub odwrotnie).\n\nWybór między parzystą (brak punktu środkowego) a nieparzystą skalą Likerta jest istotną decyzją metodologiczną, a nie tylko kwestią formatowania. W literaturze nie ma konsensusu, że jedno podejście jest uniwersalnie lepsze. Twój wybór powinien być podyktowany Twoimi ramami teoretycznymi i tym, co punkt środkowy rzeczywiście reprezentowałby w Twoim konkretnym kontekście. Co mierzysz i czy masz teoretyczne powody, by sądzić, że neutralność powinna lub nie powinna istnieć?\n\nPrzykłady skali ocen (rating scales)\n\n\n\nhttps://ppcexpo.com/ChartExpo/charts/csat-score-survey-chart\n\n\n\n\n\nhttps://www.culturemonkey.io/resources/guides/employee-survey-questions/\n\n\n\n\n\nhttps://www.culturemonkey.io/resources/guides/employee-survey-questions/\n\n\n\n\n\n\n\n\nSkala interwałowa (Interval Scale)\n\nDefinicja\nSkala interwałowa posiada uporządkowane kategorie z równymi odstępami między sąsiednimi kategoriami. Jednakże brakuje jej prawdziwego punktu zerowego.\n\n\nWłaściwości\n\nRówne odstępy między sąsiednimi kategoriami: Różnica między dowolnymi dwiema sąsiednimi wartościami reprezentuje tę samą wielkość zmiany. Na przykład różnica między 10°C a 20°C reprezentuje tę samą zmianę temperatury co różnica między 80°C a 90°C. Oznacza to, że możemy sensownie dodawać i odejmować wartości (np. “dzisiaj jest o 5 stopni cieplej niż wczoraj”).\nBrak prawdziwego punktu zerowego (zero jest arbitralne): Zero nie reprezentuje całkowitego braku mierzonej własności. Na przykład 0°C nie oznacza “braku temperatury” - to po prostu punkt zamarzania wody. To arbitralne zero mogłoby być umieszczone gdzie indziej (jak w skali Fahrenheita, gdzie woda zamarza przy 32°F).\nStosunki między wartościami nie są znaczące: Ponieważ zero jest arbitralne, nie możemy powiedzieć, że 20°C jest “dwa razy cieplejsze” niż 10°C. Gdybyśmy przeliczyć to na Fahrenheita (50°F i 68°F), stosunek byłby zupełnie inny, co pokazuje, że stosunek zależy od naszego arbitralnego wyboru skali.\n\nKluczowa intuicja: Na skali interwałowej różnice mają spójne znaczenie, ale stosunki nie.\n\n\nPrzykłady\n\nTemperatura w stopniach Celsjusza lub Fahrenheita\nLata kalendarzowe\nSkala pH\nWysokość nad poziomem morza\n\n\n\n\nSkala Ilorazowa\n\nDefinicja\nSkala ilorazowa jest najwyższym poziomem pomiaru. Ma wszystkie właściwości skali interwałowej plus prawdziwy punkt zerowy, co sprawia, że stosunki między wartościami są znaczące.\n\n\nWłaściwości\n\nWszystkie właściwości skal interwałowych\nPrawdziwy punkt zerowy\nStosunki między wartościami są znaczące\n\n\n\nPrzykłady\n\nWzrost\nWaga\nWiek\nDochód\n\n\n\n\nZnaczenie w Badaniach i Analizie\nZrozumienie typologii danych Stevensa jest kluczowe z kilku powodów:\n\nWybór odpowiednich testów statystycznych: Poziom pomiaru determinuje, które analizy statystyczne są odpowiednie dla danego zbioru danych.\nInterpretacja wyników: Znaczenie wyników statystycznych zależy od poziomu pomiaru zaangażowanych zmiennych.\nProjektowanie narzędzi pomiarowych: Przy tworzeniu ankiet lub innych narzędzi pomiarowych badacze muszą wziąć pod uwagę poziom pomiaru, który chcą osiągnąć.\nTransformacja danych: Czasami dane mogą być przekształcane z jednego poziomu na drugi, ale musi to być robione ostrożnie, aby uniknąć błędnej interpretacji.\n\n\n\nKontrowersje i Ograniczenia\nChociaż typologia Stevensa jest szeroko stosowana, spotkała się z pewnymi krytykami:\n\nSztywność: Niektórzy twierdzą, że typologia jest zbyt sztywna i że wiele rzeczywistych pomiarów mieści się pomiędzy tymi kategoriami.\nTraktowanie danych porządkowych: Trwa debata na temat tego, kiedy właściwe jest traktowanie danych porządkowych jako interwałowych dla pewnych analiz.\nSkalowanie psychologiczne: Niektóre konstrukty psychologiczne (jak inteligencja) są trudne do jednoznacznego skategoryzowania w ramach tego systemu.\n\n\n\nPodsumowanie\nTypologia danych Stevensa dostarcza fundamentalnych ram dla zrozumienia różnych rodzajów danych i ich właściwości. Rozpoznając poziom pomiaru swoich zmiennych, badacze mogą podejmować świadome decyzje dotyczące gromadzenia danych, analizy i interpretacji. Jednak ważne jest, aby pamiętać, że chociaż ta typologia jest użytecznym przewodnikiem, rzeczywiste dane często wymagają niuansowego podejścia i nie zawsze pasują idealnie do tych kategorii.\n\n\n\n\n\n\npH jako skala interwałowa\n\n\n\npH jest uważane za skalę interwałową, ponieważ:\n\nUporządkowane wartości: Niższe wartości pH wskazują na wyższą kwasowość, podczas gdy wyższe wartości wskazują na wyższą zasadowość.\nRówne przedziały: Każda zmiana pH o jedną jednostkę reprezentuje konsekwentnie dziesięciokrotną zmianę stężenia jonów wodorowych (skala logarytmiczna). Przedział między pH 4 a pH 5 jest równoważny przedziałowi między pH 7 a pH 8.\nBrak prawdziwego zera: pH 0 nie oznacza całkowitego braku jonów wodorowych. Ujemne wartości pH oraz wartości powyżej 14 są możliwe w ekstremalnych warunkach.\nStosunki nie mają znaczenia: pH 4 nie jest “dwukrotnie bardziej kwasowe” niż pH 2. Względna kwasowość jest określana przez stosunek stężeń jonów wodorowych, a nie przez same wartości pH.\n\nTe cechy są zgodne z definicją skali interwałowej: różnice między wartościami są znaczące, ale stosunki nie mogą być interpretowane.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#popularne-skale-porządkowe-w-badaniach-nad-zachowaniem",
    "href": "rozdzial2.html#popularne-skale-porządkowe-w-badaniach-nad-zachowaniem",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.5 Popularne Skale Porządkowe w Badaniach nad Zachowaniem",
    "text": "4.5 Popularne Skale Porządkowe w Badaniach nad Zachowaniem\nWiele miar w psychologii i naukach społecznych ma charakter porządkowy, nawet jeśli wyglądają jak liczby. Zrozumienie tego rozróżnienia jest kluczowe dla właściwej analizy i interpretacji. Przyjrzyjmy się najczęstszym przykładom.\n\nSkale Likerta\nSkale Likerta są szeroko stosowane w psychologii i naukach społecznych do pomiaru postaw, opinii i percepcji. Nazwane od nazwiska psychologa Rensisa Likerta, skale te zazwyczaj składają się z twierdzeń lub pytań, które respondenci oceniają na skali, często od „Zdecydowanie się nie zgadzam” do „Zdecydowanie się zgadzam”.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDlaczego Skale Likerta są Zmiennymi Porządkowymi\nSkale Likerta są porządkowe, ponieważ:\n\nPorządek bez równych odstępów: Chociaż odpowiedzi mają wyraźny porządek (np. „Zdecydowanie się nie zgadzam” &lt; „Nie zgadzam się” &lt; „Neutralnie” &lt; „Zgadzam się” &lt; „Zdecydowanie się zgadzam”), nie możemy zakładać, że psychologiczna odległość między „Nie zgadzam się” a „Neutralnie” jest równa odległości między „Neutralnie” a „Zgadzam się”.\nSubiektywna interpretacja: Różni respondenci mogą interpretować te same punkty skali w różny sposób. To, co dla jednej osoby jest „Zgadzam się”, dla innej może być „Zdecydowanie się zgadzam”.\nBrak prawdziwego punktu zerowego: Nie ma absolutnego zera reprezentującego „całkowity brak” mierzonej postawy.\n\n\n\n\n\n\n\nTip\n\n\n\nPomyśl o tym w ten sposób: Jeśli oceniasz swoje ulubione filmy jako „Uwielbiam”, „Lubię”, „Neutralnie”, „Nie lubię”, „Nienawidzę”, to wyraźnie znasz kolejność. Ale czy możesz powiedzieć, że różnica między „Uwielbiam” a „Lubię” jest dokładnie taka sama jak między „Nie lubię” a „Nienawidzę”? Prawdopodobnie nie.\n\n\n\n\n\nWyniki IQ: Złożony Przypadek\nIQ wydaje się być ciągłą miarą o charakterze przedziałowym, ale fundamentalnie ma charakter porządkowy. To jeden z najbardziej błędnie rozumianych aspektów pomiaru psychologicznego.\n\nJak Faktycznie Tworzone są Wyniki IQ\nPrzejdźmy przez konkretny przykład:\n\nZbieranie wyników surowych: 1000 dzieci rozwiązuje test z 60 pytaniami. Dziecko A odpowiada poprawnie na 45 pytań, Dziecko B na 38, Dziecko C na 52 itd.\nUszeregowanie: Wszystkie 1000 wyników surowych jest uporządkowanych od najniższego (powiedzmy, 12 poprawnych) do najwyższego (powiedzmy, 58 poprawnych).\nPrzypisanie percentyli: Dziecko A (45 poprawnych) jest na 70. centylu — lepsze od 70% dzieci. Dziecko C (52 poprawne) jest na 95. centylu.\n\n\n\n\n\n\n\nCo to jest Percentyl?\n\n\n\nPercentyl mówi ci, jaki procent osób uzyskał wynik gorszy niż ty.\nPrzykłady:\n\n\npercentyl = uzyskałeś wynik lepszy niż 50% osób (dokładnie średnia)\n\n\npercentyl = uzyskałeś wynik lepszy niż 70% osób (powyżej średniej)\n\n\npercentyl = uzyskałeś wynik lepszy niż 95% osób (znacznie powyżej średniej)\n\n\npercentyl = uzyskałeś wynik lepszy tylko niż 10% osób (poniżej średniej)\n\n\nJeśli jesteś na 80. centylu, oznacza to, że 80% osób uzyskało wynik gorszy niż ty, a 20% wynik lepszy.\n\n\n\nTransformacja matematyczna: Te percentyle są przekształcane na wyniki IQ tak, aby średnia wynosiła 100, a odchylenie standardowe 15. Dziecko A otrzymuje IQ 110, Dziecko C otrzymuje IQ 125.\n\nFundamentalny problem: Ten proces wymusza na wynikach kształt krzywej dzwonowej, nawet jeśli oryginalne wyniki surowe nie miały tego kształtu. Wyobraź sobie, że większość dzieci odpowiedziała poprawnie na 20-30 pytań lub 50-55 pytań, z niewieloma wynikami pomiędzy. Transformacja nadal wytworzyłaby gładką krzywą dzwonową wyników IQ, ukrywając tę lukę w faktycznych wynikach testowych.\n\n\n\n\n\n\nCo to jest Rozkład Dzwonowy (Normalny)?\n\n\n\nRozkład dzwonowy, zwany także rozkładem normalnym lub krzywą Gaussa, to symetryczny wzór, w którym:\n\nWiększość wartości skupia się wokół średniej (środek dzwonu)\nIm bardziej oddalamy się od średniej w obie strony, tym mniej wartości\nRozkład jest symetryczny — lewa i prawa strona są swoim lustrzanym odbiciem\n\nPomyśl o wzroście: Większość ludzi ma wzrost około średniej (170-180 cm), mniej osób jest bardzo niskich (150 cm) lub bardzo wysokich (200 cm), co tworzy kształt dzwonu.\nW przypadku IQ: Wyniki są przekształcane tak, aby większość osób uzyskiwała wyniki około 100 (szczyt dzwonu), mniej osób uzyskiwało 85 lub 115 (zbocza dzwonu), a bardzo niewiele osób uzyskiwało 70 lub 130 (ogony rozkładu).\nKluczowy problem: Rzeczywiste wyniki w teście mogą naturalnie nie podlegać temu wzorcowi, ale transformacja IQ wymusza go.\n\n\n\nhttps://en.wikipedia.org/wiki/File:IQ_curve.svg\n\n\n\n\n\n\n\n\n\n\nKluczowa Kwestia\n\n\n\nIQ 130 nie oznacza „dwukrotnie większej inteligencji” niż IQ 65. Punkty IQ wskazują jedynie pozycję osoby względem innych w próbie standaryzacyjnej, a nie absolutną ilość inteligencji.\n\n\n\n\nKompromis Metodologiczny\nW praktyce badawczej IQ jest często traktowane jako skala przedziałowa. Jest to pragmatyczny kompromis, który pozwala badaczom używać potężniejszych narzędzi statystycznych, ale wiąże się z ważnymi zastrzeżeniami.\n✅ Traktowanie IQ jako skali przedziałowej jest dopuszczalne, gdy:\n\nUżywamy standardowych testów statystycznych (korelacje, regresje, testy t) w badaniach eksploracyjnych lub aplikowanych\nPorównujemy grupy, które rozwiązywały ten sam test IQ, oceniony według tych samych norm (np. porównujemy 8-latki, które wszystkie rozwiązywały WISC-V z normami z 2024 roku — NIE porównujemy wyników WISC-V z wynikami Stanford-Binet, ani norm z 2024 z normami z 1990 roku)\nWyraźnie przyznajemy to ograniczenie w naszej interpretacji\nNasze wnioski koncentrują się na wzorcach, a nie na precyzyjnych różnicach numerycznych\n\n⚠️ Pamiętaj o ograniczeniach:\n\nTakie traktowanie działa lepiej dla wyników blisko średniej populacyjnej (IQ 85-115) niż na krańcach (IQ 70 lub IQ 145)\nZałożenie o równych odstępach jest przybliżeniem, nie rzeczywistością\nWyniki muszą być interpretowane z odpowiednią ostrożnością\n\n❌ Nigdy:\n\nNie twierdzić, że równe różnice IQ oznaczają równe różnice w zdolnościach (np. mówienie „różnica między IQ 100 a 115 reprezentuje taką samą różnicę poznawczą jak różnica między IQ 85 a 100”)\nNie używać stwierdzeń proporcjonalnych typu „dwukrotnie bardziej inteligentny” lub „o 50% mądrzejszy”\nNie mówić „Osoba A z IQ 130 jest o 15 punktów mądrzejsza niż Osoba B z IQ 115, a Osoba B jest o 15 punktów mądrzejsza niż Osoba C z IQ 100, więc wszystkie trzy różnice są równe”\nNie interpretować różnic IQ tak, jakby były pomiarami fizycznymi jak wzrost czy waga\n\n\n\n\nInne Miary Psychologiczne\nWiele szeroko stosowanych narzędzi psychologicznych ma porządkowy charakter podobny do IQ:\n\nSkale depresji (np. Inwentarz Depresji Becka): Wynik 20 nie oznacza „dwukrotnie bardziej depresyjny” niż wynik 10. Ktoś z wynikiem 30 może mieć ciężkie codzienne objawy, podczas gdy ktoś z wynikiem 15 może mieć łagodne sporadyczne objawy — ale różnica 15 punktów nie reprezentuje tej samej zmiany nasilenia depresji co różnica między wynikami 5 a 20.\nMiary lęku (np. Inwentarz Stanu i Cechy Lęku): Jeśli trzy osoby uzyskają wyniki 30, 45 i 60, znamy kolejność ich poziomów lęku, ale nie możemy powiedzieć, że psychologiczna odległość między pierwszą dwójką jest równa odległości między ostatnią dwójką.\nTesty osobowości (np. Inwentarz Wielkiej Piątki): Wynik 80 w Ekstrawersji nie oznacza „dwukrotnie bardziej ekstrawertyczny” niż wynik 40 — oznacza tylko większą ekstrawersję.\n\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\nTe miary często wykorzystują zsumowane pozycje typu Likerta lub inne metody punktacji, które nie gwarantują równych odstępów między wynikami, mimo że wyglądają jak liczby ciągłe.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#podsumowanie-2",
    "href": "rozdzial2.html#podsumowanie-2",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.6 Podsumowanie",
    "text": "4.6 Podsumowanie\nPorządkowy charakter miar behawioralnych stanowi zarówno wyzwanie koncepcyjne, jak i praktyczny dylemat. Podczas gdy te skale dostarczają cennych informacji o względnej pozycji i różnicach między grupami, nie wspierają precyzyjnych interpretacji ilościowych, jakich moglibyśmy sobie życzyć.\nKluczowy wniosek: Większość miar behawioralnych mówi nam o kolejności i względnej pozycji, a nie o absolutnych ilościach czy równych odstępach. To ich nie czyni mniej użytecznymi — to tylko oznacza, że musimy je interpretować odpowiednio i uczciwie komunikować ich ograniczenia naszym studentom, czytelnikom i odbiorcom badań.\nPraktyczne podsumowanie:\n\nWyższy wynik oznacza więcej danej cechy — to możemy powiedzieć pewnie\nGrupa A uzyskała wyższy wynik niż Grupa B — to jest ważny wniosek\nRóżnica między wynikami 100 a 115 jest dokładnie równa różnicy między 115 a 130 — tego nie możemy twierdzić\n\nUznając te ograniczenia i wybierając odpowiednie metody analityczne, możemy prowadzić rzetelne badania, zachowując przy tym integralność naukową.\n\n\n\n\n\n\n\nJak Dokładnie Działa Transformacja Percentyli na IQ (*)?\n\n\n\nTransformacja przebiega w dwóch krokach:\nKrok 1: Od percentyla do wyniku standardowego (z-score)\nPercentyl jest przekształcany na wynik standardowy, który mówi, ile odchyleń standardowych jesteś od średniej w rozkładzie normalnym.\nPrzykłady:\n\n\npercentyl (średnia) → z = 0\n\n\npercentyl → z = +1 (jedno odchylenie standardowe powyżej średniej)\n\n\npercentyl → z = -1 (jedno odchylenie standardowe poniżej średniej)\n\n\npercentyl → z = +2\n\n\npercentyl → z = -2\n\n\nCo to jest z-score i po co go używamy?\nWynik standardowy (z-score) to sposób na wyrażenie, gdzie znajduje się dana wartość względem reszty danych, używając wspólnej “miary odległości”.\nFormuła standaryzacji (z-score):\nz = \\frac{X - \\mu}{\\sigma}\nGdzie:\n\nX = surowy wynik (np. liczba poprawnych odpowiedzi)\n\\mu = średnia w grupie\n\\sigma = odchylenie standardowe w grupie\n\nSkąd się bierze ta formuła?\nProces standaryzacji składa się z dwóch operacji:\n\n(X - \\mu) — centrowanie: odejmujemy średnią od każdej wartości\n\nPo tej operacji nowa średnia = 0\nPrzykład: jeśli X = 45, \\mu = 40, to (45 - 40) = 5 punktów powyżej średniej\n\n/\\sigma — skalowanie: dzielimy przez odchylenie standardowe\n\nPo tej operacji nowe odchylenie standardowe = 1\nPrzykład: jeśli \\sigma = 10, to 5 punktów powyżej średniej = 5/10 = 0.5 odchylenia standardowego\n\n\nRezultat standaryzacji: Otrzymujemy zmienną, która ma \\mu = 0 i \\sigma = 1, niezależnie od tego, jakie były oryginalne wartości!\nIntuicyjny przykład:\nWyobraź sobie trzy testy:\n\nTest A: \\mu = 50 punktów, \\sigma = 10 punktów\nTest B: \\mu = 200 punktów, \\sigma = 40 punktów\n\nTest C: \\mu = 15 punktów, \\sigma = 3 punkty\n\nUczeń, który w każdym teście jest o jedno odchylenie standardowe powyżej średniej, otrzyma:\n\nTest A: 60 punktów → z = \\frac{60-50}{10} = +1\nTest B: 240 punktów → z = \\frac{240-200}{40} = +1\nTest C: 18 punktów → z = \\frac{18-15}{3} = +1\n\nSurowe wyniki są bardzo różne (60, 240, 18), ale z-score jest taki sam (+1), co oznacza tę samą relatywną pozycję w każdym teście. To właśnie pozwala nam porównywać wyniki z różnych testów!\nKrok 2: Od wyniku standardowego do IQ (de-standaryzacja)\nDe-standaryzacja to odwrócenie procesu standaryzacji. Jeśli standaryzacja przekształca dane do skali ze średnią 0 i odchyleniem standardowym 1, to de-standaryzacja pozwala nam wrócić do dowolnie wybranej średniej i odchylenia standardowego.\nFormuła de-standaryzacji:\nX = \\mu + \\sigma \\times z\nSkąd się bierze ten wzór?\nTo algebraiczne przekształcenie wzoru na z-score. Rozwiązujemy go względem X:\n\n\\begin{align}\nz &= \\frac{X - \\mu}{\\sigma} \\\\[0.5em]\nz \\times \\sigma &= X - \\mu \\quad \\text{(mnożymy obie strony przez } \\sigma \\text{)} \\\\[0.5em]\nX &= \\mu + \\sigma \\times z \\quad \\text{(dodajemy } \\mu \\text{ do obu stron)}\n\\end{align}\n\nCo oznacza ta formuła intuicyjnie?\n“Weź średnią (\\mu), i dodaj do niej odpowiednią liczbę odchyleń standardowych (\\sigma \\times z)”\n\nJeśli z = +1, dodajesz jedno odchylenie standardowe do średniej: X = \\mu + \\sigma\nJeśli z = -1, odejmujesz jedno odchylenie standardowe od średniej: X = \\mu - \\sigma\nJeśli z = 0, otrzymujesz dokładnie średnią: X = \\mu\n\nTransformacja IQ to de-standaryzacja z \\mu = 100 i \\sigma = 15:\n\\text{IQ} = 100 + 15 \\times z\nDlaczego akurat 100 i 15?\nTo arbitralna konwencja ustalona przez Davida Wechslera w latach 30. XX wieku dla skal WISC i WAIS. Wcześniejsze testy (jak Stanford-Binet) używały innych wartości (\\mu = 100, ale \\sigma = 16). Współczesne testy IQ stosują najczęściej konwencję \\mu = 100, \\sigma = 15, co ułatwia porównywanie wyników między różnymi testami.\nTeoretycznie moglibyśmy wybrać dowolne wartości (np. \\mu = 500, \\sigma = 100 jak w teście SAT), ale 100 i 15 stały się standardem w testach inteligencji.\nPorównanie: standaryzacja vs. de-standaryzacja (transformacja IQ)\n\n\n\n\n\n\n\n\n\n\nOperacja\nWzór\nŚrednia\nOdchylenie std.\nKierunek\n\n\n\n\nStandaryzacja\nz = \\frac{X - \\mu}{\\sigma}\n0\n1\nSurowe wyniki → z-score\n\n\nDe-standaryzacja (IQ)\n\\text{IQ} = 100 + 15 \\times z\n100\n15\nz-score → IQ\n\n\n\nLogika procesu:\n\nStandaryzacja tworzy “neutralną” skalę (\\mu = 0, \\sigma = 1) — wspólną “walutę”\nDe-standaryzacja przekształca tę neutralną skalę do wygodniejszych liczb (\\mu = 100, \\sigma = 15) — zmiana waluty według stałego kursu\n\nTo jak temperatura: Celsjusz → Kelvin (standaryzacja do “naturalnej” skali) → Fahrenheit (transformacja do innej skali).\nKonkretny przykład obliczeniowy:\nDziecko C jest na 95. centylu:\n\n\npercentyl w rozkładzie normalnym odpowiada z \\approx +1.645\n\n\\text{IQ} = 100 + 15 \\times 1.645 = 100 + 24.675 \\approx 125\n\nDziecko A jest na 70. centylu:\n\n\npercentyl odpowiada z \\approx +0.524\n\n\\text{IQ} = 100 + 15 \\times 0.524 = 100 + 7.86 \\approx 108\n\nKluczowa obserwacja:\nTen wzór można zastosować do dowolnych danych — nawet jeśli surowe wyniki nie mają rozkładu normalnego. To właśnie oznacza “wymuszanie” rozkładu normalnego — matematyka tworzy krzywą dzwonową niezależnie od rzeczywistego kształtu danych. Percentyle są przekształcane przez teoretyczny rozkład normalny, nie rzeczywisty rozkład surowych wyników.\nW R:\n\n# Standaryzacja zmiennej\nraw_scores &lt;- c(35, 42, 45, 38, 51)\nz_scores &lt;- scale(raw_scores)  # średnia = 0, SD = 1\n\n# De-standaryzacja: z powrotem do oryginalnych parametrów\nmean_original &lt;- mean(raw_scores)\nsd_original &lt;- sd(raw_scores)\nback_to_original &lt;- mean_original + sd_original * z_scores\n\n# Transformacja percentyli na IQ (de-standaryzacja z μ=100, σ=15)\npercentyle &lt;- c(0.70, 0.95)  # 70. i 95. percentyl\nz_from_percentile &lt;- qnorm(percentyle)  # Krok 1: percentyl → z\nIQ &lt;- 100 + 15 * z_from_percentile      # Krok 2: z → IQ (de-standaryzacja)\n# Wynik: 108, 125",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#ćwiczenie-identyfikacja-skal-pomiarowych",
    "href": "rozdzial2.html#ćwiczenie-identyfikacja-skal-pomiarowych",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.7 Ćwiczenie: Identyfikacja Skal Pomiarowych",
    "text": "4.7 Ćwiczenie: Identyfikacja Skal Pomiarowych\nDla każdej z poniższych zmiennych określ najbardziej odpowiednią skalę pomiaru (Nominalna, Porządkowa, Przedziałowa lub Ilorazowa). Oceń również, czy zmienna jest dyskretna czy ciągła.\n\nPłeć\nSatysfakcja klienta: Zła, Słaba, Dobra, Doskonała\nWzrost (kwestionariusz): “Jestem: bardzo niski/a, niski/a, średniego wzrostu, wysoki/a, bardzo wysoki/a”\nWzrost (cale)\nCzas reakcji (milisekundy)\nKody pocztowe: np. 90-001, 00-950, 31-011, 80-309\nWiek (lata)\nNarodowość\nAdresy ulic\nStopnie wojskowe\nUmiejscowienie polityczne na skali lewica-prawica (skala 0-10)\nLiczebność rodziny: 1 dziecko, 2 dzieci, 3 dzieci, …\nWynik testu IQ\nRozmiar koszulki (S, M, L, XL, …)\nOceny filmów (1 gwiazdka, 2 gwiazdki, 3 gwiazdki, 4 gwiazdki, 5 gwiazdek)\nTemperatura (Celsjusz)\nTemperatura (Kelwin)\nGrupy krwi: A, B, AB, 0\nKategorie dochodów: niski, średni, wysoki\nFrekwencja wyborcza (procent)\nPrzynależność partyjna\nWielkość okręgu wyborczego (liczba mandatów)\nWykształcenie: Brak dyplomu, Średnie, Licencjat, Magisterium, Doktorat\nLiczba głosów otrzymanych przez kandydata\nRok urodzenia (np. 1985, 1990, 2000)\nCzas ukończenia maratonu (godziny:minuty:sekundy)\nOdpowiedź na skali Likerta: Zdecydowanie się nie zgadzam, Nie zgadzam się, Neutralny/a, Zgadzam się, Zdecydowanie się zgadzam\nKlasa społeczna: Klasa robotnicza, Klasa średnia, Klasa wyższa\nLiczba partii politycznych w parlamencie\nOdległość od domu do lokalu wyborczego (kilometry)\n\nPamiętaj, aby uzasadnić swoje wybory dla każdej zmiennej. Zwróć szczególną uwagę na przypadki niejednoznaczne, w których badacze mogą mieć różne zdania co do odpowiedniej skali.\n\nDla przykładu: W typologii skal pomiarowych Stevensa, adresy uliczne są danymi nominalnymi. Dlaczego?\nPełnią wyłącznie funkcję etykiet/identyfikatorów Nie mają naturalnego uporządkowania (ul. Mickiewicza 5 nie jest “większa” niż ul. Słowackiego 10) Nie można wykonywać na nich sensownych operacji matematycznych Jedyna dozwolona operacja to sprawdzanie równości/nierówności (czy to ten sam adres czy inny?)\nMimo że numery domów są liczbami, w systemie adresowym funkcjonują jako etykiety, a nie wartości ilościowe. Liczba 100 w adresie “ul. Kilińskiego 100” nie jest używana matematycznie - równie dobrze mogłaby to być “ul. Jabłkowa” czy “ul. Zeusa”, jeśli chodzi o jej funkcję w adresie.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#appendix-a",
    "href": "rozdzial2.html#appendix-a",
    "title": "4  Typy Danych w Naukach Społecznych",
    "section": "4.8 Appendix A",
    "text": "4.8 Appendix A\n\n\n\n\n\n\nDlaczego dodawanie/odejmowanie działa dla danych interwałowych, ale mnożenie/dzielenie wymaga danych ilorazowych\n\n\n\nSkale interwałowe mają równe odstępy między wartościami, ale arbitralny punkt zerowy (np. temperatura w Celsjuszach, daty kalendarzowe). Skale ilorazowe mają zarówno równe odstępy, jak i bezwzględne zero (np. temperatura w Kelwinach, wzrost, masa).\n\nDodawanie i odejmowanie: prawidłowe dla skal interwałowych\nRóżnice zachowują spójne proporcjonalne relacje przy transformacjach liniowych. Przy konwersji między skalami za pomocą y = a + bx gdzie b &gt; 0, stała addytywna a się redukuje:\n(y_2 - y_1) = (a + bx_2) - (a + bx_1) = b(x_2 - x_1)\nPrzykład: Przy konwersji z Celsjusza na Fahrenheita gdzie F = 1.8C + 32:\n\nKażda różnica 10°C zawsze konwertuje się na 18°F: 18 = 1.8 \\times 10\nSprawdźmy (20°C - 10°C) \\rightarrow (68°F - 50°F) = 18°F\nSprawdźmy (100°C - 90°C) \\rightarrow (212°F - 194°F) = 18°F\n\nRelacja jest spójna: różnica 10 stopni Celsjusza zawsze odpowiada różnicy 18 stopni Fahrenheita, niezależnie od tego, gdzie na skali mierzymy.\n\n\nMnożenie i dzielenie: wymagają skal ilorazowych\nIlorazy są niespójne, gdy punkt zerowy jest arbitralny. Stała addytywna a NIE redukuje się w ilorazach:\n\\frac{y_2}{y_1} = \\frac{a + bx_2}{a + bx_1} \\neq b \\cdot \\frac{x_2}{x_1}\nJeśli a \\neq 0 (brak bezwzględnego zera), ilorazy zmieniają się nieprzewidywalnie w zależności od porównywanych wartości.\nPrzykład: Ilorazy temperatur dają niespójne wyniki:\n\nCzy 20°C jest “dwa razy cieplej” niż 10°C?\n\nCelsjusz: 20/10 = 2.0\nFahrenheit: 68/50 = 1.36\nKelwin: 293.15/283.15 = 1.035\n\nA co z 100°C vs. 90°C?\n\nCelsjusz: 100/90 = 1.11\nFahrenheit: 212/194 = 1.09\n\n\nIlorazy różnią się w zależności zarówno od skali, JAK I od porównywanych temperatur. Tylko przy bezwzględnym zerze ilorazy mają spójne fizyczne znaczenie.\n\n\nImplikacje dla miar statystycznych\nŚrednia arytmetyczna jest prawidłowa dla skal interwałowych, ponieważ wykorzystuje dodawanie:\n\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\nPrzy transformacji do skali y, średnia transformuje się spójnie: \\bar{y} = a + b\\bar{x}\nŚrednia geometryczna wymaga skal ilorazowych, ponieważ wykorzystuje mnożenie:\n\\text{SG} = \\sqrt[n]{x_1 \\times x_2 \\times \\cdots \\times x_n}\nŚrednia geometryczna temperatur w Celsjuszach daje inny wynik niż średnia geometryczna tych samych temperatur w Fahrenheitach (po konwersji). To czyni średnią geometryczną bezsensowną dla danych interwałowych.\nPrzykład: Dla temperatur 10°C i 20°C:\n\nŚrednia geometryczna w Celsjuszach: \\sqrt{10 \\times 20} = 14.14°C \\rightarrow 57.45°F\nŚrednia geometryczna w Fahrenheitach: \\sqrt{50 \\times 68} = 58.31°F \\rightarrow 14.62°C\n\nTe wartości się nie zgadzają! Średnia geometryczna zależy od arbitralnego punktu zerowego.\n\n\nWariancja i odchylenie standardowe: prawidłowe dla skal interwałowych\nWariancja i odchylenie standardowe są dopuszczalne dla danych interwałowych, ponieważ operują na odchyleniach od średniej, które są różnicami. Co kluczowe, wariancja jest niezmienna względem translacji: dodanie stałej do wszystkich wartości nie zmienia wariancji, ponieważ odchylenia pozostają takie same.\nPrzy transformacji liniowej y = a + bx, wariancja transformuje się przewidywalnie:\n\\text{Var}(y) = b^2 \\text{Var}(x)\nStała a redukuje się przy obliczaniu odchyleń, tak jak przy prostych różnicach.\nPrzykład dowodu: Dla temperatur 10°C i 20°C:\nW Celsjuszach:\n\nŚrednia: \\bar{x} = (10 + 20)/2 = 15°C\nOdchylenia: (10 - 15) = -5, (20 - 15) = 5\nWariancja: \\text{Var}(X) = [(-5)^2 + (5)^2]/2 = 50/2 = 25°C^2\nOdchylenie standardowe: \\text{SD}(X) = 5°C\n\nW Fahrenheitach:\n\nKonwersja: 10°C → 50°F, 20°C → 68°F\nŚrednia: \\bar{y} = (50 + 68)/2 = 59°F\nOdchylenia: (50 - 59) = -9, (68 - 59) = 9\nWariancja: \\text{Var}(Y) = [(-9)^2 + (9)^2]/2 = 162/2 = 81°F^2\nOdchylenie standardowe: \\text{SD}(Y) = 9°F\n\nSprawdzenie transformacji:\n\nNachylenie konwersji: b = 1.8 (z F = 1.8C + 32)\nPrzewidywana wariancja: 1.8^2 \\times 25 = 3.24 \\times 25 = 81°F^2 ✓\nPrzewidywane odchylenie: 1.8 \\times 5 = 9°F ✓\n\nIdealne dopasowanie! Wariancja i odchylenie standardowe transformują się spójnie i przewidywalnie, czyniąc je prawidłowymi miarami rozproszenia dla danych interwałowych.\nDemonstracja niezmienności względem translacji: Jeśli przesuniemy wszystkie temperatury o +100°C (dodając 110°C i 120°C):\n\nNowa średnia: (110 + 120)/2 = 115°C\nNowe odchylenia: (110 - 115) = -5, (120 - 115) = 5\nNowa wariancja: \\text{Var}(X) = [(-5)^2 + (5)^2]/2 = 25°C^2 (niezmieniona!)\n\nWariancja pozostaje 25°C², ponieważ rozproszenie się nie zmieniło, zmienił się tylko punkt odniesienia.\nKluczowa zasada: Operacje oparte na dodawaniu/odejmowaniu i różnicach działają dla skal interwałowych, ponieważ arbitralna stała a się redukuje. Operacje obejmujące mnożenie/dzielenie lub ilorazy wymagają skal ilorazowych, ponieważ a zniekształca wyniki. Wariancja i odchylenie standardowe działają, ponieważ są niezmienne względem translacji i oparte na odchyleniach od średniej (różnicach), a nie ilorazach.\n\n\nPodsumowanie: prawidłowe miary statystyczne według skali pomiarowej\n\n\n\n\n\n\n\n\n\n\nMiary statystyczne\nNominalna\nPorządkowa\nInterwałowa\nIlorazowa\n\n\n\n\nDominanta\n✓\n✓\n✓\n✓\n\n\nMediana\n✗\n✓\n✓\n✓\n\n\nŚrednia arytmetyczna\n✗\n✗\n✓\n✓\n\n\nŚrednia geometryczna\n✗\n✗\n✗\n✓\n\n\nWariancja i odchylenie stand.\n✗\n✗\n✓\n✓\n\n\nKowariancja\n✗\n✗\n✓\n✓\n\n\nKorelacja Pearsona\n✗\n✗\n✓\n✓\n\n\nKorelacja Spearmana\n✗\n✓\n✓\n✓\n\n\nWspółczynnik zmienności\n✗\n✗\n✗\n✓\n\n\n\nUwagi:\n\nSkale nominalne (np. kolory, kategorie) obsługują tylko miary oparte na częstościach, jak dominanta\nSkale porządkowe (np. rankingi, skale Likerta) dodają medianę i korelacje oparte na rangach\nSkale interwałowe (np. Celsjusz, daty kalendarzowe) obsługują wszystkie miary oparte na dodawaniu/odejmowaniu\nSkale ilorazowe (np. wzrost, masa, Kelwin) dodatkowo obsługują miary wymagające mnożenia/dzielenia i sensownych ilorazów\nWspółczynnik zmienności (\\text{CV} = \\text{SD}/\\text{Średnia}) wymaga sensownego punktu zerowego, więc tylko skale ilorazowe",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Typy Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "5.1 Introduction\nUnivariate descriptive statistics help us summarize and understand the characteristics of a single variable. When we collect data, we often have many observations that are difficult to interpret without some form of summary. Descriptive statistics allow us to:\nIn this chapter, we will explore the fundamental tools for describing numerical data, starting with basic notation and progressing through measures of central tendency, dispersion, and visualization techniques.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction",
    "href": "chapter5.html#introduction",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "Identify the “typical” or “central” value in our data\nUnderstand how spread out or variable the data are\nDetect unusual observations (outliers)\nCommunicate findings efficiently to others",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#overview-four-types-of-descriptive-measures",
    "href": "chapter5.html#overview-four-types-of-descriptive-measures",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.2 Overview: Four Types of Descriptive Measures",
    "text": "5.2 Overview: Four Types of Descriptive Measures\nUnivariate descriptive statistics can be organized into four main categories, each addressing a different question about our data:\n\n1. Measures of Central Tendency\nQuestion: What is the “typical” or “average” value?\nCommon measures:\n\nMean (arithmetic average)\nMedian (middle value)\nMode (most frequent value)\n\nPurpose: These measures help us identify where the “center” of the data is located. They provide a single value that represents the entire dataset.\n\n\n2. Measures of Variability (Dispersion)\nQuestion: How spread out are the data? How much do observations differ from each other?\nCommon measures:\n\nRange (max - min)\nVariance (average squared deviation from mean)\nStandard deviation (square root of variance)\nInterquartile range (IQR - spread of middle 50%)\n\nPurpose: These measures tell us whether observations cluster tightly around the center or are widely dispersed. Two datasets can have the same mean but very different variability.\n\n\n3. Measures of Relative Position (Standing)\nQuestion: Where does a particular observation stand relative to others? What proportion of data falls below a given value?\nCommon measures:\n\nQuantiles (general concept)\nPercentiles (divide data into 100 parts)\nQuartiles (divide data into 4 parts)\nStandardized scores (z-scores)\n\nPurpose: These measures help us understand the position of individual observations within the overall distribution. They answer questions like “How does this student’s score compare to others?”\n\n\n4. Measures of Shape\nQuestion: What is the overall form or pattern of the distribution?\nCommon measures:\n\nSkewness (asymmetry - is there a long tail on one side?)\nKurtosis (tailedness - are there many extreme values?)\nModality (number of peaks - unimodal, bimodal, multimodal)\n\nPurpose: These measures describe the overall pattern of the distribution. While we won’t calculate these numerically in this course, we will identify shape characteristics visually using histograms and boxplots.\nRelationship between categories:\nThese four types of measures complement each other. A complete description of univariate data typically includes:\n\nAt least one measure of central tendency (mean or median)\nAt least one measure of variability (standard deviation or IQR)\nVisual displays (histogram, boxplot) that reveal shape\nInformation about relative position when comparing specific observations\n\nIn the sections that follow, we will explore each of these categories in detail, starting with the mathematical notation needed to express these concepts precisely.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measurement-scales-and-appropriate-statistics",
    "href": "chapter5.html#measurement-scales-and-appropriate-statistics",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.3 Measurement Scales and Appropriate Statistics",
    "text": "5.3 Measurement Scales and Appropriate Statistics\nBefore diving into specific statistical measures, it’s important to understand that not all statistics are appropriate for all types of data. The type of measurement scale determines which statistical measures we can meaningfully use.\n\nThe Four Measurement Scales\n1. Nominal Scale\nCategories with no inherent order (e.g., gender, country, political party affiliation, eye color).\nProperties: Categories are different, but we cannot say one is “greater than” another.\n2. Ordinal Scale\nCategories with a meaningful order, but intervals between categories are not necessarily equal (e.g., education level: elementary/high school/bachelor’s/master’s/PhD; survey responses: strongly disagree/disagree/neutral/agree/strongly agree).\nProperties: We can rank observations, but we cannot quantify the difference between ranks.\n3. Interval Scale\nNumeric scale with equal intervals, but no true zero point (e.g., temperature in Celsius or Fahrenheit, calendar years).\nProperties: Differences are meaningful (20°C to 30°C is the same change as 30°C to 40°C), but ratios are not (20°C is not “twice as hot” as 10°C).\n4. Ratio Scale\nNumeric scale with equal intervals AND a true zero point (e.g., height, weight, income, age, distance).\nProperties: Both differences and ratios are meaningful (20 kg is twice as heavy as 10 kg).\n\n\nWhich Statistics for Which Scales?\n\n\n\nMeasure\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nMode\n✓\n✓\n✓\n✓\n\n\nMedian\n✗\n✓\n✓\n✓\n\n\nMean\n✗\n✗\n✓\n✓\n\n\nRange\n✗\n✗\n✓\n✓\n\n\nVariance/Std. Dev.\n✗\n✗\n✓\n✓\n\n\nIQR\n✗\n✓\n✓\n✓\n\n\nQuantiles/Percentiles\n✗\n✓\n✓\n✓\n\n\n\nKey insights:\n\nNominal data: Only the mode makes sense. We can count frequencies, but we cannot calculate means or medians.\nOrdinal data: The median and IQR are appropriate because they only require ordering. The mean is not appropriate because it assumes equal intervals.\nInterval/Ratio data: All measures are appropriate. These are the most flexible measurement scales.\n\nExample illustrating why scale matters:\nConsider education levels: elementary (1), high school (2), bachelor’s (3), master’s (4), PhD (5).\n\nMode: Valid - “Most respondents have a bachelor’s degree”\nMedian: Valid - “The median education level is bachelor’s degree”\nMean: Problematic - “The mean education level is 3.2” is hard to interpret because the intervals between levels are not equal (the difference between elementary and high school is not the same as the difference between master’s and PhD)\n\nThroughout this chapter, we will focus primarily on interval and ratio data, where all statistical measures are appropriate. However, keep these distinctions in mind when working with different types of data in practice.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#sigma-notation",
    "href": "chapter5.html#sigma-notation",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.4 Sigma Notation",
    "text": "5.4 Sigma Notation\nBefore we dive into statistical measures, we need to understand sigma notation, which provides a compact way to express the sum of many values.\nThe Greek letter \\Sigma (capital sigma) means “sum.” When we write:\n\\sum_{i=1}^{n} x_i\nThis means: “Add up all values of x from the first observation (i=1) to the last observation (i=n).”\nBreaking down the notation:\n\n\\Sigma = the summation operator (“add these up”)\ni=1 (below \\Sigma) = start with the first observation\nn (above \\Sigma) = continue until the n-th observation\nx_i = the value of variable x for observation i\n\nExample:\nSuppose we have five observations: x_1 = 2, x_2 = 5, x_3 = 3, x_4 = 8, x_5 = 7\nThen:\n\\sum_{i=1}^{5} x_i = x_1 + x_2 + x_3 + x_4 + x_5 = 2 + 5 + 3 + 8 + 7 = 25\nUseful sigma notation properties:\n\\sum_{i=1}^{n} (x_i + y_i) = \\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} y_i\n\\sum_{i=1}^{n} c \\cdot x_i = c \\sum_{i=1}^{n} x_i \\text{ (where } c \\text{ is a constant)}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#data-distribution-and-frequency-distribution",
    "href": "chapter5.html#data-distribution-and-frequency-distribution",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.5 Data Distribution and Frequency Distribution",
    "text": "5.5 Data Distribution and Frequency Distribution\n\nWhat is a Data Distribution?\nA data distribution refers to how values in a dataset are spread across the possible range of values. Understanding the distribution helps us see patterns, identify typical values, and detect unusual observations.\nWhen we talk about a distribution, we are asking: “What values occur in our data, and how often does each value (or range of values) appear?”\n\n\nFrequency Distribution\nA frequency distribution organizes data by showing how many times each value (or range of values) occurs. This can be presented as:\n\nA table showing values and their counts\nA relative frequency distribution (showing proportions or percentages)\nA cumulative frequency distribution (showing running totals)\n\nExample:\nSuppose we surveyed 20 students about how many books they read last month:\n\nbooks &lt;- c(2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8, 8, 8, 9, 10)\n\n# Create frequency table\nfreq_table &lt;- as.data.frame(table(books))\nnames(freq_table) &lt;- c(\"Books_Read\", \"Frequency\")\nfreq_table$Relative_Frequency &lt;- freq_table$Frequency / sum(freq_table$Frequency)\nfreq_table$Cumulative_Frequency &lt;- cumsum(freq_table$Frequency)\n\nkable(freq_table, \n      col.names = c(\"Books Read\", \"Frequency\", \"Relative Frequency\", \"Cumulative Frequency\"),\n      caption = \"Frequency Distribution of Books Read\")\n\n\nFrequency Distribution of Books Read\n\n\nBooks Read\nFrequency\nRelative Frequency\nCumulative Frequency\n\n\n\n\n2\n1\n0.05\n1\n\n\n3\n2\n0.10\n3\n\n\n4\n3\n0.15\n6\n\n\n5\n4\n0.20\n10\n\n\n6\n3\n0.15\n13\n\n\n7\n2\n0.10\n15\n\n\n8\n3\n0.15\n18\n\n\n9\n1\n0.05\n19\n\n\n10\n1\n0.05\n20\n\n\n\n\n\nInterpretation:\n\nFrequency: The count of students who read that number of books\nRelative Frequency: The proportion of all students (e.g., 0.20 = 20% read 5 books)\nCumulative Frequency: The running total (e.g., 10 students read 5 or fewer books)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-central-tendency-1",
    "href": "chapter5.html#measures-of-central-tendency-1",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.6 Measures of Central Tendency",
    "text": "5.6 Measures of Central Tendency\nMeasures of central tendency tell us about the “typical” or “central” value in a dataset. The three most common measures are the mean, median, and mode.\nApplicable measurement scales:\n\nMode: Nominal, Ordinal, Interval, Ratio\nMedian: Ordinal, Interval, Ratio (requires ordering)\nMean: Interval, Ratio (requires equal intervals and arithmetic operations)\n\n\nMean (Arithmetic Average)\nThe mean is the sum of all values divided by the number of observations. It represents the arithmetic average.\nApplicable to: Interval and Ratio data only\nFormula:\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\nwhere \\bar{x} (pronounced “x-bar”) is the sample mean and n is the number of observations.\nExample:\nIf we have test scores: 75, 82, 88, 90, 95\n\\bar{x} = \\frac{75 + 82 + 88 + 90 + 95}{5} = \\frac{430}{5} = 86\nPlain English: The mean is what we get if we could redistribute all the values equally among all observations. If five students scored a total of 430 points, they would each get 86 points if we divided the total evenly.\n\n\nThe Mean as a Balancing Point\nOne of the most important conceptual ways to understand the mean is as a balancing point or center of gravity of the data.\nImagine placing weights on a seesaw at positions corresponding to your data values. The mean is the point where the seesaw would balance perfectly.\nMathematical insight:\nThe sum of deviations from the mean always equals zero:\n\\sum_{i=1}^{n} (x_i - \\bar{x}) = 0\nThis means the positive deviations (values above the mean) exactly cancel out the negative deviations (values below the mean).\nVisualization:\n\n# Example data\ndata_points &lt;- c(2, 4, 5, 8, 11)\nmean_val &lt;- mean(data_points)\n\n# Set up plotting area\npar(mar = c(4, 2, 3, 2))\nplot(c(0, 13), c(0, 2.5), type = \"n\", xlab = \"Value\", ylab = \"\",\n     main = \"The Mean as a Balancing Point\", yaxt = \"n\", bty = \"n\")\n\n# Draw the seesaw (fulcrum at the mean)\nsegments(0, 1, 13, 1, lwd = 4, col = \"brown\")\npolygon(c(mean_val - 0.3, mean_val + 0.3, mean_val), \n        c(0.7, 0.7, 0.3), \n        col = \"gray30\", border = \"black\", lwd = 2)\n\n# Plot data points as weights on the seesaw\npoints(data_points, rep(1, length(data_points)), \n       pch = 19, cex = 3, col = \"darkblue\")\n\n# Add vertical lines showing distances from mean\nfor(i in 1:length(data_points)) {\n  segments(data_points[i], 1, mean_val, 1, \n           lty = 2, col = \"red\", lwd = 1.5)\n}\n\n# Mark the mean\npoints(mean_val, 1, pch = 17, cex = 4, col = \"red\")\ntext(mean_val, 0.4, paste(\"Mean =\", round(mean_val, 1)), \n     col = \"red\", cex = 1.5, font = 2)\n\n# Add value labels\ntext(data_points, rep(1.4, length(data_points)), \n     as.character(data_points), cex = 1.2, font = 2)\n\n# Simplified summary below\ndeviations &lt;- data_points - mean_val\ntext(6.5, 2.2, \n     sprintf(\"Sum of deviations from mean = %.1f\", sum(deviations)),\n     cex = 1.1)\n\n\n\n\n\n\n\n\nInterpretation: The mean (6) acts as a fulcrum. The data point at 2 is 4 units below the mean, while the point at 11 is 5 units above. The negative and positive deviations balance out, making the mean the perfect balance point.\nWhy this matters: This property explains why the mean is sensitive to extreme values (outliers). A single very large or very small value can “pull” the mean toward it, just as a heavy weight far from the fulcrum would tip a seesaw.\n\n\nMedian\nThe median is the middle value when data are arranged in order. It divides the dataset into two equal halves.\nApplicable to: Ordinal, Interval, and Ratio data (requires ordering)\nHow to find the median:\n\nSort the data from smallest to largest\nIf n is odd: the median is the middle value at position \\frac{n+1}{2}\nIf n is even: the median is the average of the two middle values at positions \\frac{n}{2} and \\frac{n}{2} + 1\n\nExample with odd n:\nData: 3, 7, 8, 12, 15 (already sorted, n = 5)\nPosition of median: \\frac{5+1}{2} = 3\nMedian = 8 (the third value)\nExample with even n:\nData: 3, 7, 8, 12, 15, 20 (already sorted, n = 6)\nThe two middle values are at positions 3 and 4: 8 and 12\n\\text{Median} = \\frac{8 + 12}{2} = 10\nPlain English: The median is the value that sits in the middle when we line up all observations from smallest to largest. Half the observations are smaller than the median, and half are larger.\nKey advantage: The median is resistant to outliers. Extreme values don’t affect it much, making it useful when data contain outliers.\n\n\nMode\nThe mode is the value that appears most frequently in the dataset.\nApplicable to: Nominal, Ordinal, Interval, and Ratio data (all scales)\nProperties:\n\nA dataset can have no mode (all values appear equally often)\nA dataset can have one mode (unimodal)\nA dataset can have multiple modes (bimodal, multimodal)\n\nExample:\nData: 2, 3, 3, 5, 5, 5, 7, 8, 8\nMode = 5 (appears three times, more than any other value)\nPlain English: The mode tells us which value is most common or typical in the dataset. It’s particularly useful for categorical data (e.g., “blue” is the most common eye color in the sample).\n\n\nComparing Mean, Median, and Mode\n\n# Generate distributions\nset.seed(42)\nleft_skew &lt;- rbeta(2000, 8, 2) * 50 + 30\nsymmetric &lt;- rnorm(2000, 55, 8)\nright_skew &lt;- rbeta(2000, 2, 8) * 50 + 30\n\n# Function to estimate mode using density\nget_mode &lt;- function(x) {\n  d &lt;- density(x)\n  d$x[which.max(d$y)]\n}\n\n# Vertical layout\npar(mfrow = c(3, 1), mar = c(4, 4, 3, 1))\n\n# Left-skewed\nhist(left_skew, main = \"Left-Skewed Distribution\", \n     xlab = \"Value\", col = \"lightblue\", breaks = 30, \n     xlim = c(25, 85))\nabline(v = mean(left_skew), col = \"red\", lwd = 2.5, lty = 1)\nabline(v = median(left_skew), col = \"blue\", lwd = 2.5, lty = 2)\nabline(v = get_mode(left_skew), col = \"darkgreen\", lwd = 2.5, lty = 3)\nlegend(\"topleft\", \n       legend = c(sprintf(\"Mean (%.1f)\", mean(left_skew)),\n                  sprintf(\"Median (%.1f)\", median(left_skew)),\n                  sprintf(\"Mode (%.1f)\", get_mode(left_skew))),\n       col = c(\"red\", \"blue\", \"darkgreen\"), \n       lty = c(1, 2, 3), lwd = 2.5, cex = 0.9)\n\n# Symmetric\nhist(symmetric, main = \"Symmetric Distribution\", \n     xlab = \"Value\", col = \"lightgreen\", breaks = 30,\n     xlim = c(25, 85))\nabline(v = mean(symmetric), col = \"red\", lwd = 2.5, lty = 1)\nabline(v = median(symmetric), col = \"blue\", lwd = 2.5, lty = 2)\nabline(v = get_mode(symmetric), col = \"darkgreen\", lwd = 2.5, lty = 3)\nlegend(\"topleft\", \n       legend = c(sprintf(\"Mean (%.1f)\", mean(symmetric)),\n                  sprintf(\"Median (%.1f)\", median(symmetric)),\n                  sprintf(\"Mode (%.1f)\", get_mode(symmetric))),\n       col = c(\"red\", \"blue\", \"darkgreen\"), \n       lty = c(1, 2, 3), lwd = 2.5, cex = 0.9)\n\n# Right-skewed\nhist(right_skew, main = \"Right-Skewed Distribution\", \n     xlab = \"Value\", col = \"lightyellow\", breaks = 30,\n     xlim = c(25, 85))\nabline(v = mean(right_skew), col = \"red\", lwd = 2.5, lty = 1)\nabline(v = median(right_skew), col = \"blue\", lwd = 2.5, lty = 2)\nabline(v = get_mode(right_skew), col = \"darkgreen\", lwd = 2.5, lty = 3)\nlegend(\"topright\", \n       legend = c(sprintf(\"Mean (%.1f)\", mean(right_skew)),\n                  sprintf(\"Median (%.1f)\", median(right_skew)),\n                  sprintf(\"Mode (%.1f)\", get_mode(right_skew))),\n       col = c(\"red\", \"blue\", \"darkgreen\"), \n       lty = c(1, 2, 3), lwd = 2.5, cex = 0.9)\n\n\n\n\n\n\n\n\nKey insights:\n\nIn symmetric distributions, mean ≈ median ≈ mode\nIn right-skewed distributions (long tail to the right), mean &gt; median &gt; mode\nIn left-skewed distributions (long tail to the left), mean &lt; median &lt; mode\n\nThe mean is pulled toward the tail, the median stays in the middle, and the mode remains at the peak.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-dispersion-variability",
    "href": "chapter5.html#measures-of-dispersion-variability",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.7 Measures of Dispersion (Variability)",
    "text": "5.7 Measures of Dispersion (Variability)\nWhile measures of central tendency tell us about typical values, measures of dispersion tell us how spread out the data are. Two datasets can have the same mean but very different spreads.\nApplicable measurement scales:\n\nRange: Interval, Ratio\nVariance/Standard Deviation: Interval, Ratio\nIQR: Ordinal, Interval, Ratio (requires ordering)\n\n\nRange\nThe range is the simplest measure of dispersion: the difference between the maximum and minimum values.\nApplicable to: Interval and Ratio data\n\\text{Range} = \\max(x) - \\min(x)\nExample:\nDataset A: 10, 12, 15, 18, 20 → Range = 20 - 10 = 10\nDataset B: 10, 15, 15, 15, 20 → Range = 20 - 10 = 10\nPlain English: The range tells us the span from the smallest to the largest observation. It gives a quick sense of spread but is highly sensitive to outliers (since it only uses two values).\n\n\nVisualizing Variability: A Working Example\nTo understand variance and standard deviation, let’s work through a complete example with data X = (2, 2, 3, 4, 5, 5). We’ll treat these as sequential observations (like measurements over time) to visualize how individual values deviate from their mean.\n\nStep 1: Calculate the Mean\n\\bar{x} = \\frac{2 + 2 + 3 + 4 + 5 + 5}{6} = \\frac{21}{6} = 3.5\n\n\nStep 2: Visualize Deviations from the Mean\n\nX &lt;- c(2, 2, 3, 4, 5, 5)\nmean_X &lt;- mean(X)\n\nplot(X, type = \"b\", pch = 19, col = \"darkblue\", \n     xlab = \"Observation\", ylab = \"Value\",\n     main = \"Deviations from the Mean\",\n     ylim = c(0, 6))\nabline(h = mean_X, col = \"red\", lwd = 2, lty = 2)\ntext(1, mean_X + 0.3, paste(\"Mean =\", mean_X), col = \"red\", pos = 4)\n\n# Add deviation lines\nfor(i in 1:length(X)) {\n  segments(i, mean_X, i, X[i], col = \"gray\", lty = 3)\n}\n\n\n\n\n\n\n\n\nThe gray dashed lines show the deviations from the mean. Variance will quantify the average squared length of these deviations.\n\n\n\nWhy Do We Square Deviations?\nThis is one of the most important conceptual questions in statistics. Why not just average the deviations (x_i - \\bar{x}) directly?\nProblem: If we simply averaged the deviations, they would always sum to zero (as we saw with the balancing point property). Let’s verify this with our example:\n\n\n\nx_i\nx_i - \\bar{x}\n\n\n\n\n2\n-1.5\n\n\n2\n-1.5\n\n\n3\n-0.5\n\n\n4\n0.5\n\n\n5\n1.5\n\n\n5\n1.5\n\n\nSum\n0.0\n\n\n\nThis happens because positive deviations (above the mean) exactly cancel negative deviations (below the mean):\n\\sum_{i=1}^{n}(x_i - \\bar{x}) = 0\nSolutions to this problem:\n\nSquare the deviations (variance approach): (x_i - \\bar{x})^2 makes all values positive\nTake absolute values (alternative approach): |x_i - \\bar{x}| also makes all values positive\n\nWhy squaring is preferred:\n\nMathematical convenience: Squaring has better mathematical properties for statistical theory\nEmphasizes larger deviations: Squaring gives more weight to extreme deviations (a point 10 units away contributes 100 to variance, while a point 5 units away contributes only 25)\n\nThe units problem: Because we squared the deviations, variance is in squared units. If our data are in centimeters, variance is in square centimeters. This is why we often prefer the standard deviation.\n\n\nVariance\nThe variance measures the average squared deviation from the mean. It quantifies how far, on average, each observation is from the mean.\nApplicable to: Interval and Ratio data\n\nManual Calculation: Population vs. Sample Variance\nLet’s continue with our example X = (2, 2, 3, 4, 5, 5) and calculate the squared deviations:\n\n\n\nx_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\n\n\n\n\n2\n-1.5\n2.25\n\n\n2\n-1.5\n2.25\n\n\n3\n-0.5\n0.25\n\n\n4\n0.5\n0.25\n\n\n5\n1.5\n2.25\n\n\n5\n1.5\n2.25\n\n\nSum\n\n9.5\n\n\n\nPopulation variance (if this were the entire population):\n\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n} = \\frac{9.5}{6} \\approx 1.583\n\n# Population variance (no Bessel correction)\nvar_pop &lt;- sum((X - mean_X)^2) / length(X)\ncat(\"Population variance:\", var_pop, \"\\n\")\n\nPopulation variance: 1.583 \n\n\n\n\nBessel’s Correction: Why n-1?\nThe Problem: When we use sample data to estimate population variance, the formula \\frac{\\sum(x_i - \\bar{x})^2}{n} tends to underestimate the true population variance. This happens because \\bar{x} is calculated from the same data, making the deviations artificially smaller (we’re measuring deviations from the sample mean, not the true population mean).\nThe Solution: Bessel’s correction adjusts for this bias by dividing by n-1 instead of n. This produces an unbiased estimate of the population variance.\nSample variance formula:\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\nFor our example:\ns^2 = \\frac{9.5}{5} = 1.9\n\n# Sample variance (with Bessel correction)\nvar_sample &lt;- var(X)  # R uses Bessel correction by default\ncat(\"Sample variance:\", var_sample, \"\\n\")\n\nSample variance: 1.9 \n\n\nKey difference: The sample variance (1.9) is larger than the population variance (1.583), providing an unbiased estimate when working with sample data.\nPlain English: The variance tells us, on average, how far squared each observation is from the mean. A larger variance means more spread-out data; a smaller variance means data cluster more tightly around the mean.\n\n\nAnother Example\nData: 2, 4, 6, 8, 10\nMean: \\bar{x} = \\frac{2+4+6+8+10}{5} = 6\n\n\n\nx_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\n\n\n\n\n2\n-4\n16\n\n\n4\n-2\n4\n\n\n6\n0\n0\n\n\n8\n2\n4\n\n\n10\n4\n16\n\n\nSum\n\n40\n\n\n\ns^2 = \\frac{40}{5-1} = \\frac{40}{4} = 10\n\n\n\nStandard Deviation\nThe standard deviation is simply the square root of the variance. It returns us to the original units of measurement.\nApplicable to: Interval and Ratio data\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\nFor our first example:\ns = \\sqrt{1.9} \\approx 1.378\nFor our second example:\ns = \\sqrt{10} \\approx 3.16\n\n# Standard deviations\nsd_sample &lt;- sd(X)\ncat(\"Sample SD (first example):\", sd_sample, \"\\n\")\n\nSample SD (first example): 1.378 \n\nX2 &lt;- c(2, 4, 6, 8, 10)\ncat(\"Sample SD (second example):\", sd(X2), \"\\n\")\n\nSample SD (second example): 3.162 \n\n\nPlain English: The standard deviation tells us the typical distance of observations from the mean, in the original units of measurement. It’s more interpretable than variance because it’s in the same units as the data.\nRough interpretation: In many distributions (especially symmetric, bell-shaped distributions), about 68% of observations fall within one standard deviation of the mean, and about 95% fall within two standard deviations.\n\n\nInterquartile Range (IQR)\nThe interquartile range measures the spread of the middle 50% of the data. It’s calculated as:\nApplicable to: Ordinal, Interval, and Ratio data (requires ordering)\n\\text{IQR} = Q_3 - Q_1\nwhere Q_1 is the first quartile (25th percentile) and Q_3 is the third quartile (75th percentile).\nPlain English: The IQR tells us the range that contains the middle half of our data. It’s resistant to outliers, making it useful when extreme values are present.\nExample:\nData: 2, 4, 5, 7, 8, 11, 12, 15, 18, 20\n\nQ_1 = 5 (25% of data fall below 5)\nQ_3 = 15 (75% of data fall below 15)\n\\text{IQR} = 15 - 5 = 10\n\nThe middle 50% of observations span 10 units.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-relative-position",
    "href": "chapter5.html#measures-of-relative-position",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.8 Measures of Relative Position",
    "text": "5.8 Measures of Relative Position\nMeasures of relative position help us understand where a particular observation stands within the overall distribution. These measures answer questions like “How does this value compare to others in the dataset?”\nApplicable measurement scales:\n\nQuantiles/Percentiles/Quartiles: Ordinal, Interval, Ratio (requires ordering)\n\n\nQuantiles\nA quantile is a value that divides a dataset into equal-sized groups. More formally, the p-th quantile is a value below which a proportion p of the data falls.\nApplicable to: Ordinal, Interval, and Ratio data\nDefinition:\nFor a proportion p (where 0 &lt; p &lt; 1), the p-th quantile q_p satisfies:\n\nAt least proportion p of observations are less than or equal to q_p\nAt least proportion (1-p) of observations are greater than or equal to q_p\n\nPlain English: Quantiles are “cut points” that divide our sorted data into segments. They tell us the value below which a certain proportion of our data falls.\nImportant note: Quantiles are a general concept. Percentiles, quartiles, and the median are all specific types of quantiles.\nExamples of quantiles:\n\nThe 0.5 quantile (50th percentile) is the median - half the data fall below it\nThe 0.25 quantile (25th percentile) is the first quartile (Q_1)\nThe 0.75 quantile (75th percentile) is the third quartile (Q_3)\nThe 0.90 quantile (90th percentile) means 90% of data fall below this value\n\n\n\nPercentiles\nA percentile is a specific type of quantile that divides the data into 100 equal parts. The k-th percentile is the value below which k percent of observations fall.\nApplicable to: Ordinal, Interval, and Ratio data\nRelationship to quantiles:\nThe k-th percentile corresponds to the \\frac{k}{100} quantile. For example:\n\n25th percentile = 0.25 quantile\n50th percentile = 0.50 quantile = median\n90th percentile = 0.90 quantile\n\nExamples:\n\nThe 50th percentile is the median (50% of data fall below it)\nThe 90th percentile means 90% of observations fall below this value\nThe 10th percentile means only 10% of observations fall below this value\n\nInterpretation: If you score at the 85th percentile on a test, you performed better than 85% of test-takers.\n\n\nQuartiles\nQuartiles are specific quantiles that divide the data into four equal parts. They are the 25th, 50th, and 75th percentiles:\nApplicable to: Ordinal, Interval, and Ratio data\n\nQ_1 (First quartile or 25th percentile): 25% of data fall below this value\nQ_2 (Second quartile or 50th percentile): The median - 50% fall below\nQ_3 (Third quartile or 75th percentile): 75% of data fall below this value\n\n\n# Visualization of quartiles\nset.seed(456)\nexample_data &lt;- sort(rnorm(100, 50, 10))\n\npar(mar = c(5, 4, 4, 2))\nplot(1:100, example_data, pch = 19, cex = 0.8, col = \"gray40\",\n     xlab = \"Observation (sorted)\", ylab = \"Value\",\n     main = \"Quartiles Divide Data into Four Equal Parts\")\n\n# Add quartile lines\nabline(h = quantile(example_data, 0.25), col = \"blue\", lwd = 2, lty = 2)\nabline(h = quantile(example_data, 0.50), col = \"red\", lwd = 2, lty = 1)\nabline(h = quantile(example_data, 0.75), col = \"blue\", lwd = 2, lty = 2)\n\n# Add shaded regions\nrect(0, min(example_data), 101, quantile(example_data, 0.25), \n     col = rgb(0, 0, 1, 0.1), border = NA)\nrect(0, quantile(example_data, 0.25), 101, quantile(example_data, 0.50), \n     col = rgb(0, 1, 0, 0.1), border = NA)\nrect(0, quantile(example_data, 0.50), 101, quantile(example_data, 0.75), \n     col = rgb(1, 1, 0, 0.1), border = NA)\nrect(0, quantile(example_data, 0.75), 101, max(example_data), \n     col = rgb(1, 0, 0, 0.1), border = NA)\n\n# Labels\ntext(95, quantile(example_data, 0.25), \"Q1 (25%)\", pos = 3, font = 2)\ntext(95, quantile(example_data, 0.50), \"Q2 (50%)\", pos = 3, font = 2)\ntext(95, quantile(example_data, 0.75), \"Q3 (75%)\", pos = 3, font = 2)\n\nlegend(\"topleft\", \n       legend = c(\"Each region contains 25% of observations\"),\n       bty = \"n\", cex = 1.1)\n\n\n\n\n\n\n\n\n\n\nComputing Quartiles: The Tukey Method\nThere are several methods for computing quartiles. In this course, we use the Tukey method (also called the hinges method):\nSteps:\n\nFind the median of the entire dataset\nExclude the median if n is odd\nQ_1 = median of the lower half\nQ_3 = median of the upper half\n\nExample with even n:\nData: 2, 4, 5, 7, 8, 10, 12, 15 (n = 8)\nMedian position: between positions 4 and 5 → Median = \\frac{7+8}{2} = 7.5\nLower half: 2, 4, 5, 7 → Q_1 = \\frac{4+5}{2} = 4.5\nUpper half: 8, 10, 12, 15 → Q_3 = \\frac{10+12}{2} = 11\nExample with odd n:\nData: 2, 4, 5, 7, 8, 10, 12, 15, 18 (n = 9)\nMedian: 8 (position 5)\nLower half (excluding median): 2, 4, 5, 7 → Q_1 = \\frac{4+5}{2} = 4.5\nUpper half (excluding median): 10, 12, 15, 18 → Q_3 = \\frac{12+15}{2} = 13.5\nNote: R’s quantile() function has multiple methods. The Tukey method corresponds to type = 2, while R’s default is type = 7.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-shape-1",
    "href": "chapter5.html#measures-of-shape-1",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.9 Measures of Shape",
    "text": "5.9 Measures of Shape\nMeasures of shape describe the overall form or pattern of the distribution. While we typically assess shape visually using histograms and boxplots, it’s important to understand the key concepts.\n\nSkewness (Asymmetry)\nSkewness refers to the asymmetry of a distribution. It tells us whether the distribution has a longer tail on one side.\nTypes of skewness:\n\nSymmetric (no skew): The distribution looks the same on both sides of the center. Mean ≈ Median.\nRight-skewed (positive skew): Long tail extends to the right. Mean &gt; Median. Most values cluster on the left, with a few large values pulling the tail.\nLeft-skewed (negative skew): Long tail extends to the left. Mean &lt; Median. Most values cluster on the right, with a few small values pulling the tail.\n\nPlain English: Skewness tells us which direction the “tail” points. Income distributions are typically right-skewed (most people earn moderate incomes, but a few earn extremely high incomes). Test scores on an easy exam might be left-skewed (most students score high, but a few score very low).\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(patchwork)\n\n# Generate distributions\nset.seed(42)\nleft_skew &lt;- rbeta(2000, 8, 2) * 50 + 30\nsymmetric &lt;- rnorm(2000, 55, 8)\nright_skew &lt;- rbeta(2000, 2, 8) * 50 + 30\n\n# Create data frame\ndf_skew &lt;- data.frame(\n  value = c(left_skew, symmetric, right_skew),\n  type = rep(c(\"Left-Skewed\", \"Symmetric\", \"Right-Skewed\"), each = 2000)\n)\n\n# Calculate statistics for each distribution\nstats_skew &lt;- df_skew %&gt;%\n  group_by(type) %&gt;%\n  summarise(\n    mean_val = mean(value),\n    median_val = median(value),\n    mode_val = {\n      d &lt;- density(value)\n      d$x[which.max(d$y)]\n    }\n  )\n\n# Reorder factor levels for proper display\ndf_skew$type &lt;- factor(df_skew$type, \n                       levels = c(\"Left-Skewed\", \"Symmetric\", \"Right-Skewed\"))\nstats_skew$type &lt;- factor(stats_skew$type, \n                          levels = c(\"Left-Skewed\", \"Symmetric\", \"Right-Skewed\"))\n\n# Create plot\nggplot(df_skew, aes(x = value)) +\n  geom_histogram(aes(fill = type), bins = 30, color = \"white\", alpha = 0.7) +\n  geom_vline(data = stats_skew, aes(xintercept = mean_val), \n             color = \"red\", linewidth = 1.2, linetype = \"solid\") +\n  geom_vline(data = stats_skew, aes(xintercept = median_val), \n             color = \"blue\", linewidth = 1.2, linetype = \"dashed\") +\n  geom_vline(data = stats_skew, aes(xintercept = mode_val), \n             color = \"darkgreen\", linewidth = 1.2, linetype = \"dotted\") +\n  facet_wrap(~ type, ncol = 1, scales = \"free_y\") +\n  scale_fill_manual(values = c(\"Left-Skewed\" = \"lightblue\", \n                                \"Symmetric\" = \"lightgreen\", \n                                \"Right-Skewed\" = \"#FFFFE0\")) +\n  labs(x = \"Value\", y = \"Count\", \n       title = \"Comparing Skewness: Mean, Median, and Mode\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n        strip.text = element_text(face = \"bold\", size = 11)) +\n  geom_text(data = stats_skew, \n            aes(x = 75, y = Inf, \n                label = sprintf(\"Mean: %.1f\\nMedian: %.1f\\nMode: %.1f\", \n                               mean_val, median_val, mode_val)),\n            vjust = 1.5, hjust = 1, size = 3.5, \n            color = \"black\", fontface = \"bold\")\n\n\n\n\n\n\n\n\nKey insights:\n\nIn symmetric distributions, mean ≈ median ≈ mode\nIn right-skewed distributions (long tail to the right), mean &gt; median &gt; mode\nIn left-skewed distributions (long tail to the left), mean &lt; median &lt; mode\n\nThe mean is pulled toward the tail, the median stays in the middle, and the mode remains at the peak.\n\n\nKurtosis (Tailedness)\nKurtosis refers to whether a distribution has heavy tails (many extreme values) or light tails (few extreme values) compared to a normal distribution.\nTypes:\n\nHigh kurtosis: Heavy tails with many outliers; sharp peak in center\nLow kurtosis: Light tails with few outliers; flatter peak\n\nPlain English: Kurtosis tells us whether extreme values are common or rare in our data. Financial returns often have high kurtosis - most days see small changes, but occasionally there are very large movements (market crashes or rallies).\n\n# Generate distributions with different kurtosis\nset.seed(123)\n# Low kurtosis (uniform-like)\nlow_kurt &lt;- runif(2000, 40, 70)\n# Normal kurtosis\nnormal_kurt &lt;- rnorm(2000, 55, 8)\n# High kurtosis (heavy tails)\nhigh_kurt &lt;- rt(2000, df = 3) * 8 + 55\n\n# Create data frame\ndf_kurt &lt;- data.frame(\n  value = c(low_kurt, normal_kurt, high_kurt),\n  type = rep(c(\"Low Kurtosis\\n(Light tails)\", \n               \"Normal Kurtosis\", \n               \"High Kurtosis\\n(Heavy tails)\"), each = 2000)\n)\n\n# Reorder factor levels\ndf_kurt$type &lt;- factor(df_kurt$type, \n                       levels = c(\"Low Kurtosis\\n(Light tails)\", \n                                 \"Normal Kurtosis\", \n                                 \"High Kurtosis\\n(Heavy tails)\"))\n\n# Create plot\nggplot(df_kurt, aes(x = value)) +\n  geom_histogram(aes(fill = type), bins = 40, color = \"white\", alpha = 0.7) +\n  facet_wrap(~ type, ncol = 1, scales = \"free_y\") +\n  scale_fill_manual(values = c(\"Low Kurtosis\\n(Light tails)\" = \"#FFB6C1\", \n                                \"Normal Kurtosis\" = \"#98FB98\", \n                                \"High Kurtosis\\n(Heavy tails)\" = \"#FFD700\")) +\n  labs(x = \"Value\", y = \"Count\", \n       title = \"Comparing Kurtosis: Tail Behavior\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n        strip.text = element_text(face = \"bold\", size = 11)) +\n  coord_cartesian(xlim = c(20, 90))\n\n\n\n\n\n\n\n\nKey insights:\n\nLow kurtosis distributions have fewer extreme values and a flatter peak\nHigh kurtosis distributions have more extreme values (heavy tails) and a sharper peak\nNormal distribution serves as the reference point for kurtosis comparisons\n\n\n\n\n\nModality (Number of Peaks)\nModality refers to the number of distinct peaks or “modes” in the distribution.\nTypes:\n\nUnimodal: One clear peak\nBimodal: Two distinct peaks\nMultimodal: Three or more peaks\nUniform: No peaks; all values roughly equally common\n\nPlain English: The number of peaks can reveal important features of your data. A bimodal distribution might indicate two distinct subgroups (e.g., heights of adults might show peaks for men and women).\n\n# Visualize different shapes\nset.seed(789)\n\npar(mfrow = c(1, 3), mar = c(4, 4, 3, 1))\n\n# Unimodal symmetric\nsymmetric_data &lt;- rnorm(1000, 50, 10)\nhist(symmetric_data, breaks = 30, main = \"Unimodal Symmetric\", \n     xlab = \"Value\", col = \"lightblue\", probability = TRUE)\nlines(density(symmetric_data), col = \"darkblue\", lwd = 2)\n\n# Right-skewed\nskewed_data &lt;- rexp(1000, 1/20) + 20\nhist(skewed_data, breaks = 30, main = \"Right-Skewed (Unimodal)\", \n     xlab = \"Value\", col = \"lightgreen\", probability = TRUE)\nlines(density(skewed_data), col = \"darkgreen\", lwd = 2)\n\n# Bimodal\nbimodal_data &lt;- c(rnorm(500, 30, 5), rnorm(500, 60, 5))\nhist(bimodal_data, breaks = 30, main = \"Bimodal\", \n     xlab = \"Value\", col = \"lightyellow\", probability = TRUE)\nlines(density(bimodal_data), col = \"orange\", lwd = 2)\n\n\n\n\n\n\n\n\nVisual assessment: We primarily identify shape characteristics through:\n\nHistograms: Show the overall form, number of peaks, and direction of skew\nBoxplots: Reveal skewness through whisker lengths and median position\nDensity plots: Smooth curves that highlight the overall pattern",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#visualization-histograms",
    "href": "chapter5.html#visualization-histograms",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.10 Visualization: Histograms",
    "text": "5.10 Visualization: Histograms\nA histogram displays the frequency distribution of numerical data by dividing the range into bins (intervals) and showing the count or proportion of observations in each bin.\n\nHistogram Construction\nKey decisions:\n\nNumber of bins: Too few bins lose detail; too many bins create noise\nBin boundaries: Should bins be [a, b) (closed on left, open on right) or (a, b] (open on left, closed on right)?\nY-axis scale: Frequency counts, relative frequencies, or density?\n\n\n\nChoosing the Optimal Number of Bins\nSeveral rules help determine an appropriate number of bins:\n1. Sturges’ Rule (default in R):\nk = \\lceil \\log_2(n) + 1 \\rceil\nwhere n is the sample size. Works well for symmetric, unimodal distributions.\n2. Scott’s Rule:\n\\text{Bin width} = \\frac{3.5 \\cdot s}{n^{1/3}}\nwhere s is the sample standard deviation. More robust to outliers than Sturges’ rule.\n3. Freedman-Diaconis Rule:\n\\text{Bin width} = \\frac{2 \\cdot IQR}{n^{1/3}}\nwhere IQR is the interquartile range. Most robust to outliers; recommended for skewed distributions.\n4. Square Root Rule:\nk = \\lceil \\sqrt{n} \\rceil\nSimple but often produces too few bins for large datasets.\n\n# Example: Different bin selection methods\nset.seed(789)\nsample_data &lt;- c(rnorm(200, 50, 10), rnorm(50, 75, 8))\nn &lt;- length(sample_data)\n\n# Sturges\nsturges_bins &lt;- ceiling(log2(n) + 1)\n\n# Scott\nscott_width &lt;- 3.5 * sd(sample_data) / (n^(1/3))\nscott_bins &lt;- ceiling(diff(range(sample_data)) / scott_width)\n\n# Freedman-Diaconis\nfd_width &lt;- 2 * IQR(sample_data) / (n^(1/3))\nfd_bins &lt;- ceiling(diff(range(sample_data)) / fd_width)\n\n# Square root\nsqrt_bins &lt;- ceiling(sqrt(n))\n\ncat(\"Sturges:\", sturges_bins, \"bins\\n\")\n\nSturges: 9 bins\n\ncat(\"Scott:\", scott_bins, \"bins\\n\")\n\nScott: 10 bins\n\ncat(\"Freedman-Diaconis:\", fd_bins, \"bins\\n\")\n\nFreedman-Diaconis: 14 bins\n\ncat(\"Square root:\", sqrt_bins, \"bins\\n\")\n\nSquare root: 16 bins\n\n\n\n\nTypes of Histograms: Y-Axis Scales\n\n1. Frequency Histogram\nThe frequency histogram shows the count of observations in each bin. The y-axis represents the number of observations.\n\nY-axis: Absolute count (frequency)\nUse when: You want to see raw counts\nLimitation: Hard to compare distributions with different sample sizes\n\n\n\n2. Relative Frequency Histogram\nThe relative frequency histogram shows the proportion of observations in each bin. Each bar height represents the fraction of the total.\n\nY-axis: Relative frequency (proportion) = \\frac{\\text{count in bin}}{n}\nUse when: Comparing distributions with different sample sizes\nProperty: All bar heights sum to 1.0\n\n\n\n3. Density Histogram\nThe density histogram adjusts for bin width so that the area (not height) of each bar represents the proportion of observations.\n\nY-axis: Density = \\frac{\\text{relative frequency}}{\\text{bin width}}\nUse when: Bins have different widths, or when connecting to probability density functions\nProperty: Total area under all bars equals 1.0\n\nKey difference: In a density histogram, the area of each bar equals the relative frequency, while the height is density.\n\n# Generate example data\nset.seed(789)\nsample_data &lt;- c(rnorm(200, 50, 10), rnorm(50, 75, 8))\n\npar(mfrow = c(3, 1), mar = c(4, 4, 3, 2))\n\n# 1. Frequency histogram\nhist(sample_data, breaks = 20, main = \"Frequency Histogram\", \n     xlab = \"Value\", ylab = \"Frequency (Count)\", \n     col = \"lightblue\", border = \"black\", freq = TRUE)\n\n# 2. Relative frequency histogram - manual scaling\nh &lt;- hist(sample_data, breaks = 20, plot = FALSE)\nh$counts &lt;- h$counts / sum(h$counts)\nplot(h, main = \"Relative Frequency Histogram\",\n     xlab = \"Value\", ylab = \"Relative Frequency (Proportion)\",\n     col = \"lightgreen\", border = \"black\", freq = TRUE)\n\n# 3. Density histogram\nhist(sample_data, breaks = 20, main = \"Density Histogram\", \n     xlab = \"Value\", ylab = \"Density\", \n     col = \"lightyellow\", border = \"black\", freq = FALSE)\n\n\n\n\n\n\n\n\nMathematical relationship:\nFor a bin with boundaries [a, b):\n\nFrequency: f = \\text{count of observations in } [a, b)\nRelative frequency: \\frac{f}{n}\nDensity: \\frac{f/n}{b-a}\nArea of bar: (b-a) \\times \\frac{f/n}{b-a} = \\frac{f}{n} (equals relative frequency)\n\n\n\n\nOpen vs. Closed Bins\nRight-open bins: [a, b)\nThe bin includes values \\geq a and &lt; b. The value b itself goes into the next bin.\nRight-closed bins: (a, b]\nThe bin includes values &gt; a and \\leq b. The value a goes into the previous bin.\nExample:\nConsider data: 1, 2, 3, 4, 5, 6, 7, 8 with bins [1,3), [3,5), [5,7), [7,9)\nRight-open bins [a, b):\n\nBin [1, 3): contains 1, 2 (count = 2)\nBin [3, 5): contains 3, 4 (count = 2)\nBin [5, 7): contains 5, 6 (count = 2)\nBin [7, 9): contains 7, 8 (count = 2)\n\nRight-closed bins (a, b]:\n\nBin (1, 3]: contains 2, 3 (count = 2)\nBin (3, 5]: contains 4, 5 (count = 2)\nBin (5, 7]: contains 6, 7 (count = 2)\nBin (7, 9]: contains 8 (count = 1, since 9 is not in data)\n\nR convention: R’s hist() function uses right-closed bins by default (i.e., (a, b]). You can change this with the right = FALSE argument to get right-open bins [a, b).\n\npar(mfrow = c(1, 2), mar = c(4, 4, 3, 2))\n\n# Right-closed (default)\nhist(sample_data, breaks = 20, main = \"Right-Closed Bins (a, b]\", \n     xlab = \"Value\", col = \"lightblue\", border = \"black\", right = TRUE)\n\n# Right-open\nhist(sample_data, breaks = 20, main = \"Right-Open Bins [a, b)\", \n     xlab = \"Value\", col = \"lightcoral\", border = \"black\", right = FALSE)\n\n\n\n\n\n\n\n\n\n\nEffect of Number of Bins\n\npar(mfrow = c(1, 3), mar = c(4, 4, 3, 2))\n\n# Different numbers of bins\nhist(sample_data, breaks = 10, main = \"10 Bins (Too Few?)\", \n     xlab = \"Value\", col = \"lightblue\", border = \"black\")\nhist(sample_data, breaks = 20, main = \"20 Bins (Balanced)\", \n     xlab = \"Value\", col = \"lightgreen\", border = \"black\")\nhist(sample_data, breaks = 40, main = \"40 Bins (Too Many?)\", \n     xlab = \"Value\", col = \"lightyellow\", border = \"black\")\n\n\n\n\n\n\n\n\nInterpretation:\n\n10 bins: Shows general shape but may miss details (potential bimodality not visible)\n20 bins: Balances detail and clarity; reveals bimodal structure\n40 bins: Shows more detail but may emphasize random noise, making patterns harder to see\n\nWhat histograms reveal:\n\nShape: Symmetric, skewed left, skewed right, bimodal, multimodal\nCenter: Where most data are located\nSpread: How wide the distribution is\nOutliers: Unusual values separated from the main body\nGaps: Ranges where no data exist\n\nGeneral recommendation: Start with Sturges’ or Freedman-Diaconis rule, then adjust based on what patterns you want to emphasize. For exploratory analysis, try several different bin numbers.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#visualization-tukey-boxplots",
    "href": "chapter5.html#visualization-tukey-boxplots",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.11 Visualization: Tukey Boxplots",
    "text": "5.11 Visualization: Tukey Boxplots\nThe Tukey boxplot (or box-and-whisker plot) provides a visual summary of the distribution based on five key values: minimum, Q_1, median, Q_3, and maximum. It also identifies outliers.\n\nBoxplot Construction\nStep 1: Calculate the five-number summary\n\nMinimum (excluding outliers)\nFirst quartile (Q_1)\nMedian (Q_2)\nThird quartile (Q_3)\nMaximum (excluding outliers)\n\nStep 2: Calculate fences for outlier detection\n\\text{Lower fence} = Q_1 - 1.5 \\times \\text{IQR} \\text{Upper fence} = Q_3 + 1.5 \\times \\text{IQR}\nStep 3: Identify outliers\nAny observation below the lower fence or above the upper fence is classified as an outlier.\nStep 4: Draw the boxplot\n\nBox: Extends from Q_1 to Q_3 (contains the middle 50% of data)\nLine inside box: Shows the median\nWhiskers: Extend to the most extreme non-outlier observations\nIndividual points: Plotted for each outlier\n\nCRITICAL DISTINCTION: Fences vs. Whiskers\n\nFences: Theoretical boundaries used to identify outliers (not necessarily drawn)\nWhiskers: Extend to actual data points within the fences\n\nCommon mistake: Students often think whiskers extend to the fences. This is incorrect! Whiskers extend only to the most extreme actual data values that fall within the fence boundaries.\n\n# Example demonstrating fences vs. whiskers\nexample_data &lt;- c(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 25)\n\nq1 &lt;- quantile(example_data, 0.25, type = 2)\nq3 &lt;- quantile(example_data, 0.75, type = 2)\niqr &lt;- q3 - q1\nlower_fence &lt;- q1 - 1.5 * iqr\nupper_fence &lt;- q3 + 1.5 * iqr\n\noutliers &lt;- example_data[example_data &lt; lower_fence | example_data &gt; upper_fence]\nnon_outliers &lt;- example_data[example_data &gt;= lower_fence & example_data &lt;= upper_fence]\nlower_whisker &lt;- min(non_outliers)\nupper_whisker &lt;- max(non_outliers)\n\npar(mfrow = c(1, 2), mar = c(4, 3, 3, 2))\n\n# Left panel: Diagram showing fences and whiskers\nplot(c(0, 30), c(0, 2), type = \"n\", xlab = \"Value\", ylab = \"\", \n     main = \"Understanding Fences vs. Whiskers\", yaxt = \"n\", bty = \"n\")\n\n# Data points\npoints(example_data, rep(1, length(example_data)), \n       pch = 19, cex = 2, \n       col = ifelse(example_data %in% outliers, \"red\", \"darkblue\"))\n\n# Fences (dashed)\nabline(v = lower_fence, col = \"orange\", lwd = 2, lty = 2)\nabline(v = upper_fence, col = \"orange\", lwd = 2, lty = 2)\n\n# Whiskers (solid)\nabline(v = lower_whisker, col = \"darkgreen\", lwd = 3)\nabline(v = upper_whisker, col = \"darkgreen\", lwd = 3)\n\n# Box (Q1 to Q3)\nrect(q1, 0.7, q3, 1.3, border = \"black\", lwd = 2)\nsegments(median(example_data), 0.7, median(example_data), 1.3, lwd = 3)\n\n# Labels\ntext(lower_fence, 1.7, sprintf(\"Lower fence\\n%.1f\", lower_fence), \n     col = \"orange\", cex = 0.9)\ntext(upper_fence, 1.7, sprintf(\"Upper fence\\n%.1f\", upper_fence), \n     col = \"orange\", cex = 0.9)\ntext(lower_whisker, 0.3, sprintf(\"Lower whisker\\n%d\", lower_whisker), \n     col = \"darkgreen\", cex = 0.9, font = 2)\ntext(upper_whisker, 0.3, sprintf(\"Upper whisker\\n%d\", upper_whisker), \n     col = \"darkgreen\", cex = 0.9, font = 2)\n\nlegend(\"top\", \n       legend = c(\"Non-outliers\", \"Outliers\", \"Fences (dashed)\", \"Whiskers (solid)\"),\n       col = c(\"darkblue\", \"red\", \"orange\", \"darkgreen\"),\n       pch = c(19, 19, NA, NA), lty = c(NA, NA, 2, 1), lwd = 2,\n       ncol = 2)\n\n# Right panel: Actual boxplot\nboxplot(example_data, main = \"Corresponding Boxplot\", \n        ylab = \"Value\", col = \"lightblue\", ylim = c(0, 30))\n\n# Annotate boxplot components\ntext(1.35, q1, \"Q1\", font = 2)\ntext(1.35, median(example_data), \"Median\", font = 2)\ntext(1.35, q3, \"Q3\", font = 2)\ntext(1.35, lower_whisker, \"Lower\\nwhisker\", font = 2, col = \"darkgreen\")\ntext(1.35, upper_whisker, \"Upper\\nwhisker\", font = 2, col = \"darkgreen\")\narrows(1.2, 25, 1.05, 25, length = 0.1, col = \"red\", lwd = 2)\ntext(1.3, 25, \"Outlier\", pos = 4, col = \"red\", font = 2)\n\n\n\n\n\n\n\n\nKey insights from boxplots:\n\nSymmetry: If the median is centered in the box and whiskers are equal length, the distribution is roughly symmetric\nSkewness: If the median is closer to Q_1 and the upper whisker is longer, the distribution is right-skewed\nOutliers: Individual points beyond the whiskers indicate unusual observations\nComparison: Side-by-side boxplots make it easy to compare distributions across groups\n\n\n\nComparing Multiple Groups with Boxplots\n\n# Generate data for three groups\nset.seed(9)\ngroup_a &lt;- rnorm(50, 50, 10)\ngroup_b &lt;- c(rnorm(45, 60, 8), c(30, 35, 85, 90, 95))  # With outliers\ngroup_c &lt;- rexp(50, 1/20) + 30  # Right-skewed\n\n# Combine into a data frame\ndata_compare &lt;- data.frame(\n  value = c(group_a, group_b, group_c),\n  group = factor(rep(c(\"Group A\\n(Symmetric)\", \n                       \"Group B\\n(With outliers)\", \n                       \"Group C\\n(Right-skewed)\"), \n                     each = 50))\n)\n\nboxplot(value ~ group, data = data_compare,\n        main = \"Comparing Distributions Across Groups\",\n        ylab = \"Value\",\n        col = c(\"#66C2A5\", \"#FC8D62\", \"#8DA0CB\"),\n        border = \"black\")\n\ngrid(nx = NA, ny = NULL, col = \"lightgray\", lty = \"dotted\")\n\n\n\n\n\n\n\n\nInterpretation:\n\nGroup A: Symmetric distribution with median near the center of the box\nGroup B: Higher center with several outliers on both ends\nGroup C: Right-skewed distribution (longer upper whisker, median closer to Q_1)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#summary-choosing-the-right-measure",
    "href": "chapter5.html#summary-choosing-the-right-measure",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.12 Summary: Choosing the Right Measure",
    "text": "5.12 Summary: Choosing the Right Measure\nSelecting appropriate descriptive statistics depends on the characteristics of your data:\nFor central tendency:\n\nMean: Use when data are symmetric and without extreme outliers. Provides the most information but sensitive to extremes. Requires interval or ratio scale.\nMedian: Use when data are skewed or contain outliers. More robust but discards some information. Requires at least ordinal scale.\nMode: Use for categorical data or when identifying the most common value. Can be used with any measurement scale.\n\nFor dispersion:\n\nStandard deviation: Use with the mean for symmetric data without outliers. Most common and interpretable. Requires interval or ratio scale.\nIQR: Use with the median for skewed data or data with outliers. Resistant to extremes. Requires at least ordinal scale.\nRange: Quick assessment but highly sensitive to outliers. Use cautiously. Requires interval or ratio scale.\n\nFor relative position:\n\nQuantiles/Percentiles: Help compare individual observations to the overall distribution. Requires at least ordinal scale.\nQuartiles: Provide standard cut points that divide data into four equal parts. Requires at least ordinal scale.\n\nFor shape:\n\nVisual inspection: Use histograms to assess symmetry, skewness, and modality\nBoxplots: Reveal skewness and identify outliers effectively\n\nFor visualization:\n\nHistograms: Show the full shape of the distribution. Good for understanding overall patterns.\nBoxplots: Efficient summaries for comparing groups. Highlight outliers and quartiles clearly.\n\nGeneral principle: Always report multiple measures. The mean and standard deviation tell one story; the median and IQR tell another. Together with visual displays, they provide a complete picture of your data. Always consider the measurement scale of your data when choosing which statistics to calculate.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#appendix-a-understanding-mean-as-a-balance-point",
    "href": "chapter5.html#appendix-a-understanding-mean-as-a-balance-point",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.13 Appendix A: Understanding Mean as a Balance Point 🎯",
    "text": "5.13 Appendix A: Understanding Mean as a Balance Point 🎯\nLet’s consider a dataset X = \\{1, 2, 6, 7, 9\\} on a number line, imagining it as a seesaw:\n\n\n\nhttps://www.gastonsanchez.com/matrix4sl/mean-as-a-balancing-point.html\n\n\nThe mean (\\mu) acts as the perfect balance point of this seesaw. For our data:\n\\mu = \\frac{1 + 2 + 6 + 7 + 9}{5} = 5\n\nWhat happens at different support points? 🤔\n\nSupport point at 6 (too high):\n\nLeft side: Values (1, 2) are below\nRight side: Values (7, 9) are above\n\\sum distances from left = (6-1) + (6-2) = 9\n\\sum distances from right = (7-6) + (9-6) = 4\nThe seesaw tilts left! ⬅️ because 9 &gt; 4\n\nSupport point at 4 (too low):\n\nLeft side: Values (1, 2) are below\nRight side: Values (6, 7, 9) are above\n\\sum distances from left = (4-1) + (4-2) = 5\n\\sum distances from right = (6-4) + (7-4) + (9-4) = 10\nThe seesaw tilts right! ➡️ because 5 &lt; 10\n\nSupport point at mean (5) (perfect balance):\n\n\\sum distances below = \\sum distances above\n((5-1) + (5-2)) = ((6-5) + (7-5) + (9-5))\n7 = 7 ✨ Perfect balance!\n\n\nThis shows why the mean is the unique balance point, where:\n\\sum_{i=1}^n (x_i - \\mu) = 0\nThe seesaw will always tilt unless the support point is placed exactly at the mean! 🎪\n\n\nMean as a Balance Point\nThis visualization shows how the arithmetic mean (5) acts as a balance point between clustered points on the left and dispersed points on the right:\nLeft side of the mean: - Points with values 2 and 3 - Close together (difference of 1 unit) - Distances from mean: 3 and 2 units - Sum of “pull” = 5 units\nRight side of the mean: - Points with values 6 and 9 - More spread out (difference of 3 units) - Distances from mean: 1 and 4 units - Sum of “pull” = 5 units\nKey observations:\n\nThe mean (5) is a balance point, even though:\n\nPoints on the left are clustered (2,3)\nPoints on the right are dispersed (6,9)\nGreen arrows show distances from the mean\n\nBalance is maintained because:\n\nSum of distances balances out: (5-2) + (5-3) = (6-5) + (9-5)\nTotal sum of distances = 5 units on each side",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#appendix-b-measures-of-relative-position-standing",
    "href": "chapter5.html#appendix-b-measures-of-relative-position-standing",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.14 Appendix B: Measures of Relative Position (Standing)",
    "text": "5.14 Appendix B: Measures of Relative Position (Standing)\nUnderstanding where values sit within a dataset is crucial for data analysis. Let’s explore these concepts step by step.\n\nQuartiles (Q): The Basics\nThink of quartiles as special numbers that split your ordered data into four equal parts.\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\n\nWhat Are Quartiles?\nFirst Quartile (Q1):\n\nSeparates the lowest 25% of data from the rest\nAlso called the 25th percentile\nExample: If Q1 = 50 in a test score dataset, 25% of students scored below 50\n\nSecond Quartile (Q2):\n\nThe median - splits data in half\nAlso called the 50th percentile\nExample: If Q2 = 70, half the students scored below 70\n\nThird Quartile (Q3):\n\nSeparates the highest 25% of data from the rest\nAlso called the 75th percentile\nExample: If Q3 = 85, 75% of students scored below 85\n\n\n\nHow to Calculate Quartiles (Step by Step) - Two Methods\nLet’s examine student test scores using both common quartile calculation methods:\nExample 1: Odd Number Case (11 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 88, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 11 values (odd)\nMedian position = (n + 1)/2 = 6\nQ2 = 78\n\nStep 2: Find Q1\n\nTukey’s Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (n + 1)/4 = (11 + 1)/4 = 3\nQ1 = 70 (3rd value)\n\n\nStep 3: Find Q3\n\nTukey’s Method:\n\nLook at upper half: 80, 82, 85, 88, 90\nQ3 = median of upper half = 85\n\nInterpolation Method:\n\nPosition = 3(n + 1)/4 = 3(12)/4 = 9\nQ3 = 85 (9th value)\n\n\nExample 2: Even Number Case (10 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 10 values (even)\nMedian positions = 5 and 6\nQ2 = (75 + 78)/2 = 76.5\n\nStep 2: Find Q1\n\nTukey’s Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (10 + 1)/4 = 2.75\nQ1 = 65 + 0.75(70 - 65) = 68.75\n\n\nStep 3: Find Q3\n\nTukey’s Method:\n\nLook at upper half: 78, 80, 82, 85, 90\nQ3 = median of upper half = 82\n\nInterpolation Method:\n\nPosition = 3(10 + 1)/4 = 8.25\nQ3 = 82 + 0.25(85 - 82) = 82.75\n\n\nImportant Notes:\n\nTukey’s Method:\n\nFirst find the median (Q2)\nSplit the data into lower and upper halves\nFind Q1 as the median of the lower half\nFind Q3 as the median of the upper half\nWhen n is odd, the median is not included in either half\n\nInterpolation Method:\n\nUses positions (n+1)/4 for Q1 and 3(n+1)/4 for Q3\nWhen position falls between values, uses linear interpolation\nDoesn’t require splitting data into halves\n\n\nBoth methods give the same results for simple positions (Example 1) but can differ when interpolation is needed (Example 2).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#appendix-c-manual-construction-of-tukey-boxplot",
    "href": "chapter5.html#appendix-c-manual-construction-of-tukey-boxplot",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.15 Appendix C: Manual Construction of Tukey Boxplot",
    "text": "5.15 Appendix C: Manual Construction of Tukey Boxplot\nStep 1: Calculate Key Components\n\nFind quartiles: Q_1, Q_2 (median), Q_3\nCalculate Interquartile Range: IQR = Q_3 - Q_1\n\nStep 2: Determine Whisker Boundaries\n\nLower fence: Q_1 - 1.5 \\times IQR\nUpper fence: Q_3 + 1.5 \\times IQR\n\nStep 3: Identify Outliers Data points are outliers if they are:\n\nBelow lower fence: x &lt; Q_1 - 1.5 \\times IQR\nAbove upper fence: x &gt; Q_3 + 1.5 \\times IQR\n\nExample: Given data: 2, 4, 6, 8, 9, 10, 11, 12, 14, 16, 50\n\nFind quartiles:\n\nQ_1 = 6\nQ_2 = 10\nQ_3 = 14\n\nCalculate IQR:\n\nIQR = 14 - 6 = 8\n\nCalculate fences:\n\nLower: 6 - (1.5 \\times 8) = -6\nUpper: 14 + (1.5 \\times 8) = 26\n\nIdentify outliers:\n\n50 &gt; 26, therefore 50 is an outlier\n\n\nGraphical Elements:\n\nBox: Draw from Q_1 to Q_3\nLine inside box: Draw at Q_2\nWhiskers: Extend to most extreme non-outlier points\nPoints: Plot outliers individually beyond whiskers\n\n\n\nCase Study: Comparing Heights Between Groups\nLet’s apply our understanding of box plots to a real dataset. We have height measurements (in centimeters) from two groups of 25 students each.\n\n# Create the dataset\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Transform dataset from wide to long format\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Display the first few rows\nhead(data_height_l)\n\n  Group_number height\n1      group_1    150\n2      group_1    160\n3      group_1    165\n4      group_1    168\n5      group_1    172\n6      group_1    173\n\n\nLet’s calculate some summary statistics for each group:\n\n# Calculate summary statistics for each group\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Calculate IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Create a comparison table\nstats_table &lt;- rbind(\n  group1_stats,\n  group2_stats\n)\nrownames(stats_table) &lt;- c(\"Group 1\", \"Group 2\")\n\n# Display the table\nstats_table\n\n        Min. 1st Qu. Median  Mean 3rd Qu. Max.\nGroup 1  150     175    180 178.6     183  200\nGroup 2  138     165    175 172.1     182  210\n\n# Display IQR values\ncat(\"IQR for Group 1:\", group1_iqr, \"\\n\")\n\nIQR for Group 1: 8 \n\ncat(\"IQR for Group 2:\", group2_iqr, \"\\n\")\n\nIQR for Group 2: 17 \n\n\n\n\nVisualizing the Height Data\nNow, let’s visualize the data using box plots and density plots:\n\n# Create horizontal boxplots\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Height Distribution by Group\",\n       x = \"Group\",\n       y = \"Height (cm)\")\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\n\n\n\n\n\n\nFigure 5.1: Box plots comparing height distributions between groups.\n\n\n\n\n\nTo complement our box plots, let’s also look at the density distributions:\n\n# Create density plots\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Height Density by Group\",\n       x = \"Height (cm)\",\n       y = \"Density\")\n\n\n\n\n\n\n\nFigure 5.2: Density plots showing the height distributions for each group.\n\n\n\n\n\n\n\nBox Plot Interpretation Exercise\nBased on the box plots and density plots above, determine whether each of the following statements is True or False. For each statement, provide a brief explanation based on evidence from the visualizations.\n\n\n\n\n\n\nExercise Questions\n\n\n\n\nStudents from group 2 (G2) in the studied sample are, on average, taller than those from group 1 (G1).\nGroup 1 (G1) height measurements are more dispersed/spread out than group 2 (G2).\nThe lowest person is in group 2 (G2).\nBoth data sets are negatively (left) skewed.\nHalf of the students in group 2 (G2) measure at least 175 cm.\n\n\n\n\nHints for Interpretation\nWhen answering these questions, consider:\n\nThe position of the median line within each box\nThe relative sizes of the boxes (IQR)\nThe positions of the minimum and maximum values\nThe symmetry of the distributions (balanced or skewed)\nThe lengths of the whiskers\n\nFor each statement, determine whether it is True or False and provide your explanation:\n\n\n\n\n\n\nAnswer Template\n\n\n\n\n\n\nStudents from G2 are, on average, taller than G1: [True/False]\n\nExplanation:\n\nG1 height is more dispersed/spread out: [True/False]\n\nExplanation:\n\nThe lowest person is in G2: [True/False]\n\nExplanation:\n\nBoth data sets are negatively (left) skewed: [True/False]\n\nExplanation:\n\nHalf of G2 measure at least 175 cm: [True/False]\n\nExplanation:\n\n\n\n\n\nLet’s review the answers to our box plot interpretation questions:\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nStudents from G2 are, on average, taller than G1: False\n\nExplanation: The median height (middle line in the boxplot) for G1 is higher than G2.\n\nG1 height is more dispersed/spread out: False\n\nExplanation: G2 shows greater dispersion. This is visible in the boxplot where G2 has a larger interquartile range (IQR) of 17.5 cm compared to G1’s 9.5 cm. G2 also has a wider range from minimum to maximum values.\n\nThe lowest person is in G2: True\n\nExplanation: The minimum value in G2 is 138 cm, which is lower than the minimum value in G1 (150 cm).\n\nBoth data sets are negatively (left) skewed: True\n\nExplanation: In both groups, the median line is positioned toward the upper part of the box, and the lower whisker is longer than the upper whisker. This indicates that there’s a longer tail on the left side of the distribution, which means negative skewness.\n\nHalf of G2 measure at least 175 cm: True\n\nExplanation: The median (middle line in the boxplot) for G2 is 175 cm, which means that 50% of the values are greater than or equal to 175 cm.\n\n\n\n\n\n\n\n\nR Code Reference\nHere’s the complete R code used in this section:\n\n# Load required packages\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n# Set display options\noptions(scipen = 999, digits = 3)\n\n# Create the dataset\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Transform dataset from wide to long format\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Display the first few rows\nhead(data_height_l)\n\n# Calculate summary statistics for each group\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Calculate IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Create horizontal boxplots\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Height Distribution by Group\",\n       x = \"Group\",\n       y = \"Height (cm)\")\n\n# Create density plots\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Height Density by Group\",\n       x = \"Height (cm)\",\n       y = \"Density\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#appendix-d-comparative-analysis-using-tukey-boxplots",
    "href": "chapter5.html#appendix-d-comparative-analysis-using-tukey-boxplots",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.16 Appendix D: Comparative Analysis Using Tukey Boxplots",
    "text": "5.16 Appendix D: Comparative Analysis Using Tukey Boxplots\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# Prepare data - exclude Oceania due to insufficient data (only 2 countries)\ndata_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007, continent != \"Oceania\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-tukey-boxplots",
    "href": "chapter5.html#understanding-tukey-boxplots",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.17 Understanding Tukey Boxplots",
    "text": "5.17 Understanding Tukey Boxplots\nA Tukey boxplot displays five key statistics and identifies outliers:\n\nMedian (Q2): The middle line in the box (50th percentile)\nFirst quartile (Q1): Bottom of the box (25th percentile)\n\nThird quartile (Q3): Top of the box (75th percentile)\nInterquartile Range (IQR): The height of the box = Q3 - Q1\nWhiskers: Extend to the most extreme values within Q1 - 1.5 \\times IQR (lower) and Q3 + 1.5 \\times IQR (upper)\nOutliers: Points beyond the whiskers (Tukey’s rule)\n\nKey interpretation points:\n\nThe median shows central tendency (not the mean!)\nThe IQR shows the middle 50% of data\nLonger boxes indicate more spread\nAsymmetric boxes or whiskers indicate skewness\nThe whiskers show the range of “typical” values\n\n\nggplot(data_2007, aes(x = reorder(continent, lifeExp, FUN = median), y = lifeExp)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 21, \n               outlier.fill = \"red\", outlier.size = 3, width = 0.6) +\n  geom_jitter(width = 0.15, alpha = 0.3, color = \"darkblue\", size = 2) +\n  labs(title = \"Life Expectancy by Continent (2007)\",\n       subtitle = \"Boxplots show distribution; red circles indicate outliers by Tukey's rule\\nNote: Oceania excluded due to insufficient data (only 2 countries)\",\n       x = \"Continent (ordered by median)\",\n       y = \"Life Expectancy (years)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12)\n  ) +\n  scale_y_continuous(breaks = seq(40, 85, by = 5))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#part-a-basic-comparisons",
    "href": "chapter5.html#part-a-basic-comparisons",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.18 Part A: Basic Comparisons",
    "text": "5.18 Part A: Basic Comparisons\nAnswer the following questions by examining the boxplot above.\n\nTrue or False:\n\nEurope has a higher median life expectancy than Africa.\nLife expectancy is more dispersed in Africa than in Europe (compare the box heights).\nAfrica’s distribution is positively (right) skewed—the upper whisker is longer than the lower whisker.\nAsia has outliers on both the low and high ends of the distribution.\nThe median life expectancy in Asia is lower than in the Americas.\nAfrica shows the largest interquartile range (IQR) among all continents.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#part-b-changes-over-time",
    "href": "chapter5.html#part-b-changes-over-time",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.19 Part B: Changes Over Time",
    "text": "5.19 Part B: Changes Over Time\n\ntime_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007), continent != \"Oceania\") %&gt;%\n  mutate(year = factor(year))\n\nggplot(time_comparison, aes(x = continent, y = lifeExp, fill = year)) +\n  geom_boxplot(alpha = 0.7, position = position_dodge(0.8), \n               outlier.shape = 21, outlier.size = 2, width = 0.7) +\n  labs(title = \"Life Expectancy: 1957 vs 2007\",\n       subtitle = \"50-year comparison of distributions (Oceania excluded due to insufficient data)\",\n       x = \"Continent\",\n       y = \"Life Expectancy (years)\",\n       fill = \"Year\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    legend.position = \"top\",\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 11)\n  ) +\n  scale_fill_manual(values = c(\"1957\" = \"#E78AC3\", \"2007\" = \"#8DA0CB\")) +\n  scale_y_continuous(breaks = seq(30, 85, by = 5))\n\n\n\n\n\n\n\n\n\nTrue or False:\n\nThe median life expectancy increased in all continents between 1957 and 2007.\nIn 1957, Asia had a larger IQR than in 2007, indicating convergence (compare box heights).\nThe entire distribution of life expectancy in Europe in 2007 is higher than Africa’s entire distribution in 1957 (check if the boxes overlap at all).\nAsia showed a larger increase in median life expectancy than the Americas between 1957 and 2007.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#part-c-summary-statistics",
    "href": "chapter5.html#part-c-summary-statistics",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.20 Part C: Summary Statistics",
    "text": "5.20 Part C: Summary Statistics\n\n# Calculate summary statistics for verification\nsummary_stats &lt;- data_2007 %&gt;%\n  group_by(continent) %&gt;%\n  summarise(\n    n = n(),\n    median = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    min = min(lifeExp),\n    max = max(lifeExp),\n    lower_fence = q1 - 1.5 * iqr,\n    upper_fence = q3 + 1.5 * iqr,\n    n_outliers = sum(lifeExp &lt; lower_fence | lifeExp &gt; upper_fence)\n  ) %&gt;%\n  arrange(desc(median))\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Summary Statistics for Life Expectancy by Continent (2007)\")\n\n\nSummary Statistics for Life Expectancy by Continent (2007)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontinent\nn\nmedian\nq1\nq3\niqr\nmin\nmax\nlower_fence\nupper_fence\nn_outliers\n\n\n\n\nEurope\n30\n78.6\n75.0\n79.8\n4.8\n71.8\n81.8\n67.9\n87.0\n0\n\n\nAmericas\n25\n72.9\n71.8\n76.4\n4.6\n60.9\n80.7\n64.8\n83.3\n1\n\n\nAsia\n33\n72.4\n65.5\n75.6\n10.2\n43.8\n82.6\n50.3\n90.9\n1\n\n\nAfrica\n52\n52.9\n47.8\n59.4\n11.6\n39.6\n76.4\n30.4\n76.9\n0\n\n\n\n\n\nNote: The lower and upper fences show the boundaries for Tukey’s outlier rule. Values beyond these fences are marked as outliers. Oceania is excluded from this analysis because it contains only 2 countries in the dataset, which is insufficient for meaningful distributional analysis using boxplots.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#thinking-questions",
    "href": "chapter5.html#thinking-questions",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.21 Thinking Questions",
    "text": "5.21 Thinking Questions\n\nWhy might Africa show more outliers than other continents? What could this tell us about within-continent variation?\nIf we wanted to compare “average” life expectancy between continents, should we use the mean or median? Why might these differ, especially for Africa?\nLooking at the 1957 vs 2007 comparison, which continent showed the most dramatic transformation? Consider both the change in median and the change in spread (IQR).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#solutions-1",
    "href": "chapter5.html#solutions-1",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.22 Solutions",
    "text": "5.22 Solutions\n\nPart A: Basic Comparisons\n1. TRUE - Europe’s median (the line in the middle of the box) is clearly much higher (~78 years) than Africa’s median (~53 years).\n2. TRUE - Africa’s box is much taller than Europe’s box, indicating greater IQR and thus more dispersion in life expectancy values.\n3. TRUE - Africa’s upper whisker is visibly longer than the lower whisker, indicating positive (right) skewness. This means there are some African countries with notably higher life expectancy than the median, creating a tail toward higher values.\n4. TRUE - Asia shows red circles (outliers) both below the lower whisker (countries with unusually low life expectancy) and above the upper whisker (countries with unusually high life expectancy).\n5. TRUE - The median line in Asia’s box is slightly lower (~71 years) than the median line in the Americas’ box (~73 years).\n6. TRUE - Africa has the tallest box among all continents, indicating the largest IQR and thus the greatest variability in life expectancy across African countries.\n\n\nPart B: Changes Over Time\n1. TRUE - For every continent, the blue box (2007) has a higher median line than the pink box (1957), showing universal improvement in life expectancy over the 50-year period.\n2. TRUE - Asia’s pink box (1957) is noticeably taller than its blue box (2007), showing that Asian countries converged (became more similar) over time. Countries that were far apart in development in 1957 have moved closer together by 2007.\n3. TRUE - Europe’s entire blue box (2007) sits above Africa’s entire pink box (1957)—there is no overlap between the distributions. Even the country with the lowest life expectancy in Europe in 2007 has higher life expectancy than the country with the highest life expectancy in Africa in 1957.\n4. TRUE - The vertical distance between Asia’s pink and blue median lines is visibly larger (~30 years: from ~41 to ~71) than the distance between the Americas’ pink and blue median lines (~21 years: from ~52 to ~73), indicating Asia had a greater absolute improvement in median life expectancy.\n\n\nThinking Questions: Sample Answers\n1. Why might Africa show more outliers?\nAfrica’s outliers reflect extreme within-continent heterogeneity. Countries like Swaziland and Lesotho face severe HIV/AIDS epidemics that dramatically reduce life expectancy, while other factors like conflict, governance quality, and economic development vary widely across the continent. The presence of outliers suggests that treating “Africa” as a homogeneous category obscures important variation, and country-specific factors matter greatly.\n2. Mean or median for comparison?\nThe median is more appropriate for comparing continents, especially Africa. The presence of outliers and the right-skewed distribution in Africa would pull the mean lower than what represents the typical country experience. The median better represents the “typical” or “central” country on each continent and is resistant to extreme values, making it a more robust measure of central tendency for this comparison.\n3. Most dramatic transformation:\nAsia showed the most dramatic transformation. First, it achieved the largest absolute increase in median life expectancy (approximately 30 years, compared to roughly 10-21 years for other continents). Second, it showed notable convergence—the 2007 box is much shorter than the 1957 box—meaning that Asian countries which were at vastly different development stages in 1957 have largely caught up with each other by 2007. This reflects the rapid economic development and healthcare improvements across much of Asia during this period, particularly in East and Southeast Asia.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#appendix-e-comparative-analysis-using-tukey-boxplots",
    "href": "chapter5.html#appendix-e-comparative-analysis-using-tukey-boxplots",
    "title": "5  Fundamentals of Univariate Descriptive Statistics",
    "section": "5.23 Appendix E: Comparative Analysis Using Tukey Boxplots",
    "text": "5.23 Appendix E: Comparative Analysis Using Tukey Boxplots\n\nlibrary(tidyverse)\nlibrary(dslabs)  # Contains gapminder data with fertility rates\n\nWarning: package 'dslabs' was built under R version 4.4.3\n\n# Prepare data for two time points\ndata_1960 &lt;- gapminder %&gt;%\n  filter(year == 1960, !is.na(fertility))\n\ndata_2015 &lt;- gapminder %&gt;%\n  filter(year == 2015, !is.na(fertility))\n\n# Combine for comparison\ndata_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1960, 2015), !is.na(fertility))\n\n\nUnderstanding Tukey Boxplots\nA Tukey boxplot displays five key statistics and identifies outliers:\n\nMedian (Q2): The middle line in the box (50th percentile)\nFirst quartile (Q1): Bottom of the box (25th percentile)\n\nThird quartile (Q3): Top of the box (75th percentile)\nInterquartile Range (IQR): The height of the box = Q3 - Q1\nWhiskers: Extend to the most extreme values within Q1 - 1.5 \\times IQR (lower) and Q3 + 1.5 \\times IQR (upper)\nOutliers: Points beyond the whiskers (Tukey’s rule)\n\nKey interpretation points:\n\nThe median shows central tendency (not the mean!)\nThe IQR shows the middle 50% of data\nLonger boxes indicate more spread\nAsymmetric boxes or whiskers indicate skewness\nThe whiskers show the range of “typical” values\n\n\n\nTemporal Comparison: 1960 vs 2015\n\nggplot(data_comparison, aes(x = reorder(continent, fertility, FUN = median), \n                             y = fertility, fill = as.factor(year))) +\n  geom_boxplot(alpha = 0.7, outlier.shape = 21, outlier.size = 3,\n               position = position_dodge(width = 0.8)) +\n  labs(title = \"Fertility Rate by Continent: 1960 vs 2015\",\n       subtitle = \"Side-by-side comparison shows demographic transition across continents\",\n       x = \"Continent (ordered by median fertility in 2015)\",\n       y = \"Fertility Rate (births per woman)\",\n       fill = \"Year\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    legend.position = \"top\"\n  ) +\n  scale_fill_manual(values = c(\"1960\" = \"coral\", \"2015\" = \"lightgreen\")) +\n  scale_y_continuous(breaks = seq(1, 8, by = 0.5))\n\n\n\n\n\n\n\n\n\n\nDistribution of Fertility Rate Changes (1960-2015)\n\n# Calculate change in fertility for each country\nfertility_change &lt;- gapminder %&gt;%\n  filter(year %in% c(1960, 2015), !is.na(fertility)) %&gt;%\n  select(country, continent, year, fertility) %&gt;%\n  pivot_wider(names_from = year, values_from = fertility, names_prefix = \"year_\") %&gt;%\n  filter(!is.na(year_1960), !is.na(year_2015)) %&gt;%\n  mutate(\n    change = year_2015 - year_1960,\n    percent_change = ((year_2015 - year_1960) / year_1960) * 100\n  )\n\n\nggplot(fertility_change, aes(x = reorder(continent, change, FUN = median), y = change)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 21, \n               outlier.fill = \"red\", outlier.size = 3) +\n  geom_jitter(width = 0.15, alpha = 0.3, color = \"darkblue\", size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", size = 1) +\n  labs(title = \"Change in Fertility Rate by Continent (1960-2015)\",\n       subtitle = \"Negative values indicate fertility decline; dashed line shows no change\",\n       x = \"Continent (ordered by median change)\",\n       y = \"Change in Fertility Rate (births per woman)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12)\n  ) +\n  scale_y_continuous(breaks = seq(-6, 1, by = 0.5))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nExercise Questions\n\nWhich continent had the highest median fertility rate in 1960?\nWhich continent experienced the largest decline in median fertility between 1960 and 2015?\nDid any continent show an increase in fertility rate during this period?\nWhich continent shows the most homogeneous change (smallest IQR for change)?\nAre there any countries that bucked the global trend and increased their fertility rates?\nCompare the variability (IQR) in fertility rates in 1960 vs 2015 for Africa. What does this tell us?\n\n\n\nAnswers with Statistical Calculations\n\n# Summary statistics for 1960 and 2015\ncomparison_stats &lt;- data_comparison %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    n = n(),\n    median = median(fertility),\n    Q1 = quantile(fertility, 0.25),\n    Q3 = quantile(fertility, 0.75),\n    IQR = IQR(fertility),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(continent, year)\n\nprint(comparison_stats)\n\n# A tibble: 10 × 7\n   continent  year     n median    Q1    Q3   IQR\n   &lt;fct&gt;     &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Africa     1960    51   6.69  6.16  6.98 0.815\n 2 Africa     2015    51   4.55  3.23  5.06 1.82 \n 3 Americas   1960    36   6.5   4.78  6.79 2.01 \n 4 Americas   2015    35   2.14  1.87  2.42 0.555\n 5 Asia       1960    47   6.3   5.62  6.96 1.34 \n 6 Asia       2015    47   2.18  1.79  2.83 1.04 \n 7 Europe     1960    39   2.63  2.34  3.10 0.765\n 8 Europe     2015    39   1.55  1.45  1.78 0.33 \n 9 Oceania    1960    12   6.42  5.55  7.01 1.46 \n10 Oceania    2015    12   3.04  2.07  3.67 1.60 \n\n\n\n# Summary statistics for changes\nchange_stats &lt;- fertility_change %&gt;%\n  group_by(continent) %&gt;%\n  summarise(\n    n = n(),\n    median_change = median(change),\n    Q1_change = quantile(change, 0.25),\n    Q3_change = quantile(change, 0.75),\n    IQR_change = IQR(change),\n    min_change = min(change),\n    max_change = max(change),\n    median_1960 = median(year_1960),\n    median_2015 = median(year_2015)\n  ) %&gt;%\n  arrange(median_change)\n\nprint(change_stats)\n\n# A tibble: 5 × 10\n  continent     n median_change Q1_change Q3_change IQR_change min_change\n  &lt;fct&gt;     &lt;int&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Asia         47         -4.17     -4.58    -3.18        1.40      -5.16\n2 Americas     35         -3.7      -4.56    -2.96        1.61      -5.54\n3 Oceania      12         -3.62     -3.78    -2.58        1.2       -4.06\n4 Africa       51         -2.14     -3.36    -1.22        2.14      -5.28\n5 Europe       39         -0.95     -1.60    -0.765       0.83      -4.41\n# ℹ 3 more variables: max_change &lt;dbl&gt;, median_1960 &lt;dbl&gt;, median_2015 &lt;dbl&gt;\n\n\n\n# Identify countries with increasing fertility\nincreasing_fertility &lt;- fertility_change %&gt;%\n  filter(change &gt; 0) %&gt;%\n  arrange(desc(change)) %&gt;%\n  select(country, continent, year_1960, year_2015, change, percent_change)\n\nprint(increasing_fertility)\n\n# A tibble: 3 × 6\n  country continent year_1960 year_2015 change percent_change\n  &lt;fct&gt;   &lt;fct&gt;         &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n1 Niger   Africa         7.05      7.51  0.46            6.52\n2 Mali    Africa         6.7       6.81  0.110           1.64\n3 Gambia  Africa         5.57      5.67  0.100           1.80\n\n\n\nAnswer Key:\n1. Highest median fertility rate in 1960: Africa (median ≈ 6.6 births per woman), though Asia and Americas were also quite high (around 5.5-6.0).\n2. Largest decline in median fertility: Asia experienced the most dramatic decline, with median fertility dropping from approximately 5.7 in 1960 to 2.2 in 2015 (decline of about 3.5 births per woman). Africa showed the smallest median decline (from ~6.6 to ~4.7).\n3. Increasing fertility rates: Yes, a few countries show positive changes, though these are outliers. The table above lists specific countries, but generally these are rare exceptions to the global fertility decline.\n4. Most homogeneous change: Europe shows the smallest IQR for change (approximately 1.0), indicating that European countries experienced similar patterns of fertility decline. Africa shows the largest IQR for change (approximately 2.5), indicating highly heterogeneous experiences across African countries.\n5. Countries bucking the trend: The increasing_fertility table shows specific countries. These are typically small island nations or countries recovering from conflicts, but they represent a tiny minority of cases.\n6. Africa’s variability comparison:\n\n1960: Africa had IQR ≈ 2.0, showing moderate variability\n2015: Africa has IQR ≈ 2.4, showing slightly increased variability\nInterpretation: Despite overall fertility decline, African countries have become more heterogeneous over time, suggesting different speeds of demographic transition. Some African countries (e.g., South Africa, Tunisia) have completed or nearly completed their demographic transition, while others (e.g., Niger, Somalia) maintain very high fertility rates.\n\n\n\nInterpretation:\nThe temporal comparison reveals several important patterns:\n\nUniversal decline: Nearly all continents experienced substantial fertility declines, reflecting the global demographic transition.\nVarying speeds: Asia’s dramatic decline (median drop of ~3.5) contrasts with Africa’s more modest decline (median drop of ~1.9), suggesting different stages and speeds of demographic transition.\nConvergence in some regions: Europe and Americas show narrower distributions in 2015 compared to 1960, indicating convergence toward replacement-level fertility.\nDivergence in Africa: Unlike other continents, Africa’s distribution has become wider, not narrower, indicating that some countries have rapidly reduced fertility while others have changed little.\nBelow-replacement fertility: Europe’s 2015 median (1.5) is below the replacement rate of 2.1, raising questions about population aging and sustainability.\n\n\nNote: This exercise uses the dslabs package which contains extended gapminder data including fertility rates. If you’re using the standard gapminder package, you’ll need to install dslabs first: install.packages(\"dslabs\").",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html",
    "href": "rozdzial5.html",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "",
    "text": "6.1 Wprowadzenie\nStatystyka opisowa jednej zmiennej (univariate descriptive statistics) pomaga nam podsumować i zrozumieć cechy pojedynczej zmiennej. Gdy zbieramy dane, często mamy wiele obserwacji, które są trudne do interpretacji bez jakiejś formy podsumowania. Statystyka opisowa pozwala nam:\nW tym rozdziale zbadamy fundamentalne narzędzia do opisywania danych numerycznych, zaczynając od podstawowej notacji i przechodząc przez miary tendencji centralnej, rozproszenia oraz techniki wizualizacji.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wprowadzenie",
    "href": "rozdzial5.html#wprowadzenie",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "",
    "text": "Zidentyfikować “typową” lub “centralną” wartość w naszych danych\nZrozumieć, jak bardzo rozproszone lub zróżnicowane są dane\nWykryć nietypowe obserwacje (outliers)\nSkutecznie komunikować wyniki innym",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#przegląd-cztery-typy-miar-opisowych",
    "href": "rozdzial5.html#przegląd-cztery-typy-miar-opisowych",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.2 Przegląd: cztery typy miar opisowych",
    "text": "6.2 Przegląd: cztery typy miar opisowych\nStatystykę opisową jednej zmiennej można podzielić na cztery główne kategorie, z których każda odpowiada na inne pytanie dotyczące naszych danych:\n\n1. Miary tendencji centralnej (Measures of Central Tendency)\nPytanie: Jaka jest “typowa” lub “przeciętna” wartość?\nPowszechne miary:\n\nŚrednia (mean) - średnia arytmetyczna\nMediana (median) - wartość środkowa\nModa (mode) - wartość najczęstsza\n\nCel: Te miary pomagają nam zidentyfikować, gdzie znajduje się “centrum” danych. Dostarczają pojedynczej wartości reprezentującej cały zbiór danych.\n\n\n2. Miary zmienności (rozproszenia) (Measures of Variability/Dispersion)\nPytanie: Jak bardzo rozproszone są dane? Jak bardzo obserwacje różnią się od siebie?\nPowszechne miary:\n\nRozstęp (range) - różnica między maksimum a minimum\nWariancja (variance) - średnie kwadratowe odchylenie od średniej\nOdchylenie standardowe (standard deviation) - pierwiastek kwadratowy z wariancji\nRozstęp międzykwartylowy (interquartile range, IQR) - rozrzut środkowych 50%\n\nCel: Te miary informują nas, czy obserwacje skupiają się blisko centrum, czy są szeroko rozproszone. Dwa zbiory danych mogą mieć tę samą średnią, ale bardzo różną zmienność.\n\n\n3. Miary pozycji względnej (Measures of Relative Position/Standing)\nPytanie: Gdzie dana obserwacja znajduje się względem innych? Jaka proporcja danych znajduje się poniżej danej wartości?\nPowszechne miary:\n\nKwantyle (quantiles) - pojęcie ogólne\nPercentyle (percentiles) - dzielą dane na 100 części\nKwartyle (quartiles) - dzielą dane na 4 części\nWyniki znormalizowane (standardized scores, z-scores)\n\nCel: Te miary pomagają nam zrozumieć pozycję poszczególnych obserwacji w całym rozkładzie. Odpowiadają na pytania typu “Jak wynik tego studenta wypada w porównaniu z innymi?”\n\n\n4. Miary kształtu rozkładu (Measures of Shape)\nPytanie: Jaka jest ogólna forma lub wzorzec rozkładu?\nPowszechne miary:\n\nSkośność (skewness) - asymetria, czy jest długi ogon po jednej stronie?\nKurtoza (kurtosis) - “ciężkość” ogonów, czy jest wiele wartości ekstremalnych?\nModalność (modality) - liczba szczytów (jednomodalny, dwumodalny, wielomodalny)\n\nCel: Te miary opisują ogólny wzorzec rozkładu. Choć nie będziemy ich obliczać numerycznie w tym kursie, będziemy identyfikować cechy kształtu wizualnie, używając histogramów i wykresów pudełkowych.\nZwiązek między kategoriami:\nTe cztery typy miar uzupełniają się wzajemnie. Kompletny opis danych jednej zmiennej zazwyczaj obejmuje:\n\nCo najmniej jedną miarę tendencji centralnej (średnią lub medianę)\nCo najmniej jedną miarę zmienności (odchylenie standardowe lub IQR)\nWizualizacje (histogram, wykres pudełkowy) pokazujące kształt\nInformacje o pozycji względnej przy porównywaniu konkretnych obserwacji\n\nW kolejnych sekcjach zbadamy każdą z tych kategorii szczegółowo, zaczynając od notacji matematycznej potrzebnej do precyzyjnego wyrażania tych pojęć.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#skale-pomiarowe-i-odpowiednie-statystyki",
    "href": "rozdzial5.html#skale-pomiarowe-i-odpowiednie-statystyki",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.3 Skale pomiarowe i odpowiednie statystyki",
    "text": "6.3 Skale pomiarowe i odpowiednie statystyki\nZanim zagłębimy się w konkretne miary statystyczne, ważne jest zrozumienie, że nie wszystkie statystyki są odpowiednie dla wszystkich typów danych. Typ skali pomiarowej określa, które miary statystyczne możemy sensownie stosować.\n\nCztery skale pomiarowe\n1. Skala nominalna (Nominal Scale)\nKategorie bez inherentnej kolejności (np. płeć, kraj, przynależność partyjna, kolor oczu).\nWłaściwości: Kategorie są różne, ale nie możemy powiedzieć, że jedna jest “większa” od drugiej.\n2. Skala porządkowa (Ordinal Scale)\nKategorie z sensowną kolejnością, ale odstępy między kategoriami niekoniecznie są równe (np. poziom wykształcenia: podstawowe/średnie/licencjat/magisterium/doktorat; odpowiedzi w ankiecie: zdecydowanie się nie zgadzam/nie zgadzam/neutralny/zgadzam się/zdecydowanie się zgadzam).\nWłaściwości: Możemy uszeregować obserwacje, ale nie możemy skwantyfikować różnicy między rangami.\n3. Skala interwałowa (Interval Scale)\nSkala numeryczna z równymi odstępami, ale bez prawdziwego zera (np. temperatura w Celsjuszu lub Fahrenheita, lata kalendarzowe).\nWłaściwości: Różnice są znaczące (20°C do 30°C to taka sama zmiana jak 30°C do 40°C), ale stosunki nie (20°C nie jest “dwa razy cieplejsze” niż 10°C).\n4. Skala ilorazowa (Ratio Scale)\nSkala numeryczna z równymi odstępami I prawdziwym zerem (np. wzrost, waga, dochód, wiek, dystans).\nWłaściwości: Zarówno różnice, jak i stosunki są znaczące (20 kg jest dwa razy cięższe niż 10 kg).\n\n\nKtóre statystyki dla których skal?\n\n\n\n\n\n\n\n\n\n\nMiara\nNominalna\nPorządkowa\nInterwałowa\nIlorazowa\n\n\n\n\nModa\n✓\n✓\n✓\n✓\n\n\nMediana\n✗\n✓\n✓\n✓\n\n\nŚrednia\n✗\n✗\n✓\n✓\n\n\nRozstęp\n✗\n✗\n✓\n✓\n\n\nWariancja/Odch. stand.\n✗\n✗\n✓\n✓\n\n\nIQR\n✗\n✓\n✓\n✓\n\n\nKwantyle/Percentyle\n✗\n✓\n✓\n✓\n\n\n\nKluczowe spostrzeżenia:\n\nDane nominalne: Tylko moda ma sens. Możemy liczyć częstości, ale nie możemy obliczać średnich czy median.\nDane porządkowe: Mediana i IQR są odpowiednie, ponieważ wymagają tylko uporządkowania. Średnia nie jest odpowiednia, bo zakłada równe odstępy.\nDane interwałowe/ilorazowe: Wszystkie miary są odpowiednie. To najbardziej elastyczne skale pomiarowe.\n\nPrzykład ilustrujący, dlaczego skala ma znaczenie:\nRozważmy poziomy wykształcenia: podstawowe (1), średnie (2), licencjat (3), magisterium (4), doktorat (5).\n\nModa: Poprawna - “Większość respondentów ma licencjat”\nMediana: Poprawna - “Mediana poziomu wykształcenia to licencjat”\nŚrednia: Problematyczna - “Średni poziom wykształcenia to 3,2” jest trudna do interpretacji, ponieważ odstępy między poziomami nie są równe (różnica między podstawowym a średnim nie jest taka sama jak różnica między magisterium a doktoratem)\n\nW tym rozdziale skupimy się głównie na danych interwałowych i ilorazowych, gdzie wszystkie miary statystyczne są odpowiednie. Należy jednak pamiętać o tych rozróżnieniach podczas pracy z różnymi typami danych w praktyce.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#notacja-sigma",
    "href": "rozdzial5.html#notacja-sigma",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.4 Notacja sigma",
    "text": "6.4 Notacja sigma\nZanim zagłębimy się w miary statystyczne, musimy zrozumieć notację sigma (sigma notation), która dostarcza zwartego sposobu wyrażania sumy wielu wartości.\nGrecka litera \\Sigma (duża sigma) oznacza “suma”. Gdy piszemy:\n\\sum_{i=1}^{n} x_i\nOznacza to: “Zsumuj wszystkie wartości x od pierwszej obserwacji (i=1) do ostatniej obserwacji (i=n).”\nRozbicie notacji:\n\n\\Sigma = operator sumowania (“zsumuj to”)\ni=1 (poniżej \\Sigma) = zacznij od pierwszej obserwacji\nn (powyżej \\Sigma) = kontynuuj do n-tej obserwacji\nx_i = wartość zmiennej x dla obserwacji i\n\nPrzykład:\nZałóżmy, że mamy pięć obserwacji: x_1 = 2, x_2 = 5, x_3 = 3, x_4 = 8, x_5 = 7\nWtedy:\n\\sum_{i=1}^{5} x_i = x_1 + x_2 + x_3 + x_4 + x_5 = 2 + 5 + 3 + 8 + 7 = 25\nPrzydatne właściwości notacji sigma:\n\\sum_{i=1}^{n} (x_i + y_i) = \\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} y_i\n\\sum_{i=1}^{n} c \\cdot x_i = c \\sum_{i=1}^{n} x_i \\text{ (gdzie } c \\text{ jest stałą)}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#rozkład-danych-i-rozkład-częstości",
    "href": "rozdzial5.html#rozkład-danych-i-rozkład-częstości",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.5 Rozkład danych i rozkład częstości",
    "text": "6.5 Rozkład danych i rozkład częstości\n\nCzym jest rozkład danych?\nRozkład danych (data distribution) odnosi się do tego, jak wartości w zbiorze danych rozkładają się w możliwym zakresie wartości. Zrozumienie rozkładu pomaga nam zobaczyć wzorce, zidentyfikować typowe wartości i wykryć nietypowe obserwacje.\nGdy mówimy o rozkładzie, pytamy: “Jakie wartości występują w naszych danych i jak często pojawia się każda wartość (lub zakres wartości)?”\n\n\nRozkład częstości\nRozkład częstości (frequency distribution) organizuje dane, pokazując, ile razy występuje każda wartość (lub zakres wartości). Może być przedstawiony jako:\n\nTabela pokazująca wartości i ich liczności\nRozkład częstości względnych (pokazujący proporcje lub procenty)\nRozkład częstości skumulowanych (pokazujący sumy narastające)\n\nPrzykład:\nZałóżmy, że zapytaliśmy 20 studentów, ile książek przeczytali w zeszłym miesiącu:\n\nbooks &lt;- c(2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8, 8, 8, 9, 10)\n\n# Utwórz tabelę częstości\nfreq_table &lt;- as.data.frame(table(books))\nnames(freq_table) &lt;- c(\"Przeczytane_ksiazki\", \"Czestość\")\nfreq_table$Czestość_wzgledna &lt;- freq_table$Czestość / sum(freq_table$Czestość)\nfreq_table$Czestość_skumulowana &lt;- cumsum(freq_table$Czestość)\n\nkable(freq_table, \n      col.names = c(\"Przeczytane książki\", \"Częstość\", \"Częstość względna\", \"Częstość skumulowana\"),\n      caption = \"Rozkład częstości przeczytanych książek\")\n\n\nRozkład częstości przeczytanych książek\n\n\n\n\n\n\n\n\nPrzeczytane książki\nCzęstość\nCzęstość względna\nCzęstość skumulowana\n\n\n\n\n2\n1\n0.05\n1\n\n\n3\n2\n0.10\n3\n\n\n4\n3\n0.15\n6\n\n\n5\n4\n0.20\n10\n\n\n6\n3\n0.15\n13\n\n\n7\n2\n0.10\n15\n\n\n8\n3\n0.15\n18\n\n\n9\n1\n0.05\n19\n\n\n10\n1\n0.05\n20\n\n\n\n\n\nInterpretacja:\n\nCzęstość: Liczba studentów, którzy przeczytali daną liczbę książek\nCzęstość względna: Proporcja wszystkich studentów (np. 0,20 = 20% przeczytało 5 książek)\nCzęstość skumulowana: Suma narastająca (np. 10 studentów przeczytało 5 lub mniej książek)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-tendencji-centralnej",
    "href": "rozdzial5.html#miary-tendencji-centralnej",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.6 Miary tendencji centralnej",
    "text": "6.6 Miary tendencji centralnej\nMiary tendencji centralnej informują nas o “typowej” lub “centralnej” wartości w zbiorze danych. Trzy najpowszechniejsze miary to średnia, mediana i moda.\nStosowne skale pomiarowe:\n\nModa: Nominalna, Porządkowa, Interwałowa, Ilorazowa\nMediana: Porządkowa, Interwałowa, Ilorazowa (wymaga uporządkowania)\nŚrednia: Interwałowa, Ilorazowa (wymaga równych odstępów i operacji arytmetycznych)\n\n\nŚrednia (średnia arytmetyczna)\nŚrednia (mean) to suma wszystkich wartości podzielona przez liczbę obserwacji. Reprezentuje średnią arytmetyczną.\nStosowna dla: Tylko danych interwałowych i ilorazowych\nWzór:\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\ngdzie \\bar{x} (czytane “x z kreską”) jest średnią z próby, a n jest liczbą obserwacji.\nPrzykład:\nJeśli mamy wyniki testów: 75, 82, 88, 90, 95\n\\bar{x} = \\frac{75 + 82 + 88 + 90 + 95}{5} = \\frac{430}{5} = 86\nProstym językiem: Średnia to wartość, którą otrzymalibyśmy, gdybyśmy mogli redystrybuować wszystkie wartości równo między wszystkie obserwacje. Jeśli pięciu studentów uzyskało łącznie 430 punktów, każdy otrzymałby 86 punktów, gdybyśmy podzielili sumę równo.\n\n\nŚrednia jako punkt równowagi\nJednym z najważniejszych sposobów pojęciowego zrozumienia średniej jest traktowanie jej jako punktu równowagi (balancing point) lub środka ciężkości (center of gravity) danych.\nWyobraźmy sobie umieszczenie ciężarków na huśtawce w pozycjach odpowiadających naszym wartościom danych. Średnia to punkt, w którym huśtawka byłaby idealnie wyważona.\nMatematyczny wgląd:\nSuma odchyleń od średniej zawsze równa się zeru:\n\\sum_{i=1}^{n} (x_i - \\bar{x}) = 0\nOznacza to, że odchylenia dodatnie (wartości powyżej średniej) dokładnie znoszą odchylenia ujemne (wartości poniżej średniej).\nWizualizacja:\n\n# Przykładowe dane\ndata_points &lt;- c(2, 4, 5, 8, 11)\nmean_val &lt;- mean(data_points)\n\n# Ustaw obszar wykresu\npar(mar = c(4, 2, 3, 2))\nplot(c(0, 13), c(0, 2.5), type = \"n\", xlab = \"Wartość\", ylab = \"\",\n     main = \"Średnia jako punkt równowagi\", yaxt = \"n\", bty = \"n\")\n\n# Narysuj huśtawkę (punkt podparcia na średniej)\nsegments(0, 1, 13, 1, lwd = 4, col = \"brown\")\npolygon(c(mean_val - 0.3, mean_val + 0.3, mean_val), \n        c(0.7, 0.7, 0.3), \n        col = \"gray30\", border = \"black\", lwd = 2)\n\n# Zaznacz punkty danych jako ciężarki na huśtawce\npoints(data_points, rep(1, length(data_points)), \n       pch = 19, cex = 3, col = \"darkblue\")\n\n# Dodaj linie pionowe pokazujące odległości od średniej\nfor(i in 1:length(data_points)) {\n  segments(data_points[i], 1, mean_val, 1, \n           lty = 2, col = \"red\", lwd = 1.5)\n}\n\n# Zaznacz średnią\npoints(mean_val, 1, pch = 17, cex = 4, col = \"red\")\ntext(mean_val, 0.4, paste(\"Średnia =\", round(mean_val, 1)), \n     col = \"red\", cex = 1.5, font = 2)\n\n# Dodaj etykiety wartości\ntext(data_points, rep(1.4, length(data_points)), \n     as.character(data_points), cex = 1.2, font = 2)\n\n# Uproszczone podsumowanie poniżej\ndeviations &lt;- data_points - mean_val\ntext(6.5, 2.2, \n     sprintf(\"Suma odchyleń od średniej = %.1f\", sum(deviations)),\n     cex = 1.1)\n\n\n\n\n\n\n\n\nInterpretacja: Średnia (6) działa jako punkt podparcia. Punkt danych na 2 jest 4 jednostki poniżej średniej, podczas gdy punkt na 11 jest 5 jednostek powyżej. Odchylenia ujemne i dodatnie się równoważą, czyniąc średnią idealnym punktem równowagi.\nDlaczego to ma znaczenie: Ta właściwość wyjaśnia, dlaczego średnia jest wrażliwa na wartości ekstremalne (outliers). Pojedyncza bardzo duża lub bardzo mała wartość może “pociągnąć” średnią w swoim kierunku, tak jak ciężki ciężarek daleko od punktu podparcia przechyliłby huśtawkę.\n\n\nMediana\nMediana (median) to wartość środkowa, gdy dane są uporządkowane. Dzieli zbiór danych na dwie równe połowy, po 50% obserwacji.\nStosowna dla: Danych porządkowych, interwałowych i ilorazowych (wymaga uporządkowania)\nJak znaleźć medianę:\n\nUporządkuj dane od najmniejszej do największej\nJeśli n jest nieparzyste: mediana to wartość środkowa na pozycji \\frac{n+1}{2}\nJeśli n jest parzyste: mediana to średnia z dwóch wartości środkowych na pozycjach \\frac{n}{2} i \\frac{n}{2} + 1\n\nPrzykład z nieparzystym n:\nDane: 3, 7, 8, 12, 15 (już uporządkowane, n = 5)\nPozycja mediany: \\frac{5+1}{2} = 3\nMediana = 8 (trzecia wartość)\nPrzykład z parzystym n:\nDane: 3, 7, 8, 12, 15, 20 (już uporządkowane, n = 6)\nDwie wartości środkowe są na pozycjach 3 i 4: 8 i 12\n\\text{Mediana} = \\frac{8 + 12}{2} = 10\nProstym językiem: Mediana to wartość, która znajduje się pośrodku, gdy ustawimy wszystkie obserwacje od najmniejszej do największej. Połowa obserwacji jest mniejsza od mediany, a połowa większa.\nKluczowa zaleta: Mediana jest odporna na wartości odstające (resistant to outliers). Wartości ekstremalne nie wpływają na nią znacząco, co czyni ją użyteczną, gdy dane zawierają wartości odstające.\n\n\nModa\nModa (mode) to wartość, która pojawia się najczęściej w zbiorze danych.\nStosowna dla: Danych nominalnych, porządkowych, interwałowych i ilorazowych (wszystkie skale)\nWłaściwości:\n\nZbiór danych może nie mieć mody (wszystkie wartości występują równie często)\nZbiór danych może mieć jedną modę (jednomodalny)\nZbiór danych może mieć wiele mod (dwumodalny, wielomodalny)\n\nPrzykład:\nDane: 2, 3, 3, 5, 5, 5, 7, 8, 8\nModa = 5 (występuje trzy razy, więcej niż jakakolwiek inna wartość)\nProstym językiem: Moda informuje nas, która wartość jest najczęstsza lub najbardziej typowa w zbiorze danych. Jest szczególnie użyteczna dla danych kategorycznych (np. “niebieski” to najczęstszy kolor oczu w próbie).\n\n\nPorównanie średniej, mediany i modalnej\n\n# Generowanie rozkładów\nset.seed(42)\nleft_skew &lt;- rbeta(2000, 8, 2) * 50 + 30\nsymmetric &lt;- rnorm(2000, 55, 8)\nright_skew &lt;- rbeta(2000, 2, 8) * 50 + 30\n\n# Funkcja do estymacji modalnej przy użyciu gęstości\nget_mode &lt;- function(x) {\n  d &lt;- density(x)\n  d$x[which.max(d$y)]\n}\n\n# Układ pionowy\npar(mfrow = c(3, 1), mar = c(4, 4, 3, 1))\n\n# Skośność lewostronna\nhist(left_skew, main = \"Rozkład lewostronnie skośny\", \n     xlab = \"Wartość\", col = \"lightblue\", breaks = 30, \n     xlim = c(25, 85))\nabline(v = mean(left_skew), col = \"red\", lwd = 2.5, lty = 1)\nabline(v = median(left_skew), col = \"blue\", lwd = 2.5, lty = 2)\nabline(v = get_mode(left_skew), col = \"darkgreen\", lwd = 2.5, lty = 3)\nlegend(\"topleft\", \n       legend = c(sprintf(\"Średnia (%.1f)\", mean(left_skew)),\n                  sprintf(\"Mediana (%.1f)\", median(left_skew)),\n                  sprintf(\"Modalna (%.1f)\", get_mode(left_skew))),\n       col = c(\"red\", \"blue\", \"darkgreen\"), \n       lty = c(1, 2, 3), lwd = 2.5, cex = 0.9)\n\n# Rozkład symetryczny\nhist(symmetric, main = \"Rozkład symetryczny\", \n     xlab = \"Wartość\", col = \"lightgreen\", breaks = 30,\n     xlim = c(25, 85))\nabline(v = mean(symmetric), col = \"red\", lwd = 2.5, lty = 1)\nabline(v = median(symmetric), col = \"blue\", lwd = 2.5, lty = 2)\nabline(v = get_mode(symmetric), col = \"darkgreen\", lwd = 2.5, lty = 3)\nlegend(\"topleft\", \n       legend = c(sprintf(\"Średnia (%.1f)\", mean(symmetric)),\n                  sprintf(\"Mediana (%.1f)\", median(symmetric)),\n                  sprintf(\"Modalna (%.1f)\", get_mode(symmetric))),\n       col = c(\"red\", \"blue\", \"darkgreen\"), \n       lty = c(1, 2, 3), lwd = 2.5, cex = 0.9)\n\n# Skośność prawostronna\nhist(right_skew, main = \"Rozkład prawostronnie skośny\", \n     xlab = \"Wartość\", col = \"lightyellow\", breaks = 30,\n     xlim = c(25, 85))\nabline(v = mean(right_skew), col = \"red\", lwd = 2.5, lty = 1)\nabline(v = median(right_skew), col = \"blue\", lwd = 2.5, lty = 2)\nabline(v = get_mode(right_skew), col = \"darkgreen\", lwd = 2.5, lty = 3)\nlegend(\"topright\", \n       legend = c(sprintf(\"Średnia (%.1f)\", mean(right_skew)),\n                  sprintf(\"Mediana (%.1f)\", median(right_skew)),\n                  sprintf(\"Modalna (%.1f)\", get_mode(right_skew))),\n       col = c(\"red\", \"blue\", \"darkgreen\"), \n       lty = c(1, 2, 3), lwd = 2.5, cex = 0.9)\n\n\n\n\n\n\n\n\nKluczowe wnioski:\n\nW rozkładach symetrycznych średnia ≈ mediana ≈ modalna\nW rozkładach prawostronnie skośnych (długi ogon po prawej), średnia &gt; mediana &gt; modalna\nW rozkładach lewostronnie skośnych (długi ogon po lewej), średnia &lt; mediana &lt; modalna\n\nŚrednia jest przesuwana w kierunku ogona, mediana pozostaje w środku, a modalna znajduje się w szczycie rozkładu.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-zmienności-rozproszenia-measures-of-dispersion-variability",
    "href": "rozdzial5.html#miary-zmienności-rozproszenia-measures-of-dispersion-variability",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.7 Miary Zmienności (Rozproszenia) / Measures of Dispersion (Variability)",
    "text": "6.7 Miary Zmienności (Rozproszenia) / Measures of Dispersion (Variability)\nPodczas gdy miary tendencji centralnej (measures of central tendency) informują nas o typowych wartościach, miary zmienności (measures of dispersion/variability) pokazują, jak bardzo dane są rozproszone. Dwa zbiory danych mogą mieć tę samą średnią, ale bardzo różne rozproszenie.\nSkale pomiarowe, do których stosujemy:\n\nRozstęp (Range): Przedziałowa, Ilorazowa\nWariancja/Odchylenie standardowe (Variance/Standard Deviation): Przedziałowa, Ilorazowa\nIQR: Porządkowa, Przedziałowa, Ilorazowa (wymaga uporządkowania)\n\n\nRozstęp / Range\nRozstęp (range) to najprostsza miara zmienności: różnica między wartością maksymalną i minimalną.\nStosuje się do: Danych przedziałowych i ilorazowych\n\\text{Rozstęp} = \\max(x) - \\min(x)\nPrzykład:\nZbiór A: 10, 12, 15, 18, 20 → Rozstęp = 20 - 10 = 10\nZbiór B: 10, 15, 15, 15, 20 → Rozstęp = 20 - 10 = 10\nProstym językiem: Rozstęp mówi nam o zakresie od najmniejszej do największej obserwacji. Daje szybkie wyobrażenie o rozproszeniu, ale jest bardzo wrażliwy na wartości odstające (outliers) (ponieważ wykorzystuje tylko dwie wartości).\n\n\nWizualizacja Zmienności: Przykład Roboczy\nAby zrozumieć wariancję i odchylenie standardowe, przeanalizujmy kompletny przykład z danymi X = (2, 2, 3, 4, 5, 5). Potraktujemy je jako sekwencyjne obserwacje (jak pomiary w czasie), aby zwizualizować, jak poszczególne wartości odstają od swojej średniej.\n\nKrok 1: Oblicz Średnią\n\\bar{x} = \\frac{2 + 2 + 3 + 4 + 5 + 5}{6} = \\frac{21}{6} = 3.5\n\n\nKrok 2: Zwizualizuj Odchylenia od Średniej\n\nX &lt;- c(2, 2, 3, 4, 5, 5)\nmean_X &lt;- mean(X)\n\nplot(X, type = \"b\", pch = 19, col = \"darkblue\", \n     xlab = \"Obserwacja\", ylab = \"Wartość\",\n     main = \"Odchylenia od średniej\",\n     ylim = c(0, 6))\nabline(h = mean_X, col = \"red\", lwd = 2, lty = 2)\ntext(1, mean_X + 0.3, paste(\"Średnia =\", mean_X), col = \"red\", pos = 4)\n\n# Dodaj linie odchyleń\nfor(i in 1:length(X)) {\n  segments(i, mean_X, i, X[i], col = \"gray\", lty = 3)\n}\n\n\n\n\n\n\n\n\nSzare przerywane linie pokazują odchylenia (deviations) od średniej (mean). Wariancja będzie mierzyć średnią kwadratową długość tych odchyleń.\n\n\n\nDlaczego Podnosimy Odchylenia do Kwadratu?\nTo jedno z najważniejszych pytań koncepcyjnych w statystyce. Dlaczego nie uśrednić po prostu odchyleń (x_i - \\bar{x}) bezpośrednio?\nProblem: Gdybyśmy po prostu uśrednili odchylenia, zawsze sumowałyby się do zera (jak widzieliśmy przy właściwości punktu równowagi). Zweryfikujmy to na naszym przykładzie:\n\n\n\nx_i\nx_i - \\bar{x}\n\n\n\n\n2\n-1.5\n\n\n2\n-1.5\n\n\n3\n-0.5\n\n\n4\n0.5\n\n\n5\n1.5\n\n\n5\n1.5\n\n\nSuma\n0.0\n\n\n\nDzieje się tak, ponieważ odchylenia dodatnie (powyżej średniej) dokładnie znoszą odchylenia ujemne (poniżej średniej):\n\\sum_{i=1}^{n}(x_i - \\bar{x}) = 0\nRozwiązania tego problemu:\n\nPodnieść odchylenia do kwadratu (podejście wariancji): (x_i - \\bar{x})^2 sprawia, że wszystkie wartości są dodatnie\nWziąć wartości bezwzględne (absolute values) (podejście alternatywne): |x_i - \\bar{x}| również sprawia, że wszystkie wartości są dodatnie\n\nDlaczego podnoszenie do kwadratu jest preferowane:\n\nWygoda matematyczna: Podnoszenie do kwadratu ma lepsze właściwości matematyczne dla teorii statystycznej\nUwypuklenie większych odchyleń: Podnoszenie do kwadratu daje większą wagę ekstremalnym odchyleniom (punkt odległy o 10 jednostek wnosi 100 do wariancji, podczas gdy punkt odległy o 5 jednostek wnosi tylko 25)\n\nProblem jednostek: Ponieważ podnieśliśmy odchylenia do kwadratu, wariancja jest w jednostkach kwadratowych (squared units). Jeśli nasze dane są w centymetrach, wariancja jest w centymetrach kwadratowych. Dlatego często preferujemy odchylenie standardowe.\n\n\nWariancja / Variance\nWariancja (variance) mierzy średnie kwadratowe odchylenie od średniej. Określa, jak daleko średnio każda obserwacja jest od średniej.\nStosuje się do: Danych przedziałowych i ilorazowych\n\nObliczenia Ręczne: Wariancja Populacji vs. Próby\nKontynuujmy nasz przykład X = (2, 2, 3, 4, 5, 5) i obliczmy odchylenia kwadratowe (squared deviations):\n\n\n\nx_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\n\n\n\n\n2\n-1.5\n2.25\n\n\n2\n-1.5\n2.25\n\n\n3\n-0.5\n0.25\n\n\n4\n0.5\n0.25\n\n\n5\n1.5\n2.25\n\n\n5\n1.5\n2.25\n\n\nSuma\n\n9.5\n\n\n\nWariancja populacji (population variance) (gdyby to była cała populacja):\n\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n} = \\frac{9.5}{6} \\approx 1.583\n\n# Wariancja populacji (bez korekty Bessela)\nvar_pop &lt;- sum((X - mean_X)^2) / length(X)\ncat(\"Wariancja populacji:\", var_pop, \"\\n\")\n\nWariancja populacji: 1.583 \n\n\n\n\nKorekta Bessela: Dlaczego n-1? / Bessel’s Correction: Why n-1?\nProblem: Gdy używamy danych z próby do oszacowania wariancji populacji, formuła \\frac{\\sum(x_i - \\bar{x})^2}{n} ma tendencję do niedoszacowania (underestimate) prawdziwej wariancji populacji. Dzieje się tak, ponieważ \\bar{x} jest obliczana z tych samych danych, co sprawia, że odchylenia są sztucznie mniejsze (mierzymy odchylenia od średniej próby, a nie od prawdziwej średniej populacji).\nRozwiązanie: Korekta Bessela (Bessel’s correction) dostosowuje to obciążenie (bias), dzieląc przez n-1 zamiast n. To daje nieobciążony estymator (unbiased estimator) wariancji populacji.\nWzór na wariancję próby (sample variance):\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\nDla naszego przykładu:\ns^2 = \\frac{9.5}{5} = 1.9\n\n# Wariancja próby (z korektą Bessela)\nvar_sample &lt;- var(X)  # R domyślnie używa korekty Bessela\ncat(\"Wariancja próby:\", var_sample, \"\\n\")\n\nWariancja próby: 1.9 \n\n\nKluczowa różnica: Wariancja próby (1.9) jest większa niż wariancja populacji (1.583), zapewniając nieobciążony estymator przy pracy z danymi z próby.\nProstym językiem: Wariancja to średnia z kwadratów odchyleń od średniej. Większa wariancja oznacza bardziej rozproszone dane; mniejsza wariancja oznacza, że dane skupiają się bardziej wokół średniej.\n\n\nKolejny Przykład\nDane: 2, 4, 6, 8, 10\nŚrednia: \\bar{x} = \\frac{2+4+6+8+10}{5} = 6\n\n\n\nx_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\n\n\n\n\n2\n-4\n16\n\n\n4\n-2\n4\n\n\n6\n0\n0\n\n\n8\n2\n4\n\n\n10\n4\n16\n\n\nSuma\n\n40\n\n\n\ns^2 = \\frac{40}{5-1} = \\frac{40}{4} = 10\n\n\n\nOdchylenie Standardowe / Standard Deviation\nOdchylenie standardowe (standard deviation) to po prostu pierwiastek kwadratowy z wariancji. Przywraca nas do oryginalnych jednostek pomiaru.\nStosuje się do: Danych przedziałowych i ilorazowych\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\nDla naszego pierwszego przykładu:\ns = \\sqrt{1.9} \\approx 1.378\nDla naszego drugiego przykładu:\ns = \\sqrt{10} \\approx 3.16\n\n# Odchylenia standardowe\nsd_sample &lt;- sd(X)\ncat(\"Odchylenie standardowe próby (pierwszy przykład):\", sd_sample, \"\\n\")\n\nOdchylenie standardowe próby (pierwszy przykład): 1.378 \n\nX2 &lt;- c(2, 4, 6, 8, 10)\ncat(\"Odchylenie standardowe próby (drugi przykład):\", sd(X2), \"\\n\")\n\nOdchylenie standardowe próby (drugi przykład): 3.162 \n\n\nProstym językiem: Odchylenie standardowe mówi nam o typowej odległości obserwacji od średniej, w oryginalnych jednostkach pomiaru. Jest bardziej interpretowalnie niż wariancja, ponieważ jest w tych samych jednostkach co dane.\nPrzybliżona interpretacja: W wielu rozkładach (szczególnie symetrycznych, w kształcie dzwonu), około 68% obserwacji mieści się w obrębie jednego odchylenia standardowego od średniej, a około 95% mieści się w obrębie dwóch odchyleń standardowych.\n\n\nRozstęp międzykwartylowy (IQR)\nRozstęp międzykwartylowy (interquartile range, IQR) mierzy rozrzut środkowych 50% danych. Oblicza się go jako:\nStosowny dla: Danych porządkowych, interwałowych i ilorazowych (wymaga uporządkowania)\n\\text{IQR} = Q_3 - Q_1\ngdzie Q_1 to pierwszy kwartyl (25. percentyl), a Q_3 to trzeci kwartyl (75. percentyl).\nProstym językiem: IQR informuje nas o zakresie, który zawiera środkową połowę naszych danych. Jest odporny na wartości odstające, co czyni go użytecznym, gdy obecne są wartości ekstremalne.\nPrzykład:\nDane: 2, 4, 5, 7, 8, 11, 12, 15, 18, 20\n\nQ_1 = 5 (25% danych znajduje się poniżej 5)\nQ_3 = 15 (75% danych znajduje się poniżej 15)\n\\text{IQR} = 15 - 5 = 10\n\nŚrodkowe 50% obserwacji rozciąga się na 10 jednostek.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-pozycji-względnej",
    "href": "rozdzial5.html#miary-pozycji-względnej",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.8 Miary pozycji względnej",
    "text": "6.8 Miary pozycji względnej\nMiary pozycji względnej pomagają nam zrozumieć, gdzie dana obserwacja znajduje się w całym rozkładzie. Te miary odpowiadają na pytania typu “Jak ta wartość wypada w porównaniu z innymi w zbiorze danych?”\nStosowne skale pomiarowe:\n\nKwantyle/Percentyle/Kwartyle: Porządkowa, Interwałowa, Ilorazowa (wymaga uporządkowania)\n\n\nKwantyle\nKwantyl (quantile) to wartość, która dzieli zbiór danych na grupy równej wielkości. Bardziej formalnie, p-ty kwantyl to wartość, poniżej której znajduje się proporcja p danych.\nStosowny dla: Danych porządkowych, interwałowych i ilorazowych\nDefinicja:\nDla proporcji p (gdzie 0 &lt; p &lt; 1), p-ty kwantyl q_p spełnia:\n\nCo najmniej proporcja p obserwacji jest mniejsza lub równa q_p\nCo najmniej proporcja (1-p) obserwacji jest większa lub równa q_p\n\nProstym językiem: Kwantyle to “punkty odcięcia”, które dzielą nasze uporządkowane dane na segmenty. Informują nas o wartości, poniżej której znajduje się określona proporcja naszych danych.\nWażna uwaga: Kwantyle to pojęcie ogólne. Percentyle, kwartyle i mediana to wszystko konkretne typy kwantyli.\nPrzykłady kwantyli:\n\n0,5 kwantyl (50. percentyl) to mediana - połowa danych znajduje się poniżej niej\n0,25 kwantyl (25. percentyl) to pierwszy kwartyl (Q_1)\n0,75 kwantyl (75. percentyl) to trzeci kwartyl (Q_3)\n0,90 kwantyl (90. percentyl) oznacza, że 90% danych znajduje się poniżej tej wartości\n\n\n\nPercentyle\nPercentyl (percentile) to konkretny typ kwantyla, który dzieli dane na 100 równych części. k-ty percentyl to wartość, poniżej której znajduje się k procent obserwacji.\nStosowny dla: Danych porządkowych, interwałowych i ilorazowych\nZwiązek z kwantylami:\nk-ty percentyl odpowiada \\frac{k}{100} kwantylowi. Na przykład:\n\n\npercentyl = 0,25 kwantyl\n\n\npercentyl = 0,50 kwantyl = mediana\n\n\npercentyl = 0,90 kwantyl\n\n\nPrzykłady:\n\n50. percentyl to mediana (50% danych znajduje się poniżej niej)\n90. percentyl oznacza, że 90% obserwacji znajduje się poniżej tej wartości\n10. percentyl oznacza, że tylko 10% obserwacji znajduje się poniżej tej wartości\n\nInterpretacja: Jeśli zdobyłeś 85. percentyl w teście, poradziłeś sobie lepiej niż 85% zdających.\n\n\nKwartyle\nKwartyle (quartiles) to konkretne kwantyle, które dzielą dane na cztery równe części. Są to 25., 50. i 75. percentyle:\nStosowne dla: Danych porządkowych, interwałowych i ilorazowych\n\nQ_1 (Pierwszy kwartyl lub 25. percentyl): 25% danych znajduje się poniżej tej wartości\nQ_2 (Drugi kwartyl lub 50. percentyl): Mediana - 50% znajduje się poniżej\nQ_3 (Trzeci kwartyl lub 75. percentyl): 75% danych znajduje się poniżej tej wartości\n\n\n# Wizualizacja kwartyli\nset.seed(456)\nexample_data &lt;- sort(rnorm(100, 50, 10))\n\npar(mar = c(5, 4, 4, 2))\nplot(1:100, example_data, pch = 19, cex = 0.8, col = \"gray40\",\n     xlab = \"Obserwacja (uporządkowana)\", ylab = \"Wartość\",\n     main = \"Kwartyle dzielą dane na cztery równe części\")\n\n# Dodaj linie kwartyli\nabline(h = quantile(example_data, 0.25), col = \"blue\", lwd = 2, lty = 2)\nabline(h = quantile(example_data, 0.50), col = \"red\", lwd = 2, lty = 1)\nabline(h = quantile(example_data, 0.75), col = \"blue\", lwd = 2, lty = 2)\n\n# Dodaj zacienione regiony\nrect(0, min(example_data), 101, quantile(example_data, 0.25), \n     col = rgb(0, 0, 1, 0.1), border = NA)\nrect(0, quantile(example_data, 0.25), 101, quantile(example_data, 0.50), \n     col = rgb(0, 1, 0, 0.1), border = NA)\nrect(0, quantile(example_data, 0.50), 101, quantile(example_data, 0.75), \n     col = rgb(1, 1, 0, 0.1), border = NA)\nrect(0, quantile(example_data, 0.75), 101, max(example_data), \n     col = rgb(1, 0, 0, 0.1), border = NA)\n\n# Etykiety\ntext(95, quantile(example_data, 0.25), \"Q1 (25%)\", pos = 3, font = 2)\ntext(95, quantile(example_data, 0.50), \"Q2 (50%)\", pos = 3, font = 2)\ntext(95, quantile(example_data, 0.75), \"Q3 (75%)\", pos = 3, font = 2)\n\nlegend(\"topleft\", \n       legend = c(\"Każdy region zawiera 25% obserwacji\"),\n       bty = \"n\", cex = 1.1)\n\n\n\n\n\n\n\n\n\n\nObliczanie kwartyli: metoda Tukeya\nIstnieje kilka metod obliczania kwartyli. W tym kursie używamy metody Tukeya (Tukey method, zwanej również metodą zawiasów, hinges method):\nKroki:\n\nZnajdź medianę całego zbioru danych\nWyklucz medianę, jeśli n jest nieparzyste\nQ_1 = mediana dolnej połowy\nQ_3 = mediana górnej połowy\n\nPrzykład z parzystym n:\nDane: 2, 4, 5, 7, 8, 10, 12, 15 (n = 8)\nPozycja mediany: między pozycjami 4 i 5 → Mediana = \\frac{7+8}{2} = 7.5\nDolna połowa: 2, 4, 5, 7 → Q_1 = \\frac{4+5}{2} = 4.5\nGórna połowa: 8, 10, 12, 15 → Q_3 = \\frac{10+12}{2} = 11\nPrzykład z nieparzystym n:\nDane: 2, 4, 5, 7, 8, 10, 12, 15, 18 (n = 9)\nMediana: 8 (pozycja 5)\nDolna połowa (z wyłączeniem mediany): 2, 4, 5, 7 → Q_1 = \\frac{4+5}{2} = 4.5\nGórna połowa (z wyłączeniem mediany): 10, 12, 15, 18 → Q_3 = \\frac{12+15}{2} = 13.5\nUwaga: Funkcja R quantile() ma wiele metod. Metoda Tukeya odpowiada type = 2, podczas gdy domyślną w R jest type = 7.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-kształtu",
    "href": "rozdzial5.html#miary-kształtu",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.9 Miary kształtu",
    "text": "6.9 Miary kształtu\nMiary kształtu opisują ogólną formę lub wzorzec rozkładu. Chociaż zazwyczaj oceniamy kształt wizualnie za pomocą histogramów i wykresów pudełkowych, ważne jest zrozumienie kluczowych koncepcji.\n\nSkośność (asymetria)\nSkośność odnosi się do asymetrii rozkładu. Informuje nas, czy rozkład ma dłuższy ogon po jednej stronie.\nRodzaje skośności:\n\nSymetryczny (brak skośności): Rozkład wygląda tak samo po obu stronach centrum. Średnia ≈ Mediana.\nPrawostronnie skośny (skośność dodatnia): Długi ogon rozciąga się w prawo. Średnia &gt; Mediana. Większość wartości skupia się po lewej stronie, a kilka dużych wartości rozciąga ogon.\nLewostronnie skośny (skośność ujemna): Długi ogon rozciąga się w lewo. Średnia &lt; Mediana. Większość wartości skupia się po prawej stronie, a kilka małych wartości rozciąga ogon.\n\nProstym językiem: Skośność mówi nam, w którą stronę “wskazuje” ogon. Rozkłady dochodów są zazwyczaj prawostronnie skośne (większość ludzi zarabia umiarkowane dochody, ale kilka osób zarabia ekstremalnie wysokie dochody). Wyniki testów z łatwego egzaminu mogą być lewostronnie skośne (większość studentów osiąga wysokie wyniki, ale kilku osiąga bardzo niskie).\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(patchwork)\n\n# Generowanie rozkładów\nset.seed(42)\nleft_skew &lt;- rbeta(2000, 8, 2) * 50 + 30\nsymmetric &lt;- rnorm(2000, 55, 8)\nright_skew &lt;- rbeta(2000, 2, 8) * 50 + 30\n\n# Tworzenie ramki danych\ndf_skew &lt;- data.frame(\n  value = c(left_skew, symmetric, right_skew),\n  type = rep(c(\"Lewostronnie skośny\", \"Symetryczny\", \"Prawostronnie skośny\"), each = 2000)\n)\n\n# Obliczanie statystyk dla każdego rozkładu\nstats_skew &lt;- df_skew %&gt;%\n  group_by(type) %&gt;%\n  summarise(\n    mean_val = mean(value),\n    median_val = median(value),\n    mode_val = {\n      d &lt;- density(value)\n      d$x[which.max(d$y)]\n    }\n  )\n\n# Zmiana kolejności poziomów czynnika dla właściwego wyświetlania\ndf_skew$type &lt;- factor(df_skew$type, \n                       levels = c(\"Lewostronnie skośny\", \"Symetryczny\", \"Prawostronnie skośny\"))\nstats_skew$type &lt;- factor(stats_skew$type, \n                          levels = c(\"Lewostronnie skośny\", \"Symetryczny\", \"Prawostronnie skośny\"))\n\n# Tworzenie wykresu\nggplot(df_skew, aes(x = value)) +\n  geom_histogram(aes(fill = type), bins = 30, color = \"white\", alpha = 0.7) +\n  geom_vline(data = stats_skew, aes(xintercept = mean_val), \n             color = \"red\", linewidth = 1.2, linetype = \"solid\") +\n  geom_vline(data = stats_skew, aes(xintercept = median_val), \n             color = \"blue\", linewidth = 1.2, linetype = \"dashed\") +\n  geom_vline(data = stats_skew, aes(xintercept = mode_val), \n             color = \"darkgreen\", linewidth = 1.2, linetype = \"dotted\") +\n  facet_wrap(~ type, ncol = 1, scales = \"free_y\") +\n  scale_fill_manual(values = c(\"Lewostronnie skośny\" = \"lightblue\", \n                                \"Symetryczny\" = \"lightgreen\", \n                                \"Prawostronnie skośny\" = \"#FFFFE0\")) +\n  labs(x = \"Wartość\", y = \"Liczebność\", \n       title = \"Porównanie skośności: Średnia, mediana i modalna\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n        strip.text = element_text(face = \"bold\", size = 11)) +\n  geom_text(data = stats_skew, \n            aes(x = 75, y = Inf, \n                label = sprintf(\"Średnia: %.1f\\nMediana: %.1f\\nModalna: %.1f\", \n                               mean_val, median_val, mode_val)),\n            vjust = 1.5, hjust = 1, size = 3.5, \n            color = \"black\", fontface = \"bold\")\n\n\n\n\n\n\n\n\nKluczowe wnioski:\n\nW rozkładach symetrycznych średnia ≈ mediana ≈ modalna\nW rozkładach prawostronnie skośnych (długi ogon po prawej), średnia &gt; mediana &gt; modalna\nW rozkładach lewostronnie skośnych (długi ogon po lewej), średnia &lt; mediana &lt; modalna\n\nŚrednia jest przesuwana w kierunku ogona, mediana pozostaje w środku, a modalna znajduje się w szczycie rozkładu.\n\n\nKurtoza (grubość ogonów)\nKurtoza odnosi się do tego, czy rozkład ma ciężkie ogony (wiele wartości ekstremalnych) czy lekkie ogony (niewiele wartości ekstremalnych) w porównaniu z rozkładem normalnym.\nRodzaje:\n\nWysoka kurtoza: Ciężkie ogony z wieloma wartościami odstającymi; ostry szczyt w centrum\nNiska kurtoza: Lekkie ogony z niewieloma wartościami odstającymi; bardziej spłaszczony szczyt\n\nProstym językiem: Kurtoza mówi nam, czy wartości ekstremalne są częste czy rzadkie w naszych danych. Zwroty finansowe często mają wysoką kurtozę - większość dni przynosi małe zmiany, ale okazjonalnie występują bardzo duże ruchy (krachy lub wzrosty na rynku).\n\n# Generowanie rozkładów o różnej kurtozie\nset.seed(123)\n# Niska kurtoza (podobny do jednostajnego)\nlow_kurt &lt;- runif(2000, 40, 70)\n# Normalna kurtoza\nnormal_kurt &lt;- rnorm(2000, 55, 8)\n# Wysoka kurtoza (ciężkie ogony)\nhigh_kurt &lt;- rt(2000, df = 3) * 8 + 55\n\n# Tworzenie ramki danych\ndf_kurt &lt;- data.frame(\n  value = c(low_kurt, normal_kurt, high_kurt),\n  type = rep(c(\"Niska kurtoza\\n(Lekkie ogony)\", \n               \"Normalna kurtoza\", \n               \"Wysoka kurtoza\\n(Ciężkie ogony)\"), each = 2000)\n)\n\n# Zmiana kolejności poziomów czynnika\ndf_kurt$type &lt;- factor(df_kurt$type, \n                       levels = c(\"Niska kurtoza\\n(Lekkie ogony)\", \n                                 \"Normalna kurtoza\", \n                                 \"Wysoka kurtoza\\n(Ciężkie ogony)\"))\n\n# Tworzenie wykresu\nggplot(df_kurt, aes(x = value)) +\n  geom_histogram(aes(fill = type), bins = 40, color = \"white\", alpha = 0.7) +\n  facet_wrap(~ type, ncol = 1, scales = \"free_y\") +\n  scale_fill_manual(values = c(\"Niska kurtoza\\n(Lekkie ogony)\" = \"#FFB6C1\", \n                                \"Normalna kurtoza\" = \"#98FB98\", \n                                \"Wysoka kurtoza\\n(Ciężkie ogony)\" = \"#FFD700\")) +\n  labs(x = \"Wartość\", y = \"Liczebność\", \n       title = \"Porównanie kurtozy: Zachowanie ogonów\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n        strip.text = element_text(face = \"bold\", size = 11)) +\n  coord_cartesian(xlim = c(20, 90))\n\n\n\n\n\n\n\n\nKluczowe wnioski:\n\nRozkłady o niskiej kurtozie mają mniej wartości ekstremalnych i bardziej spłaszczony szczyt\nRozkłady o wysokiej kurtozie mają więcej wartości ekstremalnych (ciężkie ogony) i ostrzejszy szczyt\nRozkład normalny służy jako punkt odniesienia dla porównań kurtozy\n\n\n\n\nModalność (liczba dominant)\nModalność (modality) odnosi się do liczby wyraźnych szczytów lub “mod” w rozkładzie.\nTypy:\n\nJednomodalny (unimodal): Jeden wyraźny szczyt\nDwumodalny (bimodal): Dwa wyraźne szczyty\nWielomodalny (multimodal): Trzy lub więcej szczytów\nJednostajny (uniform): Brak szczytów; wszystkie wartości mniej więcej równie powszechne\n\nProstym językiem: Liczba szczytów może ujawnić ważne cechy danych. Rozkład dwumodalny może wskazywać dwie odrębne podgrupy (np. wzrost dorosłych może pokazywać szczyty dla mężczyzn i kobiet).\n\n# Wizualizuj różne kształty\nset.seed(789)\n\npar(mfrow = c(1, 3), mar = c(4, 4, 3, 1))\n\n# Jednomodalny symetryczny\nsymmetric_data &lt;- rnorm(1000, 50, 10)\nhist(symmetric_data, breaks = 30, main = \"Jednomodalny symetryczny\", \n     xlab = \"Wartość\", col = \"lightblue\", probability = TRUE)\nlines(density(symmetric_data), col = \"darkblue\", lwd = 2)\n\n# Skośny w prawo\nskewed_data &lt;- rexp(1000, 1/20) + 20\nhist(skewed_data, breaks = 30, main = \"Skośny w prawo (jednomodalny)\", \n     xlab = \"Wartość\", col = \"lightgreen\", probability = TRUE)\nlines(density(skewed_data), col = \"darkgreen\", lwd = 2)\n\n# Dwumodalny\nbimodal_data &lt;- c(rnorm(500, 30, 5), rnorm(500, 60, 5))\nhist(bimodal_data, breaks = 30, main = \"Dwumodalny\", \n     xlab = \"Wartość\", col = \"lightyellow\", probability = TRUE)\nlines(density(bimodal_data), col = \"orange\", lwd = 2)\n\n\n\n\n\n\n\n\nOcena wizualna: Cechy kształtu identyfikujemy głównie przez:\n\nHistogramy: Pokazują ogólną formę, liczbę szczytów i kierunek skośności\nWykresy pudełkowe: Ujawniają skośność przez długość wąsów i pozycję mediany\nWykresy gęstości: Gładkie krzywe, które podkreślają ogólny wzorzec",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wizualizacja-histogramy-visualization-histograms",
    "href": "rozdzial5.html#wizualizacja-histogramy-visualization-histograms",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.10 Wizualizacja: Histogramy / Visualization: Histograms",
    "text": "6.10 Wizualizacja: Histogramy / Visualization: Histograms\nHistogram wyświetla rozkład częstości (frequency distribution) danych liczbowych poprzez podzielenie zakresu na przedziały (bins) i pokazanie liczby lub proporcji obserwacji w każdym przedziale.\n\nKonstrukcja Histogramu\nKluczowe decyzje:\n\nLiczba przedziałów: Zbyt mało przedziałów traci szczegóły; zbyt wiele przedziałów tworzy szum\nGranice przedziałów: Czy przedziały powinny być [a, b) (zamknięte z lewej, otwarte z prawej) czy (a, b] (otwarte z lewej, zamknięte z prawej)?\nSkala osi Y: Częstości (frequency), częstości względne (relative frequency) czy gęstość (density)?\n\n\n\nWybór Optymalnej Liczby Przedziałów\nKilka reguł pomaga określić odpowiednią liczbę przedziałów:\n1. Reguła Sturgesa (Sturges’ Rule) (domyślna w R):\nk = \\lceil \\log_2(n) + 1 \\rceil\ngdzie n to liczebność próby. Działa dobrze dla rozkładów symetrycznych i jednomodalnych.\n2. Reguła Scotta (Scott’s Rule):\n\\text{Szerokość przedziału} = \\frac{3.5 \\cdot s}{n^{1/3}}\ngdzie s to odchylenie standardowe próby. Bardziej odporna na wartości odstające (outliers) niż reguła Sturgesa.\n3. Reguła Freedmana-Diaconisa (Freedman-Diaconis Rule):\n\\text{Szerokość przedziału} = \\frac{2 \\cdot IQR}{n^{1/3}}\ngdzie IQR to rozstęp międzykwartylowy (interquartile range). Najbardziej odporna na wartości odstające; zalecana dla rozkładów skośnych (skewed distributions).\n4. Reguła Pierwiastka Kwadratowego (Square Root Rule):\nk = \\lceil \\sqrt{n} \\rceil\nProsta, ale często produkuje zbyt mało przedziałów dla dużych zbiorów danych.\n\n# Przykład: Różne metody wyboru przedziałów\nset.seed(789)\nsample_data &lt;- c(rnorm(200, 50, 10), rnorm(50, 75, 8))\nn &lt;- length(sample_data)\n\n# Sturges\nsturges_bins &lt;- ceiling(log2(n) + 1)\n\n# Scott\nscott_width &lt;- 3.5 * sd(sample_data) / (n^(1/3))\nscott_bins &lt;- ceiling(diff(range(sample_data)) / scott_width)\n\n# Freedman-Diaconis\nfd_width &lt;- 2 * IQR(sample_data) / (n^(1/3))\nfd_bins &lt;- ceiling(diff(range(sample_data)) / fd_width)\n\n# Pierwiastek kwadratowy\nsqrt_bins &lt;- ceiling(sqrt(n))\n\ncat(\"Sturges:\", sturges_bins, \"przedziałów\\n\")\n\nSturges: 9 przedziałów\n\ncat(\"Scott:\", scott_bins, \"przedziałów\\n\")\n\nScott: 10 przedziałów\n\ncat(\"Freedman-Diaconis:\", fd_bins, \"przedziałów\\n\")\n\nFreedman-Diaconis: 14 przedziałów\n\ncat(\"Pierwiastek kwadratowy:\", sqrt_bins, \"przedziałów\\n\")\n\nPierwiastek kwadratowy: 16 przedziałów\n\n\n\n\nTypy Histogramów: Skale Osi Y\n\n1. Histogram Częstości / Frequency Histogram\nHistogram częstości (frequency histogram) pokazuje liczbę obserwacji w każdym przedziale. Oś Y reprezentuje liczbę obserwacji.\n\nOś Y: Liczba bezwzględna (częstość)\nStosujemy gdy: Chcemy zobaczyć surowe liczby\nOgraniczenie: Trudno porównywać rozkłady o różnych liczebnościach próby\n\n\n\n2. Histogram Częstości Względnych / Relative Frequency Histogram\nHistogram częstości względnych (relative frequency histogram) pokazuje proporcję obserwacji w każdym przedziale. Wysokość każdego słupka reprezentuje ułamek całości.\n\nOś Y: Częstość względna (proporcja) = \\frac{\\text{liczba w przedziale}}{n}\nStosujemy gdy: Porównujemy rozkłady o różnych liczebnościach próby\nWłaściwość: Wszystkie wysokości słupków sumują się do 1.0\n\n\n\n3. Histogram Gęstości / Density Histogram\nHistogram gęstości (density histogram) dostosowuje się do szerokości przedziałów tak, że pole powierzchni (nie wysokość) każdego słupka reprezentuje proporcję obserwacji.\n\nOś Y: Gęstość (density) = \\frac{\\text{częstość względna}}{\\text{szerokość przedziału}}\nStosujemy gdy: Przedziały mają różne szerokości lub gdy łączymy z funkcjami gęstości prawdopodobieństwa\nWłaściwość: Całkowite pole powierzchni pod wszystkimi słupkami równa się 1.0\n\nKluczowa różnica: W histogramie gęstości pole powierzchni każdego słupka równa się częstości względnej, podczas gdy wysokość to gęstość.\n\n# Wygeneruj przykładowe dane\nset.seed(789)\nsample_data &lt;- c(rnorm(200, 50, 10), rnorm(50, 75, 8))\n\npar(mfrow = c(3, 1), mar = c(4, 4, 3, 2))\n\n# 1. Histogram częstości\nhist(sample_data, breaks = 20, main = \"Histogram Częstości\", \n     xlab = \"Wartość\", ylab = \"Częstość (Liczba)\", \n     col = \"lightblue\", border = \"black\", freq = TRUE)\n\n# 2. Histogram częstości względnych - manualne skalowanie\nh &lt;- hist(sample_data, breaks = 20, plot = FALSE)\nh$counts &lt;- h$counts / sum(h$counts)\nplot(h, main = \"Histogram Częstości Względnych\",\n     xlab = \"Wartość\", ylab = \"Częstość Względna (Proporcja)\",\n     col = \"lightgreen\", border = \"black\", freq = TRUE)\n\n# 3. Histogram gęstości\nhist(sample_data, breaks = 20, main = \"Histogram Gęstości\", \n     xlab = \"Wartość\", ylab = \"Gęstość\", \n     col = \"lightyellow\", border = \"black\", freq = FALSE)\n\n\n\n\n\n\n\n\nZwiązek matematyczny:\nDla przedziału o granicach [a, b):\n\nCzęstość: f = \\text{liczba obserwacji w } [a, b)\nCzęstość względna: \\frac{f}{n}\nGęstość: \\frac{f/n}{b-a}\nPole powierzchni słupka: (b-a) \\times \\frac{f/n}{b-a} = \\frac{f}{n} (równa się częstości względnej)\n\n\n\n\nPrzedziały Otwarte vs. Zamknięte\nPrzedziały prawostronne otwarte: [a, b)\nPrzedział zawiera wartości \\geq a i &lt; b. Sama wartość b trafia do następnego przedziału.\nPrzedziały prawostronne zamknięte: (a, b]\nPrzedział zawiera wartości &gt; a i \\leq b. Wartość a trafia do poprzedniego przedziału.\nPrzykład:\nRozważmy dane: 1, 2, 3, 4, 5, 6, 7, 8 z przedziałami [1,3), [3,5), [5,7), [7,9)\nPrzedziały prawostronne otwarte [a, b):\n\nPrzedział [1, 3): zawiera 1, 2 (liczba = 2)\nPrzedział [3, 5): zawiera 3, 4 (liczba = 2)\nPrzedział [5, 7): zawiera 5, 6 (liczba = 2)\nPrzedział [7, 9): zawiera 7, 8 (liczba = 2)\n\nPrzedziały prawostronne zamknięte (a, b]:\n\nPrzedział (1, 3]: zawiera 2, 3 (liczba = 2)\nPrzedział (3, 5]: zawiera 4, 5 (liczba = 2)\nPrzedział (5, 7]: zawiera 6, 7 (liczba = 2)\nPrzedział (7, 9]: zawiera 8 (liczba = 1, ponieważ 9 nie ma w danych)\n\nKonwencja R: Funkcja hist() w R domyślnie używa przedziałów prawostronnych zamkniętych (tj. (a, b]). Można to zmienić argumentem right = FALSE, aby uzyskać przedziały prawostronne otwarte [a, b).\n\npar(mfrow = c(1, 2), mar = c(4, 4, 3, 2))\n\n# Prawostronne zamknięte (domyślne)\nhist(sample_data, breaks = 20, main = \"Przedziały Prawostronne Zamknięte (a, b]\", \n     xlab = \"Wartość\", col = \"lightblue\", border = \"black\", right = TRUE)\n\n# Prawostronne otwarte\nhist(sample_data, breaks = 20, main = \"Przedziały Prawostronne Otwarte [a, b)\", \n     xlab = \"Wartość\", col = \"lightcoral\", border = \"black\", right = FALSE)\n\n\n\n\n\n\n\n\n\n\nWpływ Liczby Przedziałów\n\npar(mfrow = c(1, 3), mar = c(4, 4, 3, 2))\n\n# Różne liczby przedziałów\nhist(sample_data, breaks = 10, main = \"10 Przedziałów (Za mało?)\", \n     xlab = \"Wartość\", col = \"lightblue\", border = \"black\")\nhist(sample_data, breaks = 20, main = \"20 Przedziałów (Zrównoważone)\", \n     xlab = \"Wartość\", col = \"lightgreen\", border = \"black\")\nhist(sample_data, breaks = 40, main = \"40 Przedziałów (Za dużo?)\", \n     xlab = \"Wartość\", col = \"lightyellow\", border = \"black\")\n\n\n\n\n\n\n\n\nInterpretacja:\n\n10 przedziałów: Pokazuje ogólny kształt, ale może przegapić szczegóły (potencjalna bimodalność nie jest widoczna)\n20 przedziałów: Równoważy szczegóły i przejrzystość; ujawnia strukturę bimodalną (bimodal)\n40 przedziałów: Pokazuje więcej szczegółów, ale może uwypuklać losowy szum, utrudniając dostrzeżenie wzorców\n\nCo ujawniają histogramy:\n\nKształt: Symetryczny, skośny lewo, skośny prawo, bimodalny, multimodalny (symmetric, left-skewed, right-skewed, bimodal, multimodal)\nCentrum: Gdzie znajduje się większość danych\nRozproszenie: Jak szeroki jest rozkład\nWartości odstające: Niezwykłe wartości oddzielone od głównego zbioru\nLuki: Zakresy, gdzie nie ma żadnych danych\n\nOgólna rekomendacja: Zacznij od reguły Sturgesa lub Freedmana-Diaconisa, a następnie dostosuj w zależności od tego, jakie wzorce chcesz uwypuklić. Dla analizy eksploracyjnej wypróbuj kilka różnych liczb przedziałów.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wizualizacja-wykresy-pudełkowe-tukeya",
    "href": "rozdzial5.html#wizualizacja-wykresy-pudełkowe-tukeya",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.11 Wizualizacja: wykresy pudełkowe Tukeya",
    "text": "6.11 Wizualizacja: wykresy pudełkowe Tukeya\nWykres pudełkowy Tukeya (Tukey boxplot, lub box-and-whisker plot) dostarcza wizualnego podsumowania rozkładu opartego na pięciu kluczowych wartościach: minimum, Q_1, mediana, Q_3 i maksimum. Identyfikuje również wartości odstające.\n\nKonstrukcja wykresu pudełkowego\nKrok 1: Oblicz podsumowanie pięcioliczbowe (five-number summary)\n\nMinimum (z wyłączeniem wartości odstających)\nPierwszy kwartyl (Q_1)\nMediana (Q_2)\nTrzeci kwartyl (Q_3)\nMaksimum (z wyłączeniem wartości odstających)\n\nKrok 2: Oblicz płoty dla wykrywania wartości odstających (fences)\n\\text{Płot dolny} = Q_1 - 1.5 \\times \\text{IQR} \\text{Płot górny} = Q_3 + 1.5 \\times \\text{IQR}\nKrok 3: Zidentyfikuj wartości odstające\nKażda obserwacja poniżej płotu dolnego lub powyżej płotu górnego jest klasyfikowana jako wartość odstająca (outlier).\nKrok 4: Narysuj wykres pudełkowy\n\nPudełko (box): Rozciąga się od Q_1 do Q_3 (zawiera środkowe 50% danych)\nLinia wewnątrz pudełka: Pokazuje medianę\nWąsy (whiskers): Rozciągają się do najbardziej ekstremalnych obserwacji niebędących wartościami odstającymi\nPojedyncze punkty: Zaznaczone dla każdej wartości odstającej\n\nKRYTYCZNE ROZRÓŻNIENIE: Płoty vs. Wąsy\n\nPłoty (fences): Teoretyczne granice używane do identyfikacji wartości odstających (niekoniecznie rysowane)\nWąsy (whiskers): Rozciągają się do rzeczywistych punktów danych w obrębie płotów\n\nPowszechny błąd: Studenci często myślą, że wąsy rozciągają się do płotów. To niepoprawne! Wąsy rozciągają się tylko do najbardziej ekstremalnych rzeczywistych wartości danych, które znajdują się w granicach płotów.\n\n# Przykład demonstrujący płoty vs. wąsy\nexample_data &lt;- c(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 25)\n\nq1 &lt;- quantile(example_data, 0.25, type = 2)\nq3 &lt;- quantile(example_data, 0.75, type = 2)\niqr &lt;- q3 - q1\nlower_fence &lt;- q1 - 1.5 * iqr\nupper_fence &lt;- q3 + 1.5 * iqr\n\noutliers &lt;- example_data[example_data &lt; lower_fence | example_data &gt; upper_fence]\nnon_outliers &lt;- example_data[example_data &gt;= lower_fence & example_data &lt;= upper_fence]\nlower_whisker &lt;- min(non_outliers)\nupper_whisker &lt;- max(non_outliers)\n\npar(mfrow = c(1, 2), mar = c(4, 3, 3, 2))\n\n# Lewy panel: Diagram pokazujący płoty i wąsy\nplot(c(0, 30), c(0, 2), type = \"n\", xlab = \"Wartość\", ylab = \"\", \n     main = \"Zrozumienie płotów vs. wąsów\", yaxt = \"n\", bty = \"n\")\n\n# Punkty danych\npoints(example_data, rep(1, length(example_data)), \n       pch = 19, cex = 2, \n       col = ifelse(example_data %in% outliers, \"red\", \"darkblue\"))\n\n# Płoty (przerywane)\nabline(v = lower_fence, col = \"orange\", lwd = 2, lty = 2)\nabline(v = upper_fence, col = \"orange\", lwd = 2, lty = 2)\n\n# Wąsy (ciągłe)\nabline(v = lower_whisker, col = \"darkgreen\", lwd = 3)\nabline(v = upper_whisker, col = \"darkgreen\", lwd = 3)\n\n# Pudełko (Q1 do Q3)\nrect(q1, 0.7, q3, 1.3, border = \"black\", lwd = 2)\nsegments(median(example_data), 0.7, median(example_data), 1.3, lwd = 3)\n\n# Etykiety\ntext(lower_fence, 1.7, sprintf(\"Płot dolny\\n%.1f\", lower_fence), \n     col = \"orange\", cex = 0.9)\ntext(upper_fence, 1.7, sprintf(\"Płot górny\\n%.1f\", upper_fence), \n     col = \"orange\", cex = 0.9)\ntext(lower_whisker, 0.3, sprintf(\"Wąs dolny\\n%d\", lower_whisker), \n     col = \"darkgreen\", cex = 0.9, font = 2)\ntext(upper_whisker, 0.3, sprintf(\"Wąs górny\\n%d\", upper_whisker), \n     col = \"darkgreen\", cex = 0.9, font = 2)\n\nlegend(\"top\", \n       legend = c(\"Obserwacje typowe\", \"Wartości odstające\", \"Płoty (przerywane)\", \"Wąsy (ciągłe)\"),\n       col = c(\"darkblue\", \"red\", \"orange\", \"darkgreen\"),\n       pch = c(19, 19, NA, NA), lty = c(NA, NA, 2, 1), lwd = 2,\n       ncol = 2)\n\n# Prawy panel: Rzeczywisty wykres pudełkowy\nboxplot(example_data, main = \"Odpowiadający wykres pudełkowy\", \n        ylab = \"Wartość\", col = \"lightblue\", ylim = c(0, 30))\n\n# Adnotuj komponenty wykresu pudełkowego\ntext(1.35, q1, \"Q1\", font = 2)\ntext(1.35, median(example_data), \"Mediana\", font = 2)\ntext(1.35, q3, \"Q3\", font = 2)\ntext(1.35, lower_whisker, \"Wąs\\ndolny\", font = 2, col = \"darkgreen\")\ntext(1.35, upper_whisker, \"Wąs\\ngórny\", font = 2, col = \"darkgreen\")\narrows(1.2, 25, 1.05, 25, length = 0.1, col = \"red\", lwd = 2)\ntext(1.3, 25, \"Wartość\\nodstająca\", pos = 4, col = \"red\", font = 2)\n\n\n\n\n\n\n\n\nKluczowe spostrzeżenia z wykresów pudełkowych:\n\nSymetria: Jeśli mediana jest wycentrowana w pudełku, a wąsy mają równą długość, rozkład jest w przybliżeniu symetryczny\nSkośność: Jeśli mediana jest bliżej Q_1, a górny wąs jest dłuższy, rozkład jest skośny w prawo\nWartości odstające: Pojedyncze punkty poza wąsami wskazują nietypowe obserwacje\nPorównanie: Wykresy pudełkowe obok siebie ułatwiają porównywanie rozkładów między grupami\n\n\n\nPorównywanie wielu grup wykresami pudełkowymi\n\n# Generuj dane dla trzech grup\nset.seed(9)\ngroup_a &lt;- rnorm(50, 50, 10)\ngroup_b &lt;- c(rnorm(45, 60, 8), c(30, 35, 85, 90, 95))  # Z wartościami odstającymi\ngroup_c &lt;- rexp(50, 1/20) + 30  # Skośny w prawo\n\n# Połącz w ramkę danych\ndata_compare &lt;- data.frame(\n  value = c(group_a, group_b, group_c),\n  group = factor(rep(c(\"Grupa A\\n(Symetryczny)\", \n                       \"Grupa B\\n(Z wartościami odstającymi)\", \n                       \"Grupa C\\n(Skośny w prawo)\"), \n                     each = 50))\n)\n\nboxplot(value ~ group, data = data_compare,\n        main = \"Porównanie rozkładów między grupami\",\n        ylab = \"Wartość\",\n        col = c(\"#66C2A5\", \"#FC8D62\", \"#8DA0CB\"),\n        border = \"black\")\n\ngrid(nx = NA, ny = NULL, col = \"lightgray\", lty = \"dotted\")\n\n\n\n\n\n\n\n\nInterpretacja:\n\nGrupa A: Rozkład symetryczny z medianą blisko centrum pudełka\nGrupa B: Wyższe centrum z kilkoma wartościami odstającymi po obu końcach\nGrupa C: Rozkład skośny w prawo (dłuższy górny wąs, mediana bliżej Q_1)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#podsumowanie-wybór-odpowiedniej-miary",
    "href": "rozdzial5.html#podsumowanie-wybór-odpowiedniej-miary",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.12 Podsumowanie: wybór odpowiedniej miary",
    "text": "6.12 Podsumowanie: wybór odpowiedniej miary\nWybór odpowiednich statystyk opisowych zależy od cech danych:\nDla tendencji centralnej:\n\nŚrednia: Stosuj, gdy dane są symetryczne i bez ekstremalnych wartości odstających. Dostarcza najwięcej informacji, ale wrażliwa na ekstrema. Wymaga skali interwałowej lub ilorazowej.\nMediana: Stosuj, gdy dane są skośne lub zawierają wartości odstające. Bardziej odporna, ale odrzuca część informacji. Wymaga co najmniej skali porządkowej.\nModa: Stosuj dla danych kategorycznych lub gdy identyfikujesz najczęstszą wartość. Można używać z każdą skalą pomiarową.\n\nDla rozproszenia:\n\nOdchylenie standardowe: Stosuj ze średnią dla symetrycznych danych bez wartości odstających. Najczęstsza i interpretatywna. Wymaga skali interwałowej lub ilorazowej.\nIQR: Stosuj z medianą dla skośnych danych lub danych z wartościami odstającymi. Odporny na ekstrema. Wymaga co najmniej skali porządkowej.\nRozstęp: Szybka ocena, ale bardzo wrażliwy na wartości odstające. Stosuj ostrożnie. Wymaga skali interwałowej lub ilorazowej.\n\nDla pozycji względnej:\n\nKwantyle/Percentyle: Pomagają porównać poszczególne obserwacje z całym rozkładem. Wymaga co najmniej skali porządkowej.\nKwartyle: Dostarczają standardowych punktów odcięcia dzielących dane na cztery równe części. Wymaga co najmniej skali porządkowej.\n\nDla kształtu:\n\nInspekcja wizualna: Używaj histogramów do oceny symetrii, skośności i modalności\nWykresy pudełkowe: Skutecznie ujawniają skośność i identyfikują wartości odstające\n\nDla wizualizacji:\n\nHistogramy: Pokazują pełny kształt rozkładu. Dobre do zrozumienia ogólnych wzorców.\nWykresy pudełkowe: Efektywne podsumowania do porównywania grup. Wyraźnie podkreślają wartości odstające i kwartyle.\n\nOgólna zasada: Zawsze raportuj wiele miar. Średnia i odchylenie standardowe opowiadają jedną historię; mediana i IQR opowiadają inną. Razem z wizualizacjami dostarczają pełnego obrazu danych. Zawsze rozważ skalę pomiarową danych przy wyborze statystyk do obliczenia.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#appendix-a-understanding-mean-as-a-balance-point",
    "href": "rozdzial5.html#appendix-a-understanding-mean-as-a-balance-point",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.13 Appendix A: Understanding Mean as a Balance Point 🎯",
    "text": "6.13 Appendix A: Understanding Mean as a Balance Point 🎯\nLet’s consider a dataset X = \\{1, 2, 6, 7, 9\\} on a number line, imagining it as a seesaw:\n\n\n\nhttps://www.gastonsanchez.com/matrix4sl/mean-as-a-balancing-point.html\n\n\nThe mean (\\mu) acts as the perfect balance point of this seesaw. For our data:\n\\mu = \\frac{1 + 2 + 6 + 7 + 9}{5} = 5\n\nWhat happens at different support points? 🤔\n\nSupport point at 6 (too high):\n\nLeft side: Values (1, 2) are below\nRight side: Values (7, 9) are above\n\\sum distances from left = (6-1) + (6-2) = 9\n\\sum distances from right = (7-6) + (9-6) = 4\nThe seesaw tilts left! ⬅️ because 9 &gt; 4\n\nSupport point at 4 (too low):\n\nLeft side: Values (1, 2) are below\nRight side: Values (6, 7, 9) are above\n\\sum distances from left = (4-1) + (4-2) = 5\n\\sum distances from right = (6-4) + (7-4) + (9-4) = 10\nThe seesaw tilts right! ➡️ because 5 &lt; 10\n\nSupport point at mean (5) (perfect balance):\n\n\\sum distances below = \\sum distances above\n((5-1) + (5-2)) = ((6-5) + (7-5) + (9-5))\n7 = 7 ✨ Perfect balance!\n\n\nThis shows why the mean is the unique balance point, where:\n\\sum_{i=1}^n (x_i - \\mu) = 0\nThe seesaw will always tilt unless the support point is placed exactly at the mean! 🎪\n\n\nMean as a Balance Point\nThis visualization shows how the arithmetic mean (5) acts as a balance point between clustered points on the left and dispersed points on the right:\nLeft side of the mean: - Points with values 2 and 3 - Close together (difference of 1 unit) - Distances from mean: 3 and 2 units - Sum of “pull” = 5 units\nRight side of the mean: - Points with values 6 and 9 - More spread out (difference of 3 units) - Distances from mean: 1 and 4 units - Sum of “pull” = 5 units\nKey observations:\n\nThe mean (5) is a balance point, even though:\n\nPoints on the left are clustered (2,3)\nPoints on the right are dispersed (6,9)\nGreen arrows show distances from the mean\n\nBalance is maintained because:\n\nSum of distances balances out: (5-2) + (5-3) = (6-5) + (9-5)\nTotal sum of distances = 5 units on each side",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#appendix-b-measures-of-relative-position-standing",
    "href": "rozdzial5.html#appendix-b-measures-of-relative-position-standing",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.14 Appendix B: Measures of Relative Position (Standing)",
    "text": "6.14 Appendix B: Measures of Relative Position (Standing)\nUnderstanding where values sit within a dataset is crucial for data analysis. Let’s explore these concepts step by step.\n\nQuartiles (Q): The Basics\nThink of quartiles as special numbers that split your ordered data into four equal parts.\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\n\nWhat Are Quartiles?\nFirst Quartile (Q1):\n\nSeparates the lowest 25% of data from the rest\nAlso called the 25th percentile\nExample: If Q1 = 50 in a test score dataset, 25% of students scored below 50\n\nSecond Quartile (Q2):\n\nThe median - splits data in half\nAlso called the 50th percentile\nExample: If Q2 = 70, half the students scored below 70\n\nThird Quartile (Q3):\n\nSeparates the highest 25% of data from the rest\nAlso called the 75th percentile\nExample: If Q3 = 85, 75% of students scored below 85\n\n\n\nHow to Calculate Quartiles (Step by Step) - Two Methods\nLet’s examine student test scores using both common quartile calculation methods:\nExample 1: Odd Number Case (11 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 88, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 11 values (odd)\nMedian position = (n + 1)/2 = 6\nQ2 = 78\n\nStep 2: Find Q1\n\nTukey’s Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (n + 1)/4 = (11 + 1)/4 = 3\nQ1 = 70 (3rd value)\n\n\nStep 3: Find Q3\n\nTukey’s Method:\n\nLook at upper half: 80, 82, 85, 88, 90\nQ3 = median of upper half = 85\n\nInterpolation Method:\n\nPosition = 3(n + 1)/4 = 3(12)/4 = 9\nQ3 = 85 (9th value)\n\n\nExample 2: Even Number Case (10 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 10 values (even)\nMedian positions = 5 and 6\nQ2 = (75 + 78)/2 = 76.5\n\nStep 2: Find Q1\n\nTukey’s Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (10 + 1)/4 = 2.75\nQ1 = 65 + 0.75(70 - 65) = 68.75\n\n\nStep 3: Find Q3\n\nTukey’s Method:\n\nLook at upper half: 78, 80, 82, 85, 90\nQ3 = median of upper half = 82\n\nInterpolation Method:\n\nPosition = 3(10 + 1)/4 = 8.25\nQ3 = 82 + 0.25(85 - 82) = 82.75\n\n\nImportant Notes:\n\nTukey’s Method:\n\nFirst find the median (Q2)\nSplit the data into lower and upper halves\nFind Q1 as the median of the lower half\nFind Q3 as the median of the upper half\nWhen n is odd, the median is not included in either half\n\nInterpolation Method:\n\nUses positions (n+1)/4 for Q1 and 3(n+1)/4 for Q3\nWhen position falls between values, uses linear interpolation\nDoesn’t require splitting data into halves\n\n\nBoth methods give the same results for simple positions (Example 1) but can differ when interpolation is needed (Example 2).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#appendix-c-manual-construction-of-tukey-boxplot",
    "href": "rozdzial5.html#appendix-c-manual-construction-of-tukey-boxplot",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.15 Appendix C: Manual Construction of Tukey Boxplot",
    "text": "6.15 Appendix C: Manual Construction of Tukey Boxplot\nStep 1: Calculate Key Components\n\nFind quartiles: Q_1, Q_2 (median), Q_3\nCalculate Interquartile Range: IQR = Q_3 - Q_1\n\nStep 2: Determine Whisker Boundaries\n\nLower fence: Q_1 - 1.5 \\times IQR\nUpper fence: Q_3 + 1.5 \\times IQR\n\nStep 3: Identify Outliers Data points are outliers if they are:\n\nBelow lower fence: x &lt; Q_1 - 1.5 \\times IQR\nAbove upper fence: x &gt; Q_3 + 1.5 \\times IQR\n\nExample: Given data: 2, 4, 6, 8, 9, 10, 11, 12, 14, 16, 50\n\nFind quartiles:\n\nQ_1 = 6\nQ_2 = 10\nQ_3 = 14\n\nCalculate IQR:\n\nIQR = 14 - 6 = 8\n\nCalculate fences:\n\nLower: 6 - (1.5 \\times 8) = -6\nUpper: 14 + (1.5 \\times 8) = 26\n\nIdentify outliers:\n\n50 &gt; 26, therefore 50 is an outlier\n\n\nGraphical Elements:\n\nBox: Draw from Q_1 to Q_3\nLine inside box: Draw at Q_2\nWhiskers: Extend to most extreme non-outlier points\nPoints: Plot outliers individually beyond whiskers\n\n\n\nCase Study: Comparing Heights Between Groups\nLet’s apply our understanding of box plots to a real dataset. We have height measurements (in centimeters) from two groups of 25 students each.\n\n# Create the dataset\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Transform dataset from wide to long format\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Display the first few rows\nhead(data_height_l)\n\n  Group_number height\n1      group_1    150\n2      group_1    160\n3      group_1    165\n4      group_1    168\n5      group_1    172\n6      group_1    173\n\n\nLet’s calculate some summary statistics for each group:\n\n# Calculate summary statistics for each group\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Calculate IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Create a comparison table\nstats_table &lt;- rbind(\n  group1_stats,\n  group2_stats\n)\nrownames(stats_table) &lt;- c(\"Group 1\", \"Group 2\")\n\n# Display the table\nstats_table\n\n        Min. 1st Qu. Median  Mean 3rd Qu. Max.\nGroup 1  150     175    180 178.6     183  200\nGroup 2  138     165    175 172.1     182  210\n\n# Display IQR values\ncat(\"IQR for Group 1:\", group1_iqr, \"\\n\")\n\nIQR for Group 1: 8 \n\ncat(\"IQR for Group 2:\", group2_iqr, \"\\n\")\n\nIQR for Group 2: 17 \n\n\n\n\nVisualizing the Height Data\nNow, let’s visualize the data using box plots and density plots:\n\n# Create horizontal boxplots\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Height Distribution by Group\",\n       x = \"Group\",\n       y = \"Height (cm)\")\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\n\n\n\n\n\n\nFigure 6.1: Box plots comparing height distributions between groups.\n\n\n\n\n\nTo complement our box plots, let’s also look at the density distributions:\n\n# Create density plots\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Height Density by Group\",\n       x = \"Height (cm)\",\n       y = \"Density\")\n\n\n\n\n\n\n\nFigure 6.2: Density plots showing the height distributions for each group.\n\n\n\n\n\n\n\nBox Plot Interpretation Exercise\nBased on the box plots and density plots above, determine whether each of the following statements is True or False. For each statement, provide a brief explanation based on evidence from the visualizations.\n\n\n\n\n\n\nExercise Questions\n\n\n\n\nStudents from group 2 (G2) in the studied sample are, on average, taller than those from group 1 (G1).\nGroup 1 (G1) height measurements are more dispersed/spread out than group 2 (G2).\nThe lowest person is in group 2 (G2).\nBoth data sets are negatively (left) skewed.\nHalf of the students in group 2 (G2) measure at least 175 cm.\n\n\n\n\nHints for Interpretation\nWhen answering these questions, consider:\n\nThe position of the median line within each box\nThe relative sizes of the boxes (IQR)\nThe positions of the minimum and maximum values\nThe symmetry of the distributions (balanced or skewed)\nThe lengths of the whiskers\n\nFor each statement, determine whether it is True or False and provide your explanation:\n\n\n\n\n\n\nAnswer Template\n\n\n\n\n\n\nStudents from G2 are, on average, taller than G1: [True/False]\n\nExplanation:\n\nG1 height is more dispersed/spread out: [True/False]\n\nExplanation:\n\nThe lowest person is in G2: [True/False]\n\nExplanation:\n\nBoth data sets are negatively (left) skewed: [True/False]\n\nExplanation:\n\nHalf of G2 measure at least 175 cm: [True/False]\n\nExplanation:\n\n\n\n\n\nLet’s review the answers to our box plot interpretation questions:\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nStudents from G2 are, on average, taller than G1: False\n\nExplanation: The median height (middle line in the boxplot) for G1 is higher than G2.\n\nG1 height is more dispersed/spread out: False\n\nExplanation: G2 shows greater dispersion. This is visible in the boxplot where G2 has a larger interquartile range (IQR) of 17.5 cm compared to G1’s 9.5 cm. G2 also has a wider range from minimum to maximum values.\n\nThe lowest person is in G2: True\n\nExplanation: The minimum value in G2 is 138 cm, which is lower than the minimum value in G1 (150 cm).\n\nBoth data sets are negatively (left) skewed: True\n\nExplanation: In both groups, the median line is positioned toward the upper part of the box, and the lower whisker is longer than the upper whisker. This indicates that there’s a longer tail on the left side of the distribution, which means negative skewness.\n\nHalf of G2 measure at least 175 cm: True\n\nExplanation: The median (middle line in the boxplot) for G2 is 175 cm, which means that 50% of the values are greater than or equal to 175 cm.\n\n\n\n\n\n\n\n\nR Code Reference\nHere’s the complete R code used in this section:\n\n# Load required packages\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n# Set display options\noptions(scipen = 999, digits = 3)\n\n# Create the dataset\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Transform dataset from wide to long format\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Display the first few rows\nhead(data_height_l)\n\n# Calculate summary statistics for each group\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Calculate IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Create horizontal boxplots\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Height Distribution by Group\",\n       x = \"Group\",\n       y = \"Height (cm)\")\n\n# Create density plots\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Height Density by Group\",\n       x = \"Height (cm)\",\n       y = \"Density\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#appendix-d-comparative-analysis-using-tukey-boxplots",
    "href": "rozdzial5.html#appendix-d-comparative-analysis-using-tukey-boxplots",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.16 Appendix D: Comparative Analysis Using Tukey Boxplots",
    "text": "6.16 Appendix D: Comparative Analysis Using Tukey Boxplots\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# Prepare data - exclude Oceania due to insufficient data (only 2 countries)\ndata_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007, continent != \"Oceania\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#understanding-tukey-boxplots",
    "href": "rozdzial5.html#understanding-tukey-boxplots",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.17 Understanding Tukey Boxplots",
    "text": "6.17 Understanding Tukey Boxplots\nA Tukey boxplot displays five key statistics and identifies outliers:\n\nMedian (Q2): The middle line in the box (50th percentile)\nFirst quartile (Q1): Bottom of the box (25th percentile)\n\nThird quartile (Q3): Top of the box (75th percentile)\nInterquartile Range (IQR): The height of the box = Q3 - Q1\nWhiskers: Extend to the most extreme values within Q1 - 1.5 \\times IQR (lower) and Q3 + 1.5 \\times IQR (upper)\nOutliers: Points beyond the whiskers (Tukey’s rule)\n\nKey interpretation points:\n\nThe median shows central tendency (not the mean!)\nThe IQR shows the middle 50% of data\nLonger boxes indicate more spread\nAsymmetric boxes or whiskers indicate skewness\nThe whiskers show the range of “typical” values\n\n\nggplot(data_2007, aes(x = reorder(continent, lifeExp, FUN = median), y = lifeExp)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 21, \n               outlier.fill = \"red\", outlier.size = 3, width = 0.6) +\n  geom_jitter(width = 0.15, alpha = 0.3, color = \"darkblue\", size = 2) +\n  labs(title = \"Life Expectancy by Continent (2007)\",\n       subtitle = \"Boxplots show distribution; red circles indicate outliers by Tukey's rule\\nNote: Oceania excluded due to insufficient data (only 2 countries)\",\n       x = \"Continent (ordered by median)\",\n       y = \"Life Expectancy (years)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12)\n  ) +\n  scale_y_continuous(breaks = seq(40, 85, by = 5))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#part-a-basic-comparisons",
    "href": "rozdzial5.html#part-a-basic-comparisons",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.18 Part A: Basic Comparisons",
    "text": "6.18 Part A: Basic Comparisons\nAnswer the following questions by examining the boxplot above.\n\nTrue or False:\n\nEurope has a higher median life expectancy than Africa.\nLife expectancy is more dispersed in Africa than in Europe (compare the box heights).\nAfrica’s distribution is positively (right) skewed—the upper whisker is longer than the lower whisker.\nAsia has outliers on both the low and high ends of the distribution.\nThe median life expectancy in Asia is lower than in the Americas.\nAfrica shows the largest interquartile range (IQR) among all continents.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#part-b-changes-over-time",
    "href": "rozdzial5.html#part-b-changes-over-time",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.19 Part B: Changes Over Time",
    "text": "6.19 Part B: Changes Over Time\n\ntime_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007), continent != \"Oceania\") %&gt;%\n  mutate(year = factor(year))\n\nggplot(time_comparison, aes(x = continent, y = lifeExp, fill = year)) +\n  geom_boxplot(alpha = 0.7, position = position_dodge(0.8), \n               outlier.shape = 21, outlier.size = 2, width = 0.7) +\n  labs(title = \"Life Expectancy: 1957 vs 2007\",\n       subtitle = \"50-year comparison of distributions (Oceania excluded due to insufficient data)\",\n       x = \"Continent\",\n       y = \"Life Expectancy (years)\",\n       fill = \"Year\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    legend.position = \"top\",\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 11)\n  ) +\n  scale_fill_manual(values = c(\"1957\" = \"#E78AC3\", \"2007\" = \"#8DA0CB\")) +\n  scale_y_continuous(breaks = seq(30, 85, by = 5))\n\n\n\n\n\n\n\n\n\nTrue or False:\n\nThe median life expectancy increased in all continents between 1957 and 2007.\nIn 1957, Asia had a larger IQR than in 2007, indicating convergence (compare box heights).\nThe entire distribution of life expectancy in Europe in 2007 is higher than Africa’s entire distribution in 1957 (check if the boxes overlap at all).\nAsia showed a larger increase in median life expectancy than the Americas between 1957 and 2007.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#part-c-summary-statistics",
    "href": "rozdzial5.html#part-c-summary-statistics",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.20 Part C: Summary Statistics",
    "text": "6.20 Part C: Summary Statistics\n\n# Calculate summary statistics for verification\nsummary_stats &lt;- data_2007 %&gt;%\n  group_by(continent) %&gt;%\n  summarise(\n    n = n(),\n    median = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    min = min(lifeExp),\n    max = max(lifeExp),\n    lower_fence = q1 - 1.5 * iqr,\n    upper_fence = q3 + 1.5 * iqr,\n    n_outliers = sum(lifeExp &lt; lower_fence | lifeExp &gt; upper_fence)\n  ) %&gt;%\n  arrange(desc(median))\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Summary Statistics for Life Expectancy by Continent (2007)\")\n\n\nSummary Statistics for Life Expectancy by Continent (2007)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontinent\nn\nmedian\nq1\nq3\niqr\nmin\nmax\nlower_fence\nupper_fence\nn_outliers\n\n\n\n\nEurope\n30\n78.6\n75.0\n79.8\n4.8\n71.8\n81.8\n67.9\n87.0\n0\n\n\nAmericas\n25\n72.9\n71.8\n76.4\n4.6\n60.9\n80.7\n64.8\n83.3\n1\n\n\nAsia\n33\n72.4\n65.5\n75.6\n10.2\n43.8\n82.6\n50.3\n90.9\n1\n\n\nAfrica\n52\n52.9\n47.8\n59.4\n11.6\n39.6\n76.4\n30.4\n76.9\n0\n\n\n\n\n\nNote: The lower and upper fences show the boundaries for Tukey’s outlier rule. Values beyond these fences are marked as outliers. Oceania is excluded from this analysis because it contains only 2 countries in the dataset, which is insufficient for meaningful distributional analysis using boxplots.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#thinking-questions",
    "href": "rozdzial5.html#thinking-questions",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.21 Thinking Questions",
    "text": "6.21 Thinking Questions\n\nWhy might Africa show more outliers than other continents? What could this tell us about within-continent variation?\nIf we wanted to compare “average” life expectancy between continents, should we use the mean or median? Why might these differ, especially for Africa?\nLooking at the 1957 vs 2007 comparison, which continent showed the most dramatic transformation? Consider both the change in median and the change in spread (IQR).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#solutions-1",
    "href": "rozdzial5.html#solutions-1",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.22 Solutions",
    "text": "6.22 Solutions\n\nPart A: Basic Comparisons\n1. TRUE - Europe’s median (the line in the middle of the box) is clearly much higher (~78 years) than Africa’s median (~53 years).\n2. TRUE - Africa’s box is much taller than Europe’s box, indicating greater IQR and thus more dispersion in life expectancy values.\n3. TRUE - Africa’s upper whisker is visibly longer than the lower whisker, indicating positive (right) skewness. This means there are some African countries with notably higher life expectancy than the median, creating a tail toward higher values.\n4. TRUE - Asia shows red circles (outliers) both below the lower whisker (countries with unusually low life expectancy) and above the upper whisker (countries with unusually high life expectancy).\n5. TRUE - The median line in Asia’s box is slightly lower (~71 years) than the median line in the Americas’ box (~73 years).\n6. TRUE - Africa has the tallest box among all continents, indicating the largest IQR and thus the greatest variability in life expectancy across African countries.\n\n\nPart B: Changes Over Time\n1. TRUE - For every continent, the blue box (2007) has a higher median line than the pink box (1957), showing universal improvement in life expectancy over the 50-year period.\n2. TRUE - Asia’s pink box (1957) is noticeably taller than its blue box (2007), showing that Asian countries converged (became more similar) over time. Countries that were far apart in development in 1957 have moved closer together by 2007.\n3. TRUE - Europe’s entire blue box (2007) sits above Africa’s entire pink box (1957)—there is no overlap between the distributions. Even the country with the lowest life expectancy in Europe in 2007 has higher life expectancy than the country with the highest life expectancy in Africa in 1957.\n4. TRUE - The vertical distance between Asia’s pink and blue median lines is visibly larger (~30 years: from ~41 to ~71) than the distance between the Americas’ pink and blue median lines (~21 years: from ~52 to ~73), indicating Asia had a greater absolute improvement in median life expectancy.\n\n\nThinking Questions: Sample Answers\n1. Why might Africa show more outliers?\nAfrica’s outliers reflect extreme within-continent heterogeneity. Countries like Swaziland and Lesotho face severe HIV/AIDS epidemics that dramatically reduce life expectancy, while other factors like conflict, governance quality, and economic development vary widely across the continent. The presence of outliers suggests that treating “Africa” as a homogeneous category obscures important variation, and country-specific factors matter greatly.\n2. Mean or median for comparison?\nThe median is more appropriate for comparing continents, especially Africa. The presence of outliers and the right-skewed distribution in Africa would pull the mean lower than what represents the typical country experience. The median better represents the “typical” or “central” country on each continent and is resistant to extreme values, making it a more robust measure of central tendency for this comparison.\n3. Most dramatic transformation:\nAsia showed the most dramatic transformation. First, it achieved the largest absolute increase in median life expectancy (approximately 30 years, compared to roughly 10-21 years for other continents). Second, it showed notable convergence—the 2007 box is much shorter than the 1957 box—meaning that Asian countries which were at vastly different development stages in 1957 have largely caught up with each other by 2007. This reflects the rapid economic development and healthcare improvements across much of Asia during this period, particularly in East and Southeast Asia.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#appendix-e-comparative-analysis-using-tukey-boxplots",
    "href": "rozdzial5.html#appendix-e-comparative-analysis-using-tukey-boxplots",
    "title": "6  Podstawy statystyki opisowej jednej zmiennej",
    "section": "6.23 Appendix E: Comparative Analysis Using Tukey Boxplots",
    "text": "6.23 Appendix E: Comparative Analysis Using Tukey Boxplots\n\nlibrary(tidyverse)\nlibrary(dslabs)  # Contains gapminder data with fertility rates\n\nWarning: package 'dslabs' was built under R version 4.4.3\n\n# Prepare data for two time points\ndata_1960 &lt;- gapminder %&gt;%\n  filter(year == 1960, !is.na(fertility))\n\ndata_2015 &lt;- gapminder %&gt;%\n  filter(year == 2015, !is.na(fertility))\n\n# Combine for comparison\ndata_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1960, 2015), !is.na(fertility))\n\n\nUnderstanding Tukey Boxplots\nA Tukey boxplot displays five key statistics and identifies outliers:\n\nMedian (Q2): The middle line in the box (50th percentile)\nFirst quartile (Q1): Bottom of the box (25th percentile)\n\nThird quartile (Q3): Top of the box (75th percentile)\nInterquartile Range (IQR): The height of the box = Q3 - Q1\nWhiskers: Extend to the most extreme values within Q1 - 1.5 \\times IQR (lower) and Q3 + 1.5 \\times IQR (upper)\nOutliers: Points beyond the whiskers (Tukey’s rule)\n\nKey interpretation points:\n\nThe median shows central tendency (not the mean!)\nThe IQR shows the middle 50% of data\nLonger boxes indicate more spread\nAsymmetric boxes or whiskers indicate skewness\nThe whiskers show the range of “typical” values\n\n\n\nTemporal Comparison: 1960 vs 2015\n\nggplot(data_comparison, aes(x = reorder(continent, fertility, FUN = median), \n                             y = fertility, fill = as.factor(year))) +\n  geom_boxplot(alpha = 0.7, outlier.shape = 21, outlier.size = 3,\n               position = position_dodge(width = 0.8)) +\n  labs(title = \"Fertility Rate by Continent: 1960 vs 2015\",\n       subtitle = \"Side-by-side comparison shows demographic transition across continents\",\n       x = \"Continent (ordered by median fertility in 2015)\",\n       y = \"Fertility Rate (births per woman)\",\n       fill = \"Year\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    legend.position = \"top\"\n  ) +\n  scale_fill_manual(values = c(\"1960\" = \"coral\", \"2015\" = \"lightgreen\")) +\n  scale_y_continuous(breaks = seq(1, 8, by = 0.5))\n\n\n\n\n\n\n\n\n\n\nDistribution of Fertility Rate Changes (1960-2015)\n\n# Calculate change in fertility for each country\nfertility_change &lt;- gapminder %&gt;%\n  filter(year %in% c(1960, 2015), !is.na(fertility)) %&gt;%\n  select(country, continent, year, fertility) %&gt;%\n  pivot_wider(names_from = year, values_from = fertility, names_prefix = \"year_\") %&gt;%\n  filter(!is.na(year_1960), !is.na(year_2015)) %&gt;%\n  mutate(\n    change = year_2015 - year_1960,\n    percent_change = ((year_2015 - year_1960) / year_1960) * 100\n  )\n\n\nggplot(fertility_change, aes(x = reorder(continent, change, FUN = median), y = change)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 21, \n               outlier.fill = \"red\", outlier.size = 3) +\n  geom_jitter(width = 0.15, alpha = 0.3, color = \"darkblue\", size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", size = 1) +\n  labs(title = \"Change in Fertility Rate by Continent (1960-2015)\",\n       subtitle = \"Negative values indicate fertility decline; dashed line shows no change\",\n       x = \"Continent (ordered by median change)\",\n       y = \"Change in Fertility Rate (births per woman)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12)\n  ) +\n  scale_y_continuous(breaks = seq(-6, 1, by = 0.5))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nExercise Questions\n\nWhich continent had the highest median fertility rate in 1960?\nWhich continent experienced the largest decline in median fertility between 1960 and 2015?\nDid any continent show an increase in fertility rate during this period?\nWhich continent shows the most homogeneous change (smallest IQR for change)?\nAre there any countries that bucked the global trend and increased their fertility rates?\nCompare the variability (IQR) in fertility rates in 1960 vs 2015 for Africa. What does this tell us?\n\n\n\nAnswers with Statistical Calculations\n\n# Summary statistics for 1960 and 2015\ncomparison_stats &lt;- data_comparison %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    n = n(),\n    median = median(fertility),\n    Q1 = quantile(fertility, 0.25),\n    Q3 = quantile(fertility, 0.75),\n    IQR = IQR(fertility),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(continent, year)\n\nprint(comparison_stats)\n\n# A tibble: 10 × 7\n   continent  year     n median    Q1    Q3   IQR\n   &lt;fct&gt;     &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Africa     1960    51   6.69  6.16  6.98 0.815\n 2 Africa     2015    51   4.55  3.23  5.06 1.82 \n 3 Americas   1960    36   6.5   4.78  6.79 2.01 \n 4 Americas   2015    35   2.14  1.87  2.42 0.555\n 5 Asia       1960    47   6.3   5.62  6.96 1.34 \n 6 Asia       2015    47   2.18  1.79  2.83 1.04 \n 7 Europe     1960    39   2.63  2.34  3.10 0.765\n 8 Europe     2015    39   1.55  1.45  1.78 0.33 \n 9 Oceania    1960    12   6.42  5.55  7.01 1.46 \n10 Oceania    2015    12   3.04  2.07  3.67 1.60 \n\n\n\n# Summary statistics for changes\nchange_stats &lt;- fertility_change %&gt;%\n  group_by(continent) %&gt;%\n  summarise(\n    n = n(),\n    median_change = median(change),\n    Q1_change = quantile(change, 0.25),\n    Q3_change = quantile(change, 0.75),\n    IQR_change = IQR(change),\n    min_change = min(change),\n    max_change = max(change),\n    median_1960 = median(year_1960),\n    median_2015 = median(year_2015)\n  ) %&gt;%\n  arrange(median_change)\n\nprint(change_stats)\n\n# A tibble: 5 × 10\n  continent     n median_change Q1_change Q3_change IQR_change min_change\n  &lt;fct&gt;     &lt;int&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Asia         47         -4.17     -4.58    -3.18        1.40      -5.16\n2 Americas     35         -3.7      -4.56    -2.96        1.61      -5.54\n3 Oceania      12         -3.62     -3.78    -2.58        1.2       -4.06\n4 Africa       51         -2.14     -3.36    -1.22        2.14      -5.28\n5 Europe       39         -0.95     -1.60    -0.765       0.83      -4.41\n# ℹ 3 more variables: max_change &lt;dbl&gt;, median_1960 &lt;dbl&gt;, median_2015 &lt;dbl&gt;\n\n\n\n# Identify countries with increasing fertility\nincreasing_fertility &lt;- fertility_change %&gt;%\n  filter(change &gt; 0) %&gt;%\n  arrange(desc(change)) %&gt;%\n  select(country, continent, year_1960, year_2015, change, percent_change)\n\nprint(increasing_fertility)\n\n# A tibble: 3 × 6\n  country continent year_1960 year_2015 change percent_change\n  &lt;fct&gt;   &lt;fct&gt;         &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n1 Niger   Africa         7.05      7.51  0.46            6.52\n2 Mali    Africa         6.7       6.81  0.110           1.64\n3 Gambia  Africa         5.57      5.67  0.100           1.80\n\n\n\nAnswer Key:\n1. Highest median fertility rate in 1960: Africa (median ≈ 6.6 births per woman), though Asia and Americas were also quite high (around 5.5-6.0).\n2. Largest decline in median fertility: Asia experienced the most dramatic decline, with median fertility dropping from approximately 5.7 in 1960 to 2.2 in 2015 (decline of about 3.5 births per woman). Africa showed the smallest median decline (from ~6.6 to ~4.7).\n3. Increasing fertility rates: Yes, a few countries show positive changes, though these are outliers. The table above lists specific countries, but generally these are rare exceptions to the global fertility decline.\n4. Most homogeneous change: Europe shows the smallest IQR for change (approximately 1.0), indicating that European countries experienced similar patterns of fertility decline. Africa shows the largest IQR for change (approximately 2.5), indicating highly heterogeneous experiences across African countries.\n5. Countries bucking the trend: The increasing_fertility table shows specific countries. These are typically small island nations or countries recovering from conflicts, but they represent a tiny minority of cases.\n6. Africa’s variability comparison:\n\n1960: Africa had IQR ≈ 2.0, showing moderate variability\n2015: Africa has IQR ≈ 2.4, showing slightly increased variability\nInterpretation: Despite overall fertility decline, African countries have become more heterogeneous over time, suggesting different speeds of demographic transition. Some African countries (e.g., South Africa, Tunisia) have completed or nearly completed their demographic transition, while others (e.g., Niger, Somalia) maintain very high fertility rates.\n\n\n\nInterpretation:\nThe temporal comparison reveals several important patterns:\n\nUniversal decline: Nearly all continents experienced substantial fertility declines, reflecting the global demographic transition.\nVarying speeds: Asia’s dramatic decline (median drop of ~3.5) contrasts with Africa’s more modest decline (median drop of ~1.9), suggesting different stages and speeds of demographic transition.\nConvergence in some regions: Europe and Americas show narrower distributions in 2015 compared to 1960, indicating convergence toward replacement-level fertility.\nDivergence in Africa: Unlike other continents, Africa’s distribution has become wider, not narrower, indicating that some countries have rapidly reduced fertility while others have changed little.\nBelow-replacement fertility: Europe’s 2015 median (1.5) is below the replacement rate of 2.1, raising questions about population aging and sustainability.\n\n\nNote: This exercise uses the dslabs package which contains extended gapminder data including fertility rates. If you’re using the standard gapminder package, you’ll need to install dslabs first: install.packages(\"dslabs\").",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Podstawy statystyki opisowej jednej zmiennej</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "7  Data Visualization: with examples in R",
    "section": "",
    "text": "7.1 Introduction to Data Types and Visualization\nThis chapter explores fundamental types of data visualizations: bar plots, histograms, and box plots, in particular.\nBefore diving into specific visualization techniques, it’s crucial to understand the different types of data you might encounter and how they influence your choice of visualization method. We’ll explore these concepts with practical examples using the ggplot2 library in R.\nFirst, let’s load the necessary libraries:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#bar-plots",
    "href": "chapter6.html#bar-plots",
    "title": "7  Data Visualization: with examples in R",
    "section": "7.2 Bar Plots",
    "text": "7.2 Bar Plots\nBar plots are excellent for displaying categorical data or summarizing continuous data by groups.\n\nUnderstanding Bar Plots\nA bar plot represents data using rectangular bars with heights proportional to the values they represent. They are used to compare different categories or groups.\nKey components of a bar plot: 1. X-axis: Represents categories 2. Y-axis: Represents values (can be counts, percentages, or any numerical value) 3. Bars: Rectangle for each category, height corresponds to its value\n\nExample Data\nLet’s use a simple dataset of fruit sales:\n\nfruits &lt;- c(\"Apple\", \"Banana\", \"Orange\", \"Grape\")\nsales &lt;- c(120, 85, 70, 100)\n\n# Create a data frame\ndf &lt;- data.frame(fruit = fruits, sales = sales)\n\n\n\n\nHand-Drawn Bar Plot\nTo create a bar plot by hand:\n\nDraw a horizontal line (x-axis) and a vertical line (y-axis) perpendicular to each other.\nLabel the x-axis with your categories (fruits), evenly spaced.\nLabel the y-axis with a suitable scale for your values (sales, 0 to 120 in increments of 20).\nFor each category, draw a rectangle (bar) whose height corresponds to its value on the y-axis scale.\nColor or shade each bar if desired.\nAdd a title and labels for both axes.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen drawing by hand, use graph paper for more precise measurements and straighter lines. Choose a scale that allows all your data to fit while maximizing the use of space.\n\n\n\n\nBar Plot in Base R\n\n# Create bar plot\nbarplot(sales, names.arg = fruits, \n        main = \"Fruit Sales\",\n        xlab = \"Fruit Types\", ylab = \"Sales\")\n\n\n\n\n\n\n\n\n\n\nBar Plot with ggplot2\n\n# Create bar plot with ggplot2\nggplot(df, aes(x = fruit, y = sales)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Fruit Sales\",\n       x = \"Fruit Types\", y = \"Sales\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpreting Bar Plots\nWhen interpreting a bar plot, consider the following:\n\nRelative Heights: Compare the heights of the bars to understand which categories have higher or lower values.\nOrdering: Sometimes, bars are ordered by height to make comparisons easier.\nPatterns: Look for any patterns or trends across categories.\nOutliers: Identify any bars that are much taller or shorter than the others.\n\n\nExample Interpretation\nFor our fruit sales data:\n\nApples have the highest sales (120), followed by Grapes (100).\nOranges have the lowest sales (70).\nThere’s a considerable difference between the highest (Apples) and lowest (Oranges) sales.\nBananas and Grapes have similar sales figures, in the middle range.\n\nThis information could be useful for inventory management or marketing strategies in a fruit shop.\n\n\n\n\n\n\nNote\n\n\n\nBar plots are great for comparing categories, but they don’t show the distribution within each category. For that, you might need other plot types like box plots.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#histograms",
    "href": "chapter6.html#histograms",
    "title": "7  Data Visualization: with examples in R",
    "section": "7.3 Histograms",
    "text": "7.3 Histograms\nHistograms visualize the distribution of a continuous variable by dividing it into intervals (bins) and showing the frequency or density of data points in each bin.\n\nUnderstanding Histograms\nKey components of a histogram: 1. X-axis: Represents the variable’s values, divided into bins 2. Y-axis: Represents frequency, relative frequency, or density 3. Bars: Rectangle for each bin, height corresponds to the y-axis measure\nThere are three main types of histograms:\n\nFrequency Histogram: The y-axis shows the count of data points in each bin.\nRelative Frequency Histogram: The y-axis shows the proportion of data points in each bin (frequency divided by total number of data points).\nDensity Histogram: The y-axis shows the density, which is the relative frequency divided by the bin width. The total area of all bars sums to 1.\n\n\nExample Data\nLet’s use a dataset of 50 student exam scores (out of 100):\n\nset.seed(123)  # for reproducibility\nscores &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\nHand-Drawn Histogram\nTo create a frequency histogram by hand:\n\nFind the range of your data.\nChoose a number of bins (let’s use 7 bins).\nCreate a frequency table.\nDraw x and y axes.\nLabel x-axis with bin ranges and y-axis with frequency.\nDraw a rectangle for each bin, with height corresponding to its frequency.\nAdd a title and labels for both axes.\n\nFor a relative frequency histogram, divide each frequency by the total number of data points before drawing the bars.\nFor a density histogram, divide the relative frequency by the bin width before drawing the bars.\n\n\n\n\n\n\nTip\n\n\n\nThe number of bins can affect the interpretation. Too few bins may obscure important features, while too many may introduce noise. A common rule of thumb is to use the square root of the number of data points as the number of bins.\n\n\n\n\nHistograms in Base R\n\n# Frequency Histogram\nhist(scores, breaks = 7, \n     main = \"Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Relative Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Relative Frequency\")\n\n\n\n\n\n\n\n# Density Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Density Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Density\")\nlines(density(scores), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\nHistograms with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(score = scores)\n\n# Frequency Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nggplot(df, aes(x = score, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Relative Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Relative Frequency\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Density Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Density Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Density\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nInterpreting Histograms\nWhen interpreting a histogram, consider:\n\nCentral Tendency: Where is the peak of the distribution?\nSpread: How wide is the distribution?\nShape: Is it symmetric, skewed, or multi-modal?\nOutliers: Are there any unusual values far from the main distribution?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#box-plots-and-tukey-box-plots",
    "href": "chapter6.html#box-plots-and-tukey-box-plots",
    "title": "7  Data Visualization: with examples in R",
    "section": "7.4 Box Plots and Tukey Box Plots",
    "text": "7.4 Box Plots and Tukey Box Plots\nBox plots, also known as box-and-whisker plots, provide a concise summary of a distribution. We’ll focus on the Tukey-style box plot, named after the statistician John Tukey who popularized this type of plot.\n\nUnderstanding Box Plots\nA box plot represents five key statistics:\n\nMinimum value (excluding outliers)\nFirst quartile (Q1)\nMedian\nThird quartile (Q3)\nMaximum value (excluding outliers)\n\nAdditionally, box plots show:\n\nWhiskers: Lines extending from the box to the minimum and maximum values (excluding outliers)\nOutliers: Individual points beyond the whiskers\n\n\nCalculating Quartiles and Outliers\nTo create a box plot, follow these steps:\n\nOrder your data from smallest to largest.\nFind the median (middle value if odd number of data points, average of two middle values if even).\nFind Q1 (median of lower half of data) and Q3 (median of upper half of data).\nCalculate the Interquartile Range (IQR) = Q3 - Q1\nDetermine outliers using Tukey’s rule:\n\nLower outliers: &lt; Q1 - 1.5 * IQR\nUpper outliers: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe factor 1.5 in Tukey’s outlier rule is based on the properties of the normal distribution. For normally distributed data, this rule identifies about 0.7% of the data as potential outliers.\n\n\n\n\nExample Data\nLet’s use a small dataset to illustrate:\n\ndata &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\nHand-Drawn Tukey Box Plot\nTo create a Tukey box plot by hand:\n\nDraw a vertical line representing the range from minimum to maximum (2 to 15 in our example, excluding the outlier).\nDraw a box from Q1 to Q3.\nDraw a horizontal line through the box at the median.\nDraw whiskers from the box to the minimum and maximum values (excluding outliers).\nRepresent the outlier (50) as an individual point beyond the whisker.\nAdd a scale to the vertical axis and label it.\n\n\n\nBox Plot in Base R\n\n# Create box plot\nboxplot(data, main = \"Box Plot of Sample Data\",\n        ylab = \"Values\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\nTukey Box Plot with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(value = data)\n\n# Create Tukey box plot with ggplot2\nggplot(df, aes(x = \"\", y = value)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Tukey Box Plot of Sample Data\",\n       x = \"\", y = \"Values\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\nInterpreting Box Plots\nWhen interpreting a box plot, consider the following:\n\nCentral Tendency: The median shows the center of the distribution.\nSpread: The box (IQR) represents the middle 50% of the data.\nSkewness: If the median line is closer to one end of the box, the distribution is skewed.\nOutliers: Points beyond the whiskers are potential outliers.\nComparisons: When comparing multiple box plots, look at relative positions of medians, box sizes, and presence of outliers.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#conclusion",
    "href": "chapter6.html#conclusion",
    "title": "7  Data Visualization: with examples in R",
    "section": "7.5 Conclusion",
    "text": "7.5 Conclusion\nIn this chapter, we explored three fundamental types of data visualizations: bar plots, histograms, and box plots. We demonstrated how to create these plots by hand, using R’s base plotting system, and using the ggplot2 library.\nEach type of plot serves a different purpose: - Bar plots are excellent for comparing categories. - Histograms show the distribution of a continuous variable. - Box plots provide a concise summary of a distribution, highlighting central tendency, spread, and outliers.\nRemember, the choice of visualization depends on your data type and the insights you want to convey. Always consider your audience and the story you want to tell with your data when selecting and designing your visualizations.\nPractice creating these plots by hand to deepen your understanding of their construction and interpretation. Then, leverage the power of R and ggplot2 to quickly create and customize these visualizations for larger datasets and more complex analyses.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html",
    "href": "rozdzial6.html",
    "title": "8  Wizualizacja Danych: z przykładami w R",
    "section": "",
    "text": "8.1 Wprowadzenie do Typów Danych i Wizualizacji\nW tym rozdziale poznamy podstawowe typy wizualizacji danych: wykresy słupkowe, histogramy i wykresy pudełkowe. Omówimy ich tworzenie zarówno ręcznie, jak i przy użyciu R.\nPrzed zagłębieniem się w konkretne techniki wizualizacji, ważne jest zrozumienie różnych typów danych i ich wpływu na wybór metody wizualizacji. Przeanalizujemy te koncepcje na praktycznych przykładach z użyciem biblioteki ggplot2 w R.\nNajpierw załadujmy niezbędne biblioteki:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-słupkowe",
    "href": "rozdzial6.html#wykresy-słupkowe",
    "title": "8  Wizualizacja Danych: z przykładami w R",
    "section": "8.2 Wykresy Słupkowe",
    "text": "8.2 Wykresy Słupkowe\nWykresy słupkowe doskonale nadają się do prezentacji danych kategorycznych lub podsumowania danych ciągłych w grupach.\n\nZrozumienie Wykresów Słupkowych\nWykres słupkowy przedstawia dane za pomocą prostokątnych słupków, których wysokość jest proporcjonalna do reprezentowanych przez nie wartości. Służą do porównywania różnych kategorii lub grup.\nGłówne elementy wykresu słupkowego: 1. Oś X: Reprezentuje kategorie 2. Oś Y: Reprezentuje wartości (mogą to być liczebności, procenty lub dowolne wartości numeryczne) 3. Słupki: Prostokąt dla każdej kategorii, wysokość odpowiada jej wartości\n\nPrzykładowe Dane\nUżyjmy prostego zestawu danych dotyczącego sprzedaży owoców:\n\nowoce &lt;- c(\"Jabłko\", \"Banan\", \"Pomarańcza\", \"Winogrono\")\nsprzedaz &lt;- c(120, 85, 70, 100)\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(owoc = owoce, sprzedaz = sprzedaz)\n\n\n\n\nRęcznie Rysowany Wykres Słupkowy\nAby stworzyć wykres słupkowy ręcznie:\n\nNarysuj linię poziomą (oś X) i pionową (oś Y) prostopadłe do siebie.\nOznacz oś X swoimi kategoriami (owocami), równomiernie rozmieszczonymi.\nOznacz oś Y odpowiednią skalą dla Twoich wartości (sprzedaż, od 0 do 120 z przyrostami co 20).\nDla każdej kategorii narysuj prostokąt (słupek), którego wysokość odpowiada jej wartości na skali osi Y.\nJeśli chcesz, pokoloruj lub zacienuj każdy słupek.\nDodaj tytuł i etykiety dla obu osi.\n\n\n\n\n\n\n\nTip\n\n\n\nPrzy rysowaniu ręcznym użyj papieru milimetrowego dla dokładniejszych pomiarów i prostszych linii. Wybierz skalę, która pozwoli zmieścić wszystkie dane, maksymalnie wykorzystując dostępną przestrzeń.\n\n\n\n\nWykres Słupkowy w Podstawowym R\n\n# Tworzenie wykresu słupkowego\nbarplot(sprzedaz, names.arg = owoce, \n        main = \"Sprzedaż Owoców\",\n        xlab = \"Rodzaje Owoców\", ylab = \"Sprzedaż\")\n\n\n\n\n\n\n\n\n\n\nWykres Słupkowy z ggplot2\n\n# Tworzenie wykresu słupkowego z ggplot2\nggplot(df, aes(x = owoc, y = sprzedaz)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Sprzedaż Owoców\",\n       x = \"Rodzaje Owoców\", y = \"Sprzedaż\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpretacja Wykresów Słupkowych\nPodczas interpretacji wykresu słupkowego zwróć uwagę na:\n\nWzględne Wysokości: Porównaj wysokości słupków, aby zrozumieć, które kategorie mają wyższe lub niższe wartości.\nKolejność: Czasami słupki są uporządkowane według wysokości, aby ułatwić porównania.\nWzorce: Poszukaj wzorców lub trendów między kategoriami.\nWartości Odstające: Zidentyfikuj słupki, które są znacznie wyższe lub niższe od pozostałych.\n\n\nPrzykładowa Interpretacja\nDla naszych danych o sprzedaży owoców:\n\nJabłka mają najwyższą sprzedaż (120), następnie Winogrona (100).\nPomarańcze mają najniższą sprzedaż (70).\nIstnieje znaczna różnica między najwyższą (Jabłka) a najniższą (Pomarańcze) sprzedażą.\nBanany i Winogrona mają podobne wartości sprzedaży, w średnim zakresie.\n\nTa informacja może być przydatna dla zarządzania zapasami lub strategii marketingowych w sklepie owocowym.\n\n\n\n\n\n\nNote\n\n\n\nWykresy słupkowe są świetne do porównywania kategorii, ale nie pokazują rozkładu wewnątrz każdej kategorii. Do tego mogą być potrzebne inne typy wykresów, jak wykresy pudełkowe.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#histogramy",
    "href": "rozdzial6.html#histogramy",
    "title": "8  Wizualizacja Danych: z przykładami w R",
    "section": "8.3 Histogramy",
    "text": "8.3 Histogramy\nHistogramy wizualizują rozkład zmiennej ciągłej poprzez podzielenie jej na przedziały (bins) i pokazanie częstości lub gęstości punktów danych w każdym przedziale.\n\nZrozumienie Histogramów\nGłówne elementy histogramu: 1. Oś X: Reprezentuje wartości zmiennej, podzielone na przedziały 2. Oś Y: Reprezentuje częstość, względną częstość lub gęstość 3. Słupki: Prostokąt dla każdego przedziału, wysokość odpowiada mierze na osi Y\nIstnieją trzy główne typy histogramów:\n\nHistogram Częstości: Oś Y pokazuje liczbę punktów danych w każdym przedziale.\nHistogram Częstości Względnej: Oś Y pokazuje proporcję punktów danych w każdym przedziale (częstość podzielona przez całkowitą liczbę punktów danych).\nHistogram Gęstości: Oś Y pokazuje gęstość, która jest częstością względną podzieloną przez szerokość przedziału. Całkowita powierzchnia wszystkich słupków sumuje się do 1.\n\n\nPrzykładowe Dane\nUżyjmy zbioru 50 wyników egzaminów studentów (na 100 punktów):\n\nset.seed(123)  # dla powtarzalności\nwyniki &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\nRęcznie Rysowany Histogram\nAby stworzyć histogram częstości ręcznie:\n\nZnajdź zakres danych.\nWybierz liczbę przedziałów (użyjmy 7 przedziałów).\nUtwórz tabelę częstości.\nNarysuj osie X i Y.\nOznacz oś X zakresami przedziałów, a oś Y częstością.\nNarysuj prostokąt dla każdego przedziału, z wysokością odpowiadającą jego częstości.\nDodaj tytuł i etykiety dla obu osi.\n\nDla histogramu częstości względnej, podziel każdą częstość przez całkowitą liczbę punktów danych przed narysowaniem słupków.\nDla histogramu gęstości, podziel częstość względną przez szerokość przedziału przed narysowaniem słupków.\n\n\n\n\n\n\nTip\n\n\n\nLiczba przedziałów może wpłynąć na interpretację. Zbyt mało przedziałów może ukryć ważne cechy, podczas gdy zbyt wiele może wprowadzić szum. Powszechną regułą jest użycie pierwiastka kwadratowego z liczby punktów danych jako liczby przedziałów.\n\n\n\n\nHistogramy w Podstawowym R\n\n# Histogram Częstości\nhist(wyniki, breaks = 7, \n     main = \"Histogram Częstości Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Częstość\")\n\n\n\n\n\n\n\n# Histogram Częstości Względnej\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Częstości Względnej Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Częstość Względna\")\n\n\n\n\n\n\n\n# Histogram Gęstości\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Gęstości Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Gęstość\")\nlines(density(wyniki), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\nHistogramy z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wynik = wyniki)\n\n# Histogram Częstości\nggplot(df, aes(x = wynik)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Częstości Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Częstość\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram Częstości Względnej\nggplot(df, aes(x = wynik, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Częstości Względnej Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Częstość Względna\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Histogram Gęstości\nggplot(df, aes(x = wynik)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Histogram Gęstości Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Gęstość\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nInterpretacja Histogramów\nPodczas interpretacji histogramu zwróć uwagę na:\n\nTendencję Centralną: Gdzie znajduje się szczyt rozkładu?\nRozrzut: Jak szeroki jest rozkład?\nKształt: Czy jest symetryczny, skośny, czy wielomodalny?\nWartości Odstające: Czy są nietypowe wartości daleko od głównego rozkładu?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-pudełkowe-i-wykresy-pudełkowe-tukeya",
    "href": "rozdzial6.html#wykresy-pudełkowe-i-wykresy-pudełkowe-tukeya",
    "title": "8  Wizualizacja Danych: z przykładami w R",
    "section": "8.4 Wykresy Pudełkowe i Wykresy Pudełkowe Tukeya",
    "text": "8.4 Wykresy Pudełkowe i Wykresy Pudełkowe Tukeya\nWykresy pudełkowe, znane również jako wykresy skrzynkowe, dostarczają zwięzłego podsumowania rozkładu. Skupimy się na wykresie pudełkowym w stylu Tukeya, nazwanym na cześć statystyka Johna Tukeya, który spopularyzował ten typ wykresu.\n\nZrozumienie Wykresów Pudełkowych\nWykres pudełkowy przedstawia pięć kluczowych statystyk:\n\nWartość minimalna (z wyłączeniem wartości odstających)\nPierwszy kwartyl (Q1)\nMediana\nTrzeci kwartyl (Q3)\nWartość maksymalna (z wyłączeniem wartości odstających)\n\nDodatkowo wykresy pudełkowe pokazują:\n\nWąsy: Linie rozciągające się od pudełka do wartości minimalnej i maksymalnej (z wyłączeniem wartości odstających)\nWartości odstające: Indywidualne punkty poza wąsami\n\n\nObliczanie Kwartyli i Wartości Odstających\nAby stworzyć wykres pudełkowy, postępuj zgodnie z tymi krokami:\n\nUporządkuj dane od najmniejszej do największej wartości.\nZnajdź medianę (środkowa wartość dla nieparzystej liczby punktów danych, średnia z dwóch środkowych wartości dla parzystej).\nZnajdź Q1 (mediana dolnej połowy danych) i Q3 (mediana górnej połowy danych).\nOblicz Rozstęp Międzykwartylowy (IQR) = Q3 - Q1\nOkreśl wartości odstające używając reguły Tukeya:\n\nDolne wartości odstające: &lt; Q1 - 1.5 * IQR\nGórne wartości odstające: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nWspółczynnik 1.5 w regule Tukeya dla wartości odstających opiera się na właściwościach rozkładu normalnego. Dla danych o rozkładzie normalnym, ta reguła identyfikuje około 0.7% danych jako potencjalne wartości odstające.\n\n\n\n\nPrzykładowe Dane\nUżyjmy małego zbioru danych do ilustracji:\n\ndane &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\nRęcznie Rysowany Wykres Pudełkowy Tukeya\nAby stworzyć wykres pudełkowy Tukeya ręcznie:\n\nNarysuj linię pionową reprezentującą zakres od minimum do maksimum (2 do 15 w naszym przykładzie, z wyłączeniem wartości odstającej).\nNarysuj pudełko od Q1 do Q3.\nNarysuj poziomą linię przez pudełko na poziomie mediany.\nNarysuj wąsy od pudełka do wartości minimalnej i maksymalnej (z wyłączeniem wartości odstających).\nPrzedstaw wartość odstającą (50) jako indywidualny punkt poza wąsem.\nDodaj skalę do osi pionowej i oznacz ją.\n\n\n\nWykres Pudełkowy w Podstawowym R\n\n# Tworzenie wykresu pudełkowego\nboxplot(dane, main = \"Wykres Pudełkowy Przykładowych Danych\",\n        ylab = \"Wartości\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\nWykres Pudełkowy Tukeya z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wartosc = dane)\n\n# Tworzenie wykresu pudełkowego Tukeya z ggplot2\nggplot(df, aes(x = \"\", y = wartosc)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Wykres Pudełkowy Tukeya Przykładowych Danych\",\n       x = \"\", y = \"Wartości\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\nInterpretacja Wykresów Pudełkowych\nPodczas interpretacji wykresu pudełkowego zwróć uwagę na następujące elementy:\n\nTendencja Centralna: Mediana pokazuje środek rozkładu.\nRozrzut: Pudełko (IQR) reprezentuje środkowe 50% danych.\nSkośność: Jeśli linia mediany jest bliżej jednego końca pudełka, rozkład jest skośny.\nWartości Odstające: Punkty poza wąsami są potencjalnymi wartościami odstającymi.\nPorównania: Przy porównywaniu wielu wykresów pudełkowych, zwróć uwagę na względne położenie median, rozmiary pudełek i obecność wartości odstających.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "href": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "title": "8  Wizualizacja Danych: z przykładami w R",
    "section": "8.5 Zaawansowane Techniki Wizualizacji",
    "text": "8.5 Zaawansowane Techniki Wizualizacji\nOprócz podstawowych typów wykresów, warto poznać kilka bardziej zaawansowanych technik wizualizacji, które mogą być przydatne w analizie danych.\n\nWykresy Skrzypcowe\nWykresy skrzypcowe łączą cechy wykresów pudełkowych i wykresów gęstości, dając bardziej kompletny obraz rozkładu danych.\n\n# Tworzenie wykresu skrzypcowego\nggplot(df, aes(x = \"\", y = wartosc)) +\n  geom_violin(fill = \"lightblue\") +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Wykres Skrzypcowy Przykładowych Danych\",\n       x = \"\", y = \"Wartości\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWykresy Rozrzutu z Marginesami\nŁączenie wykresów rozrzutu z histogramami na marginesach może dostarczyć więcej informacji o rozkładzie danych w dwóch wymiarach.\n\n# Generowanie danych do wykresu rozrzutu\nset.seed(123)\ndf_scatter &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100)\n)\n\n# Tworzenie wykresu rozrzutu z marginesami\nlibrary(ggExtra)\np &lt;- ggplot(df_scatter, aes(x = x, y = y)) +\n  geom_point() +\n  theme_minimal()\nggMarginal(p, type = \"histogram\", fill = \"lightblue\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wnioski",
    "href": "rozdzial6.html#wnioski",
    "title": "8  Wizualizacja Danych: z przykładami w R",
    "section": "8.6 Wnioski",
    "text": "8.6 Wnioski\nW tym rozdziale poznaliśmy trzy podstawowe typy wizualizacji danych: wykresy słupkowe, histogramy i wykresy pudełkowe. Pokazaliśmy, jak tworzyć te wykresy ręcznie, używając podstawowego systemu wykresów R oraz biblioteki ggplot2.\nKażdy typ wykresu służy innemu celowi: - Wykresy słupkowe doskonale nadają się do porównywania kategorii. - Histogramy pokazują rozkład zmiennej ciągłej. - Wykresy pudełkowe dostarczają zwięzłego podsumowania rozkładu, podkreślając tendencję centralną, rozrzut i wartości odstające.\nPamiętaj, że wybór wizualizacji zależy od typu danych i wniosków, które chcesz przekazać. Zawsze bierz pod uwagę swoją docelową grupę odbiorców i historię, którą chcesz opowiedzieć za pomocą swoich danych, wybierając i projektując wizualizacje.\nĆwicz tworzenie tych wykresów ręcznie, aby pogłębić zrozumienie ich konstrukcji i interpretacji. Następnie wykorzystaj moc R i ggplot2, aby szybko tworzyć i dostosowywać te wizualizacje dla większych zbiorów danych i bardziej złożonych analiz.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#ćwiczenia-praktyczne",
    "href": "rozdzial6.html#ćwiczenia-praktyczne",
    "title": "8  Wizualizacja Danych: z przykładami w R",
    "section": "8.7 Ćwiczenia Praktyczne",
    "text": "8.7 Ćwiczenia Praktyczne\n\nZbierz dane o popularności różnych gatunków muzycznych wśród Twoich znajomych. Stwórz wykres słupkowy przedstawiający te dane.\nZmierz czas reakcji 30 osób na bodziec dźwiękowy (w milisekundach). Utwórz histogram tych danych.\nZbierz dane o wzroście 50 osób w Twojej społeczności. Stwórz wykres pudełkowy dla tych danych, osobno dla mężczyzn i kobiet.\nZnajdź zestaw danych online (np. na Kaggle) i stwórz trzy różne wizualizacje dla tych danych. Opisz, jakie wnioski można wyciągnąć z każdej wizualizacji.\nStwórz wykres skrzypcowy dla danych o cenach domów w różnych dzielnicach miasta. Porównaj go z wykresem pudełkowym tych samych danych. Jakie dodatkowe informacje dostarcza wykres skrzypcowy?\n\nPamiętaj, że praktyka jest kluczem do opanowania sztuki wizualizacji danych. Eksperymentuj z różnymi typami wykresów i parametrami, aby znaleźć najlepszy sposób przedstawienia swoich danych.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "correg_en.html",
    "href": "correg_en.html",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "",
    "text": "9.1 Introduction\nThe distinction between correlation and causation represents a fundamental challenge in statistical analysis. While correlation measures the statistical association between variables, causation implies a direct influence of one variable on another.\nStatistical relationships form the backbone of data-driven decision making across disciplines—from economics and public health to psychology and environmental science. Understanding when a relationship indicates mere association versus genuine causality is crucial for valid inference and effective policy recommendations.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#covariance",
    "href": "correg_en.html#covariance",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.2 Covariance",
    "text": "9.2 Covariance\nCovariance measures how two variables vary together, indicating both the direction and magnitude of their linear relationship.\nFormula: \\text{cov}(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\nWhere:\n\nx_i and y_i are individual data points\n\\bar{x} and \\bar{y} are the means of variables X and Y\nn is the number of observations\nWe divide by (n-1) for sample covariance (Bessel’s correction)\n\n\nStep-by-Step Manual Calculation Process\nExample 1: Student Study Hours vs. Test Scores\nData:\n\nStudy Hours (X): 2, 4, 6, 8, 10\nTest Scores (Y): 65, 70, 80, 85, 95\n\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate means\n\\bar{x} = \\frac{2+4+6+8+10}{5} = 6 hours\n\n\n\n\n\\bar{y} = \\frac{65+70+80+85+95}{5} = 79 points\n\n\n2\nCalculate deviations\n(x_i - \\bar{x}): -4, -2, 0, 2, 4\n\n\n\n\n(y_i - \\bar{y}): -14, -9, 1, 6, 16\n\n\n3\nCalculate products\n(x_i - \\bar{x})(y_i - \\bar{y}):\n\n\n\n\n(-4)(-14) = 56\n\n\n\n\n(-2)(-9) = 18\n\n\n\n\n(0)(1) = 0\n\n\n\n\n(2)(6) = 12\n\n\n\n\n(4)(16) = 64\n\n\n4\nSum the products\n\\sum = 56 + 18 + 0 + 12 + 64 = 150\n\n\n5\nDivide by (n-1)\n\\text{cov}(X,Y) = \\frac{150}{5-1} = \\frac{150}{4} = 37.5\n\n\n\nR Verification:\n\n# Define the data\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Calculate covariance\ncov_manual &lt;- cov(study_hours, test_scores)\nprint(paste(\"Covariance:\", round(cov_manual, 2)))\n\n[1] \"Covariance: 37.5\"\n\n# Verify step by step\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\nproducts &lt;- x_dev * y_dev\ncov_calculated &lt;- sum(products) / (length(study_hours) - 1)\n\n# Display calculation steps\ndata.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_deviation = x_dev,\n  Y_deviation = y_dev,\n  Product = products\n)\n\n   X  Y X_deviation Y_deviation Product\n1  2 65          -4         -14      56\n2  4 70          -2          -9      18\n3  6 80           0           1       0\n4  8 85           2           6      12\n5 10 95           4          16      64\n\n\nInterpretation: The positive covariance (37.5) indicates that study hours and test scores tend to increase together.\n\n\nPractice Problem with Solution\nCalculate covariance manually for:\n\nTemperature (°F): 32, 50, 68, 86, 95\nIce Cream Sales ($): 100, 200, 400, 600, 800\n\nSolution:\n\n\n\nStep\nCalculation\n\n\n\n\n1. Means\n\\bar{x} = \\frac{32+50+68+86+95}{5} = 66.2°F\n\n\n\n\\bar{y} = \\frac{100+200+400+600+800}{5} = 420\n\n\n2. Deviations\nX: -34.2, -16.2, 1.8, 19.8, 28.8\n\n\n\nY: -320, -220, -20, 180, 380\n\n\n3. Products\n10944, 3564, -36, 3564, 10944\n\n\n4. Sum\n28980\n\n\n5. Covariance\n\\frac{28980}{4} = 7245\n\n\n\n\n# Verify practice problem\ntemp &lt;- c(32, 50, 68, 86, 95)\nsales &lt;- c(100, 200, 400, 600, 800)\nprint(paste(\"Covariance for practice problem:\", round(cov(temp, sales), 2)))\n\n[1] \"Covariance for practice problem: 7245\"",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#correlation-coefficient",
    "href": "correg_en.html#correlation-coefficient",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.3 Correlation Coefficient",
    "text": "9.3 Correlation Coefficient\nThe correlation coefficient standardizes covariance to eliminate scale dependency, producing values between -1 and +1.\n\nInterpretation Guidelines\n\n\n\n\n\n\n\n\n\nCorrelation Value\nStrength\nInterpretation\nExample\n\n\n\n\n±0.90 to ±1.00\nVery Strong\nAlmost perfect relationship\nHeight of parents and children\n\n\n±0.70 to ±0.89\nStrong\nHighly related variables\nStudy time and grades\n\n\n±0.50 to ±0.69\nModerate\nModerately related\nExercise and weight loss\n\n\n±0.30 to ±0.49\nWeak\nWeakly related\nShoe size and reading ability\n\n\n±0.00 to ±0.29\nVery Weak/None\nLittle to no relationship\nBirth month and intelligence\n\n\n\n\n\nTypes of Correlations Visualization\n\n# Generate sample data with different correlation patterns\nn &lt;- 100\n\n# Positive linear correlation\nset.seed(123)\nyears_education &lt;- rnorm(n, 14, 3)\nannual_income &lt;- 15000 + 3500 * years_education + rnorm(n, 0, 10000)\npos_cor &lt;- round(cor(years_education, annual_income), 3)\n\n# Negative linear correlation\nscreen_time &lt;- runif(n, 1, 8)\nsleep_hours &lt;- 9 - 0.5 * screen_time + rnorm(n, 0, 0.5)\nneg_cor &lt;- round(cor(screen_time, sleep_hours), 3)\n\n# No correlation\nx_random &lt;- rnorm(n, 50, 10)\ny_random &lt;- rnorm(n, 50, 10)\nno_cor &lt;- round(cor(x_random, y_random), 3)\n\n# Non-linear correlation (quadratic)\nhours_studied &lt;- runif(n, 0, 12)\ntest_score &lt;- -2 * (hours_studied - 6)^2 + 90 + rnorm(n, 0, 5)\nnonlin_cor &lt;- round(cor(hours_studied, test_score), 3)\n\n# Create data frames with correlation values\npositive_data &lt;- data.frame(\n  x = years_education, \n  y = annual_income,\n  label = paste0(\"Positive Linear (r = \", pos_cor, \")\")\n)\n\nnegative_data &lt;- data.frame(\n  x = screen_time, \n  y = sleep_hours,\n  label = paste0(\"Negative Linear (r = \", neg_cor, \")\")\n)\n\nno_corr_data &lt;- data.frame(\n  x = x_random, \n  y = y_random,\n  label = paste0(\"No Correlation (r = \", no_cor, \")\")\n)\n\nnonlinear_data &lt;- data.frame(\n  x = hours_studied, \n  y = test_score,\n  label = paste0(\"Non-linear (Pearson r = \", nonlin_cor, \")\")\n)\n\n# Combine data\nall_data &lt;- rbind(positive_data, negative_data, no_corr_data, nonlinear_data)\n\n# Create faceted plot\nggplot(all_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE, alpha = 0.3) +\n  facet_wrap(~ label, scales = \"free\", ncol = 2) +\n  labs(\n    title = \"Different Types of Correlations\",\n    subtitle = \"Linear regression line shown in red with confidence band\",\n    x = \"\", \n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, color = \"gray50\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#pearson-correlation",
    "href": "correg_en.html#pearson-correlation",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.4 Pearson Correlation",
    "text": "9.4 Pearson Correlation\nFormula: r = \\frac{\\text{cov}(X,Y)}{s_X s_Y} = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}}\n\nComplete Manual Calculation Example\nUsing our study hours example:\n\nStudy Hours (X): 2, 4, 6, 8, 10\nTest Scores (Y): 65, 70, 80, 85, 95\n\nDetailed Calculation Steps:\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate covariance\nFrom above: \\text{cov}(X,Y) = 37.5\n\n\n2\nCalculate deviations squared\n\n\n\n\nFor X\n(x_i - \\bar{x})^2: 16, 4, 0, 4, 16\n\n\n\n\nSum = 40\n\n\n\nFor Y\n(y_i - \\bar{y})^2: 196, 81, 1, 36, 256\n\n\n\n\nSum = 570\n\n\n3\nCalculate standard deviations\n\n\n\n\ns_X\ns_X = \\sqrt{\\frac{40}{4}} = \\sqrt{10} = 3.162\n\n\n\ns_Y\ns_Y = \\sqrt{\\frac{570}{4}} = \\sqrt{142.5} = 11.937\n\n\n4\nCalculate correlation\nr = \\frac{37.5}{3.162 \\times 11.937}\n\n\n\n\nr = \\frac{37.5}{37.73} = 0.994\n\n\n\n\n# Manual calculation verification\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Calculate Pearson correlation\nr_value &lt;- cor(study_hours, test_scores, method = \"pearson\")\nprint(paste(\"Pearson correlation coefficient:\", round(r_value, 3)))\n\n[1] \"Pearson correlation coefficient: 0.993\"\n\n# Detailed calculation\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\n\n# Show calculation table\ncalc_table &lt;- data.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_dev = x_dev,\n  Y_dev = y_dev,\n  X_dev_sq = x_dev^2,\n  Y_dev_sq = y_dev^2,\n  Product = x_dev * y_dev\n)\nprint(calc_table)\n\n   X  Y X_dev Y_dev X_dev_sq Y_dev_sq Product\n1  2 65    -4   -14       16      196      56\n2  4 70    -2    -9        4       81      18\n3  6 80     0     1        0        1       0\n4  8 85     2     6        4       36      12\n5 10 95     4    16       16      256      64\n\n# Summary statistics\ncat(\"\\nSummary:\")\n\n\nSummary:\n\ncat(\"\\nSum of (X-mean)²:\", sum(x_dev^2))\n\n\nSum of (X-mean)²: 40\n\ncat(\"\\nSum of (Y-mean)²:\", sum(y_dev^2))\n\n\nSum of (Y-mean)²: 570\n\ncat(\"\\nSum of products:\", sum(x_dev * y_dev))\n\n\nSum of products: 150\n\ncat(\"\\nStandard deviation of X:\", sd(study_hours))\n\n\nStandard deviation of X: 3.162278\n\ncat(\"\\nStandard deviation of Y:\", sd(test_scores))\n\n\nStandard deviation of Y: 11.93734\n\n# Calculate confidence interval and p-value\ncor_test &lt;- cor.test(study_hours, test_scores, method = \"pearson\")\nprint(cor_test)\n\n\n    Pearson's product-moment correlation\n\ndata:  study_hours and test_scores\nt = 15, df = 3, p-value = 0.0006431\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8994446 0.9995859\nsample estimates:\n      cor \n0.9933993 \n\n\nInterpretation: r = 0.994 indicates an almost perfect positive linear relationship between study hours and test scores. The p-value &lt; 0.05 suggests this relationship is statistically significant.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#spearman-rank-correlation",
    "href": "correg_en.html#spearman-rank-correlation",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.5 Spearman Rank Correlation",
    "text": "9.5 Spearman Rank Correlation\nSpearman correlation measures monotonic relationships using ranks instead of raw values.\nFormula: \\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2 - 1)}\nWhere d_i is the difference between ranks for observation i.\n\nComplete Manual Example\nData: Math and English Scores\n\n\n\nStudent\nMath Score\nEnglish Score\n\n\n\n\nA\n78\n82\n\n\nB\n85\n90\n\n\nC\n88\n88\n\n\nD\n92\n94\n\n\nE\n95\n96\n\n\n\nRanking and Calculation:\n\n\n\n\n\n\n\n\n\n\n\n\nStudent\nMath Score\nMath Rank\nEnglish Score\nEnglish Rank\nd = (Math Rank - English Rank)\nd²\n\n\n\n\nA\n78\n1\n82\n1\n0\n0\n\n\nB\n85\n2\n90\n3\n-1\n1\n\n\nC\n88\n3\n88\n2\n1\n1\n\n\nD\n92\n4\n94\n4\n0\n0\n\n\nE\n95\n5\n96\n5\n0\n0\n\n\n\n\n\n\n\nSum:\n2\n\n\n\nCalculation: \\rho = 1 - \\frac{6 \\times 2}{5(25-1)} = 1 - \\frac{12}{120} = 1 - 0.1 = 0.9\n\n# Data\nmath_scores &lt;- c(78, 85, 88, 92, 95)\nenglish_scores &lt;- c(82, 90, 88, 94, 96)\n\n# Show ranks\nrank_table &lt;- data.frame(\n  Student = LETTERS[1:5],\n  Math_Score = math_scores,\n  Math_Rank = rank(math_scores),\n  English_Score = english_scores,\n  English_Rank = rank(english_scores)\n)\n\nrank_table$d &lt;- rank_table$Math_Rank - rank_table$English_Rank\nrank_table$d_squared &lt;- rank_table$d^2\n\nprint(rank_table)\n\n  Student Math_Score Math_Rank English_Score English_Rank  d d_squared\n1       A         78         1            82            1  0         0\n2       B         85         2            90            3 -1         1\n3       C         88         3            88            2  1         1\n4       D         92         4            94            4  0         0\n5       E         95         5            96            5  0         0\n\ncat(\"\\nSum of d²:\", sum(rank_table$d_squared))\n\n\nSum of d²: 2\n\n# Calculate Spearman correlation\nrho &lt;- cor(math_scores, english_scores, method = \"spearman\")\ncat(\"\\nSpearman correlation (R):\", round(rho, 3))\n\n\nSpearman correlation (R): 0.9\n\n# Manual calculation\nn &lt;- length(math_scores)\nrho_manual &lt;- 1 - (6 * sum(rank_table$d_squared)) / (n * (n^2 - 1))\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.9",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#cross-tabulation-and-categorical-data",
    "href": "correg_en.html#cross-tabulation-and-categorical-data",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.6 Cross-tabulation and Categorical Data",
    "text": "9.6 Cross-tabulation and Categorical Data\nCross-tabulation shows relationships between categorical variables.\n\n# Create more realistic sample data\nset.seed(123)\nn_total &lt;- 120\n\n# Create education and employment data with realistic relationship\neducation &lt;- factor(\n  c(rep(\"High School\", 40), \n    rep(\"College\", 50), \n    rep(\"Graduate\", 30)),\n  levels = c(\"High School\", \"College\", \"Graduate\")\n)\n\n# Employment status with education-related probabilities\nemployment &lt;- factor(\n  c(# High School: 75% employed\n    sample(c(rep(\"Employed\", 30), rep(\"Unemployed\", 10)), 40),\n    # College: 90% employed  \n    sample(c(rep(\"Employed\", 45), rep(\"Unemployed\", 5)), 50),\n    # Graduate: 95% employed\n    sample(c(rep(\"Employed\", 28), rep(\"Unemployed\", 2)), 30)),\n  levels = c(\"Employed\", \"Unemployed\")\n)\n\n# Create contingency table\ncontingency_table &lt;- table(education, employment)\nprint(\"Contingency Table:\")\n\n[1] \"Contingency Table:\"\n\nprint(contingency_table)\n\n             employment\neducation     Employed Unemployed\n  High School       30         10\n  College           45          5\n  Graduate          28          2\n\n# Calculate row percentages\nprint(\"\\nRow Percentages (% within each education level):\")\n\n[1] \"\\nRow Percentages (% within each education level):\"\n\nprint(round(prop.table(contingency_table, 1) * 100, 1))\n\n             employment\neducation     Employed Unemployed\n  High School     75.0       25.0\n  College         90.0       10.0\n  Graduate        93.3        6.7\n\n# Chi-square test for independence\nchi_test &lt;- chisq.test(contingency_table)\n\nWarning in chisq.test(contingency_table): Chi-squared approximation may be\nincorrect\n\nprint(chi_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 5.9623, df = 2, p-value = 0.05073",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#practical-exercises-with-solutions",
    "href": "correg_en.html#practical-exercises-with-solutions",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.7 Practical Exercises with Solutions",
    "text": "9.7 Practical Exercises with Solutions\n\nExercise 1: Calculate Pearson Correlation Manually\nData:\n\nHeight (inches): 66, 68, 70, 72, 74\nWeight (pounds): 140, 155, 170, 185, 200\n\nSolution:\n\n# Data\nheight &lt;- c(66, 68, 70, 72, 74)\nweight &lt;- c(140, 155, 170, 185, 200)\n\n# Step 1: Calculate means\nh_mean &lt;- mean(height)  # 70\nw_mean &lt;- mean(weight)  # 170\n\n# Step 2: Calculate deviations and products\ncalculation_df &lt;- data.frame(\n  Height = height,\n  Weight = weight,\n  H_dev = height - h_mean,\n  W_dev = weight - w_mean,\n  H_dev_sq = (height - h_mean)^2,\n  W_dev_sq = (weight - w_mean)^2,\n  Product = (height - h_mean) * (weight - w_mean)\n)\n\nprint(calculation_df)\n\n  Height Weight H_dev W_dev H_dev_sq W_dev_sq Product\n1     66    140    -4   -30       16      900     120\n2     68    155    -2   -15        4      225      30\n3     70    170     0     0        0        0       0\n4     72    185     2    15        4      225      30\n5     74    200     4    30       16      900     120\n\n# Step 3: Calculate correlation\ncov_hw &lt;- sum(calculation_df$Product) / (length(height) - 1)\nsd_h &lt;- sqrt(sum(calculation_df$H_dev_sq) / (length(height) - 1))\nsd_w &lt;- sqrt(sum(calculation_df$W_dev_sq) / (length(weight) - 1))\nr_manual &lt;- cov_hw / (sd_h * sd_w)\n\ncat(\"\\nManual calculation:\")\n\n\nManual calculation:\n\ncat(\"\\nCovariance:\", round(cov_hw, 2))\n\n\nCovariance: 75\n\ncat(\"\\nSD Height:\", round(sd_h, 2))\n\n\nSD Height: 3.16\n\ncat(\"\\nSD Weight:\", round(sd_w, 2))\n\n\nSD Weight: 23.72\n\ncat(\"\\nCorrelation:\", round(r_manual, 3))\n\n\nCorrelation: 1\n\n# Verify with R\ncat(\"\\n\\nR calculation:\", round(cor(height, weight), 3))\n\n\n\nR calculation: 1\n\n\n\n\nExercise 2: Calculate Spearman Correlation Manually\nData:\n\nStudent rankings in Math: 1, 3, 2, 5, 4\nStudent rankings in Science: 2, 4, 1, 5, 3\n\nSolution:\n\n# Rankings (already ranked)\nmath_rank &lt;- c(1, 3, 2, 5, 4)\nscience_rank &lt;- c(2, 4, 1, 5, 3)\n\n# Calculate differences\nd &lt;- math_rank - science_rank\nd_squared &lt;- d^2\n\n# Create table\nrank_df &lt;- data.frame(\n  Student = 1:5,\n  Math_Rank = math_rank,\n  Science_Rank = science_rank,\n  d = d,\n  d_squared = d_squared\n)\n\nprint(rank_df)\n\n  Student Math_Rank Science_Rank  d d_squared\n1       1         1            2 -1         1\n2       2         3            4 -1         1\n3       3         2            1  1         1\n4       4         5            5  0         0\n5       5         4            3  1         1\n\n# Calculate Spearman correlation\nn &lt;- length(math_rank)\nsum_d_sq &lt;- sum(d_squared)\nrho_manual &lt;- 1 - (6 * sum_d_sq) / (n * (n^2 - 1))\n\ncat(\"\\nSum of d²:\", sum_d_sq)\n\n\nSum of d²: 4\n\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.8\n\ncat(\"\\nSpearman correlation (R):\", round(cor(math_rank, science_rank, method = \"spearman\"), 3))\n\n\nSpearman correlation (R): 0.8\n\n\n\n\nExercise 3: Interpretation Practice\nInterpret these correlation values:\n\nr = 0.85 between hours of practice and performance score\n\nAnswer: Strong positive relationship. As practice hours increase, performance scores tend to increase substantially.\n\nr = -0.72 between outside temperature and heating costs\n\nAnswer: Strong negative relationship. As temperature increases, heating costs decrease substantially.\n\nr = 0.12 between shoe size and intelligence\n\nAnswer: Very weak/no meaningful relationship. Shoe size and intelligence are essentially unrelated.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#important-points-to-remember",
    "href": "correg_en.html#important-points-to-remember",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.8 Important Points to Remember",
    "text": "9.8 Important Points to Remember\n\nCorrelation measures relationship strength: Values range from -1 to +1\nCorrelation ≠ Causation: High correlation doesn’t prove one variable causes another\nChoose the right method:\n\nPearson: For linear relationships in continuous data\nSpearman: For monotonic relationships or ranked data\n\nCheck assumptions:\n\nPearson assumes linear relationship and normal distribution\nSpearman only assumes monotonic relationship\n\nWatch for outliers: Extreme values can greatly affect Pearson correlation\nVisualize your data: Always plot before calculating correlation",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-decision-tree-for-correlation-analysis",
    "href": "correg_en.html#summary-decision-tree-for-correlation-analysis",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.9 Summary: Decision Tree for Correlation Analysis",
    "text": "9.9 Summary: Decision Tree for Correlation Analysis\n\n\n\nCHOOSING THE RIGHT CORRELATION METHOD:\n\nIs your data numerical?\n├─ YES → Is the relationship linear?\n│   ├─ YES → Use PEARSON correlation\n│   └─ NO → Is it monotonic?\n│       ├─ YES → Use SPEARMAN correlation\n│       └─ NO → Consider non-linear methods\n└─ NO → Is it ordinal (ranked)?\n    ├─ YES → Use SPEARMAN correlation\n    └─ NO → Use CROSS-TABULATION for categorical data\n\n\n\nQuick Reference Card\n\n\n\n\n\n\n\n\n\nMeasure\nUse When\nFormula\nRange\n\n\n\n\nCovariance\nInitial exploration of relationship\n\\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n-∞ to +∞\n\n\nPearson r\nLinear relationships, continuous data\n\\frac{\\text{cov}(X,Y)}{s_X s_Y}\n-1 to +1\n\n\nSpearman ρ\nMonotonic relationships, ranked data\n1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n-1 to +1\n\n\nCross-tabs\nCategorical variables\nFrequency counts\nN/A",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#introduction-to-statistical-modeling-regression-analysis-and-ols-method",
    "href": "correg_en.html#introduction-to-statistical-modeling-regression-analysis-and-ols-method",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.10 Introduction to Statistical Modeling, Regression Analysis, and OLS Method",
    "text": "9.10 Introduction to Statistical Modeling, Regression Analysis, and OLS Method\n\nDeterministic vs. Stochastic Models\nIn science and social sciences, we work with two fundamentally different types of models:\nDeterministic models assume perfect, exact relationships with no randomness. For example, in physics:\nd = v \\cdot t\nIf velocity (v) is 50 km/h and time (t) is 2 hours, then distance (d) is exactly 100 km. No variation, no uncertainty.\nStochastic models acknowledge that relationships are not perfect – they include randomness and uncertainty. In social sciences, we almost always work with stochastic models because:\n\nHuman behavior is complex and influenced by many factors\nWe can’t measure everything that matters\nThere’s genuine randomness in social processes\nMeasurement itself contains errors\n\nA stochastic model captures this by writing:\nY_i = f(X_i) + \\varepsilon_i\nwhere:\n\nY_i is the outcome we want to predict (e.g., voter turnout)\nX_i is our predictor variable (e.g., GDP per capita)\nf(X_i) is the systematic part – the pattern we can explain\n\\varepsilon_i is the stochastic part – the random error or unexplained variation\n\nExample: Predicting exam scores from study hours. Even if we know someone studied 5 hours, we can’t predict their exact score – maybe they had a bad day, maybe the material was particularly hard for them, maybe they’re naturally talented. The best we can do is predict the average score for people who studied 5 hours, acknowledging there will be variation around this average.\nThe key insight: we’re trying to model the systematic relationship while acknowledging that perfect prediction is impossible.\n\n\nVisualizing Relationships: Scatterplots and Types of Patterns\nBefore fitting any model, we should always visualize our data. A scatterplot displays the relationship between two variables. Different datasets can show very different patterns:\n\npar(mfrow = c(2, 3))\nset.seed(123)\nn &lt;- 50\n\n# 1. Strong positive linear relationship\nx1 &lt;- runif(n, 0, 10)\ny1 &lt;- 2 + 3*x1 + rnorm(n, 0, 3)\nplot(x1, y1, pch = 19, col = \"steelblue\",\n     main = \"Strong Positive Linear\",\n     xlab = \"X\", ylab = \"Y\")\n\n# 2. Weak positive linear relationship\nx2 &lt;- runif(n, 0, 10)\ny2 &lt;- 2 + 1.5*x2 + rnorm(n, 0, 8)\nplot(x2, y2, pch = 19, col = \"steelblue\",\n     main = \"Weak Positive Linear\",\n     xlab = \"X\", ylab = \"Y\")\n\n# 3. Negative linear relationship\nx3 &lt;- runif(n, 0, 10)\ny3 &lt;- 50 - 2*x3 + rnorm(n, 0, 4)\nplot(x3, y3, pch = 19, col = \"steelblue\",\n     main = \"Negative Linear\",\n     xlab = \"X\", ylab = \"Y\")\n\n# 4. Curvilinear (quadratic) relationship\nx4 &lt;- runif(n, 0, 10)\ny4 &lt;- 10 + 3*x4 - 0.3*x4^2 + rnorm(n, 0, 2)\nplot(x4, y4, pch = 19, col = \"steelblue\",\n     main = \"Curvilinear (Quadratic)\",\n     xlab = \"X\", ylab = \"Y\")\n\n# 5. No relationship\nx5 &lt;- runif(n, 0, 10)\ny5 &lt;- rnorm(n, 30, 8)\nplot(x5, y5, pch = 19, col = \"steelblue\",\n     main = \"No Relationship\",\n     xlab = \"X\", ylab = \"Y\")\n\n# 6. Non-linear (exponential-like)\nx6 &lt;- runif(n, 0, 10)\ny6 &lt;- 5 * exp(0.35*x6) + rnorm(n, 0, 3)\nplot(x6, y6, pch = 19, col = \"steelblue\",\n     main = \"Non-linear (Exponential)\",\n     xlab = \"X\", ylab = \"Y\")\n\n\n\n\n\n\n\nFigure 9.1: Different types of relationships between variables\n\n\n\n\n\nWhy focus on linear relationships?\nLinear relationships are our starting point for several reasons:\n\nMany real-world relationships are approximately linear (at least within a limited range)\nLinear models are simple, interpretable, and mathematically tractable\nThey serve as a baseline – if a linear model doesn’t fit well, we know we need something more complex\nMany non-linear relationships can be transformed to linear ones (e.g., logarithmic transformations)\n\nImportant: If your scatterplot shows a clear curvilinear or non-linear pattern, a straight line will be a poor fit. Always check your data first!\n\n\nA Simple Example: Study Hours and Exam Scores\nFor the rest of this section, we’ll work with a dataset that shows an approximately linear relationship:\n\n# Generate example data\nset.seed(123)\nn &lt;- 50\nstudy_hours &lt;- runif(n, 0, 10)\nexam_scores &lt;- 50 + 4 * study_hours + rnorm(n, 0, 8)\n\n# Create scatterplot\nplot(study_hours, exam_scores,\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     main = \"Scatterplot: Study Hours vs. Exam Scores\",\n     pch = 19, col = \"steelblue\")\n\n\n\n\n\n\n\nFigure 9.2: Relationship between study hours and exam scores\n\n\n\n\n\nIn this scatterplot, we can see a general positive linear relationship: students who study more tend to score higher. But the relationship isn’t perfect – there’s scatter around any imaginary line we might draw through the points.\n\n\nThe Idea of a “Best Fit” Line\nLooking at the scatterplot, we might want to summarize the relationship with a straight line. But which line? There are infinitely many lines we could draw. We need a criterion for what makes a line “best.”\nThe regression line (or “best fit line”) is the line that minimizes the sum of squared errors (SSE). Let’s understand this step by step.\nIf we propose a line with equation \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i, then for each observation i, we can calculate:\n\nThe predicted value: \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\nThe residual (or error): e_i = Y_i - \\hat{Y}_i\n\nThe residual is the vertical distance between the actual point and our line. The Sum of Squared Errors is:\nSSE = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\nWe square the errors for two reasons:\n\nPositive and negative errors don’t cancel out\nLarger errors are penalized more heavily (a 4-unit error counts as 16 in SSE, while two 2-unit errors count as 8)\n\n\n\nVisualizing Different Lines and Their SSE\nLet’s see how SSE changes for different lines:\n\n# Try three different lines\npar(mfrow = c(2, 2))\n\n# Line 1: Too flat\nplot(study_hours, exam_scores, pch = 19, col = \"steelblue\",\n     xlab = \"Study Hours\", ylab = \"Exam Score\",\n     main = \"Line 1: Too Flat (slope = 2)\")\nabline(a = 55, b = 2, col = \"red\", lwd = 2)\npred1 &lt;- 55 + 2 * study_hours\nsse1 &lt;- sum((exam_scores - pred1)^2)\ntext(2, 90, paste(\"SSE =\", round(sse1, 1)), col = \"red\")\n\n# Line 2: Too steep\nplot(study_hours, exam_scores, pch = 19, col = \"steelblue\",\n     xlab = \"Study Hours\", ylab = \"Exam Score\",\n     main = \"Line 2: Too Steep (slope = 6)\")\nabline(a = 45, b = 6, col = \"orange\", lwd = 2)\npred2 &lt;- 45 + 6 * study_hours\nsse2 &lt;- sum((exam_scores - pred2)^2)\ntext(2, 90, paste(\"SSE =\", round(sse2, 1)), col = \"orange\")\n\n# Line 3: OLS line (best fit)\nplot(study_hours, exam_scores, pch = 19, col = \"steelblue\",\n     xlab = \"Study Hours\", ylab = \"Exam Score\",\n     main = \"Line 3: OLS Line (Best Fit)\")\nmodel &lt;- lm(exam_scores ~ study_hours)\nabline(model, col = \"darkgreen\", lwd = 2)\nsse3 &lt;- sum(residuals(model)^2)\ntext(2, 90, paste(\"SSE =\", round(sse3, 1)), col = \"darkgreen\")\n\n# Visualize residuals for OLS line\nplot(study_hours, exam_scores, pch = 19, col = \"steelblue\",\n     xlab = \"Study Hours\", ylab = \"Exam Score\",\n     main = \"OLS Line with Residuals\")\nabline(model, col = \"darkgreen\", lwd = 2)\npred3 &lt;- predict(model)\nsegments(study_hours, exam_scores, study_hours, pred3, \n         col = \"red\", lty = 2)\n\n\n\n\n\n\n\nFigure 9.3: Comparing different lines and their SSE values\n\n\n\n\n\nNotice that the OLS line has the smallest SSE – that’s what makes it the “best” fit line.\n\n\nThe Zero Model: Predicting with the Mean\nBefore introducing predictors, let’s consider the simplest possible model: predicting the same value for everyone. What value should we predict?\nThe zero model (or intercept-only model) predicts the mean for all observations:\n\\hat{Y}_i = \\bar{Y}\nThis might seem useless, but it serves as a baseline for comparison. Any predictor we add should perform better than just guessing the mean.\n\nplot(study_hours, exam_scores, pch = 19, col = \"steelblue\",\n     xlab = \"Study Hours\", ylab = \"Exam Score\",\n     main = \"Zero Model: Predict Mean for Everyone\")\nmean_score &lt;- mean(exam_scores)\nabline(h = mean_score, col = \"purple\", lwd = 2)\ntext(5, mean_score + 5, paste(\"Mean =\", round(mean_score, 1)), \n     col = \"purple\")\n\n\n\n\n\n\n\nFigure 9.4: Zero model: predicting the mean for everyone\n\n\n\n\n\n\n\nVariance Decomposition: How Good is Our Model?\nNow comes the crucial question: does including X (study hours) improve our predictions compared to the zero model?\nWe can decompose the total variance in Y into two parts:\n\nExplained variance: variation captured by our model\nUnexplained variance: residual variation (what our model misses)\n\nMathematically, for each observation:\n\\underbrace{Y_i - \\bar{Y}}_{\\text{Total deviation}} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{\\text{Explained deviation}} + \\underbrace{Y_i - \\hat{Y}_i}_{\\text{Residual}}\nSquaring and summing across all observations:\n\\underbrace{\\sum (Y_i - \\bar{Y})^2}_{\\text{Total Sum of Squares (SST)}} = \\underbrace{\\sum (\\hat{Y}_i - \\bar{Y})^2}_{\\text{Regression Sum of Squares (SSR)}} + \\underbrace{\\sum (Y_i - \\hat{Y}_i)^2}_{\\text{Sum of Squared Errors (SSE)}}\nOr more compactly:\nSST = SSR + SSE\nManual calculation example:\nSuppose we have five students:\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\n1\n2\n60\n\n\n2\n4\n73\n\n\n3\n6\n71\n\n\n4\n8\n86\n\n\n5\n10\n90\n\n\n\nStep 1: Calculate the mean: \\bar{Y} = \\frac{60 + 73 + 71 + 86 + 90}{5} = 76\nStep 2: Calculate \\bar{X} = \\frac{2 + 4 + 6 + 8 + 10}{5} = 6\nStep 3: Fit OLS line (we’ll show the formula below):\nFirst, calculate the slope:\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\n1\n2\n60\n-4\n-16\n64\n16\n\n\n2\n4\n73\n-2\n-3\n6\n4\n\n\n3\n6\n71\n0\n-5\n0\n0\n\n\n4\n8\n86\n2\n10\n20\n4\n\n\n5\n10\n90\n4\n14\n56\n16\n\n\n\n\n\n\n\nSum = 146\nSum = 40\n\n\n\n\\hat{\\beta}_1 = \\frac{146}{40} = 3.65\n\\hat{\\beta}_0 = 76 - 3.65 \\times 6 = 76 - 21.9 = 54.1\nSo our regression line is: \\hat{Y}_i = 54.1 + 3.65X_i\nStep 4: Calculate predictions and variance components:\n\n\n\n\n\n\n\n\n\n\n\n\ni\nY_i\n\\bar{Y}\n\\hat{Y}_i\nY_i - \\bar{Y}\n\\hat{Y}_i - \\bar{Y}\nY_i - \\hat{Y}_i\n\n\n\n\n1\n60\n76\n61.4\n-16\n-14.6\n-1.4\n\n\n2\n73\n76\n68.7\n-3\n-7.3\n4.3\n\n\n3\n71\n76\n76.0\n-5\n0.0\n-5.0\n\n\n4\n86\n76\n83.3\n10\n7.3\n2.7\n\n\n5\n90\n76\n90.6\n14\n14.6\n-0.6\n\n\n\nStep 5: Sum of squares:\n\nSST = (-16)^2 + (-3)^2 + (-5)^2 + 10^2 + 14^2 = 256 + 9 + 25 + 100 + 196 = 586\nSSR = (-14.6)^2 + (-7.3)^2 + 0^2 + 7.3^2 + 14.6^2 = 213.16 + 53.29 + 0 + 53.29 + 213.16 = 532.9\nSSE = (-1.4)^2 + 4.3^2 + (-5.0)^2 + 2.7^2 + (-0.6)^2 = 1.96 + 18.49 + 25 + 7.29 + 0.36 = 53.1\n\nCheck: 532.9 + 53.1 = 586 ✓\n\n\nFinding the OLS Coefficients: Minimizing SSE\nThe Ordinary Least Squares (OLS) method finds the coefficients \\hat{\\beta}_0 (intercept) and \\hat{\\beta}_1 (slope) that minimize SSE.\nFor a simple linear regression Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, the OLS estimators are:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} = \\frac{Cov(X, Y)}{Var(X)}\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\nWe already showed this calculation in the previous section. The key insight is that:\n\n\\hat{\\beta}_1 measures the average change in Y per unit change in X\n\\hat{\\beta}_0 is the predicted value of Y when X = 0\n\nInterpretation of our example: \\hat{Y}_i = 54.1 + 3.65X_i\nEach additional hour of study is associated with a 3.65-point increase in exam score. A student who doesn’t study at all (X = 0) would be predicted to score 54.1 points (though this extrapolation may not be meaningful).\n\n\nEvaluating Model Quality: R-squared\nThe R-squared (R^2) tells us what proportion of variance in Y is explained by our model:\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nFrom our example: R^2 = \\frac{532.9}{586} = 0.909 or 1 - \\frac{53.1}{586} = 0.909\nInterpretation: 90.9% of the variance in exam scores is explained by study hours. Our model does a good job, though there’s still about 9% of variance that remains unexplained – likely due to other factors like natural ability, test anxiety, quality of study, or prior knowledge.\nProperties of R^2:\n\nRanges from 0 to 1\nR^2 = 0: model no better than predicting the mean\nR^2 = 1: perfect predictions (all points on the line)\nHigher R^2 means better fit, but doesn’t guarantee causality or good predictions out of sample\n\n\npar(mfrow = c(1, 3))\n\n# High R-squared\nset.seed(1)\nx1 &lt;- 1:30\ny1 &lt;- 2 + 3*x1 + rnorm(30, 0, 3)\nplot(x1, y1, pch = 19, col = \"steelblue\", main = paste(\"High R² =\", \n     round(summary(lm(y1 ~ x1))$r.squared, 2)))\nabline(lm(y1 ~ x1), col = \"red\", lwd = 2)\n\n# Medium R-squared\ny2 &lt;- 2 + 3*x1 + rnorm(30, 0, 15)\nplot(x1, y2, pch = 19, col = \"steelblue\", main = paste(\"Medium R² =\", \n     round(summary(lm(y2 ~ x1))$r.squared, 2)))\nabline(lm(y2 ~ x1), col = \"red\", lwd = 2)\n\n# Low R-squared\ny3 &lt;- 2 + 3*x1 + rnorm(30, 0, 30)\nplot(x1, y3, pch = 19, col = \"steelblue\", main = paste(\"Low R² =\", \n     round(summary(lm(y3 ~ x1))$r.squared, 2)))\nabline(lm(y3 ~ x1), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\nFigure 9.5: Different R-squared values\n\n\n\n\n\n\n\nEvaluating Model Quality: RMSE\nWhile R^2 is scale-free, Root Mean Squared Error (RMSE) gives us the average prediction error in the original units:\nRMSE = \\sqrt{\\frac{SSE}{n}} = \\sqrt{\\frac{\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2}{n}}\nFrom our example: RMSE = \\sqrt{\\frac{53.1}{5}} = \\sqrt{10.62} = 3.26 points\nInterpretation: On average, our predictions are off by about 3.3 points.\nRMSE is useful for:\n\nComparing models on the same outcome variable\nUnderstanding prediction accuracy in meaningful units\nPenalizing large errors (like SSE)\n\n\n# Calculate RMSE\nmodel &lt;- lm(exam_scores ~ study_hours)\nrmse &lt;- sqrt(mean(residuals(model)^2))\ncat(\"RMSE:\", round(rmse, 2), \"points\\n\")\n\nRMSE: 7.37 points\n\ncat(\"R-squared:\", round(summary(model)$r.squared, 3), \"\\n\")\n\nR-squared: 0.743 \n\n\n\n\nModel Interpretation, Effect Size, and Diagnostics\nOnce we’ve fit a regression model, we need to go beyond R^2 and RMSE to understand what our model tells us. This involves three key tasks: interpreting coefficients substantively, assessing effect sizes, and checking for potential problems through diagnostics.\n\nInterpreting Regression Coefficients\nThe slope coefficient \\hat{\\beta}_1 has a specific interpretation:\n\nA one-unit increase in X is associated with a \\hat{\\beta}_1-unit change in Y, on average.\n\nFor our example (\\hat{Y}_i = 54.1 + 3.65X_i):\n\nSlope interpretation: Each additional hour of study is associated with a 3.65-point increase in exam score, on average.\nIntercept interpretation: A student with zero study hours would be predicted to score 54.1 points (though this may involve extrapolation beyond the data range).\n\nImportant considerations:\n\nThe coefficient describes association, not necessarily causation\nThe relationship is linear – we assume the effect is constant across the range of X\nThe effect is an average – individual students may vary\n\nPractical interpretation example:\nIf you study 3 hours vs. 6 hours (a 3-hour difference), the predicted score difference is:\n3 \\times 3.65 = 10.95 \\text{ points}\nThis gives us a practical sense of the relationship’s magnitude.\n\n\nEffect Size: Standardized Coefficients\nWhen comparing effects across different variables or studies, raw coefficients can be misleading because they depend on the units of measurement. Is a 3.65-point increase per hour “large” or “small”?\nStandardized coefficients (beta coefficients) express the effect in standard deviation units:\n\\hat{\\beta}_1^{*} = \\hat{\\beta}_1 \\times \\frac{SD_X}{SD_Y}\nThis tells us: a one-standard-deviation increase in X is associated with a \\hat{\\beta}_1^{*}-standard-deviation change in Y.\nManual calculation for our example:\nFirst, calculate standard deviations:\n\nSD_X = \\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{n-1}} = \\sqrt{\\frac{40}{4}} = \\sqrt{10} = 3.16 hours\nSD_Y = \\sqrt{\\frac{\\sum(Y_i - \\bar{Y})^2}{n-1}} = \\sqrt{\\frac{586}{4}} = \\sqrt{146.5} = 12.10 points\n\nThen:\n\\hat{\\beta}_1^{*} = 3.65 \\times \\frac{3.16}{12.10} = 3.65 \\times 0.261 = 0.953\nInterpretation: A one-standard-deviation increase in study hours (about 3.2 hours) is associated with a 0.953-standard-deviation increase in exam scores (about 11.5 points).\nRule of thumb for effect sizes:\n\nSmall effect: |\\hat{\\beta}_1^{*}| \\approx 0.1\nMedium effect: |\\hat{\\beta}_1^{*}| \\approx 0.3\nLarge effect: |\\hat{\\beta}_1^{*}| \\approx 0.5 or higher\n\nOur effect (0.953) is very large, suggesting study hours have a substantial relationship with exam scores.\n\n# Calculate standardized coefficient\nmodel &lt;- lm(exam_scores ~ study_hours)\nbeta_raw &lt;- coef(model)[2]\nbeta_std &lt;- beta_raw * (sd(study_hours) / sd(exam_scores))\ncat(\"Raw coefficient:\", round(beta_raw, 3), \"\\n\")\n\nRaw coefficient: 4.305 \n\ncat(\"Standardized coefficient:\", round(beta_std, 3), \"\\n\")\n\nStandardized coefficient: 0.862 \n\n\n\n\nDiagnostic Plots: Residual Analysis\nAfter fitting a model, we should check whether our assumptions hold. The most useful diagnostic tool is the residual plot: plotting residuals (e_i = Y_i - \\hat{Y}_i) against predicted values (\\hat{Y}_i).\nWhat to look for:\n\nRandom scatter: Residuals should be randomly scattered around zero\nConstant spread: The vertical spread should be roughly constant (homoscedasticity)\nNo patterns: Curved patterns suggest non-linearity\nNo outliers: Extremely large residuals may indicate problems\n\n\npar(mfrow = c(2, 2))\n\n# 1. Residuals vs. Fitted\nmodel &lt;- lm(exam_scores ~ study_hours)\nplot(fitted(model), residuals(model),\n     xlab = \"Fitted Values\",\n     ylab = \"Residuals\",\n     main = \"Residuals vs. Fitted\",\n     pch = 19, col = \"steelblue\")\nabline(h = 0, col = \"red\", lty = 2, lwd = 2)\n\n# 2. Histogram of residuals\nhist(residuals(model), \n     main = \"Histogram of Residuals\",\n     xlab = \"Residuals\",\n     col = \"lightblue\",\n     breaks = 10)\n\n# 3. Q-Q plot (checking normality)\nqqnorm(residuals(model), pch = 19, col = \"steelblue\")\nqqline(residuals(model), col = \"red\", lwd = 2)\n\n# 4. Residuals vs. Predictor\nplot(study_hours, residuals(model),\n     xlab = \"Study Hours\",\n     ylab = \"Residuals\",\n     main = \"Residuals vs. Study Hours\",\n     pch = 19, col = \"steelblue\")\nabline(h = 0, col = \"red\", lty = 2, lwd = 2)\n\n\n\n\n\n\n\nFigure 9.6: Residual diagnostic plots\n\n\n\n\n\nInterpreting the residual plot:\n\nGood: Points scattered randomly around zero with no pattern\nBad - Funnel shape: Variance increases with fitted values (heteroscedasticity)\nBad - Curved pattern: Non-linear relationship not captured by the model\nBad - Clustering: Possible subgroups or missing variables\n\n\n\nIdentifying Outliers and Influential Points\nNot all data points are equally important. Some observations can have a disproportionate influence on the regression line.\nTypes of unusual observations:\n\nOutliers: Observations with large residuals (far from the regression line)\nHigh leverage points: Observations with extreme X values\nInfluential points: Observations that, if removed, would substantially change the regression line\n\nDetecting outliers manually:\nFor our five-student example, let’s examine the residuals:\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i\ne_i\n|e_i|\n\n\n\n\n1\n2\n60\n61.4\n-1.4\n1.4\n\n\n2\n4\n73\n68.7\n4.3\n4.3\n\n\n3\n6\n71\n76.0\n-5.0\n5.0\n\n\n4\n8\n86\n83.3\n2.7\n2.7\n\n\n5\n10\n90\n90.6\n-0.6\n0.6\n\n\n\nStudent 3 has the largest residual (5.0 points below predicted). Is this an outlier?\nRule of thumb: A residual is concerning if |e_i| &gt; 2 \\times RMSE\nIn our case: 2 \\times 3.26 = 6.52 points. Student 3’s residual (5.0) is below this threshold, so it’s not a severe outlier.\nVisualizing potential outliers:\n\npar(mfrow = c(1, 2))\n\n# Plot 1: Highlight potential outliers\nmodel &lt;- lm(exam_scores ~ study_hours)\nplot(study_hours, exam_scores,\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     main = \"Identifying Outliers\",\n     pch = 19, col = \"steelblue\")\nabline(model, col = \"red\", lwd = 2)\n\n# Identify points with large residuals\nlarge_resid &lt;- abs(residuals(model)) &gt; 2 * sd(residuals(model))\nif(any(large_resid)) {\n  points(study_hours[large_resid], exam_scores[large_resid],\n         col = \"orange\", pch = 19, cex = 2)\n  text(study_hours[large_resid], exam_scores[large_resid],\n       labels = which(large_resid), pos = 3, col = \"orange\")\n}\n\n# Plot 2: Leverage plot\nplot(hatvalues(model),\n     xlab = \"Observation Number\",\n     ylab = \"Leverage\",\n     main = \"Leverage of Each Point\",\n     pch = 19, col = \"steelblue\")\nabline(h = 2 * length(coef(model)) / length(exam_scores), \n       col = \"red\", lty = 2)\n\n\n\n\n\n\n\nFigure 9.7: Identifying outliers and influential points\n\n\n\n\n\nWhat to do about outliers:\n\nInvestigate: Check if it’s a data entry error\nUnderstand: What makes this observation unusual?\nReport: Note the outlier and consider analyzing with and without it\nDon’t automatically delete: Outliers may contain important information\n\nFor example, Student 3 might have test anxiety despite adequate preparation, or might learn differently. This is substantively meaningful information, not just “noise.”\n\n\nSummary of Diagnostics\nGood model indicators:\n\nResiduals randomly scattered around zero\nConstant variance across the range of predictions\nR^2 is reasonably high (context-dependent)\nRMSE is small relative to the scale of Y\nFew or no extreme outliers\nStandardized coefficients suggest meaningful effect sizes\n\nWarning signs:\n\nSystematic patterns in residual plots\nFunnel-shaped residuals (heteroscedasticity)\nMultiple influential outliers\nVery low R^2 (model explains little variance)\nCoefficients that don’t make substantive sense\n\nAlways remember: statistical diagnostics should be combined with substantive knowledge about your research question. A statistically “good” model that doesn’t make theoretical sense is still problematic.\n\n\n\nCorrelation vs. Causation and Spurious Relationships\nOne of the most important lessons in statistics is that correlation does not imply causation. Just because two variables are related does not mean one causes the other. This section explores why this distinction matters and how spurious correlations can mislead us.\n\nThe Fundamental Problem\nWhen we find that X and Y are correlated (they move together), there are several possible explanations:\n\nX causes Y: Changes in X directly produce changes in Y\nY causes X: The causal arrow goes the other direction\nConfounding: A third variable Z causes both X and Y\nCoincidence: The relationship is spurious – just random chance\n\nRegression analysis can tell us that X and Y are associated, but it cannot tell us why they’re associated. To claim causation, we need additional evidence beyond correlation.\n\n\nClassic Examples of Correlation Without Causation\nExample 1: Ice cream sales and drowning deaths\nObservation: Countries with higher ice cream sales also have more drowning deaths. Should we ban ice cream to prevent drownings?\nNo! The confounding variable is temperature. Hot weather causes both:\n\nMore ice cream sales (people cool off with ice cream)\nMore drowning deaths (people swim more often)\n\n\nset.seed(42)\nn &lt;- 30\ntemperature &lt;- rnorm(n, 25, 5)\nice_cream &lt;- 50 + 3 * temperature + rnorm(n, 0, 10)\ndrownings &lt;- 10 + 0.8 * temperature + rnorm(n, 0, 3)\n\npar(mfrow = c(1, 2))\n\n# Plot 1: Ice cream vs. drownings (spurious correlation)\nplot(ice_cream, drownings,\n     xlab = \"Ice Cream Sales\",\n     ylab = \"Drowning Deaths\",\n     main = \"Spurious Correlation\",\n     pch = 19, col = \"steelblue\")\nabline(lm(drownings ~ ice_cream), col = \"red\", lwd = 2)\ntext(60, 35, paste(\"R² =\", round(summary(lm(drownings ~ ice_cream))$r.squared, 2)),\n     col = \"red\")\n\n# Plot 2: Both related to temperature (confounding)\nplot(temperature, ice_cream,\n     xlab = \"Temperature (°C)\",\n     ylab = \"Ice Cream Sales / Drownings\",\n     main = \"Both Related to Temperature\",\n     pch = 19, col = \"steelblue\",\n     ylim = c(0, 150))\npoints(temperature, drownings * 3, pch = 17, col = \"darkgreen\")\nabline(lm(ice_cream ~ temperature), col = \"steelblue\", lwd = 2)\nabline(lm(I(drownings * 3) ~ temperature), col = \"darkgreen\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Ice Cream\", \"Drownings (scaled)\"),\n       col = c(\"steelblue\", \"darkgreen\"), pch = c(19, 17))\n\n\n\n\n\n\n\nFigure 9.8: Example of spurious correlation through confounding\n\n\n\n\n\nExample 2: Shoe size and reading ability in children\nObservation: Children with larger shoe sizes read better. Does foot size enhance cognition?\nNo! The confounding variable is age. Older children have both:\n\nLarger shoe sizes (they’ve grown)\nBetter reading skills (they’ve had more education)\n\nExample 3: Number of firefighters and fire damage\nObservation: Fires with more firefighters present tend to have more property damage. Should we send fewer firefighters to reduce damage?\nAbsolutely not! This is reverse causation combined with confounding:\n\nBigger fires cause more damage (directly)\nBigger fires also require more firefighters (response to severity)\n\nThe number of firefighters doesn’t cause damage; fire severity causes both.\n\n\nWhat Makes a Relationship Causal?\nTo establish causation, we typically need:\n\nTemporal precedence: The cause must precede the effect\nAssociation: The variables must be correlated\nNo plausible confounders: We’ve ruled out alternative explanations\nMechanism: We understand how X affects Y\nDose-response: Larger values of X produce larger effects on Y\n\nThe gold standard for establishing causation is a randomized controlled experiment, where we randomly assign people to receive different values of X. This breaks the relationship between X and potential confounders.\nHowever, in social science, we often can’t run experiments (it would be unethical or impractical). Instead, we use:\n\nLongitudinal designs (measuring variables over time)\nNatural experiments (finding naturally occurring random variation)\nCareful control for confounders (including them in multiple regression)\nTheoretical reasoning and prior evidence\n\n\n\nSpurious Correlations\nA spurious correlation is a statistical relationship between two variables that is not due to any causal connection between them. These often arise from:\n\nCommon cause (confounding): Z causes both X and Y\nCoincidence: Random chance produces apparent patterns\nSelection bias: The way we sample creates artificial relationships\n\nFamous spurious correlations:\n\nPer capita cheese consumption correlates with deaths by bedsheet tangling (r = 0.95)\nNumber of Nicolas Cage movies correlates with swimming pool drownings (r = 0.67)\nPer capita margarine consumption correlates with divorce rate in Maine (r = 0.99)\n\nThese are clearly not causal – they’re coincidences in time-series data. The lesson: high correlation, even very high, does not prove causation.\n\n\nImplications for Regression Analysis\nWhen we interpret our regression results, we must be cautious:\nWhat regression tells us:\n\nX and Y are associated\nThe strength of the association (\\hat{\\beta}_1)\nHow much variance is explained (R^2)\nWhether the relationship is linear\n\nWhat regression does NOT tell us:\n\nWhether X causes Y\nWhether we’ve omitted important confounders\nThe direction of causation\nWhether the relationship is spurious\n\nOur study hours example:\nWe found that study hours strongly predict exam scores (R^2 = 0.909). Does studying cause higher scores?\nProbably yes, but we should consider alternative explanations:\n\nReverse causation: Maybe students who are naturally good at the subject (high ability) enjoy studying more, so they study longer. Here, ability causes both study hours and exam scores.\nConfounding: Maybe students with supportive parents both study more and perform better (parents help with homework and create good study environments).\nSelection: Maybe only motivated students were in our sample, and motivation causes both studying and performance.\n\nTo strengthen a causal claim, we could:\n\nControl for prior ability (include previous exam scores)\nControl for parental involvement\nUse an experimental design (randomly assign study hours)\nCheck if the relationship holds across different contexts\n\n\n\nPractical Advice\nWhen reporting regression results:\n\nUse careful language: “associated with” rather than “causes” or “leads to”\nAcknowledge potential confounders\nDiscuss alternative explanations\nBe explicit about the study design’s limitations\nIf claiming causation, provide theoretical reasoning and additional evidence\n\nRemember: correlation is a necessary but not sufficient condition for causation. Finding an association is just the first step in understanding whether one variable influences another.\n\n\n\nKey Assumptions of OLS\nFor OLS to provide meaningful estimates, we need several assumptions:\n\nLinearity: The relationship between X and Y is linear: Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\nStrict Exogeneity: The error term has zero expected value conditional on X: E[\\varepsilon_i | X_i] = 0\nThis means X is uncorrelated with the error – there are no omitted variables that affect both X and Y.\nHomoscedasticity (not strict, but desirable): The variance of errors is constant: Var(\\varepsilon_i | X_i) = \\sigma^2\nNo perfect collinearity (important for multiple regression): Predictor variables are not perfectly correlated.\n\nImportant note: We are not making probabilistic assumptions about the sampling distribution here. We’re simply describing the relationship in our data. For inference (hypothesis tests, confidence intervals), we would need additional assumptions, but that’s beyond our scope for now.\n\n\n\n\n\n\n\nFor What Data Can We Apply OLS Regression?\n\n\n\nOLS regression has specific requirements regarding variable types:\nDependent variable (outcome, Y):\n\nMust be continuous or quasi-continuous (e.g., discrete with many values)\nLevel of measurement (Stevens’ typology):\n\nInterval: differences between values are meaningful, but no natural zero (e.g., temperature in °C, IQ test scores)\nRatio: like interval, but with a natural zero (e.g., income, age, hours)\n\nNot suitable: Nominal variables – categories without order (e.g., eye color, gender) or ordinal variables with few categories (e.g., Likert scale 1-5)\n\nIndependent variables (predictors, X):\n\nCan be continuous, discrete, or binary (0/1)\nLevel of measurement:\n\nRatio/Interval: ideal (e.g., age, income, temperature)\nOrdinal: acceptable if treated as quasi-continuous (e.g., education level: 1=primary, 2=secondary, 3=tertiary)\nNominal binary: acceptable (e.g., gender: 0=female, 1=male)\nNominal multi-category: requires transformation into dummy variables – e.g., region: 3 binary variables for 4 regions\n\n\nExamples of correct applications:\n\nY: Exam score (0-100 points, interval) ~ X: Study hours (continuous, ratio) ✓\nY: Income (continuous, ratio) ~ X_1: Age (continuous, ratio) + X_2: Gender (binary, nominal) ✓\nY: Voter turnout (%, ratio) ~ X: GDP per capita (continuous, ratio) ✓\n\nIncorrect applications:\n\nY: Eye color (nominal) ~ X: Age – use multinomial regression ✗\nY: Party choice (nominal, 5 options) ~ X: Income – use multinomial regression ✗\nY: Agreement (Yes/No, binary) ~ X: Age – use logistic regression ✗\n\nRule of thumb: If Y has fewer than ~7 unique values, consider other methods (logistic, ordinal, or Poisson regression).\n\n\n\n\n\nExtension to Multiple Regression\nSo far, we’ve looked at simple linear regression with one predictor. In reality, social phenomena are complex and require multiple predictors.\nThe multiple regression model extends our framework:\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\cdots + \\beta_k X_{ki} + \\varepsilon_i\nFor example, predicting voter turnout:\n\\text{Turnout}_i = \\beta_0 + \\beta_1 \\text{GDP}_i + \\beta_2 \\text{Compulsory}_i + \\beta_3 \\text{Age}_i + \\varepsilon_i\nThe OLS principle remains the same: find coefficients that minimize SSE = \\sum (Y_i - \\hat{Y}_i)^2.\nThe interpretation changes slightly:\n\n\\hat{\\beta}_1 is the effect of X_1 on Y, holding all other variables constant\nThis is often called the “partial effect” or “controlling for” other variables\n\nMatrix notation (for reference):\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\nBut you don’t need to memorize this – software (like R’s lm() function) handles the calculations.\n\n# Multiple regression example\n# (using built-in mtcars dataset)\nmulti_model &lt;- lm(mpg ~ hp + wt + cyl, data = mtcars)\nsummary(multi_model)\n\n\nCall:\nlm(formula = mpg ~ hp + wt + cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9290 -1.5598 -0.5311  1.1850  5.8986 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 38.75179    1.78686  21.687 &lt; 0.0000000000000002 ***\nhp          -0.01804    0.01188  -1.519             0.140015    \nwt          -3.16697    0.74058  -4.276             0.000199 ***\ncyl         -0.94162    0.55092  -1.709             0.098480 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.512 on 28 degrees of freedom\nMultiple R-squared:  0.8431,    Adjusted R-squared:  0.8263 \nF-statistic: 50.17 on 3 and 28 DF,  p-value: 0.00000000002184\n\n\n\n\nSummary\nWe’ve covered the fundamentals of regression analysis:\n\nDeterministic models assume perfect relationships; stochastic models acknowledge uncertainty\nStatistical models include both systematic patterns and random variation\nDifferent relationships exist in data – linear models work best for linear relationships\nThe OLS method finds the line that minimizes the sum of squared errors\nWe can decompose variance: SST = SSR + SSE\nR^2 = SSR/SST measures the proportion of variance explained (0 to 1)\nRMSE measures average prediction error in original units\nCoefficients should be interpreted substantively, considering effect sizes\nStandardized coefficients allow comparison across different scales\nResidual plots and outlier detection help diagnose model problems\nCorrelation does not imply causation – confounding and spurious relationships are common\nTo establish causation, we need temporal precedence, no confounders, and theoretical mechanisms\nOLS requires assumptions like linearity and strict exogeneity\nMultiple regression extends this framework to many predictors\n\nIn practice, always visualize your data first, check whether a linear model is appropriate, examine residual plots for problems, think carefully about causal interpretation, and remember that association is not causation – even a high R^2 doesn’t prove that X causes Y.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#complete-manual-ols-calculation-a-step-by-step-example",
    "href": "correg_en.html#complete-manual-ols-calculation-a-step-by-step-example",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.11 Complete Manual OLS Calculation: A Step-by-Step Example",
    "text": "9.11 Complete Manual OLS Calculation: A Step-by-Step Example\nA professor wants to understand the relationship between hours spent studying and exam scores. She collects data from 6 students:\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nOur goal: Find the best-fitting line \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X using OLS.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-1-calculate-the-means",
    "href": "correg_en.html#step-1-calculate-the-means",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.12 Step 1: Calculate the Means",
    "text": "9.12 Step 1: Calculate the Means\nFirst, we need the mean of X and Y.\nFor X (study hours): \\bar{X} = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5\nFor Y (exam scores): \\bar{Y} = \\frac{65 + 70 + 75 + 85 + 88 + 95}{6} = \\frac{478}{6} = 79.67",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-2-calculate-deviations-from-means",
    "href": "correg_en.html#step-2-calculate-deviations-from-means",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.13 Step 2: Calculate Deviations from Means",
    "text": "9.13 Step 2: Calculate Deviations from Means\nFor each observation, calculate (X_i - \\bar{X}) and (Y_i - \\bar{Y}):\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n1 - 3.5 = -2.5\n65 - 79.67 = -14.67\n\n\nB\n2\n70\n2 - 3.5 = -1.5\n70 - 79.67 = -9.67\n\n\nC\n3\n75\n3 - 3.5 = -0.5\n75 - 79.67 = -4.67\n\n\nD\n4\n85\n4 - 3.5 = 0.5\n85 - 79.67 = 5.33\n\n\nE\n5\n88\n5 - 3.5 = 1.5\n88 - 79.67 = 8.33\n\n\nF\n6\n95\n6 - 3.5 = 2.5\n95 - 79.67 = 15.33",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-3-calculate-products-and-squares",
    "href": "correg_en.html#step-3-calculate-products-and-squares",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.14 Step 3: Calculate Products and Squares",
    "text": "9.14 Step 3: Calculate Products and Squares\nNow calculate (X_i - \\bar{X})(Y_i - \\bar{Y}) and (X_i - \\bar{X})^2:\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n(-2.5)(-14.67) = 36.68\n(-2.5)² = 6.25\n\n\nB\n(-1.5)(-9.67) = 14.51\n(-1.5)² = 2.25\n\n\nC\n(-0.5)(-4.67) = 2.34\n(-0.5)² = 0.25\n\n\nD\n(0.5)(5.33) = 2.67\n(0.5)² = 0.25\n\n\nE\n(1.5)(8.33) = 12.50\n(1.5)² = 2.25\n\n\nF\n(2.5)(15.33) = 38.33\n(2.5)² = 6.25\n\n\nSum\n107.03\n17.50",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-4-calculate-the-slope-hatbeta_1",
    "href": "correg_en.html#step-4-calculate-the-slope-hatbeta_1",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.15 Step 4: Calculate the Slope (\\hat{\\beta}_1)",
    "text": "9.15 Step 4: Calculate the Slope (\\hat{\\beta}_1)\nUsing the OLS formula: \\hat{\\beta}_1 = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} = \\frac{107.03}{17.50} = 6.12\nInterpretation: Each additional hour of study is associated with a 6.12-point increase in exam score.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-5-calculate-the-intercept-hatbeta_0",
    "href": "correg_en.html#step-5-calculate-the-intercept-hatbeta_0",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.16 Step 5: Calculate the Intercept (\\hat{\\beta}_0)",
    "text": "9.16 Step 5: Calculate the Intercept (\\hat{\\beta}_0)\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X} = 79.67 - (6.12 \\times 3.5) = 79.67 - 21.42 = 58.25\nInterpretation: A student who studies 0 hours is predicted to score 58.25 points.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-6-write-the-regression-equation",
    "href": "correg_en.html#step-6-write-the-regression-equation",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.17 Step 6: Write the Regression Equation",
    "text": "9.17 Step 6: Write the Regression Equation\n\\hat{Y} = 58.25 + 6.12X",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-7-calculate-predicted-values-and-residuals",
    "href": "correg_en.html#step-7-calculate-predicted-values-and-residuals",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.18 Step 7: Calculate Predicted Values and Residuals",
    "text": "9.18 Step 7: Calculate Predicted Values and Residuals\nUsing our equation to predict each student’s score:\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nResidual e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n58.25 + 6.12(1) = 64.37\n65 - 64.37 = 0.63\n\n\nB\n2\n70\n58.25 + 6.12(2) = 70.49\n70 - 70.49 = -0.49\n\n\nC\n3\n75\n58.25 + 6.12(3) = 76.61\n75 - 76.61 = -1.61\n\n\nD\n4\n85\n58.25 + 6.12(4) = 82.73\n85 - 82.73 = 2.27\n\n\nE\n5\n88\n58.25 + 6.12(5) = 88.85\n88 - 88.85 = -0.85\n\n\nF\n6\n95\n58.25 + 6.12(6) = 94.97\n95 - 94.97 = 0.03\n\n\n\nCheck: Sum of residuals = 0.63 - 0.49 - 1.61 + 2.27 - 0.85 + 0.03 ≈ 0 ✓",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-8-calculate-sum-of-squares",
    "href": "correg_en.html#step-8-calculate-sum-of-squares",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.19 Step 8: Calculate Sum of Squares",
    "text": "9.19 Step 8: Calculate Sum of Squares\n\nTotal Sum of Squares (SST)\nHow much total variation exists in exam scores?\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n(65 - 79.67)² = (-14.67)² = 215.21\n\n\nB\n70\n(70 - 79.67)² = (-9.67)² = 93.51\n\n\nC\n75\n(75 - 79.67)² = (-4.67)² = 21.81\n\n\nD\n85\n(85 - 79.67)² = (5.33)² = 28.41\n\n\nE\n88\n(88 - 79.67)² = (8.33)² = 69.39\n\n\nF\n95\n(95 - 79.67)² = (15.33)² = 235.01\n\n\nSum\n\nSST = 663.34\n\n\n\n\n\nRegression Sum of Squares (SSR)\nHow much variation does our model explain?\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n(64.37 - 79.67)² = (-15.30)² = 234.09\n\n\nB\n70.49\n(70.49 - 79.67)² = (-9.18)² = 84.27\n\n\nC\n76.61\n(76.61 - 79.67)² = (-3.06)² = 9.36\n\n\nD\n82.73\n(82.73 - 79.67)² = (3.06)² = 9.36\n\n\nE\n88.85\n(88.85 - 79.67)² = (9.18)² = 84.27\n\n\nF\n94.97\n(94.97 - 79.67)² = (15.30)² = 234.09\n\n\nSum\n\nSSR = 655.44\n\n\n\n\n\nError Sum of Squares (SSE)\nHow much variation is unexplained?\n\n\n\nStudent\nResidual e_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n-0.49\n0.24\n\n\nC\n-1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n-0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSum\n\nSSE = 9.10\n\n\n\nVerification: SST = SSR + SSE 663.34 ≈ 655.44 + 9.10 = 664.54 ✓ (small rounding difference)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-9-calculate-r-squared",
    "href": "correg_en.html#step-9-calculate-r-squared",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.20 Step 9: Calculate R-Squared",
    "text": "9.20 Step 9: Calculate R-Squared\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternative formula: R^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 1 - 0.014 = 0.986\n(Small difference due to rounding)\nInterpretation: Study hours explain 98.8% of the variation in exam scores. This is an extremely strong relationship.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-10-calculate-effect-sizes",
    "href": "correg_en.html#step-10-calculate-effect-sizes",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.21 Step 10: Calculate Effect Sizes",
    "text": "9.21 Step 10: Calculate Effect Sizes\n\nRaw Effect Size\nThe raw effect size is simply the slope: 6.12 points per hour\n\n\nStandardized Effect Size\nFirst, calculate standard deviations:\nFor X: s_X = \\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{n-1}} = \\sqrt{\\frac{17.50}{5}} = \\sqrt{3.50} = 1.87\nFor Y: s_Y = \\sqrt{\\frac{\\sum(Y_i - \\bar{Y})^2}{n-1}} = \\sqrt{\\frac{663.34}{5}} = \\sqrt{132.67} = 11.52\nStandardized coefficient: \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y} = 6.12 \\times \\frac{1.87}{11.52} = 6.12 \\times 0.162 = 0.99\nInterpretation: A one standard deviation increase in study hours (1.87 hours) is associated with a 0.99 standard deviation increase in exam score.\nAccording to Cohen’s guidelines:\n\nSmall effect: |β| = 0.10\nMedium effect: |β| = 0.30\nLarge effect: |β| = 0.50\n\nOur standardized effect of 0.99 is nearly twice Cohen’s “large effect” threshold.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-11-practical-significance-assessment",
    "href": "correg_en.html#step-11-practical-significance-assessment",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.22 Step 11: Practical Significance Assessment",
    "text": "9.22 Step 11: Practical Significance Assessment\n\nContext Analysis\n\nScale consideration:\n\nEffect: 6.12 points per hour\nExam scale: 0-100 points\nPercentage impact: 6.12% per hour\n\nPractical thresholds:\n\nLetter grade difference: Often 10 points\nTime to improve one letter grade: 10/6.12 = 1.63 hours\nConclusion: Less than 2 hours of extra study could change a letter grade\n\nCost-benefit analysis:\n\nBenefit: 6.12 points per hour\nCost: 1 hour of time\nDecision: Highly favorable return on investment",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-of-results",
    "href": "correg_en.html#summary-of-results",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.23 Summary of Results",
    "text": "9.23 Summary of Results\nRegression equation: \\hat{Y} = 58.25 + 6.12X\nKey statistics:\n\nSlope (\\hat{\\beta}_1): 6.12 points/hour\nIntercept (\\hat{\\beta}_0): 58.25 points\nR²: 0.988 (98.8% of variance explained)\nStandardized effect: 0.99 (very large effect)\n\nPractical interpretation:\n\nEach hour of study adds about 6 points to exam score\nThe model fits extremely well (R² near 1)\nThe effect is both statistically and practically significant\nStudents can meaningfully improve grades with modest increases in study time",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#verification-check",
    "href": "correg_en.html#verification-check",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.24 Verification Check",
    "text": "9.24 Verification Check\nTo verify our calculations, let’s check that the regression line passes through (\\bar{X}, \\bar{Y}):\n\\hat{Y} = 58.25 + 6.12(3.5) = 58.25 + 21.42 = 79.67 = \\bar{Y} ✓\nThe calculation confirms our regression line passes through the point of means, as it should.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#r-code-to-verify-manual-calculations",
    "href": "correg_en.html#r-code-to-verify-manual-calculations",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.25 R Code to Verify Manual Calculations",
    "text": "9.25 R Code to Verify Manual Calculations\nBelow is R code that checks all our manual calculations. You can run this code to confirm every step.\n\n# Step 1: Create the data\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)  # X variable\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95)  # Y variable\nn &lt;- length(study_hours)  # Sample size\n\n# Create a data frame for easier handling\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Step 2: Calculate means\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Step 3: Calculate deviations from means\ndata$x_dev &lt;- data$X - x_bar  # X deviations\ndata$y_dev &lt;- data$Y - y_bar  # Y deviations\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Step 4: Calculate products and squares for OLS formula\ndata$xy_product &lt;- data$x_dev * data$y_dev  # (Xi - X̄)(Yi - Ȳ)\ndata$x_dev_sq &lt;- data$x_dev^2  # (Xi - X̄)²\n\n# Sum of products and squares\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - X̄)(Yi - Ȳ):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - X̄)(Yi - Ȳ): 107 \n\ncat(\"Sum of (Xi - X̄)²:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - X̄)²: 17.5 \n\n# Step 5: Calculate slope (beta_1) manually\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (β₁) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (β₁) calculated manually: 6.11 \n\n# Step 6: Calculate intercept (beta_0) manually\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (β₀) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (β₀) calculated manually: 58.27 \n\n# Step 7: Compare with R's lm() function\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Step 8: Calculate predicted values and residuals\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X  # Predicted values\ndata$residual &lt;- data$Y - data$Y_hat  # Residuals\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ≈ 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ≈ 0): 0 \n\n# Step 9: Calculate Sum of Squares\n# Total Sum of Squares (SST)\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\n# Regression Sum of Squares (SSR)\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\n# Error Sum of Squares (SSE)\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\n# Verify that SST = SSR + SSE\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Step 10: Calculate R-squared\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R² (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR² (Method 1: SSR/SST): 0.9863 \n\ncat(\"R² (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR² (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R² (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR² (from lm function): 0.9863 \n\n# Step 11: Calculate Effect Sizes\n# Raw effect size (just the slope)\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\n# Standard deviations for standardized effect\nsd_x &lt;- sd(data$X)  # Standard deviation of X\nsd_y &lt;- sd(data$Y)  # Standard deviation of Y\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\n# Standardized effect size\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Correlation coefficient (should equal sqrt(R²) for simple regression)\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R²:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R²: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n# Step 12: Create visualization\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\n# Plot the data and regression line\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5)\n\n# Add the regression line\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Add the mean point\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Add vertical lines for residuals\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\n# Add legend\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# Add the equation to the plot\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R² = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\n\n\n\n# Final summary\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )\n\n\n\nRunning the Code\nTo run this R code:\n\nCopy the entire code block above\nPaste it into RStudio or any R console\nExecute the code\nCompare the output with our manual calculations\n\nThe code will:\n\nRecreate all our manual calculations step by step\nVerify results using R’s built-in lm() function\nGenerate a visualization of the data with the regression line\nDisplay all intermediate calculations with clear labels\n\n\n\nExpected Output Highlights\nWhen you run this code, you should see:\n\nSlope: 6.12 (matching our manual calculation)\nIntercept: 58.25 (matching our manual calculation)\nR²: 0.988 (matching our manual calculation)\nStandardized effect: 0.99 (matching our manual calculation)\nA plot showing the data points, regression line, and residuals\n\nThis verification confirms that our pen-and-paper calculations were correct! :::",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#model-interpretation-a-beginners-guide",
    "href": "correg_en.html#model-interpretation-a-beginners-guide",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.26 Model Interpretation: A Beginner’s Guide",
    "text": "9.26 Model Interpretation: A Beginner’s Guide\nLet’s create a simple dataset to understand regression output better. Imagine we’re studying how years of education affect annual income:\n\n# Create a simple dataset - this is our Data Generating Process (DGP)\nset.seed(123) # For reproducibility\neducation_years &lt;- 10:20  # Education from 10 to 20 years\nn &lt;- length(education_years)\n\n# True parameters in our model - using more realistic values for Poland\ntrue_intercept &lt;- 3000   # Base monthly income with no education (in PLN)\ntrue_slope &lt;- 250        # Each year of education increases monthly income by 250 PLN\n\n# Generate monthly incomes with some random noise\nincome &lt;- true_intercept + true_slope * education_years + rnorm(n, mean=0, sd=300)\n\n# Create our dataset\neducation_income &lt;- data.frame(\n  education = education_years,\n  income = income\n)\n\n# Let's visualize our data\nlibrary(ggplot2)\nggplot(education_income, aes(x = education, y = income)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  scale_y_continuous(limits = c(5000, 8500), \n                     breaks = seq(5000, 8500, by = 500),\n                     labels = scales::comma) +\n  scale_x_continuous(breaks = 10:20) +\n  labs(\n    title = \"Relationship between Education and Income in Poland\",\n    x = \"Years of Education\",\n    y = \"Monthly Income (PLN)\",\n    subtitle = \"Red line shows the estimated linear relationship\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\")\n  ) +\n  annotate(\"text\", x = 11, y = 8000, \n           label = \"Each point represents\\none person's data\", \n           hjust = 0, size = 4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nFitting the Model\nNow let’s fit a linear regression model to this data:\n\n# Fit a simple regression model\nedu_income_model &lt;- lm(income ~ education, data = education_income)\n\n# Display the results\nmodel_summary &lt;- summary(edu_income_model)\nmodel_summary\n\n\nCall:\nlm(formula = income ~ education, data = education_income)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-427.72 -206.04  -38.12  207.32  460.78 \n\nCoefficients:\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)   3095.3      447.6   6.915 0.0000695 ***\neducation      247.2       29.2   8.467 0.0000140 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 306.3 on 9 degrees of freedom\nMultiple R-squared:  0.8885,    Adjusted R-squared:  0.8761 \nF-statistic: 71.69 on 1 and 9 DF,  p-value: 0.00001403\n\n\n\n\nUnderstanding the Regression Output Step by Step\nLet’s break down what each part of this output means in simple terms:\n\n1. The Formula\nAt the top, you see income ~ education, which means we’re predicting income based on education.\n\n\n2. Residuals\nThese show how far our predictions are from the actual values. Ideally, they should be centered around zero.\n\n\n3. Coefficients Table\n\n\n\nCoefficient Estimates\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n3095.27\n447.63\n6.91\n0\n\n\neducation\n247.23\n29.20\n8.47\n0\n\n\n\n\n\nIntercept (\\beta_0):\n\nValue: Approximately 3095\nInterpretation: This is the predicted monthly income for someone with 0 years of education\nNote: Sometimes the intercept isn’t meaningful in real-world terms, especially if x=0 is outside your data range\n\nEducation (\\beta_1):\n\nValue: Approximately 247\nInterpretation: For each additional year of education, we expect monthly income to increase by this amount in PLN\nThis is our main coefficient of interest!\n\nStandard Error:\n\nMeasures how precise our estimates are\nSmaller standard errors mean more precise estimates\nThink of it as “give or take how much” for our coefficients\n\nt value:\n\nThis is the coefficient divided by its standard error\nIt tells us how many standard errors away from zero our coefficient is\nLarger absolute t values (above 2) suggest the effect is statistically significant\n\np-value:\n\nThe probability of seeing our result (or something more extreme) if there was actually no relationship\nTypically, p &lt; 0.05 is considered statistically significant\nFor education, p = 0.000014, which is significant!\n\n\n\n4. Model Fit Statistics\n\n\n\nModel Fit Statistics\n\n\nStatistic\nValue\n\n\n\n\nR-squared\n0.888\n\n\nAdjusted R-squared\n0.876\n\n\nF-statistic\n71.686\n\n\np-value\n0.000\n\n\n\n\n\nR-squared:\n\nValue: 0.888\nInterpretation: 89% of the variation in income is explained by education\nHigher is better, but be cautious of very high values (could indicate overfitting)\n\nF-statistic:\n\nTests whether the model as a whole is statistically significant\nA high F-statistic with a low p-value indicates a significant model\n\n\n\n\nVisualizing the Model Results\nLet’s visualize what our model actually tells us:\n\n# Predicted values\neducation_income$predicted &lt;- predict(edu_income_model)\neducation_income$residuals &lt;- residuals(edu_income_model)\n\n# Create a more informative plot\nggplot(education_income, aes(x = education, y = income)) +\n  # Actual data points\n  geom_point(size = 3, color = \"blue\") +\n  \n  # Regression line\n  geom_line(aes(y = predicted), color = \"red\", size = 1.2) +\n  \n  # Residual lines\n  geom_segment(aes(xend = education, yend = predicted), \n               color = \"darkgray\", linetype = \"dashed\") +\n  \n  # Set proper scales\n  scale_y_continuous(limits = c(5000, 8500), \n                     breaks = seq(5000, 8500, by = 500),\n                     labels = scales::comma) +\n  scale_x_continuous(breaks = 10:20) +\n  \n  # Annotations\n  annotate(\"text\", x = 19, y = 7850, \n           label = paste(\"Slope =\", round(coef(edu_income_model)[2]), \"PLN per year\"),\n           color = \"red\", hjust = 1, fontface = \"bold\") +\n  \n  annotate(\"text\", x = 10.5, y = 5500, \n           label = paste(\"Intercept =\", round(coef(edu_income_model)[1]), \"PLN\"),\n           color = \"red\", hjust = 0, fontface = \"bold\") +\n  \n  annotate(\"text\", x = 14, y = 8200, \n           label = paste(\"R² =\", round(model_summary$r.squared, 2)),\n           color = \"black\", fontface = \"bold\") +\n  \n  # Labels\n  labs(\n    title = \"Interpreting the Education-Income Regression Model\",\n    subtitle = \"Red line shows predicted income for each education level\",\n    x = \"Years of Education\",\n    y = \"Monthly Income (PLN)\",\n    caption = \"Gray dashed lines represent residuals (prediction errors)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\")\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nReal-World Interpretation\n\nA person with 16 years of education (college graduate) would be predicted to earn about: \\hat{Y} = 3095 + 247 \\times 16 = 7051 \\text{ PLN monthly}\nThe model suggests that each additional year of education is associated with a 247 PLN increase in monthly income.\nOur model explains approximately 89% of the variation in income in our sample.\nThe relationship is statistically significant (p &lt; 0.001), meaning it’s very unlikely to observe this relationship if education truly had no effect on income.\n\n\n\nImportant Cautions for Beginners\n\nCorrelation ≠ Causation: Our model shows association, not necessarily causation\nOmitted Variables: Other factors might influence both education and income\nExtrapolation: Be careful predicting outside the range of your data\nLinear Relationship: We’ve assumed the relationship is linear, which may not always be true",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#mathematical-derivation-of-ols-estimators",
    "href": "correg_en.html#mathematical-derivation-of-ols-estimators",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.27 Mathematical Derivation of OLS Estimators (*)",
    "text": "9.27 Mathematical Derivation of OLS Estimators (*)\nFor simple linear regression, the OLS estimators are obtained by minimizing the sum of squared residuals:\nSSE = \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2\nTaking partial derivatives with respect to \\beta_0 and \\beta_1 and setting them equal to zero yields the normal equations. Solving this system produces:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} = \\frac{Cov(X,Y)}{Var(X)}\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\n\nProperties of OLS Estimators\nThe OLS procedure guarantees several important properties:\n\nZero sum of residuals: \\sum_{i=1}^{n} e_i = 0\nOrthogonality of residuals and predictors: \\sum_{i=1}^{n} X_i e_i = 0\nThe fitted regression line passes through the point (\\bar{X}, \\bar{Y})\nZero covariance between fitted values and residuals: \\sum_{i=1}^{n} \\hat{Y}_i e_i = 0\n\n\n\nClassical Linear Model Assumptions\n\nCore Assumptions\nFor OLS estimators to possess desirable statistical properties, the following assumptions must hold:\n\n\nAssumption 1: Linearity in Parameters\nThe relationship between the dependent and independent variables is linear in the parameters: Y_i = \\beta_0 + \\beta_1 X_{1i} + ... + \\beta_k X_{ki} + \\varepsilon_i\n\n\nAssumption 2: Strict Exogeneity\nThe error term has zero conditional expectation given all values of the independent variables: E[\\varepsilon_i | X] = 0\nThis assumption implies that the independent variables contain no information about the mean of the error term. It is stronger than contemporaneous exogeneity and rules out feedback from past errors to current regressors. This assumption is critical for unbiased estimation and is often violated in time series contexts with lagged dependent variables or in the presence of omitted variables.\nThis assumption is particularly important for our discussion of spurious correlations. Violations of the exogeneity assumption lead to endogeneity problems, which we will discuss later.\n\n\nAssumption 3: No Perfect Multicollinearity\nIn multiple regression, no independent variable can be expressed as a perfect linear combination of other independent variables. The matrix X'X must be invertible.\n\n\nAssumption 4: Homoscedasticity\nThe variance of the error term is constant across all observations: Var(\\varepsilon_i | X) = \\sigma^2\nThis assumption ensures that the precision of the regression does not vary systematically with the level of the independent variables.\n\n\nAssumption 5: No Autocorrelation\nThe error terms are uncorrelated with each other: Cov(\\varepsilon_i, \\varepsilon_j | X) = 0 \\text{ for } i \\neq j\n\n\nAssumption 6: Normality of Errors (for inference)\nThe error terms follow a normal distribution: \\varepsilon_i \\sim N(0, \\sigma^2)\nThis assumption is not required for the unbiasedness or consistency of OLS estimators but is necessary for exact finite-sample inference.\n\n\n\nGauss-Markov Theorem\nUnder Assumptions 1-5, the OLS estimators are BLUE (Best Linear Unbiased Estimators):\n\nBest: Minimum variance among the class of linear unbiased estimators\nLinear: The estimators are linear functions of the dependent variable\nUnbiased: E[\\hat{\\beta}] = \\beta\n\n\n\nVisualization of OLS Methodology\n\nGeometric Interpretation\n\n# Comprehensive visualization of OLS regression\nlibrary(ggplot2)\n\n# Generate sample data\nset.seed(42)\nn &lt;- 50\nx &lt;- runif(n, 0, 100)\nepsilon &lt;- rnorm(n, 0, 15)\ny &lt;- 20 + 0.8*x + epsilon\n\n# Create data frame\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, data = data)\ndata$fitted &lt;- fitted(model)\ndata$residuals &lt;- residuals(model)\n\n# Create comprehensive plot\nggplot(data, aes(x = x, y = y)) +\n  # Add confidence interval\n  geom_smooth(method = \"lm\", se = TRUE, alpha = 0.15, fill = \"blue\") +\n  # Add regression line\n  geom_line(aes(y = fitted), color = \"blue\", linewidth = 1.2) +\n  # Add residual segments\n  geom_segment(aes(xend = x, yend = fitted), \n               color = \"red\", alpha = 0.5, linewidth = 0.7) +\n  # Add observed points\n  geom_point(size = 2.5, alpha = 0.8) +\n  # Add fitted values\n  geom_point(aes(y = fitted), color = \"blue\", size = 1.5, alpha = 0.6) +\n  # Annotations\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(size = 11),\n    plot.title = element_text(size = 12, face = \"bold\")\n  ) +\n  labs(\n    title = \"Ordinary Least Squares Regression\",\n    subtitle = sprintf(\"Estimated equation: Y = %.2f + %.3f X  (R² = %.3f, RSE = %.2f)\",\n                      coef(model)[1], coef(model)[2], \n                      summary(model)$r.squared, \n                      summary(model)$sigma),\n    x = \"Independent Variable (X)\",\n    y = \"Dependent Variable (Y)\"\n  ) +\n  annotate(\"text\", x = min(x) + 5, y = max(y) - 5,\n           label = sprintf(\"SSE = %.1f\", sum(residuals(model)^2)),\n           hjust = 0, size = 3.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nVisualization of Squared Residuals\n\n# Demonstrate why squaring is necessary\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Generate example with clear pattern\nset.seed(123)\nn &lt;- 20\nx &lt;- seq(1, 10, length.out = n)\ny &lt;- 2 + 1.5*x + rnorm(n, 0, 2)\ndf &lt;- data.frame(x = x, y = y)\n\n# Fit model\nmodel &lt;- lm(y ~ x, df)\ndf$fitted &lt;- fitted(model)\ndf$residual &lt;- residuals(model)\n\n# Plot 1: Raw residuals\np1 &lt;- ggplot(df, aes(x = x)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(aes(y = 0, yend = residual, xend = x), \n               color = ifelse(df$residual &gt; 0, \"darkgreen\", \"darkred\"),\n               linewidth = 1) +\n  geom_point(aes(y = residual), size = 3,\n             color = ifelse(df$residual &gt; 0, \"darkgreen\", \"darkred\")) +\n  theme_minimal() +\n  labs(title = \"Residuals (ei)\",\n       subtitle = sprintf(\"Sum = %.2f (not meaningful)\", sum(df$residual)),\n       x = \"X\", y = \"Residual\") +\n  ylim(c(-6, 6))\n\n# Plot 2: Squared residuals\np2 &lt;- ggplot(df, aes(x = x)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(aes(y = 0, yend = residual^2, xend = x), \n               color = \"darkred\", linewidth = 1) +\n  geom_point(aes(y = residual^2), size = 3, color = \"darkred\") +\n  theme_minimal() +\n  labs(title = \"Squared Residuals (ei²)\",\n       subtitle = sprintf(\"Sum = %.2f (minimized by OLS)\", sum(df$residual^2)),\n       x = \"X\", y = \"Squared Residual\") +\n  ylim(c(0, 36))\n\n# Combine plots\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\nDiagnostic Analysis\n\nResidual Diagnostics\nAssessment of model assumptions requires careful examination of residual patterns:\n\n# Generate diagnostic plots\npar(mfrow = c(2, 2))\n\n# Residuals vs Fitted Values\nplot(model, which = 1)\n# Tests linearity and homoscedasticity assumptions\n\n# Normal Q-Q Plot\nplot(model, which = 2)\n# Tests normality assumption\n\n# Scale-Location Plot\nplot(model, which = 3)\n# Tests homoscedasticity assumption\n\n# Cook's Distance\nplot(model, which = 4)\n\n\n\n\n\n\n\n# Identifies influential observations\n\n\n\nTesting Assumptions Formally\n\n# Formal statistical tests\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(car)\n\n# Test for heteroscedasticity\n# Breusch-Pagan test\nbptest(model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model\nBP = 0.37876, df = 1, p-value = 0.5383\n\n# Test for autocorrelation\n# Durbin-Watson test\ndwtest(model)\n\n\n    Durbin-Watson test\n\ndata:  model\nDW = 2.1781, p-value = 0.5599\nalternative hypothesis: true autocorrelation is greater than 0\n\n# Test for normality\n# Shapiro-Wilk test on residuals\nshapiro.test(residuals(model))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(model)\nW = 0.98221, p-value = 0.9593\n\n# Test for linearity\n# Rainbow test\nraintest(model)\n\n\n    Rainbow test\n\ndata:  model\nRain = 1.2139, df1 = 10, df2 = 8, p-value = 0.3995\n\n\n\n\n\n\nKey Learning Points\n\nThe Sum of Squared Errors (SSE) is what Ordinary Least Squares (OLS) regression minimizes\nEach residual contributes its squared value to the total SSE\nThe OLS line has a lower SSE than any other possible line\nLarge residuals contribute disproportionately to the SSE due to the squaring operation\nThis is why outliers can have such a strong influence on regression lines\n\n\nStep-by-Step SSE Minimization\nTo illustrate the process of finding the minimum SSE, we can create a sequence that passes through the optimal point, showing how the SSE first decreases to a minimum and then increases again:\n\n# Create sample data\nset.seed(123)\nx &lt;- 1:20\ny &lt;- 2 + 3*x + rnorm(20, 0, 5)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, data = data)\ncoef &lt;- coefficients(model)\n\n# Create a sequence of steps that passes through the optimal OLS line\nsteps &lt;- 9  # Use odd number to have a middle point at the optimum\nstep_seq &lt;- data.frame(\n  step = 1:steps,\n  intercept = seq(coef[1] - 8, coef[1] + 8, length.out = steps),\n  slope = seq(coef[2] - 1.5, coef[2] + 1.5, length.out = steps)\n)\n\n# Mark the middle step (optimal OLS solution)\noptimal_step &lt;- ceiling(steps/2)\n\n# Calculate SSE for each step\nstep_seq$sse &lt;- sapply(1:nrow(step_seq), function(i) {\n  predicted &lt;- step_seq$intercept[i] + step_seq$slope[i] * x\n  sum((y - predicted)^2)\n})\n\n# Create a \"journey through the SSE valley\" plot\np2 &lt;- ggplot(data, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_abline(data = step_seq, \n              aes(intercept = intercept, slope = slope, \n                  color = sse, group = step),\n              size = 1) +\n  # Highlight the optimal line\n  geom_abline(intercept = step_seq$intercept[optimal_step], \n              slope = step_seq$slope[optimal_step],\n              color = \"green\", size = 1.5) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  labs(title = \"Journey Through the SSE Valley\",\n       subtitle = \"The green line represents the OLS solution with minimum SSE\",\n       color = \"SSE Value\") +\n  theme_minimal()\n\n# Create an SSE valley plot\np3 &lt;- ggplot(step_seq, aes(x = step, y = sse)) +\n  geom_line(size = 1) +\n  geom_point(size = 3, aes(color = sse)) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  # Highlight the optimal point\n  geom_point(data = step_seq[optimal_step, ], aes(x = step, y = sse), \n             size = 5, color = \"green\") +\n  # Add annotation\n  annotate(\"text\", x = optimal_step, y = step_seq$sse[optimal_step] * 1.1, \n           label = \"Minimum SSE\", color = \"darkgreen\", fontface = \"bold\") +\n  labs(title = \"The SSE Valley: Decreasing Then Increasing\",\n       subtitle = \"The SSE reaches its minimum at the OLS solution\",\n       x = \"Step\",\n       y = \"Sum of Squared Errors\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Display both plots\ngrid.arrange(p2, p3, ncol = 1, heights = c(3, 2))\n\n\n\n\nSSE minimization visualization\n\n\n\n\nIn R, the lm() function fits linear regression models:\n\nmodel &lt;- lm(y ~ x, data = data_frame)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#key-assumptions-of-linear-regression",
    "href": "correg_en.html#key-assumptions-of-linear-regression",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.28 Key Assumptions of Linear Regression (*)",
    "text": "9.28 Key Assumptions of Linear Regression (*)\n\nStrict Exogeneity: The Fundamental Assumption\nThe most crucial assumption in regression is strict exogeneity:\nE[\\varepsilon|X] = 0\nThis means:\n\nThe error term has zero mean conditional on X\nX contains no information about the average error\nThere are no systematic patterns in how our predictions are wrong\n\nLet’s visualize when this assumption holds and when it doesn’t:\n\n# Generate data\nset.seed(789)\nx &lt;- seq(1, 10, by = 0.2)\n\n# Case 1: Exogenous errors\ny_exog &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\n\n# Case 2: Non-exogenous errors (error variance increases with x)\ny_nonexog &lt;- 2 + 3*x + 0.5*x*rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_exog &lt;- data.frame(\n  x = x,\n  y = y_exog,\n  type = \"Exogenous Errors\\n(Assumption Satisfied)\"\n)\n\ndata_nonexog &lt;- data.frame(\n  x = x,\n  y = y_nonexog,\n  type = \"Non-Exogenous Errors\\n(Assumption Violated)\"\n)\n\ndata_combined &lt;- rbind(data_exog, data_nonexog)\n\n# Create plots with residuals\nplot_residuals &lt;- function(data, title) {\n  model &lt;- lm(y ~ x, data = data)\n  data$predicted &lt;- predict(model)\n  data$residuals &lt;- residuals(model)\n  \n  p1 &lt;- ggplot(data, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    theme_minimal() +\n    labs(title = title)\n  \n  p2 &lt;- ggplot(data, aes(x = x, y = residuals)) +\n    geom_point() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n    theme_minimal() +\n    labs(y = \"Residuals\")\n  \n  list(p1, p2)\n}\n\n# Generate plots\nplots_exog &lt;- plot_residuals(data_exog, \"Exogenous Errors\")\nplots_nonexog &lt;- plot_residuals(data_nonexog, \"Non-Exogenous Errors\")\n\n# Arrange plots\ngridExtra::grid.arrange(\n  plots_exog[[1]], plots_exog[[2]],\n  plots_nonexog[[1]], plots_nonexog[[2]],\n  ncol = 2\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 9.9: Exogeneity vs. Non-Exogeneity Examples\n\n\n\n\n\n\n\nLinearity: The Form Assumption\nThe relationship between X and Y should be linear in parameters:\nE[Y|X] = \\beta_0 + \\beta_1X\nNote that this doesn’t mean X and Y must have a straight-line relationship - we can transform variables. Let’s see different types of relationships:\n\n# Generate data\nset.seed(101)\nx &lt;- seq(1, 10, by = 0.1)\n\n# Different relationships\ndata_relationships &lt;- data.frame(\n  x = rep(x, 3),\n  y = c(\n    # Linear\n    2 + 3*x + rnorm(length(x), 0, 2),\n    # Quadratic\n    2 + 0.5*x^2 + rnorm(length(x), 0, 2),\n    # Exponential\n    exp(0.3*x) + rnorm(length(x), 0, 2)\n  ),\n  type = rep(c(\"Linear\", \"Quadratic\", \"Exponential\"), each = length(x))\n)\n\n# Plot\nggplot(data_relationships, aes(x = x, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  facet_wrap(~type, scales = \"free_y\") +\n  theme_minimal() +\n  labs(subtitle = \"Red: linear fit, Blue: true relationship\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 9.10: Linear and Nonlinear Relationships\n\n\n\n\n\n\n\nUnderstanding Violations and Solutions\nWhen linearity is violated:\n\nTransform variables:\n\nLog transformation: for exponential relationships\nSquare root: for moderate nonlinearity\nPower transformations: for more complex relationships\n\n\n\n# Generate exponential data\nset.seed(102)\nx &lt;- seq(1, 10, by = 0.2)\ny &lt;- exp(0.3*x) + rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_trans &lt;- data.frame(\n  x = x,\n  y = y,\n  log_y = log(y)\n)\n\nWarning in log(y): NaNs produced\n\n# Original scale plot\np1 &lt;- ggplot(data_trans, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Original Scale\")\n\n# Log scale plot\np2 &lt;- ggplot(data_trans, aes(x = x, y = log_y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Log-Transformed Y\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFigure 9.11: Effect of Variable Transformations",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#spurious-correlation-causes-and-examples",
    "href": "correg_en.html#spurious-correlation-causes-and-examples",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.29 Spurious Correlation: Causes and Examples",
    "text": "9.29 Spurious Correlation: Causes and Examples\nSpurious correlation occurs when variables appear related but the relationship is not causal. These misleading correlations arise from several sources:\n\nRandom coincidence (chance)\nConfounding variables (hidden third factors)\nSelection biases\nImproper statistical analysis\nReverse causality\nEndogeneity problems (including simultaneity)\n\n\nRandom Coincidence (Chance)\nWith sufficient data mining or small sample sizes, seemingly meaningful correlations can emerge purely by chance. This is especially problematic when researchers conduct multiple analyses without appropriate corrections for multiple comparisons, a practice known as “p-hacking.”\n\n# Create a realistic example of spurious correlation based on actual country data\n# Using country data on chocolate consumption and Nobel prize winners\n# This example is inspired by a published correlation (Messerli, 2012)\nset.seed(123)\ncountries &lt;- c(\"Switzerland\", \"Sweden\", \"Denmark\", \"Belgium\", \"Austria\", \n               \"Norway\", \"Germany\", \"Netherlands\", \"United Kingdom\", \"Finland\", \n               \"France\", \"Italy\", \"Spain\", \"Poland\", \"Greece\", \"Portugal\")\n\n# Create realistic data: Chocolate consumption correlates with GDP per capita\n# Higher GDP countries tend to consume more chocolate and have better research funding\ngdp_per_capita &lt;- c(87097, 58977, 67218, 51096, 53879, 89154, 51860, 57534, \n                    46510, 53982, 43659, 35551, 30416, 17841, 20192, 24567)\n\n# Normalize GDP values to make them more manageable\ngdp_normalized &lt;- (gdp_per_capita - min(gdp_per_capita)) / \n                 (max(gdp_per_capita) - min(gdp_per_capita))\n\n# More realistic chocolate consumption - loosely based on real consumption patterns\n# plus some randomness, but influenced by GDP\nchocolate_consumption &lt;- 4 + 8 * gdp_normalized + rnorm(16, 0, 0.8)\n\n# Nobel prizes - also influenced by GDP (research funding) with noise\n# The relationship is non-linear, but will show up as correlated\nnobel_prizes &lt;- 2 + 12 * gdp_normalized^1.2 + rnorm(16, 0, 1.5)\n\n# Create dataframe\ncountry_data &lt;- data.frame(\n  country = countries,\n  chocolate = round(chocolate_consumption, 1),\n  nobel = round(nobel_prizes, 1),\n  gdp = gdp_per_capita\n)\n\n# Fit regression model - chocolate vs nobel without controlling for GDP\nchocolate_nobel_model &lt;- lm(nobel ~ chocolate, data = country_data)\n\n# Better model that reveals the confounding\nfull_model &lt;- lm(nobel ~ chocolate + gdp, data = country_data)\n\n# Plot the apparent relationship\nggplot(country_data, aes(x = chocolate, y = nobel)) +\n  geom_point(color = \"darkblue\", size = 3, alpha = 0.7) +\n  geom_text(aes(label = country), hjust = -0.2, vjust = 0, size = 3) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  labs(\n    title = \"Apparent Correlation: Chocolate Consumption vs. Nobel Prizes\",\n    subtitle = \"Demonstrates how confounding variables create spurious correlations\",\n    x = \"Chocolate Consumption (kg per capita)\",\n    y = \"Nobel Prizes per 10M Population\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Show regression results\nsummary(chocolate_nobel_model)\n\n\nCall:\nlm(formula = nobel ~ chocolate, data = country_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9080 -1.4228  0.0294  0.5962  3.2977 \n\nCoefficients:\n            Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept)  -4.0518     1.3633  -2.972     0.0101 *  \nchocolate     1.3322     0.1682   7.921 0.00000154 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.626 on 14 degrees of freedom\nMultiple R-squared:  0.8176,    Adjusted R-squared:  0.8045 \nF-statistic: 62.75 on 1 and 14 DF,  p-value: 0.000001536\n\n# Demonstrate multiple testing problem\np_values &lt;- numeric(100)\nfor(i in 1:100) {\n  # Generate two completely random variables with n=20\n  x &lt;- rnorm(20)\n  y &lt;- rnorm(20)\n  # Test for correlation and store p-value\n  p_values[i] &lt;- cor.test(x, y)$p.value\n}\n\n# How many \"significant\" results at alpha = 0.05?\nsum(p_values &lt; 0.05)\n\n[1] 3\n\n# Visualize the multiple testing phenomenon\nhist(p_values, breaks = 20, main = \"P-values from 100 Tests of Random Data\",\n     xlab = \"P-value\", col = \"lightblue\", border = \"white\")\nabline(v = 0.05, col = \"red\", lwd = 2, lty = 2)\ntext(0.15, 20, paste(\"Approximately\", sum(p_values &lt; 0.05),\n                     \"tests are 'significant'\\nby random chance alone!\"), \n     col = \"darkred\")\n\n\n\n\n\n\n\n\nThis example demonstrates how seemingly compelling correlations can emerge between unrelated variables due to confounding factors and chance. The correlation between chocolate consumption and Nobel prizes appears significant (p &lt; 0.05) when analyzed directly, even though it’s explained by a third variable - national wealth (GDP per capita).\nWealthier countries typically consume more chocolate and simultaneously invest more in education and research, leading to more Nobel prizes. Without controlling for this confounding factor, we would mistakenly conclude a direct relationship between chocolate and Nobel prizes.\nThe multiple testing demonstration further illustrates why spurious correlations appear so frequently in research. When conducting 100 statistical tests on completely random data, we expect approximately 5 “significant” results at α = 0.05 purely by chance. In real research settings where hundreds of variables might be analyzed, the probability of finding false positive correlations increases dramatically.\nThis example underscores three critical points:\n\nSmall sample sizes (16 countries) are particularly vulnerable to chance correlations\nConfounding variables can create strong apparent associations between unrelated factors\nMultiple testing without appropriate corrections virtually guarantees finding “significant” but meaningless patterns\n\nSuch findings explain why replication is essential in research and why most initial “discoveries” fail to hold up in subsequent studies.\n\n\nConfounding Variables (Hidden Third Factors)\nConfounding occurs when an external variable influences both the predictor and outcome variables, creating an apparent relationship that may disappear when the confounder is accounted for.\n\n# Create sample data\nn &lt;- 200\nability &lt;- rnorm(n, 100, 15)                       # Natural ability \neducation &lt;- 10 + 0.05 * ability + rnorm(n, 0, 2)  # Education affected by ability\nincome &lt;- 10000 + 2000 * education + 100 * ability + rnorm(n, 0, 5000)  # Income affected by both\n\nomitted_var_data &lt;- data.frame(\n  ability = ability,\n  education = education,\n  income = income\n)\n\n# Model without accounting for ability\nmodel_naive &lt;- lm(income ~ education, data = omitted_var_data)\n\n# Model accounting for ability\nmodel_full &lt;- lm(income ~ education + ability, data = omitted_var_data)\n\n# Show results\nsummary(model_naive)\n\n\nCall:\nlm(formula = income ~ education, data = omitted_var_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14422.9  -3362.1    142.7   3647.7  14229.6 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  18982.0     2410.5   7.875    0.000000000000221 ***\neducation     2050.9      158.7  12.926 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5066 on 198 degrees of freedom\nMultiple R-squared:  0.4576,    Adjusted R-squared:  0.4549 \nF-statistic: 167.1 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_full)\n\n\nCall:\nlm(formula = income ~ education + ability, data = omitted_var_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12739.9  -3388.7    -41.1   3572.1  14976.8 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 13203.84    3018.85   4.374            0.0000198 ***\neducation    1871.43     166.03  11.272 &lt; 0.0000000000000002 ***\nability        85.60      27.87   3.071              0.00243 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4961 on 197 degrees of freedom\nMultiple R-squared:  0.4824,    Adjusted R-squared:  0.4772 \nF-statistic: 91.81 on 2 and 197 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Create visualization with ability shown through color\nggplot(omitted_var_data, aes(x = education, y = income, color = ability)) +\n  geom_point(alpha = 0.8) +\n  scale_color_viridis_c(name = \"Ability Score\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) + \n  labs(\n    title = \"Income vs. Education, Colored by Ability\",\n    subtitle = \"Visualizing the confounding variable\",\n    x = \"Years of Education\",\n    y = \"Annual Income (PLN)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis example illustrates omitted variable bias: without accounting for ability, the estimated effect of education on income is exaggerated (2,423 PLN per year vs. 1,962 PLN per year). The confounding occurs because ability influences both education and income, creating a spurious component in the observed correlation.\n\nClassic Example: Ice Cream and Drownings\nA classic example of confounding involves the correlation between ice cream sales and drowning incidents, both influenced by temperature:\n\n# Create sample data\nn &lt;- 100\ntemperature &lt;- runif(n, 5, 35)  # Temperature in Celsius\n\n# Both ice cream sales and drownings are influenced by temperature\nice_cream_sales &lt;- 100 + 10 * temperature + rnorm(n, 0, 20)\ndrownings &lt;- 1 + 0.3 * temperature + rnorm(n, 0, 1)\n\nconfounding_data &lt;- data.frame(\n  temperature = temperature,\n  ice_cream_sales = ice_cream_sales,\n  drownings = drownings\n)\n\n# Model without controlling for temperature\nmodel_naive &lt;- lm(drownings ~ ice_cream_sales, data = confounding_data)\n\n# Model controlling for temperature\nmodel_full &lt;- lm(drownings ~ ice_cream_sales + temperature, data = confounding_data)\n\n# Show results\nsummary(model_naive)\n\n\nCall:\nlm(formula = drownings ~ ice_cream_sales, data = confounding_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8163 -0.7597  0.0118  0.7846  2.5797 \n\nCoefficients:\n                 Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)     -1.503063   0.370590  -4.056              0.0001 ***\nice_cream_sales  0.028074   0.001205  23.305 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.088 on 98 degrees of freedom\nMultiple R-squared:  0.8471,    Adjusted R-squared:  0.8456 \nF-statistic: 543.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_full)\n\n\nCall:\nlm(formula = drownings ~ ice_cream_sales + temperature, data = confounding_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.85074 -0.61169  0.01186  0.60556  2.01776 \n\nCoefficients:\n                 Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)      1.243785   0.530123   2.346         0.021 *  \nice_cream_sales -0.002262   0.004839  -0.467         0.641    \ntemperature      0.317442   0.049515   6.411 0.00000000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9169 on 97 degrees of freedom\nMultiple R-squared:  0.8926,    Adjusted R-squared:  0.8904 \nF-statistic: 403.2 on 2 and 97 DF,  p-value: &lt; 0.00000000000000022\n\n# Create visualization\nggplot(confounding_data, aes(x = ice_cream_sales, y = drownings, color = temperature)) +\n  geom_point(alpha = 0.8) +\n  scale_color_viridis_c(name = \"Temperature (°C)\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"The Ice Cream and Drownings Correlation\",\n    subtitle = \"Temperature as a confounding variable\",\n    x = \"Ice Cream Sales\",\n    y = \"Drowning Incidents\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe naive model shows a statistically significant relationship between ice cream sales and drownings. However, once temperature is included in the model, the coefficient for ice cream sales decreases substantially and becomes statistically insignificant. This demonstrates how failing to account for confounding variables can lead to spurious correlations.\n\n\n\nReverse Causality\nReverse causality occurs when the assumed direction of causation is incorrect. Consider this example of anxiety and relaxation techniques:\n\n# Create sample data\nn &lt;- 200\nanxiety_level &lt;- runif(n, 1, 10)  # Anxiety level (1-10)\n\n# People with higher anxiety tend to use more relaxation techniques\nrelaxation_techniques &lt;- 1 + 0.7 * anxiety_level + rnorm(n, 0, 1)\n\nreverse_data &lt;- data.frame(\n  anxiety = anxiety_level,\n  relaxation = relaxation_techniques\n)\n\n# Fit models in both directions\nmodel_incorrect &lt;- lm(anxiety ~ relaxation, data = reverse_data)\nmodel_correct &lt;- lm(relaxation ~ anxiety, data = reverse_data)\n\n# Show regression results\nsummary(model_incorrect)\n\n\nCall:\nlm(formula = anxiety ~ relaxation, data = reverse_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9651 -0.7285 -0.0923  0.7247  3.7996 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) -0.09482    0.21973  -0.432               0.667    \nrelaxation   1.15419    0.04105  28.114 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.182 on 198 degrees of freedom\nMultiple R-squared:  0.7997,    Adjusted R-squared:  0.7987 \nF-statistic: 790.4 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_correct)\n\n\nCall:\nlm(formula = relaxation ~ anxiety, data = reverse_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.15178 -0.51571 -0.00222  0.55513  2.04334 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  1.05726    0.15286   6.917      0.0000000000624 ***\nanxiety      0.69284    0.02464  28.114 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9161 on 198 degrees of freedom\nMultiple R-squared:  0.7997,    Adjusted R-squared:  0.7987 \nF-statistic: 790.4 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(reverse_data, aes(x = relaxation, y = anxiety)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Anxiety and Relaxation Techniques\",\n    subtitle = \"Example of reverse causality\",\n    x = \"Use of Relaxation Techniques (frequency/week)\",\n    y = \"Anxiety Level (1-10 scale)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nBoth regression models show statistically significant relationships, but they imply different causal mechanisms. The incorrect model suggests that relaxation techniques increase anxiety, while the correct model reflects the true data generating process: anxiety drives the use of relaxation techniques.\n\n\nCollider Bias (Selection Bias)\nCollider bias occurs when conditioning on a variable that is affected by both the independent and dependent variables of interest, creating an artificial relationship between variables that are actually independent.\n\n# Create sample data\nn &lt;- 1000\n\n# Generate two independent variables (no relationship between them)\nintelligence &lt;- rnorm(n, 100, 15)  # IQ score\nfamily_wealth &lt;- rnorm(n, 50, 15)  # Wealth score (independent from intelligence)\n  \n# True data-generating process: admission depends on both intelligence and wealth\nadmission_score &lt;- 0.4 * intelligence + 0.4 * family_wealth + rnorm(n, 0, 10)\nadmitted &lt;- admission_score &gt; median(admission_score)  # Binary admission variable\n\n# Create full dataset\nfull_data &lt;- data.frame(\n  intelligence = intelligence,\n  wealth = family_wealth,\n  admission_score = admission_score,\n  admitted = admitted\n)\n\n# Regression in full population (true model)\nfull_model &lt;- lm(intelligence ~ wealth, data = full_data)\nsummary(full_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth, data = full_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-49.608 -10.115   0.119  10.832  55.581 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 101.42330    1.73139   58.58 &lt;0.0000000000000002 ***\nwealth       -0.02701    0.03334   -0.81               0.418    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.41 on 998 degrees of freedom\nMultiple R-squared:  0.0006569, Adjusted R-squared:  -0.0003444 \nF-statistic: 0.656 on 1 and 998 DF,  p-value: 0.4182\n\n# Get just the admitted students\nadmitted_only &lt;- full_data[full_data$admitted, ]\n\n# Regression in admitted students (conditioning on the collider)\nadmitted_model &lt;- lm(intelligence ~ wealth, data = admitted_only)\nsummary(admitted_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth, data = admitted_only)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.511  -9.064   0.721   8.965  48.267 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 115.4750     2.6165  44.133 &lt; 0.0000000000000002 ***\nwealth       -0.1704     0.0462  -3.689              0.00025 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.91 on 498 degrees of freedom\nMultiple R-squared:  0.0266,    Adjusted R-squared:  0.02464 \nF-statistic: 13.61 on 1 and 498 DF,  p-value: 0.0002501\n\n# Additional analysis - regression with the collider as a control variable\n# This demonstrates how controlling for a collider introduces bias\ncollider_control_model &lt;- lm(intelligence ~ wealth + admitted, data = full_data)\nsummary(collider_control_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth + admitted, data = full_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.729  -8.871   0.700   8.974  48.044 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  102.90069    1.56858  65.601 &lt; 0.0000000000000002 ***\nwealth        -0.19813    0.03224  -6.145        0.00000000116 ***\nadmittedTRUE  14.09944    0.94256  14.959 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.93 on 997 degrees of freedom\nMultiple R-squared:  0.1838,    Adjusted R-squared:  0.1822 \nF-statistic: 112.3 on 2 and 997 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Plot for full population\np1 &lt;- ggplot(full_data, aes(x = wealth, y = intelligence)) +\n  geom_point(alpha = 0.3, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Full Population\",\n    subtitle = paste(\"Correlation:\", round(cor(full_data$intelligence, full_data$wealth), 3)),\n    x = \"Family Wealth Score\",\n    y = \"Intelligence Score\"\n  ) +\n  theme_minimal()\n\n# Plot for admitted students\np2 &lt;- ggplot(admitted_only, aes(x = wealth, y = intelligence)) +\n  geom_point(alpha = 0.3, color = \"darkgreen\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Admitted Students Only\",\n    subtitle = paste(\"Correlation:\", round(cor(admitted_only$intelligence, admitted_only$wealth), 3)),\n    x = \"Family Wealth Score\",\n    y = \"Intelligence Score\"\n  ) +\n  theme_minimal()\n\n# Display plots side by side\nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis example demonstrates collider bias in three ways:\n\nIn the full population, intelligence and wealth have no relationship (coefficient near zero, p-value = 0.87)\nAmong admitted students (conditioning on the collider), a significant negative relationship appears (coefficient = -0.39, p-value &lt; 0.001)\nWhen controlling for admission status in a regression, a spurious relationship is introduced (coefficient = -0.16, p-value &lt; 0.001)\n\nThe collider bias creates relationships between variables that are truly independent. This can be represented in a directed acyclic graph (DAG):\n\\text{Intelligence} \\rightarrow \\text{Admission} \\leftarrow \\text{Wealth}\nWhen we condition on admission (the collider), we create a spurious association between intelligence and wealth.\n\n\nImproper Analysis\nInappropriate statistical methods can produce spurious correlations. Common issues include using linear models for non-linear relationships, ignoring data clustering, or mishandling time series data.\n\n# Generate data with a true non-linear relationship\nn &lt;- 100\nx &lt;- seq(-3, 3, length.out = n)\ny &lt;- x^2 + rnorm(n, 0, 1)  # Quadratic relationship\n\nimproper_data &lt;- data.frame(x = x, y = y)\n\n# Fit incorrect linear model\nwrong_model &lt;- lm(y ~ x, data = improper_data)\n\n# Fit correct quadratic model\ncorrect_model &lt;- lm(y ~ x + I(x^2), data = improper_data)\n\n# Show results\nsummary(wrong_model)\n\n\nCall:\nlm(formula = y ~ x, data = improper_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2176 -2.1477 -0.6468  2.4365  7.3457 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  3.14689    0.28951  10.870 &lt;0.0000000000000002 ***\nx            0.08123    0.16548   0.491               0.625    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.895 on 98 degrees of freedom\nMultiple R-squared:  0.002453,  Adjusted R-squared:  -0.007726 \nF-statistic: 0.2409 on 1 and 98 DF,  p-value: 0.6246\n\nsummary(correct_model)\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = improper_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81022 -0.65587  0.01935  0.61168  2.68894 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  0.12407    0.14498   0.856               0.394    \nx            0.08123    0.05524   1.470               0.145    \nI(x^2)       0.98766    0.03531  27.972 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9664 on 97 degrees of freedom\nMultiple R-squared:   0.89, Adjusted R-squared:  0.8877 \nF-statistic: 392.3 on 2 and 97 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(improper_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), color = \"green\", se = FALSE) +\n  labs(\n    title = \"Improper Analysis Example\",\n    subtitle = \"Linear model (red) vs. Quadratic model (green)\",\n    x = \"Variable X\",\n    y = \"Variable Y\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe linear model incorrectly suggests no relationship between x and y (coefficient near zero, p-value = 0.847), while the quadratic model reveals the true relationship (R^2 = 0.90). This demonstrates how model misspecification can create spurious non-correlations, masking real relationships that exist in different forms.\n\n\nEndogeneity and Its Sources\nEndogeneity occurs when an explanatory variable is correlated with the error term in a regression model. This violates the exogeneity assumption of OLS regression and leads to biased estimates. There are several sources of endogeneity:\n\nOmitted Variable Bias\nAs shown in the education-income example, when important variables are omitted from the model, their effects are absorbed into the error term, which becomes correlated with included variables.\n\n\nMeasurement Error\nWhen variables are measured with error, the observed values differ from true values, creating correlation between the error term and the predictors.\n\n\nSimultaneity (Bidirectional Causality)\nWhen the dependent variable also affects the independent variable, creating a feedback loop. Let’s demonstrate this:\n\n# Create sample data with mutual influence\nn &lt;- 100\n\n# Initialize variables\neconomic_growth &lt;- rnorm(n, 2, 1)\nemployment_rate &lt;- rnorm(n, 60, 5)\n\n# Create mutual influence through iterations\nfor(i in 1:3) {\n  economic_growth &lt;- 2 + 0.05 * employment_rate + rnorm(n, 0, 0.5)\n  employment_rate &lt;- 50 + 5 * economic_growth + rnorm(n, 0, 2)\n}\n\nsimultaneity_data &lt;- data.frame(\n  growth = economic_growth,\n  employment = employment_rate\n)\n\n# Model estimating effect of growth on employment\nmodel_growth_on_emp &lt;- lm(employment ~ growth, data = simultaneity_data)\n\n# Model estimating effect of employment on growth\nmodel_emp_on_growth &lt;- lm(growth ~ employment, data = simultaneity_data)\n\n# Show results\nsummary(model_growth_on_emp)\n\n\nCall:\nlm(formula = employment ~ growth, data = simultaneity_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.603 -1.500 -0.099  1.387  5.673 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  49.9665     2.0717   24.12 &lt;0.0000000000000002 ***\ngrowth        5.0151     0.3528   14.22 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.045 on 98 degrees of freedom\nMultiple R-squared:  0.6734,    Adjusted R-squared:  0.6701 \nF-statistic: 202.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_emp_on_growth)\n\n\nCall:\nlm(formula = growth ~ employment, data = simultaneity_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11417 -0.20626 -0.02185  0.22646  0.72941 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) -4.801257   0.749557  -6.405        0.00000000523 ***\nemployment   0.134283   0.009446  14.216 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3346 on 98 degrees of freedom\nMultiple R-squared:  0.6734,    Adjusted R-squared:  0.6701 \nF-statistic: 202.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(simultaneity_data, aes(x = growth, y = employment)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Simultaneity Between Economic Growth and Employment\",\n    x = \"Economic Growth (%)\",\n    y = \"Employment Rate (%)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe true data generating process is a system of simultaneous equations:\n\\text{Growth}_i = \\alpha_0 + \\alpha_1 \\text{Employment}_i + u_i \\text{Employment}_i = \\beta_0 + \\beta_1 \\text{Growth}_i + v_i\nStandard OLS regression cannot consistently estimate either equation because each explanatory variable is correlated with the error term in its respective equation.\n\n\nSelection Bias\nWhen the sample is not randomly selected from the population, the selection process can introduce correlation between the error term and the predictors. The collider bias example demonstrates a form of selection bias.\nThe consequences of endogeneity include: - Biased coefficient estimates - Incorrect standard errors - Invalid hypothesis tests - Misleading causal interpretations\nAddressing endogeneity requires specialized methods such as instrumental variables, system estimation, panel data methods, or experimental designs.\n\n\n\n\n\n\nUnderstanding Endogeneity in Regression\n\n\n\nEndogeneity is a critical concept in statistical analysis that occurs when an explanatory variable in a regression model is correlated with the error term. This creates challenges for accurately understanding cause-and-effect relationships in research. Let’s examine the three main types of endogeneity and how they affect research outcomes.\n\nOmitted Variable Bias (OVB)\nOmitted Variable Bias occurs when an important variable that affects both the dependent and independent variables is left out of the analysis. This omission leads to incorrect conclusions about the relationship between the variables we’re studying.\nConsider a study examining the relationship between education and income:\nExample: Education and Income The observed relationship shows that more education correlates with higher income. However, an individual’s inherent abilities affect both their educational attainment and their earning potential. Without accounting for ability, we may overestimate education’s direct effect on income.\nThe statistical representation shows why this matters:\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2z_i + \\epsilon_i (Complete model)\ny_i = \\beta_0 + \\beta_1x_i + u_i (Incomplete model)\nWhen we omit an important variable, our estimates of the remaining relationships become biased and unreliable.\n\n\nSimultaneity\nSimultaneity occurs when two variables simultaneously influence each other, making it difficult to determine the direction of causation. This creates a feedback loop that complicates statistical analysis.\nCommon Examples of Simultaneity:\nAcademic Performance and Study Habits represent a clear case of simultaneity. Academic performance influences how much time students dedicate to studying, while study time affects academic performance. This two-way relationship makes it challenging to measure the isolated effect of either variable.\nMarket Dynamics provide another example. Prices influence demand, while demand influences prices. This concurrent relationship requires special analytical approaches to understand the true relationships.\n\n\nMeasurement Error\nMeasurement error occurs when we cannot accurately measure our variables of interest. This imprecision can significantly impact our analysis and conclusions.\nCommon Sources of Measurement Error:\nSelf-Reported Data presents a significant challenge. When participants report their own behaviors or characteristics, such as study time, the reported values often differ from actual values. This discrepancy affects our ability to measure true relationships.\nTechnical Limitations also contribute to measurement error through imprecise measuring tools, inconsistent measurement conditions, and recording or data entry errors.\n\n\nAddressing Endogeneity in Research\n\nIdentification Strategies\n\n# Example of controlling for omitted variables\nmodel_simple &lt;- lm(income ~ education, data = df)\nmodel_full &lt;- lm(income ~ education + ability + experience + region, data = df)\n\n# Compare coefficients\nsummary(model_simple)\nsummary(model_full)\n\n\nInclude Additional Variables: Collect data on potentially important omitted variables and include relevant control variables in your analysis. For example, including measures of ability when studying education’s effect on income.\nUse Panel Data: Collect data across multiple time periods to control for unobserved fixed characteristics and analyze changes over time.\nInstrumental Variables: Find variables that affect your independent variable but not your dependent variable to isolate the relationship of interest.\n\n\n\nImproving Measurement\n\nMultiple Measurements: Take several measurements of key variables, use averaging to reduce random error, and compare different measurement methods.\nBetter Data Collection: Use validated measurement instruments, implement quality control procedures, and document potential sources of error.\n\n\n\n\nBest Practices for Researchers\nResearch Design fundamentally shapes your ability to address endogeneity. Plan for potential endogeneity issues before collecting data, include measures for potentially important control variables, and consider using multiple measurement approaches.\nAnalysis should include testing for endogeneity when possible, using appropriate statistical methods for your specific situation, and documenting assumptions and limitations.\nReporting must clearly describe potential endogeneity concerns, explain how you addressed these issues, and discuss implications for your conclusions.\n\n\n\n\n\n\n\n\n\n\nFormal Derivation of OLS Estimators (*)\n\n\n\n\nObjective and Setup\nWe seek to find the values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that minimize the sum of squared residuals:\nSSE = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nThis is an unconstrained optimization problem where we treat SSE as a function of two variables: SSE(\\hat{\\beta}_0, \\hat{\\beta}_1).\n\n\nMathematical Prerequisites\nChain Rule for Composite Functions: For f(g(x)), the derivative is: \\frac{d}{dx}[f(g(x))] = f'(g(x)) \\cdot g'(x)\nIn our context:\n\nOuter function: f(u) = u^2 with derivative f'(u) = 2u\nInner function: u = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\n\nFirst-Order Conditions: At a minimum, both partial derivatives equal zero: \\frac{\\partial SSE}{\\partial \\hat{\\beta}_0} = 0 \\quad \\text{and} \\quad \\frac{\\partial SSE}{\\partial \\hat{\\beta}_1} = 0\n\n\nDerivation of \\hat{\\beta}_0\nStep 1: Take the partial derivative with respect to \\hat{\\beta}_0:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_0} = \\frac{\\partial}{\\partial \\hat{\\beta}_0} \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2\nStep 2: Apply the chain rule to each term: = \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot \\frac{\\partial}{\\partial \\hat{\\beta}_0}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\n= \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot (-1)\n= -2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\nStep 3: Set equal to zero and solve: -2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\nDividing by -2 and expanding: \\sum_{i=1}^n y_i - n\\hat{\\beta}_0 - \\hat{\\beta}_1\\sum_{i=1}^n x_i = 0\nStep 4: Isolate \\hat{\\beta}_0: n\\hat{\\beta}_0 = \\sum_{i=1}^n y_i - \\hat{\\beta}_1\\sum_{i=1}^n x_i\n\\boxed{\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}}\nInterpretation: The intercept adjusts to ensure the regression line passes through the point of means (\\bar{x}, \\bar{y}).\n\n\nDerivation of \\hat{\\beta}_1\nStep 1: Take the partial derivative with respect to \\hat{\\beta}_1:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_1} = \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot (-x_i)\n= -2\\sum_{i=1}^n x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\nStep 2: Substitute the expression for \\hat{\\beta}_0: = -2\\sum_{i=1}^n x_i(y_i - (\\bar{y} - \\hat{\\beta}_1\\bar{x}) - \\hat{\\beta}_1x_i)\n= -2\\sum_{i=1}^n x_i((y_i - \\bar{y}) - \\hat{\\beta}_1(x_i - \\bar{x}))\nStep 3: Set equal to zero and expand: \\sum_{i=1}^n x_i(y_i - \\bar{y}) - \\hat{\\beta}_1\\sum_{i=1}^n x_i(x_i - \\bar{x}) = 0\nStep 4: Use the algebraic identity \\sum_{i=1}^n x_i(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}):\nThis identity holds because: \\sum_{i=1}^n x_i(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x} + \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) + \\bar{x}\\sum_{i=1}^n(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) + 0\nSimilarly, \\sum_{i=1}^n x_i(x_i - \\bar{x}) = \\sum_{i=1}^n (x_i - \\bar{x})^2.\nStep 5: Solve for \\hat{\\beta}_1: \\hat{\\beta}_1\\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\n\\boxed{\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}}\n\n\nVerification of Minimum (Second-Order Conditions)\nTo confirm we have found a minimum (not a maximum or saddle point), we examine the Hessian matrix of second partial derivatives:\nSecond partial derivatives:\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_0^2} = 2n &gt; 0\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_1^2} = 2\\sum_{i=1}^n x_i^2 &gt; 0\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_0 \\partial \\hat{\\beta}_1} = 2\\sum_{i=1}^n x_i\nHessian matrix: \\mathbf{H} = 2\\begin{bmatrix} n & \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i & \\sum_{i=1}^n x_i^2 \\end{bmatrix}\nPositive definiteness check:\n\nFirst leading principal minor: 2n &gt; 0 ✓\nSecond leading principal minor (determinant): \\det(\\mathbf{H}) = 4\\left(n\\sum_{i=1}^n x_i^2 - \\left(\\sum_{i=1}^n x_i\\right)^2\\right) = 4n\\sum_{i=1}^n(x_i - \\bar{x})^2 &gt; 0 ✓\n\nSince the Hessian is positive definite, we have confirmed a minimum.\n\n\nGeometric Interpretation\n\n# Visualizing the optimization surface\nlibrary(tidyverse)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Generate sample data\nset.seed(42)\nx &lt;- runif(20, 1, 8)\ny &lt;- 2 + 3*x + rnorm(20, 0, 1)\n\n# Create grid of beta values\nbeta0_seq &lt;- seq(0, 4, length.out = 50)\nbeta1_seq &lt;- seq(2, 4, length.out = 50)\ngrid &lt;- expand.grid(beta0 = beta0_seq, beta1 = beta1_seq)\n\n# Calculate SSE for each combination\ngrid$SSE &lt;- apply(grid, 1, function(params) {\n  sum((y - (params[1] + params[2]*x))^2)\n})\n\n# Create contour plot\nggplot(grid, aes(x = beta0, y = beta1, z = SSE)) +\n  geom_contour_filled(aes(fill = after_stat(level))) +\n  geom_point(x = coef(lm(y ~ x))[1], \n             y = coef(lm(y ~ x))[2], \n             color = \"red\", size = 3) +\n  labs(title = \"SSE Surface in Parameter Space\",\n       subtitle = \"Red point shows the OLS minimum\",\n       x = expression(hat(beta)[0]),\n       y = expression(hat(beta)[1])) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.1-understanding-correlation-measures-a-self-study-tutorial-using-stress-level-and-cognitive-performance-data",
    "href": "correg_en.html#appendix-a.1-understanding-correlation-measures-a-self-study-tutorial-using-stress-level-and-cognitive-performance-data",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.30 Appendix A.1: Understanding Correlation Measures: A Self-Study Tutorial (Using Stress Level and Cognitive Performance Data)",
    "text": "9.30 Appendix A.1: Understanding Correlation Measures: A Self-Study Tutorial (Using Stress Level and Cognitive Performance Data)\n\nDataset Overview\n\ndata &lt;- data.frame(\n  anxiety_level = c(8, 5, 11, 14, 7, 10),\n  cognitive_performance = c(85, 90, 62, 55, 80, 65)\n)\n\n\n\n1. Covariance Calculation\n\nStep 1: Calculate Means\n\n\n\n\n\n\n\n\nVariable\nCalculation\nResult\n\n\n\n\nMean Anxiety (\\bar{x})\n(8 + 5 + 11 + 14 + 7 + 10) ÷ 6\n9.17\n\n\nMean Cognitive (\\bar{y})\n(85 + 90 + 62 + 55 + 80 + 65) ÷ 6\n72.83\n\n\n\n\n\nStep 2: Calculate Deviations and Products\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\n(x_i - \\bar{x})\n(y_i - \\bar{y})\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n1\n8\n85\n-1.17\n12.17\n-14.24\n\n\n2\n5\n90\n-4.17\n17.17\n-71.60\n\n\n3\n11\n62\n1.83\n-10.83\n-19.82\n\n\n4\n14\n55\n4.83\n-17.83\n-86.12\n\n\n5\n7\n80\n-2.17\n7.17\n-15.56\n\n\n6\n10\n65\n0.83\n-7.83\n-6.50\n\n\nSum\n55\n437\n0.00\n0.00\n-213.84\n\n\n\n\n\nStep 3: Calculate Covariance\n \\text{Cov}(X,Y) = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n-1} = \\frac{-213.84}{5} = -42.77 \n\n\n\n2. Pearson Correlation Coefficient\n\nStep 1: Calculate Squared Deviations\n\n\n\n\n\n\n\n\n\n\ni\n(x_i - \\bar{x})\n(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n-1.17\n12.17\n1.37\n148.11\n\n\n2\n-4.17\n17.17\n17.39\n294.81\n\n\n3\n1.83\n-10.83\n3.35\n117.29\n\n\n4\n4.83\n-17.83\n23.33\n317.91\n\n\n5\n-2.17\n7.17\n4.71\n51.41\n\n\n6\n0.83\n-7.83\n0.69\n61.31\n\n\nSum\n0.00\n0.00\n50.84\n990.84\n\n\n\n\n\nStep 2: Calculate Standard Deviations\n\n\n\n\n\n\n\n\n\nMeasure\nFormula\nCalculation\nResult\n\n\n\n\ns_x\n\\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n-1}}\n\\sqrt{\\frac{50.84}{5}}\n3.19\n\n\ns_y\n\\sqrt{\\frac{\\sum (y_i - \\bar{y})^2}{n-1}}\n\\sqrt{\\frac{990.84}{5}}\n14.08\n\n\n\n\n\nStep 3: Calculate Pearson Correlation\n r = \\frac{\\text{Cov}(X,Y)}{s_x s_y} = \\frac{-42.77}{3.19 \\times 14.08} = -0.95 \n\n\n\n3. Spearman Rank Correlation\n\nStep 1: Assign Ranks\n\n\n\ni\nx_i\ny_i\nRank x_i\nRank y_i\nd_i\nd_i^2\n\n\n\n\n1\n8\n85\n3\n2\n1\n1\n\n\n2\n5\n90\n1\n1\n0\n0\n\n\n3\n11\n62\n5\n5\n0\n0\n\n\n4\n14\n55\n6\n6\n0\n0\n\n\n5\n7\n80\n2\n3\n-1\n1\n\n\n6\n10\n65\n4\n4\n0\n0\n\n\nSum\n\n\n\n\n\n2\n\n\n\n\n\nStep 2: Calculate Spearman Correlation\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} = 1 - \\frac{6(2)}{6(36-1)} = 1 - \\frac{12}{210} = 0.94 \n\n\n\nVerification using R\n\n# Calculate correlations using R\ncor(data$anxiety_level, data$cognitive_performance, method = \"pearson\")\n\n[1] -0.9527979\n\ncor(data$anxiety_level, data$cognitive_performance, method = \"spearman\")\n\n[1] -0.9428571\n\n\n\n\nInterpretation\n\nThe strong negative Pearson correlation (r = -0.95) indicates a very strong negative linear relationship between anxiety level and cognitive performance.\nThe strong positive Spearman correlation (ρ = 0.94) shows that the relationship is also strongly monotonic.\nThe difference between Pearson and Spearman correlations suggests that while there is a strong relationship, it might not be perfectly linear.\n\n\n\nExercise\n\nVerify each calculation step in the tables above.\nTry calculating these measures with a modified dataset:\n\nAdd one outlier and observe how it affects both correlation coefficients\nChange one pair of values and recalculate",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.2-calculating-covariance-pearson-spearman-correlation-and-ols---worked-example",
    "href": "correg_en.html#appendix-a.2-calculating-covariance-pearson-spearman-correlation-and-ols---worked-example",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.31 Appendix A.2: Calculating Covariance, Pearson, Spearman Correlation, and OLS - worked example",
    "text": "9.31 Appendix A.2: Calculating Covariance, Pearson, Spearman Correlation, and OLS - worked example\nA political science student is investigating the relationship between district magnitude (DM) and Gallagher’s disproportionality index (GH) in parliamentary elections across 10 randomly selected democracies.\nData on electoral district magnitudes (\\text{DM}) and Gallagher index:\n\n\n\n\\text{DM} (X)\nGallagher (Y)\n\n\n\n\n2\n18.2\n\n\n3\n16.7\n\n\n4\n15.8\n\n\n5\n15.3\n\n\n6\n15.0\n\n\n7\n14.8\n\n\n8\n14.7\n\n\n9\n14.6\n\n\n10\n14.55\n\n\n11\n14.52\n\n\n\n\nStep 1: Calculate Basic Statistics\nCalculation of means:\nFor \\text{DM} (X): \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\nDetailed calculation:\n2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 65 \\bar{x} = \\frac{65}{10} = 6.5\nFor Gallagher index (Y): \\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\nDetailed calculation:\n18.2 + 16.7 + 15.8 + 15.3 + 15.0 + 14.8 + 14.7 + 14.6 + 14.55 + 14.52 = 154.17 \\bar{y} = \\frac{154.17}{10} = 15.417\n\n\nStep 2: Detailed Covariance Calculations\nComplete working table showing all calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18.2\n-4.5\n2.783\n-12.5235\n20.25\n7.7451\n\n\n2\n3\n16.7\n-3.5\n1.283\n-4.4905\n12.25\n1.6461\n\n\n3\n4\n15.8\n-2.5\n0.383\n-0.9575\n6.25\n0.1467\n\n\n4\n5\n15.3\n-1.5\n-0.117\n0.1755\n2.25\n0.0137\n\n\n5\n6\n15.0\n-0.5\n-0.417\n0.2085\n0.25\n0.1739\n\n\n6\n7\n14.8\n0.5\n-0.617\n-0.3085\n0.25\n0.3807\n\n\n7\n8\n14.7\n1.5\n-0.717\n-1.0755\n2.25\n0.5141\n\n\n8\n9\n14.6\n2.5\n-0.817\n-2.0425\n6.25\n0.6675\n\n\n9\n10\n14.55\n3.5\n-0.867\n-3.0345\n12.25\n0.7517\n\n\n10\n11\n14.52\n4.5\n-0.897\n-4.0365\n20.25\n0.8047\n\n\nSum\n65\n154.17\n0\n0\n-28.085\n82.5\n12.8442\n\n\n\nCovariance calculation: \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\text{Cov}(X,Y) = \\frac{-28.085}{9} = -3.120556\n\n\nStep 3: Standard Deviation Calculations\nFor \\text{DM} (X): \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}}\n\\sigma_x = \\sqrt{\\frac{82.5}{9}} = \\sqrt{9.1667} = 3.026582\nFor Gallagher (Y): \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}}\n\\sigma_y = \\sqrt{\\frac{12.8442}{9}} = \\sqrt{1.4271} = 1.194612\n\n\nStep 4: Pearson Correlation Calculation\nr = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\nr = \\frac{-3.120556}{3.026582 \\times 1.194612} = \\frac{-3.120556}{3.615752} = -0.863044\n\n\nStep 5: Spearman Rank Correlation Calculation\nComplete ranking table with all calculations:\n\n\n\ni\nX_i\nY_i\nRank X_i\nRank Y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n18.2\n1\n10\n-9\n81\n\n\n2\n3\n16.7\n2\n9\n-7\n49\n\n\n3\n4\n15.8\n3\n8\n-5\n25\n\n\n4\n5\n15.3\n4\n7\n-3\n9\n\n\n5\n6\n15.0\n5\n6\n-1\n1\n\n\n6\n7\n14.8\n6\n5\n1\n1\n\n\n7\n8\n14.7\n7\n4\n3\n9\n\n\n8\n9\n14.6\n8\n3\n5\n25\n\n\n9\n10\n14.55\n9\n2\n7\n49\n\n\n10\n11\n14.52\n10\n1\n9\n81\n\n\nSum\n\n\n\n\n\n330\n\n\n\nSpearman correlation calculation: \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n\\rho = 1 - \\frac{6 \\times 330}{10(100 - 1)} = 1 - \\frac{1980}{990} = 1 - 2 = -1\n\n\nStep 6: R Verification\n\n# Create vectors\nDM &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nGH &lt;- c(18.2, 16.7, 15.8, 15.3, 15.0, 14.8, 14.7, 14.6, 14.55, 14.52)\n\n# Calculate covariance\ncov(DM, GH)\n\n[1] -3.120556\n\n# Calculate correlations\ncor(DM, GH, method = \"pearson\")\n\n[1] -0.8627742\n\ncor(DM, GH, method = \"spearman\")\n\n[1] -1\n\n\n\n\nStep 7: Basic Visualization\n\nlibrary(ggplot2)\n\n# Create data frame\ndata &lt;- data.frame(DM = DM, GH = GH)\n\n# Create scatter plot\nggplot(data, aes(x = DM, y = GH)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"District Magnitude vs Gallagher Index\",\n    x = \"District Magnitude (DM)\",\n    y = \"Gallagher Index (GH)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nOLS Estimation and Goodness-of-Fit Measures\n\n\nStep 1: Calculate OLS Estimates\nUsing previously calculated values:\n\n\\sum(X_i - \\bar{X})(Y_i - \\bar{Y}) = -28.085\n\\sum(X_i - \\bar{X})^2 = 82.5\n\\bar{X} = 6.5\n\\bar{Y} = 15.417\n\nCalculate slope (\\hat{\\beta_1}): \\hat{\\beta_1} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\hat{\\beta_1} = -28,085 ÷ 82,5 = -0,3404\nCalculate intercept (\\hat{\\beta_0}): \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 15,417 - (-0,3404 × 6,5)\n   = 15,417 + 2,2126\n   = 17,6296\nTherefore, the OLS regression equation is: \\hat{Y} = 17.6296 - 0.3404X\n\n\nStep 2: Calculate Fitted Values and Residuals\nComplete table showing all calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i\ne_i = Y_i - \\hat{Y}_i\ne_i^2\n(Y_i - \\bar{Y})^2\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18.2\n16.9488\n1.2512\n1.5655\n7.7451\n2.3404\n\n\n2\n3\n16.7\n16.6084\n0.0916\n0.0084\n1.6461\n1.4241\n\n\n3\n4\n15.8\n16.2680\n-0.4680\n0.2190\n0.1467\n0.7225\n\n\n4\n5\n15.3\n15.9276\n-0.6276\n0.3939\n0.0137\n0.2601\n\n\n5\n6\n15.0\n15.5872\n-0.5872\n0.3448\n0.1739\n0.0289\n\n\n6\n7\n14.8\n15.2468\n-0.4468\n0.1996\n0.3807\n0.0290\n\n\n7\n8\n14.7\n14.9064\n-0.2064\n0.0426\n0.5141\n0.2610\n\n\n8\n9\n14.6\n14.5660\n0.0340\n0.0012\n0.6675\n0.7241\n\n\n9\n10\n14.55\n14.2256\n0.3244\n0.1052\n0.7517\n1.4184\n\n\n10\n11\n14.52\n13.8852\n0.6348\n0.4030\n0.8047\n2.3439\n\n\nSum\n65\n154.17\n154.17\n0\n3.2832\n12.8442\n9.5524\n\n\n\nCalculations for fitted values:\nFor X = 2:\nŶ = 17.6296 + (-0.3404 × 2) = 16.9488\n\nFor X = 3:\nŶ = 17.6296 + (-0.3404 × 3) = 16.6084\n\n[... continue for all values]\n\n\nStep 3: Calculate Goodness-of-Fit Measures\nSum of Squared Errors (SSE): SSE = \\sum e_i^2\nSSE = 3.2832\nSum of Squared Total (SST): SST = \\sum(Y_i - \\bar{Y})^2\nSST = 12.8442\nSum of Squared Regression (SSR): SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\nSSR = 9.5524\nVerify decomposition: SST = SSR + SSE\n12.8442 = 9.5524 + 3.2832 (within rounding error)\nR-squared calculation: R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nR² = 9.5524 ÷ 12.8442\n   = 0.7438\n\n\nStep 4: R Verification\n\n# Fit linear model\nmodel &lt;- lm(GH ~ DM, data = data)\n\n# View summary statistics\nsummary(model)\n\n\nCall:\nlm(formula = GH ~ DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62764 -0.46274 -0.08615  0.26624  1.25109 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept) 17.62976    0.50121  35.174 0.000000000467 ***\nDM          -0.34042    0.07053  -4.827        0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6406 on 8 degrees of freedom\nMultiple R-squared:  0.7444,    Adjusted R-squared:  0.7124 \nF-statistic:  23.3 on 1 and 8 DF,  p-value: 0.00131\n\n# Calculate R-squared manually\nSST &lt;- sum((GH - mean(GH))^2)\nSSE &lt;- sum(residuals(model)^2)\nSSR &lt;- SST - SSE\nR2_manual &lt;- SSR/SST\nR2_manual\n\n[1] 0.7443793\n\n\n\n\nStep 5: Residual Analysis\n\n# Create residual plots\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\nStep 6: Predicted vs Actual Values Plot\n\n# Create predicted vs actual plot\nggplot(data.frame(\n  Actual = GH,\n  Predicted = fitted(model)\n), aes(x = Predicted, y = Actual)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Predicted vs Actual Values\",\n    x = \"Predicted Gallagher Index\",\n    y = \"Actual Gallagher Index\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nLog-Transformed Models\n\n\nStep 1: Data Transformation\nFirst, calculate natural logarithms of variables:\n\n\n\ni\nX_i\nY_i\n\\ln(X_i)\n\\ln(Y_i)\n\n\n\n\n1\n2\n18.2\n0.6931\n2.9014\n\n\n2\n3\n16.7\n1.0986\n2.8154\n\n\n3\n4\n15.8\n1.3863\n2.7600\n\n\n4\n5\n15.3\n1.6094\n2.7278\n\n\n5\n6\n15.0\n1.7918\n2.7081\n\n\n6\n7\n14.8\n1.9459\n2.6946\n\n\n7\n8\n14.7\n2.0794\n2.6878\n\n\n8\n9\n14.6\n2.1972\n2.6810\n\n\n9\n10\n14.55\n2.3026\n2.6777\n\n\n10\n11\n14.52\n2.3979\n2.6757\n\n\n\n\n\nStep 2: Compare Different Model Specifications\nWe estimate three alternative specifications:\n\nLog-linear model: \\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nLinear-log model: Y_i = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\nLog-log model: \\ln(Y_i) = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\n\n\n# Create transformed variables\ndata$log_DM &lt;- log(data$DM)\ndata$log_GH &lt;- log(data$GH)\n\n# Fit models\nmodel_linear &lt;- lm(GH ~ DM, data = data)\nmodel_loglinear &lt;- lm(log_GH ~ DM, data = data)\nmodel_linearlog &lt;- lm(GH ~ log_DM, data = data)\nmodel_loglog &lt;- lm(log_GH ~ log_DM, data = data)\n\n# Compare R-squared values\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Linear\", \"Log-linear\", \"Linear-log\", \"Log-log\"),\n  R_squared = c(\n    summary(model_linear)$r.squared,\n    summary(model_loglinear)$r.squared,\n    summary(model_linearlog)$r.squared,\n    summary(model_loglog)$r.squared\n  )\n)\n\n# Display comparison\nmodels_comparison\n\n       Model R_squared\n1     Linear 0.7443793\n2 Log-linear 0.7670346\n3 Linear-log 0.9141560\n4    Log-log 0.9288088\n\n\n\n\nStep 3: Visual Comparison\n\n# Create plots for each model\np1 &lt;- ggplot(data, aes(x = DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Linear Model\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Log-linear Model\") +\n  theme_minimal()\n\np3 &lt;- ggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Linear-log Model\") +\n  theme_minimal()\n\np4 &lt;- ggplot(data, aes(x = log_DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Log-log Model\") +\n  theme_minimal()\n\n# Arrange plots in a grid\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Residual Analysis for Best Model\nBased on R-squared values, analyze residuals for the best-fitting model:\n\n# Residual plots for best model\npar(mfrow = c(2, 2))\nplot(model_linearlog)\n\n\n\n\n\n\n\n\n\n\nStep 5: Interpretation of Best Model\nThe linear-log model coefficients:\n\nsummary(model_linearlog)\n\n\nCall:\nlm(formula = GH ~ log_DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40702 -0.30207 -0.04907  0.22905  0.60549 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)  19.0223     0.4079   46.64 0.0000000000494 ***\nlog_DM       -2.0599     0.2232   -9.23 0.0000153880425 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3712 on 8 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9034 \nF-statistic: 85.19 on 1 and 8 DF,  p-value: 0.00001539\n\n\nInterpretation: - \\hat{\\beta_0} represents the expected Gallagher Index when ln(DM) = 0 (i.e., when DM = 1) - \\hat{\\beta_1} represents the change in Gallagher Index associated with a one-unit increase in ln(DM)\n\n\nStep 6: Model Predictions\n\n# Create prediction plot for best model\nggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Linear-log Model: Gallagher Index vs ln(District Magnitude)\",\n    x = \"ln(District Magnitude)\",\n    y = \"Gallagher Index\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nStep 7: Elasticity Analysis\nFor the log-log model, coefficients represent elasticities directly. Calculate average elasticity for the linear-log model:\n\n# Calculate elasticity at means\nmean_DM &lt;- mean(data$DM)\nmean_GH &lt;- mean(data$GH)\nbeta1 &lt;- coef(model_linearlog)[2]\nelasticity &lt;- beta1 * (1/mean_GH)\nelasticity\n\n    log_DM \n-0.1336136 \n\n\nThis represents the percentage change in the Gallagher Index for a 1% change in District Magnitude.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.3-understanding-pearson-spearman-and-kendall",
    "href": "correg_en.html#appendix-a.3-understanding-pearson-spearman-and-kendall",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.32 Appendix A.3: Understanding Pearson, Spearman, and Kendall",
    "text": "9.32 Appendix A.3: Understanding Pearson, Spearman, and Kendall\n\nDataset\n\ndata &lt;- data.frame(\n  x = c(2, 4, 5, 3, 8),\n  y = c(3, 5, 4, 4, 7)\n)\n\n\n\nPearson Correlation\n r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}} \n\nStep-by-Step Calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n2\n3\n-2.4\n-1.6\n3.84\n5.76\n2.56\n\n\n2\n4\n5\n-0.4\n0.4\n-0.16\n0.16\n0.16\n\n\n3\n5\n4\n0.6\n-0.6\n-0.36\n0.36\n0.36\n\n\n4\n3\n4\n-1.4\n-0.6\n0.84\n1.96\n0.36\n\n\n5\n8\n7\n3.6\n2.4\n8.64\n12.96\n5.76\n\n\nSum\n22\n23\n0\n0\n12.8\n21.2\n9.2\n\n\n\n\\bar{x} = 4.4 \\bar{y} = 4.6\n r = \\frac{12.8}{\\sqrt{21.2 \\times 9.2}} = \\frac{12.8}{\\sqrt{195.04}} = \\frac{12.8}{13.97} = 0.92 \n\n\n\nSpearman Correlation\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} \n\nStep-by-Step Calculations:\n\n\n\ni\nx_i\ny_i\nRank x_i\nRank y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n3\n1\n1\n0\n0\n\n\n2\n4\n5\n3\n5\n-2\n4\n\n\n3\n5\n4\n4\n2.5\n1.5\n2.25\n\n\n4\n3\n4\n2\n2.5\n-0.5\n0.25\n\n\n5\n8\n7\n5\n4\n1\n1\n\n\nSum\n\n\n\n\n\n7.5\n\n\n\n \\rho = 1 - \\frac{6(7.5)}{5(25-1)} = 1 - \\frac{45}{120} = 0.82 \n\n\n\nKendall’s Tau\n \\tau = \\frac{\\text{number of concordant pairs} - \\text{number of discordant pairs}}{\\frac{1}{2}n(n-1)} \n\nStep-by-Step Calculations:\n\n\n\nPair (i,j)\nx_i,x_j\ny_i,y_j\nx_j-x_i\ny_j-y_i\nResult\n\n\n\n\n(1,2)\n2,4\n3,5\n+2\n+2\nC\n\n\n(1,3)\n2,5\n3,4\n+3\n+1\nC\n\n\n(1,4)\n2,3\n3,4\n+1\n+1\nC\n\n\n(1,5)\n2,8\n3,7\n+6\n+4\nC\n\n\n(2,3)\n4,5\n5,4\n+1\n-1\nD\n\n\n(2,4)\n4,3\n5,4\n-1\n-1\nC\n\n\n(2,5)\n4,8\n5,7\n+4\n+2\nC\n\n\n(3,4)\n5,3\n4,4\n-2\n0\nD\n\n\n(3,5)\n5,8\n4,7\n+3\n+3\nC\n\n\n(4,5)\n3,8\n4,7\n+5\n+3\nC\n\n\n\nNumber of concordant pairs = 8 Number of discordant pairs = 2  \\tau = \\frac{8-2}{10} = 0.74 \n\n\n\nVerification in R\n\ncat(\"Pearson:\", round(cor(data$x, data$y, method=\"pearson\"), 2), \"\\n\")\n\nPearson: 0.92 \n\ncat(\"Spearman:\", round(cor(data$x, data$y, method=\"spearman\"), 2), \"\\n\")\n\nSpearman: 0.82 \n\ncat(\"Kendall:\", round(cor(data$x, data$y, method=\"kendall\"), 2), \"\\n\")\n\nKendall: 0.74 \n\n\n\n\nInterpretation of Results\n\nPearson Correlation (r = 0.92)\n\nStrong positive linear correlation\nIndicates a very strong linear relationship between variables\n\nSpearman Correlation (ρ = 0.82)\n\nAlso strong positive correlation\nSlightly lower than Pearson’s, suggesting some deviations from monotonicity\n\nKendall’s Tau (τ = 0.74)\n\nLowest of the three values, but still indicates strong association\nMore robust to outliers\n\n\n\n\nComparison of Measures\n\nDifferences in Values:\n\nPearson (0.92) - highest value, strong linearity\nSpearman (0.82) - considers only ranking\nKendall (0.74) - most conservative measure\n\nPractical Application:\n\nAll measures confirm strong positive association\nDifferences between measures indicate slight deviations from perfect linearity\nKendall provides the most conservative estimate of relationship strength\n\n\n\n\nExercises\n\nChange y[3] from 4 to 6 and recalculate all three correlations\nAdd an outlier (x=10, y=2) and recalculate correlations\nCompare which measure is most sensitive to changes in the data\n\n\n\nKey Points to Remember\n\nPearson Correlation:\n\nMeasures linear relationship\nMost sensitive to outliers\nRequires interval or ratio data\n\nSpearman Correlation:\n\nMeasures monotonic relationship\nLess sensitive to outliers\nWorks with ordinal data\n\nKendall’s Tau:\n\nMeasures ordinal association\nMost robust to outliers\nBest for small samples and tied ranks",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-b-bias-in-ols-estimation-with-endogenous-regressors",
    "href": "correg_en.html#appendix-b-bias-in-ols-estimation-with-endogenous-regressors",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.33 Appendix B: Bias in OLS Estimation with Endogenous Regressors",
    "text": "9.33 Appendix B: Bias in OLS Estimation with Endogenous Regressors\nIn this tutorial, we will explore the bias in Ordinary Least Squares (OLS) estimation when the error term is correlated with the explanatory variable, a situation known as endogeneity. We will first derive the bias mathematically and then illustrate it using a simulated dataset in R.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#theoretical-derivation",
    "href": "correg_en.html#theoretical-derivation",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.34 Theoretical Derivation",
    "text": "9.34 Theoretical Derivation\nConsider a data generating process (DGP) where the true relationship between x and y is:\n y = 2x + e \nHowever, there is an endogeneity problem because the error term e is correlated with x in the following way:\n e = 1x + u \nwhere u is an independent error term.\nIf we estimate the simple linear model y = \\hat{\\beta_0} + \\hat{\\beta_1}x + \\varepsilon using OLS, the OLS estimator of \\hat{\\beta_1} will be biased due to the endogeneity issue.\nTo understand the bias, let’s derive the expected value of the OLS estimator \\hat{\\beta}_1:\n\\begin{align*}\nE[\\hat{\\beta}_1] &= E[(X'X)^{-1}X'y] \\\\\n                 &= E[(X'X)^{-1}X'(2x + 1x + u)] \\\\\n                 &= E[(X'X)^{-1}X'(3x + u)] \\\\\n                 &= 3 + E[(X'X)^{-1}X'u]\n\\end{align*}\nIf the error term u is uncorrelated with x, then E[(X'X)^{-1}X'u] = 0, and the OLS estimator would be unbiased: E[\\hat{\\beta}_1] = 3. However, in this case, the original error term e is correlated with x, so u is also likely to be correlated with x.\nAssuming E[(X'X)^{-1}X'u] \\neq 0, the OLS estimator will be biased:\n\\begin{align*}\n\\text{Bias}(\\hat{\\beta}_1) &= E[\\hat{\\beta}_1] - \\beta_{1,\\text{true}} \\\\\n                           &= 3 + E[(X'X)^{-1}X'u] - 2 \\\\\n                           &= 1 + E[(X'X)^{-1}X'u]\n\\end{align*}\nThe direction and magnitude of the bias will depend on the correlation between x and u. If x and u are positively correlated, the bias will be positive, and the OLS estimator will overestimate the true coefficient. Conversely, if x and u are negatively correlated, the bias will be negative, and the OLS estimator will underestimate the true coefficient.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#simulation-in-r",
    "href": "correg_en.html#simulation-in-r",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.35 Simulation in R",
    "text": "9.35 Simulation in R\nLet’s create a simple dataset with 10 observations where x is in the interval 1:10, and generate y values based on the given DGP: y = 2x + e, where e = 1x + u, and u is a random error term.\n\nset.seed(123)  # for reproducibility\nx &lt;- 1:10\nu &lt;- rnorm(10, mean = 0, sd = 1)\ne &lt;- 1*x + u\n# e &lt;- 1*x\ny &lt;- 2*x + e\n\n# Generate the data frame\ndata &lt;- data.frame(x = x, y = y)\n\n# Estimate the OLS model\nmodel &lt;- lm(y ~ x, data = data)\n\n# Print the model summary\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1348 -0.5624 -0.1393  0.3854  1.6814 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)   0.5255     0.6673   0.787         0.454    \nx             2.9180     0.1075  27.134 0.00000000367 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9768 on 8 degrees of freedom\nMultiple R-squared:  0.9893,    Adjusted R-squared:  0.9879 \nF-statistic: 736.3 on 1 and 8 DF,  p-value: 0.000000003666\n\n\nIn this example, the true relationship is y = 2x + e, where e = 1x + u. However, when we estimate the OLS model, we get:\n \\hat{y} = 0.18376 + 3.05874x \nThe estimated coefficient for x is 3.05874, which is biased upward from the true value of 2. This bias is due to the correlation between the error term e and the explanatory variable x.\nTo visualize the bias using ggplot2, we can plot the true relationship (y = 2x) and the estimated OLS relationship:\n\nlibrary(ggplot2)\n\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 2, color = \"blue\", linewidth = 1, linetype = \"dashed\") +\n  geom_abline(intercept = coef(model)[1], slope = coef(model)[2], color = \"red\", linewidth = 1) +\n  labs(title = \"True vs. Estimated Relationship\", x = \"x\", y = \"y\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  scale_color_manual(name = \"Lines\", values = c(\"blue\", \"red\"), \n                     labels = c(\"True\", \"OLS\"))\n\n\n\n\n\n\n\n\nThe plot will show that the estimated OLS line (red) is steeper than the true relationship line (blue), illustrating the upward bias in the estimated coefficient.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#conclusion",
    "href": "correg_en.html#conclusion",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.36 Conclusion",
    "text": "9.36 Conclusion\nIn summary, when the error term is correlated with the explanatory variable (endogeneity), the OLS estimator will be biased. The direction and magnitude of the bias depend on the nature of the correlation between the error term and the explanatory variable. This tutorial demonstrated the bias both mathematically and through a simulated example in R, using ggplot2 for visualization.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-c.-ols---worked-examples",
    "href": "correg_en.html#appendix-c.-ols---worked-examples",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.37 Appendix C. OLS - Worked Examples",
    "text": "9.37 Appendix C. OLS - Worked Examples",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#example-1.-practice-hours-vs.-skill-rating-visual-guide-to-ols-and-r-squared",
    "href": "correg_en.html#example-1.-practice-hours-vs.-skill-rating-visual-guide-to-ols-and-r-squared",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.38 Example 1. Practice Hours vs. Skill Rating: Visual Guide to OLS and R-squared",
    "text": "9.38 Example 1. Practice Hours vs. Skill Rating: Visual Guide to OLS and R-squared\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\n\n# Our simple dataset\npractice &lt;- c(1, 2, 3, 4, 5, 6)\nskill &lt;- c(3, 7, 5, 8, 10, 9)\n\n# Create data frame\ndata &lt;- data.frame(\n  Student = 1:6,\n  Practice = practice,\n  Skill = skill\n)\n\nprint(data)\n\n  Student Practice Skill\n1       1        1     3\n2       2        2     7\n3       3        3     5\n4       4        4     8\n5       5        5    10\n6       6        6     9\n\n\n\nManual Calculations\n\nStep 1: Calculate the Means\nThe mean is the average value, calculated by summing all observations and dividing by the number of observations.\nMean of Practice Hours (X̄):\n\\bar{X} = \\frac{\\sum_{i=1}^{n} X_i}{n} = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5\nMean of Skill Ratings (Ȳ):\n\\bar{Y} = \\frac{\\sum_{i=1}^{n} Y_i}{n} = \\frac{3 + 7 + 5 + 8 + 10 + 9}{6} = \\frac{42}{6} = 7\n\n# Calculate means\nmean_x &lt;- mean(practice)\nmean_y &lt;- mean(skill)\n\ncat(\"Mean Practice Hours:\", mean_x, \"\\n\")\n\nMean Practice Hours: 3.5 \n\ncat(\"Mean Skill Rating:\", mean_y, \"\\n\")\n\nMean Skill Rating: 7 \n\n\n\n\nStep 2: Calculate Variance and Standard Deviation\nVariance measures how spread out the data is from the mean. We use the sample variance formula (dividing by n-1).\nVariance of X (Practice Hours):\nFirst, calculate deviations from the mean (X_i - \\bar{X}):\n\n\n\nStudent\nX_i\nX_i - \\bar{X}\n(X_i - \\bar{X})^2\n\n\n\n\n1\n1\n1 - 3.5 = -2.5\n(-2.5)^2 = 6.25\n\n\n2\n2\n2 - 3.5 = -1.5\n(-1.5)^2 = 2.25\n\n\n3\n3\n3 - 3.5 = -0.5\n(-0.5)^2 = 0.25\n\n\n4\n4\n4 - 3.5 = 0.5\n(0.5)^2 = 0.25\n\n\n5\n5\n5 - 3.5 = 1.5\n(1.5)^2 = 2.25\n\n\n6\n6\n6 - 3.5 = 2.5\n(2.5)^2 = 6.25\n\n\nSum\n\n\n17.5\n\n\n\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1} = \\frac{17.5}{5} = 3.5\ns_X = \\sqrt{3.5} = 1.871\nVariance of Y (Skill Ratings):\n\n\n\nStudent\nY_i\nY_i - \\bar{Y}\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n3\n3 - 7 = -4\n(-4)^2 = 16\n\n\n2\n7\n7 - 7 = 0\n(0)^2 = 0\n\n\n3\n5\n5 - 7 = -2\n(-2)^2 = 4\n\n\n4\n8\n8 - 7 = 1\n(1)^2 = 1\n\n\n5\n10\n10 - 7 = 3\n(3)^2 = 9\n\n\n6\n9\n9 - 7 = 2\n(2)^2 = 4\n\n\nSum\n\n\n34\n\n\n\ns^2_Y = \\frac{\\sum(Y_i - \\bar{Y})^2}{n-1} = \\frac{34}{5} = 6.8\ns_Y = \\sqrt{6.8} = 2.608\n\n# Verify variance and standard deviation\ncat(\"Variance of Practice:\", var(practice), \"\\n\")\n\nVariance of Practice: 3.5 \n\ncat(\"SD of Practice:\", sd(practice), \"\\n\")\n\nSD of Practice: 1.870829 \n\ncat(\"Variance of Skill:\", var(skill), \"\\n\")\n\nVariance of Skill: 6.8 \n\ncat(\"SD of Skill:\", sd(skill), \"\\n\")\n\nSD of Skill: 2.607681 \n\n\n\n\nStep 3: Calculate Covariance\nCovariance measures how two variables vary together. A positive covariance indicates that as one variable increases, the other tends to increase.\ns_{XY} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nLet’s calculate the products for each observation:\n\n\n\n\n\n\n\n\n\nStudent\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n\n\n\n\n1\n-2.5\n-4\n(-2.5) \\times (-4) = 10.0\n\n\n2\n-1.5\n0\n(-1.5) \\times (0) = 0.0\n\n\n3\n-0.5\n-2\n(-0.5) \\times (-2) = 1.0\n\n\n4\n0.5\n1\n(0.5) \\times (1) = 0.5\n\n\n5\n1.5\n3\n(1.5) \\times (3) = 4.5\n\n\n6\n2.5\n2\n(2.5) \\times (2) = 5.0\n\n\nSum\n\n\n21.0\n\n\n\ns_{XY} = \\frac{21.0}{5} = 4.2\n\n# Verify covariance\ncat(\"Covariance:\", cov(practice, skill), \"\\n\")\n\nCovariance: 4.2 \n\n\n\n\nStep 4: Calculate Pearson Correlation Coefficient\nThe correlation coefficient standardizes the covariance to a scale from -1 to +1:\nr = \\frac{s_{XY}}{s_X \\cdot s_Y} = \\frac{4.2}{1.871 \\times 2.608} = \\frac{4.2}{4.879} = 0.861\nThis gives us a correlation of 0.861, indicating a strong positive relationship between practice hours and skill rating.\n\n# Verify correlation\ncat(\"Correlation:\", cor(practice, skill), \"\\n\")\n\nCorrelation: 0.8609161 \n\n\n\n\nStep 5: Calculate OLS Regression Coefficients\nThe Ordinary Least Squares (OLS) method finds the values of \\beta_0 and \\beta_1 that minimize the sum of squared errors.\nSlope estimator:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\nUsing our calculated values:\n\\hat{\\beta_1} = \\frac{4.2}{3.5} = 1.2\nIntercept estimator:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 7 - (1.2 \\times 3.5) = 7 - 4.2 = 2.8\nOur regression equation is:\n\\hat{Y} = 2.8 + 1.2X\nThis means:\n\nWhen practice hours = 0, predicted skill = 2.8\nFor each 1-hour increase in practice, skill rating increases by 1.2 points\n\n\n# Fit the model\nmodel &lt;- lm(skill ~ practice)\ncoef_model &lt;- coef(model)\n\ncat(\"Intercept (β₀):\", coef_model[1], \"\\n\")\n\nIntercept (β₀): 2.8 \n\ncat(\"Slope (β₁):\", coef_model[2], \"\\n\")\n\nSlope (β₁): 1.2 \n\n# Get predictions\ndata$predicted &lt;- predict(model)\ndata$residual &lt;- residuals(model)\n\n\n\nStep 6: Calculate Predicted Values and Residuals\nUsing \\hat{Y} = 2.8 + 1.2X:\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 2.8 + 1.2X_i\nResidual (Y_i - \\hat{Y}_i)\n\n\n\n\n1\n1\n3\n2.8 + 1.2(1) = 4.0\n3 - 4.0 = -1.0\n\n\n2\n2\n7\n2.8 + 1.2(2) = 5.2\n7 - 5.2 = 1.8\n\n\n3\n3\n5\n2.8 + 1.2(3) = 6.4\n5 - 6.4 = -1.4\n\n\n4\n4\n8\n2.8 + 1.2(4) = 7.6\n8 - 7.6 = 0.4\n\n\n5\n5\n10\n2.8 + 1.2(5) = 8.8\n10 - 8.8 = 1.2\n\n\n6\n6\n9\n2.8 + 1.2(6) = 10.0\n9 - 10.0 = -1.0\n\n\n\n\n\nStep 7: Calculate Sum of Squares\nSST (Total Sum of Squares) - Total variation in Y:\nSST = \\sum(Y_i - \\bar{Y})^2\nFrom our earlier variance calculation:\nSST = (n-1) \\times s^2_Y = 5 \\times 6.8 = 34\nSSE (Sum of Squared Errors) - Unexplained variation:\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n\n\n\n\n\n\n\n\n\n\nStudent\nY_i\n\\hat{Y}_i\n(Y_i - \\hat{Y}_i)\n(Y_i - \\hat{Y}_i)^2\n\n\n\n\n1\n3\n4.0\n-1.0\n1.00\n\n\n2\n7\n5.2\n1.8\n3.24\n\n\n3\n5\n6.4\n-1.4\n1.96\n\n\n4\n8\n7.6\n0.4\n0.16\n\n\n5\n10\n8.8\n1.2\n1.44\n\n\n6\n9\n10.0\n-1.0\n1.00\n\n\nSum\n\n\n\n8.80\n\n\n\nSSE = 8.80\nSSR (Sum of Squares Regression) - Explained variation:\nSSR = SST - SSE = 34 - 8.80 = 25.20\n\n\nStep 8: Calculate R-squared\nR-squared tells us what proportion of the total variation in Y is explained by our model:\nR^2 = \\frac{SSR}{SST} = \\frac{25.20}{34} = 0.741\nAlternative formula:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{8.80}{34} = 1 - 0.259 = 0.741\nOr simply:\nR^2 = r^2 = (0.861)^2 = 0.741\nThis means our model explains 74.1% of the variation in skill ratings!\n\n# Verify sum of squares and R-squared\nSST &lt;- sum((skill - mean_y)^2)\nSSR &lt;- sum((data$predicted - mean_y)^2)\nSSE &lt;- sum(data$residual^2)\nr_squared &lt;- SSR / SST\n\ncat(\"SST (Total):\", SST, \"\\n\")\n\nSST (Total): 34 \n\ncat(\"SSR (Explained):\", SSR, \"\\n\")\n\nSSR (Explained): 25.2 \n\ncat(\"SSE (Unexplained):\", SSE, \"\\n\")\n\nSSE (Unexplained): 8.8 \n\ncat(\"R-squared:\", r_squared, \"\\n\")\n\nR-squared: 0.7411765 \n\ncat(\"R-squared (from correlation):\", cor(practice, skill)^2, \"\\n\")\n\nR-squared (from correlation): 0.7411765 \n\n\n\n\n\nVisualization 1: The OLS Best-Fit Line\nThis plot shows how OLS minimizes the sum of squared residuals (vertical distances from points to the line).\n\nggplot(data, aes(x = Practice, y = Skill)) +\n  geom_hline(yintercept = mean_y, linetype = \"dashed\", \n             color = \"gray50\", linewidth = 0.8, alpha = 0.7) +\n  geom_segment(aes(xend = Practice, yend = predicted), \n               color = \"red\", linetype = \"dotted\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1.2) +\n  geom_point(size = 5, color = \"darkblue\") +\n  geom_point(aes(y = predicted), size = 3, color = \"blue\", shape = 17) +\n  annotate(\"text\", x = 1.5, y = mean_y + 0.3, \n           label = paste0(\"Mean (ȳ = \", mean_y, \")\"), \n           color = \"gray30\", size = 4) +\n  annotate(\"text\", x = 5, y = 4, \n           label = \"Residuals (errors)\\nOLS minimizes Σ(residuals²)\", \n           color = \"red\", size = 3.5, hjust = 0) +\n  annotate(\"text\", x = 2, y = 10.5, \n           label = paste0(\"ŷ = \", round(coef_model[1], 1), \n                         \" + \", round(coef_model[2], 1), \"x\"), \n           color = \"blue\", size = 5, fontface = \"bold\") +\n  labs(\n    title = \"OLS Regression: Minimizing Squared Residuals\",\n    subtitle = \"Blue triangles are predicted values; red lines show residuals\",\n    x = \"Practice Hours\",\n    y = \"Skill Rating\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKey Insight: OLS finds the unique line that makes the sum of squared red distances as small as possible!\n\n\nVisualization 2: Variance Decomposition (SST = SSR + SSE)\nThis shows how total variation is split into explained and unexplained components.\n\n# Create the decomposition plot\nggplot(data, aes(x = Practice)) +\n  # Total deviation (SST)\n  geom_segment(aes(y = mean_y, yend = Skill, xend = Practice), \n               color = \"purple\", linewidth = 1.2, alpha = 0.6,\n               arrow = arrow(length = unit(0.15, \"inches\"))) +\n  \n  # Explained deviation (SSR)\n  geom_segment(aes(y = mean_y, yend = predicted, xend = Practice), \n               color = \"green\", linewidth = 1.5, alpha = 0.8) +\n  \n  # Unexplained deviation (SSE)\n  geom_segment(aes(y = predicted, yend = Skill, xend = Practice), \n               color = \"red\", linewidth = 1.2, alpha = 0.8,\n               linetype = \"dashed\") +\n  \n  # Mean line\n  geom_hline(yintercept = mean_y, linetype = \"solid\", \n             color = \"gray40\", linewidth = 1) +\n  \n  # Regression line\n  geom_smooth(aes(y = Skill), method = \"lm\", se = FALSE, \n              color = \"blue\", linewidth = 1) +\n  \n  # Points\n  geom_point(aes(y = Skill), size = 5, color = \"darkblue\") +\n  geom_point(aes(y = predicted), size = 3, color = \"blue\", shape = 15) +\n  \n  # Annotations\n  annotate(\"text\", x = 6.5, y = mean_y, \n           label = \"Mean\", color = \"gray40\", size = 4, hjust = 0) +\n  annotate(\"text\", x = 6.5, y = 9.5, \n           label = \"Regression Line\", color = \"blue\", size = 4, hjust = 0) +\n  \n  # Legend\n  annotate(\"segment\", x = 0.5, xend = 0.5, y = 2, yend = 3.5,\n           color = \"purple\", linewidth = 1.2, \n           arrow = arrow(length = unit(0.12, \"inches\"))) +\n  annotate(\"text\", x = 0.7, y = 2.75, \n           label = \"Total (SST)\", color = \"purple\", \n           size = 3.5, hjust = 0, fontface = \"bold\") +\n  \n  annotate(\"segment\", x = 0.5, xend = 0.5, y = 4.5, yend = 5.5,\n           color = \"green\", linewidth = 1.5) +\n  annotate(\"text\", x = 0.7, y = 5, \n           label = \"Explained (SSR)\", color = \"green\", \n           size = 3.5, hjust = 0, fontface = \"bold\") +\n  \n  annotate(\"segment\", x = 0.5, xend = 0.5, y = 6.5, yend = 7.5,\n           color = \"red\", linewidth = 1.2, linetype = \"dashed\") +\n  annotate(\"text\", x = 0.7, y = 7, \n           label = \"Unexplained (SSE)\", color = \"red\", \n           size = 3.5, hjust = 0, fontface = \"bold\") +\n  \n  labs(\n    title = \"Variance Decomposition: SST = SSR + SSE\",\n    subtitle = \"Purple = total deviation | Green = explained by model | Red = residual error\",\n    x = \"Practice Hours\",\n    y = \"Skill Rating\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 14)) +\n  coord_cartesian(xlim = c(0.3, 7.5), ylim = c(2, 11))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nMathematical Decomposition:\nFor each observation: (Y_i - \\bar{Y})^2 = (\\hat{Y}_i - \\bar{Y})^2 + (Y_i - \\hat{Y}_i)^2\n\nPurple arrows: Total deviation from mean = (Y_i - \\bar{Y})\nGreen bars: Model’s explanation = (\\hat{Y}_i - \\bar{Y})\nRed dashed: What’s left over = (Y_i - \\hat{Y}_i)\n\n\n\nVisualization 3: R-squared as a Proportion\n\n# Calculate sum of squares\nSST &lt;- sum((skill - mean_y)^2)\nSSR &lt;- sum((data$predicted - mean_y)^2)\nSSE &lt;- sum(data$residual^2)\nr_squared &lt;- SSR / SST\n\n# Create bar chart data\nss_data &lt;- data.frame(\n  Component = c(\"Total (SST)\", \"Explained (SSR)\", \"Unexplained (SSE)\"),\n  Value = c(SST, SSR, SSE),\n  Color = c(\"purple\", \"green\", \"red\")\n)\n\np1 &lt;- ggplot(ss_data, aes(x = Component, y = Value, fill = Component)) +\n  geom_bar(stat = \"identity\", alpha = 0.7, color = \"black\") +\n  geom_text(aes(label = round(Value, 2)), vjust = -0.5, size = 5, fontface = \"bold\") +\n  scale_fill_manual(values = c(\"green\", \"red\", \"purple\")) +\n  labs(\n    title = \"Sum of Squares Breakdown\",\n    y = \"Sum of Squares\",\n    x = \"\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"))\n\n# Create proportion visualization\nprop_data &lt;- data.frame(\n  Component = c(\"Explained\\n(SSR)\", \"Unexplained\\n(SSE)\"),\n  Proportion = c(SSR/SST, SSE/SST),\n  Percentage = c(SSR/SST * 100, SSE/SST * 100)\n)\n\np2 &lt;- ggplot(prop_data, aes(x = \"\", y = Proportion, fill = Component)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\", linewidth = 2) +\n  coord_polar(theta = \"y\") +\n  scale_fill_manual(values = c(\"green3\", \"red3\")) +\n  geom_text(aes(label = paste0(round(Percentage, 1), \"%\")), \n            position = position_stack(vjust = 0.5), \n            size = 6, fontface = \"bold\", color = \"white\") +\n  labs(title = paste0(\"R² = \", round(r_squared, 3))) +\n  theme_void(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n        legend.position = \"bottom\",\n        legend.title = element_blank())\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\nR-squared Formulas:\nR^2 = \\frac{SSR}{SST} = \\frac{25.20}{34} = 0.74\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{8.80}{34} = 0.74\nInterpretation: Our model explains 74% of the variation in skill ratings!\n\n\nVisualization 4: R² as Correlation Between Deviations\nThis shows the geometric interpretation of R²: how well predicted deviations from the mean match actual deviations.\n\n# Calculate deviations\ndata$actual_dev &lt;- skill - mean_y\ndata$predicted_dev &lt;- data$predicted - mean_y\n\n# Side-by-side comparison\np1 &lt;- ggplot(data, aes(x = Practice)) +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.8) +\n  geom_segment(aes(y = 0, yend = actual_dev, xend = Practice), \n               color = \"purple\", linewidth = 2, alpha = 0.7,\n               arrow = arrow(length = unit(0.15, \"inches\"))) +\n  geom_point(aes(y = actual_dev), size = 4, color = \"purple\") +\n  labs(\n    title = \"Actual Deviations from Mean\",\n    subtitle = expression(Y[i] - bar(Y)),\n    x = \"Practice Hours\",\n    y = \"Deviation from Mean\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(data, aes(x = Practice)) +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.8) +\n  geom_segment(aes(y = 0, yend = predicted_dev, xend = Practice), \n               color = \"green\", linewidth = 2, alpha = 0.7,\n               arrow = arrow(length = unit(0.15, \"inches\"))) +\n  geom_point(aes(y = predicted_dev), size = 4, color = \"green\") +\n  labs(\n    title = \"Predicted Deviations from Mean\",\n    subtitle = expression(hat(Y)[i] - bar(Y)),\n    x = \"Practice Hours\",\n    y = \"Deviation from Mean\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(plot.title = element_text(face = \"bold\"))\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nVisualization 5: Scatterplot of Deviations (R² Interpretation)\n\nggplot(data, aes(x = predicted_dev, y = actual_dev)) +\n  geom_abline(slope = 1, intercept = 0, \n              linetype = \"dashed\", color = \"gray50\", linewidth = 1) +\n  geom_point(size = 5, color = \"darkblue\", alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\", \n              fill = \"lightblue\", alpha = 0.3) +\n  annotate(\"text\", x = -3, y = 2.5, \n           label = paste0(\"If perfect fit:\\nall points on this line\\n(R² = 1)\"), \n           color = \"gray40\", size = 3.5, hjust = 0) +\n  annotate(\"text\", x = 1.5, y = -3, \n           label = paste0(\"Correlation = \", round(sqrt(r_squared), 3), \n                         \"\\nR² = \", round(r_squared, 3)), \n           color = \"blue\", size = 5, fontface = \"bold\") +\n  labs(\n    title = \"R² Measures How Well Predicted Deviations Match Actual Deviations\",\n    subtitle = \"Each point compares model's prediction to reality (both relative to mean)\",\n    x = expression(paste(\"Predicted Deviation: \", hat(Y)[i] - bar(Y))),\n    y = expression(paste(\"Actual Deviation: \", Y[i] - bar(Y)))\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 13))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKey Insight:\nR² is literally the square of the correlation between:\n\nWhat the model predicts (relative to mean)\nWhat actually happened (relative to mean)\n\nIf R^2 = 1: all points fall exactly on the diagonal (perfect predictions)\nIf R^2 = 0: no relationship between predicted and actual deviations\n\n\nSummary Table\n\n# Create summary table\nsummary_stats &lt;- data.frame(\n  Statistic = c(\"Sample Size (n)\", \n                \"Mean Practice (X̄)\", \n                \"Mean Skill (Ȳ)\",\n                \"Correlation (r)\",\n                \"Intercept (β₀)\",\n                \"Slope (β₁)\",\n                \"R-squared\",\n                \"SST (Total)\",\n                \"SSR (Explained)\",\n                \"SSE (Unexplained)\"),\n  Value = c(6,\n            round(mean_x, 2),\n            round(mean_y, 2),\n            round(cor(practice, skill), 3),\n            round(coef_model[1], 2),\n            round(coef_model[2], 2),\n            round(r_squared, 3),\n            round(SST, 2),\n            round(SSR, 2),\n            round(SSE, 2))\n)\n\nknitr::kable(summary_stats, align = c(\"l\", \"r\"))\n\n\n\n\nStatistic\nValue\n\n\n\n\nSample Size (n)\n6.000\n\n\nMean Practice (X̄)\n3.500\n\n\nMean Skill (Ȳ)\n7.000\n\n\nCorrelation (r)\n0.861\n\n\nIntercept (β₀)\n2.800\n\n\nSlope (β₁)\n1.200\n\n\nR-squared\n0.741\n\n\nSST (Total)\n34.000\n\n\nSSR (Explained)\n25.200\n\n\nSSE (Unexplained)\n8.800\n\n\n\n\n\n\n\nKey Takeaways\nOLS Regression:\n\nFinds the line that minimizes the sum of squared residuals\nProduces unbiased estimates with minimum variance (BLUE)\n\nR-squared (0.74) means:\n\n74% of variation in skill is explained by practice hours\nThe correlation between predicted and actual deviations is \\sqrt{0.74} = 0.86\nSSR is 74% of SST; SSE is 26% of SST\n\nGeometric Interpretation:\n\nTotal variation = distance of each point from the mean\nModel captures 74% of these distances through the regression line\nRemaining 26% is unexplained (residuals)\n\nPractical Implication:\nEach additional hour of practice increases expected skill by 1.2 points, and this relationship explains most (but not all) of the variation we observe!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#example-2.-income-and-voter-turnout",
    "href": "correg_en.html#example-2.-income-and-voter-turnout",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.39 Example 2. Income and Voter Turnout",
    "text": "9.39 Example 2. Income and Voter Turnout\nBackground\nIn preparation for upcoming municipal elections, the Electoral Commission of a mid-sized European city conducted research on voter participation patterns across different city districts. A key question emerged:\nDoes economic prosperity of a district correlate with civic engagement, specifically voter turnout?\nData Collection\nSample: 5 representative districts\nTime Period: Data from recent municipal elections\nVariables:\n\nIncome: Average annual household income per capita (thousands €)\nTurnout: Percentage of registered voters who voted in the election\n\n\nInitial R Output for Reference\n\n# Data\nincome &lt;- c(50, 45, 56, 40, 60)  # thousands €\nturnout &lt;- c(60, 56, 70, 50, 75) # %\n\n# Full model check\nmodel &lt;- lm(turnout ~ income)\nsummary(model)\n\n\nCall:\nlm(formula = turnout ~ income)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \nincome       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n\n\n\nDispersion Measures\nMeans:\n\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n} = \\frac{50 + 45 + 56 + 40 + 60}{5} = \\frac{251}{5} = 50.2\n\\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n} = \\frac{60 + 56 + 70 + 50 + 75}{5} = \\frac{311}{5} = 62.2\n\n# Verification\nmean(income)  # 50.2\n\n[1] 50.2\n\nmean(turnout) # 62.2\n\n[1] 62.2\n\n\nVariances:\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X: (-0.2, -5.2, 5.8, -10.2, 9.8)\ns^2_X = \\frac{0.04 + 27.04 + 33.64 + 104.04 + 96.04}{4} = \\frac{260.8}{4} = 65.2\nDeviations for Y: (-2.2, -6.2, 7.8, -12.2, 12.8)\ns^2_Y = \\frac{4.84 + 38.44 + 60.84 + 148.84 + 163.84}{4} = \\frac{416.8}{4} = 104.2\n\n# Verification\nvar(income)  # 65.2\n\n[1] 65.2\n\nvar(turnout) # 104.2\n\n[1] 104.2\n\n\n\n\nCovariance and Correlation\nCovariance:\ns_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-0.2 \\times -2.2) = 0.44 (-5.2 \\times -6.2) = 32.24 (5.8 \\times 7.8) = 45.24 (-10.2 \\times -12.2) = 124.44 (9.8 \\times 12.8) = 125.44\ns_{XY} = \\frac{327.8}{4} = 81.95\n\n# Verification\ncov(income, turnout) # 81.95\n\n[1] 81.95\n\n\nCorrelation:\nr_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{81.95}{\\sqrt{65.2}\\sqrt{104.2}} = 0.994\n\n# Verification\ncor(income, turnout) # 0.994\n\n[1] 0.9942402\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{81.95}{65.2} = 1.2571429\nIntercept:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\nStep by step:\n\n1.2571429 \\times 50.2 = 63.1085714\n\\hat{\\beta_0} = 62.2 - 63.1085714 = -0.9085714\n\n\n# Verification\ncoef(model)  # Exact coefficients from R\n\n(Intercept)      income \n -0.8964724   1.2569018 \n\n\n\n\nDetailed Decomposition of Variance and R-squared\nStep 1: Calculate predicted values (\\hat{Y}):\n \\hat{Y} = -0.9085714 + 1.2571429X\nThe predicted values \\hat{Y} for each X value:\nFor X = 50:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (50)\n \\hat{Y} = -0.9085714 + 62.857145 \\hat{Y} = 61.9485736\nFor X = 45:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (45)\n \\hat{Y} = -0.9085714 + 56.5714305 \\hat{Y} = 55.6535591\nFor X = 56:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (56)\n \\hat{Y} = -0.9085714 + 70.4200024 \\hat{Y} = 69.5114310\nFor X = 40:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (40)\n \\hat{Y} = -0.9085714 + 50.2657160 \\hat{Y} = 49.3571446\nFor X = 60:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (60)\n \\hat{Y} = -0.9085714 + 75.4285740 \\hat{Y} = 74.5200026\n\n# Verification of predicted values\ny_hat &lt;- -0.9085714 + 1.2571429 * income\ndata.frame(\n  X = income,\n  Y = turnout,\n  Y_hat = y_hat,\n  row.names = 1:5\n)\n\n   X  Y    Y_hat\n1 50 60 61.94857\n2 45 56 55.66286\n3 56 70 69.49143\n4 40 50 49.37714\n5 60 75 74.52000\n\n\nStep 2: Calculate SST (Total Sum of Squares)\nSST = \\sum(Y_i - \\bar{Y})^2 \\text{ where } \\bar{Y} = 62.2\n(60 - 62.2)^2 = (-2.2)^2 = 4.84 (56 - 62.2)^2 = (-6.2)^2 = 38.44 (70 - 62.2)^2 = (7.8)^2 = 60.84 (50 - 62.2)^2 = (-12.2)^2 = 148.84 (75 - 62.2)^2 = (12.8)^2 = 163.84\nSST = 4.84 + 38.44 + 60.84 + 148.84 + 163.84 = 416.8\nStep 3: Calculate SSR (Regression Sum of Squares)\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\n(61.9485736 - 62.2)^2 = (-0.2514264)^2 = 0.0632151 (55.6535591 - 62.2)^2 = (-6.5464409)^2 = 42.8558689 (69.5114310 - 62.2)^2 = (7.3114310)^2 = 53.4570178 (49.3571446 - 62.2)^2 = (-12.8428554)^2 = 164.9389370 (74.5200026 - 62.2)^2 = (12.3200026)^2 = 151.7824640\nSSR = 413.0975028\nStep 4: Calculate SSE (Error Sum of Squares)\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n(60 - 61.9485736)^2 = (-1.9485736)^2 = 3.7969384 (56 - 55.6535591)^2 = (0.3464409)^2 = 0.1200212 (70 - 69.5114310)^2 = (0.4885690)^2 = 0.2387198 (50 - 49.3571446)^2 = (0.6428554)^2 = 0.4132631 (75 - 74.5200026)^2 = (0.4799974)^2 = 0.2303975\nSSE = 4.7024972\nStep 5: Verify decomposition\nSST = SSR + SSE 416.8 = 413.0975028 + 4.7024972\nStep 6: Calculate R-squared\nR^2 = \\frac{SSR}{SST} = \\frac{413.0975028}{416.8} = 0.9916\n\n# Verification\nsummary(model)$r.squared  # Should match our calculation\n\n[1] 0.9885135\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\ndf &lt;- data.frame(income = income, turnout = turnout)\n\nggplot(df, aes(x = income, y = turnout)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Voter Turnout vs Income per Capita\",\n    x = \"Income per Capita (thousands €)\",\n    y = \"Voter Turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    text = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\nInterpretation\nThe analysis shows:\n\nA very strong positive correlation (r = 0.994) between income and voter turnout\nThe regression equation \\hat{Y} = -0.9085714 + 1.2571429X indicates that:\n\nFor each €1,000 increase in income, turnout increases by about 1.26 percentage points\nThe intercept (-0.9086) has little practical meaning as income is never zero\n\nThe R-squared of 0.9916 indicates that 99.16% of the variance in turnout is explained by income",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#example-3.-anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "href": "correg_en.html#example-3.-anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.40 Example 3. Anxiety Levels and Cognitive Performance: A Laboratory Study",
    "text": "9.40 Example 3. Anxiety Levels and Cognitive Performance: A Laboratory Study\n\nData and Context\nIn a psychology experiment, researchers measured the relationship between anxiety levels (measured by galvanic skin response, GSR) and cognitive performance (score on a working memory task).\n\n# Data\nanxiety &lt;- c(2.1, 3.4, 4.2, 5.1, 5.8, 6.4, 7.2, 8.0)  # GSR readings\nperformance &lt;- c(92, 88, 84, 78, 74, 70, 65, 62)      # Working memory scores\n\n# Initial model check\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8993 -0.6660  0.2162  0.6106  1.5262 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 105.3248     1.3189   79.86 0.00000000026 ***\nanxiety      -5.4407     0.2359  -23.06 0.00000043549 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 6 degrees of freedom\nMultiple R-squared:  0.9888,    Adjusted R-squared:  0.987 \nF-statistic: 531.9 on 1 and 6 DF,  p-value: 0.0000004355\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{2.1 + 3.4 + 4.2 + 5.1 + 5.8 + 6.4 + 7.2 + 8.0}{8} = \\frac{42.2}{8} = 5.275\n\\bar{Y} = \\frac{92 + 88 + 84 + 78 + 74 + 70 + 65 + 62}{8} = \\frac{613}{8} = 76.625\n\n# Verification\nmean(anxiety)\n\n[1] 5.275\n\nmean(performance)\n\n[1] 76.625\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(2.1 - 5.275) = -3.175\n(3.4 - 5.275) = -1.875\n(4.2 - 5.275) = -1.075\n(5.1 - 5.275) = -0.175\n(5.8 - 5.275) = 0.525\n(6.4 - 5.275) = 1.125\n(7.2 - 5.275) = 1.925\n(8.0 - 5.275) = 2.725\n\nSquared deviations:\n10.08063 + 3.51563 + 1.15563 + 0.03063 + 0.27563 + 1.26563 + 3.70563 +\n7.42563 = 27.45500\ns^2_X = \\frac{27.45500}{7} = 3.922143\nSimilarly for Y: Deviations:\n15.375, 11.375, 7.375, 1.375, -2.625, -6.625, -11.625, -14.625\ns^2_Y = \\frac{236.875 + 129.391 + 54.391 + 1.891 + 6.891 + 43.891 + 135.141 + 213.891}{7} = \\frac{822.362}{7} = 117.4803\n\n# Verification\nvar(anxiety)\n\n[1] 3.922143\n\nvar(performance)\n\n[1] 117.4107\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-3.175 × 15.375) = -48.815625\n(-1.875 × 11.375) = -21.328125\n(-1.075 × 7.375) = -7.928125\n(-0.175 × 1.375) = -0.240625\n(0.525 × -2.625) = -1.378125\n(1.125 × -6.625) = -7.453125\n(1.925 × -11.625) = -22.378125\n(2.725 × -14.625) = -39.853125\nSum = -149.375\ns_{XY} = \\frac{-149.375}{7} = -21.33929\n\n# Verification\ncov(anxiety, performance)\n\n[1] -21.33929\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-21.33929}{\\sqrt{3.922143}\\sqrt{117.4803}} = -0.9932\n\n# Verification\ncor(anxiety, performance)\n\n[1] -0.9944073\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-21.33929}{3.922143} = -5.4407\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-5.4407 × 5.275 = -28.6997\n\\hat{\\beta_0} = 76.625 - (-28.6997) = 105.3247\n\n\n# Verification\ncoef(model)\n\n(Intercept)     anxiety \n 105.324804   -5.440721 \n\n\n\n\n4. R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}): \\hat{Y} = 105.3247 - 5.4407X\n\n# Predicted values\ny_hat &lt;- 105.3247 - 5.4407 * anxiety\ndata.frame(\n  Anxiety = anxiety,\n  Performance = performance,\n  Predicted = y_hat,\n  row.names = 1:8\n)\n\n  Anxiety Performance Predicted\n1     2.1          92  93.89923\n2     3.4          88  86.82632\n3     4.2          84  82.47376\n4     5.1          78  77.57713\n5     5.8          74  73.76864\n6     6.4          70  70.50422\n7     7.2          65  66.15166\n8     8.0          62  61.79910\n\n\nStep 2: Sum of Squares\nSST = \\sum(Y_i - \\bar{Y})^2 = 822.362\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 816.3094\nSSE = \\sum(Y_i - \\hat{Y}_i)^2 = 6.0526\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{816.3094}{822.362} = 0.9926\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.9888459\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\n\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Cognitive Performance vs. Anxiety Levels\",\n    x = \"Anxiety (GSR)\",\n    y = \"Performance Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.993) between anxiety and cognitive performance\nFor each unit increase in GSR (anxiety), performance decreases by approximately 5.44 points\nThe model explains 99.26% of the variance in performance scores\nThe relationship appears to be strongly linear, suggesting a reliable anxiety-performance relationship\nThe high intercept (105.32) represents the theoretical maximum performance at zero anxiety\n\n\n\nStudy Limitations\n\nSmall sample size (n=8)\nPossible other confounding variables\nLimited range of anxiety levels\nCross-sectional rather than longitudinal data",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#example-4.-anxiety-vs.-performance-correlation-and-regression-analysis",
    "href": "correg_en.html#example-4.-anxiety-vs.-performance-correlation-and-regression-analysis",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.41 Example 4. Anxiety vs. Performance: Correlation and Regression Analysis",
    "text": "9.41 Example 4. Anxiety vs. Performance: Correlation and Regression Analysis\nIn this tutorial, we’ll explore the relationship between test anxiety levels and exam performance among university students. Research suggests that while a small amount of anxiety can be motivating, excessive anxiety typically impairs performance through reduced concentration, working memory interference, and physical symptoms (Yerkes-Dodson law). We’ll analyze data from 8 students to understand this relationship mathematically.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#data-presentation",
    "href": "correg_en.html#data-presentation",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.42 Data Presentation",
    "text": "9.42 Data Presentation\n\nThe Dataset\nWe collected data from 8 students, measuring:\n\nX: Test anxiety score (1-10 scale, where 1 = very low, 10 = very high)\nY: Exam performance (percentage score)\n\n\n# Our data\nanxiety &lt;- c(2.5, 3.2, 4.1, 4.8, 5.6, 6.3, 7.0, 7.9)  # Anxiety scores\nperformance &lt;- c(80, 85, 78, 82, 77, 74, 68, 72)      # Exam scores (%)\n\n# Create a data frame for easy viewing\ndata &lt;- data.frame(\n  Student = 1:8,\n  Anxiety = anxiety,\n  Performance = performance\n)\nprint(data)\n\n  Student Anxiety Performance\n1       1     2.5          80\n2       2     3.2          85\n3       3     4.1          78\n4       4     4.8          82\n5       5     5.6          77\n6       6     6.3          74\n7       7     7.0          68\n8       8     7.9          72\n\n\n\n\nInitial Visualization\nLet’s first visualize our data to get an intuitive sense of the relationship:\n\nlibrary(ggplot2)\n\n# Create scatterplot\nggplot(data, aes(x = Anxiety, y = Performance)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  labs(\n    title = \"Test Anxiety vs. Exam Performance\",\n    x = \"Test Anxiety Score\",\n    y = \"Exam Performance (%)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(2, 8, 1)) +\n  scale_y_continuous(breaks = seq(65, 90, 5))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-statistics",
    "href": "correg_en.html#summary-statistics",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.43 Summary Statistics",
    "text": "9.43 Summary Statistics\n\nCalculating the Means\nThe mean is the average value, calculated by summing all observations and dividing by the number of observations.\nMean of Anxiety Scores (X): \\bar{X} = \\frac{\\sum_{i=1}^{n} X_i}{n} = \\frac{2.5 + 3.2 + 4.1 + 4.8 + 5.6 + 6.3 + 7.0 + 7.9}{8}\n\\bar{X} = \\frac{41.4}{8} = 5.175\nMean of Performance Scores (Y): \\bar{Y} = \\frac{\\sum_{i=1}^{n} Y_i}{n} = \\frac{80 + 85 + 78 + 82 + 77 + 74 + 68 + 72}{8}\n\\bar{Y} = \\frac{616}{8} = 77\n\n# Verify our calculations\nmean_x &lt;- mean(anxiety)\nmean_y &lt;- mean(performance)\ncat(\"Mean Anxiety:\", mean_x, \"\\n\")\n\nMean Anxiety: 5.175 \n\ncat(\"Mean Performance:\", mean_y, \"\\n\")\n\nMean Performance: 77 \n\n\n\n\nCalculating Variance and Standard Deviation\nVariance measures how spread out the data is from the mean. We use the sample variance formula (dividing by n-1).\nVariance of X:\nFirst, calculate deviations from the mean (X_i - \\bar{X}):\n\n\n\nStudent\nX_i\nX_i - \\bar{X}\n(X_i - \\bar{X})^2\n\n\n\n\n1\n2.5\n2.5 - 5.175 = -2.675\n(-2.675)^2 = 7.1556\n\n\n2\n3.2\n3.2 - 5.175 = -1.975\n(-1.975)^2 = 3.9006\n\n\n3\n4.1\n4.1 - 5.175 = -1.075\n(-1.075)^2 = 1.1556\n\n\n4\n4.8\n4.8 - 5.175 = -0.375\n(-0.375)^2 = 0.1406\n\n\n5\n5.6\n5.6 - 5.175 = 0.425\n(0.425)^2 = 0.1806\n\n\n6\n6.3\n6.3 - 5.175 = 1.125\n(1.125)^2 = 1.2656\n\n\n7\n7.0\n7.0 - 5.175 = 1.825\n(1.825)^2 = 3.3306\n\n\n8\n7.9\n7.9 - 5.175 = 2.725\n(2.725)^2 = 7.4256\n\n\nSum\n\n\n24.555\n\n\n\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1} = \\frac{24.555}{7} = 3.5079\ns_X = \\sqrt{3.5079} = 1.8730\nVariance of Y:\n\n\n\nStudent\nY_i\nY_i - \\bar{Y}\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n80\n80 - 77 = 3\n(3)^2 = 9\n\n\n2\n85\n85 - 77 = 8\n(8)^2 = 64\n\n\n3\n78\n78 - 77 = 1\n(1)^2 = 1\n\n\n4\n82\n82 - 77 = 5\n(5)^2 = 25\n\n\n5\n77\n77 - 77 = 0\n(0)^2 = 0\n\n\n6\n74\n74 - 77 = -3\n(-3)^2 = 9\n\n\n7\n68\n68 - 77 = -9\n(-9)^2 = 81\n\n\n8\n72\n72 - 77 = -5\n(-5)^2 = 25\n\n\nSum\n\n\n214\n\n\n\ns^2_Y = \\frac{\\sum(Y_i - \\bar{Y})^2}{n-1} = \\frac{214}{7} = 30.5714\ns_Y = \\sqrt{30.5714} = 5.5291\n\n# Verify variance and standard deviation\ncat(\"Variance of Anxiety:\", var(anxiety), \"\\n\")\n\nVariance of Anxiety: 3.507857 \n\ncat(\"SD of Anxiety:\", sd(anxiety), \"\\n\")\n\nSD of Anxiety: 1.872927 \n\ncat(\"Variance of Performance:\", var(performance), \"\\n\")\n\nVariance of Performance: 30.57143 \n\ncat(\"SD of Performance:\", sd(performance), \"\\n\")\n\nSD of Performance: 5.529144",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#covariance-and-correlation-2",
    "href": "correg_en.html#covariance-and-correlation-2",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.44 Covariance and Correlation",
    "text": "9.44 Covariance and Correlation\n\nCalculating Covariance\nCovariance measures how two variables vary together. A negative covariance indicates that as one variable increases, the other tends to decrease.\ns_{XY} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nLet’s calculate the products for each observation:\n\n\n\n\n\n\n\n\n\nStudent\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n\n\n\n\n1\n-2.675\n3\n(-2.675) \\times (3) = -8.025\n\n\n2\n-1.975\n8\n(-1.975) \\times (8) = -15.800\n\n\n3\n-1.075\n1\n(-1.075) \\times (1) = -1.075\n\n\n4\n-0.375\n5\n(-0.375) \\times (5) = -1.875\n\n\n5\n0.425\n0\n(0.425) \\times (0) = 0\n\n\n6\n1.125\n-3\n(1.125) \\times (-3) = -3.375\n\n\n7\n1.825\n-9\n(1.825) \\times (-9) = -16.425\n\n\n8\n2.725\n-5\n(2.725) \\times (-5) = -13.625\n\n\nSum\n\n\n-60.2\n\n\n\ns_{XY} = \\frac{-60.2}{7} = -8.6\n\n\nCalculating Pearson Correlation Coefficient\nThe correlation coefficient standardizes the covariance to a scale from -1 to +1:\nr = \\frac{s_{XY}}{s_X \\cdot s_Y} = \\frac{-8.6}{1.8730 \\times 5.5291} = \\frac{-8.6}{10.3560} = -0.831\nThis gives us a correlation of -0.831, indicating a strong negative relationship between anxiety and performance.\n\n# Verify correlation\nactual_cor &lt;- cor(anxiety, performance)\ncat(\"Pearson correlation coefficient:\", actual_cor, \"\\n\")\n\nPearson correlation coefficient: -0.8304618",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#simple-ols-regression",
    "href": "correg_en.html#simple-ols-regression",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.45 Simple OLS Regression",
    "text": "9.45 Simple OLS Regression\n\nThe Problem of Modeling Relationships\nWhen we observe a relationship between two variables, we want to find a mathematical model that:\n\nDescribes the relationship\nAllows us to make predictions\nQuantifies the strength of the relationship\n\nThe simplest model is a straight line: Y = \\beta_0 + \\beta_1 X + \\epsilon\nWhere:\n\nY is the outcome variable (performance)\nX is the predictor variable (anxiety)\n\\beta_0 is the intercept (performance when anxiety = 0)\n\\beta_1 is the slope (change in performance per unit change in anxiety)\n\\epsilon is the error term (unexplained variation)\n\n\n\nThe Idea of Sum of Squared Errors (SSE)\n\nWhy Do We Need a Criterion?\nImagine trying to draw a line through our data points. There are infinitely many lines we could draw! Some would go through the middle of the points, others might be too high or too low, too steep or too flat. We need a systematic way to determine which line is “best.”\n\n# Visualize multiple possible lines\nlibrary(ggplot2)\n\n# Create different possible lines\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  # Bad line 1: Too steep\n  geom_abline(intercept = 100, slope = -5, color = \"gray\", linetype = \"dashed\", alpha = 0.5) +\n  # Bad line 2: Too flat\n  geom_abline(intercept = 78, slope = -0.5, color = \"gray\", linetype = \"dashed\", alpha = 0.5) +\n  # Bad line 3: Wrong direction\n  geom_abline(intercept = 65, slope = 2, color = \"gray\", linetype = \"dashed\", alpha = 0.5) +\n  # Good line (we'll calculate this properly)\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Which Line Best Fits Our Data?\",\n    subtitle = \"Gray dashed lines show poor fits, red line shows the optimal fit\",\n    x = \"Test Anxiety Score\",\n    y = \"Exam Performance (%)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWhat Are Errors (Residuals)?\nFor any line we draw, each data point will have an error or residual - the vertical distance from the point to the line. This represents how “wrong” our prediction is for that point.\n\nPositive error: The actual value is above the predicted value (we underestimated)\nNegative error: The actual value is below the predicted value (we overestimated)\n\n\n# Visualize errors for the regression line\nmodel &lt;- lm(performance ~ anxiety)\npredicted &lt;- predict(model)\n\nggplot(data.frame(anxiety, performance, predicted), aes(x = anxiety, y = performance)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_segment(aes(xend = anxiety, yend = predicted), \n               color = \"gray50\", linetype = \"dashed\", alpha = 0.7) +\n  # Add labels for a few errors\n  geom_text(aes(x = 3.2, y = 86, label = \"Error = +4.9\"), size = 3, color = \"gray40\") +\n  geom_text(aes(x = 7.0, y = 69, label = \"Error = -1.4\"), size = 3, color = \"gray40\") +\n  labs(\n    title = \"Visualizing Errors (Residuals) in Regression\",\n    subtitle = \"Dashed lines show the vertical distance from each point to the regression line\",\n    x = \"Test Anxiety Score\",\n    y = \"Exam Performance (%)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWhy Square the Errors?\nSimply adding up the errors won’t work because positive and negative errors cancel out. We could use absolute values, but squaring has several advantages:\n\nMathematical convenience: Squared functions are differentiable, making it easier to find the minimum using calculus\nPenalizes large errors more: A few large errors are worse than many small errors\n\nError of 4: squared = 16\nTwo errors of 2: squared = 4 + 4 = 8\nFour errors of 1: squared = 1 + 1 + 1 + 1 = 4\n\nCreates a smooth, bowl-shaped function: This guarantees a unique minimum\n\n\n\nThe SSE Formula\nThe Sum of Squared Errors is: SSE = \\sum_{i=1}^{n}(Y_i - \\hat{Y_i})^2 = \\sum_{i=1}^{n}(Y_i - (\\beta_0 + \\beta_1 X_i))^2\n\n\\min_{\\beta}\\ \\sum_{i=1}^n\\bigl(Y_i-\\hat{Y}_i(\\beta)\\bigr)^2,\n\\quad\\text{where } \\hat{Y}_i(\\beta)=\\beta_0+\\beta_1X_i.\n\nThis formula:\n\nTakes each actual value (Y_i)\nSubtracts the predicted value (\\hat{Y}_i(\\beta)=\\beta_0+\\beta_1X_i)\nSquares the difference\nAdds them all up\n\n\n\nFinding the Best Line\nThe “best” line is the one that minimizes SSE. Using calculus (taking derivatives with respect to \\beta_0 and \\beta_1 and setting them to zero), we get the OLS formulas.\n\n\n\nOLS Estimators\nThe Ordinary Least Squares (OLS) method finds the values of \\beta_0 and \\beta_1 that minimize SSE:\nSlope estimator: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\nIntercept estimator: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\n\nCalculating the OLS Parameters\nUsing our calculated values:\n\ns_{XY} = -8.6\ns^2_X = 3.5079\n\\bar{X} = 5.175\n\\bar{Y} = 77\n\nStep 1: Calculate the slope \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-8.6}{3.5079} = -2.451\nStep 2: Calculate the intercept \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} = 77 - (-2.451 \\times 5.175) = 77 + 12.684 = 89.684\n\n# Verify with R\nmodel &lt;- lm(performance ~ anxiety)\ncoef(model)\n\n(Intercept)     anxiety \n  89.687233   -2.451639 \n\n\nOur regression equation is: \\hat{Y} = 89.684 - 2.451X\nThis means:\n\nWhen anxiety = 0, predicted performance = 89.68%\nFor each 1-point increase in anxiety, performance decreases by 2.45%",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#plotting-the-model",
    "href": "correg_en.html#plotting-the-model",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.46 Plotting the Model",
    "text": "9.46 Plotting the Model\n\n# Create comprehensive plot\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1) +\n  geom_text(aes(label = paste0(\"(\", round(anxiety, 1), \", \", performance, \")\")),\n            vjust = -1, size = 3) +\n  annotate(\"text\", x = 3, y = 70, \n           label = paste0(\"ŷ = \", round(coef(model)[1], 2), \" - \", \n                         abs(round(coef(model)[2], 2)), \"x\"),\n           size = 5, color = \"red\") +\n  labs(\n    title = \"Regression Line: Performance vs. Anxiety\",\n    subtitle = \"Higher anxiety is associated with lower exam performance\",\n    x = \"Test Anxiety Score\",\n    y = \"Exam Performance (%)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(2, 8, 1)) +\n  scale_y_continuous(breaks = seq(65, 90, 5))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#model-evaluation",
    "href": "correg_en.html#model-evaluation",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.47 Model Evaluation",
    "text": "9.47 Model Evaluation\n\nVariance Decomposition\nThe total variation in Y can be decomposed into two parts:\nSST = SSE + SSR\nWhere:\n\nSST (Total Sum of Squares): Total variation in Y\nSSE (Sum of Squared Errors): Unexplained variation\n\nSSR (Sum of Squares Regression): Explained variation\n\n\n\n\n\n\n\nTip\n\n\n\nImagine you’re trying to understand why people have different salaries at a company. Some people make $40,000, others make $80,000, and some make $120,000. There’s variation in salaries - they’re not all the same.\n\nThe Total Variation (SST)\nThis is simply asking: “How spread out are all the salaries from the average salary?”\nIf the average salary is $70,000, then SST measures how far each person’s salary differs from $70,000, squares those differences (to make them all positive), and adds them up. It’s the total amount of variation we’re trying to understand.\n\n\nThe Explained Variation (SSR)\nNow suppose we build a model that predicts salary based on years of experience. Our model might say: - 2 years experience → predicts $50,000 - 5 years experience → predicts $70,000\n- 10 years experience → predicts $100,000\nSSR measures how much these predictions vary from the average. It’s the variation that our model successfully “explains” through the relationship with experience. It’s like saying “this much of the salary differences between people is because they have different amounts of experience.”\n\n\nThe Unexplained Variation (SSE)\nThis is what’s left over - the part our model can’t explain.\nMaybe two people both have 5 years of experience, but one makes $65,000 and another makes $75,000. Our model predicted $70,000 for both. These differences from our predictions (the errors) represent variation due to other factors we haven’t captured - maybe education, performance, negotiation skills, or just random luck.\n\n\n9.48 The Key Insight\nThe beautiful thing is that these three always relate as: Total Variation = Explained Variation + Unexplained Variation\nIt’s like having a pie chart of “why salaries differ”: - One slice is “differences explained by experience” (SSR) - The other slice is “differences due to other stuff” (SSE) - Together they make the whole pie (SST)\n\n\n9.49 Why This Matters\nThis decomposition lets us calculate R-squared (R²), which is simply: R^2 = \\frac{SSR}{SST} = \\frac{\\text{Explained Variation}}{\\text{Total Variation}}\nIf R² = 0.70, it means our model explains 70% of why the Y values differ from each other. The remaining 30% is due to factors we haven’t captured or random noise.\nThink of it like solving a mystery: SST is the total mystery to solve, SSR is how much of the mystery you’ve solved, and SSE is what remains unsolved!\n\n\n\nLet’s calculate each:\nStep 1: Calculate predicted values\nUsing \\hat{Y} = 89.684 - 2.451X:\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i = 89.684 - 2.451X_i\n\n\n\n\n1\n2.5\n80\n89.684 - 2.451(2.5) = 83.556\n\n\n2\n3.2\n85\n89.684 - 2.451(3.2) = 81.841\n\n\n3\n4.1\n78\n89.684 - 2.451(4.1) = 79.635\n\n\n4\n4.8\n82\n89.684 - 2.451(4.8) = 77.919\n\n\n5\n5.6\n77\n89.684 - 2.451(5.6) = 75.968\n\n\n6\n6.3\n74\n89.684 - 2.451(6.3) = 74.253\n\n\n7\n7.0\n68\n89.684 - 2.451(7.0) = 72.527\n\n\n8\n7.9\n72\n89.684 - 2.451(7.9) = 70.321\n\n\n\nStep 2: Calculate sum of squares\nSST (Total variation): SST = \\sum(Y_i - \\bar{Y})^2\nFrom our earlier variance calculation: SST = (n-1) \\times s^2_Y = 7 \\times 30.5714 = 214\nSSE (Unexplained variation): SSE = \\sum(Y_i - \\hat{Y}_i)^2\n\n\n\n\n\n\n\n\n\n\ni\nY_i\n\\hat{Y}_i\nY_i - \\hat{Y}_i\n(Y_i - \\hat{Y}_i)^2\n\n\n\n\n1\n80\n83.556\n-3.556\n12.645\n\n\n2\n85\n81.841\n3.159\n9.979\n\n\n3\n78\n79.635\n-1.635\n2.673\n\n\n4\n82\n77.919\n4.081\n16.655\n\n\n5\n77\n75.968\n1.032\n1.065\n\n\n6\n74\n74.253\n-0.253\n0.064\n\n\n7\n68\n72.527\n-4.527\n20.494\n\n\n8\n72\n70.321\n1.679\n2.819\n\n\nSum\n\n\n\n66.394\n\n\n\nSSR (Explained variation): SSR = SST - SSE = 214 - 66.394 = 147.606\n\n# Verify calculations\nanova_table &lt;- anova(model)\ncat(\"SSR (Regression):\", anova_table$`Sum Sq`[1], \"\\n\")\n\nSSR (Regression): 147.5887 \n\ncat(\"SSE (Residual):\", anova_table$`Sum Sq`[2], \"\\n\")\n\nSSE (Residual): 66.41132 \n\ncat(\"SST (Total):\", sum(anova_table$`Sum Sq`), \"\\n\")\n\nSST (Total): 214 \n\n\n\n\nR-squared (Coefficient of Determination)\nR-squared tells us what proportion of the total variation in Y is explained by our model:\nR^2 = \\frac{SSR}{SST} = \\frac{147.606}{214} = 0.690\nThis means our model explains 69.0% of the variation in exam performance.\nAlternative formula using correlation: R^2 = r^2 = (-0.831)^2 = 0.691\n\n# Verify R-squared\ncat(\"R-squared:\", summary(model)$r.squared, \"\\n\")\n\nR-squared: 0.6896667 \n\ncat(\"Correlation squared:\", cor(anxiety, performance)^2, \"\\n\")\n\nCorrelation squared: 0.6896667",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-key-insight",
    "href": "correg_en.html#the-key-insight",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.48 The Key Insight",
    "text": "9.48 The Key Insight\nThe beautiful thing is that these three always relate as: Total Variation = Explained Variation + Unexplained Variation\nIt’s like having a pie chart of “why salaries differ”: - One slice is “differences explained by experience” (SSR) - The other slice is “differences due to other stuff” (SSE) - Together they make the whole pie (SST)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#why-this-matters",
    "href": "correg_en.html#why-this-matters",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.49 Why This Matters",
    "text": "9.49 Why This Matters\nThis decomposition lets us calculate R-squared (R²), which is simply: R^2 = \\frac{SSR}{SST} = \\frac{\\text{Explained Variation}}{\\text{Total Variation}}\nIf R² = 0.70, it means our model explains 70% of why the Y values differ from each other. The remaining 30% is due to factors we haven’t captured or random noise.\nThink of it like solving a mystery: SST is the total mystery to solve, SSR is how much of the mystery you’ve solved, and SSE is what remains unsolved!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#effect-size",
    "href": "correg_en.html#effect-size",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.50 Effect Size",
    "text": "9.50 Effect Size\nThe slope coefficient \\hat{\\beta_1} = -2.451 is our effect size. It tells us:\n\nMagnitude: Each 1-point increase in anxiety is associated with a 2.45% decrease in performance\nPractical significance: A student moving from low anxiety (3) to high anxiety (7) would see an expected performance decrease of 2.451 \\times 4 = 9.80\\%\n\nStandardized effect size (correlation coefficient): The correlation r = -0.831 indicates a strong negative relationship.\nCohen’s guidelines for interpreting correlation:\n\nSmall effect: |r| = 0.10\nMedium effect: |r| = 0.30\nLarge effect: |r| = 0.50\n\nOur |r| = 0.831 represents a large effect size.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#confidence-intervals-and-statistical-significance",
    "href": "correg_en.html#confidence-intervals-and-statistical-significance",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.51 Confidence Intervals and Statistical Significance",
    "text": "9.51 Confidence Intervals and Statistical Significance\n\nStandard Error of the Regression\nFirst, we need the standard error of the residuals:\ns_e = \\sqrt{\\frac{SSE}{n-2}} = \\sqrt{\\frac{66.394}{6}} = \\sqrt{11.066} = 3.327\n\n\nStandard Error of the Slope\nThe standard error of \\hat{\\beta_1} is:\nSE(\\hat{\\beta_1}) = \\frac{s_e}{\\sqrt{\\sum(X_i - \\bar{X})^2}} = \\frac{3.327}{\\sqrt{24.555}} = \\frac{3.327}{4.955} = 0.671\n\n\n95% Confidence Interval for the Slope\nIn plain English: A confidence interval gives us a range of plausible values for our true slope. If we repeated this study many times, 95% of the intervals we calculate would contain the true slope value.\nThe formula uses a critical value (approximately 2.45 for 6 degrees of freedom):\nCI = \\hat{\\beta_1} \\pm (critical\\_value \\times SE(\\hat{\\beta_1})) CI = -2.451 \\pm (2.45 \\times 0.671) CI = -2.451 \\pm 1.644 CI = [-4.095, -0.807]\nInterpretation: We are 95% confident that the true change in performance per unit change in anxiety is between -4.10% and -0.81%.\n\n\nStatistical Significance\nTo test if the relationship is statistically significant (i.e., not due to chance), we calculate a t-statistic:\nt = \\frac{\\hat{\\beta_1}}{SE(\\hat{\\beta_1})} = \\frac{-2.451}{0.671} = -3.653\nIn plain English: This t-value tells us how many standard errors our slope is away from zero. An absolute value of 3.65 is quite large (typically, values beyond ±2.45 are considered significant for our sample size), providing strong evidence of a real negative relationship between anxiety and performance.\n\n# Verify calculations with R\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.526 -2.116  0.400  2.050  4.081 \n\nCoefficients:\n            Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)  89.6872     3.6682  24.450 0.000000308 ***\nanxiety      -2.4516     0.6714  -3.652      0.0107 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.327 on 6 degrees of freedom\nMultiple R-squared:  0.6897,    Adjusted R-squared:  0.6379 \nF-statistic: 13.33 on 1 and 6 DF,  p-value: 0.01069\n\nconfint(model)\n\n                2.5 %     97.5 %\n(Intercept) 80.711582 98.6628838\nanxiety     -4.094474 -0.8088048",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-and-interpretation",
    "href": "correg_en.html#summary-and-interpretation",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.52 Summary and Interpretation",
    "text": "9.52 Summary and Interpretation\n\nWhat We’ve Learned\n\nThere is a negative relationship between test anxiety and exam performance (r = -0.831)\nThe relationship is moderately strong - anxiety explains 69.0% of the variation in performance\nThe effect is substantial - each 1-point increase in anxiety is associated with about a 2.5% decrease in performance\nThe relationship is statistically significant - very unlikely to be due to chance\n\n\n\nPractical Implications\nOur analysis suggests that high test anxiety impairs performance, possibly through:\n\nCognitive interference (worrying thoughts compete for working memory)\nPhysical symptoms (sweating, rapid heartbeat) that distract from the task\nNegative self-talk reducing confidence and motivation\nTest-taking behaviors (rushing, second-guessing answers)\n\n\n\nRecommendations Based on Findings\nGiven the strong negative relationship, interventions might include:\n\nTeaching anxiety management techniques (deep breathing, progressive muscle relaxation)\nCognitive restructuring to address catastrophic thinking\nStudy skills training to increase preparation confidence\nPractice tests to reduce fear of the unknown\n\n\n\nLimitations of Our Analysis\n\nSmall sample size (n = 8) limits generalizability\nLinear assumption - the relationship might be curved (optimal anxiety might exist)\nOther variables not considered (preparation time, ability, sleep, etc.)\nCausation vs. correlation - we cannot prove anxiety causes poor performance\nSelf-reported anxiety - subjective measures may not reflect physiological arousal\n\n\n\nComplete R Code\n\n# Complete analysis code\nanxiety &lt;- c(2.5, 3.2, 4.1, 4.8, 5.6, 6.3, 7.0, 7.9)\nperformance &lt;- c(80, 85, 78, 82, 77, 74, 68, 72)\n\n# Descriptive statistics\nmean(anxiety); sd(anxiety)\nmean(performance); sd(performance)\n\n# Correlation\ncor(anxiety, performance)\n\n# Regression\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\nconfint(model)\n\n# Visualization\nlibrary(ggplot2)\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_minimal()\n\n# Diagnostics\nplot(model)\n\n# ANOVA table\nanova(model)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#example-5.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "href": "correg_en.html#example-5.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.53 Example 5. District Magnitude and Electoral Disproportionality: A Comparative Analysis",
    "text": "9.53 Example 5. District Magnitude and Electoral Disproportionality: A Comparative Analysis\n\nData Generating Process\nLet’s set up a DGP where:\n\\begin{aligned}\n& Y_{\\text{Gallagher}} = 12 - 0.8X_{\\text{DM}} + \\varepsilon \\\\\n& \\varepsilon \\sim \\mathcal{N}(0, 1) \\\\\n& X_{\\text{DM}} \\in \\{3, 5, 7, 10, 12, 15\\}\n\\end{aligned}\n\n# DGP\nmagnitude &lt;- c(3, 5, 7, 10, 12, 15)\nepsilon &lt;- rnorm(6, mean = 0, sd = 1)\ngallagher &lt;- 12 - 0.8 * magnitude + epsilon\n\n# Round (sampled from the DGP) Gallagher indices to one decimal place\ngallagher &lt;- round(c(9.0, 7.8, 9.2, 4.1, 2.5, 1.7), 1)\n\n# Show data\ndata.frame(\n  District_Magnitude = magnitude,\n  Gallagher_Index = gallagher\n)\n\n  District_Magnitude Gallagher_Index\n1                  3             9.0\n2                  5             7.8\n3                  7             9.2\n4                 10             4.1\n5                 12             2.5\n6                 15             1.7\n\n# Initial model check\nmodel &lt;- lm(gallagher ~ magnitude)\nsummary(model)\n\n\nCall:\nlm(formula = gallagher ~ magnitude)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.6516 -0.4628  2.3260 -0.6908 -0.9020  0.3813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7349     1.3034   9.003 0.000843 ***\nmagnitude    -0.6944     0.1359  -5.110 0.006934 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.368 on 4 degrees of freedom\nMultiple R-squared:  0.8672,    Adjusted R-squared:  0.834 \nF-statistic: 26.11 on 1 and 4 DF,  p-value: 0.006934\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{3 + 5 + 7 + 10 + 12 + 15}{6} = \\frac{52}{6} = 8.6667\n\\bar{Y} = \\frac{9.0 + 7.8 + 9.2 + 4.1 + 2.5 + 1.7}{6} = \\frac{34.3}{6} = 5.7167\n\n# Verification\nmean(magnitude)\n\n[1] 8.666667\n\nmean(gallagher)\n\n[1] 5.716667\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(3 - 8.6667) = -5.6667\n(5 - 8.6667) = -3.6667\n(7 - 8.6667) = -1.6667\n(10 - 8.6667) = 1.3333\n(12 - 8.6667) = 3.3333\n(15 - 8.6667) = 6.3333\n\nSquared deviations:\n\n32.1115 + 13.4445 + 2.7779 + 1.7777 + 11.1109 + 40.1107 = 101.3332\n\ns^2_X = \\frac{101.3332}{5} = 20.2666\nFor Y: Deviations: 3.2833, 2.0833, 3.4833, -1.6167, -3.2167, -4.0167\ns^2_Y = \\frac{56.3483}{5} = 11.2697\n\n# Verification\nvar(magnitude)\n\n[1] 20.26667\n\nvar(gallagher)\n\n[1] 11.26967\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n\n(-5.6667 × 3.2833) = -18.6057\n(-3.6667 × 2.0833) = -7.6387\n(-1.6667 × 3.4833) = -5.8056\n(1.3333 × -1.6167) = -2.1556\n(3.3333 × -3.2167) = -10.7223\n(6.3333 × -4.0167) = -25.4391\n\nSum = -70.3670\ns_{XY} = \\frac{-70.3670}{5} = -14.0734\n\n# Verification\ncov(magnitude, gallagher)\n\n[1] -14.07333\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-14.0734}{\\sqrt{20.2666}\\sqrt{11.2697}} = -0.9279\n\n# Verification\ncor(magnitude, gallagher)\n\n[1] -0.9312157\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-14.0734}{20.2666} = -0.6944\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-0.6944 × 8.6667 = -6.0181\n\\hat{\\beta_0} = 5.7167 - (-6.0181) = 11.7348\n\n\n# Verification\ncoef(model)\n\n(Intercept)   magnitude \n 11.7348684  -0.6944079 \n\n\n\n\nR-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = 11.7348 - 0.6944X\n\n# Predicted values\ny_hat &lt;- 11.7348 - 0.6944 * magnitude\ndata.frame(\n  Magnitude = magnitude,\n  Gallagher = gallagher,\n  Predicted = y_hat,\n  row.names = 1:6\n)\n\n  Magnitude Gallagher Predicted\n1         3       9.0    9.6516\n2         5       7.8    8.2628\n3         7       9.2    6.8740\n4        10       4.1    4.7908\n5        12       2.5    3.4020\n6        15       1.7    1.3188\n\n\nStep 2: Sum of Squares SST = \\sum(Y_i - \\bar{Y})^2 = 56.3483 SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 48.5271 SSE = \\sum(Y_i - \\hat{Y}_i)^2 = 7.8212\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{48.5271}{56.3483} = 0.8612\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.8671626\n\n\n\n\nVisualization - True vs. Estimated Parameters\n\nTrue DGP: Y = 12 - 0.8X + ε\nEstimated Model: Y = 11.7348 - 0.6944X\n\n\nlibrary(ggplot2)\n\n# Create data frame with original data\ndf &lt;- data.frame(\n  magnitude = magnitude,\n  gallagher = gallagher\n)\n\n# Create sequence for smooth lines\nx_seq &lt;- seq(min(magnitude), max(magnitude), length.out = 100)\n\n# Calculate predicted values for both lines\ntrue_dgp &lt;- 12 - 0.8 * x_seq\nestimated &lt;- 11.7348 - 0.6944 * x_seq\n\n# Combine into a data frame for plotting\nlines_df &lt;- data.frame(\n  magnitude = rep(x_seq, 2),\n  value = c(true_dgp, estimated),\n  Model = rep(c(\"True DGP\", \"Estimated\"), each = length(x_seq))\n)\n\n# Create plot\nggplot() +\n  geom_line(data = lines_df, \n            aes(x = magnitude, y = value, color = Model, linetype = Model),\n            size = 1) +\n  geom_point(data = df, \n             aes(x = magnitude, y = gallagher),\n             color = \"black\", \n             size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  labs(\n    title = \"True DGP vs. Estimated Regression Line\",\n    subtitle = \"Black points show observed data with random noise\",\n    x = \"District Magnitude\",\n    y = \"Gallagher Index\",\n    caption = \"True DGP: Y = 12 - 0.8X + ε\\nEstimated: Y = 11.73 - 0.69X\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.caption = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\n\nObservations about Model Fit\n\nSlope Comparison\n\nTrue slope: -0.8\nEstimated slope: -0.69\nThe estimated slope is reasonably close to the true parameter\n\nIntercept Comparison\n\nTrue intercept: 12\nEstimated intercept: 11.73\nThe estimated intercept very closely approximates the true value\n\nVisual Patterns\n\nThe lines are nearly parallel, showing good slope recovery\nPoints scatter around both lines due to the random error term (ε)\nThe small sample size (n=6) leads to some imprecision in estimation\nThe estimated line (blue) provides a good approximation of the true DGP (red dashed)\n\nImpact of Random Error\n\nThe scatter of points around the true DGP line reflects the N(0,1) error term\nThis noise leads to the slight differences in estimated parameters\nWith a larger sample, we would expect even closer convergence to true parameters\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.93) between district magnitude and electoral disproportionality\nFor each unit increase in district magnitude, the Gallagher index decreases by approximately 0.69 points\nThe model explains 86.12% of the variance in disproportionality\nThe relationship appears strongly linear with moderate scatter\nThe intercept (11.73) represents the expected disproportionality in a hypothetical single-member district system\n\n\n\nStudy Context\n\nData represents simulated observations from a DGP with moderate noise\nSample shows how increasing district magnitude tends to reduce disproportionality\nRandom component reflects other institutional and political factors affecting disproportionality\n\n\n\nLimitations\n\nSmall sample size (n=6)\nSimulated rather than real-world data\nAssumes linear relationship\nDoes not account for other institutional features",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-d-ols-regression-in-matrix-form---manual-calculations",
    "href": "correg_en.html#appendix-d-ols-regression-in-matrix-form---manual-calculations",
    "title": "9  Introduction to Correlation and Regression Analysis",
    "section": "9.54 Appendix D: OLS Regression in Matrix Form - Manual Calculations (*)",
    "text": "9.54 Appendix D: OLS Regression in Matrix Form - Manual Calculations (*)\n\nA.1 Essential Linear Algebra Concepts\nBefore diving into OLS regression, we need to understand some fundamental linear algebra concepts.\n\nA.1.1 Matrix Transpose\nThe transpose of a matrix \\mathbf{A}, denoted \\mathbf{A}^T, is obtained by swapping rows and columns:\n\n\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\implies \\mathbf{A}^T = \\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 4 & 6 \\end{bmatrix}\n\nProperties:\n\n(A^T)^T = A\n(AB)^T = B^T A^T\n(A + B)^T = A^T + B^T\n\n\n\nA.1.2 Matrix Multiplication\nFor matrices \\mathbf{A} (of size m \\times n) and \\mathbf{B} (of size n \\times p), the product \\mathbf{AB} is an m \\times p matrix where element (i,j) is:\n\n(\\mathbf{AB})_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}\n\nExample:\n\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 1(5)+2(7) & 1(6)+2(8) \\\\ 3(5)+4(7) & 3(6)+4(8) \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n\n\n\nA.1.3 Identity Matrix\nThe identity matrix \\mathbf{I}_n is an n \\times n matrix with 1’s on the diagonal and 0’s elsewhere:\n\n\\mathbf{I}_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\nProperty: \\mathbf{AI} = \\mathbf{IA} = \\mathbf{A}\n\n\nA.1.4 Determinant\nThe determinant is a scalar value that characterizes certain properties of a square matrix. It indicates whether a matrix is invertible.\nFor a 2 \\times 2 matrix:\n\n\\det\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = ad - bc\n\nExample:\n\n\\det\\begin{bmatrix} 3 & 8 \\\\ 4 & 6 \\end{bmatrix} = 3(6) - 8(4) = 18 - 32 = -14\n\nFor a 3 \\times 3 matrix (cofactor expansion along first row):\n\n\\det\\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} = a\\begin{vmatrix} e & f \\\\ h & i \\end{vmatrix} - b\\begin{vmatrix} d & f \\\\ g & i \\end{vmatrix} + c\\begin{vmatrix} d & e \\\\ g & h \\end{vmatrix}\n\nProperties:\n\nIf \\det(\\mathbf{A}) = 0, the matrix is singular (not invertible)\nIf \\det(\\mathbf{A}) \\neq 0, the matrix is non-singular (invertible)\n\\det(\\mathbf{AB}) = \\det(\\mathbf{A})\\det(\\mathbf{B})\n\\det(\\mathbf{A}^T) = \\det(\\mathbf{A})\n\n\n\nA.1.5 Matrix Inverse\nThe inverse of matrix \\mathbf{A}, denoted \\mathbf{A}^{-1}, satisfies:\n\n\\mathbf{AA}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\n\nFor a 2 \\times 2 matrix:\n\n\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad-bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\nFor larger matrices, the inverse can be computed as:\n\n\\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})}\\text{adj}(\\mathbf{A})\n\nwhere \\text{adj}(\\mathbf{A}) is the adjugate (transpose of the cofactor matrix).\nProperties:\n\n(A^{-1})^{-1} = A\n(AB)^{-1} = B^{-1}A^{-1}\n(A^T)^{-1} = (A^{-1})^T\n\n\n\nA.1.6 Matrix Rank\nThe rank of a matrix is the maximum number of linearly independent rows (or columns). A matrix \\mathbf{X} of size n \\times p has:\n\nFull column rank if \\text{rank}(\\mathbf{X}) = p (all columns are independent)\nRank deficiency if \\text{rank}(\\mathbf{X}) &lt; p (multicollinearity exists)\n\nFull column rank is required for \\mathbf{X}^T\\mathbf{X} to be invertible in regression.\n\n\nA.1.7 Quadratic Forms\nA quadratic form is an expression of the type:\n\n\\mathbf{x}^T\\mathbf{A}\\mathbf{x} = \\sum_{i=1}^n\\sum_{j=1}^n a_{ij}x_ix_j\n\nFor a vector \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} and matrix \\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}:\n\n\\mathbf{x}^T\\mathbf{A}\\mathbf{x} = a_{11}x_1^2 + (a_{12}+a_{21})x_1x_2 + a_{22}x_2^2\n\nThis is crucial for understanding the OLS objective function.\n\n\n\nA.2 OLS Minimization Problem and Derivation\n\nA.2.1 The Objective Function\nIn matrix notation, the linear regression model is:\n\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\nwhere:\n\n\\mathbf{y} is an n \\times 1 vector of observed responses\n\\mathbf{X} is an n \\times (p+1) design matrix of predictors (including intercept)\n\\boldsymbol{\\beta} is a (p+1) \\times 1 vector of regression coefficients\n\\boldsymbol{\\epsilon} is an n \\times 1 vector of errors\n\nThe sum of squared residuals (SSR) is:\n\nS(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2\n\nIn matrix form, this becomes:\n\nS(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n\n\n\nA.2.2 Expanding the Objective Function\nLet’s expand the expression:\n\nS(\\boldsymbol{\\beta}) = (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n\n\n= \\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\nNote that \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} is a scalar, so it equals its transpose:\n\n\\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} = (\\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta})^T = \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y}\n\nTherefore:\n\nS(\\boldsymbol{\\beta}) = \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\n\n\nA.2.3 Taking the Derivative\nTo minimize S(\\boldsymbol{\\beta}), we take the derivative with respect to \\boldsymbol{\\beta} and set it equal to zero.\nMatrix calculus rules needed:\n\n\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}(\\mathbf{a}^T\\boldsymbol{\\beta}) = \\mathbf{a}\n\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}(\\boldsymbol{\\beta}^T\\mathbf{A}\\boldsymbol{\\beta}) = 2\\mathbf{A}\\boldsymbol{\\beta} (when \\mathbf{A} is symmetric)\n\nApplying these rules:\n\n\\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\n\n\nA.2.4 Setting the Derivative to Zero (Normal Equations)\nSet \\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = \\mathbf{0}:\n\n-2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0}\n\nSimplify:\n\n\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}\n\nThese are called the normal equations.\n\n\nA.2.5 Solving for \\hat{\\boldsymbol{\\beta}}\nAssuming \\mathbf{X}^T\\mathbf{X} is invertible (full rank assumption), multiply both sides by (\\mathbf{X}^T\\mathbf{X})^{-1}:\n\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n\nThis is the OLS estimator.\n\n\nA.2.6 Verifying This is a Minimum\nTo confirm this is a minimum (not maximum or saddle point), we check the second derivative:\n\n\\frac{\\partial^2 S}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T} = 2\\mathbf{X}^T\\mathbf{X}\n\nThis is the Hessian matrix. Since \\mathbf{X}^T\\mathbf{X} is positive definite (when \\mathbf{X} has full column rank), the Hessian is positive definite, confirming we have a minimum.\n\n\n\nA.3 Case 1: Simple Linear Regression (One Predictor)\n\nA.3.1 Setting Up the Problem\nConsider the following dataset with n = 5 observations:\n\n\n\nObservation\nx\ny\n\n\n\n\n1\n1\n2\n\n\n2\n2\n4\n\n\n3\n3\n5\n\n\n4\n4\n4\n\n\n5\n5\n5\n\n\n\nOur model is: y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\n\nA.3.2 Constructing the Design Matrix\nThe design matrix \\mathbf{X} includes a column of ones for the intercept:\n\n\\mathbf{X} = \\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 4 \\\\\n1 & 5\n\\end{bmatrix}, \\quad\n\\mathbf{y} = \\begin{bmatrix}\n2 \\\\\n4 \\\\\n5 \\\\\n4 \\\\\n5\n\\end{bmatrix}\n\n\n\nA.3.3 Step 1: Calculate \\mathbf{X}^T\\mathbf{X}\n\n\\mathbf{X}^T = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5\n\\end{bmatrix}\n\n\n\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 4 \\\\\n1 & 5\n\\end{bmatrix}\n\nElement-by-element calculation:\n\n(1,1) element: 1(1) + 1(1) + 1(1) + 1(1) + 1(1) = 5\n(1,2) element: 1(1) + 1(2) + 1(3) + 1(4) + 1(5) = 15\n(2,1) element: 1(1) + 2(1) + 3(1) + 4(1) + 5(1) = 15\n(2,2) element: 1(1) + 2(2) + 3(3) + 4(4) + 5(5) = 1 + 4 + 9 + 16 + 25 = 55\n\n\n\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n5 & 15 \\\\\n15 & 55\n\\end{bmatrix}\n\n\n\nA.3.4 Step 2: Calculate (\\mathbf{X}^T\\mathbf{X})^{-1}\nFor a 2 \\times 2 matrix \\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}, the inverse is:\n\n\\mathbf{A}^{-1} = \\frac{1}{ad - bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\nCalculate the determinant:\n\n\\det(\\mathbf{X}^T\\mathbf{X}) = 5(55) - 15(15) = 275 - 225 = 50\n\nCalculate the inverse:\n\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\frac{1}{50}\\begin{bmatrix}\n55 & -15 \\\\\n-15 & 5\n\\end{bmatrix} = \\begin{bmatrix}\n1.1 & -0.3 \\\\\n-0.3 & 0.1\n\\end{bmatrix}\n\n\n\nA.3.5 Step 3: Calculate \\mathbf{X}^T\\mathbf{y}\n\n\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\n2 \\\\\n4 \\\\\n5 \\\\\n4 \\\\\n5\n\\end{bmatrix}\n\nElement-by-element calculation:\n\nFirst element: 1(2) + 1(4) + 1(5) + 1(4) + 1(5) = 20\nSecond element: 1(2) + 2(4) + 3(5) + 4(4) + 5(5) = 2 + 8 + 15 + 16 + 25 = 66\n\n\n\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n20 \\\\\n66\n\\end{bmatrix}\n\n\n\nA.3.6 Step 4: Calculate \\hat{\\boldsymbol{\\beta}}\n\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n1.1 & -0.3 \\\\\n-0.3 & 0.1\n\\end{bmatrix}\n\\begin{bmatrix}\n20 \\\\\n66\n\\end{bmatrix}\n\nElement-by-element calculation:\n\n\\hat{\\beta}_0: 1.1(20) + (-0.3)(66) = 22 - 19.8 = 2.2\n\\hat{\\beta}_1: (-0.3)(20) + 0.1(66) = -6 + 6.6 = 0.6\n\n\n\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix}\n2.2 \\\\\n0.6\n\\end{bmatrix}\n\nResult: The fitted regression line is \\hat{y} = 2.2 + 0.6x\n\n\n\nA.4 Case 2: Multiple Linear Regression (Two Predictors)\n\nA.4.1 Setting Up the Problem\nConsider a dataset with n = 5 observations and two predictors:\n\n\n\nObservation\nx_1\nx_2\ny\n\n\n\n\n1\n1\n3\n3\n\n\n2\n2\n2\n5\n\n\n3\n3\n5\n7\n\n\n4\n4\n4\n9\n\n\n5\n5\n7\n10\n\n\n\nOur model is: y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i\n\n\nA.4.2 Constructing the Design Matrix\n\n\\mathbf{X} = \\begin{bmatrix}\n1 & 1 & 3 \\\\\n1 & 2 & 2 \\\\\n1 & 3 & 5 \\\\\n1 & 4 & 4 \\\\\n1 & 5 & 7\n\\end{bmatrix}, \\quad\n\\mathbf{y} = \\begin{bmatrix}\n3 \\\\\n5 \\\\\n7 \\\\\n9 \\\\\n10\n\\end{bmatrix}\n\n\n\nA.4.3 Step 1: Calculate \\mathbf{X}^T\\mathbf{X}\n\n\\mathbf{X}^T = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5 \\\\\n3 & 2 & 5 & 4 & 7\n\\end{bmatrix}\n\n\n\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5 \\\\\n3 & 2 & 5 & 4 & 7\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 1 & 3 \\\\\n1 & 2 & 2 \\\\\n1 & 3 & 5 \\\\\n1 & 4 & 4 \\\\\n1 & 5 & 7\n\\end{bmatrix}\n\nElement-by-element calculation:\nRow 1:\n\n(1,1): 1 + 1 + 1 + 1 + 1 = 5\n(1,2): 1 + 2 + 3 + 4 + 5 = 15\n(1,3): 3 + 2 + 5 + 4 + 7 = 21\n\nRow 2:\n\n(2,1): 1 + 2 + 3 + 4 + 5 = 15\n(2,2): 1 + 4 + 9 + 16 + 25 = 55\n(2,3): 3 + 4 + 15 + 16 + 35 = 73\n\nRow 3:\n\n(3,1): 3 + 2 + 5 + 4 + 7 = 21\n(3,2): 3 + 4 + 15 + 16 + 35 = 73\n(3,3): 9 + 4 + 25 + 16 + 49 = 103\n\n\n\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n5 & 15 & 21 \\\\\n15 & 55 & 73 \\\\\n21 & 73 & 103\n\\end{bmatrix}\n\n\n\nA.4.4 Step 2: Calculate (\\mathbf{X}^T\\mathbf{X})^{-1}\nFor a 3 \\times 3 matrix, we use: \\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})}\\text{adj}(\\mathbf{A})\nStep 2a: Calculate the determinant using cofactor expansion along the first row:\n\n\\det(\\mathbf{X}^T\\mathbf{X}) = 5\\begin{vmatrix}55 & 73 \\\\ 73 & 103\\end{vmatrix} - 15\\begin{vmatrix}15 & 73 \\\\ 21 & 103\\end{vmatrix} + 21\\begin{vmatrix}15 & 55 \\\\ 21 & 73\\end{vmatrix}\n\nCalculate each 2 \\times 2 determinant:\n\n\\begin{vmatrix}55 & 73 \\\\ 73 & 103\\end{vmatrix} = 55(103) - 73(73) = 5665 - 5329 = 336\n\\begin{vmatrix}15 & 73 \\\\ 21 & 103\\end{vmatrix} = 15(103) - 73(21) = 1545 - 1533 = 12\n\\begin{vmatrix}15 & 55 \\\\ 21 & 73\\end{vmatrix} = 15(73) - 55(21) = 1095 - 1155 = -60\n\n\n\\det(\\mathbf{X}^T\\mathbf{X}) = 5(336) - 15(12) + 21(-60) = 1680 - 180 - 1260 = 240\n\nStep 2b: Calculate the cofactor matrix\nThe cofactor C_{ij} is (-1)^{i+j} times the determinant of the matrix obtained by deleting row i and column j.\nC_{11} = (+1)\\begin{vmatrix}55 & 73 \\\\ 73 & 103\\end{vmatrix} = 336\nC_{12} = (-1)\\begin{vmatrix}15 & 73 \\\\ 21 & 103\\end{vmatrix} = -12\nC_{13} = (+1)\\begin{vmatrix}15 & 55 \\\\ 21 & 73\\end{vmatrix} = -60\nC_{21} = (-1)\\begin{vmatrix}15 & 21 \\\\ 73 & 103\\end{vmatrix} = -(15 \\times 103 - 21 \\times 73) = -(1545 - 1533) = -12\nC_{22} = (+1)\\begin{vmatrix}5 & 21 \\\\ 21 & 103\\end{vmatrix} = 5(103) - 21(21) = 515 - 441 = 74\nC_{23} = (-1)\\begin{vmatrix}5 & 15 \\\\ 21 & 73\\end{vmatrix} = -(5 \\times 73 - 15 \\times 21) = -(365 - 315) = -50\nC_{31} = (+1)\\begin{vmatrix}15 & 21 \\\\ 55 & 73\\end{vmatrix} = 15(73) - 21(55) = 1095 - 1155 = -60\nC_{32} = (-1)\\begin{vmatrix}5 & 21 \\\\ 15 & 73\\end{vmatrix} = -(5 \\times 73 - 21 \\times 15) = -(365 - 315) = -50\nC_{33} = (+1)\\begin{vmatrix}5 & 15 \\\\ 15 & 55\\end{vmatrix} = 5(55) - 15(15) = 275 - 225 = 50\nCofactor matrix:\n\n\\mathbf{C} = \\begin{bmatrix}\n336 & -12 & -60 \\\\\n-12 & 74 & -50 \\\\\n-60 & -50 & 50\n\\end{bmatrix}\n\nStep 2c: Calculate the adjugate (transpose of cofactor matrix):\n\n\\text{adj}(\\mathbf{X}^T\\mathbf{X}) = \\mathbf{C}^T = \\begin{bmatrix}\n336 & -12 & -60 \\\\\n-12 & 74 & -50 \\\\\n-60 & -50 & 50\n\\end{bmatrix}\n\nStep 2d: Calculate the inverse:\n\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\frac{1}{240}\\begin{bmatrix}\n336 & -12 & -60 \\\\\n-12 & 74 & -50 \\\\\n-60 & -50 & 50\n\\end{bmatrix} = \\begin{bmatrix}\n1.4 & -0.05 & -0.25 \\\\\n-0.05 & 0.308\\overline{3} & -0.208\\overline{3} \\\\\n-0.25 & -0.208\\overline{3} & 0.208\\overline{3}\n\\end{bmatrix}\n\nFor simplicity in calculations, we’ll use:\n\n(\\mathbf{X}^T\\mathbf{X})^{-1} \\approx \\begin{bmatrix}\n1.400 & -0.050 & -0.250 \\\\\n-0.050 & 0.308 & -0.208 \\\\\n-0.250 & -0.208 & 0.208\n\\end{bmatrix}\n\n\n\nA.4.5 Step 3: Calculate \\mathbf{X}^T\\mathbf{y}\n\n\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5 \\\\\n3 & 2 & 5 & 4 & 7\n\\end{bmatrix}\n\\begin{bmatrix}\n3 \\\\\n5 \\\\\n7 \\\\\n9 \\\\\n10\n\\end{bmatrix}\n\nElement-by-element calculation:\n\nFirst: 3 + 5 + 7 + 9 + 10 = 34\nSecond: 3 + 10 + 21 + 36 + 50 = 120\nThird: 9 + 10 + 35 + 36 + 70 = 160\n\n\n\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n34 \\\\\n120 \\\\\n160\n\\end{bmatrix}\n\n\n\nA.4.6 Step 4: Calculate \\hat{\\boldsymbol{\\beta}}\n\n\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix}\n1.400 & -0.050 & -0.250 \\\\\n-0.050 & 0.308 & -0.208 \\\\\n-0.250 & -0.208 & 0.208\n\\end{bmatrix}\n\\begin{bmatrix}\n34 \\\\\n120 \\\\\n160\n\\end{bmatrix}\n\nElement-by-element calculation:\n\n\\hat{\\beta}_0 = 1.400(34) - 0.050(120) - 0.250(160) = 47.6 - 6.0 - 40.0 = 1.6\n\\hat{\\beta}_1 = -0.050(34) + 0.308(120) - 0.208(160) = -1.7 + 37.0 - 33.3 = 2.0\n\\hat{\\beta}_2 = -0.250(34) - 0.208(120) + 0.208(160) = -8.5 - 25.0 + 33.3 = -0.2\n\n\n\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix}\n1.6 \\\\\n2.0 \\\\\n-0.2\n\\end{bmatrix}\n\nResult: The fitted regression is \\hat{y} = 1.6 + 2.0x_1 - 0.2x_2\n(Note: With exact fractions, the result would be slightly different, but this illustrates the manual calculation process.)\n\n\n\nA.5 The Gauss-Markov Theorem\n\nA.5.1 Statement of the Theorem\nUnder the classical linear regression assumptions, the OLS estimator \\hat{\\boldsymbol{\\beta}} is the Best Linear Unbiased Estimator (BLUE) of \\boldsymbol{\\beta}.\n“Best” means it has the minimum variance among all linear unbiased estimators.\n\n\nA.5.2 Required Assumptions\n\nLinearity: \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\nStrict Exogeneity: E[\\boldsymbol{\\epsilon}|\\mathbf{X}] = \\mathbf{0}\nNo Perfect Multicollinearity: \\mathbf{X} has full column rank, i.e., \\text{rank}(\\mathbf{X}) = p+1\nSpherical Errors: \\text{Var}(\\boldsymbol{\\epsilon}|\\mathbf{X}) = \\sigma^2\\mathbf{I}_n\n\nHomoscedasticity: \\text{Var}(\\epsilon_i) = \\sigma^2 for all i\nNo autocorrelation: \\text{Cov}(\\epsilon_i, \\epsilon_j) = 0 for i \\neq j\n\n\n\n\nA.5.3 Proof that OLS is Unbiased\nStarting with: \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\nSubstitute \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}:\n\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon})\n\n\n= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}\n\n\n= \\mathbf{I}\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}\n\n\n= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}\n\nTaking expectations:\n\nE[\\hat{\\boldsymbol{\\beta}}|\\mathbf{X}] = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T E[\\boldsymbol{\\epsilon}|\\mathbf{X}]\n\nSince E[\\boldsymbol{\\epsilon}|\\mathbf{X}] = \\mathbf{0}:\n\nE[\\hat{\\boldsymbol{\\beta}}|\\mathbf{X}] = \\boldsymbol{\\beta}\n\nTherefore, OLS is unbiased.\n\n\nA.5.4 Variance of the OLS Estimator\nFrom \\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}:\n\n\\text{Var}(\\hat{\\boldsymbol{\\beta}}|\\mathbf{X}) = \\text{Var}[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}|\\mathbf{X}]\n\nSince (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T is non-random conditional on \\mathbf{X}:\n\n= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\text{Var}(\\boldsymbol{\\epsilon}|\\mathbf{X}) \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\nWith spherical errors \\text{Var}(\\boldsymbol{\\epsilon}|\\mathbf{X}) = \\sigma^2\\mathbf{I}:\n\n= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T (\\sigma^2\\mathbf{I}) \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\n\n= \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\n\n= \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\n\nThis is the variance-covariance matrix of \\hat{\\boldsymbol{\\beta}}.\n\n\nA.5.5 Example: Variance Calculation for Simple Regression\nFrom Case 1, we had:\n\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\begin{bmatrix}\n1.1 & -0.3 \\\\\n-0.3 & 0.1\n\\end{bmatrix}\n\nWe estimate \\sigma^2 using the residual sum of squares:\n\n\\hat{\\sigma}^2 = \\frac{\\text{RSS}}{n - (p + 1)} = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n - 2}\n\nFor our example, the fitted values are: \\hat{y}_i = 2.2 + 0.6x_i\n\n\n\n\n\n\n\n\n\n\ni\ny_i\n\\hat{y}_i\ny_i - \\hat{y}_i\n(y_i - \\hat{y}_i)^2\n\n\n\n\n1\n2\n2.8\n-0.8\n0.64\n\n\n2\n4\n3.4\n0.6\n0.36\n\n\n3\n5\n4.0\n1.0\n1.00\n\n\n4\n4\n4.6\n-0.6\n0.36\n\n\n5\n5\n5.2\n-0.2\n0.04\n\n\n\n\n\\text{RSS} = 0.64 + 0.36 + 1.00 + 0.36 + 0.04 = 2.4\n\n\n\\hat{\\sigma}^2 = \\frac{2.4}{5 - 2} = \\frac{2.4}{3} = 0.8\n\nThe variance-covariance matrix of \\hat{\\boldsymbol{\\beta}} is:\n\n\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = 0.8 \\begin{bmatrix}\n1.1 & -0.3 \\\\\n-0.3 & 0.1\n\\end{bmatrix} = \\begin{bmatrix}\n0.88 & -0.24 \\\\\n-0.24 & 0.08\n\\end{bmatrix}\n\nStandard errors:\n\n\\text{SE}(\\hat{\\beta}_0) = \\sqrt{0.88} = 0.938\n\\text{SE}(\\hat{\\beta}_1) = \\sqrt{0.08} = 0.283\n\nThese standard errors are used for hypothesis testing and confidence intervals.\n\n\nA.5.6 Why OLS is “Best” (Sketch of Proof)\nThe Gauss-Markov theorem states that among all linear unbiased estimators, OLS has the minimum variance.\nConsider any other linear unbiased estimator:\n\n\\tilde{\\boldsymbol{\\beta}} = \\mathbf{C}\\mathbf{y}\n\nwhere \\mathbf{C} is some matrix. For unbiasedness:\n\nE[\\tilde{\\boldsymbol{\\beta}}] = \\mathbf{C}E[\\mathbf{y}] = \\mathbf{C}\\mathbf{X}\\boldsymbol{\\beta} = \\boldsymbol{\\beta}\n\nThis requires \\mathbf{C}\\mathbf{X} = \\mathbf{I}.\nWe can write \\mathbf{C} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D} for some matrix \\mathbf{D} where \\mathbf{D}\\mathbf{X} = \\mathbf{0}.\nThe variance of \\tilde{\\boldsymbol{\\beta}} is:\n\n\\text{Var}(\\tilde{\\boldsymbol{\\beta}}) = \\sigma^2\\mathbf{C}\\mathbf{C}^T\n\n\n= \\sigma^2[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D}][(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D}]^T\n\n\n= \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} + \\sigma^2\\mathbf{D}\\mathbf{D}^T\n\nSince \\mathbf{D}\\mathbf{D}^T is positive semi-definite:\n\n\\text{Var}(\\tilde{\\boldsymbol{\\beta}}) - \\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2\\mathbf{D}\\mathbf{D}^T \\geq \\mathbf{0}\n\nTherefore, OLS has the smallest variance among all linear unbiased estimators.\n\n\n\nA.6 Key Takeaways\n\nMatrix notation provides an elegant and scalable framework for regression that extends naturally from simple to multiple regression.\nThe OLS minimization derives from setting \\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = \\mathbf{0}, yielding the normal equations \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}.\nThe OLS solution \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} requires that \\mathbf{X}^T\\mathbf{X} be invertible (no perfect multicollinearity).\nUnder the Gauss-Markov assumptions, OLS is BLUE: Best (minimum variance), Linear, Unbiased Estimator.\nThe variance of OLS estimates is \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}, which can be used for inference.\nKey linear algebra tools include matrix transpose, multiplication, determinant, inverse, and rank—all essential for understanding OLS.\nAssumption violations (e.g., heteroscedasticity, autocorrelation) don’t make OLS biased, but they invalidate the “best” property and standard inference procedures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_pl.html",
    "href": "correg_pl.html",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "",
    "text": "10.1 Wprowadzenie\nRóżnica między korelacją (correlation) a przyczynowością/kausalnością (causation) to jedno z podstawowych wyzwań w analizie statystycznej. Korelacja mierzy statystyczny związek między zmiennymi, natomiast przyczynowość oznacza bezpośredni wpływ jednej zmiennej na drugą.\nZależności statystyczne stanowią fundament podejmowania decyzji opartych na danych w wielu dyscyplinach — od ekonomii i zdrowia publicznego po psychologię i nauki o środowisku. Zrozumienie, kiedy związek wskazuje jedynie na asocjację (association), a kiedy na prawdziwą kausalność (genuine causality), jest kluczowe dla poprawnych wniosków i skutecznych rekomendacji politycznych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kowariancja-covariance",
    "href": "correg_pl.html#kowariancja-covariance",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.2 Kowariancja (Covariance)",
    "text": "10.2 Kowariancja (Covariance)\nKowariancja (covariance) mierzy, w jaki sposób dwie zmienne współzmieniają się, wskazując zarówno kierunek, jak i siłę ich liniowego związku.\nWzór (z próby):\n\n\\operatorname{cov}(X,Y)\n= \\frac{\\sum_{i=1}^{n} (x_i - \\bar x)(y_i - \\bar y)}{n - 1}.\n\nGdzie:\n\nx_i i y_i to poszczególne obserwacje,\n\\bar{x} i \\bar{y} to średnie odpowiednio zmiennych X i Y,\nn to liczba obserwacji,\ndzielimy przez (n-1), ponieważ liczymy kowariancję z próby (tzw. poprawka Bessela; Bessel’s correction).\n\n\nObliczenia ręczne krok po kroku (Step-by-Step Manual Calculation Process)\nPrzykład 1: Godziny nauki a wyniki testu\nDane:\n\nGodziny nauki (X): 2, 4, 6, 8, 10\nWyniki testu (Y): 65, 70, 80, 85, 95\n\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nOblicz średnie\n\\bar{x}=\\frac{2+4+6+8+10}{5}=6 godz.\n\n\n\n\n\\bar{y}=\\frac{65+70+80+85+95}{5}=79 pkt\n\n\n2\nOdchylenia od średnich\n(x_i-\\bar{x}): -4, -2, 0, 2, 4\n\n\n\n\n(y_i-\\bar{y}): -14, -9, 1, 6, 16\n\n\n3\nIloczyny odchyleń\n(x_i-\\bar{x})(y_i-\\bar{y}): 56, 18, 0, 12, 64\n\n\n4\nSuma iloczynów\n\\sum = 56+18+0+12+64=150\n\n\n5\nPodziel przez (n-1)\n\\operatorname{cov}(X,Y)=\\frac{150}{5-1}=\\frac{150}{4}=37.5\n\n\n\nWeryfikacja w R (R Verification):\n\n# Definicja danych\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Kowariancja\ncov_manual &lt;- cov(study_hours, test_scores)\nprint(paste(\"Covariance:\", round(cov_manual, 2)))\n\n[1] \"Covariance: 37.5\"\n\n# Weryfikacja krok po kroku\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\nproducts &lt;- x_dev * y_dev\ncov_calculated &lt;- sum(products) / (length(study_hours) - 1)\n\n# Tabela obliczeń\ndata.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_deviation = x_dev,\n  Y_deviation = y_dev,\n  Product = products\n)\n\n   X  Y X_deviation Y_deviation Product\n1  2 65          -4         -14      56\n2  4 70          -2          -9      18\n3  6 80           0           1       0\n4  8 85           2           6      12\n5 10 95           4          16      64\n\n\nInterpretacja: Dodatnia kowariancja (37.5) wskazuje, że wraz ze wzrostem liczby godzin nauki rosną także wyniki testu — zmienne mają tendencję do wspólnego wzrostu.\n\n\nZadanie ćwiczeniowe z rozwiązaniem (Practice Problem with Solution)\nPolicz ręcznie kowariancję dla:\n\nTemperatura (°F): 32, 50, 68, 86, 95\nSprzedaż lodów ($): 100, 200, 400, 600, 800\n\nRozwiązanie:\n\n\n\n\n\n\n\nKrok\nObliczenie\n\n\n\n\n1. Średnie\n\\bar{x}=\\frac{32+50+68+86+95}{5}=66.2^{\\circ}\\mathrm{F}\n\n\n\n\\bar{y}=\\frac{100+200+400+600+800}{5}=420\n\n\n2. Odchylenia\nX: -34.2, -16.2, 1.8, 19.8, 28.8\n\n\n\nY: -320, -220, -20, 180, 380\n\n\n3. Iloczyny\n10944, 3564, -36, 3564, 10944\n\n\n4. Suma\n28980\n\n\n5. Kowariancja\n\\frac{28980}{4}=7245\n\n\n\n\n# Weryfikacja zadania\ntemp &lt;- c(32, 50, 68, 86, 95)\nsales &lt;- c(100, 200, 400, 600, 800)\nprint(paste(\"Covariance for practice problem:\", round(cov(temp, sales), 2)))\n\n[1] \"Covariance for practice problem: 7245\"",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#współczynnik-korelacji-correlation-coefficient",
    "href": "correg_pl.html#współczynnik-korelacji-correlation-coefficient",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.3 Współczynnik korelacji (Correlation Coefficient)",
    "text": "10.3 Współczynnik korelacji (Correlation Coefficient)\nWspółczynnik korelacji (correlation coefficient) standaryzuje kowariancję, usuwając zależność od skali i przyjmując wartości od -1 do +1.\n\nWskazówki interpretacyjne (Interpretation Guidelines)\n\n\n\n\n\n\n\n\n\nWartość korelacji\nSiła\nInterpretacja\nPrzykład\n\n\n\n\n±0.90 do ±1.00\nBardzo silna\nNiemal doskonały związek\nWzrost rodziców i dzieci\n\n\n±0.70 do ±0.89\nSilna\nZmienne silnie powiązane\nCzas nauki i oceny\n\n\n±0.50 do ±0.69\nUmiarkowana\nUmiarkowany związek\nĆwiczenia a spadek masy\n\n\n±0.30 do ±0.49\nSłaba\nSłaby związek\nRozmiar buta a umiejętność czytania\n\n\n±0.00 do ±0.29\nBardzo słaba/brak\nZnikomy lub brak związku\nMiesiąc urodzenia a inteligencja\n\n\n\n\n\nWizualizacja typów zależności korelacyjnych (Types of Correlations Visualization)\n\n# Generowanie przykładowych danych dla różnych wzorców korelacji\nn &lt;- 100\n\n# Dodatnia zależność liniowa\nset.seed(123)\nyears_education &lt;- rnorm(n, 14, 3)\nannual_income &lt;- 15000 + 3500 * years_education + rnorm(n, 0, 10000)\npos_cor &lt;- round(cor(years_education, annual_income), 3)\n\n# Ujemna zależność liniowa\nscreen_time &lt;- runif(n, 1, 8)\nsleep_hours &lt;- 9 - 0.5 * screen_time + rnorm(n, 0, 0.5)\nneg_cor &lt;- round(cor(screen_time, sleep_hours), 3)\n\n# Brak korelacji\nx_random &lt;- rnorm(n, 50, 10)\ny_random &lt;- rnorm(n, 50, 10)\nno_cor &lt;- round(cor(x_random, y_random), 3)\n\n# Nieliniowa zależność (kwadratowa)\nhours_studied &lt;- runif(n, 0, 12)\ntest_score &lt;- -2 * (hours_studied - 6)^2 + 90 + rnorm(n, 0, 5)\nnonlin_cor &lt;- round(cor(hours_studied, test_score), 3)\n\n# Ramki danych z wartościami korelacji\npositive_data &lt;- data.frame(\n  x = years_education, \n  y = annual_income,\n  label = paste0(\"Dodatnia liniowa (r = \", pos_cor, \")\")\n)\n\nnegative_data &lt;- data.frame(\n  x = screen_time, \n  y = sleep_hours,\n  label = paste0(\"Ujemna liniowa (r = \", neg_cor, \")\")\n)\n\nno_corr_data &lt;- data.frame(\n  x = x_random, \n  y = y_random,\n  label = paste0(\"Brak korelacji (r = \", no_cor, \")\")\n)\n\nnonlinear_data &lt;- data.frame(\n  x = hours_studied, \n  y = test_score,\n  label = paste0(\"Nieliniowa (Pearson r = \", nonlin_cor, \")\")\n)\n\n# Połączenie danych\nall_data &lt;- rbind(positive_data, negative_data, no_corr_data, nonlinear_data)\n\n# Wykres fasetowy\nggplot(all_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE, alpha = 0.3) +\n  facet_wrap(~ label, scales = \"free\", ncol = 2) +\n  labs(\n    title = \"Różne typy korelacji\",\n    subtitle = \"Linia regresji liniowej (na czerwono) z pasmem ufności\",\n    x = \"\", \n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, color = \"gray50\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#korelacja-pearsona-pearson-correlation",
    "href": "correg_pl.html#korelacja-pearsona-pearson-correlation",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.4 Korelacja Pearsona (Pearson Correlation)",
    "text": "10.4 Korelacja Pearsona (Pearson Correlation)\nWzór:\n\nr\n= \\frac{\\operatorname{cov}(X,Y)}{s_X\\, s_Y}\n= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}\n{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2}\\,\\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}}.\n\n\nPełny przykład obliczeń ręcznych (Complete Manual Calculation Example)\nNa danych o godzinach nauki:\n\nGodziny nauki (X): 2, 4, 6, 8, 10\nWyniki testu (Y): 65, 70, 80, 85, 95\n\nSzczegółowe kroki:\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nKowariancja\nZ powyżej: \\operatorname{cov}(X,Y) = 37.5\n\n\n2\nKwadraty odchyleń\n\n\n\n\nDla X\n(x_i-\\bar{x})^2: 16, 4, 0, 4, 16\n\n\n\n\nSuma = 40\n\n\n\nDla Y\n(y_i-\\bar{y})^2: 196, 81, 1, 36, 256\n\n\n\n\nSuma = 570\n\n\n3\nOdchylenia standardowe (standard deviations)\n\n\n\n\ns_X\ns_X=\\sqrt{\\frac{40}{4}}=\\sqrt{10}=3.162\n\n\n\ns_Y\ns_Y=\\sqrt{\\frac{570}{4}}=\\sqrt{142.5}=11.937\n\n\n4\nKorelacja\nr=\\frac{37.5}{3.162 \\times 11.937}\n\n\n\n\nr=\\frac{37.5}{37.73}\\approx 0.994\n\n\n\n\n# Weryfikacja obliczeń\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Współczynnik korelacji Pearsona\nr_value &lt;- cor(study_hours, test_scores, method = \"pearson\")\nprint(paste(\"Pearson correlation coefficient:\", round(r_value, 3)))\n\n[1] \"Pearson correlation coefficient: 0.993\"\n\n# Obliczenia szczegółowe\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\n\n# Tabela obliczeń\ncalc_table &lt;- data.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_dev = x_dev,\n  Y_dev = y_dev,\n  X_dev_sq = x_dev^2,\n  Y_dev_sq = y_dev^2,\n  Product = x_dev * y_dev\n)\nprint(calc_table)\n\n   X  Y X_dev Y_dev X_dev_sq Y_dev_sq Product\n1  2 65    -4   -14       16      196      56\n2  4 70    -2    -9        4       81      18\n3  6 80     0     1        0        1       0\n4  8 85     2     6        4       36      12\n5 10 95     4    16       16      256      64\n\n# Statystyki podsumowujące\ncat(\"\\nSummary:\")\n\n\nSummary:\n\ncat(\"\\nSum of (X-mean)^2:\", sum(x_dev^2))\n\n\nSum of (X-mean)^2: 40\n\ncat(\"\\nSum of (Y-mean)^2:\", sum(y_dev^2))\n\n\nSum of (Y-mean)^2: 570\n\ncat(\"\\nSum of products:\", sum(x_dev * y_dev))\n\n\nSum of products: 150\n\ncat(\"\\nStandard deviation of X:\", sd(study_hours))\n\n\nStandard deviation of X: 3.162278\n\ncat(\"\\nStandard deviation of Y:\", sd(test_scores))\n\n\nStandard deviation of Y: 11.93734\n\n# Przedział ufności i p-value\ncor_test &lt;- cor.test(study_hours, test_scores, method = \"pearson\")\nprint(cor_test)\n\n\n    Pearson's product-moment correlation\n\ndata:  study_hours and test_scores\nt = 15, df = 3, p-value = 0.0006431\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8994446 0.9995859\nsample estimates:\n      cor \n0.9933993 \n\n\nInterpretacja: r \\approx 0.994 wskazuje na niemal doskonały dodatni liniowy związek między godzinami nauki a wynikiem testu. Wartość p &lt; 0.05 sugeruje statystyczną istotność tej zależności.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#korelacja-rang-spearmana-spearman-rank-correlation",
    "href": "correg_pl.html#korelacja-rang-spearmana-spearman-rank-correlation",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.5 Korelacja rang Spearmana (Spearman Rank Correlation)",
    "text": "10.5 Korelacja rang Spearmana (Spearman Rank Correlation)\nKorelacja Spearmana mierzy monotoniczne zależności, używając rang zamiast surowych wartości.\nWzór:\n\n\\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2 - 1)},\n\ngdzie d_i to różnica rang dla obserwacji i.\n\nPełny przykład obliczeń ręcznych (Complete Manual Example)\nDane: Wyniki z matematyki i angielskiego\n\n\n\nUczeń\nMatematyka\nAngielski\n\n\n\n\nA\n78\n82\n\n\nB\n85\n90\n\n\nC\n88\n88\n\n\nD\n92\n94\n\n\nE\n95\n96\n\n\n\nRangowanie i obliczenia:\n\n\n\n\n\n\n\n\n\n\n\n\nUczeń\nWynik z mat.\nRanga mat.\nWynik z ang.\nRanga ang.\nd = \\text{ranga mat.} - \\text{ranga ang.}\nd^2\n\n\n\n\nA\n78\n1\n82\n1\n0\n0\n\n\nB\n85\n2\n90\n3\n-1\n1\n\n\nC\n88\n3\n88\n2\n1\n1\n\n\nD\n92\n4\n94\n4\n0\n0\n\n\nE\n95\n5\n96\n5\n0\n0\n\n\n\n\n\n\n\nSuma:\n2\n\n\n\nObliczenie:\n\n\\rho = 1 - \\frac{6 \\times 2}{5(25-1)} = 1 - \\frac{12}{120} = 0.9.\n\n\n# Dane\nmath_scores &lt;- c(78, 85, 88, 92, 95)\nenglish_scores &lt;- c(82, 90, 88, 94, 96)\n\n# Rangi\nrank_table &lt;- data.frame(\n  Student = LETTERS[1:5],\n  Math_Score = math_scores,\n  Math_Rank = rank(math_scores),\n  English_Score = english_scores,\n  English_Rank = rank(english_scores)\n)\n\nrank_table$d &lt;- rank_table$Math_Rank - rank_table$English_Rank\nrank_table$d_squared &lt;- rank_table$d^2\n\nprint(rank_table)\n\n  Student Math_Score Math_Rank English_Score English_Rank  d d_squared\n1       A         78         1            82            1  0         0\n2       B         85         2            90            3 -1         1\n3       C         88         3            88            2  1         1\n4       D         92         4            94            4  0         0\n5       E         95         5            96            5  0         0\n\ncat(\"\\nSum of d^2:\", sum(rank_table$d_squared))\n\n\nSum of d^2: 2\n\n# Korelacja Spearmana\nrho &lt;- cor(math_scores, english_scores, method = \"spearman\")\ncat(\"\\nSpearman correlation (R):\", round(rho, 3))\n\n\nSpearman correlation (R): 0.9\n\n# Obliczenie ręczne\nn &lt;- length(math_scores)\nrho_manual &lt;- 1 - (6 * sum(rank_table$d_squared)) / (n * (n^2 - 1))\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.9",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#tabele-krzyżowe-cross-tabulation-i-dane-kategoryczne",
    "href": "correg_pl.html#tabele-krzyżowe-cross-tabulation-i-dane-kategoryczne",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.6 Tabele krzyżowe (Cross-tabulation) i dane kategoryczne",
    "text": "10.6 Tabele krzyżowe (Cross-tabulation) i dane kategoryczne\nTabela krzyżowa (cross-tabulation, contingency table) pokazuje zależności między zmiennymi kategorycznymi.\n\n# Bardziej realistyczne dane przykładowe\nset.seed(123)\nn_total &lt;- 120\n\n# Poziom edukacji a zatrudnienie\neducation &lt;- factor(\n  c(rep(\"High School\", 40), \n    rep(\"College\", 50), \n    rep(\"Graduate\", 30)),\n  levels = c(\"High School\", \"College\", \"Graduate\")\n)\n\n# Status zatrudnienia z prawdopodobieństwami zależnymi od edukacji\nemployment &lt;- factor(\n  c(\n    # High School: 75% employed\n    sample(c(rep(\"Employed\", 30), rep(\"Unemployed\", 10)), 40),\n    # College: 90% employed  \n    sample(c(rep(\"Employed\", 45), rep(\"Unemployed\", 5)), 50),\n    # Graduate: 95% employed\n    sample(c(rep(\"Employed\", 28), rep(\"Unemployed\", 2)), 30)\n  ),\n  levels = c(\"Employed\", \"Unemployed\")\n)\n\n# Tabela kontyngencji\ncontingency_table &lt;- table(education, employment)\nprint(\"Contingency Table:\")\n\n[1] \"Contingency Table:\"\n\nprint(contingency_table)\n\n             employment\neducation     Employed Unemployed\n  High School       30         10\n  College           45          5\n  Graduate          28          2\n\n# Procenty w wierszach\nprint(\"\\nRow Percentages (% within each education level):\")\n\n[1] \"\\nRow Percentages (% within each education level):\"\n\nprint(round(prop.table(contingency_table, 1) * 100, 1))\n\n             employment\neducation     Employed Unemployed\n  High School     75.0       25.0\n  College         90.0       10.0\n  Graduate        93.3        6.7\n\n# Test niezależności chi-kwadrat (Chi-square test)\nchi_test &lt;- chisq.test(contingency_table)\n\nWarning in chisq.test(contingency_table): Chi-squared approximation may be\nincorrect\n\nprint(chi_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 5.9623, df = 2, p-value = 0.05073",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#ćwiczenia-praktyczne-z-rozwiązaniami-practical-exercises-with-solutions",
    "href": "correg_pl.html#ćwiczenia-praktyczne-z-rozwiązaniami-practical-exercises-with-solutions",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.7 Ćwiczenia praktyczne z rozwiązaniami (Practical Exercises with Solutions)",
    "text": "10.7 Ćwiczenia praktyczne z rozwiązaniami (Practical Exercises with Solutions)\n\nĆwiczenie 1: Ręczne obliczenie korelacji Pearsona (Calculate Pearson Correlation Manually)\nDane:\n\nWzrost (cale): 66, 68, 70, 72, 74\nWaga (funty): 140, 155, 170, 185, 200\n\nRozwiązanie:\n\n# Dane\nheight &lt;- c(66, 68, 70, 72, 74)\nweight &lt;- c(140, 155, 170, 185, 200)\n\n# Krok 1: Średnie\nh_mean &lt;- mean(height)  # 70\nw_mean &lt;- mean(weight)  # 170\n\n# Krok 2: Odchylenia i iloczyny\ncalculation_df &lt;- data.frame(\n  Height = height,\n  Weight = weight,\n  H_dev = height - h_mean,\n  W_dev = weight - w_mean,\n  H_dev_sq = (height - h_mean)^2,\n  W_dev_sq = (weight - w_mean)^2,\n  Product = (height - h_mean) * (weight - w_mean)\n)\n\nprint(calculation_df)\n\n  Height Weight H_dev W_dev H_dev_sq W_dev_sq Product\n1     66    140    -4   -30       16      900     120\n2     68    155    -2   -15        4      225      30\n3     70    170     0     0        0        0       0\n4     72    185     2    15        4      225      30\n5     74    200     4    30       16      900     120\n\n# Krok 3: Korelacja\ncov_hw &lt;- sum(calculation_df$Product) / (length(height) - 1)\nsd_h &lt;- sqrt(sum(calculation_df$H_dev_sq) / (length(height) - 1))\nsd_w &lt;- sqrt(sum(calculation_df$W_dev_sq) / (length(weight) - 1))\nr_manual &lt;- cov_hw / (sd_h * sd_w)\n\ncat(\"\\nManual calculation:\")\n\n\nManual calculation:\n\ncat(\"\\nCovariance:\", round(cov_hw, 2))\n\n\nCovariance: 75\n\ncat(\"\\nSD Height:\", round(sd_h, 2))\n\n\nSD Height: 3.16\n\ncat(\"\\nSD Weight:\", round(sd_w, 2))\n\n\nSD Weight: 23.72\n\ncat(\"\\nCorrelation:\", round(r_manual, 3))\n\n\nCorrelation: 1\n\n# Weryfikacja w R\ncat(\"\\n\\nR calculation:\", round(cor(height, weight), 3))\n\n\n\nR calculation: 1\n\n\n\n\nĆwiczenie 2: Ręczne obliczenie korelacji Spearmana (Calculate Spearman Correlation Manually)\nDane:\n\nRangi uczniów z matematyki: 1, 3, 2, 5, 4\nRangi uczniów z nauk ścisłych (science): 2, 4, 1, 5, 3\n\nRozwiązanie:\n\n# Rangi (już zrankowane)\nmath_rank &lt;- c(1, 3, 2, 5, 4)\nscience_rank &lt;- c(2, 4, 1, 5, 3)\n\n# Różnice\nd &lt;- math_rank - science_rank\nd_squared &lt;- d^2\n\n# Tabela\nrank_df &lt;- data.frame(\n  Student = 1:5,\n  Math_Rank = math_rank,\n  Science_Rank = science_rank,\n  d = d,\n  d_squared = d_squared\n)\n\nprint(rank_df)\n\n  Student Math_Rank Science_Rank  d d_squared\n1       1         1            2 -1         1\n2       2         3            4 -1         1\n3       3         2            1  1         1\n4       4         5            5  0         0\n5       5         4            3  1         1\n\n# Korelacja Spearmana\nn &lt;- length(math_rank)\nsum_d_sq &lt;- sum(d_squared)\nrho_manual &lt;- 1 - (6 * sum_d_sq) / (n * (n^2 - 1))\n\ncat(\"\\nSum of d^2:\", sum_d_sq)\n\n\nSum of d^2: 4\n\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.8\n\ncat(\"\\nSpearman correlation (R):\", round(cor(math_rank, science_rank, method = \"spearman\"), 3))\n\n\nSpearman correlation (R): 0.8\n\n\n\n\nĆwiczenie 3: Interpretacja wyników (Interpretation Practice)\nZinterpretuj następujące wartości korelacji:\n\nr = 0.85 między godzinami treningu a wynikiem sprawdzianu sprawności\nOdpowiedź: Silny dodatni związek. Wraz ze wzrostem liczby godzin treningu wyniki istotnie rosną.\nr = -0.72 między temperaturą na zewnątrz a kosztami ogrzewania\nOdpowiedź: Silny ujemny związek. Wraz ze wzrostem temperatury koszty ogrzewania wyraźnie maleją.\nr = 0.12 między rozmiarem buta a inteligencją\nOdpowiedź: Bardzo słaby/brak istotnego związku. Zmienne są praktycznie niezależne.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#najważniejsze-rzeczy-do-zapamiętania-important-points-to-remember",
    "href": "correg_pl.html#najważniejsze-rzeczy-do-zapamiętania-important-points-to-remember",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.8 Najważniejsze rzeczy do zapamiętania (Important Points to Remember)",
    "text": "10.8 Najważniejsze rzeczy do zapamiętania (Important Points to Remember)\n\nKorelacja mierzy siłę związku: Wartości od -1 do +1.\nKorelacja ≠ przyczynowość (Correlation ≠ Causation): Wysoka korelacja nie dowodzi wpływu jednej zmiennej na drugą.\nDobierz właściwą metodę:\n\nPearson: Związki liniowe dla danych ciągłych.\nSpearman: Związki monotoniczne lub dane rangowe.\n\nSprawdź założenia:\n\nPearson: liniowość i (w praktyce) rozkład zbliżony do normalnego.\nSpearman: wymagana jedynie monotoniczność.\n\nUwaga na obserwacje odstające (outliers): Mogą silnie wpływać na korelację Pearsona.\nZawsze wizualizuj dane: Wykresy pomagają ocenić kształt zależności.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#podsumowanie-drzewko-decyzyjne-do-analizy-korelacji-summary-decision-tree-for-correlation-analysis",
    "href": "correg_pl.html#podsumowanie-drzewko-decyzyjne-do-analizy-korelacji-summary-decision-tree-for-correlation-analysis",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.9 Podsumowanie: drzewko decyzyjne do analizy korelacji (Summary: Decision Tree for Correlation Analysis)",
    "text": "10.9 Podsumowanie: drzewko decyzyjne do analizy korelacji (Summary: Decision Tree for Correlation Analysis)\n\n\n\nWYBÓR WŁAŚCIWEJ MIARY KORELACJI:\n\nCzy dane są liczbowe (numeryczne)?\n├─ TAK → Czy związek jest liniowy?\n│   ├─ TAK → Użyj korelacji PEARSONA\n│   └─ NIE → Czy związek jest monotoniczny?\n│       ├─ TAK → Użyj korelacji SPEARMANA\n│       └─ NIE → Rozważ metody nieliniowe\n└─ NIE → Czy dane są porządkowe (rangi)?\n    ├─ TAK → Użyj korelacji SPEARMANA\n    └─ NIE → Użyj TABEL KRZYŻOWYCH dla danych kategorycznych\n\n\n\nŚciąga (Quick Reference Card)\n\n\n\n\n\n\n\n\n\nMiara\nKiedy używać (Use When)\nWzór (Formula)\nZakres (Range)\n\n\n\n\nKowariancja (Covariance)\nWstępne badanie związku\n\\displaystyle \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n-\\infty do +\\infty\n\n\nPearson r\nZwiązki liniowe, dane ciągłe\n\\displaystyle \\frac{\\operatorname{cov}(X,Y)}{s_X s_Y}\n-1 do +1\n\n\nSpearman \\rho\nZwiązki monotoniczne, rangi\n\\displaystyle 1-\\frac{6\\sum d_i^2}{n(n^2-1)}\n-1 do +1\n\n\nTabele krzyżowe (Cross-tabs)\nZmienne kategoryczne\nZliczenia częstości\nn/d",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#wprowadzenie-do-modelowania-statystycznego-analizy-regresji-i-metody-mnk",
    "href": "correg_pl.html#wprowadzenie-do-modelowania-statystycznego-analizy-regresji-i-metody-mnk",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.10 Wprowadzenie do Modelowania Statystycznego, Analizy Regresji i Metody MNK",
    "text": "10.10 Wprowadzenie do Modelowania Statystycznego, Analizy Regresji i Metody MNK\n\nModele Deterministyczne a Stochastyczne\nW naukach ścisłych i społecznych pracujemy z dwoma fundamentalnie różnymi typami modeli:\nModele deterministyczne (ang. deterministic models) zakładają idealne, dokładne relacje bez żadnej losowości. Na przykład w fizyce:\nd = v \\cdot t\nJeśli prędkość (v) wynosi 50 km/h, a czas (t) wynosi 2 godziny, to odległość (d) wynosi dokładnie 100 km. Bez wariacji (ang. variation), bez niepewności.\nModele stochastyczne (ang. stochastic models) uznają, że relacje nie są idealne – zawierają losowość i niepewność. W naukach społecznych niemal zawsze pracujemy z modelami stochastycznymi, ponieważ:\n\nLudzkie zachowanie jest złożone i zależy od wielu czynników\nNie możemy zmierzyć wszystkiego, co ma znaczenie\nW procesach społecznych istnieje autentyczna losowość\nPomiar sam w sobie zawiera błędy (ang. measurement errors)\n\nModel stochastyczny zapisujemy jako:\nY_i = f(X_i) + \\varepsilon_i\ngdzie:\n\nY_i to wynik (ang. outcome), który chcemy przewidzieć (np. frekwencja wyborcza)\nX_i to nasza zmienna predykcyjna (ang. predictor variable) (np. PKB per capita)\nf(X_i) to część systematyczna (ang. systematic part) – wzorzec, który możemy wyjaśnić\n\\varepsilon_i to część stochastyczna (ang. stochastic part) – błąd losowy (ang. random error) lub niewytłumaczona wariancja (ang. unexplained variation)\n\nPrzykład: Przewidywanie wyników egzaminów na podstawie godzin nauki. Nawet jeśli wiemy, że ktoś uczył się 5 godzin, nie możemy przewidzieć dokładnego wyniku – może miał zły dzień, może materiał był dla niego szczególnie trudny, może jest naturalnie utalentowany. Najlepsze, co możemy zrobić, to przewidzieć średni wynik dla osób, które uczyły się 5 godzin, uznając, że będzie wariancja wokół tej średniej.\nKluczowy wniosek: staramy się modelować systematyczną relację (ang. systematic relationship), jednocześnie uznając, że idealna predykcja jest niemożliwa.\n\n\nWizualizacja Relacji: Wykresy Rozrzutu i Typy Wzorców\nZanim dopasujemy jakikolwiek model, zawsze powinniśmy wizualizować nasze dane. Wykres rozrzutu (ang. scatterplot) przedstawia relację między dwiema zmiennymi. Różne zbiory danych mogą pokazywać bardzo różne wzorce:\n\npar(mfrow = c(2, 3))\nset.seed(123)\nn &lt;- 50\n\n# 1. Silna dodatnia relacja liniowa\nx1 &lt;- runif(n, 0, 10)\ny1 &lt;- 2 + 3*x1 + rnorm(n, 0, 3)\nplot(x1, y1, pch = 19, col = \"steelblue\",\n     main = \"Silna Dodatnia Liniowa\",\n     xlab = \"X\", ylab = \"Y\")\n\n# 2. Słaba dodatnia relacja liniowa\nx2 &lt;- runif(n, 0, 10)\ny2 &lt;- 2 + 1.5*x2 + rnorm(n, 0, 8)\nplot(x2, y2, pch = 19, col = \"steelblue\",\n     main = \"Słaba Dodatnia Liniowa\",\n     xlab = \"X\", ylab = \"Y\")\n\n# 3. Ujemna relacja liniowa\nx3 &lt;- runif(n, 0, 10)\ny3 &lt;- 50 - 2*x3 + rnorm(n, 0, 4)\nplot(x3, y3, pch = 19, col = \"steelblue\",\n     main = \"Ujemna Liniowa\",\n     xlab = \"X\", ylab = \"Y\")\n\n# 4. Relacja krzywoliniowa (kwadratowa)\nx4 &lt;- runif(n, 0, 10)\ny4 &lt;- 10 + 3*x4 - 0.3*x4^2 + rnorm(n, 0, 2)\nplot(x4, y4, pch = 19, col = \"steelblue\",\n     main = \"Krzywoliniowa (Kwadratowa)\",\n     xlab = \"X\", ylab = \"Y\")\n\n# 5. Brak relacji\nx5 &lt;- runif(n, 0, 10)\ny5 &lt;- rnorm(n, 30, 8)\nplot(x5, y5, pch = 19, col = \"steelblue\",\n     main = \"Brak Relacji\",\n     xlab = \"X\", ylab = \"Y\")\n\n# 6. Relacja nieliniowa (wykładnicza)\nx6 &lt;- runif(n, 0, 10)\ny6 &lt;- 5 * exp(0.35*x6) + rnorm(n, 0, 3)\nplot(x6, y6, pch = 19, col = \"steelblue\",\n     main = \"Nieliniowa (Wykładnicza)\",\n     xlab = \"X\", ylab = \"Y\")\n\n\n\n\n\n\n\nFigure 10.1: Różne typy relacji między zmiennymi\n\n\n\n\n\nDlaczego skupiamy się na relacjach liniowych?\nRelacje liniowe (ang. linear relationships) są naszym punktem wyjścia z kilku powodów:\n\nWiele rzeczywistych relacji jest w przybliżeniu liniowych (przynajmniej w ograniczonym zakresie)\nModele liniowe (ang. linear models) są proste, interpretowalny i matematycznie przystępne\nSłużą jako punkt odniesienia (ang. baseline) – jeśli model liniowy nie pasuje dobrze, wiemy, że potrzebujemy czegoś bardziej złożonego\nWiele relacji nieliniowych można przekształcić do postaci liniowej (np. transformacje logarytmiczne)\n\nWażne: Jeśli wykres rozrzutu pokazuje wyraźny wzorzec krzywoliniowy (ang. curvilinear pattern) lub nieliniowy (ang. non-linear pattern), linia prosta będzie słabo dopasowana. Zawsze najpierw sprawdzaj dane!\n\n\nProsty Przykład: Godziny Nauki i Wyniki Egzaminów\nW dalszej części tej sekcji będziemy pracować ze zbiorem danych, który pokazuje w przybliżeniu liniową relację:\n\n# Generujemy przykładowe dane\nset.seed(123)\nn &lt;- 50\nstudy_hours &lt;- runif(n, 0, 10)\nexam_scores &lt;- 50 + 4 * study_hours + rnorm(n, 0, 8)\n\n# Tworzymy wykres rozrzutu\nplot(study_hours, exam_scores,\n     xlab = \"Godziny Nauki\",\n     ylab = \"Wynik Egzaminu\",\n     main = \"Wykres Rozrzutu: Godziny Nauki vs. Wyniki Egzaminów\",\n     pch = 19, col = \"steelblue\")\n\n\n\n\n\n\n\nFigure 10.2: Relacja między godzinami nauki a wynikami egzaminów\n\n\n\n\n\nNa tym wykresie rozrzutu możemy zauważyć ogólną dodatnią relację liniową (ang. positive linear relationship): studenci, którzy uczą się więcej, zwykle osiągają wyższe wyniki. Ale relacja nie jest idealna – jest rozproszenie (ang. scatter) wokół jakiejkolwiek wyimaginowanej linii, którą moglibyśmy narysować przez punkty.\n\n\nIdea Linii “Najlepszego Dopasowania”\nPatrząc na wykres rozrzutu, możemy chcieć podsumować relację prostą linią. Ale którą linią? Możemy narysować nieskończenie wiele linii. Potrzebujemy kryterium (ang. criterion), które określi, co czyni linię “najlepszą”.\nLinia regresji (ang. regression line) lub “linia najlepszego dopasowania” (ang. best fit line) to linia, która minimalizuje sumę kwadratów błędów (SSE). Zrozummy to krok po kroku.\nJeśli proponujemy linię z równaniem \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i, to dla każdej obserwacji i możemy obliczyć:\n\nWartość przewidywaną (ang. predicted value): \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\nResztę (ang. residual) lub błąd (ang. error): e_i = Y_i - \\hat{Y}_i\n\nReszta to pionowa odległość między rzeczywistym punktem a naszą linią. Suma kwadratów błędów (SSE, ang. Sum of Squared Errors) wynosi:\nSSE = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\nPodnosimy błędy do kwadratu z dwóch powodów:\n\nDodatnie i ujemne błędy się nie znoszą\nWiększe błędy są bardziej karane (błąd 4-jednostkowy liczy się jako 16 w SSE, podczas gdy dwa błędy 2-jednostkowe liczą się jako 8)\n\n\n\nWizualizacja Różnych Linii i Ich SSE\nZobaczmy, jak SSE zmienia się dla różnych linii:\n\n# Wypróbujmy trzy różne linie\npar(mfrow = c(2, 2))\n\n# Linia 1: Za płaska\nplot(study_hours, exam_scores, pch = 19, col = \"steelblue\",\n     xlab = \"Godziny Nauki\", ylab = \"Wynik Egzaminu\",\n     main = \"Linia 1: Za Płaska (nachylenie = 2)\")\nabline(a = 55, b = 2, col = \"red\", lwd = 2)\npred1 &lt;- 55 + 2 * study_hours\nsse1 &lt;- sum((exam_scores - pred1)^2)\ntext(2, 90, paste(\"SSE =\", round(sse1, 1)), col = \"red\")\n\n# Linia 2: Za stroma\nplot(study_hours, exam_scores, pch = 19, col = \"steelblue\",\n     xlab = \"Godziny Nauki\", ylab = \"Wynik Egzaminu\",\n     main = \"Linia 2: Za Stroma (nachylenie = 6)\")\nabline(a = 45, b = 6, col = \"orange\", lwd = 2)\npred2 &lt;- 45 + 6 * study_hours\nsse2 &lt;- sum((exam_scores - pred2)^2)\ntext(2, 90, paste(\"SSE =\", round(sse2, 1)), col = \"orange\")\n\n# Linia 3: Linia MNK (najlepsze dopasowanie)\nplot(study_hours, exam_scores, pch = 19, col = \"steelblue\",\n     xlab = \"Godziny Nauki\", ylab = \"Wynik Egzaminu\",\n     main = \"Linia 3: Linia MNK (Najlepsze Dopasowanie)\")\nmodel &lt;- lm(exam_scores ~ study_hours)\nabline(model, col = \"darkgreen\", lwd = 2)\nsse3 &lt;- sum(residuals(model)^2)\ntext(2, 90, paste(\"SSE =\", round(sse3, 1)), col = \"darkgreen\")\n\n# Wizualizacja reszt dla linii MNK\nplot(study_hours, exam_scores, pch = 19, col = \"steelblue\",\n     xlab = \"Godziny Nauki\", ylab = \"Wynik Egzaminu\",\n     main = \"Linia MNK z Resztami\")\nabline(model, col = \"darkgreen\", lwd = 2)\npred3 &lt;- predict(model)\nsegments(study_hours, exam_scores, study_hours, pred3, \n         col = \"red\", lty = 2)\n\n\n\n\n\n\n\nFigure 10.3: Porównanie różnych linii i ich wartości SSE\n\n\n\n\n\nZauważ, że linia MNK ma najmniejsze SSE – to właśnie czyni ją linią “najlepszego” dopasowania (ang. best fit).\n\n\nModel Zerowy: Przewidywanie ze Średniej\nZanim wprowadzimy predyktory, rozważmy najprostszy możliwy model: przewidywanie tej samej wartości dla wszystkich. Jaką wartość powinniśmy przewidzieć?\nModel zerowy (ang. zero model lub intercept-only model) przewiduje średnią dla wszystkich obserwacji:\n\\hat{Y}_i = \\bar{Y}\nMoże się to wydawać bezużyteczne, ale służy jako punkt odniesienia (ang. baseline) do porównania. Każdy predyktor, który dodamy, powinien działać lepiej niż samo zgadywanie średniej.\n\nplot(study_hours, exam_scores, pch = 19, col = \"steelblue\",\n     xlab = \"Godziny Nauki\", ylab = \"Wynik Egzaminu\",\n     main = \"Model Zerowy: Przewidywanie Średniej dla Wszystkich\")\nmean_score &lt;- mean(exam_scores)\nabline(h = mean_score, col = \"purple\", lwd = 2)\ntext(5, mean_score + 5, paste(\"Średnia =\", round(mean_score, 1)), \n     col = \"purple\")\n\n\n\n\n\n\n\nFigure 10.4: Model zerowy: przewidywanie średniej dla wszystkich\n\n\n\n\n\n\n\nDekompozycja Wariancji: Jak Dobry Jest Nasz Model?\nTeraz pojawia się kluczowe pytanie: czy włączenie X (godziny nauki) poprawia nasze predykcje w porównaniu z modelem zerowym?\nMożemy rozłożyć całkowitą wariancję (ang. total variance) w Y na dwie części:\n\nWariancja wyjaśniona (ang. explained variance): zmienność uchwycona przez nasz model\nWariancja niewyjaśniona (ang. unexplained variance): zmienność rezydualna (ang. residual variation) – to, czego nasz model nie wyjaśnia\n\nMatematycznie, dla każdej obserwacji:\n\\underbrace{Y_i - \\bar{Y}}_{\\text{Całkowite odchylenie (total deviation)}} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{\\text{Odchylenie wyjaśnione (explained deviation)}} + \\underbrace{Y_i - \\hat{Y}_i}_{\\text{Reszta (residual)}}\nPodnosząc do kwadratu i sumując po wszystkich obserwacjach:\n\\underbrace{\\sum (Y_i - \\bar{Y})^2}_{\\text{Całkowita Suma Kwadratów (SST)}} = \\underbrace{\\sum (\\hat{Y}_i - \\bar{Y})^2}_{\\text{Suma Kwadratów Regresji (SSR)}} + \\underbrace{\\sum (Y_i - \\hat{Y}_i)^2}_{\\text{Suma Kwadratów Błędów (SSE)}}\nGdzie:\n\nSST = Total Sum of Squares\nSSR = Regression Sum of Squares\nSSE = Sum of Squared Errors\n\nLub bardziej zwięźle:\nSST = SSR + SSE\nPrzykład obliczeń ręcznych:\nZałóżmy, że mamy pięciu studentów:\n\n\n\nStudent\nGodziny Nauki (X)\nWynik Egzaminu (Y)\n\n\n\n\n1\n2\n60\n\n\n2\n4\n73\n\n\n3\n6\n71\n\n\n4\n8\n86\n\n\n5\n10\n90\n\n\n\nKrok 1: Obliczamy średnią: \\bar{Y} = \\frac{60 + 73 + 71 + 86 + 90}{5} = 76\nKrok 2: Obliczamy \\bar{X} = \\frac{2 + 4 + 6 + 8 + 10}{5} = 6\nKrok 3: Dopasowujemy linię MNK (pokażemy wzór poniżej):\nNajpierw obliczamy nachylenie (ang. slope):\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\n1\n2\n60\n-4\n-16\n64\n16\n\n\n2\n4\n73\n-2\n-3\n6\n4\n\n\n3\n6\n71\n0\n-5\n0\n0\n\n\n4\n8\n86\n2\n10\n20\n4\n\n\n5\n10\n90\n4\n14\n56\n16\n\n\n\n\n\n\n\nSuma = 146\nSuma = 40\n\n\n\n\\hat{\\beta}_1 = \\frac{146}{40} = 3.65\n\\hat{\\beta}_0 = 76 - 3.65 \\times 6 = 76 - 21.9 = 54.1\nZatem nasza linia regresji to: \\hat{Y}_i = 54.1 + 3.65X_i\nKrok 4: Obliczamy predykcje (ang. predictions) i komponenty wariancji:\n\n\n\n\n\n\n\n\n\n\n\n\ni\nY_i\n\\bar{Y}\n\\hat{Y}_i\nY_i - \\bar{Y}\n\\hat{Y}_i - \\bar{Y}\nY_i - \\hat{Y}_i\n\n\n\n\n1\n60\n76\n61.4\n-16\n-14.6\n-1.4\n\n\n2\n73\n76\n68.7\n-3\n-7.3\n4.3\n\n\n3\n71\n76\n76.0\n-5\n0.0\n-5.0\n\n\n4\n86\n76\n83.3\n10\n7.3\n2.7\n\n\n5\n90\n76\n90.6\n14\n14.6\n-0.6\n\n\n\nKrok 5: Sumy kwadratów:\n\nSST = (-16)^2 + (-3)^2 + (-5)^2 + 10^2 + 14^2 = 256 + 9 + 25 + 100 + 196 = 586\nSSR = (-14.6)^2 + (-7.3)^2 + 0^2 + 7.3^2 + 14.6^2 = 213.16 + 53.29 + 0 + 53.29 + 213.16 = 532.9\nSSE = (-1.4)^2 + 4.3^2 + (-5.0)^2 + 2.7^2 + (-0.6)^2 = 1.96 + 18.49 + 25 + 7.29 + 0.36 = 53.1\n\nSprawdzenie: 532.9 + 53.1 = 586 ✓\n\n\nZnajdowanie Współczynników MNK: Minimalizacja SSE\nMetoda Najmniejszych Kwadratów (MNK, ang. Ordinary Least Squares, OLS) znajduje współczynniki \\hat{\\beta}_0 (wyraz wolny, ang. intercept) i \\hat{\\beta}_1 (nachylenie, ang. slope), które minimalizują SSE.\nDla prostej regresji liniowej (ang. simple linear regression) Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, estymatory MNK (ang. OLS estimators) to:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} = \\frac{Cov(X, Y)}{Var(X)}\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\nPokazaliśmy już to obliczenie w poprzedniej sekcji. Kluczowy wniosek to:\n\n\\hat{\\beta}_1 mierzy średnią zmianę Y na jednostkę zmiany X\n\\hat{\\beta}_0 to przewidywana wartość Y, gdy X = 0\n\nInterpretacja naszego przykładu: \\hat{Y}_i = 54.1 + 3.65X_i\nKażda dodatkowa godzina nauki jest związana ze wzrostem wyniku egzaminu o 3.65 punktu. Student, który w ogóle się nie uczy (X = 0), uzyskałby przewidywany wynik 54.1 punktu (chociaż ta ekstrapolacja może nie być sensowna).\n\n\nOcena Jakości Modelu: R-kwadrat\nR-kwadrat (R^2, współczynnik determinacji, ang. coefficient of determination) mówi nam, jaka proporcja wariancji w Y jest wyjaśniona przez nasz model:\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nZ naszego przykładu: R^2 = \\frac{532.9}{586} = 0.909 lub 1 - \\frac{53.1}{586} = 0.909\nInterpretacja: 90.9% wariancji w wynikach egzaminów jest wyjaśnione przez godziny nauki. Nasz model działa dobrze, chociaż wciąż jest około 9% wariancji, która pozostaje niewyjaśniona – prawdopodobnie z powodu innych czynników, takich jak naturalne zdolności, lęk przed testem, jakość nauki lub wcześniejsza wiedza.\nWłaściwości R^2:\n\nZakres od 0 do 1\nR^2 = 0: model nie lepszy niż przewidywanie średniej\nR^2 = 1: idealne predykcje (wszystkie punkty na linii)\nWyższe R^2 oznacza lepsze dopasowanie (ang. better fit), ale nie gwarantuje związku przyczynowego ani dobrych predykcji poza próbą (ang. out of sample)\n\n\npar(mfrow = c(1, 3))\n\n# Wysokie R-kwadrat\nset.seed(1)\nx1 &lt;- 1:30\ny1 &lt;- 2 + 3*x1 + rnorm(30, 0, 3)\nplot(x1, y1, pch = 19, col = \"steelblue\", main = paste(\"Wysokie R² =\", \n     round(summary(lm(y1 ~ x1))$r.squared, 2)))\nabline(lm(y1 ~ x1), col = \"red\", lwd = 2)\n\n# Średnie R-kwadrat\ny2 &lt;- 2 + 3*x1 + rnorm(30, 0, 15)\nplot(x1, y2, pch = 19, col = \"steelblue\", main = paste(\"Średnie R² =\", \n     round(summary(lm(y2 ~ x1))$r.squared, 2)))\nabline(lm(y2 ~ x1), col = \"red\", lwd = 2)\n\n# Niskie R-kwadrat\ny3 &lt;- 2 + 3*x1 + rnorm(30, 0, 30)\nplot(x1, y3, pch = 19, col = \"steelblue\", main = paste(\"Niskie R² =\", \n     round(summary(lm(y3 ~ x1))$r.squared, 2)))\nabline(lm(y3 ~ x1), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\nFigure 10.5: Różne wartości R-kwadrat\n\n\n\n\n\n\n\nOcena Jakości Modelu: RMSE\nPodczas gdy R^2 jest niezależne od skali, pierwiastek błędu średniokwadratowego (RMSE, ang. Root Mean Squared Error) podaje nam średni błąd predykcji (ang. prediction error) w oryginalnych jednostkach:\nRMSE = \\sqrt{\\frac{SSE}{n}} = \\sqrt{\\frac{\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2}{n}}\nZ naszego przykładu: RMSE = \\sqrt{\\frac{53.1}{5}} = \\sqrt{10.62} = 3.26 punktów\nInterpretacja: Średnio nasze predykcje są błędne o około 3.3 punktu.\nRMSE jest użyteczne do:\n\nPorównywania modeli dla tej samej zmiennej wynikowej\nZrozumienia dokładności predykcji (ang. prediction accuracy) w znaczących jednostkach\nKarania dużych błędów (jak SSE)\n\n\n# Obliczamy RMSE\nmodel &lt;- lm(exam_scores ~ study_hours)\nrmse &lt;- sqrt(mean(residuals(model)^2))\ncat(\"RMSE:\", round(rmse, 2), \"punktów\\n\")\n\nRMSE: 7.37 punktów\n\ncat(\"R-kwadrat:\", round(summary(model)$r.squared, 3), \"\\n\")\n\nR-kwadrat: 0.743 \n\n\n\n\nInterpretacja Modelu, Wielkość Efektu i Diagnostyka\nPo dopasowaniu (ang. fitting) modelu regresji musimy wyjść poza R^2 i RMSE, aby zrozumieć, co nasz model nam mówi. Obejmuje to trzy kluczowe zadania: interpretację współczynników merytorycznie (ang. substantive interpretation), ocenę wielkości efektów (ang. effect sizes) i sprawdzanie potencjalnych problemów poprzez diagnostykę (ang. diagnostics).\n\nInterpretacja Współczynników Regresji\nWspółczynnik nachylenia \\hat{\\beta}_1 ma specyficzną interpretację:\n\nWzrost X o jedną jednostkę jest związany ze zmianą Y o \\hat{\\beta}_1 jednostek, średnio.\n\nDla naszego przykładu (\\hat{Y}_i = 54.1 + 3.65X_i):\n\nInterpretacja nachylenia: Każda dodatkowa godzina nauki jest związana ze wzrostem wyniku egzaminu o 3.65 punktu, średnio.\nInterpretacja wyrazu wolnego: Student z zerowymi godzinami nauki otrzymałby przewidywany wynik 54.1 punktu (chociaż może to obejmować ekstrapolację (ang. extrapolation) poza zakres danych).\n\nWażne uwagi:\n\nWspółczynnik opisuje związek (ang. association), niekoniecznie przyczynowość (ang. causation)\nRelacja jest liniowa – zakładamy, że efekt jest stały w całym zakresie X\nEfekt jest średni – indywidualni studenci mogą się różnić\n\nPrzykład praktycznej interpretacji:\nJeśli uczysz się 3 godziny vs. 6 godzin (różnica 3 godzin), przewidywana różnica wyniku to:\n3 \\times 3.65 = 10.95 \\text{ punktów}\nTo daje nam praktyczne poczucie wielkości relacji (ang. magnitude of the relationship).\n\n\nWielkość Efektu: Współczynniki Standaryzowane\nPrzy porównywaniu efektów między różnymi zmiennymi lub badaniami, surowe współczynniki (ang. raw coefficients) mogą być mylące, ponieważ zależą od jednostek pomiaru (ang. units of measurement). Czy wzrost o 3.65 punktu na godzinę jest “duży” czy “mały”?\nWspółczynniki standaryzowane (współczynniki beta, ang. standardized coefficients lub beta coefficients) wyrażają efekt w jednostkach odchylenia standardowego (ang. standard deviation units):\n\\hat{\\beta}_1^{*} = \\hat{\\beta}_1 \\times \\frac{SD_X}{SD_Y}\nTo mówi nam: wzrost X o jedno odchylenie standardowe jest związany ze zmianą Y o \\hat{\\beta}_1^{*} odchylenia standardowego.\nObliczenie ręczne dla naszego przykładu:\nNajpierw obliczamy odchylenia standardowe:\n\nSD_X = \\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{n-1}} = \\sqrt{\\frac{40}{4}} = \\sqrt{10} = 3.16 godzin\nSD_Y = \\sqrt{\\frac{\\sum(Y_i - \\bar{Y})^2}{n-1}} = \\sqrt{\\frac{586}{4}} = \\sqrt{146.5} = 12.10 punktów\n\nNastępnie:\n\\hat{\\beta}_1^{*} = 3.65 \\times \\frac{3.16}{12.10} = 3.65 \\times 0.261 = 0.953\nInterpretacja: Wzrost godzin nauki o jedno odchylenie standardowe (około 3.2 godziny) jest związany ze wzrostem wyników egzaminów o 0.953 odchylenia standardowego (około 11.5 punktu).\nZasada kciuka dla wielkości efektów:\n\nMały efekt (ang. small effect): |\\hat{\\beta}_1^{*}| \\approx 0.1\nŚredni efekt (ang. medium effect): |\\hat{\\beta}_1^{*}| \\approx 0.3\nDuży efekt (ang. large effect): |\\hat{\\beta}_1^{*}| \\approx 0.5 lub wyższy\n\nNasz efekt (0.953) jest bardzo duży, sugerując, że godziny nauki mają znaczący związek z wynikami egzaminów.\n\n# Obliczamy współczynnik standaryzowany\nmodel &lt;- lm(exam_scores ~ study_hours)\nbeta_raw &lt;- coef(model)[2]\nbeta_std &lt;- beta_raw * (sd(study_hours) / sd(exam_scores))\ncat(\"Współczynnik surowy:\", round(beta_raw, 3), \"\\n\")\n\nWspółczynnik surowy: 4.305 \n\ncat(\"Współczynnik standaryzowany:\", round(beta_std, 3), \"\\n\")\n\nWspółczynnik standaryzowany: 0.862 \n\n\n\n\nWykresy Diagnostyczne: Analiza Reszt\nPo dopasowaniu modelu powinniśmy sprawdzić, czy nasze założenia (ang. assumptions) są spełnione. Najbardziej użytecznym narzędziem diagnostycznym jest wykres reszt (ang. residual plot): wykres reszt (e_i = Y_i - \\hat{Y}_i) względem wartości przewidywanych (ang. fitted values) (\\hat{Y}_i).\nCzego szukać:\n\nLosowe rozproszenie (ang. random scatter): Reszty powinny być losowo rozproszone wokół zera\nStałe rozproszenie: Rozproszenie pionowe powinno być w przybliżeniu stałe (homoskedastyczność, ang. homoscedasticity)\nBrak wzorców (ang. no patterns): Zakrzywione wzorce sugerują nieliniowość (ang. non-linearity)\nBrak wartości odstających (ang. no outliers): Ekstremalnie duże reszty mogą wskazywać problemy\n\n\npar(mfrow = c(2, 2))\n\n# 1. Reszty vs. Dopasowane\nmodel &lt;- lm(exam_scores ~ study_hours)\nplot(fitted(model), residuals(model),\n     xlab = \"Wartości Dopasowane\",\n     ylab = \"Reszty\",\n     main = \"Reszty vs. Dopasowane\",\n     pch = 19, col = \"steelblue\")\nabline(h = 0, col = \"red\", lty = 2, lwd = 2)\n\n# 2. Histogram reszt\nhist(residuals(model), \n     main = \"Histogram Reszt\",\n     xlab = \"Reszty\",\n     col = \"lightblue\",\n     breaks = 10)\n\n# 3. Wykres Q-Q (sprawdzanie normalności)\nqqnorm(residuals(model), pch = 19, col = \"steelblue\")\nqqline(residuals(model), col = \"red\", lwd = 2)\n\n# 4. Reszty vs. Predyktor\nplot(study_hours, residuals(model),\n     xlab = \"Godziny Nauki\",\n     ylab = \"Reszty\",\n     main = \"Reszty vs. Godziny Nauki\",\n     pch = 19, col = \"steelblue\")\nabline(h = 0, col = \"red\", lty = 2, lwd = 2)\n\n\n\n\n\n\n\nFigure 10.6: Wykresy diagnostyczne reszt\n\n\n\n\n\nInterpretacja wykresu reszt:\n\nDobre: Punkty losowo rozproszone wokół zera bez wzorca\nZłe - kształt lejka (ang. funnel shape): Wariancja rośnie wraz z wartościami dopasowanymi (heteroskedastyczność, ang. heteroscedasticity)\nZłe - zakrzywiony wzorzec (ang. curved pattern): Relacja nieliniowa nie uchwycona przez model\nZłe - grupowanie (ang. clustering): Możliwe podgrupy (ang. subgroups) lub brakujące zmienne (ang. missing variables)\n\n\n\nIdentyfikacja Wartości Odstających i Punktów Wpływowych\nNie wszystkie punkty danych są równie ważne. Niektóre obserwacje mogą mieć nieproporcjonalny wpływ (ang. disproportionate influence) na linię regresji.\nTypy niezwykłych obserwacji:\n\nWartości odstające (ang. outliers): Obserwacje z dużymi resztami (daleko od linii regresji)\nPunkty o wysokiej dźwigni (ang. high leverage points): Obserwacje z ekstremalnymi wartościami X\nPunkty wpływowe (ang. influential points): Obserwacje, które po usunięciu znacząco zmienią linię regresji\n\nWykrywanie wartości odstających ręcznie:\nDla naszego przykładu pięciu studentów zbadajmy reszty:\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i\ne_i\n|e_i|\n\n\n\n\n1\n2\n60\n61.4\n-1.4\n1.4\n\n\n2\n4\n73\n68.7\n4.3\n4.3\n\n\n3\n6\n71\n76.0\n-5.0\n5.0\n\n\n4\n8\n86\n83.3\n2.7\n2.7\n\n\n5\n10\n90\n90.6\n-0.6\n0.6\n\n\n\nStudent 3 ma największą resztę (5.0 punktów poniżej przewidywanej). Czy to wartość odstająca?\nZasada kciuka: Reszta jest niepokojąca, jeśli |e_i| &gt; 2 \\times RMSE\nW naszym przypadku: 2 \\times 3.26 = 6.52 punktów. Reszta Studenta 3 (5.0) jest poniżej tego progu (ang. threshold), więc nie jest poważną wartością odstającą.\nWizualizacja potencjalnych wartości odstających:\n\npar(mfrow = c(1, 2))\n\n# Wykres 1: Podświetlenie potencjalnych wartości odstających\nmodel &lt;- lm(exam_scores ~ study_hours)\nplot(study_hours, exam_scores,\n     xlab = \"Godziny Nauki\",\n     ylab = \"Wynik Egzaminu\",\n     main = \"Identyfikacja Wartości Odstających\",\n     pch = 19, col = \"steelblue\")\nabline(model, col = \"red\", lwd = 2)\n\n# Identyfikujemy punkty z dużymi resztami\nlarge_resid &lt;- abs(residuals(model)) &gt; 2 * sd(residuals(model))\nif(any(large_resid)) {\n  points(study_hours[large_resid], exam_scores[large_resid],\n         col = \"orange\", pch = 19, cex = 2)\n  text(study_hours[large_resid], exam_scores[large_resid],\n       labels = which(large_resid), pos = 3, col = \"orange\")\n}\n\n# Wykres 2: Wykres dźwigni\nplot(hatvalues(model),\n     xlab = \"Numer Obserwacji\",\n     ylab = \"Dźwignia (Leverage)\",\n     main = \"Dźwignia Każdego Punktu\",\n     pch = 19, col = \"steelblue\")\nabline(h = 2 * length(coef(model)) / length(exam_scores), \n       col = \"red\", lty = 2)\n\n\n\n\n\n\n\nFigure 10.7: Identyfikacja wartości odstających i punktów wpływowych\n\n\n\n\n\nCo zrobić z wartościami odstającymi:\n\nZbadać (ang. investigate): Sprawdzić, czy to błąd wprowadzania danych (ang. data entry error)\nZrozumieć: Co sprawia, że ta obserwacja jest niezwykła?\nRaportować (ang. report): Odnotować wartość odstającą i rozważyć analizę z nią i bez niej\nNie usuwać automatycznie: Wartości odstające mogą zawierać ważne informacje\n\nNa przykład, Student 3 może mieć lęk przed testem (ang. test anxiety) pomimo odpowiedniego przygotowania lub może uczyć się inaczej. To są merytorycznie znaczące informacje, a nie tylko “szum”.\n\n\nPodsumowanie Diagnostyki\nWskaźniki dobrego modelu:\n\nReszty losowo rozproszone wokół zera\nStała wariancja w całym zakresie predykcji\nR^2 jest rozsądnie wysokie (zależne od kontekstu)\nRMSE jest małe względem skali Y\nNiewiele lub brak ekstremalnych wartości odstających\nWspółczynniki standaryzowane sugerują znaczące wielkości efektów\n\nSygnały ostrzegawcze (ang. warning signs):\n\nSystematyczne wzorce (ang. systematic patterns) na wykresach reszt\nReszty w kształcie lejka (heteroskedastyczność)\nWiele wpływowych wartości odstających\nBardzo niskie R^2 (model wyjaśnia niewiele wariancji)\nWspółczynniki, które nie mają sensu merytorycznego (ang. substantive sense)\n\nZawsze pamiętaj: diagnostykę statystyczną należy łączyć z wiedzą merytoryczną (ang. substantive knowledge) o pytaniu badawczym. Model statystycznie “dobry”, który nie ma sensu teoretycznego, jest wciąż problematyczny.\n\n\n\nKorelacja vs. Przyczynowość i Relacje Pozorne\nJedna z najważniejszych lekcji w statystyce to: korelacja nie implikuje przyczynowości (ang. correlation does not imply causation). Sama obecność związku między dwoma zmiennymi nie oznacza, że jedna powoduje drugą. Ta sekcja bada, dlaczego to rozróżnienie ma znaczenie i jak korelacje pozorne (ang. spurious correlations) mogą nas wprowadzać w błąd.\n\nFundamentalny Problem\nKiedy znajdujemy, że X i Y są skorelowane (poruszają się razem), istnieje kilka możliwych wyjaśnień:\n\nX powoduje Y: Zmiany w X bezpośrednio powodują zmiany w Y\nY powoduje X: Strzałka przyczynowa (ang. causal arrow) biegnie w drugą stronę\nZakłócenie (ang. confounding): Trzecia zmienna Z powoduje zarówno X, jak i Y\nPrzypadek: Relacja jest pozorna – tylko przypadkowy zbieg okoliczności\n\nAnaliza regresji może nam powiedzieć, że X i Y są związane (ang. associated), ale nie może nam powiedzieć dlaczego są związane. Aby twierdzić o przyczynowości, potrzebujemy dodatkowych dowodów poza korelacją.\n\n\nKlasyczne Przykłady Korelacji Bez Przyczynowości\nPrzykład 1: Sprzedaż lodów i zgony przez utonięcie\nObserwacja: Kraje z wyższą sprzedażą lodów mają też więcej zgonów przez utonięcie. Czy powinniśmy zakazać lodów, aby zapobiec utonięciom?\nNie! Zmienna zakłócająca (ang. confounding variable lub confounder) to temperatura. Gorąca pogoda powoduje zarówno:\n\nWięcej sprzedaży lodów (ludzie chłodzą się lodami)\nWięcej zgonów przez utonięcie (ludzie częściej pływają)\n\n\nset.seed(42)\nn &lt;- 30\ntemperature &lt;- rnorm(n, 25, 5)\nice_cream &lt;- 50 + 3 * temperature + rnorm(n, 0, 10)\ndrownings &lt;- 10 + 0.8 * temperature + rnorm(n, 0, 3)\n\npar(mfrow = c(1, 2))\n\n# Wykres 1: Lody vs. utonięcia (korelacja pozorna)\nplot(ice_cream, drownings,\n     xlab = \"Sprzedaż Lodów\",\n     ylab = \"Zgony przez Utonięcie\",\n     main = \"Korelacja Pozorna\",\n     pch = 19, col = \"steelblue\")\nabline(lm(drownings ~ ice_cream), col = \"red\", lwd = 2)\ntext(60, 35, paste(\"R² =\", round(summary(lm(drownings ~ ice_cream))$r.squared, 2)),\n     col = \"red\")\n\n# Wykres 2: Obie związane z temperaturą (zakłócenie)\nplot(temperature, ice_cream,\n     xlab = \"Temperatura (°C)\",\n     ylab = \"Sprzedaż Lodów / Utonięcia\",\n     main = \"Obie Związane z Temperaturą\",\n     pch = 19, col = \"steelblue\",\n     ylim = c(0, 150))\npoints(temperature, drownings * 3, pch = 17, col = \"darkgreen\")\nabline(lm(ice_cream ~ temperature), col = \"steelblue\", lwd = 2)\nabline(lm(I(drownings * 3) ~ temperature), col = \"darkgreen\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Lody\", \"Utonięcia (skalowane)\"),\n       col = c(\"steelblue\", \"darkgreen\"), pch = c(19, 17))\n\n\n\n\n\n\n\nFigure 10.8: Przykład korelacji pozornej przez zakłócenie\n\n\n\n\n\nPrzykład 2: Rozmiar buta i umiejętność czytania u dzieci\nObserwacja: Dzieci z większymi butami czytają lepiej. Czy rozmiar stopy poprawia poznanie?\nNie! Zmienna zakłócająca to wiek. Starsze dzieci mają zarówno:\n\nWiększe rozmiary butów (urosły)\nLepsze umiejętności czytania (miały więcej edukacji)\n\nPrzykład 3: Liczba strażaków i szkody od pożaru\nObserwacja: Pożary z większą liczbą obecnych strażaków zwykle mają więcej szkód majątkowych. Czy powinniśmy wysyłać mniej strażaków, aby zmniejszyć szkody?\nAbsolutnie nie! To odwrotna przyczynowość (ang. reverse causation) połączona z zakłóceniem:\n\nWiększe pożary powodują więcej szkód (bezpośrednio)\nWiększe pożary też wymagają więcej strażaków (reakcja na dotkliwość)\n\nLiczba strażaków nie powoduje szkód; dotkliwość pożaru (ang. fire severity) powoduje obie rzeczy.\n\n\nCo Czyni Relację Przyczynową?\nAby ustalić przyczynowość (ang. establish causation), zazwyczaj potrzebujemy:\n\nPierwszeństwo czasowe (ang. temporal precedence): Przyczyna musi poprzedzać skutek\nZwiązek (ang. association): Zmienne muszą być skorelowane\nBrak wiarygodnych zmiennych zakłócających: Wykluczyliśmy alternatywne wyjaśnienia (ang. alternative explanations)\nMechanizm (ang. mechanism): Rozumiemy, jak X wpływa na Y\nZależność dawka-odpowiedź (ang. dose-response relationship): Większe wartości X powodują większe efekty na Y\n\nZłoty standard (ang. gold standard) ustalania przyczynowości to randomizowany eksperyment kontrolowany (ang. randomized controlled experiment), gdzie losowo przypisujemy ludzi do otrzymania różnych wartości X. To przerywa relację między X a potencjalnymi zmiennymi zakłócającymi.\nJednak w naukach społecznych często nie możemy przeprowadzać eksperymentów (byłoby to nieetyczne lub niepraktyczne). Zamiast tego używamy:\n\nProjektów longitudinalnych (ang. longitudinal designs) – mierzenie zmiennych w czasie\nEksperymentów naturalnych (ang. natural experiments) – znajdowanie naturalnie występującej zmienności losowej\nStarannej kontroli zmiennych zakłócających (ang. controlling for confounders) – włączanie ich do regresji wielorakiej\nRozumowania teoretycznego (ang. theoretical reasoning) i wcześniejszych dowodów\n\n\n\nKorelacje Pozorne\nKorelacja pozorna (ang. spurious correlation) to statystyczna relacja między dwiema zmiennymi, która nie wynika z żadnego związku przyczynowego między nimi. Często powstają przez:\n\nWspólną przyczynę (ang. common cause) – zakłócenie: Z powoduje zarówno X, jak i Y\nPrzypadek: Losowy zbieg okoliczności tworzy pozorne wzorce\nBłąd selekcji (ang. selection bias): Sposób próbkowania tworzy sztuczne relacje\n\nSłynne korelacje pozorne:\n\nKonsumpcja sera per capita koreluje ze zgonami przez zaplątanie się w prześcieradło (r = 0.95)\nLiczba filmów Nicolasa Cage’a koreluje z utonięciami w basenach (r = 0.67)\nKonsumpcja margaryny per capita koreluje ze wskaźnikiem rozwodów w Maine (r = 0.99)\n\nTo oczywiście nie są związki przyczynowe (ang. causal relationships) – to przypadki w danych szeregów czasowych. Lekcja: wysoka korelacja, nawet bardzo wysoka, nie dowodzi przyczynowości.\n\n\nImplikacje dla Analizy Regresji\nKiedy interpretujemy nasze wyniki regresji, musimy być ostrożni:\nCo regresja nam mówi:\n\nX i Y są związane\nSiła związku (ang. strength of association) (\\hat{\\beta}_1)\nIle wariancji jest wyjaśnione (R^2)\nCzy relacja jest liniowa\n\nCzego regresja NIE mówi:\n\nCzy X powoduje Y\nCzy pominęliśmy ważne zmienne zakłócające (ang. omitted confounders)\nKierunek przyczynowości (ang. direction of causation)\nCzy relacja jest pozorna\n\nNasz przykład z godzinami nauki:\nZnaleźliśmy, że godziny nauki silnie przewidują wyniki egzaminów (R^2 = 0.909). Czy nauka powoduje wyższe wyniki?\nPrawdopodobnie tak, ale powinniśmy rozważyć alternatywne wyjaśnienia:\n\nOdwrotna przyczynowość: Może studenci, którzy są naturalnie dobrzy w przedmiocie (wysokie zdolności, ang. high ability), lubią się uczyć więcej, więc uczą się dłużej. Tutaj zdolności powodują zarówno godziny nauki, jak i wyniki egzaminów.\nZakłócenie: Może studenci ze wspierającymi rodzicami (ang. supportive parents) zarówno uczą się więcej, jak i osiągają lepsze wyniki (rodzice pomagają w odrabianiu lekcji i tworzą dobre środowisko do nauki).\nSelekcja: Może w naszej próbie byli tylko zmotywowani studenci, a motywacja powoduje zarówno naukę, jak i wyniki.\n\nAby wzmocnić twierdzenie przyczynowe (ang. causal claim), moglibyśmy:\n\nKontrolować wcześniejsze zdolności (ang. prior ability) – włączyć poprzednie wyniki egzaminów\nKontrolować zaangażowanie rodziców (ang. parental involvement)\nUżyć projektu eksperymentalnego (ang. experimental design) – losowo przypisać godziny nauki\nSprawdzić, czy relacja utrzymuje się w różnych kontekstach\n\n\n\nPraktyczne Rady\nPrzy raportowaniu wyników regresji:\n\nUżywaj ostrożnego języka: “związane z” zamiast “powoduje” lub “prowadzi do”\nUznaj potencjalne zmienne zakłócające\nOmów alternatywne wyjaśnienia\nBądź jawny co do ograniczeń (ang. limitations) projektu badania\nJeśli twierdzisz o przyczynowości, podaj rozumowanie teoretyczne i dodatkowe dowody\n\nPamiętaj: korelacja jest koniecznym, ale niewystarczającym warunkiem przyczynowości (ang. correlation is a necessary but not sufficient condition for causation). Znalezienie związku to tylko pierwszy krok w zrozumieniu, czy jedna zmienna wpływa na drugą.\n\n\n\nKluczowe Założenia MNK\nAby MNK dostarczała znaczących oszacowań (ang. meaningful estimates), potrzebujemy kilku założeń:\n\nLiniowość (ang. linearity): Relacja między X i Y jest liniowa: Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\nŚcisła Egzogeniczność (ang. strict exogeneity): Składnik błędu (ang. error term) ma zerową wartość oczekiwaną warunkową (ang. conditional expected value) na X: E[\\varepsilon_i | X_i] = 0\nTo oznacza, że X jest nieskorelowane z błędem – nie ma pominiętych zmiennych (ang. omitted variables), które wpływają zarówno na X, jak i Y.\nHomoskedastyczność (ang. homoscedasticity) – nie ścisła, ale pożądana: Wariancja błędów jest stała: Var(\\varepsilon_i | X_i) = \\sigma^2\nBrak idealnej współliniowości (ang. no perfect collinearity) – ważne dla regresji wielorakiej: Zmienne predykcyjne nie są idealnie skorelowane.\n\nWażna uwaga: Nie zakładamy tutaj probabilistycznych założeń o rozkładzie próbkowania (ang. sampling distribution). Po prostu opisujemy relację w naszych danych. Dla wnioskowania (ang. inference) – testów hipotez (ang. hypothesis tests), przedziałów ufności (ang. confidence intervals) – potrzebowalibyśmy dodatkowych założeń, ale to wykracza poza nasz obecny zakres.\n\n\n\n\n\n\n\nDla Jakich Danych Możemy Stosować Regresję MNK?\n\n\n\nRegresja MNK ma specyficzne wymagania dotyczące typów zmiennych:\nZmienna zależna (wynikowa, Y):\n\nMusi być ciągła (ang. continuous) lub quasi-ciągła (np. dyskretna z wieloma wartościami)\nPoziom pomiaru (typologia Stevensa):\n\nPrzedziałowy (ang. interval): różnice między wartościami mają sens, ale brak naturalnego zera (np. temperatura w °C, wyniki testów IQ)\nIlorazowy (ang. ratio): jak przedziałowy, ale z naturalnym zerem (np. dochód, wiek, liczba godzin)\n\nNie nadaje się: Zmienne nominalne (ang. nominal) – kategorie bez porządku (np. kolor oczu, płeć) lub ordinalne (ang. ordinal) z niewielką liczbą kategorii (np. skala Likerta 1-5)\n\nZmienne niezależne (predyktory, X):\n\nMogą być ciągłe, dyskretne lub binarne (0/1)\nPoziom pomiaru:\n\nIlorazowy/Przedziałowy: idealny (np. wiek, dochód, temperatura)\nOrdinalny: akceptowalny, jeśli traktujemy jako quasi-ciągły (np. poziom wykształcenia: 1=podstawowe, 2=średnie, 3=wyższe)\nNominalny binarny: akceptowalny (np. płeć: 0=kobieta, 1=mężczyzna)\nNominalny wielokategorialny: wymaga przekształcenia na zmienne binarne (ang. dummy variables) – np. region: 3 zmienne binarne dla 4 regionów\n\n\nPrzykłady poprawnych zastosowań:\n\nY: Wynik egzaminu (0-100 punktów, przedziałowy) ~ X: Godziny nauki (ciągły, ilorazowy) ✓\nY: Dochód (ciągły, ilorazowy) ~ X_1: Wiek (ciągły, ilorazowy) + X_2: Płeć (binarny, nominalny) ✓\nY: Frekwencja wyborcza (%, ilorazowy) ~ X: PKB per capita (ciągły, ilorazowy) ✓\n\nNiepoprawne zastosowania:\n\nY: Kolor oczu (nominalny) ~ X: Wiek – użyj regresji wielomianowej (ang. multinomial regression) ✗\nY: Wybór partii (nominalny, 5 opcji) ~ X: Dochód – użyj regresji wielomianowej ✗\nY: Zgoda (Tak/Nie, binarny) ~ X: Wiek – użyj regresji logistycznej (ang. logistic regression) ✗\n\nZasada praktyczna: Jeśli Y ma mniej niż ~7 unikalnych wartości, rozważ inne metody (regresja logistyczna, porządkowa, Poissona).\n\n\n\n\n\nRozszerzenie do Regresji Wielorakiej\nDo tej pory patrzyliśmy na prostą regresję liniową (ang. simple linear regression) z jednym predyktorem. W rzeczywistości zjawiska społeczne są złożone i wymagają wielu predyktorów.\nModel regresji wielorakiej (ang. multiple regression model) rozszerza nasze ramy:\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\cdots + \\beta_k X_{ki} + \\varepsilon_i\nNa przykład, przewidywanie frekwencji wyborczej:\n\\text{Frekwencja}_i = \\beta_0 + \\beta_1 \\text{PKB}_i + \\beta_2 \\text{Obowiązkowe}_i + \\beta_3 \\text{Wiek}_i + \\varepsilon_i\nZasada MNK pozostaje ta sama: znajdź współczynniki, które minimalizują SSE = \\sum (Y_i - \\hat{Y}_i)^2.\nInterpretacja zmienia się nieznacznie:\n\n\\hat{\\beta}_1 to efekt X_1 na Y, przy utrzymaniu wszystkich innych zmiennych stałych (ang. holding all other variables constant)\nTo jest często nazywane “efektem cząstkowym” (ang. partial effect) lub “kontrolowaniem” (ang. controlling for) innych zmiennych\n\nNotacja macierzowa (ang. matrix notation) – dla referencji:\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\nAle nie musisz tego zapamiętywać – oprogramowanie (jak funkcja lm() w R) obsługuje obliczenia.\n\n# Przykład regresji wielorakiej\n# (używając wbudowanego zbioru danych mtcars)\nmulti_model &lt;- lm(mpg ~ hp + wt + cyl, data = mtcars)\nsummary(multi_model)\n\n\nCall:\nlm(formula = mpg ~ hp + wt + cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9290 -1.5598 -0.5311  1.1850  5.8986 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 38.75179    1.78686  21.687 &lt; 0.0000000000000002 ***\nhp          -0.01804    0.01188  -1.519             0.140015    \nwt          -3.16697    0.74058  -4.276             0.000199 ***\ncyl         -0.94162    0.55092  -1.709             0.098480 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.512 on 28 degrees of freedom\nMultiple R-squared:  0.8431,    Adjusted R-squared:  0.8263 \nF-statistic: 50.17 on 3 and 28 DF,  p-value: 0.00000000002184\n\n\n\n\nPodsumowanie\nOmówiliśmy podstawy analizy regresji:\n\nModele deterministyczne zakładają idealne relacje; modele stochastyczne uznają niepewność\nModele statystyczne zawierają zarówno systematyczne wzorce, jak i losową zmienność\nW danych istnieją różne relacje – modele liniowe działają najlepiej dla relacji liniowych\nMetoda MNK znajduje linię, która minimalizuje sumę kwadratów błędów\nMożemy rozłożyć wariancję: SST = SSR + SSE\nR^2 = SSR/SST mierzy proporcję wariancji wyjaśnionej (od 0 do 1)\nRMSE mierzy średni błąd predykcji w oryginalnych jednostkach\nWspółczynniki należy interpretować merytorycznie, uwzględniając wielkości efektów\nWspółczynniki standaryzowane pozwalają na porównanie między różnymi skalami\nWykresy reszt i wykrywanie wartości odstających pomagają diagnozować problemy modelu\nKorelacja nie implikuje przyczynowości – zakłócenie i relacje pozorne są powszechne\nAby ustalić przyczynowość, potrzebujemy pierwszeństwa czasowego, braku zmiennych zakłócających i mechanizmów teoretycznych\nMNK wymaga założeń takich jak liniowość i ścisła egzogeniczność\nRegresja wieloraka rozszerza te ramy o wiele predyktorów\n\nW praktyce zawsze najpierw wizualizuj swoje dane, sprawdź, czy model liniowy jest odpowiedni, zbadaj wykresy reszt pod kątem problemów, dokładnie przemyśl interpretację przyczynową i pamiętaj, że związek to nie przyczynowość – nawet wysokie R^2 nie dowodzi, że X powoduje Y.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#ręczne-obliczenia-ols-krok-po-kroku",
    "href": "correg_pl.html#ręczne-obliczenia-ols-krok-po-kroku",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.11 Ręczne obliczenia OLS krok po kroku",
    "text": "10.11 Ręczne obliczenia OLS krok po kroku\nBadaczka chce zbadać zależność między godzinami nauki a wynikiem testu (6 studentów):\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nCelem jest wyznaczyć \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X metodą OLS.\n\nKrok 1: Średnie\nDla X:\n\\bar{X} = \\frac{1+2+3+4+5+6}{6} = \\frac{21}{6} = 3.5\nDla Y:\n\\bar{Y} = \\frac{65+70+75+85+88+95}{6} = \\frac{478}{6} = 79.67\n\n\nKrok 2: Odchylenia od średnich\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n−2.5\n−14.67\n\n\nB\n2\n70\n−1.5\n−9.67\n\n\nC\n3\n75\n−0.5\n−4.67\n\n\nD\n4\n85\n0.5\n5.33\n\n\nE\n5\n88\n1.5\n8.33\n\n\nF\n6\n95\n2.5\n15.33\n\n\n\n\n\nKrok 3: Iloczyny i kwadraty\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n36.68\n6.25\n\n\nB\n14.51\n2.25\n\n\nC\n2.34\n0.25\n\n\nD\n2.67\n0.25\n\n\nE\n12.50\n2.25\n\n\nF\n38.33\n6.25\n\n\nSuma\n107.03\n17.50\n\n\n\n\n\nKrok 4: Nachylenie \\hat{\\beta}_1\n\\hat{\\beta}_1 = \\frac{107.03}{17.50} = 6.12\nInterpretacja: +6.12 punktu za każdą dodatkową godzinę nauki.\n\n\nKrok 5: Wyraz wolny \\hat{\\beta}_0\n\\hat{\\beta}_0 = 79.67 - 6.12 \\cdot 3.5 = 58.25\nInterpretacja: przy 0 godzinach przewidujemy 58.25.\n\n\nKrok 6: Równanie regresji\n\\hat{Y} = 58.25 + 6.12 X\n\n\nKrok 7: Predykcje i reszty\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nReszta e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n64.37\n0.63\n\n\nB\n2\n70\n70.49\n−0.49\n\n\nC\n3\n75\n76.61\n−1.61\n\n\nD\n4\n85\n82.73\n2.27\n\n\nE\n5\n88\n88.85\n−0.85\n\n\nF\n6\n95\n94.97\n0.03\n\n\n\nKontrola: suma reszt ≈ 0 ✓\n\n\nKrok 8: Sumy kwadratów\nSST — całkowita:\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n215.21\n\n\nB\n70\n93.51\n\n\nC\n75\n21.81\n\n\nD\n85\n28.41\n\n\nE\n88\n69.39\n\n\nF\n95\n235.01\n\n\nSuma\n\nSST = 663.34\n\n\n\nSSR — wyjaśniona:\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n234.09\n\n\nB\n70.49\n84.27\n\n\nC\n76.61\n9.36\n\n\nD\n82.73\n9.36\n\n\nE\n88.85\n84.27\n\n\nF\n94.97\n234.09\n\n\nSuma\n\nSSR = 655.44\n\n\n\nSSE — błąd:\n\n\n\nStudent\ne_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n−0.49\n0.24\n\n\nC\n−1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n−0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSuma\n\nSSE = 9.10\n\n\n\nWeryfikacja: SST \\approx SSR + SSE\n663.34 \\approx 655.44 + 9.10 = 664.54 (drobne różnice zaokrągleń).\n\n\nKrok 9: R^2\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternatywnie:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 0.986\nInterpretacja: ~98.8% zmienności wyników wyjaśniają godziny nauki — bardzo silny związek.\n\n\nKrok 10: Wielkości efektu\nSurowa: 6.12 pkt/godz.\nStandaryzowana:\n\ns_X = \\sqrt{17.50/5} = 1.87,\ns_Y = \\sqrt{663.34/5} = 11.52,\n\\beta_{\\text{std}} = 6.12 \\cdot (1.87/11.52) = 0.99 → bardzo duży efekt (wg Cohena).\n\n\n\nKrok 11: Istotność praktyczna\n\nSkala: 6.12% na 100-punktowej skali / godz.,\nProgi: zmiana oceny (10 pkt) ≈ 1.63 h,\nKoszt-efekt: korzystny — sensowna inwestycja czasu.\n\n\n\nPodsumowanie wyników\n\nRównanie: \\hat{Y} = 58.25 + 6.12 X\nNachylenie: 6.12 pkt/godz.\nWyraz wolny: 58.25 pkt\nR^2: 0.988\n\\beta_{\\text{std}}: 0.99\n\nW praktyce: każda godzina nauki to ≈ +6 pkt; dopasowanie znakomite; efekt istotny statystycznie i praktycznie.\n\n\nKontrola wyniku\nSprawdź, że linia przechodzi przez (\\bar{X}, \\bar{Y}):\n58.25 + 6.12 \\cdot 3.5 = 79.67 = \\bar{Y} ✓",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kod-r-do-weryfikacji-obliczeń",
    "href": "correg_pl.html#kod-r-do-weryfikacji-obliczeń",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.12 Kod R do weryfikacji obliczeń",
    "text": "10.12 Kod R do weryfikacji obliczeń\n\n# Krok 1: Dane\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)      # X\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95) # Y\nn &lt;- length(study_hours)\n\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Krok 2: Średnie\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Krok 3: Odchylenia od średnich\ndata$x_dev &lt;- data$X - x_bar\ndata$y_dev &lt;- data$Y - y_bar\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Krok 4: Iloczyny i kwadraty\ndata$xy_product &lt;- data$x_dev * data$y_dev\ndata$x_dev_sq &lt;- data$x_dev^2\n\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - X̄)(Yi - Ȳ):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - X̄)(Yi - Ȳ): 107 \n\ncat(\"Sum of (Xi - X̄)²:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - X̄)²: 17.5 \n\n# Krok 5: Nachylenie (beta_1)\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (β₁) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (β₁) calculated manually: 6.11 \n\n# Krok 6: Wyraz wolny (beta_0)\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (β₀) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (β₀) calculated manually: 58.27 \n\n# Krok 7: Porównanie z lm()\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Krok 8: Predykcje i reszty\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X\ndata$residual &lt;- data$Y - data$Y_hat\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ≈ 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ≈ 0): 0 \n\n# Krok 9: Sumy kwadratów\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Krok 10: R-kwadrat\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R² (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR² (Method 1: SSR/SST): 0.9863 \n\ncat(\"R² (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR² (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R² (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR² (from lm function): 0.9863 \n\n# Krok 11: Wielkości efektu\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\nsd_x &lt;- sd(data$X)\nsd_y &lt;- sd(data$Y)\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Korelacja (dla regresji prostej |r| = sqrt(R^2))\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R²:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R²: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n\n\n# Krok 12: Wizualizacja\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5,\n     xlim = c(0, 7), ylim = c(60, 100))\n\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Punkt średnich\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Reszty jako odcinki pionowe\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# Równanie na wykresie\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R² = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\nAnaliza regresji OLS - godziny nauki vs. wynik testu\n\n\n\n# Podsumowanie końcowe\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#jak-uruchomić-kod",
    "href": "correg_pl.html#jak-uruchomić-kod",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.13 Jak uruchomić kod",
    "text": "10.13 Jak uruchomić kod\n\nSkopiuj cały blok kodu,\nWklej do RStudio,\nUruchom chunk po chunk lub cały dokument,\nPorównaj wyniki z obliczeniami ręcznymi.\n\nCo zobaczysz:\n\nNachylenie: 6.12,\nWyraz wolny: 58.25,\nR^2: ≈ 0.988,\nEfekt standaryzowany: ≈ 0.99,\nWykres z punktami, linią regresji i resztami.\n\nTo potwierdza poprawność obliczeń manualnych. :::",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#appendix-a-obliczanie-kowariancji-korelacji-pearsona-i-spearmana-oraz-modelowanie-ols---przykład-wprowadzający",
    "href": "correg_pl.html#appendix-a-obliczanie-kowariancji-korelacji-pearsona-i-spearmana-oraz-modelowanie-ols---przykład-wprowadzający",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.14 Appendix A: Obliczanie Kowariancji, Korelacji Pearsona i Spearmana, oraz modelowanie OLS - przykład wprowadzający",
    "text": "10.14 Appendix A: Obliczanie Kowariancji, Korelacji Pearsona i Spearmana, oraz modelowanie OLS - przykład wprowadzający\nStudentka politologii bada związek między wielkością okręgu wyborczego (DM) a wskaźnikiem dysproporcjonalności Gallaghera (GH) w wyborach parlamentarnych w 10 losowo wybranych demokracjach.\nDane dotyczące wielkości okręgu wyborczego (\\text{DM}) i indeksu Gallaghera:\n\n\n\n\\text{DM} (X)\nGallagher (Y)\n\n\n\n\n2\n18,2\n\n\n3\n16,7\n\n\n4\n15,8\n\n\n5\n15,3\n\n\n6\n15,0\n\n\n7\n14,8\n\n\n8\n14,7\n\n\n9\n14,6\n\n\n10\n14,55\n\n\n11\n14,52\n\n\n\n\nKrok 1: Obliczanie Podstawowych Statystyk\nObliczanie średnich:\nDla \\text{DM} (X): \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\nSzczegółowe obliczenia:\n2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 65 \\bar{x} = \\frac{65}{10} = 6,5\nDla indeksu Gallaghera (Y): \\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\nSzczegółowe obliczenia:\n18,2 + 16,7 + 15,8 + 15,3 + 15,0 + 14,8 + 14,7 + 14,6 + 14,55 + 14,52 = 154,17 \\bar{y} = \\frac{154,17}{10} = 15,417\n\n\nKrok 2: Szczegółowe Obliczenia Kowariancji\nPełna tabela robocza ze wszystkimi obliczeniami:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18,2\n-4,5\n2,783\n-12,5235\n20,25\n7,7451\n\n\n2\n3\n16,7\n-3,5\n1,283\n-4,4905\n12,25\n1,6461\n\n\n3\n4\n15,8\n-2,5\n0,383\n-0,9575\n6,25\n0,1467\n\n\n4\n5\n15,3\n-1,5\n-0,117\n0,1755\n2,25\n0,0137\n\n\n5\n6\n15,0\n-0,5\n-0,417\n0,2085\n0,25\n0,1739\n\n\n6\n7\n14,8\n0,5\n-0,617\n-0,3085\n0,25\n0,3807\n\n\n7\n8\n14,7\n1,5\n-0,717\n-1,0755\n2,25\n0,5141\n\n\n8\n9\n14,6\n2,5\n-0,817\n-2,0425\n6,25\n0,6675\n\n\n9\n10\n14,55\n3,5\n-0,867\n-3,0345\n12,25\n0,7517\n\n\n10\n11\n14,52\n4,5\n-0,897\n-4,0365\n20,25\n0,8047\n\n\nSuma\n65\n154,17\n0\n0\n-28,085\n82,5\n12,8442\n\n\n\nObliczanie kowariancji: \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\text{Cov}(X,Y) = \\frac{-28,085}{9} = -3,120556\n\n\nKrok 3: Obliczanie Odchylenia Standardowego\nDla \\text{DM} (X): \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}}\n\\sigma_x = \\sqrt{\\frac{82,5}{9}} = \\sqrt{9,1667} = 3,026582\nDla Gallaghera (Y): \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}}\n\\sigma_y = \\sqrt{\\frac{12,8442}{9}} = \\sqrt{1,4271} = 1,194612\n\n\nKrok 4: Obliczanie Korelacji Pearsona\nr = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\nr = \\frac{-3,120556}{3,026582 \\times 1,194612} = \\frac{-3,120556}{3,615752} = -0,863044\n\n\nKrok 5: Obliczanie Korelacji Rangowej Spearmana\nPełna tabela rangowa ze wszystkimi obliczeniami:\n\n\n\ni\nX_i\nY_i\nRanga X_i\nRanga Y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n18,2\n1\n10\n-9\n81\n\n\n2\n3\n16,7\n2\n9\n-7\n49\n\n\n3\n4\n15,8\n3\n8\n-5\n25\n\n\n4\n5\n15,3\n4\n7\n-3\n9\n\n\n5\n6\n15,0\n5\n6\n-1\n1\n\n\n6\n7\n14,8\n6\n5\n1\n1\n\n\n7\n8\n14,7\n7\n4\n3\n9\n\n\n8\n9\n14,6\n8\n3\n5\n25\n\n\n9\n10\n14,55\n9\n2\n7\n49\n\n\n10\n11\n14,52\n10\n1\n9\n81\n\n\nSuma\n\n\n\n\n\n330\n\n\n\nObliczanie korelacji Spearmana: \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n\\rho = 1 - \\frac{6 \\times 330}{10(100 - 1)} = 1 - \\frac{1980}{990} = 1 - 2 = -1\n\n\nKrok 6: Weryfikacja w R\n\n# Tworzenie wektorów\nDM &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nGH &lt;- c(18.2, 16.7, 15.8, 15.3, 15.0, 14.8, 14.7, 14.6, 14.55, 14.52)\n\n# Obliczanie kowariancji\ncov(DM, GH)\n\n[1] -3.120556\n\n# Obliczanie korelacji\ncor(DM, GH, method = \"pearson\")\n\n[1] -0.8627742\n\ncor(DM, GH, method = \"spearman\")\n\n[1] -1\n\n\n\n\nKrok 7: Podstawowa Wizualizacja\n\nlibrary(ggplot2)\n\n# Tworzenie ramki danych\ndata &lt;- data.frame(DM = DM, GH = GH)\n\n# Tworzenie wykresu rozrzutu\nggplot(data, aes(x = DM, y = GH)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Wielkość Okręgu vs Indeks Gallaghera\",\n    x = \"Wielkość Okręgu (DM)\",\n    y = \"Indeks Gallaghera (GH)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nEstymacja OLS i Miary Dopasowania Modelu\n\n\nKrok 1: Obliczanie Estymatorów OLS\nKorzystając z wcześniej obliczonych wartości:\n\n\\sum(X_i - \\bar{X})(Y_i - \\bar{Y}) = -28,085\n\\sum(X_i - \\bar{X})^2 = 82,5\n\\bar{X} = 6,5\n\\bar{Y} = 15,417\n\nObliczanie nachylenia (\\hat{\\beta_1}):\n\\hat{\\beta_1} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\hat{\\beta_1} = -28,085 ÷ 82,5 = -0,3404\nObliczanie wyrazu wolnego (\\hat{\\beta_0}): \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 15,417 - (-0,3404 × 6,5)\n   = 15,417 + 2,2126\n   = 17,6296\nZatem równanie regresji OLS ma postać: \\hat{Y} = 17,6296 - 0,3404X\n\n\nKrok 2: Obliczanie Wartości Dopasowanych i Reszt\nPełna tabela ze wszystkimi obliczeniami:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i\ne_i = Y_i - \\hat{Y}_i\ne_i^2\n(Y_i - \\bar{Y})^2\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18,2\n16,9488\n1,2512\n1,5655\n7,7451\n2,3404\n\n\n2\n3\n16,7\n16,6084\n0,0916\n0,0084\n1,6461\n1,4241\n\n\n3\n4\n15,8\n16,2680\n-0,4680\n0,2190\n0,1467\n0,7225\n\n\n4\n5\n15,3\n15,9276\n-0,6276\n0,3939\n0,0137\n0,2601\n\n\n5\n6\n15,0\n15,5872\n-0,5872\n0,3448\n0,1739\n0,0289\n\n\n6\n7\n14,8\n15,2468\n-0,4468\n0,1996\n0,3807\n0,0290\n\n\n7\n8\n14,7\n14,9064\n-0,2064\n0,0426\n0,5141\n0,2610\n\n\n8\n9\n14,6\n14,5660\n0,0340\n0,0012\n0,6675\n0,7241\n\n\n9\n10\n14,55\n14,2256\n0,3244\n0,1052\n0,7517\n1,4184\n\n\n10\n11\n14,52\n13,8852\n0,6348\n0,4030\n0,8047\n2,3439\n\n\nSuma\n65\n154,17\n154,17\n0\n3,2832\n12,8442\n9,5524\n\n\n\nObliczenia dla wartości dopasowanych:\nDla X = 2:\nŶ = 17,6296 + (-0,3404 × 2) = 16,9488\n\nDla X = 3:\nŶ = 17,6296 + (-0,3404 × 3) = 16,6084\n\n[... kontynuacja dla wszystkich wartości]\n\n\nKrok 3: Obliczanie Miar Dopasowania\nSuma kwadratów reszt (SSE): SSE = \\sum e_i^2\nSSE = 3,2832\nCałkowita suma kwadratów (SST): SST = \\sum(Y_i - \\bar{Y})^2\nSST = 12,8442\nSuma kwadratów regresji (SSR): SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\nSSR = 9,5524\nWeryfikacja dekompozycji: SST = SSR + SSE\n12,8442 = 9,5524 + 3,2832 (w granicach błędu zaokrąglenia)\nObliczanie współczynnika determinacji R-kwadrat: R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nR² = 9,5524 ÷ 12,8442\n   = 0,7438\n\n\nKrok 4: Weryfikacja w R\n\n# Dopasowanie modelu liniowego\nmodel &lt;- lm(GH ~ DM, data = data)\n\n# Podsumowanie statystyk\nsummary(model)\n\n\nCall:\nlm(formula = GH ~ DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62764 -0.46274 -0.08615  0.26624  1.25109 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept) 17.62976    0.50121  35.174 0.000000000467 ***\nDM          -0.34042    0.07053  -4.827        0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6406 on 8 degrees of freedom\nMultiple R-squared:  0.7444,    Adjusted R-squared:  0.7124 \nF-statistic:  23.3 on 1 and 8 DF,  p-value: 0.00131\n\n# Ręczne obliczenie R-kwadrat\nSST &lt;- sum((GH - mean(GH))^2)\nSSE &lt;- sum(residuals(model)^2)\nSSR &lt;- SST - SSE\nR2_manual &lt;- SSR/SST\nR2_manual\n\n[1] 0.7443793\n\n\n\n\nKrok 5: Analiza Reszt\n\n# Tworzenie wykresów reszt\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\nKrok 6: Wykres Wartości Przewidywanych vs Rzeczywistych\n\n# Tworzenie wykresu wartości przewidywanych vs rzeczywistych\nggplot(data.frame(\n  Rzeczywiste = GH,\n  Przewidywane = fitted(model)\n), aes(x = Przewidywane, y = Rzeczywiste)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Wartości Przewidywane vs Rzeczywiste\",\n    x = \"Przewidywany Indeks Gallaghera\",\n    y = \"Rzeczywisty Indeks Gallaghera\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nModele z Transformacją Logarytmiczną\n\n\nKrok 1: Transformacja Danych\nNajpierw obliczamy logarytmy naturalne zmiennych:\n\n\n\ni\nX_i\nY_i\n\\ln(X_i)\n\\ln(Y_i)\n\n\n\n\n1\n2\n18,2\n0,6931\n2,9014\n\n\n2\n3\n16,7\n1,0986\n2,8154\n\n\n3\n4\n15,8\n1,3863\n2,7600\n\n\n4\n5\n15,3\n1,6094\n2,7278\n\n\n5\n6\n15,0\n1,7918\n2,7081\n\n\n6\n7\n14,8\n1,9459\n2,6946\n\n\n7\n8\n14,7\n2,0794\n2,6878\n\n\n8\n9\n14,6\n2,1972\n2,6810\n\n\n9\n10\n14,55\n2,3026\n2,6777\n\n\n10\n11\n14,52\n2,3979\n2,6757\n\n\n\n\n\nKrok 2: Porównanie Różnych Specyfikacji Modelu\nSzacujemy trzy alternatywne specyfikacje:\n\nModel log-liniowy: \\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nModel liniowo-logarytmiczny: Y_i = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\nModel log-log: \\ln(Y_i) = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\n\n\n# Tworzenie zmiennych transformowanych\ndata$log_DM &lt;- log(data$DM)\ndata$log_GH &lt;- log(data$GH)\n\n# Dopasowanie modeli\nmodel_linear &lt;- lm(GH ~ DM, data = data)\nmodel_loglinear &lt;- lm(log_GH ~ DM, data = data)\nmodel_linearlog &lt;- lm(GH ~ log_DM, data = data)\nmodel_loglog &lt;- lm(log_GH ~ log_DM, data = data)\n\n# Porównanie wartości R-kwadrat\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Liniowy\", \"Log-liniowy\", \"Liniowo-logarytmiczny\", \"Log-log\"),\n  R_kwadrat = c(\n    summary(model_linear)$r.squared,\n    summary(model_loglinear)$r.squared,\n    summary(model_linearlog)$r.squared,\n    summary(model_loglog)$r.squared\n  )\n)\n\n# Wyświetlenie porównania\nmodels_comparison\n\n                  Model R_kwadrat\n1               Liniowy 0.7443793\n2           Log-liniowy 0.7670346\n3 Liniowo-logarytmiczny 0.9141560\n4               Log-log 0.9288088\n\n\n\n\nKrok 3: Porównanie Wizualne\n\n# Tworzenie wykresów dla każdego modelu\np1 &lt;- ggplot(data, aes(x = DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Liniowy\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Log-liniowy\") +\n  theme_minimal()\n\np3 &lt;- ggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Liniowo-logarytmiczny\") +\n  theme_minimal()\n\np4 &lt;- ggplot(data, aes(x = log_DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Log-log\") +\n  theme_minimal()\n\n# Układanie wykresów w siatkę\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nKrok 4: Analiza Reszt dla Najlepszego Modelu\nNa podstawie wartości R-kwadrat, analiza reszt dla najlepiej dopasowanego modelu:\n\n# Wykresy reszt dla najlepszego modelu\npar(mfrow = c(2, 2))\nplot(model_linearlog)\n\n\n\n\n\n\n\n\n\n\nKrok 5: Interpretacja Najlepszego Modelu\nWspółczynniki modelu liniowo-logarytmicznego:\n\nsummary(model_linearlog)\n\n\nCall:\nlm(formula = GH ~ log_DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40702 -0.30207 -0.04907  0.22905  0.60549 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)  19.0223     0.4079   46.64 0.0000000000494 ***\nlog_DM       -2.0599     0.2232   -9.23 0.0000153880425 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3712 on 8 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9034 \nF-statistic: 85.19 on 1 and 8 DF,  p-value: 0.00001539\n\n\nInterpretacja:\n\n\\hat{\\beta_0} reprezentuje oczekiwany Indeks Gallaghera, gdy ln(DM) = 0 (czyli gdy DM = 1)\n\\hat{\\beta_1} reprezentuje zmianę Indeksu Gallaghera związaną z jednostkowym wzrostem ln(DM)\n\n\n\nKrok 6: Predykcje Modelu\n\n# Tworzenie wykresu predykcji dla najlepszego modelu\nggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Model Liniowo-logarytmiczny: Indeks Gallaghera vs ln(Wielkość Okręgu)\",\n    x = \"ln(Wielkość Okręgu)\",\n    y = \"Indeks Gallaghera\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nKrok 7: Analiza Elastyczności\nDla modelu log-log współczynniki bezpośrednio reprezentują elastyczności. Obliczenie średniej elastyczności dla modelu liniowo-logarytmicznego:\n\n# Obliczenie elastyczności przy wartościach średnich\nmean_DM &lt;- mean(data$DM)\nmean_GH &lt;- mean(data$GH)\nbeta1 &lt;- coef(model_linearlog)[2]\nelastycznosc &lt;- beta1 * (1/mean_GH)\nelastycznosc\n\n    log_DM \n-0.1336136 \n\n\nWartość ta reprezentuje procentową zmianę Indeksu Gallaghera przy jednoprocentowej zmianie Wielkości Okręgu.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#appendix-b.-prosty-model-regresji-liniowej-mnkols---przykłady-różne",
    "href": "correg_pl.html#appendix-b.-prosty-model-regresji-liniowej-mnkols---przykłady-różne",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.15 Appendix B. Prosty Model Regresji Liniowej (MNK/OLS) - przykłady różne",
    "text": "10.15 Appendix B. Prosty Model Regresji Liniowej (MNK/OLS) - przykłady różne",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przykład-0.-czas-nauki-vs.-ocena-przewodnik-po-mnk-i-r-kwadrat",
    "href": "correg_pl.html#przykład-0.-czas-nauki-vs.-ocena-przewodnik-po-mnk-i-r-kwadrat",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.16 Przykład 0. Czas Nauki vs. Ocena: Przewodnik po MNK i R-kwadrat",
    "text": "10.16 Przykład 0. Czas Nauki vs. Ocena: Przewodnik po MNK i R-kwadrat\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\n\n# Nasz prosty zbiór danych\npractice &lt;- c(1, 2, 3, 4, 5, 6)\nskill &lt;- c(3, 7, 5, 8, 10, 9)\n\n# Tworzenie ramki danych\ndata &lt;- data.frame(\n  Student = 1:6,\n  Practice = practice,\n  Skill = skill\n)\n\nprint(data)\n\n  Student Practice Skill\n1       1        1     3\n2       2        2     7\n3       3        3     5\n4       4        4     8\n5       5        5    10\n6       6        6     9\n\n\n\nObliczenia Ręczne\n\nKrok 1: Obliczenie Średnich\nŚrednia to wartość przeciętna, obliczana przez zsumowanie wszystkich obserwacji i podzielenie przez ich liczbę.\nŚrednia Godzin Ćwiczeń (X̄):\n\\bar{X} = \\frac{\\sum_{i=1}^{n} X_i}{n} = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5\nŚrednia Ocen Umiejętności (Ȳ):\n\\bar{Y} = \\frac{\\sum_{i=1}^{n} Y_i}{n} = \\frac{3 + 7 + 5 + 8 + 10 + 9}{6} = \\frac{42}{6} = 7\n\n# Obliczenie średnich\nmean_x &lt;- mean(practice)\nmean_y &lt;- mean(skill)\n\ncat(\"Średnia Godzin Ćwiczeń:\", mean_x, \"\\n\")\n\nŚrednia Godzin Ćwiczeń: 3.5 \n\ncat(\"Średnia Oceny Umiejętności:\", mean_y, \"\\n\")\n\nŚrednia Oceny Umiejętności: 7 \n\n\n\n\nKrok 2: Obliczenie Wariancji i Odchylenia Standardowego\nWariancja mierzy, jak bardzo dane są rozproszone wokół średniej. Używamy wzoru na wariancję próbkową (dzielenie przez n-1).\nWariancja X (Godziny Ćwiczeń):\nNajpierw obliczamy odchylenia od średniej (X_i - \\bar{X}):\n\n\n\nStudent\nX_i\nX_i - \\bar{X}\n(X_i - \\bar{X})^2\n\n\n\n\n1\n1\n1 - 3.5 = -2.5\n(-2.5)^2 = 6.25\n\n\n2\n2\n2 - 3.5 = -1.5\n(-1.5)^2 = 2.25\n\n\n3\n3\n3 - 3.5 = -0.5\n(-0.5)^2 = 0.25\n\n\n4\n4\n4 - 3.5 = 0.5\n(0.5)^2 = 0.25\n\n\n5\n5\n5 - 3.5 = 1.5\n(1.5)^2 = 2.25\n\n\n6\n6\n6 - 3.5 = 2.5\n(2.5)^2 = 6.25\n\n\nSuma\n\n\n17.5\n\n\n\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1} = \\frac{17.5}{5} = 3.5\ns_X = \\sqrt{3.5} = 1.871\nWariancja Y (Oceny Umiejętności):\n\n\n\nStudent\nY_i\nY_i - \\bar{Y}\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n3\n3 - 7 = -4\n(-4)^2 = 16\n\n\n2\n7\n7 - 7 = 0\n(0)^2 = 0\n\n\n3\n5\n5 - 7 = -2\n(-2)^2 = 4\n\n\n4\n8\n8 - 7 = 1\n(1)^2 = 1\n\n\n5\n10\n10 - 7 = 3\n(3)^2 = 9\n\n\n6\n9\n9 - 7 = 2\n(2)^2 = 4\n\n\nSuma\n\n\n34\n\n\n\ns^2_Y = \\frac{\\sum(Y_i - \\bar{Y})^2}{n-1} = \\frac{34}{5} = 6.8\ns_Y = \\sqrt{6.8} = 2.608\n\n# Weryfikacja wariancji i odchylenia standardowego\ncat(\"Wariancja Ćwiczeń:\", var(practice), \"\\n\")\n\nWariancja Ćwiczeń: 3.5 \n\ncat(\"Odch. Stand. Ćwiczeń:\", sd(practice), \"\\n\")\n\nOdch. Stand. Ćwiczeń: 1.870829 \n\ncat(\"Wariancja Umiejętności:\", var(skill), \"\\n\")\n\nWariancja Umiejętności: 6.8 \n\ncat(\"Odch. Stand. Umiejętności:\", sd(skill), \"\\n\")\n\nOdch. Stand. Umiejętności: 2.607681 \n\n\n\n\nKrok 3: Obliczenie Kowariancji\nKowariancja mierzy, jak dwie zmienne zmieniają się wspólnie. Dodatnia kowariancja wskazuje, że gdy jedna zmienna rośnie, druga również ma tendencję do wzrostu.\ns_{XY} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nObliczmy iloczyny dla każdej obserwacji:\n\n\n\n\n\n\n\n\n\nStudent\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n\n\n\n\n1\n-2.5\n-4\n(-2.5) \\times (-4) = 10.0\n\n\n2\n-1.5\n0\n(-1.5) \\times (0) = 0.0\n\n\n3\n-0.5\n-2\n(-0.5) \\times (-2) = 1.0\n\n\n4\n0.5\n1\n(0.5) \\times (1) = 0.5\n\n\n5\n1.5\n3\n(1.5) \\times (3) = 4.5\n\n\n6\n2.5\n2\n(2.5) \\times (2) = 5.0\n\n\nSuma\n\n\n21.0\n\n\n\ns_{XY} = \\frac{21.0}{5} = 4.2\n\n# Weryfikacja kowariancji\ncat(\"Kowariancja:\", cov(practice, skill), \"\\n\")\n\nKowariancja: 4.2 \n\n\n\n\nKrok 4: Obliczenie Współczynnika Korelacji Pearsona\nWspółczynnik korelacji standaryzuje kowariancję do skali od -1 do +1:\nr = \\frac{s_{XY}}{s_X \\cdot s_Y} = \\frac{4.2}{1.871 \\times 2.608} = \\frac{4.2}{4.879} = 0.861\nOtrzymujemy korelację wynoszącą 0.861, co wskazuje na silny dodatni związek między godzinami ćwiczeń a oceną umiejętności.\n\n# Weryfikacja korelacji\ncat(\"Korelacja:\", cor(practice, skill), \"\\n\")\n\nKorelacja: 0.8609161 \n\n\n\n\nKrok 5: Obliczenie Współczynników Regresji MNK\nMetoda Najmniejszych Kwadratów (MNK) znajduje wartości \\beta_0 i \\beta_1, które minimalizują sumę kwadratów błędów.\nEstymator nachylenia:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\nUżywając naszych obliczonych wartości:\n\\hat{\\beta_1} = \\frac{4.2}{3.5} = 1.2\nEstymator wyrazu wolnego:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 7 - (1.2 \\times 3.5) = 7 - 4.2 = 2.8\nNasze równanie regresji:\n\\hat{Y} = 2.8 + 1.2X\nTo oznacza:\n\nGdy godziny ćwiczeń = 0, przewidywana umiejętność = 2.8\nKażda dodatkowa godzina ćwiczeń zwiększa ocenę umiejętności o 1.2 punktu\n\n\n# Dopasowanie modelu\nmodel &lt;- lm(skill ~ practice)\ncoef_model &lt;- coef(model)\n\ncat(\"Wyraz wolny (β₀):\", coef_model[1], \"\\n\")\n\nWyraz wolny (β₀): 2.8 \n\ncat(\"Nachylenie (β₁):\", coef_model[2], \"\\n\")\n\nNachylenie (β₁): 1.2 \n\n# Obliczenie przewidywań\ndata$predicted &lt;- predict(model)\ndata$residual &lt;- residuals(model)\n\n\n\nKrok 6: Obliczenie Wartości Przewidywanych i Reszt\nUżywając \\hat{Y} = 2.8 + 1.2X:\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 2.8 + 1.2X_i\nReszta (Y_i - \\hat{Y}_i)\n\n\n\n\n1\n1\n3\n2.8 + 1.2(1) = 4.0\n3 - 4.0 = -1.0\n\n\n2\n2\n7\n2.8 + 1.2(2) = 5.2\n7 - 5.2 = 1.8\n\n\n3\n3\n5\n2.8 + 1.2(3) = 6.4\n5 - 6.4 = -1.4\n\n\n4\n4\n8\n2.8 + 1.2(4) = 7.6\n8 - 7.6 = 0.4\n\n\n5\n5\n10\n2.8 + 1.2(5) = 8.8\n10 - 8.8 = 1.2\n\n\n6\n6\n9\n2.8 + 1.2(6) = 10.0\n9 - 10.0 = -1.0\n\n\n\n\n\nKrok 7: Obliczenie Sum Kwadratów\nSST (Całkowita Suma Kwadratów) - Całkowita zmienność Y:\nSST = \\sum(Y_i - \\bar{Y})^2\nZ naszych wcześniejszych obliczeń wariancji:\nSST = (n-1) \\times s^2_Y = 5 \\times 6.8 = 34\nSSE (Suma Kwadratów Błędów) - Zmienność niewyjaśniona:\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n\n\n\n\n\n\n\n\n\n\nStudent\nY_i\n\\hat{Y}_i\n(Y_i - \\hat{Y}_i)\n(Y_i - \\hat{Y}_i)^2\n\n\n\n\n1\n3\n4.0\n-1.0\n1.00\n\n\n2\n7\n5.2\n1.8\n3.24\n\n\n3\n5\n6.4\n-1.4\n1.96\n\n\n4\n8\n7.6\n0.4\n0.16\n\n\n5\n10\n8.8\n1.2\n1.44\n\n\n6\n9\n10.0\n-1.0\n1.00\n\n\nSuma\n\n\n\n8.80\n\n\n\nSSE = 8.80\nSSR (Suma Kwadratów Regresji) - Zmienność wyjaśniona:\nSSR = SST - SSE = 34 - 8.80 = 25.20\n\n\nKrok 8: Obliczenie R-kwadrat\nR-kwadrat informuje nas, jaka część całkowitej zmienności Y jest wyjaśniona przez nasz model:\nR^2 = \\frac{SSR}{SST} = \\frac{25.20}{34} = 0.741\nAlternatywny wzór:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{8.80}{34} = 1 - 0.259 = 0.741\nLub po prostu:\nR^2 = r^2 = (0.861)^2 = 0.741\nTo oznacza, że nasz model wyjaśnia 74.1% zmienności ocen umiejętności!\n\n# Weryfikacja sum kwadratów i R-kwadrat\nSST &lt;- sum((skill - mean_y)^2)\nSSR &lt;- sum((data$predicted - mean_y)^2)\nSSE &lt;- sum(data$residual^2)\nr_squared &lt;- SSR / SST\n\ncat(\"SST (Całkowita):\", SST, \"\\n\")\n\nSST (Całkowita): 34 \n\ncat(\"SSR (Wyjaśniona):\", SSR, \"\\n\")\n\nSSR (Wyjaśniona): 25.2 \n\ncat(\"SSE (Niewyjaśniona):\", SSE, \"\\n\")\n\nSSE (Niewyjaśniona): 8.8 \n\ncat(\"R-kwadrat:\", r_squared, \"\\n\")\n\nR-kwadrat: 0.7411765 \n\ncat(\"R-kwadrat (z korelacji):\", cor(practice, skill)^2, \"\\n\")\n\nR-kwadrat (z korelacji): 0.7411765 \n\n\n\n\n\nWizualizacja 1: Linia Najlepszego Dopasowania MNK\nTen wykres pokazuje, jak MNK minimalizuje sumę kwadratów reszt (pionowe odległości od punktów do linii).\n\nggplot(data, aes(x = Practice, y = Skill)) +\n  geom_hline(yintercept = mean_y, linetype = \"dashed\", \n             color = \"gray50\", linewidth = 0.8, alpha = 0.7) +\n  geom_segment(aes(xend = Practice, yend = predicted), \n               color = \"red\", linetype = \"dotted\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1.2) +\n  geom_point(size = 5, color = \"darkblue\") +\n  geom_point(aes(y = predicted), size = 3, color = \"blue\", shape = 17) +\n  annotate(\"text\", x = 1.5, y = mean_y + 0.3, \n           label = paste0(\"Średnia (ȳ = \", mean_y, \")\"), \n           color = \"gray30\", size = 4) +\n  annotate(\"text\", x = 5, y = 4, \n           label = \"Reszty (błędy)\\nMNK minimalizuje Σ(reszty²)\", \n           color = \"red\", size = 3.5, hjust = 0) +\n  annotate(\"text\", x = 2, y = 10.5, \n           label = paste0(\"ŷ = \", round(coef_model[1], 1), \n                         \" + \", round(coef_model[2], 1), \"x\"), \n           color = \"blue\", size = 5, fontface = \"bold\") +\n  labs(\n    title = \"Regresja MNK: Minimalizacja Kwadratów Reszt\",\n    subtitle = \"Niebieskie trójkąty to wartości przewidywane; czerwone linie pokazują reszty\",\n    x = \"Godziny Ćwiczeń\",\n    y = \"Ocena Umiejętności\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKluczowa Obserwacja: MNK znajduje unikalną linię, która czyni sumę kwadratów czerwonych odległości jak najmniejszą!\n\n\nWizualizacja 2: Dekompozycja Wariancji (SST = SSR + SSE)\nTo pokazuje, jak całkowita zmienność jest dzielona na składniki wyjaśniony i niewyjaśniony.\n\n# Tworzenie wykresu dekompozycji\nggplot(data, aes(x = Practice)) +\n  # Całkowite odchylenie (SST)\n  geom_segment(aes(y = mean_y, yend = Skill, xend = Practice), \n               color = \"purple\", linewidth = 1.2, alpha = 0.6,\n               arrow = arrow(length = unit(0.15, \"inches\"))) +\n  \n  # Odchylenie wyjaśnione (SSR)\n  geom_segment(aes(y = mean_y, yend = predicted, xend = Practice), \n               color = \"green\", linewidth = 1.5, alpha = 0.8) +\n  \n  # Odchylenie niewyjaśnione (SSE)\n  geom_segment(aes(y = predicted, yend = Skill, xend = Practice), \n               color = \"red\", linewidth = 1.2, alpha = 0.8,\n               linetype = \"dashed\") +\n  \n  # Linia średniej\n  geom_hline(yintercept = mean_y, linetype = \"solid\", \n             color = \"gray40\", linewidth = 1) +\n  \n  # Linia regresji\n  geom_smooth(aes(y = Skill), method = \"lm\", se = FALSE, \n              color = \"blue\", linewidth = 1) +\n  \n  # Punkty\n  geom_point(aes(y = Skill), size = 5, color = \"darkblue\") +\n  geom_point(aes(y = predicted), size = 3, color = \"blue\", shape = 15) +\n  \n  # Adnotacje\n  annotate(\"text\", x = 6.5, y = mean_y, \n           label = \"Średnia\", color = \"gray40\", size = 4, hjust = 0) +\n  annotate(\"text\", x = 6.5, y = 9.5, \n           label = \"Linia Regresji\", color = \"blue\", size = 4, hjust = 0) +\n  \n  # Legenda\n  annotate(\"segment\", x = 0.5, xend = 0.5, y = 2, yend = 3.5,\n           color = \"purple\", linewidth = 1.2, \n           arrow = arrow(length = unit(0.12, \"inches\"))) +\n  annotate(\"text\", x = 0.7, y = 2.75, \n           label = \"Całkowite (SST)\", color = \"purple\", \n           size = 3.5, hjust = 0, fontface = \"bold\") +\n  \n  annotate(\"segment\", x = 0.5, xend = 0.5, y = 4.5, yend = 5.5,\n           color = \"green\", linewidth = 1.5) +\n  annotate(\"text\", x = 0.7, y = 5, \n           label = \"Wyjaśnione (SSR)\", color = \"green\", \n           size = 3.5, hjust = 0, fontface = \"bold\") +\n  \n  annotate(\"segment\", x = 0.5, xend = 0.5, y = 6.5, yend = 7.5,\n           color = \"red\", linewidth = 1.2, linetype = \"dashed\") +\n  annotate(\"text\", x = 0.7, y = 7, \n           label = \"Niewyjaśnione (SSE)\", color = \"red\", \n           size = 3.5, hjust = 0, fontface = \"bold\") +\n  \n  labs(\n    title = \"Dekompozycja Wariancji: SST = SSR + SSE\",\n    subtitle = \"Fioletowe = odchylenie całkowite | Zielone = wyjaśnione przez model | Czerwone = błąd resztowy\",\n    x = \"Godziny Ćwiczeń\",\n    y = \"Ocena Umiejętności\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 14)) +\n  coord_cartesian(xlim = c(0.3, 7.5), ylim = c(2, 11))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nDekompozycja Matematyczna:\nDla każdej obserwacji: (Y_i - \\bar{Y})^2 = (\\hat{Y}_i - \\bar{Y})^2 + (Y_i - \\hat{Y}_i)^2\n\nFioletowe strzałki: Całkowite odchylenie od średniej = (Y_i - \\bar{Y})\nZielone słupki: Wyjaśnienie modelu = (\\hat{Y}_i - \\bar{Y})\nCzerwone przerywane: Co zostaje = (Y_i - \\hat{Y}_i)\n\n\n\nWizualizacja 3: R-kwadrat jako Proporcja\n\n# Obliczenie sum kwadratów\nSST &lt;- sum((skill - mean_y)^2)\nSSR &lt;- sum((data$predicted - mean_y)^2)\nSSE &lt;- sum(data$residual^2)\nr_squared &lt;- SSR / SST\n\n# Dane do wykresu słupkowego\nss_data &lt;- data.frame(\n  Component = c(\"Całkowita (SST)\", \"Wyjaśniona (SSR)\", \"Niewyjaśniona (SSE)\"),\n  Value = c(SST, SSR, SSE),\n  Color = c(\"purple\", \"green\", \"red\")\n)\n\np1 &lt;- ggplot(ss_data, aes(x = Component, y = Value, fill = Component)) +\n  geom_bar(stat = \"identity\", alpha = 0.7, color = \"black\") +\n  geom_text(aes(label = round(Value, 2)), vjust = -0.5, size = 5, fontface = \"bold\") +\n  scale_fill_manual(values = c(\"green\", \"red\", \"purple\")) +\n  labs(\n    title = \"Rozkład Sum Kwadratów\",\n    y = \"Suma Kwadratów\",\n    x = \"\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"))\n\n# Wizualizacja proporcji\nprop_data &lt;- data.frame(\n  Component = c(\"Wyjaśniona\\n(SSR)\", \"Niewyjaśniona\\n(SSE)\"),\n  Proportion = c(SSR/SST, SSE/SST),\n  Percentage = c(SSR/SST * 100, SSE/SST * 100)\n)\n\np2 &lt;- ggplot(prop_data, aes(x = \"\", y = Proportion, fill = Component)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\", linewidth = 2) +\n  coord_polar(theta = \"y\") +\n  scale_fill_manual(values = c(\"green3\", \"red3\")) +\n  geom_text(aes(label = paste0(round(Percentage, 1), \"%\")), \n            position = position_stack(vjust = 0.5), \n            size = 6, fontface = \"bold\", color = \"white\") +\n  labs(title = paste0(\"R² = \", round(r_squared, 3))) +\n  theme_void(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n        legend.position = \"bottom\",\n        legend.title = element_blank())\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\nWzory na R-kwadrat:\nR^2 = \\frac{SSR}{SST} = \\frac{25.20}{34} = 0.74\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{8.80}{34} = 0.74\nInterpretacja: Nasz model wyjaśnia 74% zmienności ocen umiejętności!\n\n\nWizualizacja 4: R² jako Korelacja między Odchyleniami\nTo pokazuje geometryczną interpretację R²: jak dobrze przewidywane odchylenia od średniej pasują do rzeczywistych odchyleń.\n\n# Obliczenie odchyleń\ndata$actual_dev &lt;- skill - mean_y\ndata$predicted_dev &lt;- data$predicted - mean_y\n\n# Porównanie obok siebie\np1 &lt;- ggplot(data, aes(x = Practice)) +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.8) +\n  geom_segment(aes(y = 0, yend = actual_dev, xend = Practice), \n               color = \"purple\", linewidth = 2, alpha = 0.7,\n               arrow = arrow(length = unit(0.15, \"inches\"))) +\n  geom_point(aes(y = actual_dev), size = 4, color = \"purple\") +\n  labs(\n    title = \"Rzeczywiste Odchylenia od Średniej\",\n    subtitle = expression(Y[i] - bar(Y)),\n    x = \"Godziny Ćwiczeń\",\n    y = \"Odchylenie od Średniej\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(data, aes(x = Practice)) +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.8) +\n  geom_segment(aes(y = 0, yend = predicted_dev, xend = Practice), \n               color = \"green\", linewidth = 2, alpha = 0.7,\n               arrow = arrow(length = unit(0.15, \"inches\"))) +\n  geom_point(aes(y = predicted_dev), size = 4, color = \"green\") +\n  labs(\n    title = \"Przewidywane Odchylenia od Średniej\",\n    subtitle = expression(hat(Y)[i] - bar(Y)),\n    x = \"Godziny Ćwiczeń\",\n    y = \"Odchylenie od Średniej\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(plot.title = element_text(face = \"bold\"))\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nWizualizacja 5: Wykres Rozrzutu Odchyleń (Interpretacja R²)\n\nggplot(data, aes(x = predicted_dev, y = actual_dev)) +\n  geom_abline(slope = 1, intercept = 0, \n              linetype = \"dashed\", color = \"gray50\", linewidth = 1) +\n  geom_point(size = 5, color = \"darkblue\", alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\", \n              fill = \"lightblue\", alpha = 0.3) +\n  annotate(\"text\", x = -3, y = 2.5, \n           label = paste0(\"Gdyby dopasowanie idealne:\\nwszystkie punkty na tej linii\\n(R² = 1)\"), \n           color = \"gray40\", size = 3.5, hjust = 0) +\n  annotate(\"text\", x = 1.5, y = -3, \n           label = paste0(\"Korelacja = \", round(sqrt(r_squared), 3), \n                         \"\\nR² = \", round(r_squared, 3)), \n           color = \"blue\", size = 5, fontface = \"bold\") +\n  labs(\n    title = \"R² Mierzy, Jak Dobrze Przewidywane Odchylenia Pasują do Rzeczywistych\",\n    subtitle = \"Każdy punkt porównuje przewidywanie modelu z rzeczywistością (obie względem średniej)\",\n    x = expression(paste(\"Odchylenie Przewidywane: \", hat(Y)[i] - bar(Y))),\n    y = expression(paste(\"Odchylenie Rzeczywiste: \", Y[i] - bar(Y)))\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", size = 13))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKluczowa Obserwacja:\nR² to dosłownie kwadrat korelacji między:\n\nTym, co przewiduje model (względem średniej)\nTym, co rzeczywiście się zdarzyło (względem średniej)\n\nJeśli R^2 = 1: wszystkie punkty leżą dokładnie na przekątnej (idealne przewidywania)\nJeśli R^2 = 0: brak związku między przewidywanymi a rzeczywistymi odchyleniami\n\n\nTabela Podsumowująca\n\n# Tworzenie tabeli podsumowującej\nsummary_stats &lt;- data.frame(\n  Statystyka = c(\"Liczebność próby (n)\", \n                \"Średnia Ćwiczeń (X̄)\", \n                \"Średnia Umiejętności (Ȳ)\",\n                \"Korelacja (r)\",\n                \"Wyraz wolny (β₀)\",\n                \"Nachylenie (β₁)\",\n                \"R-kwadrat\",\n                \"SST (Całkowita)\",\n                \"SSR (Wyjaśniona)\",\n                \"SSE (Niewyjaśniona)\"),\n  Wartość = c(6,\n            round(mean_x, 2),\n            round(mean_y, 2),\n            round(cor(practice, skill), 3),\n            round(coef_model[1], 2),\n            round(coef_model[2], 2),\n            round(r_squared, 3),\n            round(SST, 2),\n            round(SSR, 2),\n            round(SSE, 2))\n)\n\nknitr::kable(summary_stats, align = c(\"l\", \"r\"))\n\n\n\n\nStatystyka\nWartość\n\n\n\n\nLiczebność próby (n)\n6.000\n\n\nŚrednia Ćwiczeń (X̄)\n3.500\n\n\nŚrednia Umiejętności (Ȳ)\n7.000\n\n\nKorelacja (r)\n0.861\n\n\nWyraz wolny (β₀)\n2.800\n\n\nNachylenie (β₁)\n1.200\n\n\nR-kwadrat\n0.741\n\n\nSST (Całkowita)\n34.000\n\n\nSSR (Wyjaśniona)\n25.200\n\n\nSSE (Niewyjaśniona)\n8.800\n\n\n\n\n\n\n\nKluczowe Wnioski\nRegresja MNK:\n\nZnajduje linię minimalizującą sumę kwadratów reszt\nProdukuje nieobciążone estymatory o minimalnej wariancji (BLUE)\n\nR-kwadrat (0.74) oznacza:\n\n74% zmienności umiejętności jest wyjaśnione przez godziny ćwiczeń\nKorelacja między przewidywanymi a rzeczywistymi odchyleniami wynosi \\sqrt{0.74} = 0.86\nSSR stanowi 74% SST; SSE stanowi 26% SST\n\nInterpretacja Geometryczna:\n\nCałkowita zmienność = odległość każdego punktu od średniej\nModel uchwytuje 74% tych odległości przez linię regresji\nPozostałe 26% jest niewyjaśnione (reszty)\n\nImplikacja Praktyczna:\nKażda dodatkowa godzina ćwiczeń zwiększa oczekiwaną umiejętność o 1.2 punktu, a ten związek wyjaśnia większość (ale nie całość) obserwowanej zmienności!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przykład-1.-analiza-związku-między-dobrobytem-ekonomicznym-a-frekwencją-wyborczą",
    "href": "correg_pl.html#przykład-1.-analiza-związku-między-dobrobytem-ekonomicznym-a-frekwencją-wyborczą",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.17 Przykład 1. Analiza związku między dobrobytem ekonomicznym a frekwencją wyborczą",
    "text": "10.17 Przykład 1. Analiza związku między dobrobytem ekonomicznym a frekwencją wyborczą\nAnaliza związku między dobrobytem ekonomicznym a frekwencją wyborczą w dzielnicach średniej wielkości europejskiego miasta na podstawie danych z wyborów samorządowych.\n\nDane\nPróba obejmuje pięć reprezentatywnych dzielnic:\n\n\n\nDzielnica\nDochód (tys. €)\nFrekwencja (%)\n\n\n\n\nDzielnica A\n50\n60\n\n\nDzielnica B\n45\n56\n\n\nDzielnica C\n56\n70\n\n\nDzielnica D\n40\n50\n\n\nDzielnica E\n60\n75\n\n\n\n\n# Wczytanie bibliotek\nlibrary(tidyverse)\n\n# Utworzenie zbioru danych\ndane &lt;- data.frame(\n  dzielnica = c(\"Dzielnica A\", \"Dzielnica B\", \"Dzielnica C\", \"Dzielnica D\", \"Dzielnica E\"),\n  dochod = c(50, 45, 56, 40, 60),\n  frekwencja = c(60, 56, 70, 50, 75)\n)\n\n# Podgląd danych\ndane\n\n    dzielnica dochod frekwencja\n1 Dzielnica A     50         60\n2 Dzielnica B     45         56\n3 Dzielnica C     56         70\n4 Dzielnica D     40         50\n5 Dzielnica E     60         75\n\n\n\n\nCzęść 1: Statystyki opisowe\n\n# Statystyki dla dochodu\ncat(\"DOCHÓD (tys. €):\\n\")\n\nDOCHÓD (tys. €):\n\ncat(\"Średnia:\", mean(dane$dochod), \"\\n\")\n\nŚrednia: 50.2 \n\ncat(\"Mediana:\", median(dane$dochod), \"\\n\")\n\nMediana: 50 \n\ncat(\"Odchylenie standardowe:\", round(sd(dane$dochod), 2), \"\\n\")\n\nOdchylenie standardowe: 8.07 \n\ncat(\"Zakres:\", min(dane$dochod), \"-\", max(dane$dochod), \"\\n\\n\")\n\nZakres: 40 - 60 \n\n# Statystyki dla frekwencji\ncat(\"FREKWENCJA (%):\\n\")\n\nFREKWENCJA (%):\n\ncat(\"Średnia:\", mean(dane$frekwencja), \"\\n\")\n\nŚrednia: 62.2 \n\ncat(\"Mediana:\", median(dane$frekwencja), \"\\n\")\n\nMediana: 60 \n\ncat(\"Odchylenie standardowe:\", round(sd(dane$frekwencja), 2), \"\\n\")\n\nOdchylenie standardowe: 10.21 \n\ncat(\"Zakres:\", min(dane$frekwencja), \"-\", max(dane$frekwencja))\n\nZakres: 50 - 75\n\n\n\n\nCzęść 2: Analiza korelacji\n\n# Test korelacji Pearsona\nkorelacja &lt;- cor.test(dane$dochod, dane$frekwencja)\n\ncat(\"Współczynnik korelacji (r):\", round(korelacja$estimate, 3), \"\\n\")\n\nWspółczynnik korelacji (r): 0.994 \n\ncat(\"P-value:\", round(korelacja$p.value, 3), \"\\n\")\n\nP-value: 0.001 \n\n# Interpretacja\nif (korelacja$p.value &lt; 0.05) {\n  cat(\"Wynik jest statystycznie istotny (p &lt; 0.05)\")\n} else {\n  cat(\"Wynik nie jest statystycznie istotny (p ≥ 0.05)\")\n}\n\nWynik jest statystycznie istotny (p &lt; 0.05)\n\n\n\n\nCzęść 3: Model regresji liniowej\n\n# Dopasowanie modelu\nmodel &lt;- lm(frekwencja ~ dochod, data = dane)\n\n# Podstawowe informacje o modelu\nsummary(model)\n\n\nCall:\nlm(formula = frekwencja ~ dochod, data = dane)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \ndochod       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n# R-squared - jak dobrze model wyjaśnia dane\ncat(\"\\nModel wyjaśnia\", round(summary(model)$r.squared * 100, 1), \"% zmienności frekwencji\")\n\n\nModel wyjaśnia 98.9 % zmienności frekwencji\n\n\n\n\nCzęść 4: Wizualizacja\n\n# Wykres rozrzutu z linią regresji\nggplot(dane, aes(x = dochod, y = frekwencja)) +\n  geom_point(size = 4, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\", alpha = 0.3) +\n  geom_text(aes(label = dzielnica), vjust = -1, size = 3) +\n  labs(\n    title = \"Dochód vs Frekwencja wyborcza\",\n    subtitle = paste(\"r =\", round(korelacja$estimate, 3)),\n    x = \"Średni dochód (tys. €)\",\n    y = \"Frekwencja wyborcza (%)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCzęść 5: Interpretacja wyników\n\n# Pobranie współczynników\nwspolczynniki &lt;- coef(model)\nprzeciecie &lt;- round(wspolczynniki[1], 1)\nnachylenie &lt;- round(wspolczynniki[2], 2)\n\ncat(\"RÓWNANIE REGRESJI:\\n\")\n\nRÓWNANIE REGRESJI:\n\ncat(\"Frekwencja =\", przeciecie, \"+\", nachylenie, \"× Dochód\\n\\n\")\n\nFrekwencja = -0.9 + 1.26 × Dochód\n\ncat(\"INTERPRETACJA:\\n\")\n\nINTERPRETACJA:\n\ncat(\"• Wzrost dochodu o 1000€ zwiększa frekwencję o\", nachylenie, \"punktów procentowych\\n\")\n\n• Wzrost dochodu o 1000€ zwiększa frekwencję o 1.26 punktów procentowych\n\ncat(\"• Korelacja jest\", ifelse(korelacja$estimate &gt; 0, \"dodatnia\", \"ujemna\"), \n    \"i\", ifelse(abs(korelacja$estimate) &gt; 0.7, \"silna\", \n               ifelse(abs(korelacja$estimate) &gt; 0.5, \"umiarkowana\", \"słaba\")))\n\n• Korelacja jest dodatnia i silna\n\n\n\n\nWnioski\nGłówne ustalenia:\n\nIstnieje silna dodatnia korelacja (r = 0.994) między dochodem a frekwencją wyborczą\nModel wyjaśnia 98.9% zmienności w danych\nDzielnice z wyższymi dochodami mają wyższą frekwencję wyborczą\n\nWażne ograniczenie:\n⚠️ Mała próba (n=5) oznacza, że wyniki należy traktować ostrożnie i nie można ich generalizować na całą populację bez dodatkowych badań.\nPraktyczne zastosowanie:\nWyniki sugerują, że działania mające na celu zwiększenie frekwencji wyborczej powinny szczególnie koncentrować się na dzielnicach o niższych dochodach.\nOgraniczenia i zastrzeżenia:\n⚠️ Krytyczne ograniczenia:\n\nBardzo mała próba (n=5) znacznie ogranicza możliwość generalizacji\nNiska moc statystyczna - ryzyko błędów II rodzaju\nBrak kontroli zmiennych zakłócających (wiek, wykształcenie, gęstość zaludnienia)\nMożliwa korelacja pozorna - potrzebne dodatkowe zmienne kontrolne\n\nRekomendacje dla przyszłych badań:\n\nZwiększenie próby do wszystkich dzielnic miasta\nWłączenie zmiennych demograficznych i socjoekonomicznych\nAnaliza danych longitudinalnych z kilku cykli wyborczych",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przykład-2.-związek-między-wielkością-okręgu-a-dysproporcjonalnością-wyborczą-1",
    "href": "correg_pl.html#przykład-2.-związek-między-wielkością-okręgu-a-dysproporcjonalnością-wyborczą-1",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.18 Przykład 2. Związek Między Wielkością Okręgu a Dysproporcjonalnością Wyborczą (1)",
    "text": "10.18 Przykład 2. Związek Między Wielkością Okręgu a Dysproporcjonalnością Wyborczą (1)\nTa analiza bada związek między wielkością okręgu wyborczego (DM) a wskaźnikiem dysproporcjonalności Loosemore-Hanby (LH) w wyborach parlamentarnych w 6 krajach. Indeks Loosemore-Hanby mierzy dysproporcjonalność wyborczą, gdzie wyższe wartości wskazują na większą dysproporcjonalność między głosami a mandatami.\n\nDane\n\n\nWarning: package 'knitr' was built under R version 4.4.3\n\n\n\nWielkość Okręgu i Indeks LH według Kraju\n\n\nCountry\nDM\nLH\n\n\n\n\nA\n3\n15.50\n\n\nB\n4\n14.25\n\n\nC\n5\n13.50\n\n\nD\n6\n13.50\n\n\nE\n7\n13.00\n\n\nF\n8\n12.75\n\n\n\n\n\n\n\nKrok 1: Obliczenie wariancji i odchylenia standardowego dla DM i LH\n\nObliczenia dla Wielkości Okręgu (DM)\nNajpierw obliczam średnią wartości DM:\n\\bar{x}_{DM} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\frac{3 + 4 + 5 + 6 + 7 + 8}{6} = \\frac{33}{6} = 5.5\nNastępnie obliczam wariancję używając formuły z korektą Bessela:\n\\sigma^2_{DM} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\n\n\nKraj\nDM (x_i)\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\n\n\n\n\nA\n3\n3 - 5.5 = -2.5\n(-2.5)^2 = 6.25\n\n\nB\n4\n4 - 5.5 = -1.5\n(-1.5)^2 = 2.25\n\n\nC\n5\n5 - 5.5 = -0.5\n(-0.5)^2 = 0.25\n\n\nD\n6\n6 - 5.5 = 0.5\n(0.5)^2 = 0.25\n\n\nE\n7\n7 - 5.5 = 1.5\n(1.5)^2 = 2.25\n\n\nF\n8\n8 - 5.5 = 2.5\n(2.5)^2 = 6.25\n\n\nSuma\n\n\n17.5\n\n\n\n\\sigma^2_{DM} = \\frac{17.5}{6-1} = \\frac{17.5}{5} = 3.5\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji:\n\\sigma_{DM} = \\sqrt{\\sigma^2_{DM}} = \\sqrt{3.5} = 1.871\n\n\nObliczenia dla Indeksu LH\nNajpierw obliczam średnią wartości LH:\n\\bar{x}_{LH} = \\frac{15.5 + 14.25 + 13.5 + 13.5 + 13 + 12.75}{6} = \\frac{82.5}{6} = 13.75\nNastępnie obliczam wariancję:\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n15.5\n15.5 - 13.75 = 1.75\n(1.75)^2 = 3.0625\n\n\nB\n14.25\n14.25 - 13.75 = 0.5\n(0.5)^2 = 0.25\n\n\nC\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nD\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nE\n13\n13 - 13.75 = -0.75\n(-0.75)^2 = 0.5625\n\n\nF\n12.75\n12.75 - 13.75 = -1\n(-1)^2 = 1\n\n\nSuma\n\n\n5\n\n\n\n\\sigma^2_{LH} = \\frac{5}{6-1} = \\frac{5}{5} = 1\nOdchylenie standardowe wynosi:\n\\sigma_{LH} = \\sqrt{\\sigma^2_{LH}} = \\sqrt{1} = 1\nPodsumowanie Zadania 1:\n\nWariancja DM (z korektą Bessela): 3.5\nOdchylenie Standardowe DM: 1.871\nWariancja LH (z korektą Bessela): 1\nOdchylenie Standardowe LH: 1\n\n\n\n\nKrok 2: Obliczenie kowariancji między DM i LH dla tej próby danych\nKowariancja jest obliczana przy użyciu formuły z korektą Bessela:\nCov(DM, LH) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\nA\n3\n15.5\n-2.5\n1.75\n(-2.5)(1.75) = -4.375\n\n\nB\n4\n14.25\n-1.5\n0.5\n(-1.5)(0.5) = -0.75\n\n\nC\n5\n13.5\n-0.5\n-0.25\n(-0.5)(-0.25) = 0.125\n\n\nD\n6\n13.5\n0.5\n-0.25\n(0.5)(-0.25) = -0.125\n\n\nE\n7\n13\n1.5\n-0.75\n(1.5)(-0.75) = -1.125\n\n\nF\n8\n12.75\n2.5\n-1\n(2.5)(-1) = -2.5\n\n\nSuma\n\n\n\n\n-8.75\n\n\n\nCov(DM, LH) = \\frac{-8.75}{5} = -1.75\nKowariancja między DM i LH: -1.75\nUjemna kowariancja wskazuje na odwrotną zależność: gdy wielkość okręgu wzrasta, indeks dysproporcjonalności LH ma tendencję do spadku.\n\n\nKrok 3: Obliczenie współczynnika korelacji liniowej Pearsona między DM i LH\nWspółczynnik korelacji Pearsona obliczany jest przy użyciu formuły:\nr = \\frac{Cov(DM, LH)}{\\sigma_{DM} \\cdot \\sigma_{LH}}\nMamy już obliczone:\n\nCov(DM, LH) = -1.75\n\\sigma_{DM} = 1.871\n\\sigma_{LH} = 1\n\nr = \\frac{-1.75}{1.871 \\cdot 1} = \\frac{-1.75}{1.871} = -0.935\nWspółczynnik korelacji Pearsona: -0.935\n\nInterpretacja:\nWspółczynnik korelacji -0.935 wskazuje:\n\nKierunek: Znak ujemny pokazuje odwrotną zależność między wielkością okręgu a indeksem LH.\nSiła: Wartość bezwzględna 0.935 wskazuje na bardzo silną korelację (blisko -1).\nInterpretacja praktyczna: Ponieważ wyższe wartości indeksu LH wskazują na większą dysproporcjonalność, ta silna ujemna korelacja sugeruje, że gdy wielkość okręgu wzrasta, dysproporcjonalność wyborcza ma tendencję do znacznego spadku. Innymi słowy, systemy wyborcze z większymi okręgami (więcej przedstawicieli wybieranych z jednego okręgu) zwykle dają bardziej proporcjonalne wyniki (niższa dysproporcjonalność).\n\nOdkrycie to jest zgodne z teorią nauk politycznych, która sugeruje, że większe okręgi zapewniają więcej możliwości mniejszym partiom, aby uzyskać reprezentację, co prowadzi do wyników wyborczych, które lepiej odzwierciedlają rozkład głosów między partiami.\n\n\n\nKrok 4: Skonstruowanie modelu regresji liniowej prostej i obliczenie R-kwadrat\nFormuła dla regresji liniowej prostej:\nLH = \\beta_0 + \\beta_1 \\cdot DM + \\varepsilon\nGdzie:\n\n\\beta_0 to wyraz wolny (przecięcie)\n\\beta_1 to współczynnik nachylenia dla DM\n\nFormuła do obliczenia \\beta_1 to:\n\\beta_1 = \\frac{Cov(DM, LH)}{\\sigma^2_{DM}} = \\frac{-1.75}{3.5} = -0.5\nAby obliczyć \\beta_0, używam:\n\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x} = 13.75 - (-0.5) \\cdot 5.5 = 13.75 + 2.75 = 16.5\nZatem równanie regresji to:\nLH = 16.5 - 0.5 \\cdot DM\n\nObliczanie wartości przewidywanych i błędów\nUżywając naszego równania regresji, obliczam przewidywane wartości LH:\n\\hat{LH} = 16.5 - 0.5 \\cdot DM\n\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i = 16.5 - 0.5x_i)\nBłąd (y_i - \\hat{y}_i)\nBłąd bezwzględny (|y_i - \\hat{y}_i|)\nBłąd kwadratowy ([y_i - \\hat{y}_i]^2)\n\n\n\n\nA\n3\n15.5\n16.5 - 0.5(3) = 16.5 - 1.5 = 15\n15.5 - 15 = 0.5\n|0.5| = 0.5\n(0.5)^2 = 0.25\n\n\nB\n4\n14.25\n16.5 - 0.5(4) = 16.5 - 2 = 14.5\n14.25 - 14.5 = -0.25\n|-0.25| = 0.25\n(-0.25)^2 = 0.0625\n\n\nC\n5\n13.5\n16.5 - 0.5(5) = 16.5 - 2.5 = 14\n13.5 - 14 = -0.5\n|-0.5| = 0.5\n(-0.5)^2 = 0.25\n\n\nD\n6\n13.5\n16.5 - 0.5(6) = 16.5 - 3 = 13.5\n13.5 - 13.5 = 0\n|0| = 0\n(0)^2 = 0\n\n\nE\n7\n13\n16.5 - 0.5(7) = 16.5 - 3.5 = 13\n13 - 13 = 0\n|0| = 0\n(0)^2 = 0\n\n\nF\n8\n12.75\n16.5 - 0.5(8) = 16.5 - 4 = 12.5\n12.75 - 12.5 = 0.25\n|0.25| = 0.25\n(0.25)^2 = 0.0625\n\n\nSuma\n\n\n\n\n1.5\n0.625\n\n\n\n\n\nObliczanie R-kwadrat\n\nSST (Całkowita suma kwadratów)\n\nSST = \\sum_{i=1}^{n}(y_i - \\bar{y})^2\nObliczyliśmy już te odchylenia w Kroku 1:\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n15.5\n1.75\n3.0625\n\n\nB\n14.25\n0.5\n0.25\n\n\nC\n13.5\n-0.25\n0.0625\n\n\nD\n13.5\n-0.25\n0.0625\n\n\nE\n13\n-0.75\n0.5625\n\n\nF\n12.75\n-1\n1\n\n\nSuma\n\n\n5\n\n\n\nSST = 5\n\nSSR (Suma kwadratów regresji)\n\nSSR = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2\n\n\n\n\n\n\n\n\n\nKraj\nPrzewidywane LH (\\hat{y}_i)\n\\hat{y}_i - \\bar{y}\n(\\hat{y}_i - \\bar{y})^2\n\n\n\n\nA\n15\n15 - 13.75 = 1.25\n(1.25)^2 = 1.5625\n\n\nB\n14.5\n14.5 - 13.75 = 0.75\n(0.75)^2 = 0.5625\n\n\nC\n14\n14 - 13.75 = 0.25\n(0.25)^2 = 0.0625\n\n\nD\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nE\n13\n13 - 13.75 = -0.75\n(-0.75)^2 = 0.5625\n\n\nF\n12.5\n12.5 - 13.75 = -1.25\n(-1.25)^2 = 1.5625\n\n\nSuma\n\n\n4.375\n\n\n\nSSR = 4.375\n\nSSE (Suma kwadratów błędów)\n\nSSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\nZ tabeli powyżej, suma kwadratów błędów wynosi:\nSSE = 0.625\n\nWeryfikacja\n\nMożemy zweryfikować nasze obliczenia sprawdzając, czy SST = SSR + SSE:\n5 = 4.375 + 0.625 = 5 \\checkmark\n\nObliczanie R-kwadrat\n\nR^2 = \\frac{SSR}{SST} = \\frac{4.375}{5} = 0.875\n\n\nObliczanie RMSE (Root Mean Square Error)\nRMSE jest obliczane przy użyciu formuły:\nRMSE = \\sqrt{\\frac{SSE}{n}} = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}}\nUżywając naszego obliczonego SSE:\nRMSE = \\sqrt{\\frac{0.625}{6}} = \\sqrt{0.104} \\approx 0.323\n\n\nObliczanie MAE (Mean Absolute Error)\nMAE jest obliczane przy użyciu formuły:\nMAE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}\nUżywając sum z tabeli:\nMAE = \\frac{1.5}{6} = 0.25\nModel regresji: LH = 16.5 - 0.5 \\cdot DM\nR-kwadrat: 0.875\nRMSE: 0.323\nMAE: 0.25\n\n\nInterpretacja:\n\nRównanie regresji: Dla każdego wzrostu jednostkowego wielkości okręgu, indeks dysproporcjonalności LH jest oczekiwany spadek o 0.5 jednostki. Wyraz wolny (16.5) reprezentuje oczekiwany indeks LH, gdy wielkość okręgu wynosi zero (choć nie ma to praktycznego znaczenia, ponieważ wielkość okręgu nie może wynosić zero).\nR-kwadrat: 0.875 wskazuje, że około 87.5% wariancji w dysproporcjonalności wyborczej (indeks LH) może być wyjaśnione przez wielkość okręgu. Jest to wysoka wartość, sugerująca, że wielkość okręgu jest rzeczywiście silnym predyktorem dysproporcjonalności wyborczej.\nRMSE i MAE: Niskie wartości RMSE (0.323) i MAE (0.25) wskazują, że model dobrze dopasowuje się do danych, z małymi błędami predykcji.\nImplikacje polityczne: Odkrycia sugerują, że zwiększanie wielkości okręgu mogłoby być skuteczną strategią reformy wyborczej dla krajów dążących do zmniejszenia dysproporcjonalności między głosami a mandatami. Jednak korzyści marginalne wydają się zmniejszać wraz ze wzrostem wielkości okręgu, jak widać w wzorcu danych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przykład-3.-analiza-związku-między-wielkością-okręgu-a-wskaźnikiem-dysproporcjonalności-wyborczej-2",
    "href": "correg_pl.html#przykład-3.-analiza-związku-między-wielkością-okręgu-a-wskaźnikiem-dysproporcjonalności-wyborczej-2",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.19 Przykład 3. Analiza związku między wielkością okręgu a wskaźnikiem dysproporcjonalności wyborczej (2)",
    "text": "10.19 Przykład 3. Analiza związku między wielkością okręgu a wskaźnikiem dysproporcjonalności wyborczej (2)\nTa analiza bada związek między wielkością okręgu (DM) a wskaźnikiem dysproporcjonalności Loosemore-Hanby (LH) w wyborach parlamentarnych w 6 krajach. Wskaźnik Loosemore-Hanby mierzy dysproporcjonalność wyborczą, przy czym wyższe wartości wskazują na większą dysproporcjonalność między głosami a mandatami.\n\nDane\n\n\n\nWielkość okręgu i wskaźnik LH według kraju\n\n\nKraj\nDM\nLH\n\n\n\n\nA\n4\n12\n\n\nB\n10\n8\n\n\nC\n3\n15\n\n\nD\n8\n10\n\n\nE\n7\n6\n\n\nF\n4\n13\n\n\n\n\n\n\n\nKrok 1: Obliczenie wariancji i odchylenia standardowego dla DM i LH\n\nObliczenia dla wielkości okręgu (DM)\nNajpierw obliczę średnią wartości DM:\n\\bar{x}_{DM} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\frac{4 + 10 + 3 + 8 + 7 + 4}{6} = \\frac{36}{6} = 6\nNastępnie obliczę wariancję z korektą Bessela, korzystając z wzoru:\ns^2_{DM} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\n\n\nKraj\nDM (x_i)\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\n\n\n\n\nA\n4\n4 - 6 = -2\n(-2)^2 = 4\n\n\nB\n10\n10 - 6 = 4\n(4)^2 = 16\n\n\nC\n3\n3 - 6 = -3\n(-3)^2 = 9\n\n\nD\n8\n8 - 6 = 2\n(2)^2 = 4\n\n\nE\n7\n7 - 6 = 1\n(1)^2 = 1\n\n\nF\n4\n4 - 6 = -2\n(-2)^2 = 4\n\n\nSuma\n\n\n38\n\n\n\ns^2_{DM} = \\frac{38}{6-1} = \\frac{38}{5} = 7.6\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji:\ns_{DM} = \\sqrt{s^2_{DM}} = \\sqrt{7.6} = 2.757\n\n\nObliczenia dla wskaźnika LH\nNajpierw obliczę średnią wartości LH:\n\\bar{y}_{LH} = \\frac{12 + 8 + 15 + 10 + 6 + 13}{6} = \\frac{64}{6} = 10.667\nNastępnie obliczę wariancję z korektą Bessela:\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n12\n12 - 10.667 = 1.333\n(1.333)^2 = 1.777\n\n\nB\n8\n8 - 10.667 = -2.667\n(-2.667)^2 = 7.113\n\n\nC\n15\n15 - 10.667 = 4.333\n(4.333)^2 = 18.775\n\n\nD\n10\n10 - 10.667 = -0.667\n(-0.667)^2 = 0.445\n\n\nE\n6\n6 - 10.667 = -4.667\n(-4.667)^2 = 21.781\n\n\nF\n13\n13 - 10.667 = 2.333\n(2.333)^2 = 5.443\n\n\nSuma\n\n\n55.334\n\n\n\ns^2_{LH} = \\frac{55.334}{6-1} = \\frac{55.334}{5} = 11.067\nOdchylenie standardowe to:\ns_{LH} = \\sqrt{s^2_{LH}} = \\sqrt{11.067} = 3.327\nPodsumowanie kroku 1:\n\nWariancja DM (z korektą Bessela): 7.6\nOdchylenie standardowe DM: 2.757\nWariancja LH (z korektą Bessela): 11.067\nOdchylenie standardowe LH: 3.327\n\n\n\n\nKrok 2: Obliczenie kowariancji między DM a LH dla tej próby danych\nKowariancja jest obliczana przy użyciu wzoru z korektą Bessela:\nCov(DM, LH) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\nA\n4\n12\n-2\n1.333\n(-2)(1.333) = -2.666\n\n\nB\n10\n8\n4\n-2.667\n(4)(-2.667) = -10.668\n\n\nC\n3\n15\n-3\n4.333\n(-3)(4.333) = -12.999\n\n\nD\n8\n10\n2\n-0.667\n(2)(-0.667) = -1.334\n\n\nE\n7\n6\n1\n-4.667\n(1)(-4.667) = -4.667\n\n\nF\n4\n13\n-2\n2.333\n(-2)(2.333) = -4.666\n\n\nSuma\n\n\n\n\n-37\n\n\n\nCov(DM, LH) = \\frac{-37}{6-1} = \\frac{-37}{5} = -7.4\nKowariancja między DM a LH: -7.4\nUjemna kowariancja wskazuje na odwrotną zależność: wraz ze wzrostem wielkości okręgu wskaźnik dysproporcjonalności LH ma tendencję do spadku.\n\n\nKrok 3: Obliczenie współczynnika korelacji liniowej Pearsona między DM a LH\nWspółczynnik korelacji Pearsona oblicza się przy użyciu wzoru:\nr = \\frac{Cov(DM, LH)}{s_{DM} \\cdot s_{LH}}\nMamy już obliczone:\n\nCov(DM, LH) = -7.4\ns_{DM} = 2.757\ns_{LH} = 3.327\n\nr = \\frac{-7.4}{2.757 \\cdot 3.327} = \\frac{-7.4}{9.172} = -0.807\nWspółczynnik korelacji Pearsona: -0.807\n\nInterpretacja:\nWspółczynnik korelacji -0.807 wskazuje:\n\nKierunek: Ujemny znak pokazuje odwrotną zależność między wielkością okręgu a wskaźnikiem LH.\nSiła: Wartość bezwzględna 0.807 wskazuje na silną korelację (blisko -1).\nInterpretacja praktyczna: Ponieważ wyższe wartości wskaźnika LH wskazują na większą dysproporcjonalność, ta silna ujemna korelacja sugeruje, że wraz ze wzrostem wielkości okręgu, dysproporcjonalność wyborcza ma tendencję do znacznego spadku. Innymi słowy, systemy wyborcze z większymi okręgami wyborczymi (więcej przedstawicieli wybieranych w okręgu) mają tendencję do generowania bardziej proporcjonalnych wyników (mniejsza dysproporcjonalność).\n\nUstalenie to jest zgodne z teorią nauk politycznych, która sugeruje, że większe okręgi wyborcze zapewniają mniejszym partiom więcej możliwości uzyskania reprezentacji, co prowadzi do wyników wyborczych, które lepiej odzwierciedlają rozkład głosów między partiami.\n\n\n\nKrok 4: Skonstruowanie prostego modelu regresji liniowej i obliczenie R-kwadrat\nUżyję wzoru na prostą regresję liniową:\nLH = \\beta_0 + \\beta_1 \\cdot DM + \\varepsilon\nGdzie:\n\n\\beta_0 to wyraz wolny\n\\beta_1 to współczynnik nachylenia dla DM\n\nWzór na obliczenie \\beta_1 z korektą Bessela to:\n\\beta_1 = \\frac{Cov(DM, LH)}{s^2_{DM}} = \\frac{-7.4}{7.6} = -0.974\nAby obliczyć \\beta_0, użyję:\n\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x} = 10.667 - (-0.974) \\cdot 6 = 10.667 + 5.844 = 16.511\nZatem równanie regresji to:\nLH = 16.511 - 0.974 \\cdot DM\n\nObliczanie R-kwadrat\nAby właściwie obliczyć R-kwadrat, muszę obliczyć następujące sumy kwadratów:\n\nSST (Całkowita suma kwadratów): Mierzy całkowitą zmienność zmiennej zależnej (LH)\nSSR (Suma kwadratów regresji): Mierzy zmienność wyjaśnioną przez model regresji\nSSE (Suma kwadratów błędów): Mierzy niewyjaśnioną zmienność modelu\n\nNajpierw obliczę przewidywane wartości LH, używając naszego równania regresji:\n\\hat{LH} = 16.511 - 0.974 \\cdot DM\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i = 16.511 - 0.974x_i)\n\n\n\n\nA\n4\n12\n16.511 - 0.974(4) = 16.511 - 3.896 = 12.615\n\n\nB\n10\n8\n16.511 - 0.974(10) = 16.511 - 9.74 = 6.771\n\n\nC\n3\n15\n16.511 - 0.974(3) = 16.511 - 2.922 = 13.589\n\n\nD\n8\n10\n16.511 - 0.974(8) = 16.511 - 7.792 = 8.719\n\n\nE\n7\n6\n16.511 - 0.974(7) = 16.511 - 6.818 = 9.693\n\n\nF\n4\n13\n16.511 - 0.974(4) = 16.511 - 3.896 = 12.615\n\n\n\nTeraz, obliczę każdą sumę kwadratów:\n\nSST (Całkowita suma kwadratów)\n\nSST = \\sum_{i=1}^{n}(y_i - \\bar{y})^2\nJuż obliczyliśmy te odchylenia w Kroku 1:\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n12\n1.333\n1.777\n\n\nB\n8\n-2.667\n7.113\n\n\nC\n15\n4.333\n18.775\n\n\nD\n10\n-0.667\n0.445\n\n\nE\n6\n-4.667\n21.781\n\n\nF\n13\n2.333\n5.443\n\n\nSuma\n\n\n55.334\n\n\n\nSST = 55.334\n\nSSR (Suma kwadratów regresji)\n\nSSR = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2\n\n\n\n\n\n\n\n\n\nKraj\nPrzewidywane LH (\\hat{y}_i)\n\\hat{y}_i - \\bar{y}\n(\\hat{y}_i - \\bar{y})^2\n\n\n\n\nA\n12.615\n12.615 - 10.667 = 1.948\n(1.948)^2 = 3.795\n\n\nB\n6.771\n6.771 - 10.667 = -3.896\n(-3.896)^2 = 15.178\n\n\nC\n13.589\n13.589 - 10.667 = 2.922\n(2.922)^2 = 8.538\n\n\nD\n8.719\n8.719 - 10.667 = -1.948\n(-1.948)^2 = 3.795\n\n\nE\n9.693\n9.693 - 10.667 = -0.974\n(-0.974)^2 = 0.949\n\n\nF\n12.615\n12.615 - 10.667 = 1.948\n(1.948)^2 = 3.795\n\n\nSuma\n\n\n36.05\n\n\n\nSSR = 36.05\n\nSSE (Suma kwadratów błędów)\n\nSSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i)\ny_i - \\hat{y}_i\n(y_i - \\hat{y}_i)^2\n\n\n\n\nA\n12\n12.615\n12 - 12.615 = -0.615\n(-0.615)^2 = 0.378\n\n\nB\n8\n6.771\n8 - 6.771 = 1.229\n(1.229)^2 = 1.510\n\n\nC\n15\n13.589\n15 - 13.589 = 1.411\n(1.411)^2 = 1.991\n\n\nD\n10\n8.719\n10 - 8.719 = 1.281\n(1.281)^2 = 1.641\n\n\nE\n6\n9.693\n6 - 9.693 = -3.693\n(-3.693)^2 = 13.638\n\n\nF\n13\n12.615\n13 - 12.615 = 0.385\n(0.385)^2 = 0.148\n\n\nSuma\n\n\n\n19.306\n\n\n\nSSE = 19.306\n\nWeryfikacja\n\nMożemy zweryfikować nasze obliczenia, sprawdzając czy SST = SSR + SSE:\n55.334 \\approx 36.05 + 19.306 = 55.356\nNiewielka różnica (0.022) wynika z zaokrągleń w obliczeniach.\n\nObliczanie R-kwadrat\n\nR^2 = \\frac{SSR}{SST} = \\frac{36.05}{55.334} = 0.652\nAlternatywnie, możemy też obliczyć:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{19.306}{55.334} = 1 - 0.349 = 0.651\nDrobna różnica wynika z zaokrągleń.\n\n\nObliczanie RMSE (Pierwiastek średniego błędu kwadratowego)\nObliczanie RMSE (Pierwiastek średniego błędu kwadratowego)\nRMSE oblicza się przy użyciu wzoru:\nRMSE = \\sqrt{\\frac{SSE}{n}} = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}}\nKorzystając z naszej obliczonej SSE:\nRMSE = \\sqrt{\\frac{19.306}{6}} = \\sqrt{3.218} = 1.794\nKorekta Bessela (dzielenie przez n-1 zamiast n) stosuje się do estymacji wariancji próby, ale nie jest standardowo stosowana przy obliczaniu RMSE, gdyż RMSE jest miarą błędu predykcji, a nie estymatorem parametru populacji.\n\n\nObliczanie MAE (Średni błąd bezwzględny)\nMAE oblicza się przy użyciu wzoru:\nMAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i)\n|y_i - \\hat{y}_i|\n\n\n\n\nA\n12\n12.615\n|12 - 12.615| = 0.615\n\n\nB\n8\n6.771\n|8 - 6.771| = 1.229\n\n\nC\n15\n13.589\n|15 - 13.589| = 1.411\n\n\nD\n10\n8.719\n|10 - 8.719| = 1.281\n\n\nE\n6\n9.693\n|6 - 9.693| = 3.693\n\n\nF\n13\n12.615\n|13 - 12.615| = 0.385\n\n\nSuma\n\n\n8.614\n\n\n\nMAE = \\frac{8.614}{6} = 1.436\nModel regresji: LH = 16.511 - 0.974 \\cdot DM\nR-kwadrat: 0.651\nRMSE: 1.794\nMAE: 1.436\n\n\nInterpretacja:\n\nRównanie regresji: Dla każdego jednostkowego wzrostu wielkości okręgu, wskaźnik dysproporcjonalności LH zmniejsza się o 0.974 jednostki. Wyraz wolny (16.511) reprezentuje oczekiwany wskaźnik LH, gdy wielkość okręgu wynosi zero (choć nie ma to praktycznego znaczenia, ponieważ wielkość okręgu nie może wynosić zero).\nR-kwadrat: 0.651 wskazuje, że około 65.1% wariancji dysproporcjonalności wyborczej (wskaźnik LH) może być wyjaśnione przez wielkość okręgu. Jest to dość wysoka wartość, sugerująca, że wielkość okręgu jest rzeczywiście silnym predyktorem dysproporcjonalności wyborczej, choć mniejszym niż w poprzednim zestawie danych.\nRMSE: Wartość 1.794 informuje nas o przeciętnym błędzie prognozy modelu. Jest to miara dokładności przewidywań modelu wyrażona w jednostkach zmiennej zależnej (LH).\nMAE: Wartość 1.436 informuje nas o przeciętnym bezwzględnym błędzie prognozy modelu. W porównaniu z RMSE, MAE jest mniej czuły na wartości odstające, co potwierdza, że niektóre obserwacje (np. dla kraju E) mają stosunkowo duży błąd predykcji.\nImplikacje polityczne: Wyniki sugerują, że zwiększenie wielkości okręgu mogłoby być skuteczną strategią reform wyborczych dla krajów starających się zmniejszyć dysproporcjonalność między głosami a mandatami. Jednakże, korzyści marginalne wydają się zmniejszać wraz ze wzrostem wielkości okręgu, jak widać we wzorcu danych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przykład-4.-lęk-vs.-wyniki-egzaminu-analiza-korelacji-i-regresji",
    "href": "correg_pl.html#przykład-4.-lęk-vs.-wyniki-egzaminu-analiza-korelacji-i-regresji",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.20 Przykład 4. Lęk vs. Wyniki Egzaminu: Analiza Korelacji i Regresji",
    "text": "10.20 Przykład 4. Lęk vs. Wyniki Egzaminu: Analiza Korelacji i Regresji\nW tym tutorialu zbadamy związek między poziomem lęku przed egzaminem a wynikami egzaminacyjnymi wśród studentów uniwersytetu. Badania sugerują, że podczas gdy niewielka ilość lęku może być motywująca, nadmierny lęk zazwyczaj pogarsza wyniki poprzez zmniejszoną koncentrację, zakłócenia pamięci roboczej i objawy fizyczne (Yerkes-Dodson law). Przeanalizujemy dane od 8 studentów, aby zrozumieć ten związek matematycznie.\n\nPrezentacja Danych\n\nZbiór Danych\nZebraliśmy dane od 8 studentów, mierząc:\n\nX: Wynik lęku przed testem (skala 1-10, gdzie 1 = bardzo niski, 10 = bardzo wysoki)\nY: Wyniki egzaminu (wynik procentowy)\n\n\n# Nasze dane\nlęk &lt;- c(2.5, 3.2, 4.1, 4.8, 5.6, 6.3, 7.0, 7.9)      # Wyniki lęku\nwyniki &lt;- c(80, 85, 78, 82, 77, 74, 68, 72)            # Wyniki egzaminu (%)\n\n# Stworzenie ramki danych dla łatwego przeglądu\ndane &lt;- data.frame(\n  Student = 1:8,\n  Lęk = lęk,\n  Wyniki = wyniki\n)\nprint(dane)\n\n  Student Lęk Wyniki\n1       1 2.5     80\n2       2 3.2     85\n3       3 4.1     78\n4       4 4.8     82\n5       5 5.6     77\n6       6 6.3     74\n7       7 7.0     68\n8       8 7.9     72\n\n\n\n\nWstępna Wizualizacja\nNajpierw zwizualizujmy nasze dane, aby uzyskać intuicyjne zrozumienie związku:\n\nlibrary(ggplot2)\n\n# Stworzenie wykresu punktowego\nggplot(dane, aes(x = Lęk, y = Wyniki)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  labs(\n    title = \"Lęk przed Testem vs. Wyniki Egzaminu\",\n    x = \"Wynik Lęku przed Testem\",\n    y = \"Wyniki Egzaminu (%)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(2, 8, 1)) +\n  scale_y_continuous(breaks = seq(65, 90, 5))\n\n\n\n\n\n\n\n\n\n\n\nStatystyki Opisowe\n\nObliczanie Średnich\nŚrednia to wartość przeciętna, obliczana przez zsumowanie wszystkich obserwacji i podzielenie przez liczbę obserwacji.\nŚrednia Wyników Lęku (X): \\bar{X} = \\frac{\\sum_{i=1}^{n} X_i}{n} = \\frac{2,5 + 3,2 + 4,1 + 4,8 + 5,6 + 6,3 + 7,0 + 7,9}{8}\n\\bar{X} = \\frac{41,4}{8} = 5,175\nŚrednia Wyników Egzaminu (Y): \\bar{Y} = \\frac{\\sum_{i=1}^{n} Y_i}{n} = \\frac{80 + 85 + 78 + 82 + 77 + 74 + 68 + 72}{8}\n\\bar{Y} = \\frac{616}{8} = 77\n\n# Weryfikacja naszych obliczeń\nśrednia_x &lt;- mean(lęk)\nśrednia_y &lt;- mean(wyniki)\ncat(\"Średni Lęk:\", średnia_x, \"\\n\")\n\nŚredni Lęk: 5.175 \n\ncat(\"Średnie Wyniki:\", średnia_y, \"\\n\")\n\nŚrednie Wyniki: 77 \n\n\n\n\nObliczanie Wariancji i Odchylenia Standardowego\nWariancja mierzy, jak bardzo rozproszone są dane od średniej. Używamy wzoru na wariancję próbkową (dzieląc przez n-1).\nWariancja X:\nNajpierw obliczamy odchylenia od średniej (X_i - \\bar{X}):\n\n\n\nStudent\nX_i\nX_i - \\bar{X}\n(X_i - \\bar{X})^2\n\n\n\n\n1\n2,5\n2,5 - 5,175 = -2,675\n(-2,675)^2 = 7,1556\n\n\n2\n3,2\n3,2 - 5,175 = -1,975\n(-1,975)^2 = 3,9006\n\n\n3\n4,1\n4,1 - 5,175 = -1,075\n(-1,075)^2 = 1,1556\n\n\n4\n4,8\n4,8 - 5,175 = -0,375\n(-0,375)^2 = 0,1406\n\n\n5\n5,6\n5,6 - 5,175 = 0,425\n(0,425)^2 = 0,1806\n\n\n6\n6,3\n6,3 - 5,175 = 1,125\n(1,125)^2 = 1,2656\n\n\n7\n7,0\n7,0 - 5,175 = 1,825\n(1,825)^2 = 3,3306\n\n\n8\n7,9\n7,9 - 5,175 = 2,725\n(2,725)^2 = 7,4256\n\n\nSuma\n\n\n24,555\n\n\n\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1} = \\frac{24,555}{7} = 3,5079\ns_X = \\sqrt{3,5079} = 1,8730\nWariancja Y:\n\n\n\nStudent\nY_i\nY_i - \\bar{Y}\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n80\n80 - 77 = 3\n(3)^2 = 9\n\n\n2\n85\n85 - 77 = 8\n(8)^2 = 64\n\n\n3\n78\n78 - 77 = 1\n(1)^2 = 1\n\n\n4\n82\n82 - 77 = 5\n(5)^2 = 25\n\n\n5\n77\n77 - 77 = 0\n(0)^2 = 0\n\n\n6\n74\n74 - 77 = -3\n(-3)^2 = 9\n\n\n7\n68\n68 - 77 = -9\n(-9)^2 = 81\n\n\n8\n72\n72 - 77 = -5\n(-5)^2 = 25\n\n\nSuma\n\n\n214\n\n\n\ns^2_Y = \\frac{\\sum(Y_i - \\bar{Y})^2}{n-1} = \\frac{214}{7} = 30,5714\ns_Y = \\sqrt{30,5714} = 5,5291\n\n# Weryfikacja wariancji i odchylenia standardowego\ncat(\"Wariancja Lęku:\", var(lęk), \"\\n\")\n\nWariancja Lęku: 3.507857 \n\ncat(\"Odch. Stand. Lęku:\", sd(lęk), \"\\n\")\n\nOdch. Stand. Lęku: 1.872927 \n\ncat(\"Wariancja Wyników:\", var(wyniki), \"\\n\")\n\nWariancja Wyników: 30.57143 \n\ncat(\"Odch. Stand. Wyników:\", sd(wyniki), \"\\n\")\n\nOdch. Stand. Wyników: 5.529144 \n\n\n\n\n\nKowariancja i Korelacja\n\nObliczanie Kowariancji\nKowariancja mierzy, jak dwie zmienne zmieniają się razem. Ujemna kowariancja wskazuje, że gdy jedna zmienna wzrasta, druga ma tendencję do spadku.\ns_{XY} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nObliczmy iloczyny dla każdej obserwacji:\n\n\n\n\n\n\n\n\n\nStudent\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n\n\n\n\n1\n-2,675\n3\n(-2,675) \\times (3) = -8,025\n\n\n2\n-1,975\n8\n(-1,975) \\times (8) = -15,800\n\n\n3\n-1,075\n1\n(-1,075) \\times (1) = -1,075\n\n\n4\n-0,375\n5\n(-0,375) \\times (5) = -1,875\n\n\n5\n0,425\n0\n(0,425) \\times (0) = 0\n\n\n6\n1,125\n-3\n(1,125) \\times (-3) = -3,375\n\n\n7\n1,825\n-9\n(1,825) \\times (-9) = -16,425\n\n\n8\n2,725\n-5\n(2,725) \\times (-5) = -13,625\n\n\nSuma\n\n\n-60,2\n\n\n\ns_{XY} = \\frac{-60,2}{7} = -8,6\n\n\nObliczanie Współczynnika Korelacji Pearsona\nWspółczynnik korelacji standaryzuje kowariancję do skali od -1 do +1:\nr = \\frac{s_{XY}}{s_X \\cdot s_Y} = \\frac{-8,6}{1,8730 \\times 5,5291} = \\frac{-8,6}{10,3560} = -0,831\nTo daje nam korelację -0,831, wskazującą na silny ujemny związek między lękiem a wynikami.\n\n# Weryfikacja korelacji\nrzeczywista_kor &lt;- cor(lęk, wyniki)\ncat(\"Współczynnik korelacji Pearsona:\", rzeczywista_kor, \"\\n\")\n\nWspółczynnik korelacji Pearsona: -0.8304618 \n\n\n\n\n\nProsta Regresja MNK\n\nProblem Modelowania Związków\nKiedy obserwujemy związek między dwiema zmiennymi, chcemy znaleźć model matematyczny, który:\n\nOpisuje związek\nPozwala nam dokonywać prognoz\nKwantyfikuje siłę związku\n\nNajprostszym modelem jest linia prosta: Y = \\beta_0 + \\beta_1 X + \\epsilon\nGdzie:\n\nY to zmienna wynikowa (wyniki)\nX to zmienna predykcyjna (lęk)\n\\beta_0 to punkt przecięcia (wyniki gdy lęk = 0)\n\\beta_1 to nachylenie (zmiana wyników na jednostkę zmiany lęku)\n\\epsilon to składnik błędu (niewyjaśniona zmienność)\n\n\n\nIdea Sumy Kwadratów Błędów (SSE)\nWyobraź sobie próbę narysowania linii przez nasze punkty danych. Jest nieskończenie wiele linii, które moglibyśmy narysować! Niektóre przeszłyby przez środek punktów, inne mogłyby być za wysokie lub za niskie, za strome lub za płaskie. Potrzebujemy systematycznego sposobu określenia, która linia jest “najlepsza”.\n\nCzym są Błędy (Reszty)?\nDla każdej linii, którą narysujemy, każdy punkt danych będzie miał błąd lub resztę - pionową odległość od punktu do linii. To reprezentuje, jak “błędna” jest nasza prognoza dla tego punktu.\n\nBłąd dodatni: Rzeczywista wartość jest powyżej przewidywanej wartości (niedoszacowaliśmy)\nBłąd ujemny: Rzeczywista wartość jest poniżej przewidywanej wartości (przeszacowaliśmy)\n\n\n\nDlaczego Podnosimy Błędy do Kwadratu?\nProste dodawanie błędów nie zadziała, ponieważ błędy dodatnie i ujemne się znoszą. Moglibyśmy użyć wartości bezwzględnych, ale podnoszenie do kwadratu ma kilka zalet:\n\nWygoda matematyczna: Funkcje kwadratowe są różniczkowalne, co ułatwia znalezienie minimum za pomocą rachunku różniczkowego\nPenalizuje większe błędy bardziej: Kilka dużych błędów jest gorsze niż wiele małych błędów\nTworzy gładką, miskowatą funkcję: To gwarantuje unikalne minimum\n\n\n\nWzór SSE\nSuma Kwadratów Błędów to: SSE = \\sum_{i=1}^{n}(Y_i - \\hat{Y_i})^2 = \\sum_{i=1}^{n}(Y_i - (\\beta_0 + \\beta_1 X_i))^2\n\n\nZnajdowanie Najlepszej Linii\n“Najlepsza” linia to ta, która minimalizuje SSE. Używając rachunku różniczkowego (biorąc pochodne względem \\beta_0 i \\beta_1 i przyrównując je do zera), otrzymujemy wzory MNK.\n\n\n\nEstymatory MNK\nMetoda Najmniejszych Kwadratów (MNK) znajduje wartości \\beta_0 i \\beta_1, które minimalizują SSE:\nEstymator nachylenia: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\nEstymator punktu przecięcia: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\n\nObliczanie Parametrów MNK\nUżywając naszych obliczonych wartości:\n\ns_{XY} = -8,6\ns^2_X = 3,5079\n\\bar{X} = 5,175\n\\bar{Y} = 77\n\nKrok 1: Oblicz nachylenie \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-8,6}{3,5079} = -2,451\nKrok 2: Oblicz punkt przecięcia \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} = 77 - (-2,451 \\times 5,175) = 77 + 12,684 = 89,684\n\n# Weryfikacja z R\nmodel &lt;- lm(wyniki ~ lęk)\ncoef(model)\n\n(Intercept)         lęk \n  89.687233   -2.451639 \n\n\nNasze równanie regresji to: \\hat{Y} = 89,684 - 2,451X\nTo oznacza:\n\nGdy lęk = 0, przewidywane wyniki = 89,68%\nZa każdy wzrost lęku o 1 punkt, wyniki spadają o 2,45%\n\n\n\n\nWykres Modelu\n\nlibrary(ggplot2)\n\n# Stworzenie kompleksowego wykresu\nggplot(data.frame(lęk, wyniki), aes(x = lęk, y = wyniki)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1) +\n  geom_text(aes(label = paste0(\"(\", round(lęk, 1), \", \", wyniki, \")\")),\n            vjust = -1, size = 3) +\n  annotate(\"text\", x = 3, y = 70, \n           label = paste0(\"ŷ = \", round(coef(model)[1], 2), \" - \", \n                         abs(round(coef(model)[2], 2)), \"x\"),\n           size = 5, color = \"red\") +\n  labs(\n    title = \"Linia Regresji: Wyniki vs. Lęk\",\n    subtitle = \"Wyższy lęk jest związany z niższymi wynikami egzaminu\",\n    x = \"Wynik Lęku przed Testem\",\n    y = \"Wyniki Egzaminu (%)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(2, 8, 1)) +\n  scale_y_continuous(breaks = seq(65, 90, 5))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nOcena Modelu\n\nDekompozycja Wariancji\nCałkowita zmienność w Y może być rozłożona na dwie części:\nSST = SSE + SSR\nGdzie:\n\nSST (Sum of Squares Total): Całkowita zmienność w Y\nSSE (Sum of Squared Errors): Niewyjaśniona zmienność\n\nSSR (Sum of Squares Regression): Wyjaśniona zmienność\n\n\n\n\n\n\n\nTip\n\n\n\nWyobraź sobie, że chcesz zrozumieć, dlaczego pensje w firmie się różnią. Jedni zarabiają 40 000, inni 80 000, a jeszcze inni 120 000. Mamy więc zmienność wynagrodzeń — nie są takie same.\n\nCałkowita zmienność (SST)\nTo pytanie: „Jak bardzo wszystkie pensje są rozproszone wokół średniej pensji?” Jeśli średnia to 70 000, to SST mierzy, o ile każda pensja różni się od 70 000, podnosi te różnice do kwadratu (żeby były dodatnie) i sumuje. To całkowita ilość zmienności, którą próbujemy wyjaśnić.\n\n\nZmienność wyjaśniona (SSR)\nZałóżmy, że budujemy model przewidujący pensję na podstawie lat doświadczenia. Model może mówić:\n\n2 lata doświadczenia → przewiduje 50 000\n5 lat doświadczenia → przewiduje 70 000\n10 lat doświadczenia → przewiduje 100 000\n\nSSR mierzy, jak bardzo te przewidywania różnią się od średniej. To ta część zmienności, którą model „wyjaśnia” relacją z doświadczeniem. Innymi słowy: „tę część różnic w pensjach da się przypisać różnym poziomom doświadczenia”.\n\n\nZmienność niewyjaśniona (SSE)\nTo to, co zostaje — część, której model nie tłumaczy. Może być tak, że dwie osoby mają po 5 lat doświadczenia, ale jedna zarabia 65 000, a druga 75 000. Model obu przewidział 70 000. Te różnice względem przewidywań (błędy) reprezentują zmienność wynikającą z innych czynników — np. edukacja, wyniki pracy, umiejętności negocjacyjne albo po prostu losowość.\n\n\n10.21 Kluczowa intuicja\nPiękne jest to, że te trzy wielkości zawsze spełniają zależność: Całkowita zmienność = Zmienność wyjaśniona + Zmienność niewyjaśniona czyli SST = SSR + SSE.\nMożesz myśleć o tym jak o „wykresie kołowym powodów, dlaczego pensje się różnią”:\n\nJeden kawałek to „różnice wyjaśnione doświadczeniem” (SSR),\nDrugi kawałek to „różnice z innych powodów” (SSE),\nRazem dają całość (SST).\n\n\n\n10.22 Dlaczego to ważne\nTo rozbicie pozwala policzyć współczynnik determinacji R^2:\n\nR^2 = \\frac{\\mathrm{SSR}}{\\mathrm{SST}} = \\frac{\\text{Zmienność wyjaśniona}}{\\text{Całkowita zmienność}}.\n\nJeśli R^2 = 0{,}70, to model wyjaśnia 70% tego, dlaczego wartości Y (tu: pensje) różnią się między sobą. Pozostałe 30% to czynniki nieuwzględnione lub szum losowy.\nPomyśl o tym jak o rozwiązywaniu zagadki: SST to cała zagadka do rozwiązania, SSR to część już rozwiązana, a SSE to to, co wciąż pozostaje do wyjaśnienia!\n\n\n\nObliczmy każdą:\nKrok 1: Oblicz przewidywane wartości\nUżywając \\hat{Y} = 89,684 - 2,451X:\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i = 89,684 - 2,451X_i\n\n\n\n\n1\n2,5\n80\n89,684 - 2,451(2,5) = 83,556\n\n\n2\n3,2\n85\n89,684 - 2,451(3,2) = 81,841\n\n\n3\n4,1\n78\n89,684 - 2,451(4,1) = 79,635\n\n\n4\n4,8\n82\n89,684 - 2,451(4,8) = 77,919\n\n\n5\n5,6\n77\n89,684 - 2,451(5,6) = 75,968\n\n\n6\n6,3\n74\n89,684 - 2,451(6,3) = 74,253\n\n\n7\n7,0\n68\n89,684 - 2,451(7,0) = 72,527\n\n\n8\n7,9\n72\n89,684 - 2,451(7,9) = 70,321\n\n\n\nKrok 2: Oblicz sumy kwadratów\nSST (Całkowita zmienność): SST = \\sum(Y_i - \\bar{Y})^2\nZ naszych wcześniejszych obliczeń wariancji: SST = (n-1) \\times s^2_Y = 7 \\times 30,5714 = 214\nSSE (Niewyjaśniona zmienność): SSE = \\sum(Y_i - \\hat{Y}_i)^2\n\n\n\n\n\n\n\n\n\n\ni\nY_i\n\\hat{Y}_i\nY_i - \\hat{Y}_i\n(Y_i - \\hat{Y}_i)^2\n\n\n\n\n1\n80\n83,556\n-3,556\n12,645\n\n\n2\n85\n81,841\n3,159\n9,979\n\n\n3\n78\n79,635\n-1,635\n2,673\n\n\n4\n82\n77,919\n4,081\n16,655\n\n\n5\n77\n75,968\n1,032\n1,065\n\n\n6\n74\n74,253\n-0,253\n0,064\n\n\n7\n68\n72,527\n-4,527\n20,494\n\n\n8\n72\n70,321\n1,679\n2,819\n\n\nSuma\n\n\n\n66,394\n\n\n\nSSR (Wyjaśniona zmienność): SSR = SST - SSE = 214 - 66,394 = 147,606\n\n# Weryfikacja obliczeń\ntabela_anova &lt;- anova(model)\ncat(\"SSR (Regresja):\", tabela_anova$`Sum Sq`[1], \"\\n\")\n\nSSR (Regresja): 147.5887 \n\ncat(\"SSE (Reszty):\", tabela_anova$`Sum Sq`[2], \"\\n\")\n\nSSE (Reszty): 66.41132 \n\ncat(\"SST (Całkowita):\", sum(tabela_anova$`Sum Sq`), \"\\n\")\n\nSST (Całkowita): 214 \n\n\n\n\nR-kwadrat (Współczynnik Determinacji)\nR-kwadrat mówi nam, jaka część całkowitej zmienności w Y jest wyjaśniona przez nasz model:\nR^2 = \\frac{SSR}{SST} = \\frac{147,606}{214} = 0,690\nTo oznacza, że nasz model wyjaśnia 69,0% zmienności w wynikach egzaminu.\nAlternatywny wzór używający korelacji: R^2 = r^2 = (-0,831)^2 = 0,691\n\n# Weryfikacja R-kwadrat\ncat(\"R-kwadrat:\", summary(model)$r.squared, \"\\n\")\n\nR-kwadrat: 0.6896667 \n\ncat(\"Korelacja do kwadratu:\", cor(lęk, wyniki)^2, \"\\n\")\n\nKorelacja do kwadratu: 0.6896667 \n\n\n\n\n\nWielkość Efektu\nWspółczynnik nachylenia \\hat{\\beta_1} = -2,451 to nasza wielkość efektu. Mówi nam:\n\nWielkość: Każdy wzrost lęku o 1 punkt jest związany z 2,45% spadkiem wyników\nZnaczenie praktyczne: Student przechodzący od niskiego lęku (3) do wysokiego lęku (7) doświadczyłby oczekiwanego spadku wyników o 2,451 \\times 4 = 9,80\\%\n\nStandaryzowana wielkość efektu (współczynnik korelacji): Korelacja r = -0,831 wskazuje na silny ujemny związek.\nWytyczne Cohena dla interpretacji korelacji:\n\nMały efekt: |r| = 0,10\nŚredni efekt: |r| = 0,30\nDuży efekt: |r| = 0,50\n\nNasze |r| = 0,831 reprezentuje dużą wielkość efektu.\n\n\nPrzedziały Ufności i Istotność Statystyczna\n\nBłąd Standardowy Regresji\nNajpierw potrzebujemy błędu standardowego reszt:\ns_e = \\sqrt{\\frac{SSE}{n-2}} = \\sqrt{\\frac{66,394}{6}} = \\sqrt{11,066} = 3,327\n\n\nBłąd Standardowy Nachylenia\nBłąd standardowy \\hat{\\beta_1} to:\nBE(\\hat{\\beta_1}) = \\frac{s_e}{\\sqrt{\\sum(X_i - \\bar{X})^2}} = \\frac{3,327}{\\sqrt{24,555}} = \\frac{3,327}{4,955} = 0,671\n\n\n95% Przedział Ufności dla Nachylenia\nMówiąc prościej: Przedział ufności daje nam zakres prawdopodobnych wartości dla naszego prawdziwego nachylenia. Gdybyśmy powtórzyli to badanie wiele razy, 95% obliczonych przez nas przedziałów zawierałoby prawdziwą wartość nachylenia.\nWzór używa wartości krytycznej (około 2,45 dla 6 stopni swobody):\nPU = \\hat{\\beta_1} \\pm (wartość\\_krytyczna \\times BE(\\hat{\\beta_1})) PU = -2,451 \\pm (2,45 \\times 0,671) PU = -2,451 \\pm 1,644 PU = [-4,095, -0,807]\nInterpretacja: Jesteśmy w 95% pewni, że prawdziwa zmiana wyników na jednostkę zmiany lęku mieści się między -4,10% a -0,81%.\n\n\nIstotność Statystyczna\nAby sprawdzić, czy związek jest statystycznie istotny (tj. nie jest przypadkowy), obliczamy statystykę t:\nt = \\frac{\\hat{\\beta_1}}{BE(\\hat{\\beta_1})} = \\frac{-2,451}{0,671} = -3,653\nMówiąc prościej: Ta wartość t mówi nam, o ile błędów standardowych nasze nachylenie odbiega od zera. Wartość bezwzględna 3,65 jest dość duża (zazwyczaj wartości przekraczające ±2,45 są uważane za istotne dla naszej wielkości próby), dostarczając silnego dowodu na rzeczywisty ujemny związek między lękiem a wynikami.\n\n# Weryfikacja obliczeń z R\nsummary(model)\n\n\nCall:\nlm(formula = wyniki ~ lęk)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.526 -2.116  0.400  2.050  4.081 \n\nCoefficients:\n            Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)  89.6872     3.6682  24.450 0.000000308 ***\nlęk          -2.4516     0.6714  -3.652      0.0107 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.327 on 6 degrees of freedom\nMultiple R-squared:  0.6897,    Adjusted R-squared:  0.6379 \nF-statistic: 13.33 on 1 and 6 DF,  p-value: 0.01069\n\nconfint(model)\n\n                2.5 %     97.5 %\n(Intercept) 80.711582 98.6628838\nlęk         -4.094474 -0.8088048\n\n\n\n\n\nPodsumowanie i Interpretacja\n\nCzego się Nauczyliśmy\n\nIstnieje ujemny związek między lękiem przed testem a wynikami egzaminu (r = -0,831)\nZwiązek jest umiarkowanie silny - lęk wyjaśnia 69,0% zmienności w wynikach\nEfekt jest znaczący - każdy wzrost lęku o 1 punkt jest związany z około 2,5% spadkiem wyników\nZwiązek jest statystycznie istotny - bardzo mało prawdopodobne, żeby był przypadkowy\n\n\n\nImplikacje Praktyczne\nNasza analiza sugeruje, że wysoki lęk przed testem pogarsza wyniki, prawdopodobnie poprzez:\n\nZakłócenia poznawcze (niepokojące myśli konkurują o pamięć roboczą)\nObjawy fizyczne (pocenie się, szybkie bicie serca), które odwracają uwagę od zadania\nNegatywny monolog wewnętrzny zmniejszający pewność siebie i motywację\nZachowania podczas testu (pośpiech, podważanie odpowiedzi)\n\n\n\nRekomendacje na Podstawie Ustaleń\nBiorąc pod uwagę silny ujemny związek, interwencje mogłyby obejmować:\n\nNauczanie technik zarządzania lękiem (głębokie oddychanie, progresywne rozluźnianie mięśni)\nRestrukturyzację poznawczą w celu radzenia sobie z katastroficznym myśleniem\nTrening umiejętności nauki w celu zwiększenia pewności siebie w przygotowaniu\nTesty próbne w celu zmniejszenia strachu przed nieznanym\n\n\n\nOgraniczenia Naszej Analizy\n\nMała wielkość próby (n = 8) ogranicza możliwość uogólnienia\nZałożenie liniowości - związek może być krzywoliniowy (może istnieć optymalny lęk)\nInne zmienne nieuwzględnione (czas przygotowania, zdolności, sen, itp.)\nPrzyczynowość vs. korelacja - nie możemy udowodnić, że lęk powoduje słabe wyniki\nSamoopisowy lęk - subiektywne miary mogą nie odzwierciedlać pobudzenia fizjologicznego\n\n\n\n\nDodatek: Kompletny Kod R\n\n# Kompletny kod analizy\nlęk &lt;- c(2.5, 3.2, 4.1, 4.8, 5.6, 6.3, 7.0, 7.9)\nwyniki &lt;- c(80, 85, 78, 82, 77, 74, 68, 72)\n\n# Statystyki opisowe\nmean(lęk); sd(lęk)\n\n[1] 5.175\n\n\n[1] 1.872927\n\nmean(wyniki); sd(wyniki)\n\n[1] 77\n\n\n[1] 5.529144\n\n# Korelacja\ncor(lęk, wyniki)\n\n[1] -0.8304618\n\n# Regresja\nmodel &lt;- lm(wyniki ~ lęk)\nsummary(model)\n\n\nCall:\nlm(formula = wyniki ~ lęk)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.526 -2.116  0.400  2.050  4.081 \n\nCoefficients:\n            Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)  89.6872     3.6682  24.450 0.000000308 ***\nlęk          -2.4516     0.6714  -3.652      0.0107 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.327 on 6 degrees of freedom\nMultiple R-squared:  0.6897,    Adjusted R-squared:  0.6379 \nF-statistic: 13.33 on 1 and 6 DF,  p-value: 0.01069\n\nconfint(model)\n\n                2.5 %     97.5 %\n(Intercept) 80.711582 98.6628838\nlęk         -4.094474 -0.8088048\n\n# Wizualizacja\nlibrary(ggplot2)\nggplot(data.frame(lęk, wyniki), aes(x = lęk, y = wyniki)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Diagnostyka\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Tabela ANOVA\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: wyniki\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nlęk        1 147.589 147.589  13.334 0.01069 *\nResiduals  6  66.411  11.069                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kluczowa-intuicja",
    "href": "correg_pl.html#kluczowa-intuicja",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.21 Kluczowa intuicja",
    "text": "10.21 Kluczowa intuicja\nPiękne jest to, że te trzy wielkości zawsze spełniają zależność: Całkowita zmienność = Zmienność wyjaśniona + Zmienność niewyjaśniona czyli SST = SSR + SSE.\nMożesz myśleć o tym jak o „wykresie kołowym powodów, dlaczego pensje się różnią”:\n\nJeden kawałek to „różnice wyjaśnione doświadczeniem” (SSR),\nDrugi kawałek to „różnice z innych powodów” (SSE),\nRazem dają całość (SST).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#dlaczego-to-ważne",
    "href": "correg_pl.html#dlaczego-to-ważne",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.22 Dlaczego to ważne",
    "text": "10.22 Dlaczego to ważne\nTo rozbicie pozwala policzyć współczynnik determinacji R^2:\n\nR^2 = \\frac{\\mathrm{SSR}}{\\mathrm{SST}} = \\frac{\\text{Zmienność wyjaśniona}}{\\text{Całkowita zmienność}}.\n\nJeśli R^2 = 0{,}70, to model wyjaśnia 70% tego, dlaczego wartości Y (tu: pensje) różnią się między sobą. Pozostałe 30% to czynniki nieuwzględnione lub szum losowy.\nPomyśl o tym jak o rozwiązywaniu zagadki: SST to cała zagadka do rozwiązania, SSR to część już rozwiązana, a SSE to to, co wciąż pozostaje do wyjaśnienia!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przykład-5.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis-en",
    "href": "correg_pl.html#przykład-5.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis-en",
    "title": "10  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.23 Przykład 5. District Magnitude and Electoral Disproportionality: A Comparative Analysis (*) [EN]",
    "text": "10.23 Przykład 5. District Magnitude and Electoral Disproportionality: A Comparative Analysis (*) [EN]\n\nData Generating Process\nLet’s set up a DGP where:\n\\begin{aligned}\n& Y_{\\text{Gallagher}} = 12 - 0.8X_{\\text{DM}} + \\varepsilon \\\\\n& \\varepsilon \\sim \\mathcal{N}(0, 1) \\\\\n& X_{\\text{DM}} \\in \\{3, 5, 7, 10, 12, 15\\}\n\\end{aligned}\n\n# DGP\nmagnitude &lt;- c(3, 5, 7, 10, 12, 15)\nepsilon &lt;- rnorm(6, mean = 0, sd = 1)\ngallagher &lt;- 12 - 0.8 * magnitude + epsilon\n\n# Round (sampled from the DGP) Gallagher indices to one decimal place\ngallagher &lt;- round(c(9.0, 7.8, 9.2, 4.1, 2.5, 1.7), 1)\n\n# Show data\ndata.frame(\n  District_Magnitude = magnitude,\n  Gallagher_Index = gallagher\n)\n\n  District_Magnitude Gallagher_Index\n1                  3             9.0\n2                  5             7.8\n3                  7             9.2\n4                 10             4.1\n5                 12             2.5\n6                 15             1.7\n\n# Initial model check\nmodel &lt;- lm(gallagher ~ magnitude)\nsummary(model)\n\n\nCall:\nlm(formula = gallagher ~ magnitude)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.6516 -0.4628  2.3260 -0.6908 -0.9020  0.3813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7349     1.3034   9.003 0.000843 ***\nmagnitude    -0.6944     0.1359  -5.110 0.006934 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.368 on 4 degrees of freedom\nMultiple R-squared:  0.8672,    Adjusted R-squared:  0.834 \nF-statistic: 26.11 on 1 and 4 DF,  p-value: 0.006934\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{3 + 5 + 7 + 10 + 12 + 15}{6} = \\frac{52}{6} = 8.6667\n\\bar{Y} = \\frac{9.0 + 7.8 + 9.2 + 4.1 + 2.5 + 1.7}{6} = \\frac{34.3}{6} = 5.7167\n\n# Verification\nmean(magnitude)\n\n[1] 8.666667\n\nmean(gallagher)\n\n[1] 5.716667\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(3 - 8.6667) = -5.6667\n(5 - 8.6667) = -3.6667\n(7 - 8.6667) = -1.6667\n(10 - 8.6667) = 1.3333\n(12 - 8.6667) = 3.3333\n(15 - 8.6667) = 6.3333\n\nSquared deviations:\n\n32.1115 + 13.4445 + 2.7779 + 1.7777 + 11.1109 + 40.1107 = 101.3332\n\ns^2_X = \\frac{101.3332}{5} = 20.2666\nFor Y: Deviations: 3.2833, 2.0833, 3.4833, -1.6167, -3.2167, -4.0167\ns^2_Y = \\frac{56.3483}{5} = 11.2697\n\n# Verification\nvar(magnitude)\n\n[1] 20.26667\n\nvar(gallagher)\n\n[1] 11.26967\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n\n(-5.6667 × 3.2833) = -18.6057\n(-3.6667 × 2.0833) = -7.6387\n(-1.6667 × 3.4833) = -5.8056\n(1.3333 × -1.6167) = -2.1556\n(3.3333 × -3.2167) = -10.7223\n(6.3333 × -4.0167) = -25.4391\n\nSum = -70.3670\ns_{XY} = \\frac{-70.3670}{5} = -14.0734\n\n# Verification\ncov(magnitude, gallagher)\n\n[1] -14.07333\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-14.0734}{\\sqrt{20.2666}\\sqrt{11.2697}} = -0.9279\n\n# Verification\ncor(magnitude, gallagher)\n\n[1] -0.9312157\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-14.0734}{20.2666} = -0.6944\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-0.6944 × 8.6667 = -6.0181\n\\hat{\\beta_0} = 5.7167 - (-6.0181) = 11.7348\n\n\n# Verification\ncoef(model)\n\n(Intercept)   magnitude \n 11.7348684  -0.6944079 \n\n\n\n\nR-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = 11.7348 - 0.6944X\n\n# Predicted values\ny_hat &lt;- 11.7348 - 0.6944 * magnitude\ndata.frame(\n  Magnitude = magnitude,\n  Gallagher = gallagher,\n  Predicted = y_hat,\n  row.names = 1:6\n)\n\n  Magnitude Gallagher Predicted\n1         3       9.0    9.6516\n2         5       7.8    8.2628\n3         7       9.2    6.8740\n4        10       4.1    4.7908\n5        12       2.5    3.4020\n6        15       1.7    1.3188\n\n\nStep 2: Sum of Squares SST = \\sum(Y_i - \\bar{Y})^2 = 56.3483 SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 48.5271 SSE = \\sum(Y_i - \\hat{Y}_i)^2 = 7.8212\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{48.5271}{56.3483} = 0.8612\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.8671626\n\n\n\n\nVisualization - True vs. Estimated Parameters\n\nTrue DGP: Y = 12 - 0.8X + ε\nEstimated Model: Y = 11.7348 - 0.6944X\n\n\nlibrary(ggplot2)\n\n# Create data frame with original data\ndf &lt;- data.frame(\n  magnitude = magnitude,\n  gallagher = gallagher\n)\n\n# Create sequence for smooth lines\nx_seq &lt;- seq(min(magnitude), max(magnitude), length.out = 100)\n\n# Calculate predicted values for both lines\ntrue_dgp &lt;- 12 - 0.8 * x_seq\nestimated &lt;- 11.7348 - 0.6944 * x_seq\n\n# Combine into a data frame for plotting\nlines_df &lt;- data.frame(\n  magnitude = rep(x_seq, 2),\n  value = c(true_dgp, estimated),\n  Model = rep(c(\"True DGP\", \"Estimated\"), each = length(x_seq))\n)\n\n# Create plot\nggplot() +\n  geom_line(data = lines_df, \n            aes(x = magnitude, y = value, color = Model, linetype = Model),\n            size = 1) +\n  geom_point(data = df, \n             aes(x = magnitude, y = gallagher),\n             color = \"black\", \n             size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  labs(\n    title = \"True DGP vs. Estimated Regression Line\",\n    subtitle = \"Black points show observed data with random noise\",\n    x = \"District Magnitude\",\n    y = \"Gallagher Index\",\n    caption = \"True DGP: Y = 12 - 0.8X + ε\\nEstimated: Y = 11.73 - 0.69X\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.caption = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\n\nObservations about Model Fit\n\nSlope Comparison\n\nTrue slope: -0.8\nEstimated slope: -0.69\nThe estimated slope is reasonably close to the true parameter\n\nIntercept Comparison\n\nTrue intercept: 12\nEstimated intercept: 11.73\nThe estimated intercept very closely approximates the true value\n\nVisual Patterns\n\nThe lines are nearly parallel, showing good slope recovery\nPoints scatter around both lines due to the random error term (ε)\nThe small sample size (n=6) leads to some imprecision in estimation\nThe estimated line (blue) provides a good approximation of the true DGP (red dashed)\n\nImpact of Random Error\n\nThe scatter of points around the true DGP line reflects the N(0,1) error term\nThis noise leads to the slight differences in estimated parameters\nWith a larger sample, we would expect even closer convergence to true parameters\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.93) between district magnitude and electoral disproportionality\nFor each unit increase in district magnitude, the Gallagher index decreases by approximately 0.69 points\nThe model explains 86.12% of the variance in disproportionality\nThe relationship appears strongly linear with moderate scatter\nThe intercept (11.73) represents the expected disproportionality in a hypothetical single-member district system\n\n\n\nStudy Context\n\nData represents simulated observations from a DGP with moderate noise\nSample shows how increasing district magnitude tends to reduce disproportionality\nRandom component reflects other institutional and political factors affecting disproportionality\n\n\n\nLimitations\n\nSmall sample size (n=6)\nSimulated rather than real-world data\nAssumes linear relationship\nDoes not account for other institutional features",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "inference_en.html",
    "href": "inference_en.html",
    "title": "13  Introduction to Statistical Inference: The Logic of Statistical Hypothesis Testing",
    "section": "",
    "text": "(…)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Statistical Inference: The Logic of Statistical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "inference_pl.html",
    "href": "inference_pl.html",
    "title": "14  Wprowadzenie do Wnioskowania Statystycznego: Logika Testowania Hipotez Statystycznych",
    "section": "",
    "text": "(…)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wprowadzenie do Wnioskowania Statystycznego: Logika Testowania Hipotez Statystycznych</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\n\n\n\nBlair, G., Coppock, A., & Humphreys, M. (2023). Research design in the social sciences: declaration, diagnosis, and redesign. Princeton University Press. https://book.declaredesign.org/\nBryman, A., 2016. Social research methods. Oxford University Press.\nBueno de Mesquita, Ethan and Anthony Fowler. 2021. Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis. Princeton University Press.\nCausality for Machine Learning. https://ff13.fastforwardlabs.com/\nCetinkaya-Rundel, M., Diez, D.M. and Barr, C.D., 2019 (4th ed.). OpenIntro Statistics: an Open-source Textbook: https://www.openintro.org/book/os/\nClaude [Large language model], 2024. https://www.anthropic.com\nConcepts and Computation: An Introduction to Political Methodology. https://pos3713.github.io/notes/\nHannay, K. (2019). Introduction to statistics and data science. http://khannay.com/StatsBook/\nIsmay, C. and Kim, A.Y., 2019. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. https://moderndive.com/index.html\nNavarro, D.J. and Foxcroft, D.R. (2019). Learning statistics with Jamovi: a tutorial for psychology students and other beginners. (Version 0.70). DOI: 10.24384/hgc3-7p15\nRemler, D.K. and Van Ryzin, G.G., 2014. Research methods in practice: Strategies for description and causation. Sage Publications.\nSanchez, G., Marzban, E. (2020) All Models Are Wrong: Concepts of Statistical Learning. https://allmodelsarewrong.github.io\nSchneider, W. J. (2023). Psycheval: A psychological evaluation toolkit. https://github.com/wjschne/psycheval\nTimbers, T., Campbell, T., & Lee, M. (2022). Data science: A first introduction. Chapman and Hall/CRC. https://datasciencebook.ca/",
    "crumbs": [
      "References"
    ]
  }
]