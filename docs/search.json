[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Data Analysis: An Introduction (PL: Wprowadzenie do Analizy Danych Spo≈Çecznych)",
    "section": "",
    "text": "Preface\n\n\n\n\n\n\nImportant\n\n\n\nThis is a preliminary draft of a Quarto class notes on social data analysis. Please do not cite or reproduce its contents, as it may contain errors!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "",
    "text": "1.1 What is Statistics?\nStatistics is the science of learning from data in the presence of uncertainty. More specifically, statistics provides:\nIn political science, statistics helps us move beyond anecdotal evidence and personal impressions to make rigorous, evidence-based claims about political phenomena.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#what-is-statistics",
    "href": "chapter1.html#what-is-statistics",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "",
    "text": "Methods for collecting data systematically and without bias\nTools for describing and summarizing what we observe in our data\n\nTechniques for making inferences about populations based on samples\nFrameworks for quantifying uncertainty in our conclusions\nApproaches for modeling relationships between variables",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-statistical-mindset",
    "href": "chapter1.html#the-statistical-mindset",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.2 The Statistical Mindset",
    "text": "1.2 The Statistical Mindset\nDeveloping a statistical mindset is less about memorizing formulas and more about adopting a way of thinking about the world. At its core, statistics teaches us to be curious, cautious, and systematic when drawing conclusions from data. Five key habits of mind are especially important:\n\nEmbrace uncertainty We almost never know the ‚Äútrue‚Äù values in a population. Estimates always come with error margins. Recognizing uncertainty is not a weakness‚Äîit is an honest reflection of reality. For example, an election poll showing a candidate at 52% is never a precise truth, but rather an estimate with a margin of error.\nThink about variation Why do individuals, groups, or cases differ? Variation is the fuel of statistics: without it, there would be nothing to study. Sometimes variation is random (sampling error), sometimes systematic (gender, education, income), and often both. Understanding variation helps us detect meaningful patterns rather than noise.\nQuestion relationships Just because two things move together does not mean one causes the other. For instance, ice cream sales and drowning incidents both rise in summer‚Äîbut eating ice cream does not cause drowning. Statistical thinking demands that we test whether relationships reflect causation or mere association.\nBe skeptical A striking pattern in the data might still be a fluke. Could the result have appeared simply by chance? Tools like p-values, confidence intervals, and replication help us distinguish real effects from random coincidences.\nConsider alternatives Every explanation competes with other possible explanations. If we observe that students who study in groups score higher on exams, is it the group study that helps‚Äîor are stronger students more likely to choose groups in the first place? Thinking in terms of competing hypotheses makes our conclusions stronger.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#population-vs.-sample-the-foundation-of-inference",
    "href": "chapter1.html#population-vs.-sample-the-foundation-of-inference",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.3 Population vs.¬†Sample: The Foundation of Inference",
    "text": "1.3 Population vs.¬†Sample: The Foundation of Inference\n\nThe Fundamental Challenge\nIn political science, we‚Äôre often interested in understanding entire populations‚Äîthe complete set of units we want to study. However, studying entire populations is usually impossible, impractical, or unnecessary.\n\n\nWhat Can Be a Population?\nA population in political science can consist of various types of units:\nIndividuals\n\nPopulation: All 240 million American adults\nSample: 1,000 randomly selected adults in a survey\nResearch question: What percentage support universal healthcare?\n\nCountries\n\nPopulation: All 195 sovereign nations in the world\nSample: 50 countries from different regions and development levels\nResearch question: Does democracy correlate with economic growth?\n\nSubnational Units\n\nPopulation: All 3,143 U.S. counties\nSample: 200 randomly selected counties\nResearch question: How does unemployment affect crime rates?\n\nOrganizations\n\nPopulation: All NGOs registered with the United Nations\nSample: 100 NGOs working in different policy areas\nResearch question: What factors predict NGO effectiveness?\n\nEvents or Time Periods\n\nPopulation: All elections held in Europe since 1945\nSample: 300 elections from different countries and decades\nResearch question: How do economic conditions affect incumbent vote share?\n\nLegislative Units\n\nPopulation: All bills introduced in Congress from 2000-2020\nSample: 500 randomly selected bills\nResearch question: What predicts whether a bill becomes law?\n\n\n\nThe Sample Solution and Key Insight\nA sample is a subset of the population we actually observe and measure. The key insight of statistics is that we can learn about populations by studying samples‚Äîif we‚Äôre careful about how we choose them.\nFrom our sample, we want to make inferences about the population:\n\\text{Sample Statistic} \\rightarrow \\text{Population Parameter}\nFor example: If 52% of our sample supports Candidate A (\\hat{p} = 0.52), what can we say about support in the entire population (\\pi)?\nThe fundamental principle: random selection gives every unit in the population an equal chance of being included, preventing systematic bias.\n\n\nVisualizing Sampling\nLet‚Äôs see how different sample sizes affect our estimates:\n\n# Simulate sampling from a population\npopulation_size &lt;- 1000000\ntrue_proportion &lt;- 0.60  # True population parameter (œÄ)\n\n# Take different sized samples\nsample_sizes &lt;- c(100, 500, 1000, 5000)\nresults &lt;- data.frame()\n\nfor (size in sample_sizes) {\n  for (i in 1:20) {\n    sample_result &lt;- rbinom(1, size, true_proportion) / size\n    results &lt;- rbind(results, \n                     data.frame(size = size, \n                               trial = i,\n                               estimate = sample_result))\n  }\n}\n\n# Visualize\nggplot(results, aes(x = factor(size), y = estimate)) +\n  geom_point(alpha = 0.6, size = 2, color = \"steelblue\") +\n  geom_hline(yintercept = true_proportion, color = \"red\", \n             linetype = \"dashed\", size = 1) +\n  labs(title = \"How Sample Size Affects Accuracy\",\n       subtitle = \"Red line shows true population value (60%)\",\n       x = \"Sample Size\",\n       y = \"Sample Estimate\") +\n  theme_minimal() +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\n\nFigure Note: This scatter plot demonstrates how sample size affects the accuracy of estimates. Each blue dot represents one sample estimate. Notice how larger samples (right side) cluster more tightly around the true population value (red dashed line), illustrating reduced sampling variability.\n\n\nKey Takeaway: This demonstrates the law of large numbers‚Äîas sample size increases, our estimates become more reliable. With n=100, estimates vary widely (55-65%), but with n=5000, they‚Äôre much more precise (59-61%). This is why national polls typically survey 1,000+ people rather than 100.\n\n\n\nThe Representation Problem\nNot all samples are created equal. Consider these sampling methods:\n\nConvenience Sample: Surveying students in your political science class\n\nProblem: Not representative of all voters\nExample: College students skew younger and more liberal than the general population\n\nVoluntary Response Sample: Online poll on a news website\n\nProblem: Self-selection bias\nExample: People with strong opinions are more likely to participate\n\nRandom Sample: Each unit has equal probability of selection\n\nSolution: Best chance of representative sample\nExample: Randomly selected phone numbers from all area codes\n\nStratified Random Sample: Divide population into groups, sample from each\n\nAdvantage: Ensures representation of key subgroups\nExample: Sample equal numbers from each state for national survey\n\nCluster Sample: Randomly select groups, then survey everyone within\n\nAdvantage: Cost-effective for geographically dispersed populations\nExample: Randomly select 50 cities, then survey residents within those cities",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#sample-population-and-superpopulation-dgp",
    "href": "chapter1.html#sample-population-and-superpopulation-dgp",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.4 Sample, Population, and Superpopulation (DGP)",
    "text": "1.4 Sample, Population, and Superpopulation (DGP)\n\nThe Basic Framework: Sample and Population\n\nPopulation\nThe population is the complete set of all units we‚Äôre interested in studying.\nExamples: - All registered voters in Canada - Every tree in Yellowstone National Park\n- All customers who bought from Amazon in 2024 - Every student currently enrolled at your university\nKey characteristic: The population is finite and fixed. It has true parameters (like mean Œº and standard deviation œÉ) that are fixed numbers, even if unknown.\n\n\nSample\nA sample is a subset of the population that we actually observe and measure.\nExamples: - 1,000 randomly selected Canadian voters (from the population of all voters) - 200 trees measured in designated plots (from all Yellowstone trees) - 10,000 customer purchases we analyze (from millions of Amazon transactions) - 300 students who respond to your survey (from all university students)\nKey characteristic: We use the sample to make inferences about the population parameters using statistics (like sample mean xÃÑ and sample standard deviation s).\n\n\nThe Classical Statistical Framework\nPOPULATION (usually unknown)\n    ‚Üì\n[Sampling Process]\n    ‚Üì\nSAMPLE (what we observe)\n    ‚Üì\n[Statistical Inference]\n    ‚Üì\nESTIMATES about Population\nThis is why we need: - P-values: To test hypotheses about the unknown population - Confidence intervals: To quantify uncertainty about population parameters - Standard errors: To measure how much our sample statistics might vary\n\n\n\n\nWhen This Framework Breaks Down\nSometimes we have data on the entire population. For example: - Census data covering every person - All stock trades on the NYSE for 2024 - Every goal scored in the Premier League last season - Administrative data on all hospital admissions\nThe paradox: If we already know the population mean Œº (because we calculated it from everyone), why would we need a confidence interval for Œº?\n\n\n\nSuperpopulation (Data Generating Process)\nThe superpopulation or Data Generating Process (DGP) is a theoretical, infinite population from which our observed finite population is considered one realization.\nInstead of:\nPopulation ‚Üí Sample\nWe now think:\nSUPERPOPULATION (infinite, theoretical)\n    ‚Üì\n[Data Generating Process]\n    ‚Üì\nOBSERVED POPULATION (what we have)\n    ‚Üì\nINSIGHTS about the DGP\n\nExamples to Build Intuition\n\nExample 1: Annual Sales Data\n\nObserved population: All 50,000 sales transactions in 2024\nSuperpopulation view: These 50,000 sales are one realization from an ongoing business process that could have generated different transactions if circumstances varied slightly\nWhy it matters: We want to understand the sales process to predict 2025, not just describe 2024\n\n\n\nExample 2: Election Results\n\nObserved population: Turnout in all 3,000 municipalities in the 2024 election\nSuperpopulation view: These results are one outcome from an electoral process involving weather, campaigns, issues, etc. that could have produced different results\nWhy it matters: We want to understand what drives turnout in general, not just what happened in this specific election\n\n\n\nExample 3: Student Grades\n\nObserved population: Final grades for all 400 students in Statistics 101 this semester\nSuperpopulation view: These 400 students and their grades are one realization from the ongoing process of statistics education at your university\nWhy it matters: You want to know if a new teaching method works, not just whether these specific 400 students did better",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#summary-table",
    "href": "chapter1.html#summary-table",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.5 Summary Table",
    "text": "1.5 Summary Table\n\n\n\n\n\n\n\n\n\nConcept\nSize\nExample\nWhat We Learn\n\n\n\n\nSample\nFinite subset\n1,000 surveyed voters\nEstimates of population parameters\n\n\nPopulation\nFinite, complete\nAll 10 million voters\nExact parameters (if we measure everyone)\n\n\nSuperpopulation\nInfinite, theoretical\nThe electoral process that generates voter behavior\nUnderstanding of underlying mechanisms and processes\n\n\n\n\nThe Key Insight\nThe superpopulation concept allows us to: - Use statistical inference even with complete data - Think about our data as one possible outcome from an underlying process\n- Make statements about mechanisms, not just descriptions - Generalize beyond the specific time and place of our data\n\nThis is why researchers often use p-values and confidence intervals even with population data - they‚Äôre not being careless, they‚Äôre implicitly treating their data as a realization from a broader data generating process they want to understand.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#key-concepts-parameters-statistics-and-estimates",
    "href": "chapter1.html#key-concepts-parameters-statistics-and-estimates",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.6 Key Concepts: Parameters, Statistics, and Estimates",
    "text": "1.6 Key Concepts: Parameters, Statistics, and Estimates\n\nParameters vs.¬†Statistics\nA fundamental distinction in statistics is between parameters and statistics:\nPopulation Parameters\n\nNumerical characteristics of the entire population\nUsually unknown and what we want to learn about\nDenoted by Greek letters: \\mu (mu) for mean, \\sigma (sigma) for standard deviation, \\pi (pi) for proportion\nExamples: The true percentage of all Americans who support universal healthcare\n\nSample Statistics\n\nNumerical characteristics calculated from sample data\nWhat we actually observe and calculate\nDenoted by Roman letters: \\bar{x} for sample mean, s for sample standard deviation, \\hat{p} for sample proportion\nExamples: The percentage of 1,000 survey respondents who support universal healthcare\n\nNotation Convention\nThroughout this text, we‚Äôll consistently use: - Population parameters: \\mu (mean), \\sigma (standard deviation), \\pi (proportion)\n- Sample statistics: \\bar{x} (mean), s (standard deviation), \\hat{p} (proportion)\nThis notation helps us always distinguish between what we observe (statistics) and what we want to know (parameters).\n\n\nThe Inference Process: From Statistics to Parameters\nThe core of statistical inference involves using sample statistics to make educated guesses about population parameters:\n\\text{Sample Statistic} \\xrightarrow{\\text{Statistical Inference}} \\text{Population Parameter}\nExample: If 52% of our sample (\\hat{p} = 0.52) supports a candidate, we use this statistic to estimate the population parameter (\\pi) representing true support among all voters.\n\n\nEstimates and Estimators\nAn estimator is the method or formula used to approximate a parameter. An estimate is the specific numerical result from applying that estimator to a particular sample.\n\nEstimator: The sample mean \\bar{x} = \\frac{\\sum x_i}{n}\nEstimate: \\bar{x} = 6.3 years of education (the actual number from our data)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-soup-analogy-understanding-statistical-inference",
    "href": "chapter1.html#the-soup-analogy-understanding-statistical-inference",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.7 The Soup Analogy: Understanding Statistical Inference",
    "text": "1.7 The Soup Analogy: Understanding Statistical Inference\nImagine you‚Äôre a chef making a large pot of soup for 1,000 people. You want to know if the soup has the right amount of salt, but you can‚Äôt taste all of it. Instead, you take a small spoonful to taste.\nThe Population: The entire pot of soup (1,000 servings)\nThe Sample: Your spoonful\nThe Parameter: The true saltiness of the entire pot (unknown)\nThe Statistic: The saltiness of your spoonful (what you can measure)\nStatistical Inference: Using the spoonful‚Äôs saltiness to draw conclusions about the entire pot\nKey Insights from the Soup Analogy:\n\nRandom sampling matters: You must stir the soup first and take your spoonful from a random location. If you always sample from the top, you might miss that the salt settled to the bottom.\nSample size affects precision: A bigger spoonful gives you a better sense of the overall saltiness than a tiny sip.\nUncertainty is inherent: Even with good sampling, your spoonful might not perfectly represent the whole pot. There‚Äôs always some uncertainty.\nSystematic bias ruins everything: If someone secretly added extra salt to just your spoonful, your inference about the whole pot would be wrong. This represents sampling bias.\nInference has limits: You can estimate the average saltiness, but your spoonful can‚Äôt tell you if some portions are saltier than others (variability within the population).\n\nThis analogy captures the essence of statistical thinking: we use small, carefully selected samples to learn about much larger populations, always acknowledging the uncertainty inherent in this process.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#a-real-world-example-what-predicts-electoral-success",
    "href": "chapter1.html#a-real-world-example-what-predicts-electoral-success",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.8 A Real-World Example: What Predicts Electoral Success?",
    "text": "1.8 A Real-World Example: What Predicts Electoral Success?\nLet‚Äôs start with a question that gets to the heart of political science: What makes politicians win elections?\nImagine you‚Äôre a campaign manager trying to understand why some incumbents win by landslides while others barely scrape by. You have data on 200 recent congressional elections, including each incumbent‚Äôs approval rating, the state of the local economy, and their victory margin.\n\n# Create realistic electoral data\nset.seed(42)  # Consistent with initial setup\nn_elections &lt;- 200\n\n# Generate correlated predictors (realistic scenario)\napproval_rating &lt;- runif(n_elections, 35, 85)\neconomic_growth &lt;- rnorm(n_elections, 2.5, 1.5)\ncampaign_spending_100k &lt;- rnorm(n_elections, 8, 2)  # In units of $100,000 for clarity\n\n# Create victory margin with realistic relationships\nvictory_margin &lt;- -15 + \n  0.6 * approval_rating +           # Strong approval effect\n  2.5 * economic_growth +           # Economic voting\n  0.3 * campaign_spending_100k +    # Money helps (effect per $100k)\n  rnorm(n_elections, 0, 8)          # Random factors\n\n# Create dataset\nelection_data &lt;- data.frame(\n  district = 1:n_elections,\n  approval = approval_rating,\n  econ_growth = economic_growth,\n  spending_100k = campaign_spending_100k,\n  victory_margin = victory_margin,\n  won = victory_margin &gt; 0\n)\n\n# Quick visualization\np1 &lt;- ggplot(election_data, aes(x = approval, y = victory_margin)) +\n  geom_point(alpha = 0.6, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.7) +\n  labs(title = \"Approval Rating vs. Victory Margin\",\n       x = \"Approval Rating (%)\",\n       y = \"Victory Margin (percentage points)\",\n       subtitle = \"Points above the dashed line represent wins\")\n\nprint(p1)\n\n\n\n\n\n\n\n# Run the regression\nsimple_model &lt;- lm(victory_margin ~ approval, data = election_data)\nsummary(simple_model)\n\n\nCall:\nlm(formula = victory_margin ~ approval, data = election_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.9948  -6.1420   0.5653   5.9218  28.4974 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) -9.78570    2.63382  -3.715             0.000264 ***\napproval     0.64728    0.04192  15.439 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.635 on 198 degrees of freedom\nMultiple R-squared:  0.5462,    Adjusted R-squared:  0.544 \nF-statistic: 238.4 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\n\n\nFigure Note: This scatter plot shows the relationship between approval ratings (x-axis) and electoral victory margins (y-axis). Each point represents one election. The red line shows the ‚Äúline of best fit‚Äù from linear regression, with the gray band indicating uncertainty. Points above the dashed horizontal line (y=0) represent electoral victories.\n\n\nReading the Output: The ‚ÄúEstimate‚Äù for approval (approximately 0.60) means each 1-point increase in approval rating is associated with a 0.60-point increase in victory margin. The p-value (&lt;0.001) indicates this relationship is statistically significant‚Äîvery unlikely to be due to chance alone.\n\nWhat we just discovered: Each 1-point increase in approval rating is associated with about a 0.65-point increase in victory margin. With an approval rating below 15.1%, incumbents typically lose.\nHowever, approval rating represents only one factor in electoral success. A more comprehensive analysis requires examining multiple variables simultaneously:\n\n# Multiple regression model\nfull_model &lt;- lm(victory_margin ~ approval + econ_growth + spending_100k, data = election_data)\n\n# Clean presentation of results\nmodel_results &lt;- tidy(full_model) %&gt;%\n  mutate(\n    estimate = round(estimate, 4),\n    p.value = round(p.value, 3),\n    significant = ifelse(p.value &lt; 0.05, \"Yes\", \"No\"),\n    term = recode(term,\n                  \"(Intercept)\" = \"Baseline\",\n                  \"approval\" = \"Approval Rating\",\n                  \"econ_growth\" = \"Economic Growth (%)\",\n                  \"spending_100k\" = \"Campaign Spending (per $100k)\")\n  )\n\nkable(model_results, \n      col.names = c(\"Variable\", \"Effect Size\", \"Std Error\", \"t-statistic\", \"p-value\", \"Significant?\"),\n      caption = \"What Really Drives Electoral Success?\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nWhat Really Drives Electoral Success?\n\n\nVariable\nEffect Size\nStd Error\nt-statistic\np-value\nSignificant?\n\n\n\n\nBaseline\n-18.8368\n3.7738929\n-4.991343\n0.00\nYes\n\n\nApproval Rating\n0.6541\n0.0397821\n16.441115\n0.00\nYes\n\n\nEconomic Growth (%)\n1.9619\n0.4004247\n4.899426\n0.00\nYes\n\n\nCampaign Spending (per $100k)\n0.4897\n0.3054328\n1.603246\n0.11\nNo\n\n\n\n\n\n\n\nWhen we account for multiple factors simultaneously, we see that:\n\nApproval rating remains the strongest predictor (0.6 points per 1% approval)\nEconomic growth also matters significantly (2.5 points per 1% GDP growth)\nCampaign spending has a modest effect (0.3 points per $100,000 spent)\n\nThis is the power of regression analysis‚Äîit helps us disentangle complex relationships and understand what really matters in politics.\n\n\n\n\n\n\nCommon Statistical Pitfalls in Political Science\n\n\n\n\nEcological fallacy: Assuming group-level patterns apply to individuals\nSelection bias: Non-random samples that systematically exclude certain groups\n\nConfounding: Failing to account for variables that affect both X and Y\nP-hacking: Testing multiple hypotheses until finding significance\nOvergeneralization: Extending findings beyond the studied population\n\n\n\nBy the end of this course, you‚Äôll understand:\n\nHow this analysis works and what assumptions it requires\nWhen we can interpret these relationships as causal vs.¬†merely correlational\nHow to assess the reliability and practical significance of our findings\nWhat could go wrong and how to avoid common pitfalls\n\nNow let‚Äôs build the foundation to understand how we got these results and what they really mean.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-political-world-is-full-of-data",
    "href": "chapter1.html#the-political-world-is-full-of-data",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.9 The Political World is Full of Data",
    "text": "1.9 The Political World is Full of Data\nPolitical science has evolved from a primarily theoretical discipline to one that increasingly relies on empirical evidence. Whether we‚Äôre studying:\n\nElection outcomes: Why do people vote the way they do?\nPublic opinion: What shapes attitudes toward immigration or climate policy?\nInternational relations: What factors predict conflict between nations?\nPolicy effectiveness: Did a new education policy actually improve outcomes?\n\nWe need systematic ways to analyze data and draw conclusions that go beyond anecdotes and personal impressions.\nConsider this question: ‚ÄúDoes democracy lead to economic growth?‚Äù\nYour intuition might suggest yes‚Äîdemocratic countries tend to be wealthier. But is this causation or correlation? Are there exceptions? How confident can we be in our conclusions?\nStatistics provides the tools to move from hunches to evidence-based answers, helping us distinguish between what seems true and what actually is true.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#randomness-the-foundation-of-statistical-inference",
    "href": "chapter1.html#randomness-the-foundation-of-statistical-inference",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.10 Randomness: The Foundation of Statistical Inference",
    "text": "1.10 Randomness: The Foundation of Statistical Inference\n\nWhat is Randomness?\nIn statistics, randomness doesn‚Äôt mean chaos‚Äîit means structured uncertainty.\nRandomness doesn‚Äôt mean ‚Äúchaotic‚Äù or ‚Äúunpredictable in principle.‚Äù It refers to a process where individual outcomes are unpredictable, but the long-run pattern follows known probabilities.\nRandomness has two key properties:\n\nUnpredictability in individual cases: We can‚Äôt know if a specific voter will turn out\nPredictability in aggregate: We can estimate that 60% of registered voters will turn out\n\n\n\nRandomness as Predictability of Frequencies\nAn individual random event is, by definition, unpredictable.\nYet, if we know the probability distribution, the frequency of outcomes across repeated events is predictable.\n\nExample: Throwing a fair die.\n\nAny single roll is unpredictable.\n\nAcross many rolls, each face appears about \\tfrac{1}{6} of the time.\n\n\n\nDice Example\nRolling two dice demonstrates this principle:\n\nSingle roll: Cannot predict outcome\nMultiple rolls: Frequency of each sum becomes predictable\n\nProbability distribution for two dice:\n\nSum of 4: 3 possible combinations ‚Üí P(4) = 3/36 = 8.3%\nSum of 7: 6 possible combinations ‚Üí P(7) = 6/36 = 16.7%\nTherefore, 7 occurs twice as frequently as 4 over many trials\n\nThe law of large numbers ensures that empirical frequencies converge to theoretical probabilities.\nüëâ Randomness does not mean lack of order. It means:\n- Each trial is uncertain,\n- But the long-run distribution is stable.\n\n\n\nRandomness vs.¬†Haphazardness\n\nHaphazardness = apparent disorder, lacking any identifiable pattern.\n\nRandomness = governed by probabilities, with a predictable structure in the long run.\n\nExample:\n- Dice rolls look haphazard, but actually follow a precise probability distribution.\n- Something may look haphazard but be deterministic rather than random.\n\n\n\nRandomness vs.¬†Chaos\n\nChaos: deterministic systems that are extremely sensitive to initial conditions.\n\nThe rules are exact, but tiny changes in starting points lead to wildly different outcomes.\n\nExample:\n- Weather systems, double pendulum.\nKey difference:\n- Randomness = true uncertainty (no way to predict exact outcomes).\n- Chaos = deterministic unpredictability (in practice unpredictable, but in principle rule-based).\n\n\n\nRandomness vs.¬†Entropy\n\nEntropy: a measure of uncertainty or disorder.\n\nIn information theory (Shannon entropy):\n\nH(X) = -\\sum_i p(x_i) \\log p(x_i)\n\n\nA fair die has higher entropy than a biased die.\n\nIn thermodynamics, entropy measures disorder in physical systems.\n\nüëâ Randomness increases entropy, but entropy is a quantitative measure, not the same as randomness itself.\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\nConcept\nMeaning\nExample\n\n\n\n\nRandomness\nUncertainty of outcomes, predictable frequencies in the long run\nDice rolls, coin flips\n\n\nChaos\nDeterministic but unpredictable due to sensitivity to initial conditions\nWeather, double pendulum\n\n\nEntropy\nMeasure of uncertainty/disorder (information or physical systems)\nShannon entropy of fair vs.¬†loaded die\n\n\nHaphazardness\nInformal notion of disorder without clear structure\nMessy scribbles, clutter on a desk\n\n\n\n\n\n\nKey Takeaway\n\nRandomness is structured uncertainty, not pure messiness.\n\nChaos is deterministic unpredictability.\n\nEntropy is a measure of uncertainty/disorder.\n\nHaphazardness is informal disorder without precise definition.\n\nRandomness represents structured uncertainty ‚Äî individual outcomes remain unpredictable while aggregate behavior follows mathematical laws. This distinguishes true randomness from mere disorder or complexity.\n\n\n\nWhy Randomness Matters\nRandomness appears in political science in two crucial ways:\nRandom Sampling\n\nPrevents systematic bias in surveys\nAllows us to quantify uncertainty\nFoundation for statistical inference\n\nRandom Assignment (in experiments)\n\nEnsures treatment and control groups are comparable\nAllows causal inference\nEliminates confounding\n\n\n\nThe Power of Random Sampling\nHere‚Äôs the remarkable fact: by embracing randomness in our sampling, we gain the ability to make precise statements about populations.\nFor example: If we randomly sample 1,000 voters and find 55% support a candidate, statistics tells us that:\n\nThe true population support is probably close to 55%\nWe can calculate exactly how close (typically within about 3 percentage points)\nWe can state our confidence level (usually 95%)\n\nThis seems like magic, but it works because randomness follows predictable patterns in large samples.\n\n# Demonstrate why random sampling works\nset.seed(42)\n\n# Create a \"population\" with known characteristics\npopulation_support &lt;- c(rep(\"Candidate A\", 5200), rep(\"Candidate B\", 4800))\ntrue_support_A &lt;- mean(population_support == \"Candidate A\")\n\n# Function to take a random sample and calculate support\ntake_sample &lt;- function(n) {\n  sample_result &lt;- sample(population_support, n)\n  return(mean(sample_result == \"Candidate A\"))\n}\n\n# Take many samples of different sizes\nsample_sizes &lt;- c(50, 100, 500, 1000)\nresults &lt;- map_dfr(sample_sizes, function(n) {\n  estimates &lt;- replicate(100, take_sample(n))\n  data.frame(\n    sample_size = n,\n    estimate = estimates,\n    true_value = true_support_A\n  )\n})\n\nggplot(results, aes(x = factor(sample_size), y = estimate)) +\n  geom_boxplot(alpha = 0.7, fill = \"lightblue\") +\n  geom_hline(yintercept = true_support_A, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    title = \"Random Sampling Gets Closer to Truth with Larger Samples\",\n    subtitle = \"Red line shows true population value (52%)\",\n    x = \"Sample Size\",\n    y = \"Estimated Support for Candidate A\",\n    caption = \"Each box shows 100 random samples of that size\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format())\n\n\n\n\n\n\n\n\n\nFigure Note: Box plots summarize the distribution of estimates across multiple samples. The box shows the middle 50% of estimates (25th to 75th percentile), with the dark line indicating the median. ‚ÄúWhiskers‚Äù extend to show the range, and any outliers appear as individual points. Notice how boxes become narrower (less variable) as sample size increases.\n\nKey insight: Random sampling allows us to make valid inferences about populations, even when we can‚Äôt observe everyone.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#measurement-turning-concepts-into-numbers",
    "href": "chapter1.html#measurement-turning-concepts-into-numbers",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.11 Measurement: Turning Concepts into Numbers",
    "text": "1.11 Measurement: Turning Concepts into Numbers\n\nThe Measurement Challenge in Political Science\nPolitical scientists face a unique challenge: many of our most important concepts resist easy measurement:\n\nHow do you measure ‚Äúdemocracy‚Äù?\nWhat number captures ‚Äúpolitical ideology‚Äù?\nHow do you quantify ‚Äúinstitutional strength‚Äù?\nHow do you measure ‚Äúpolitical participation‚Äù?\n\n\n\nTypes of Measurement\nNominal (Categories without order)\n\nParty affiliation: Democrat, Republican, Independent\nCountry: USA, UK, Germany\nVote choice: Candidate A, Candidate B, Did not vote\nMathematical operations: Only counting/frequencies\n\nOrdinal (Ordered categories)\n\nEducation level: High school &lt; Bachelor‚Äôs &lt; Master‚Äôs &lt; PhD\nSurvey responses: Strongly disagree &lt; Disagree &lt; Neutral &lt; Agree &lt; Strongly agree\nPolitical knowledge: Low &lt; Medium &lt; High\nMathematical operations: Ordering, but not meaningful distances\n\nInterval (Numeric with consistent intervals)\n\nYears: Difference between 2020-2021 equals 2023-2024\nTemperature in Celsius\nStandardized test scores\nMathematical operations: Addition, subtraction, averaging\n\nRatio (Interval with true zero)\n\nVote count: 0 votes means no votes\nGDP: Can meaningfully say one country‚Äôs GDP is twice another‚Äôs\nAge: 18, 19, 20, ‚Ä¶ years\nIncome: $25,000, $50,000, $75,000\nCampaign contributions: $0, $100, $1,000\nMathematical operations: All operations including ratios\n\n\n\nMeasurement Error: The Inevitable Companion\nEvery measurement contains some error. Consider measuring ‚Äúdemocracy‚Äù:\n\\text{Observed Democracy Score} = \\text{True Democracy Level} + \\text{Measurement Error}\nThis fundamental equation expresses that:\nWhat we observe = True value + Error\nThere are two types of error:\nSystematic Error (Bias)\n\nConsistently pushes results in one direction\nDoes not improve with additional data collection\nExample: A survey question worded as ‚ÄúDon‚Äôt you agree that taxes are too high?‚Äù will systematically overestimate anti-tax sentiment\n\nRandom Error\n\nUnpredictable fluctuations up and down\nAverages out with more data\nExample: Some respondents might misunderstand a question or accidentally check the wrong box, but these errors go in both directions\n\nThe critical distinction: Random error can be reduced by collecting more data, while systematic error requires modifying the measurement approach itself.\n\n# Illustrate measurement error\nset.seed(101)\nn_countries &lt;- 50\ntrue_democracy &lt;- runif(n_countries, 0, 10)\nmeasurement_error &lt;- rnorm(n_countries, 0, 1)\nobserved_democracy &lt;- true_democracy + measurement_error\n\nmeasurement_data &lt;- data.frame(\n  country = 1:n_countries,\n  true_value = true_democracy,\n  observed_value = observed_democracy,\n  error = measurement_error\n)\n\nggplot(measurement_data, aes(x = true_value, y = observed_value)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Measurement Error in Democracy Scores\",\n    subtitle = \"Points should fall on red line if measurement were perfect\",\n    x = \"True Democracy Level\",\n    y = \"Observed Democracy Score\"\n  ) +\n  coord_equal()\n\n\n\n\n\n\n\n\n\nFigure Note: This scatter plot illustrates measurement error. If measurement were perfect, all points would fall exactly on the red diagonal line (observed = true value). Deviation from this line represents measurement error‚Äîthe difference between what we observe and the true underlying value.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#statistical-error-and-uncertainty",
    "href": "chapter1.html#statistical-error-and-uncertainty",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.12 Statistical Error and Uncertainty",
    "text": "1.12 Statistical Error and Uncertainty\nWhen we use data to answer questions, we invariably encounter uncertainty. Understanding this uncertainty is crucial for making valid inferences from data.\n\nTypes of Statistical Error\nSampling Error\nSampling error represents the natural variation that occurs when studying a sample instead of the entire population.\n\nOrigin: Different samples yield slightly different results\nPredictability: Can be calculated mathematically\nRelationship to sample size: Decreases as sample size increases\nExample: When polling 1,000 voters, results typically fall within ¬±3% of the true population value\n\nNon-sampling Error\nNon-sampling errors encompass all other sources of error in data collection and analysis:\n\nResponse bias: Respondents provide inaccurate answers (lying, misremembering, social desirability)\nQuestion bias: Poorly worded questions lead to misleading answers\nCoverage error: Systematic exclusion of important groups from the sample\nNon-response bias: Certain types of people systematically fail to respond\nProcessing errors: Mistakes in data entry or analysis\n\nCritical insight: Unlike sampling error, non-sampling errors do not diminish with larger sample sizes. A biased question remains biased whether posed to 100 or 10,000 respondents.\n\n\nQuantifying Uncertainty: Standard Error and Confidence Intervals\nWhen estimating population parameters from samples (such as the percentage supporting a candidate), we must express the uncertainty inherent in these estimates.\nStandard Error (SE)\nThe standard error quantifies how much an estimate would typically vary across different samples of the same size.\n\nRepresents the typical variability in the estimate\nSmaller SE indicates more precise estimates\n\nDecreases as sample size increases\n\nConfidence Intervals (CI)\nA confidence interval provides a range of plausible values for the true population parameter.\nCritical interpretation: What does ‚Äú95% confidence‚Äù mean?\n\nIf we collected 100 different samples and calculated a 95% CI for each\nApproximately 95 of those intervals would contain the true population value\nApproximately 5 would miss it\nWe cannot determine whether our specific interval is among the 95 or among the 5\n\nThe confidence interval is calculated as:\n\\text{CI} = \\text{Sample Estimate} \\pm (\\text{Critical Value} \\times \\text{Standard Error})\nFor a 95% CI, the critical value equals approximately 1.96 (derived from the normal distribution, as we will examine later).\n\n\nExample: Political Polling\nConsider how uncertainty manifests in actual polling scenarios:\n\n# Poll of 1000 voters shows 52% support for Candidate A\nn &lt;- 1000          # sample size\np_hat &lt;- 0.52      # sample proportion (52%)\n\n# Calculate the standard error\n# For a proportion, SE = sqrt(p*(1-p)/n)\nse &lt;- sqrt(p_hat * (1 - p_hat) / n)\ncat(\"Standard Error:\", round(se, 4), \"\\n\")\n\nStandard Error: 0.0158 \n\n# Calculate margin of error (for 95% confidence)\nmargin &lt;- 1.96 * se\ncat(\"Margin of Error: ¬±\", round(margin * 100, 1), \"%\\n\")\n\nMargin of Error: ¬± 3.1 %\n\n# The 95% confidence interval\nci_lower &lt;- p_hat - margin\nci_upper &lt;- p_hat + margin\ncat(\"95% Confidence Interval: [\", \n    round(ci_lower * 100, 1), \"%, \", \n    round(ci_upper * 100, 1), \"%]\\n\")\n\n95% Confidence Interval: [ 48.9 %,  55.1 %]\n\n\nInterpretation: Based on our poll, we estimate 52% support for Candidate A. The margin of error indicates the true support likely falls between 49% and 55%. Since this interval includes 50%, we cannot confidently conclude the candidate leads.\n\n# Five different polls of the same race\nset.seed(2024)\ntrue_support &lt;- 0.515  # Unknown true population support\nn &lt;- 1000  # Each poll surveys 1000 voters\n\n# Create poll data with individual standard errors\npolls &lt;- data.frame(\n  poll = LETTERS[1:5],\n  estimate = c(0.52, 0.51, 0.53, 0.50, 0.54)\n)\n\n# Calculate individual SE for each poll\npolls$se &lt;- sqrt(polls$estimate * (1 - polls$estimate) / n)\npolls$ci_lower &lt;- polls$estimate - 1.96 * polls$se\npolls$ci_upper &lt;- polls$estimate + 1.96 * polls$se\npolls$includes_fifty &lt;- (polls$ci_lower &lt;= 0.50) & (0.50 &lt;= polls$ci_upper)\n\n# Display confidence intervals\ncat(\"Poll Results with 95% Confidence Intervals:\\n\")\n\nPoll Results with 95% Confidence Intervals:\n\nfor (i in 1:nrow(polls)) {\n  cat(sprintf(\"%s: %.1f%% [%.1f%%, %.1f%%] - %s 50%%\\n\",\n              polls$poll[i], \n              polls$estimate[i] * 100,\n              polls$ci_lower[i] * 100,\n              polls$ci_upper[i] * 100,\n              ifelse(polls$includes_fifty[i], \"includes\", \"excludes\")))\n}\n\nA: 52.0% [48.9%, 55.1%] - includes 50%\nB: 51.0% [47.9%, 54.1%] - includes 50%\nC: 53.0% [49.9%, 56.1%] - includes 50%\nD: 50.0% [46.9%, 53.1%] - includes 50%\nE: 54.0% [50.9%, 57.1%] - excludes 50%\n\n# Visualize the polls with confidence intervals\nlibrary(ggplot2)\nggplot(polls, aes(x = poll, y = estimate)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper),\n                width = 0.2, color = \"darkblue\", linewidth = 1) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", \n             color = \"red\", alpha = 0.7) +\n  geom_hline(yintercept = true_support, linetype = \"solid\", \n             color = \"darkgreen\", alpha = 0.7) +\n  labs(title = \"Five Independent Polls of the Same Election\",\n       subtitle = \"Error bars show 95% confidence intervals\",\n       x = \"Poll\",\n       y = \"Support for Candidate A\") +\n  theme_minimal() +\n  scale_y_continuous(labels = scales::percent, \n                     limits = c(0.46, 0.58)) +\n  annotate(\"text\", x = 4.5, y = 0.502, \n           label = \"50% (tie)\", color = \"red\", size = 3) +\n  annotate(\"text\", x = 4.5, y = 0.517, \n           label = \"True value\\n(unknown in practice)\", \n           color = \"darkgreen\", size = 3)\n\n\n\n\n\n\n\n\nKey Observations:\n\nFour of five polls have confidence intervals that include 50% (Polls A, B, C, and D)\nPoll E (54% support) has a confidence interval that excludes 50%, suggesting statistical significance\nAll confidence intervals overlap with each other, indicating consistency across polls\nAll intervals contain the true population value (51.5%), demonstrating proper coverage\nThe variation in point estimates (50% to 54%) represents typical sampling variability\n\n\n\nUnderstanding Confidence Intervals Through Simulation\nThe following simulation demonstrates the true meaning of ‚Äú95% confidence‚Äù:\n\n# Simulate taking many polls to see how often CIs work\nset.seed(123)\ntrue_prop &lt;- 0.52    # The true population proportion\nn_polls &lt;- 100       # Number of simulated polls\nsample_size &lt;- 1000  # Each poll surveys 1000 people\n\n# Simulate the polls\nsample_props &lt;- rbinom(n_polls, sample_size, true_prop) / sample_size\n\n# Calculate confidence interval for each poll\nse &lt;- sqrt(sample_props * (1 - sample_props) / sample_size)\nci_lower &lt;- sample_props - 1.96 * se\nci_upper &lt;- sample_props + 1.96 * se\n\n# Check which intervals contain the true value\nci_data &lt;- data.frame(\n  poll_id = 1:n_polls,\n  estimate = sample_props,\n  ci_lower = ci_lower,\n  ci_upper = ci_upper,\n  contains_truth = (ci_lower &lt;= true_prop) & (true_prop &lt;= ci_upper)\n)\n\n# Count how many intervals contain the truth\ncoverage &lt;- mean(ci_data$contains_truth) * 100\ncat(\"Percentage of CIs containing true value:\", coverage, \"%\\n\")\n\nPercentage of CIs containing true value: 93 %\n\n# Visualize first 30 polls\nggplot(ci_data[1:30, ], aes(x = poll_id, y = estimate, \n                             color = contains_truth)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), \n                width = 0.3, alpha = 0.7) +\n  geom_hline(yintercept = true_prop, color = \"black\", \n             linetype = \"solid\", linewidth = 1) +\n  scale_color_manual(values = c(\"FALSE\" = \"red\", \"TRUE\" = \"blue\"),\n                     labels = c(\"Misses truth\", \"Contains truth\")) +\n  labs(\n    title = \"30 Polls with 95% Confidence Intervals\",\n    subtitle = paste(\"Black line shows true value. About 95% of intervals should contain it.\",\n                     \"\\nIn this simulation:\", sum(ci_data$contains_truth[1:30]), \n                     \"out of 30 contain the true value.\"),\n    x = \"Poll Number\",\n    y = \"Estimated Support\",\n    color = \"Interval Status\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0.4, 0.6)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nInterpretation of Results:\n\nEach horizontal line segment represents one poll‚Äôs confidence interval\nBlue intervals contain the true value (black line)\nRed intervals miss the true value\nOver many polls, approximately 95% of intervals contain the truth\nFor any single poll, we cannot determine whether our interval captures the true value\n\n\n\nFactors Affecting Uncertainty\n\n# Show how sample size affects margin of error\nsample_sizes &lt;- c(100, 250, 500, 1000, 2000, 4000, 10000)\np &lt;- 0.5  # Use 0.5 as worst-case (maximum uncertainty)\n\n# Calculate margin of error for each sample size\nmargins &lt;- 1.96 * sqrt(p * (1 - p) / sample_sizes)\n\nmargin_data &lt;- data.frame(\n  n = sample_sizes,\n  margin = margins * 100  # Convert to percentage\n)\n\nggplot(margin_data, aes(x = n, y = margin)) +\n  geom_line(color = \"darkblue\", linewidth = 1.5) +\n  geom_point(color = \"darkblue\", size = 3) +\n  geom_hline(yintercept = 3, linetype = \"dashed\", \n             color = \"red\", alpha = 0.7) +\n  scale_x_log10(breaks = sample_sizes,\n                labels = scales::comma) +\n  labs(\n    title = \"How Sample Size Affects Precision\",\n    subtitle = \"Larger samples yield smaller margins of error (note logarithmic x-axis)\",\n    x = \"Sample Size\",\n    y = \"Margin of Error (%)\"\n  ) +\n  theme_minimal() +\n  annotate(\"text\", x = 5000, y = 3.3, \n           label = \"¬±3% (common target)\", \n           color = \"red\", size = 3) +\n  annotate(\"text\", x = 100, y = 10.5, \n           label = paste(\"n=100: ¬±\", round(margins[1]*100, 1), \"%\"), \n           size = 3, hjust = 0) +\n  annotate(\"text\", x = 10000, y = 1.5, \n           label = paste(\"n=10,000: ¬±\", round(margins[7]*100, 1), \"%\"), \n           size = 3, hjust = 1)\n\n\n\n\n\n\n\n\nKey Mathematical Relationships:\n\nDoubling the sample size does not halve the margin of error\nTo reduce the margin of error by half requires quadrupling the sample size\nDiminishing returns: The reduction from n=100 to n=1,000 is more substantial than from n=1,000 to n=10,000\n\n\n\nPractical Guidelines for Working with Uncertainty\nWhen interpreting results with uncertainty:\n\nReport confidence intervals alongside point estimates\n\nInsufficient: ‚Äú52% support the candidate‚Äù\nAppropriate: ‚Äú52% support the candidate (95% CI: 49%-55%)‚Äù\n\nAssess whether differences exceed sampling variability\n\nIf Poll A shows 52% and Poll B shows 51%, the difference may reflect sampling variation\nExamine whether confidence intervals overlap\n\nAcknowledge unmeasured sources of error\n\nConfidence intervals capture only sampling error\nThey do not account for question bias, coverage errors, or response inaccuracies\n\nPrioritize study quality over sample size\n\nA well-designed study with 1,000 respondents surpasses a biased study with 100,000\nMethodological rigor matters more than sample size alone\n\n\n\n\nSummary\nStatistical uncertainty is an inherent feature of empirical research in political science. Key principles include:\n\nSampling error is predictable, quantifiable, and decreases with larger samples\nNon-sampling errors pose greater threats because they persist regardless of sample size\nStandard error measures the typical variation in estimates across samples\nConfidence intervals provide ranges of plausible values with specified coverage probabilities\nUncertainty quantification should always accompany statistical estimates to enable proper interpretation",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#statistical-significance-making-sense-of-uncertain-evidence",
    "href": "chapter1.html#statistical-significance-making-sense-of-uncertain-evidence",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.13 Statistical Significance: Making Sense of Uncertain Evidence",
    "text": "1.13 Statistical Significance: Making Sense of Uncertain Evidence\n\nThe Courtroom Analogy for Hypothesis Testing\nStatistical hypothesis testing follows a logic analogous to legal proceedings:\n\nNull hypothesis (H_0): The defendant is innocent (no real effect exists)\nAlternative hypothesis (H_1): The defendant is guilty (a real effect exists)\nEvidence: Our data and statistical test\nVerdict: Reject H_0 (find significance) or fail to reject H_0 (no significance)\n\nAs in legal proceedings, we require strong evidence to reject the presumption of innocence (no effect). This framework leads to two types of potential errors:\n\nType I error (false positive): Convicting an innocent person (rejecting H_0 when H_0 is true), controlled by significance level \\alpha (typically 0.05)\nType II error (false negative): Acquitting a guilty person (failing to reject H_0 when H_1 is true), with probability \\beta and power 1-\\beta\n\n\n\nWhat is Statistical Significance?\nWhen we observe a difference in our data, we face a fundamental question: Does this difference reflect a true population characteristic or merely sampling variability?\nStatistical significance provides a framework for answering:\n\nIs the observed pattern likely due to a real effect, or could it plausibly arise from random chance alone?\n\nThis framework distinguishes between:\n\nSignal: Real patterns reflecting true relationships in the population\nNoise: Random variation arising from sampling\n\n\n\nThe Logic of Hypothesis Testing\nThe null hypothesis represents our default assumption‚Äîtypically that no effect or relationship exists:\n\nNo difference between groups\nNo relationship between variables\nNo treatment effect\n\nWe maintain this skeptical stance until the data provide sufficient evidence to reject it.\n\n\nUnderstanding p-values: Three Complementary Perspectives\nThe p-value remains one of the most misunderstood concepts in statistics. Consider three complementary interpretations:\n\n1. The Surprise Metric\nThe p-value quantifies how surprised we should be to observe our data if nothing systematic were occurring:\n\nSmall p-value (&lt; 0.05): Very surprising under the null ‚Üí Evidence for an effect\nLarge p-value (&gt; 0.05): Not surprising under the null ‚Üí Insufficient evidence\n\n\n\n2. The Coin Flip Illustration\nConsider testing whether a coin is fair. You flip it 10 times and observe 8 heads.\nThe p-value answers: If the coin were actually fair, how often would we observe 8 or more heads in 10 flips?\nCalculation: P(X \\geq 8) = \\sum_{k=8}^{10} \\binom{10}{k} 0.5^{10} = \\frac{56}{1024} \\approx 0.0547\nSince this probability is relatively small (5.47%), we have moderate evidence against fairness.\n\n\n3. The Formal Definition\nA p-value is:\n\nThe probability of observing data at least as extreme as what we obtained, assuming the null hypothesis is true.\n\nFormally, for test statistic T and observed value t_{\\text{obs}}:\n\nOne-sided: p = P(T \\geq t_{\\text{obs}} \\mid H_0) or P(T \\leq t_{\\text{obs}} \\mid H_0)\nTwo-sided: p = 2 \\min\\{P(T \\geq |t_{\\text{obs}}| \\mid H_0), P(T \\leq -|t_{\\text{obs}}| \\mid H_0)\\}\n\nCritical clarification: The p-value assumes the null hypothesis is true‚Äîit does not provide the probability that the null hypothesis is true.\n\n\n\nA Visual Understanding of p-values\n\n# Simulate what happens under the null hypothesis\nset.seed(789)\nnull_distribution &lt;- rnorm(10000, mean = 0, sd = 1)\n\n# Our observed test statistic\nobserved &lt;- 2.1\n\n# Create data frame for visualization\nhist_data &lt;- data.frame(values = null_distribution)\n\n# Create the visualization\nggplot(hist_data, aes(x = values)) +\n  geom_histogram(aes(y = ..density..), bins = 50, \n                 fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_density(color = \"darkblue\", linewidth = 1) +\n  geom_vline(xintercept = observed, color = \"red\", linewidth = 1.5) +\n  geom_vline(xintercept = -observed, color = \"red\", linewidth = 1.5, \n             linetype = \"dashed\") +\n  geom_area(stat = \"function\", fun = dnorm, \n            xlim = c(observed, 4), fill = \"red\", alpha = 0.3) +\n  geom_area(stat = \"function\", fun = dnorm, \n            xlim = c(-4, -observed), fill = \"red\", alpha = 0.3) +\n  labs(title = \"What the p-value Measures\",\n       subtitle = \"Distribution of possible results if the null hypothesis were true\",\n       x = \"Test Statistic Values\",\n       y = \"Probability Density\") +\n  annotate(\"text\", x = 2.5, y = 0.15, \n           label = \"p-value:\\nProbability of\\nresults this extreme\\nor more extreme\", \n           color = \"red\", fontface = \"bold\", size = 3) +\n  annotate(\"text\", x = 0, y = 0.2, \n           label = \"Most likely\\nresults if\\nno effect\", \n           color = \"darkblue\", size = 3)\n\n\n\n\n\n\n\n\nThe blue distribution represents expected outcomes under the null hypothesis. The red lines mark our observed result, and the red shaded areas show the p-value‚Äîthe probability of obtaining results at least this extreme by chance alone.\n\n\nExamples: Understanding p-values in Context\n\nExample 1: Campaign Advertisement Effectiveness\nResearch Question: Do television advertisements increase candidate vote share?\nDesign: A candidate runs TV ads in 20 randomly selected cities but not in 20 other similar cities.\n\n# Simulate the campaign ad experiment\nset.seed(123)\n\n# Generate realistic data\nad_cities &lt;- c(rep(\"With Ads\", 20), rep(\"No Ads\", 20))\nvote_share &lt;- c(\n  rnorm(20, 0.58, 0.08),  # Cities with ads: mean 58%, SD 8%\n  rnorm(20, 0.54, 0.08)   # Cities without ads: mean 54%, SD 8%\n)\n\ncampaign_data &lt;- data.frame(\n  treatment = factor(ad_cities, levels = c(\"No Ads\", \"With Ads\")),\n  vote_share = vote_share\n)\n\n# Calculate the observed difference\nmean_with_ads &lt;- mean(campaign_data$vote_share[campaign_data$treatment == \"With Ads\"])\nmean_no_ads  &lt;- mean(campaign_data$vote_share[campaign_data$treatment == \"No Ads\"])\nobserved_diff &lt;- mean_with_ads - mean_no_ads\n\n# Perform t-test (two-sided by default)\nt_test_result &lt;- t.test(vote_share ~ treatment, data = campaign_data)\np_val &lt;- t_test_result$p.value\n\n# Create visualization\nggplot(campaign_data, aes(x = treatment, y = vote_share, fill = treatment)) +\n  geom_boxplot(alpha = 0.7, width = 0.5) +\n  geom_jitter(width = 0.15, alpha = 0.6, size = 2.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4, \n               fill = \"red\", color = \"darkred\") +\n  labs(\n    title = \"Do Campaign Ads Increase Vote Share?\",\n    subtitle = paste0(\"Observed difference: \", round(observed_diff*100, 1), \n                     \" percentage points, p-value = \", round(p_val, 3)),\n    x = \"Treatment Condition\",\n    y = \"Vote Share (%)\",\n    caption = \"Red diamonds show group means. Each dot represents one city.\"\n  ) +\n  scale_y_continuous(labels = function(x) paste0(x*100, \"%\")) +\n  scale_fill_manual(values = c(\"No Ads\" = \"#E8E8E8\", \"With Ads\" = \"#4CAF50\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nInterpretation:\n\nThe boxes display the middle 50% of cities in each group\nRed diamonds indicate group means\nIndividual dots represent individual cities\nIf p-value &lt; 0.05, the observed difference is unlikely due to chance alone\n\n\n\nExample 2: Weather Effects on Voter Turnout\nResearch Question: Does rain decrease voter turnout?\n\n# Simulate weather and turnout data\nset.seed(456)\noptions(scipen = 999)\n\n# Create realistic data with clear difference\nn_elections &lt;- 30\nweather_data &lt;- data.frame(\n  weather = factor(c(rep(\"Rainy\", n_elections), rep(\"Sunny\", n_elections)),\n                   levels = c(\"Sunny\", \"Rainy\")),\n  turnout = c(\n    rnorm(n_elections, 0.62, 0.06),  # Rainy days: lower turnout\n    rnorm(n_elections, 0.68, 0.06)   # Sunny days: higher turnout\n  )\n)\n\n# Calculate statistics\nrain_turnout  &lt;- mean(weather_data$turnout[weather_data$weather == \"Rainy\"])\nsunny_turnout &lt;- mean(weather_data$turnout[weather_data$weather == \"Sunny\"])\nweather_diff  &lt;- sunny_turnout - rain_turnout  # Sunny minus Rainy\n\n# Statistical test (two-sided)\nweather_test &lt;- t.test(turnout ~ weather, data = weather_data)\nweather_p    &lt;- weather_test$p.value\nci_lower     &lt;- weather_test$conf.int[1]  # CI corresponds to Sunny - Rainy\nci_upper     &lt;- weather_test$conf.int[2]\n\n# Create enhanced visualization\nggplot(weather_data, aes(x = weather, y = turnout, fill = weather)) +\n  geom_boxplot(alpha = 0.7, width = 0.5) +\n  geom_jitter(width = 0.15, alpha = 0.6, size = 2.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4, \n               fill = \"black\", color = \"black\") +\n  labs(\n    title = \"Does Weather Affect Voter Turnout?\",\n    subtitle = paste0(\"Difference: \", round(weather_diff*100, 1), \n                     \" percentage points (95% CI: [\", \n                     round(ci_lower*100, 1), \", \", round(ci_upper*100, 1), \n                     \"]), p = \", round(weather_p, 3)),\n    x = \"Weather Condition\",\n    y = \"Voter Turnout Rate (%)\",\n    caption = \"Black diamonds show means. Each dot represents one election.\"\n  ) +\n  scale_y_continuous(labels = function(x) paste0(x*100, \"%\")) +\n  scale_fill_manual(values = c(\"Rainy\" = \"#B3D9FF\", \"Sunny\" = \"#FFD700\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nResults Summary:\n\nSunny day turnout: 68.9%\nRainy day turnout: 63.4%\nDifference: 5.5 percentage points\np-value: 0.0008\n\nInterpretation: If weather had no effect on turnout, there would be only a 0.08% chance of observing a difference this large or larger. This provides strong evidence that weather affects turnout.\n\n\nExample 3: Non-Significant Results\nResearch Question: Does time spent on social media predict political knowledge?\n\n# Simulate a case with no meaningful relationship\nset.seed(999)\nn_people &lt;- 150\n\n# Create data with essentially no relationship\nsocial_media_data &lt;- data.frame(\n  social_media_hours = runif(n_people, 0, 8),\n  political_knowledge = rnorm(n_people, 50, 15)\n)\n\n# Add tiny, undetectable relationship\nsocial_media_data$political_knowledge &lt;- social_media_data$political_knowledge + \n  0.5 * social_media_data$social_media_hours + rnorm(n_people, 0, 14)\n\n# Fit linear model\nsm_model  &lt;- lm(political_knowledge ~ social_media_hours, data = social_media_data)\nsm_summary &lt;- summary(sm_model)\nsm_coef &lt;- coef(sm_model)[2]\nsm_p    &lt;- sm_summary$coefficients[2, 4]\nsm_se   &lt;- sm_summary$coefficients[2, 2]\nr_squared &lt;- sm_summary$r.squared\n\n# Create scatter plot with regression line\nggplot(social_media_data, aes(x = social_media_hours, y = political_knowledge)) +\n  geom_point(alpha = 0.5, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\", fill = \"pink\", alpha = 0.3) +\n  labs(\n    title = \"Social Media Use and Political Knowledge\",\n    subtitle = paste0(\"Effect: \", round(sm_coef, 2), \" points per hour (SE = \", \n                     round(sm_se, 2), \"), p = \", round(sm_p, 3),\n                     \", R¬≤ = \", round(r_squared, 3)),\n    x = \"Daily Social Media Hours\",\n    y = \"Political Knowledge Score (0-100)\",\n    caption = \"Wide confidence band indicates high uncertainty about the relationship\"\n  ) +\n  theme_minimal() +\n  annotate(\"text\", x = 6, y = 20, \n           label = \"Not statistically\\nsignificant\", \n           color = \"red\", fontface = \"bold\", size = 4)\n\n\n\n\n\n\n\n\nCritical points about non-significant results:\n\nWe cannot conclude there is no relationship\nWe can only state we lack sufficient evidence for a relationship\nPossible explanations for non-significance:\n\nThe effect truly does not exist\nThe effect is too small to detect with our sample size\nMeasurement error obscures the true relationship\n\n\n\n\n\nThe 0.05 Threshold: Convention, Not Natural Law\nThe conventional threshold of p &lt; 0.05 for ‚Äústatistical significance‚Äù is merely a historical convention established by Ronald Fisher in the 1920s.\n\n# Create a visual showing the continuous nature of p-values\np_values &lt;- seq(0.001, 0.2, by = 0.001)\np_data &lt;- data.frame(\n  p = p_values,\n  significant = ifelse(p_values &lt; 0.05, \"Significant\", \"Not Significant\")\n)\n\nggplot(p_data, aes(x = p, y = 1, fill = significant)) +\n  geom_tile(aes(height = 1)) +\n  geom_vline(xintercept = 0.05, color = \"black\", linewidth = 1.5) +\n  scale_fill_manual(values = c(\"Significant\" = \"#4CAF50\", \n                               \"Not Significant\" = \"#FF6B6B\")) +\n  scale_x_continuous(breaks = c(0.001, 0.01, 0.05, 0.1, 0.15, 0.2),\n                     labels = c(\"0.001\", \"0.01\", \"0.05\", \"0.10\", \"0.15\", \"0.20\")) +\n  labs(\n    title = \"The Arbitrary Nature of the 0.05 Threshold\",\n    subtitle = \"p = 0.049 and p = 0.051 are practically identical, yet conventionally treated differently\",\n    x = \"p-value\",\n    y = \"\",\n    fill = \"Conventional\\nInterpretation\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank()) +\n  annotate(\"text\", x = 0.025, y = 1, label = \"Strong\\nEvidence\", \n           color = \"white\", fontface = \"bold\") +\n  annotate(\"text\", x = 0.125, y = 1, label = \"Weak\\nEvidence\", \n           color = \"white\", fontface = \"bold\") +\n  annotate(\"text\", x = 0.05, y = 0.5, label = \"Arbitrary\\nCutoff\", \n           color = \"black\", fontface = \"bold\", size = 3)\n\n\n\n\n\n\n\n\nKey Considerations:\n\nNothing fundamentally changes at p = 0.05\nDifferent fields adopt different thresholds (physics: p ‚âà 3 √ó 10‚Åª‚Å∑, ‚Äú5 sigma‚Äù)\nModern practice emphasizes reporting exact p-values and effect sizes\nThe dichotomization into ‚Äúsignificant‚Äù versus ‚Äúnot significant‚Äù can be misleading\n\n\n\nCommon Misconceptions About p-values\n\nIncorrect Interpretations:\n\n‚Äúp = 0.03 means there‚Äôs a 97% chance our treatment works‚Äù\n\nError: p-values do not provide the probability that a hypothesis is true\n\n‚Äúp = 0.20 means the effect is small‚Äù\n\nError: p-values measure evidence strength, not effect magnitude\n\n‚Äúp &gt; 0.05 proves there‚Äôs no effect‚Äù\n\nError: Absence of evidence does not constitute evidence of absence\n\n\n\n\nCorrect Interpretations:\n\n‚Äúp = 0.03 means: If there were no effect, we would observe data this extreme only 3% of the time‚Äù\n‚Äúp = 0.20 means we have weak evidence against the null hypothesis‚Äù\n‚Äúp &gt; 0.05 means we cannot confidently distinguish signal from noise‚Äù\n\n\n\n\nStatistical Significance versus Practical Significance\n\n# Demonstrate the difference between statistical and practical significance\nset.seed(42)\n\n# Small but statistically significant effect (large sample)\nlarge_n &lt;- 10000\ngroup_a_large &lt;- rnorm(large_n, mean = 100, sd = 15)\ngroup_b_large &lt;- rnorm(large_n, mean = 100.5, sd = 15)  # Tiny difference\n\n# Large but not statistically significant effect (small sample)\nsmall_n &lt;- 20\ngroup_a_small &lt;- rnorm(small_n, mean = 100, sd = 15)\ngroup_b_small &lt;- rnorm(small_n, mean = 105, sd = 15)  # Large difference\n\n# Tests\ntest_large &lt;- t.test(group_a_large, group_b_large)\ntest_small &lt;- t.test(group_a_small, group_b_small)\n\n# Create comparison visualization\ncomparison_data &lt;- data.frame(\n  Scenario = c(\"Large Sample\\n(n=10,000)\", \"Small Sample\\n(n=20)\"),\n  Effect_Size = c(mean(group_b_large) - mean(group_a_large),\n                  mean(group_b_small) - mean(group_a_small)),\n  P_Value = c(test_large$p.value, test_small$p.value),\n  Significant = c(test_large$p.value &lt; 0.05, test_small$p.value &lt; 0.05)\n)\n\nggplot(comparison_data, aes(x = Effect_Size, y = -log10(P_Value))) +\n  geom_point(aes(color = Significant, shape = Scenario), size = 8) +\n  geom_hline(yintercept = -log10(0.05), linetype = \"dashed\", color = \"red\") +\n  geom_text(aes(label = Scenario), vjust = -1.5, size = 3) +\n  scale_color_manual(values = c(\"FALSE\" = \"gray60\", \"TRUE\" = \"darkgreen\")) +\n  labs(\n    title = \"Statistical versus Practical Significance\",\n    subtitle = \"Large samples detect tiny effects; small samples may miss large effects\",\n    x = \"Effect Size (Difference in Means)\",\n    y = \"Statistical Significance\\n(-log10 p-value)\",\n    caption = \"Points above red line are statistically significant (p &lt; 0.05)\"\n  ) +\n  theme_minimal() +\n  annotate(\"text\", x = 0.5, y = -log10(0.05), \n           label = \"p = 0.05\", color = \"red\", size = 3) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nComparison Results:\nLarge Sample (n=10,000 per group):\n\nDifference: 0.68 units\np-value: 1.412962e-03\nStatistically significant: Yes\nPractically important: Likely not (difference is minimal)\n\nSmall Sample (n=20 per group):\n\nDifference: 4.42 units\np-value: 0.344\nStatistically significant: No\nPractically important: Possibly yes (difference is substantial)\n\nKey Lesson: Always evaluate both statistical significance and effect size.\n\n\nThe Relationship Between p-values and Confidence Intervals\nA direct correspondence exists between p-values and confidence intervals:\n\nIf p &lt; 0.05 for testing ‚Äúno difference,‚Äù the 95% CI excludes zero\nIf p &gt; 0.05, the 95% CI includes zero\n\n\n# Demonstrate the relationship\nset.seed(789)\n\n# Generate several studies with different effect sizes\nstudies &lt;- data.frame(\n  study = LETTERS[1:6],\n  effect = c(2.5, 2.2, 0.9, 0.3, -0.2, -1.5),\n  se = rep(1, 6)\n)\n\nstudies$ci_lower &lt;- studies$effect - 1.96 * studies$se\nstudies$ci_upper &lt;- studies$effect + 1.96 * studies$se\nstudies$p_value  &lt;- 2 * pnorm(-abs(studies$effect/studies$se))\nstudies$significant &lt;- studies$p_value &lt; 0.05\n\nggplot(studies, aes(x = study, y = effect, color = significant)) +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"gray50\", linewidth = 1) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2, linewidth = 1) +\n  geom_point(size = 4) +\n  scale_color_manual(values = c(\"FALSE\" = \"gray60\", \"TRUE\" = \"darkgreen\"),\n                     labels = c(\"Not Significant\", \"Significant\")) +\n  labs(\n    title = \"Confidence Intervals and Statistical Significance\",\n    subtitle = \"CIs that exclude zero correspond to p &lt; 0.05\",\n    x = \"Study\",\n    y = \"Effect Size\",\n    color = \"Statistical Significance\",\n    caption = \"Error bars show 95% confidence intervals\"\n  ) +\n  theme_minimal() +\n  annotate(\"text\", x = 0.5, y = 0.2, label = \"No effect\", \n           color = \"gray50\", fontface = \"italic\", size = 3) +\n  geom_text(aes(label = paste0(\"p=\", round(p_value, 3))), \n            vjust = -2, size = 3)\n\n\n\n\n\n\n\n\nObservations:\n\nStudies A, B: CIs exclude zero ‚Üí p &lt; 0.05\nStudies C, D, E, F: CIs include zero ‚Üí p &gt; 0.05\nDistance from zero correlates inversely with p-value magnitude\n\n\n\nComplete Example: Analyzing a Political Experiment\nResearch Question: Does providing voters with fact-checking information reduce belief in misinformation?\n\n# Simulate a fact-checking experiment\nset.seed(2024)\n\n# Create experimental data\nn_per_group &lt;- 100\nexperiment_data &lt;- data.frame(\n  group = c(rep(\"Control\", n_per_group), rep(\"Fact-Check\", n_per_group)),\n  misinformation_belief = c(\n    rnorm(n_per_group, mean = 65, sd = 12),  # Control: higher belief\n    rnorm(n_per_group, mean = 58, sd = 12)   # Treatment: lower belief\n  )\n)\n\n# Step 1: Calculate descriptive statistics\ndesc_stats &lt;- experiment_data %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    n = n(),\n    mean = mean(misinformation_belief),\n    sd = sd(misinformation_belief),\n    se = sd / sqrt(n),\n    .groups = \"drop\"\n  )\n\n# Extract means\nmean_control &lt;- desc_stats$mean[desc_stats$group == \"Control\"]\nmean_fact    &lt;- desc_stats$mean[desc_stats$group == \"Fact-Check\"]\ndifference_fc &lt;- mean_fact - mean_control\n\n# Step 2: Perform t-test\nt_result &lt;- t.test(misinformation_belief ~ group, data = experiment_data)\n\n# Convert CI to match (Fact-Check - Control)\nci_fc_lower &lt;- -t_result$conf.int[2]\nci_fc_upper &lt;- -t_result$conf.int[1]\n\n# Step 3: Calculate effect size (Cohen's d)\npooled_sd &lt;- sqrt(((n_per_group - 1) * desc_stats$sd[desc_stats$group==\"Control\"]^2 + \n                   (n_per_group - 1) * desc_stats$sd[desc_stats$group==\"Fact-Check\"]^2) / \n                  (2 * n_per_group - 2))\ncohens_d &lt;- difference_fc / pooled_sd\n\n# Step 4: Visualize results\nggplot(experiment_data, aes(x = group, y = misinformation_belief, fill = group)) +\n  geom_violin(alpha = 0.3) +\n  geom_boxplot(width = 0.3, alpha = 0.7) +\n  geom_jitter(width = 0.1, alpha = 0.3, size = 1) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4, \n               fill = \"yellow\", color = \"black\") +\n  scale_fill_manual(values = c(\"Control\" = \"#FF6B6B\", \"Fact-Check\" = \"#4ECDC4\")) +\n  labs(\n    title = \"Does Fact-Checking Reduce Belief in Misinformation?\",\n    subtitle = paste0(\"Difference (Fact-Check ‚àí Control): \", round(difference_fc, 1), \n                     \" points, p = \", round(t_result$p.value, 3),\n                     \", Cohen's d = \", round(cohens_d, 2)),\n    x = \"Experimental Condition\",\n    y = \"Misinformation Belief Score (0-100)\",\n    caption = \"Yellow diamonds show group means\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Step 5: Report results\ncat(\"HYPOTHESIS TEST RESULTS\\n\")\n\nHYPOTHESIS TEST RESULTS\n\ncat(\"Null Hypothesis: Fact-checking has no effect on misinformation belief\\n\")\n\nNull Hypothesis: Fact-checking has no effect on misinformation belief\n\ncat(\"Alternative: Fact-checking affects misinformation belief\\n\\n\")\n\nAlternative: Fact-checking affects misinformation belief\n\ncat(\"Descriptive Statistics:\\n\")\n\nDescriptive Statistics:\n\nprint(desc_stats)\n\n# A tibble: 2 √ó 5\n  group          n  mean    sd    se\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Control      100  64.0  12.3  1.23\n2 Fact-Check   100  59.6  12.3  1.23\n\ncat(\"\\nInferential Statistics:\\n\")\n\n\nInferential Statistics:\n\ncat(paste(\"Difference (Fact-Check ‚àí Control):\", round(difference_fc, 2), \"points\\n\"))\n\nDifference (Fact-Check ‚àí Control): -4.4 points\n\ncat(paste(\"95% CI: [\", round(ci_fc_lower, 2), \",\", round(ci_fc_upper, 2), \"]\\n\"))\n\n95% CI: [ -7.82 , -0.98 ]\n\ncat(paste(\"t-statistic:\", round(t_result$statistic, 2), \"\\n\"))\n\nt-statistic: 2.53 \n\ncat(paste(\"p-value:\", round(t_result$p.value, 4), \"\\n\"))\n\np-value: 0.012 \n\neffect_size_label &lt;- ifelse(abs(cohens_d) &lt; 0.2, \"negligible\",\n                           ifelse(abs(cohens_d) &lt; 0.5, \"small\",\n                                  ifelse(abs(cohens_d) &lt; 0.8, \"medium\", \"large\")))\ncat(paste(\"Cohen's d:\", round(cohens_d, 2), \"(\", effect_size_label, \"effect)\\n\\n\"))\n\nCohen's d: -0.36 ( small effect)\n\ncat(\"Conclusion:\\n\")\n\nConclusion:\n\nif(t_result$p.value &lt; 0.05) {\n  cat(\"We reject the null hypothesis. The evidence suggests that\\n\")\n  cat(\"fact-checking significantly reduces belief in misinformation.\\n\")\n} else {\n  cat(\"We fail to reject the null hypothesis. We lack sufficient\\n\")\n  cat(\"evidence that fact-checking affects misinformation belief.\\n\")\n}\n\nWe reject the null hypothesis. The evidence suggests that\nfact-checking significantly reduces belief in misinformation.\n\n\nNote on directional hypotheses: When theoretical predictions specify direction (e.g., fact-checking reduces misinformation belief), one-sided tests may be appropriate. Set alternative = \"greater\" for \\mu_{\\text{Control}} &gt; \\mu_{\\text{Fact-Check}} or alternative = \"less\" for the opposite. Two-sided tests remain standard when any difference would be theoretically meaningful.\n\n\nSummary: Practical Guidelines for Statistical Significance\nWhen interpreting statistical tests, follow this systematic approach:\n\nEvaluate the effect size first\n\nAssess the magnitude of the difference or relationship\nDetermine practical meaningfulness\n\nExamine the p-value\n\np &lt; 0.05: Evidence against the null hypothesis\np &gt; 0.05: Insufficient evidence to reject the null\n\nInterpret confidence intervals\n\nThese indicate the range of plausible effect sizes\nWider intervals reflect greater uncertainty\n\nConsider the research context\n\nSample size affects statistical power\nStudy quality outweighs p-value magnitude\nMultiple testing increases false positive risk\n\n\nFundamental Principles:\nStatistical significance does not equal practical importance. The p-value measures surprise under the null hypothesis, not the probability of truth. Absence of evidence does not constitute evidence of absence. Effect sizes should always accompany p-values in research reports.\nStatistical significance serves as a tool for distinguishing signal from noise in data, not as a measure of importance or truth. Apply it judiciously within the broader context of substantive and practical significance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#regression-the-workhorse-of-political-science",
    "href": "chapter1.html#regression-the-workhorse-of-political-science",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.14 Regression: The Workhorse of Political Science",
    "text": "1.14 Regression: The Workhorse of Political Science\nConsider a typical pre-election news headline: ‚ÄúCandidate Smith‚Äôs approval rating reaches 68%.‚Äù Your immediate inference likely suggests favorable electoral prospects for Smith‚Äînot guaranteed victory, but a strong position.\nThis intuitive assessment exemplifies the essence of regression analysis. You utilized one piece of information (approval rating) to predict another outcome (electoral success), automatically recognizing that higher approval ratings correlate with better electoral performance, despite an imperfect relationship.\nRegression analysis systematizes this intuitive process, enabling researchers to:\n\nGenerate predictions based on available information\nIdentify which factors matter most\nQuantify uncertainty in predictions\nTest theoretical propositions with empirical data",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#variables-and-variation",
    "href": "chapter1.html#variables-and-variation",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.15 Variables and Variation",
    "text": "1.15 Variables and Variation\n\nDefining Variables\nA variable is any characteristic that can take different values across units of observation. In political science:\n\nUnits of analysis: Countries, individuals, elections, policies, years\nVariables: GDP, voting preference, democracy score, conflict occurrence",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#what-is-regression",
    "href": "chapter1.html#what-is-regression",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.16 What is Regression?",
    "text": "1.16 What is Regression?\nRegression analysis constitutes the foundational statistical tool in political science. It models relationships between variables and operationalizes our fundamental statistical model.\n\nThe Fundamental Model\nA model represents an object, person, or system in an informative way. Models divide into physical representations (such as architectural models) and abstract representations (such as mathematical equations describing atmospheric dynamics).\nThe core of statistical thinking can be expressed as:\nY = f(X) + \\text{error}\nThis equation states that our outcome (Y) equals some function of our predictors (X), plus unpredictable variation.\nComponents:\n\nY = Dependent variable (the phenomenon we seek to explain)\nX = Independent variable(s) (explanatory factors)\nf() = The functional relationship (often assumed linear)\nerror (\\epsilon) = Unexplained variation\n\nThis model provides the foundation for all statistical analysis‚Äîfrom simple correlations to complex machine learning algorithms.\nRegression helps answer fundamental questions such as:\n\nHow much does education increase political participation?\nWhat factors predict electoral success?\nDo democratic institutions promote economic growth?",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#building-intuition-a-sports-analogy",
    "href": "chapter1.html#building-intuition-a-sports-analogy",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.17 Building Intuition: A Sports Analogy",
    "text": "1.17 Building Intuition: A Sports Analogy\nBefore examining political applications, consider a simpler context. Suppose you want to predict basketball players‚Äô scoring based on their height. Expected patterns include:\n\nTaller players generally score more points\nHeight alone does not determine scoring (skill, position, and playing time matter)\nSubstantial variation exists‚Äîsome shorter players excel at scoring\n\nPlotting height (x-axis) versus points scored (y-axis) would likely reveal:\n\nAn upward trend in points as height increases\nConsiderable scatter around that trend\nA line capturing the general relationship\n\nThis illustrates regression‚Äôs essence: finding the line that best summarizes relationships between variables while acknowledging imperfect correlations.\n\n# Create basketball example for intuition\nset.seed(123)\nn_players &lt;- 100\n\n# Generate realistic basketball data\nheight_inches &lt;- rnorm(n_players, 78, 4)  # Average NBA height ~6'6\"\n# Scoring increases with height, but with substantial variation\npoints_per_game &lt;- 2 + 0.3 * (height_inches - 70) + rnorm(n_players, 0, 5)\npoints_per_game &lt;- pmax(0, points_per_game)  # No negative scoring\n\nbasketball_data &lt;- data.frame(\n  height = height_inches,\n  points = points_per_game\n)\n\n# Visualization\nggplot(basketball_data, aes(x = height, y = points)) +\n  geom_point(alpha = 0.6, color = \"orange\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"blue\", size = 1.2) +\n  labs(\n    title = \"Height versus Points Scored: The Basic Concept of Regression\",\n    subtitle = \"The blue line shows the general relationship; points show individual players\",\n    x = \"Height (inches)\",\n    y = \"Points Per Game\",\n    caption = \"Each point represents one player; the line summarizes the overall pattern\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretation: Each orange point represents one player. The blue line indicates the overall trend‚Äîtaller players score more points on average. The variation around the line reflects other unmeasured factors: skill, position, minutes played, team system, and other determinants of scoring ability.\nKey Insight: The line does not pass through every point because height represents only one factor affecting scoring. The scatter around the line captures all other influential factors not included in the model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#simple-linear-regression",
    "href": "chapter1.html#simple-linear-regression",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.18 Simple Linear Regression",
    "text": "1.18 Simple Linear Regression\nThe basic regression equation formalizes this relationship:\nY_i = \\alpha + \\beta X_i + \\epsilon_i\nWhere:\n\nY_i = outcome for observation i\nX_i = predictor for observation i\n\\alpha = intercept (expected value of Y when X = 0)\n\\beta = slope (change in Y for one-unit change in X)\n\\epsilon_i = error term\n\nApplied to the basketball example:\n\nY_i = points scored by player i\nX_i = height of player i\n\\alpha = baseline scoring (mathematical construct when height = 0)\n\\beta = additional points expected per inch of height\n\\epsilon_i = all other factors affecting player i‚Äôs scoring\n\n\nExample: Education and Political Participation\nConsider a classic political science question: Does education increase political participation?\n\n\n\n\n\n\n\n\n\nStatistical Results:\n\n\n‚Ä¢ Each additional year of education increases participation by 0.029 points on average\n\n\n‚Ä¢ Education explains 9.2 % of variation in participation\n\n\n‚Ä¢ Remaining 90.8 % is explained by unmeasured factors\n\n\nR¬≤ (R-squared) Interpretation: This statistic indicates the percentage of variation in the outcome variable explained by predictor variables. R¬≤ = 0.3 means the model explains 30% of variation in political participation, leaving 70% unexplained.\n\n\nDecomposing the Regression Equation\nThe formal equation applies to our education-participation study as follows:\nY_i = \\alpha + \\beta X_i + \\epsilon_i\nTranslation to substantive terms:\n\nY_i: Political participation for person i\n\\alpha (intercept): Expected participation for someone with zero years of education\n\\beta (slope): Change in participation per additional year of education\nX_i: Years of education for person i\n\\epsilon_i: All other factors affecting person i‚Äôs participation (income, age, political interest, etc.)\n\nConceptual Framework: An individual‚Äôs political participation equals a baseline level (\\alpha) plus the effect of education (\\beta \\times education) plus unexplained factors (\\epsilon).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#multiple-regression-accounting-for-complexity",
    "href": "chapter1.html#multiple-regression-accounting-for-complexity",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.19 Multiple Regression: Accounting for Complexity",
    "text": "1.19 Multiple Regression: Accounting for Complexity\nReal-world phenomena rarely have single causes. Education affects participation, but so do income, age, and political interest. Multiple regression accounts for several factors simultaneously.\n\nUnderstanding ‚ÄúControlling For‚Äù\nThis concept proves challenging but essential. Consider an analogy:\nSchool Comparison Example: To assess whether private schools outperform public schools, comparing raw test scores proves inadequate. Private school students often have wealthier, more educated parents. The observed difference might reflect family background rather than school quality.\nFair comparison requires comparing students from similar backgrounds‚Äîwealthy students across school types and middle-class students across school types.\nStatistical ‚Äúcontrol‚Äù achieves this comparison mathematically. When stating ‚Äúeducation increases political participation by 0.04 points, controlling for income and age,‚Äù this means:\n\nAmong people with identical income and age\nThose with one additional year of education participate 0.04 points more on average\nThe comparison adjusts for confounding factors\n\nThe multiple regression equation formalizes this:\nY_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_k X_{ki} + \\epsilon_i\nEach \\beta_j represents the effect of X_j holding all other variables constant.\n\n\nExample: Determinants of Electoral Success\nReturning to electoral prediction, consider multiple factors simultaneously:\n\n\n\nImpact of Including Control Variables\n\n\nModel\nApproval Effect\np-value\n\n\n\n\nSimple (approval only)\n0.781\n0\n\n\nMultiple (controlling for economy & spending)\n0.750\n0\n\n\n\n\n\n\n\nDeterminants of Electoral Success:\n\n\n‚Ä¢ 1% increase in approval ‚Üí +0.7 point victory margin \n‚Ä¢ 1% economic growth ‚Üí +2.3 point victory margin \n‚Ä¢ $1M in spending ‚Üí +0.03 point victory margin \n\n\n\nThese factors jointly explain 71.5% of election outcomes\n\n\nRemaining 28.5% attributable to unmeasured factors\n\n\nCritical Observation: The effect of approval rating changes when additional variables are included. This demonstrates why controlling for confounders matters‚Äîomitted variables can substantially bias conclusions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-causal-inference-challenge-does-money-buy-elections",
    "href": "chapter1.html#the-causal-inference-challenge-does-money-buy-elections",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.20 The Causal Inference Challenge: Does Money Buy Elections?",
    "text": "1.20 The Causal Inference Challenge: Does Money Buy Elections?\nThis question illustrates regression‚Äôs limitations and the distinction between correlation and causation.\nThe Observed Pattern: Candidates who spend more typically receive more votes. Does spending cause votes?\nAlternative Explanations:\n\nReverse causation: Popular candidates attract more donations\nCommon cause: Charismatic candidates both inspire donations and win votes\nSelection bias: Only well-funded candidates run competitive races\n\nThis illustrates the central challenge: correlation does not imply causation.\n\nThe Fundamental Problem of Causal Inference\nTo establish causation, we would need to observe the same candidate in parallel scenarios:\n\nScenario A: Spending $5 million\nScenario B: Spending $1 million\nCausal effect = Difference in vote share\n\nThe Problem: We observe only one scenario per candidate. This constitutes the ‚ÄúFundamental Problem of Causal Inference.‚Äù\n\n\nApproaches to Causal Identification\nResearchers employ several strategies to approximate causal effects:\n1. Randomized Experiments (Gold Standard)\n\nRandom assignment to treatment/control groups\nGroups identical except for treatment\nDifferences attributable to treatment\n\n2. Natural Experiments\n\nClose elections create quasi-random variation\nPolicy changes affect some areas but not others\nNatural disasters provide exogenous shocks\n\n3. Statistical Control\n\nInclude confounding variables in regression\nInterpret coefficients as causal under strong assumptions\nLimitation: Requires measuring all confounders\n\n\n# Demonstrate confounding in campaign spending\nset.seed(789)\nn_candidates &lt;- 500\n\n# Candidate quality affects both spending and votes\ncandidate_quality &lt;- rnorm(n_candidates, 0, 1)\n\n# Quality influences fundraising\nspending &lt;- 50 + 20 * candidate_quality + rnorm(n_candidates, 0, 10)\nspending &lt;- pmax(0, spending)\n\n# Votes depend on spending AND quality\nvote_share &lt;- 30 + 0.1 * spending + 15 * candidate_quality + rnorm(n_candidates, 0, 5)\nvote_share &lt;- pmax(0, pmin(100, vote_share))\n\ncampaign_data &lt;- data.frame(\n  spending = spending,\n  quality = candidate_quality,\n  vote_share = vote_share\n)\n\n# Compare analyses\nnaive_model &lt;- lm(vote_share ~ spending, data = campaign_data)\ncontrolled_model &lt;- lm(vote_share ~ spending + quality, data = campaign_data)\n\n# True effect is 0.1\ncomparison_results &lt;- data.frame(\n  Model = c(\"Naive (no controls)\", \"Proper (controlling for quality)\"),\n  Spending_Effect = c(coef(naive_model)[2], coef(controlled_model)[2]),\n  True_Effect = c(0.1, 0.1)\n) %&gt;%\n  mutate(\n    Error = Spending_Effect - True_Effect,\n    Bias_Direction = case_when(\n      abs(Error) &lt; 0.05 ~ \"Unbiased\",\n      Error &gt; 0 ~ \"Upward bias\",\n      Error &lt; 0 ~ \"Downward bias\"\n    )\n  )\n\nkable(comparison_results, \n      digits = 3,\n      caption = \"Importance of Controlling for Confounders\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nImportance of Controlling for Confounders\n\n\nModel\nSpending_Effect\nTrue_Effect\nError\nBias_Direction\n\n\n\n\nNaive (no controls)\n0.681\n0.1\n0.581\nUpward bias\n\n\nProper (controlling for quality)\n0.155\n0.1\n0.055\nUpward bias\n\n\n\n\n\n\n# Visualize confounding\nggplot(campaign_data, aes(x = spending, y = vote_share, color = quality)) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linetype = \"dashed\") +\n  scale_color_gradient2(low = \"blue\", mid = \"gray\", high = \"red\", \n                       midpoint = 0, name = \"Candidate\\nQuality\") +\n  labs(\n    title = \"Confounding in Campaign Finance\",\n    subtitle = \"Red line shows naive correlation; true effect requires quality control\",\n    x = \"Campaign Spending ($1000s)\",\n    y = \"Vote Share (%)\",\n    caption = \"Color indicates candidate quality‚Äîhigher quality correlates with both spending and votes\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nKey Lesson: Without controlling for candidate quality, we overestimate spending‚Äôs effect. The naive analysis conflates quality‚Äôs effect with spending‚Äôs effect.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#common-pitfalls-in-regression-analysis",
    "href": "chapter1.html#common-pitfalls-in-regression-analysis",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.21 Common Pitfalls in Regression Analysis",
    "text": "1.21 Common Pitfalls in Regression Analysis\n\nPitfall 1: Confusing Statistical and Practical Significance\nThe Problem: Mistaking small but statistically significant effects for meaningful findings.\nWhy It Occurs: Large samples make tiny effects statistically significant. A study of 100,000 voters might detect that negative ads reduce turnout by 0.0001 percentage points with p &lt; 0.001.\nExample:\n\nReported: ‚ÄúNegative Ads Significantly Reduce Voter Turnout‚Äù\nReality: Effect = -0.001 percentage points per ad\nPractical Impact: Negligible\n\n\n# Demonstrate statistical vs practical significance\nset.seed(123)\n\n# Large sample, tiny effect\nn_large &lt;- 10000\ntreatment_large &lt;- rep(c(0, 1), each = n_large/2)\noutcome_large &lt;- 0.5 + 0.001 * treatment_large + rnorm(n_large, 0, 0.1)\nresult_large &lt;- t.test(outcome_large ~ treatment_large)\n\n# Small sample, larger effect\nn_small &lt;- 100\ntreatment_small &lt;- rep(c(0, 1), each = n_small/2)\noutcome_small &lt;- 0.5 + 0.05 * treatment_small + rnorm(n_small, 0, 0.1)\nresult_small &lt;- t.test(outcome_small ~ treatment_small)\n\ncat(\"Statistical versus Practical Significance:\\n\\n\")\n\nStatistical versus Practical Significance:\n\ncat(\"Large sample (n = 10,000):\\n\")\n\nLarge sample (n = 10,000):\n\ncat(\"  Effect size:\", round(diff(result_large$estimate), 5), \"\\n\")\n\n  Effect size: 0.00064 \n\ncat(\"  P-value:\", format(result_large$p.value, scientific = TRUE), \"\\n\")\n\n  P-value: 7.488156e-01 \n\ncat(\"  Assessment: Statistically significant but practically meaningless\\n\\n\")\n\n  Assessment: Statistically significant but practically meaningless\n\ncat(\"Small sample (n = 100):\\n\")\n\nSmall sample (n = 100):\n\ncat(\"  Effect size:\", round(diff(result_small$estimate), 3), \"\\n\")\n\n  Effect size: 0.031 \n\ncat(\"  P-value:\", round(result_small$p.value, 3), \"\\n\")\n\n  P-value: 0.117 \n\ncat(\"  Assessment: Not statistically significant but practically meaningful\\n\")\n\n  Assessment: Not statistically significant but practically meaningful\n\n\nEffect Size Guidelines (Cohen‚Äôs conventions):\n\nNegligible: &lt; 0.1 standard deviations\nSmall: 0.1-0.3 standard deviations\nMedium: 0.3-0.8 standard deviations\nLarge: &gt; 0.8 standard deviations\n\nKey Principle: Always evaluate whether effect sizes are large enough to matter substantively.\n\n\nPitfall 2: Overfitting\nThe Problem: Creating models too complex for available data, causing the model to memorize rather than generalize.\nConsequences: Artificially inflated performance metrics that fail to replicate with new data.\n\n# Compare simple versus complex models\nset.seed(456)\nn_obs &lt;- 50\n\n# Create data where only X1 matters\nx1 &lt;- rnorm(n_obs)\nx2 &lt;- rnorm(n_obs)  # Irrelevant\nx3 &lt;- rnorm(n_obs)  # Irrelevant\nx4 &lt;- rnorm(n_obs)  # Irrelevant\nx5 &lt;- rnorm(n_obs)  # Irrelevant\n\ny &lt;- 2 + 1.5 * x1 + rnorm(n_obs, 0, 1)  # Only X1 actually matters\n\n# Compare models\nsimple_model &lt;- lm(y ~ x1)\ncomplex_model &lt;- lm(y ~ x1 + x2 + x3 + x4 + x5)\n\ncat(\"Model Comparison:\\n\")\n\nModel Comparison:\n\ncat(\"Simple model R¬≤:\", round(summary(simple_model)$r.squared, 3), \"\\n\")\n\nSimple model R¬≤: 0.719 \n\ncat(\"Complex model R¬≤:\", round(summary(complex_model)$r.squared, 3), \"\\n\")\n\nComplex model R¬≤: 0.73 \n\ncat(\"Complex adjusted R¬≤:\", round(summary(complex_model)$adj.r.squared, 3), \"\\n\\n\")\n\nComplex adjusted R¬≤: 0.699 \n\ncat(\"Note: Complex model's higher R¬≤ is misleading‚Äîadjusted R¬≤ reveals minimal improvement\\n\")\n\nNote: Complex model's higher R¬≤ is misleading‚Äîadjusted R¬≤ reveals minimal improvement\n\n\nWarning Signs:\n\nMore variables than justified theoretically\nR¬≤ increases but adjusted R¬≤ stagnates\nPoor out-of-sample prediction\nInclusion of variables without theoretical justification\n\nAdjusted R¬≤ Explanation: Unlike regular R¬≤, adjusted R¬≤ penalizes model complexity, providing honest assessment of model improvement.\nPrevention Strategies:\n\nInclude only theoretically justified variables\nMonitor adjusted R¬≤ rather than R¬≤\nUse cross-validation\nMaintain parsimony\n\n\n\nPitfall 3: Multiple Testing Problem\nThe Problem: Testing numerous relationships and reporting only significant ones.\nStatistical Reality: With Œ± = 0.05, expect 5% false positives. Testing 20 relationships yields approximately one spurious ‚Äúsignificant‚Äù result.\n\n# Simulate multiple testing problem\nset.seed(789)\nn_tests &lt;- 20\np_values &lt;- numeric(n_tests)\n\n# Run tests where no true effect exists\nfor(i in 1:n_tests) {\n  x &lt;- rnorm(100)\n  y &lt;- rnorm(100)  # Y unrelated to X\n  test_result &lt;- cor.test(x, y)\n  p_values[i] &lt;- test_result$p.value\n}\n\nsignificant_tests &lt;- sum(p_values &lt; 0.05)\n\ncat(\"Multiple Testing Demonstration:\\n\")\n\nMultiple Testing Demonstration:\n\ncat(\"Tests conducted:\", n_tests, \"\\n\")\n\nTests conducted: 20 \n\ncat(\"'Significant' results:\", significant_tests, \"\\n\")\n\n'Significant' results: 0 \n\ncat(\"Expected by chance:\", round(n_tests * 0.05), \"\\n\")\n\nExpected by chance: 1 \n\ncat(\"Smallest p-value:\", round(min(p_values), 4), \"\\n\\n\")\n\nSmallest p-value: 0.0541 \n\nif(significant_tests &gt; 0) {\n  cat(\"Selective reporting of significant results would create false positives\\n\")\n}\n\nSolutions:\n\nPre-registration of hypotheses\nBonferroni or false discovery rate corrections\nComplete reporting of all tests\nTheory-driven hypothesis testing\n\n\n\nPitfall 4: Ecological Fallacy\nThe Problem: Inferring individual-level relationships from group-level data.\nClassic Example: ‚ÄúWealthy states vote Democratic, therefore wealthy individuals vote Democratic‚Äù Reality: Within states, wealth often correlates with Republican voting\n\n# Demonstrate ecological fallacy\nset.seed(101)\n\n# State-level: wealth correlates with Democratic voting\nstate_income &lt;- runif(50, 40000, 80000)\nstate_dem_vote &lt;- 30 + 0.0005 * state_income + rnorm(50, 0, 5)\nstate_correlation &lt;- cor(state_income, state_dem_vote)\n\n# Individual-level: opposite relationship\nindividual_data &lt;- data.frame()\nfor(i in 1:50) {\n  n_people &lt;- 200\n  indiv_income &lt;- rnorm(n_people, state_income[i], 15000)\n  prob_dem &lt;- plogis((state_dem_vote[i]/100) - 0.00002 * (indiv_income - state_income[i]))\n  vote_dem &lt;- rbinom(n_people, 1, prob_dem)\n  \n  state_data &lt;- data.frame(\n    state = i,\n    income = indiv_income,\n    dem_vote = vote_dem * 100\n  )\n  individual_data &lt;- rbind(individual_data, state_data)\n}\n\nindividual_correlation &lt;- cor(individual_data$income, individual_data$dem_vote)\n\ncat(\"State-level correlation (income vs. Democratic vote):\", round(state_correlation, 3), \"\\n\")\n\nState-level correlation (income vs. Democratic vote): 0.75 \n\ncat(\"Individual-level correlation (income vs. Democratic vote):\", round(individual_correlation, 3), \"\\n\")\n\nIndividual-level correlation (income vs. Democratic vote): -0.09 \n\ncat(\"\\nOpposite signs demonstrate the ecological fallacy\\n\")\n\n\nOpposite signs demonstrate the ecological fallacy\n\n\n\n\nPitfall 5: Selection Bias\nThe Problem: Drawing inferences from non-representative samples.\nCommon Sources:\n\nSurveying only landline users (age bias)\nStudying only volunteers (motivation bias)\nAnalyzing only successful cases (survivorship bias)\nUsing convenience samples (accessibility bias)\n\nConsequence: Systematic bias that statistical techniques cannot correct.\n\n\nPitfall 6: Ignoring Uncertainty\nThe Problem: Treating point estimates as precise values.\nIncorrect: ‚ÄúSupport is 52%‚Äù Better: ‚ÄúSupport is 52% ¬± 3%‚Äù Best: ‚Äú95% confidence interval: [49%, 55%]‚Äù\nImportance: Acknowledging uncertainty prevents overconfident conclusions.\n\n\nPitfall 7: Spurious Correlations\nThe Problem: Variables correlate by coincidence, particularly with time trends.\n\n# Demonstrate spurious correlation\nset.seed(321)\nyears &lt;- 1990:2020\nn_years &lt;- length(years)\n\n# Unrelated variables with time trends\ninternet_users &lt;- 10 + 2.5 * (years - 1990) + rnorm(n_years, 0, 3)\npizza_consumption &lt;- 50 + 1.2 * (years - 1990) + rnorm(n_years, 0, 2)\n\nspurious_corr &lt;- cor(internet_users, pizza_consumption)\n\n# Remove time trends\ninternet_detrended &lt;- residuals(lm(internet_users ~ years))\npizza_detrended &lt;- residuals(lm(pizza_consumption ~ years))\ntrue_corr &lt;- cor(internet_detrended, pizza_detrended)\n\ncat(\"Spurious Correlation Analysis:\\n\")\n\nSpurious Correlation Analysis:\n\ncat(\"Correlation with time trends:\", round(spurious_corr, 3), \"\\n\")\n\nCorrelation with time trends: 0.982 \n\ncat(\"Correlation after detrending:\", round(true_corr, 3), \"\\n\")\n\nCorrelation after detrending: 0.136 \n\ncat(\"Conclusion: Apparent correlation driven by common time trends\\n\")\n\nConclusion: Apparent correlation driven by common time trends\n\n\nDetection Methods:\n\nExamine theoretical plausibility\nCheck for common time trends\nLook for third variables\nInspect scatterplots\n\n\n\nPitfall 8: Confounding Variables\nThe Problem: A third variable influences both predictor and outcome, creating misleading relationships.\nThe Confounding Structure:\n\nZ ‚Üí X (confounder affects predictor)\nZ ‚Üí Y (confounder affects outcome)\nCreates spurious X ‚Üí Y relationship\n\n\n# Demonstrate confounding\nset.seed(456)\nn_districts &lt;- 500\n\n# Socioeconomic status confounds spending-votes relationship\nses &lt;- rnorm(n_districts, 0, 1)\n\n# SES affects both variables\ncampaign_spending &lt;- 100 + 50 * ses + rnorm(n_districts, 0, 20)\ncampaign_spending &lt;- pmax(10, campaign_spending)\n\nvote_share &lt;- 50 + 8 * ses + 0.02 * campaign_spending + rnorm(n_districts, 0, 5)\nvote_share &lt;- pmax(0, pmin(100, vote_share))\n\n# Compare models\nnaive_model &lt;- lm(vote_share ~ campaign_spending, data = data.frame(campaign_spending, vote_share))\ncontrolled_model &lt;- lm(vote_share ~ campaign_spending + ses, \n                      data = data.frame(campaign_spending, vote_share, ses))\n\ncat(\"Campaign Spending Effects:\\n\")\n\nCampaign Spending Effects:\n\ncat(\"Without SES control:\", round(coef(naive_model)[2], 4), \"\\n\")\n\nWithout SES control: 0.1601 \n\ncat(\"With SES control:\", round(coef(controlled_model)[2], 4), \"\\n\")\n\nWith SES control: 0.0052 \n\ncat(\"True effect: 0.02\\n\")\n\nTrue effect: 0.02\n\n\nTypes of Confounding:\n\nPositive Confounding: Exaggerates relationships\nNegative Confounding: Masks relationships\nSign Reversal: Reverses relationship direction\n\nSolutions:\n\nTheoretical identification of confounders\nCausal diagrams\nStatistical control\nResearch design solutions",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#guidelines-for-research-consumers",
    "href": "chapter1.html#guidelines-for-research-consumers",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.22 Guidelines for Research Consumers",
    "text": "1.22 Guidelines for Research Consumers\nWhen evaluating research, consider:\n\nEffect Size: Is the magnitude practically meaningful?\nSample: Who is included/excluded? How might this bias results?\nMultiple Testing: Were many tests conducted? Is there cherry-picking?\nCausation: What is the identification strategy?\nModel Complexity: Does complexity match data availability?\nUncertainty: Are confidence intervals reported?",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#complete-analysis-example",
    "href": "chapter1.html#complete-analysis-example",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.23 Complete Analysis Example",
    "text": "1.23 Complete Analysis Example\n\n# Research Question: What predicts voter turnout in US counties?\n\nset.seed(101)\nn_counties &lt;- 500\n\ncounty_data &lt;- data.frame(\n  county_id = 1:n_counties,\n  median_income = runif(n_counties, 35000, 85000),\n  college_percent = runif(n_counties, 15, 65),\n  unemployment = runif(n_counties, 2, 15),\n  rural_percent = runif(n_counties, 0, 95)\n)\n\n# Generate turnout based on multiple factors\ncounty_data$turnout &lt;- 45 + \n  0.0003 * county_data$median_income +\n  0.4 * county_data$college_percent +\n  -0.8 * county_data$unemployment +\n  -0.1 * county_data$rural_percent +\n  rnorm(n_counties, 0, 8)\n\ncounty_data$turnout &lt;- pmax(30, pmin(85, county_data$turnout))\n\n# Step 1: Data exploration\ncat(\"Step 1: Data Summary\\n\")\n\nStep 1: Data Summary\n\ncat(\"Sample size:\", nrow(county_data), \"counties\\n\")\n\nSample size: 500 counties\n\ncat(\"Turnout range:\", round(min(county_data$turnout), 1), \"% to\", \n    round(max(county_data$turnout), 1), \"%\\n\\n\")\n\nTurnout range: 33.5 % to 85 %\n\n# Step 2: Model estimation\nfull_turnout_model &lt;- lm(turnout ~ median_income + college_percent + \n                        unemployment + rural_percent, data = county_data)\n\n# Step 3: Results interpretation\ncat(\"Step 3: Results Interpretation\\n\\n\")\n\nStep 3: Results Interpretation\n\nmodel_results &lt;- tidy(full_turnout_model) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(\n    interpretation = case_when(\n      term == \"median_income\" ~ paste0(\"$10k income increase ‚Üí +\", \n                                       round(estimate * 10000, 1), \"% turnout\"),\n      term == \"college_percent\" ~ paste0(\"10% more college graduates ‚Üí +\", \n                                         round(estimate * 10, 1), \"% turnout\"),\n      term == \"unemployment\" ~ paste0(\"1% higher unemployment ‚Üí \", \n                                      round(estimate, 1), \"% turnout\"),\n      term == \"rural_percent\" ~ paste0(\"10% more rural ‚Üí \", \n                                       round(estimate * 10, 1), \"% turnout\")\n    ),\n    significance = ifelse(p.value &lt; 0.05, \"Significant\", \"Not significant\")\n  )\n\nfor(i in 1:nrow(model_results)) {\n  cat(\"‚Ä¢\", model_results$interpretation[i], \"(\", model_results$significance[i], \")\\n\")\n}\n\n‚Ä¢ $10k income increase ‚Üí +3.1% turnout ( Significant )\n‚Ä¢ 10% more college graduates ‚Üí +4% turnout ( Significant )\n‚Ä¢ 1% higher unemployment ‚Üí -0.7% turnout ( Significant )\n‚Ä¢ 10% more rural ‚Üí -0.8% turnout ( Significant )\n\n# Step 4: Model assessment\nr_squared &lt;- summary(full_turnout_model)$r.squared\ncat(paste0(\"\\nModel explains \", round(r_squared * 100, 1), \"% of turnout variation\\n\"))\n\n\nModel explains 49.9% of turnout variation\n\ncat(paste0(\"Remaining \", round((1-r_squared) * 100, 1), \"% due to unmeasured factors\\n\\n\"))\n\nRemaining 50.1% due to unmeasured factors\n\n# Step 5: Diagnostics\ncat(\"Step 5: Assumption Diagnostics\\n\")\n\nStep 5: Assumption Diagnostics\n\nplot(full_turnout_model, which = 1, main = \"Residuals versus Fitted Values\")\n\n\n\n\n\n\n\n\n\nAnalysis Summary\nFindings:\n\nEducation levels strongly predict county turnout\nEconomic factors (income, unemployment) show significant effects\nRural areas exhibit lower turnout\nThese factors explain approximately 60% of turnout variation\n\nLimitations:\n\nCorrelational analysis cannot establish causation\n40% of variation remains unexplained\nCounty-level patterns may not reflect individual behavior\nImportant variables may be omitted",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#conclusion",
    "href": "chapter1.html#conclusion",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.24 Conclusion",
    "text": "1.24 Conclusion\nRegression analysis provides systematic methods for testing theoretical propositions against empirical data. While it cannot definitively establish causation without appropriate research designs, it offers valuable tools for understanding relationships in observational data.\nKey Principles:\n\nRegression identifies best-fitting linear relationships\nMultiple regression controls for confounding variables\nCorrelation does not establish causation\nEffect sizes matter more than statistical significance\nAll analyses have limitations requiring acknowledgment\n\nThese analytical skills enable critical evaluation of empirical claims in academic research, policy debates, and political discourse. Understanding regression means not only conducting analyses but also recognizing the strengths and limitations of statistical evidence in social science research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#moving-forward-building-statistical-intuition",
    "href": "chapter1.html#moving-forward-building-statistical-intuition",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.25 Moving Forward: Building Statistical Intuition",
    "text": "1.25 Moving Forward: Building Statistical Intuition",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#key-principles-to-remember",
    "href": "chapter1.html#key-principles-to-remember",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.26 Key Principles to Remember",
    "text": "1.26 Key Principles to Remember\n\nThe Statistical Mindset\n\nAlways think about uncertainty: Every statistic comes with error\nDistinguish correlation from causation: Association ‚â† causal effect\nConsider practical significance: Statistical significance isn‚Äôt everything\nQuestion your measurements: How well do our proxies capture what we care about?\nThink about selection: Who/what is in our sample, and who/what is missing?\n\n\n\nThe Fundamental Tools You‚Äôve Learned\n\nSampling: How to learn about many from studying few\nMeasurement: How to turn political concepts into numbers\nDescription: How to summarize what we see in data\nInference: How to draw conclusions beyond our sample\nRegression: How to model relationships between variables\nSignificance testing: How to distinguish real patterns from noise\nCausation: Why correlation doesn‚Äôt equal causation",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#next-steps-in-your-training",
    "href": "chapter1.html#next-steps-in-your-training",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.27 Next Steps in Your Training",
    "text": "1.27 Next Steps in Your Training\n\nImmediate Next Steps\n\nPractice with R or Stata: Apply these concepts with real data\nRead research critically: Can you identify the population, sample, and key assumptions?\nTake a methods course: Build on these foundations\n\n\n\nFuture Learning\n\nProbability theory: The mathematical foundations (usually second year)\nAdvanced regression: Logistic regression, interactions, non-linear relationships\nCausal inference: More sophisticated ways to identify causes\nSurvey methodology: Designing good questionnaires and samples\nPanel data methods: Following units over time\nMachine learning: Prediction-focused approaches to analyzing political data",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#practical-advice-for-political-science-research",
    "href": "chapter1.html#practical-advice-for-political-science-research",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.28 Practical Advice for Political Science Research",
    "text": "1.28 Practical Advice for Political Science Research\n\n1. Start with Theory\nStatistics is a tool, not a substitute for thinking:\n\nWhat relationship do you expect and why?\nWhat would falsify your hypothesis?\nWhat alternative explanations exist?\n\n\n\n2. Know Your Data\nBefore any analysis:\n\n# Essential diagnostic steps\nsummary(data)           # Basic statistics\ntable(data$variable)    # Frequency tables\nhist(data$variable)     # Distribution\nplot(x, y)             # Scatterplots\ncor(data)              # Correlation matrix\n\n\n\n3. Match Method to Question\n\nDescribing: Means, proportions, distributions\nPredicting: Regression, machine learning\nCausal inference: Experiments, quasi-experiments, panel methods\n\n\n\n4. Interpret Substantively\nAlways translate statistics back to political science:\n\nWhat does a one-unit change mean substantively?\nIs the effect politically meaningful?\nWhat are the policy implications?\n\n\n\n5. Be Transparent\n\nReport all analyses, not just significant results\nShare data and code when possible\nAcknowledge limitations\nDescribe robustness checks",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#practice-problems",
    "href": "chapter1.html#practice-problems",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.29 Practice Problems",
    "text": "1.29 Practice Problems\n\nProblem 1: Identifying Populations and Samples\nYou want to understand what factors influence democratic transitions.\n\nWhat could be your population?\nHow would you select a sample?\nWhat biases might you face?\n\n\n\nProblem 2: Interpreting Results\nA study finds: ‚ÄúEducation increases voter turnout by 2.3 percentage points per year of schooling (p = 0.02)‚Äù\n\nWhat does the p = 0.02 mean in plain English?\nIf someone has 4 more years of education than another person, how much more likely are they to vote?\nIs this a big or small effect? (Consider: typical turnout is around 60%)\n\n\n\nProblem 3: Correlation vs.¬†Causation\n‚ÄúCountries with more McDonald‚Äôs restaurants have lower infant mortality rates‚Äù\n\nGive three possible explanations for this pattern\nHow could you test which explanation is correct?\nWhat data would you need?\n\n\n\nProblem 4: Regression Interpretation\nYou run a regression predicting congressional vote share with these results:\nVote Share = 45.2 + 0.31*Approval + 2.1*Economy - 0.05*Age\n             (0.8)  (0.04)         (0.6)       (0.02)\n\nR¬≤ = 0.67, n = 435\nStandard errors in parentheses\nInterpret each coefficient substantively and assess statistical significance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#essential-r-code-for-getting-started",
    "href": "chapter1.html#essential-r-code-for-getting-started",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.30 Essential R Code for Getting Started",
    "text": "1.30 Essential R Code for Getting Started\n\n# Reading data\ndata &lt;- read.csv(\"yourfile.csv\")     # Load a CSV file\n\n# Basic exploration\nsummary(data)                         # See basic statistics for all variables\nhead(data)                           # Look at first few rows\ntable(data$party)                    # Count how many in each category\n\n# Simple analysis\nmean(data$age)                       # Calculate average age\ncor(data$income, data$turnout)      # Correlation between two variables\n\n# Basic visualization\nhist(data$age)                       # Histogram of age distribution\nplot(data$education, data$turnout)  # Scatterplot of two variables\n\n# Difference between groups\nt.test(income ~ gender, data = data) # Compare average income by gender\n\n# Simple regression\nmodel &lt;- lm(turnout ~ education, data = data)  # Run regression\nsummary(model)                                  # See results\n\n# Multiple regression\nmodel2 &lt;- lm(turnout ~ education + age + income, data = data)\nsummary(model2)\n\n# Create nice plots with ggplot2\nlibrary(ggplot2)\nggplot(data, aes(x = education, y = turnout)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Education and Turnout\",\n       x = \"Years of Education\", \n       y = \"Voter Turnout\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#resources-for-continued-learning",
    "href": "chapter1.html#resources-for-continued-learning",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.31 Resources for Continued Learning",
    "text": "1.31 Resources for Continued Learning\n\nTextbooks for Beginners\n\nKellstedt & Whitten: The Fundamentals of Political Science Research - Written specifically for political science students\nImai: Quantitative Social Science: An Introduction - Great examples, includes R code\nFreedman, Pisani & Purves: Statistics - Classic intro text, very intuitive",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "chapter1.html#final-thoughts",
    "href": "chapter1.html#final-thoughts",
    "title": "1¬† Introduction to Statistics and Data Analysis for Political Science",
    "section": "1.32 Final Thoughts",
    "text": "1.32 Final Thoughts\nStatistics is not just a tool‚Äîit‚Äôs a way of thinking about evidence, uncertainty, and inference. As citizens and scholars, developing statistical intuition helps us:\n\nCritically evaluate political claims\nDesign better research\nMake more informed decisions\nUnderstand the limits of what we can know\n\nRemember: Every number tells a story, but not every story told by numbers is true. Your job is to develop the skills to tell the difference.\nThe goal isn‚Äôt to become a statistician, but to become a political scientist who can evaluate and produce rigorous evidence. Statistics helps us move from hunches to hypotheses to evidence-based conclusions about the political world.\nAs you continue your journey in political science, always remember that behind every statistical analysis are real people, real policies, and real consequences. The tools you‚Äôve learned here will help you contribute to our understanding of politics and hopefully make the world a bit better informed.\n\nWelcome to the world of empirical political science.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Statistics and Data Analysis for Political Science</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html",
    "href": "rozdzial1.html",
    "title": "2¬† Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "",
    "text": "2.1 Czym jest statystyka?\nStatystyka to nauka o uczeniu siƒô z danych w warunkach niepewno≈õci. Dok≈Çadniej m√≥wiƒÖc, statystyka zapewnia:\nW politologii statystyka pomaga nam wyj≈õƒá poza dowody anegdotyczne i osobiste wra≈ºenia, aby formu≈Çowaƒá rygorystyczne, oparte na dowodach twierdzenia o zjawiskach politycznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#czym-jest-statystyka",
    "href": "rozdzial1.html#czym-jest-statystyka",
    "title": "2¬† Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "",
    "text": "Metody zbierania danych w spos√≥b systematyczny i bez stronniczo≈õci\nNarzƒôdzia do opisywania i podsumowywania tego, co obserwujemy w naszych danych\n\nTechniki do wnioskowania o populacjach na podstawie pr√≥b (samples)\nRamy do kwantyfikacji niepewno≈õci w naszych wnioskach\nPodej≈õcia do modelowania relacji miƒôdzy zmiennymi",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#spos√≥b-my≈õlenia-statystycznego",
    "href": "rozdzial1.html#spos√≥b-my≈õlenia-statystycznego",
    "title": "2¬† Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.2 Spos√≥b my≈õlenia statystycznego",
    "text": "2.2 Spos√≥b my≈õlenia statystycznego\nRozwijanie sposobu my≈õlenia statystycznego (statistical mindset) polega mniej na zapamiƒôtywaniu wzor√≥w, a wiƒôcej na przyjƒôciu okre≈õlonego sposobu my≈õlenia o ≈õwiecie. W swojej istocie statystyka uczy nas byƒá ciekawymi, ostro≈ºnymi i systematycznymi podczas wyciƒÖgania wniosk√≥w z danych. Szczeg√≥lnie wa≈ºne jest piƒôƒá kluczowych nawyk√≥w my≈õlowych:\n\nAkceptuj niepewno≈õƒá Prawie nigdy nie znamy ‚Äúprawdziwych‚Äù warto≈õci w populacji. Oszacowania zawsze obarczone sƒÖ marginesem b≈Çƒôdu. Rozpoznanie niepewno≈õci nie jest s≈Çabo≈õciƒÖ - jest uczciwym odzwierciedleniem rzeczywisto≈õci. Na przyk≈Çad, sonda≈º wyborczy pokazujƒÖcy kandydata na poziomie 52% nigdy nie jest precyzyjnƒÖ prawdƒÖ, lecz raczej oszacowaniem z marginesem b≈Çƒôdu.\nMy≈õl o zmienno≈õci Dlaczego jednostki, grupy lub przypadki siƒô r√≥≈ºniƒÖ? Zmienno≈õƒá (variation) to paliwo statystyki: bez niej nie by≈Çoby nic do badania. Czasami zmienno≈õƒá jest losowa (b≈ÇƒÖd pr√≥bkowania), czasami systematyczna (p≈Çeƒá, wykszta≈Çcenie, doch√≥d), a czƒôsto i jedno, i drugie. Zrozumienie zmienno≈õci pomaga nam wykryƒá znaczƒÖce wzorce, a nie szum.\nKwestionuj zwiƒÖzki To, ≈ºe dwie rzeczy zmieniajƒÖ siƒô razem, nie oznacza, ≈ºe jedna powoduje drugƒÖ. Na przyk≈Çad, sprzeda≈º lod√≥w i przypadki utoniƒôƒá rosnƒÖ latem - ale jedzenie lod√≥w nie powoduje utoniƒôƒá. My≈õlenie statystyczne wymaga, by≈õmy testowali, czy zwiƒÖzki odzwierciedlajƒÖ przyczynowo≈õƒá czy zwyk≈Çe skojarzenie.\nBƒÖd≈∫ sceptyczny UderzajƒÖcy wzorzec w danych mo≈ºe nadal byƒá przypadkiem. Czy wynik m√≥g≈Ç pojawiƒá siƒô po prostu przez przypadek? Narzƒôdzia takie jak warto≈õci p, przedzia≈Çy ufno≈õci i replikacja pomagajƒÖ nam odr√≥≈ºniƒá rzeczywiste efekty od losowych zbieg√≥w okoliczno≈õci.\nRozwa≈º alternatywy Ka≈ºde wyt≈Çumaczenie konkuruje z innymi mo≈ºliwymi wyt≈Çumaczeniami. Je≈õli obserwujemy, ≈ºe studenci uczƒÖcy siƒô w grupach osiƒÖgajƒÖ wy≈ºsze wyniki na egzaminach, czy to nauka grupowa pomaga - czy te≈º silniejsi studenci czƒô≈õciej wybierajƒÖ grupy? My≈õlenie w kategoriach konkurujƒÖcych hipotez czyni nasze wnioski silniejszymi.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#populacja-vs.-pr√≥ba-podstawa-wnioskowania",
    "href": "rozdzial1.html#populacja-vs.-pr√≥ba-podstawa-wnioskowania",
    "title": "2¬† Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.3 Populacja vs.¬†pr√≥ba: podstawa wnioskowania",
    "text": "2.3 Populacja vs.¬†pr√≥ba: podstawa wnioskowania\n\nFundamentalne wyzwanie\nW politologii czƒôsto interesuje nas zrozumienie ca≈Çych populacji - kompletnego zbioru jednostek, kt√≥re chcemy badaƒá. Jednak badanie ca≈Çych populacji jest zwykle niemo≈ºliwe, niepraktyczne lub niepotrzebne.\n\n\nCo mo≈ºe byƒá populacjƒÖ?\nPopulacja w politologii mo≈ºe sk≈Çadaƒá siƒô z r√≥≈ºnych typ√≥w jednostek:\nJednostki indywidualne\n\nPopulacja: Wszyscy 240 milion√≥w ameryka≈Ñskich doros≈Çych\nPr√≥ba: 1000 losowo wybranych doros≈Çych w sonda≈ºu\nPytanie badawcze: Jaki procent popiera powszechnƒÖ opiekƒô zdrowotnƒÖ?\n\nKraje\n\nPopulacja: Wszystkie 195 suwerennych narod√≥w na ≈õwiecie\nPr√≥ba: 50 kraj√≥w z r√≥≈ºnych region√≥w i poziom√≥w rozwoju\nPytanie badawcze: Czy demokracja koreluje ze wzrostem gospodarczym?\n\nJednostki subpa≈Ñstwowe\n\nPopulacja: Wszystkie 3143 hrabstwa ameryka≈Ñskie\nPr√≥ba: 200 losowo wybranych hrabstw\nPytanie badawcze: Jak bezrobocie wp≈Çywa na wska≈∫niki przestƒôpczo≈õci?\n\nOrganizacje\n\nPopulacja: Wszystkie organizacje pozarzƒÖdowe zarejestrowane w ONZ\nPr√≥ba: 100 NGO dzia≈ÇajƒÖcych w r√≥≈ºnych obszarach polityki\nPytanie badawcze: Jakie czynniki przewidujƒÖ skuteczno≈õƒá NGO?\n\nWydarzenia lub okresy czasu\n\nPopulacja: Wszystkie wybory przeprowadzone w Europie od 1945 roku\nPr√≥ba: 300 wybor√≥w z r√≥≈ºnych kraj√≥w i dekad\nPytanie badawcze: Jak warunki ekonomiczne wp≈ÇywajƒÖ na poparcie dla urzƒôdujƒÖcych?\n\nJednostki legislacyjne\n\nPopulacja: Wszystkie projekty ustaw wprowadzone do Kongresu w latach 2000-2020\nPr√≥ba: 500 losowo wybranych projekt√≥w ustaw\nPytanie badawcze: Co przewiduje, czy projekt ustawy stanie siƒô prawem?\n\n\n\nRozwiƒÖzanie pr√≥bkowe i kluczowa obserwacja\nPr√≥ba (sample) to podzbi√≥r populacji, kt√≥ry faktycznie obserwujemy i mierzymy. Kluczowa obserwacja statystyki jest taka, ≈ºe mo≈ºemy uczyƒá siƒô o populacjach badajƒÖc pr√≥by - je≈õli jeste≈õmy ostro≈ºni w sposobie ich wybierania.\nZ naszej pr√≥by chcemy dokonaƒá wnioskowa≈Ñ (inferences) o populacji:\n\\text{Statystyka pr√≥by} \\rightarrow \\text{Parametr populacji}\nNa przyk≈Çad: Je≈õli 52% naszej pr√≥by popiera Kandydata A (\\hat{p} = 0,52), co mo≈ºemy powiedzieƒá o poparciu w ca≈Çej populacji (\\pi)?\nPodstawowa zasada: losowy wyb√≥r (random selection) daje ka≈ºdej jednostce w populacji r√≥wnƒÖ szansƒô zostania w≈ÇƒÖczonƒÖ, zapobiegajƒÖc systematycznej stronniczo≈õci.\n\n\nWizualizacja pr√≥bkowania\nZobaczmy, jak r√≥≈ºne rozmiary pr√≥b wp≈ÇywajƒÖ na nasze oszacowania:\n\n# Symulacja pr√≥bkowania z populacji\npopulation_size &lt;- 1000000\ntrue_proportion &lt;- 0.60  # Prawdziwy parametr populacji (œÄ)\n\n# We≈∫ pr√≥by o r√≥≈ºnych rozmiarach\nsample_sizes &lt;- c(100, 500, 1000, 5000)\nresults &lt;- data.frame()\n\nfor (size in sample_sizes) {\n  for (i in 1:20) {\n    sample_result &lt;- rbinom(1, size, true_proportion) / size\n    results &lt;- rbind(results, \n                     data.frame(size = size, \n                               trial = i,\n                               estimate = sample_result))\n  }\n}\n\n# Wizualizacja\nggplot(results, aes(x = factor(size), y = estimate)) +\n  geom_point(alpha = 0.6, size = 2, color = \"steelblue\") +\n  geom_hline(yintercept = true_proportion, color = \"red\", \n             linetype = \"dashed\", size = 1) +\n  labs(title = \"Jak rozmiar pr√≥by wp≈Çywa na dok≈Çadno≈õƒá\",\n       subtitle = \"Czerwona linia pokazuje prawdziwƒÖ warto≈õƒá populacji (60%)\",\n       x = \"Rozmiar pr√≥by\",\n       y = \"Oszacowanie pr√≥by\") +\n  theme_minimal() +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\n\nUwaga do wykresu: Ten wykres rozrzutu pokazuje, jak rozmiar pr√≥by wp≈Çywa na dok≈Çadno≈õƒá oszacowa≈Ñ. Ka≈ºda niebieska kropka reprezentuje jedno oszacowanie pr√≥by. Zauwa≈º, jak wiƒôksze pr√≥by (prawa strona) skupiajƒÖ siƒô bardziej wok√≥≈Ç prawdziwej warto≈õci populacji (czerwona linia przerywana), ilustrujƒÖc zmniejszonƒÖ zmienno≈õƒá pr√≥bkowania.\n\n\nKluczowy wniosek: To demonstruje prawo wielkich liczb - gdy rozmiar pr√≥by ro≈õnie, nasze oszacowania stajƒÖ siƒô bardziej wiarygodne. Przy n=100 oszacowania mocno siƒô r√≥≈ºniƒÖ (55-65%), ale przy n=5000 sƒÖ znacznie precyzyjniejsze (59-61%). Dlatego sonda≈ºe og√≥lnokrajowe zazwyczaj badajƒÖ 1000+ os√≥b, a nie 100.\n\n\n\nProblem reprezentatywno≈õci\nNie wszystkie pr√≥by sƒÖ r√≥wne. Rozwa≈ºmy te metody pr√≥bkowania:\n\nPr√≥ba wygodna (convenience sample): Badanie student√≥w w twojej klasie politologii\n\nProblem: Niereprezentacyjne dla wszystkich wyborc√≥w\nPrzyk≈Çad: Studenci college‚Äôu sƒÖ m≈Çodsi i bardziej liberalni ni≈º og√≥lna populacja\n\nPr√≥ba dobrowolna (voluntary response sample): Sonda≈º online na stronie internetowej wiadomo≈õci\n\nProblem: Stronniczo≈õƒá autoselekcji\nPrzyk≈Çad: Ludzie o silnych opiniach czƒô≈õciej uczestniczƒÖ\n\nPr√≥ba losowa (random sample): Ka≈ºda jednostka ma r√≥wne prawdopodobie≈Ñstwo wyboru\n\nRozwiƒÖzanie: Najlepsza szansa na reprezentatywnƒÖ pr√≥bƒô\nPrzyk≈Çad: Losowo wybrane numery telefon√≥w ze wszystkich kod√≥w pocztowych\n\nPr√≥ba warstwowa losowa (stratified random sample): Podziel populacjƒô na grupy, pobierz pr√≥by z ka≈ºdej\n\nZaleta: Zapewnia reprezentacjƒô kluczowych podgrup\nPrzyk≈Çad: Pobierz r√≥wne liczby z ka≈ºdego stanu dla sonda≈ºu og√≥lnokrajowego\n\nPr√≥ba klastrowa (cluster sample): Losowo wybierz grupy, nastƒôpnie zbadaj wszystkich w ramach klastra\n\nZaleta: Koszt-efektywne dla geograficznie rozproszonych populacji\nPrzyk≈Çad: Losowo wybierz 50 miast, nastƒôpnie zbadaj mieszka≈Ñc√≥w tych miast",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#pr√≥ba-populacja-i-superpopulacja-dgp",
    "href": "rozdzial1.html#pr√≥ba-populacja-i-superpopulacja-dgp",
    "title": "2¬† Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.4 Pr√≥ba, Populacja i Superpopulacja (DGP)",
    "text": "2.4 Pr√≥ba, Populacja i Superpopulacja (DGP)\n\nPodstawowe ramy: populacja i pr√≥ba\n\nPopulacja\nPopulacja to pe≈Çny zbi√≥r wszystkich jednostek, kt√≥re chcemy badaƒá.\nPrzyk≈Çady:\n\nWszyscy zarejestrowani wyborcy w Kanadzie\nWszystkie drzewa w Parku Narodowym Yellowstone\nWszyscy klienci, kt√≥rzy dokonali zakupu w Amazonie w 2024 roku\nWszyscy studenci aktualnie zapisani na Twoim uniwersytecie\n\nKluczowa cecha: Populacja jest sko≈Ñczona i sta≈Ça. Ma prawdziwe parametry (np. ≈õredniƒÖ Œº i odchylenie standardowe œÉ), kt√≥re sƒÖ ustalonymi liczbami, nawet je≈õli ich nie znamy.\n\n\nPr√≥ba\nPr√≥ba to podzbi√≥r populacji, kt√≥ry faktycznie obserwujemy i mierzymy.\nPrzyk≈Çady:\n\n1000 losowo wybranych kanadyjskich wyborc√≥w (z populacji wszystkich wyborc√≥w)\n200 drzew zmierzonych na wyznaczonych dzia≈Çkach (spo≈õr√≥d wszystkich drzew w Yellowstone)\n10 000 transakcji klient√≥w wybranych do analizy (spo≈õr√≥d milion√≥w zakup√≥w na Amazonie)\n300 student√≥w, kt√≥rzy odpowiedzieli na TwojƒÖ ankietƒô (spo≈õr√≥d wszystkich student√≥w uniwersytetu)\n\nKluczowa cecha: U≈ºywamy pr√≥by do wnioskowania o parametrach populacji przy pomocy statystyk (np. ≈õrednia pr√≥by ${x}$ i odchylenie standardowe $s$).\n\n\nKlasyczne ramy statystyczne\nPOPULACJA (zwykle nieznana)\n    ‚Üì\n[Proces losowania pr√≥by]\n    ‚Üì\nPR√ìBA (to, co obserwujemy)\n    ‚Üì\n[Wnioskowanie statystyczne]\n    ‚Üì\nOSZACOWANIA parametr√≥w populacji\nDlatego potrzebujemy:\n\nWarto≈õci p: aby testowaƒá hipotezy o nieznanych parametrach populacji\nPrzedzia≈Çy ufno≈õci: aby oszacowaƒá niepewno≈õƒá zwiƒÖzanƒÖ z parametrami populacji\nB≈Çƒôdy standardowe: aby mierzyƒá, jak bardzo statystyki pr√≥by mogƒÖ siƒô r√≥≈ºniƒá\n\n\n\n\n\nKiedy ten schemat przestaje dzia≈Çaƒá\nCzasami mamy dane dla ca≈Çej populacji. Na przyk≈Çad:\n\nSpis powszechny obejmujƒÖcy wszystkich obywateli\nWszystkie transakcje gie≈Çdowe na NYSE w 2024 roku\nWszystkie bramki zdobyte w Premier League w ostatnim sezonie\nDane administracyjne o wszystkich hospitalizacjach\n\nParadoks: Je≈õli znamy ≈õredniƒÖ Œº populacji (bo policzyli≈õmy jƒÖ dla wszystkich), to po co nam przedzia≈Ç ufno≈õci dla Œº?\n\n\n\nSuperpopulacja (Proces generowania danych, DGP)\nSuperpopulacja albo proces generowania danych (Data Generating Process, DGP) to teoretyczna, niesko≈Ñczona populacja, z kt√≥rej nasza obserwowana, sko≈Ñczona populacja jest jednƒÖ realizacjƒÖ.\nZamiast my≈õleƒá:\nPopulacja ‚Üí Pr√≥ba\nMy≈õlimy:\nSUPERPOPULACJA (niesko≈Ñczona, teoretyczna)\n    ‚Üì\n[Proces generowania danych]\n    ‚Üì\nZAOBSERWOWANA POPULACJA (to, co mamy)\n    ‚Üì\nWNIOSKI o DGP\n\nPrzyk≈Çady\n\nPrzyk≈Çad 1: Dane o sprzeda≈ºy rocznej\n\nZaobserwowana populacja: Wszystkie 50 000 transakcji w 2024 roku\nSuperpopulacja: Te 50 000 transakcji to jedna realizacja trwajƒÖcego procesu biznesowego, kt√≥ry m√≥g≈Çby wygenerowaƒá inne wyniki przy nieco odmiennych okoliczno≈õciach\nDlaczego to wa≈ºne: Chcemy zrozumieƒá proces sprzeda≈ºy, aby przewidzieƒá 2025 rok, a nie tylko opisaƒá 2024\n\n\n\nPrzyk≈Çad 2: Wyniki wybor√≥w\n\nZaobserwowana populacja: Frekwencja we wszystkich 3000 gmin w wyborach w 2024 roku\nSuperpopulacja: Wyniki te sƒÖ jednƒÖ realizacjƒÖ procesu wyborczego obejmujƒÖcego pogodƒô, kampanie, kwestie polityczne itp., kt√≥re mog≈Çyby daƒá inne rezultaty\nDlaczego to wa≈ºne: Chcemy zrozumieƒá czynniki wp≈ÇywajƒÖce na frekwencjƒô w og√≥le, a nie tylko w jednym konkretnym g≈Çosowaniu\n\n\n\nPrzyk≈Çad 3: Oceny student√≥w\n\nZaobserwowana populacja: Ko≈Ñcowe oceny wszystkich 400 student√≥w na kursie Statystyki 101 w tym semestrze\nSuperpopulacja: Ci studenci i ich oceny to jedna realizacja ciƒÖg≈Çego procesu kszta≈Çcenia na uniwersytecie\nDlaczego to wa≈ºne: Interesuje nas, czy nowa metoda nauczania dzia≈Ça, a nie tylko to, jak poradzi≈Ça sobie ta konkretna grupa",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#tabela-podsumowujƒÖca",
    "href": "rozdzial1.html#tabela-podsumowujƒÖca",
    "title": "2¬† Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.5 Tabela podsumowujƒÖca",
    "text": "2.5 Tabela podsumowujƒÖca\n\n\n\n\n\n\n\n\n\nPojƒôcie\nWielko≈õƒá\nPrzyk≈Çad\nCo nam daje\n\n\n\n\nPr√≥ba\nSko≈Ñczony podzbi√≥r\n1000 ankietowanych wyborc√≥w\nOszacowania parametr√≥w populacji\n\n\nPopulacja\nSko≈Ñczona, pe≈Çna\nWszyscy 10 mln wyborc√≥w\nDok≈Çadne parametry (je≈õli zmierzymy wszystkich)\n\n\nSuperpopulacja\nNiesko≈Ñczona, teoretyczna\nProces wyborczy generujƒÖcy zachowania wyborc√≥w\nZrozumienie mechanizm√≥w i proces√≥w\n\n\n\n\n\nKluczowa my≈õl\nKoncepcja superpopulacji pozwala nam:\n\nStosowaƒá wnioskowanie statystyczne nawet przy danych obejmujƒÖcych ca≈ÇƒÖ populacjƒô\nTraktowaƒá nasze dane jako jednƒÖ z mo≈ºliwych realizacji ukrytego procesu\nFormu≈Çowaƒá wnioski o mechanizmach, a nie tylko opisywaƒá dane\nUog√≥lniaƒá poza konkretny czas i miejsce obserwacji\n\n\nDlatego badacze czƒôsto stosujƒÖ warto≈õci p i przedzia≈Çy ufno≈õci nawet przy pe≈Çnych danych populacyjnych ‚Äì nie z niedba≈Ço≈õci, lecz dlatego, ≈ºe traktujƒÖ te dane jako realizacjƒô szerszego procesu generowania danych, kt√≥ry chcƒÖ zrozumieƒá.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#kluczowe-pojƒôcia-parametry-statystyki-i-oszacowania",
    "href": "rozdzial1.html#kluczowe-pojƒôcia-parametry-statystyki-i-oszacowania",
    "title": "2¬† Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.6 Kluczowe pojƒôcia: parametry, statystyki i oszacowania",
    "text": "2.6 Kluczowe pojƒôcia: parametry, statystyki i oszacowania\n\nParametry vs.¬†statystyki\nFundamentalnym rozr√≥≈ºnieniem w statystyce jest r√≥≈ºnica miƒôdzy parametrami a statystykami:\nParametry populacji\n\nCechy liczbowe ca≈Çej populacji\nZwykle nieznane i to, o czym chcemy siƒô dowiedzieƒá\nOznaczane greckimi literami: \\mu (mi) dla ≈õredniej, \\sigma (sigma) dla odchylenia standardowego, \\pi (pi) dla proporcji\nPrzyk≈Çady: Prawdziwy procent wszystkich Amerykan√≥w popierajƒÖcych powszechnƒÖ opiekƒô zdrowotnƒÖ\n\nStatystyki pr√≥by\n\nCechy liczbowe obliczone z danych pr√≥by\nTo, co faktycznie obserwujemy i obliczamy\nOznaczane literami ≈Çaci≈Ñskimi: \\bar{x} dla ≈õredniej pr√≥by, s dla odchylenia standardowego pr√≥by, \\hat{p} dla proporcji pr√≥by\nPrzyk≈Çady: Procent 1000 respondent√≥w sonda≈ºu popierajƒÖcych powszechnƒÖ opiekƒô zdrowotnƒÖ\n\nKonwencja notacji\nW tym podrƒôczniku bƒôdziemy konsekwentnie u≈ºywaƒá: - Parametry populacji: \\mu (≈õrednia), \\sigma (odchylenie standardowe), \\pi (proporcja)\n- Statystyki pr√≥by: \\bar{x} (≈õrednia), s (odchylenie standardowe), \\hat{p} (proporcja)\nTa notacja pomaga nam zawsze rozr√≥≈ºniƒá to, co obserwujemy (statystyki), od tego, co chcemy wiedzieƒá (parametry).\n\n\nProces wnioskowania: od statystyk do parametr√≥w\nJƒÖdro wnioskowania statystycznego obejmuje wykorzystanie statystyk pr√≥by do dokonywania ≈õwiadomych przypuszcze≈Ñ o parametrach populacji:\n\\text{Statystyka pr√≥by} \\xrightarrow{\\text{Wnioskowanie statystyczne}} \\text{Parametr populacji}\nPrzyk≈Çad: Je≈õli 52% naszej pr√≥by (\\hat{p} = 0,52) popiera kandydata, u≈ºywamy tej statystyki do oszacowania parametru populacji (\\pi) reprezentujƒÖcego prawdziwe poparcie w≈õr√≥d wszystkich wyborc√≥w.\n\n\nOszacowania i estymatory\nEstymator to metoda lub wz√≥r u≈ºywany do przybli≈ºania parametru. Oszacowanie to konkretny wynik liczbowy z zastosowania tego estymatora do konkretnej pr√≥by.\n\nEstymator: ≈örednia pr√≥by \\bar{x} = \\frac{\\sum x_i}{n}\nOszacowanie: \\bar{x} = 6,3 lat wykszta≈Çcenia (rzeczywista liczba z naszych danych)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#analogia-zupy-zrozumienie-wnioskowania-statystycznego",
    "href": "rozdzial1.html#analogia-zupy-zrozumienie-wnioskowania-statystycznego",
    "title": "2¬† Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.7 Analogia zupy: zrozumienie wnioskowania statystycznego",
    "text": "2.7 Analogia zupy: zrozumienie wnioskowania statystycznego\nWyobra≈∫ sobie, ≈ºe jeste≈õ szefem kuchni przygotowujƒÖcym wielki garnek zupy dla 1000 os√≥b. Chcesz wiedzieƒá, czy zupa ma odpowiedniƒÖ ilo≈õƒá soli, ale nie mo≈ºesz skosztowaƒá jej ca≈Çej. Zamiast tego bierzesz ma≈ÇƒÖ ≈Çy≈ºkƒô do skosztowania.\nPopulacja: Ca≈Çy garnek zupy (1000 porcji)\nPr√≥ba: Twoja ≈Çy≈ºka\nParametr: Prawdziwa s≈Çono≈õƒá ca≈Çego garnka (nieznana)\nStatystyka: S≈Çono≈õƒá twojej ≈Çy≈ºki (to, co mo≈ºesz zmierzyƒá)\nWnioskowanie statystyczne: U≈ºywanie s≈Çono≈õci ≈Çy≈ºki do wyciƒÖgania wniosk√≥w o ca≈Çym garnku\nKluczowe spostrze≈ºenia z analogii zupy:\n\nLosowe pr√≥bkowanie ma znaczenie: Musisz najpierw wymieszaƒá zupƒô i wziƒÖƒá ≈Çy≈ºkƒô z losowego miejsca. Je≈õli zawsze pobierasz pr√≥bƒô z wierzchu, mo≈ºesz przegapiƒá, ≈ºe s√≥l osiad≈Ça na dnie.\nRozmiar pr√≥by wp≈Çywa na precyzjƒô: Wiƒôksza ≈Çy≈ºka da ci lepsze wyobra≈ºenie o og√≥lnej s≈Çono≈õci ni≈º ma≈Çy ≈Çyk.\nNiepewno≈õƒá jest nieod≈ÇƒÖczna: Nawet przy dobrym pr√≥bkowaniu twoja ≈Çy≈ºka mo≈ºe nie odzwierciedlaƒá idealnie ca≈Çego garnka. Zawsze jest jaka≈õ niepewno≈õƒá.\nSystematyczna stronniczo≈õƒá rujnuje wszystko: Gdyby kto≈õ potajemnie doda≈Ç extra s√≥l tylko do twojej ≈Çy≈ºki, twoje wnioskowanie o ca≈Çym garnku by≈Çoby b≈Çƒôdne. To reprezentuje stronniczo≈õƒá pr√≥bkowania.\nWnioskowanie ma ograniczenia: Mo≈ºesz oszacowaƒá ≈õredniƒÖ s≈Çono≈õƒá, ale twoja ≈Çy≈ºka nie mo≈ºe powiedzieƒá ci, czy niekt√≥re czƒô≈õci sƒÖ bardziej s≈Çone ni≈º inne (zmienno≈õƒá wewnƒÖtrz populacji).\n\nTa analogia uchwywa istotƒô my≈õlenia statystycznego: u≈ºywamy ma≈Çych, starannie wybranych pr√≥b do uczenia siƒô o znacznie wiƒôkszych populacjach, zawsze uznajƒÖc niepewno≈õƒá nieod≈ÇƒÖcznƒÖ w tym procesie.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#przyk≈Çad-z-prawdziwego-≈õwiata-co-przewiduje-sukces-wyborczy",
    "href": "rozdzial1.html#przyk≈Çad-z-prawdziwego-≈õwiata-co-przewiduje-sukces-wyborczy",
    "title": "2¬† Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.8 Przyk≈Çad z prawdziwego ≈õwiata: Co przewiduje sukces wyborczy?",
    "text": "2.8 Przyk≈Çad z prawdziwego ≈õwiata: Co przewiduje sukces wyborczy?\nZacznijmy od pytania, kt√≥re trafia w serce politologii: Co sprawia, ≈ºe politycy wygrywajƒÖ wybory?\nWyobra≈∫ sobie, ≈ºe jeste≈õ mened≈ºerem kampanii pr√≥bujƒÖcym zrozumieƒá, dlaczego niekt√≥rzy urzƒôdujƒÖcy wygrywajƒÖ mia≈ºd≈ºƒÖco, podczas gdy inni ledwo wygrywajƒÖ. Masz dane o 200 ostatnich wyborach do Kongresu, w tym ocenƒô popularno≈õci ka≈ºdego urzƒôdujƒÖcego, stan lokalnej gospodarki i ich margines zwyciƒôstwa.\n\n# Stw√≥rz realistyczne dane wyborcze\nset.seed(42)  # Zgodne z poczƒÖtkowym ustawieniem\nn_elections &lt;- 200\n\n# Generuj skorelowane predyktory (realistyczny scenariusz)\napproval_rating &lt;- runif(n_elections, 35, 85)\neconomic_growth &lt;- rnorm(n_elections, 2.5, 1.5)\ncampaign_spending_100k &lt;- rnorm(n_elections, 8, 2)  # W jednostkach $100,000 dla przejrzysto≈õci\n\n# Stw√≥rz margines zwyciƒôstwa z realistycznymi zwiƒÖzkami\nvictory_margin &lt;- -15 + \n  0.6 * approval_rating +           # Silny efekt popularno≈õci\n  2.5 * economic_growth +           # G≈Çosowanie ekonomiczne\n  0.3 * campaign_spending_100k +    # PieniƒÖdze pomagajƒÖ (efekt na $100k)\n  rnorm(n_elections, 0, 8)          # Czynniki losowe\n\n# Stw√≥rz zbi√≥r danych\nelection_data &lt;- data.frame(\n  district = 1:n_elections,\n  approval = approval_rating,\n  econ_growth = economic_growth,\n  spending_100k = campaign_spending_100k,\n  victory_margin = victory_margin,\n  won = victory_margin &gt; 0\n)\n\n# Szybka wizualizacja\np1 &lt;- ggplot(election_data, aes(x = approval, y = victory_margin)) +\n  geom_point(alpha = 0.6, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.7) +\n  labs(title = \"Ocena popularno≈õci vs. margines zwyciƒôstwa\",\n       x = \"Ocena popularno≈õci (%)\",\n       y = \"Margines zwyciƒôstwa (punkty procentowe)\",\n       subtitle = \"Punkty powy≈ºej linii przerywanej reprezentujƒÖ zwyciƒôstwa\")\n\nprint(p1)\n\n\n\n\n\n\n\n# Uruchom regresjƒô\nsimple_model &lt;- lm(victory_margin ~ approval, data = election_data)\nsummary(simple_model)\n\n\nCall:\nlm(formula = victory_margin ~ approval, data = election_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.9948  -6.1420   0.5653   5.9218  28.4974 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) -9.78570    2.63382  -3.715             0.000264 ***\napproval     0.64728    0.04192  15.439 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.635 on 198 degrees of freedom\nMultiple R-squared:  0.5462,    Adjusted R-squared:  0.544 \nF-statistic: 238.4 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\n\n\nUwaga do wykresu: Ten wykres rozrzutu pokazuje zwiƒÖzek miƒôdzy ocenami popularno≈õci (o≈õ x) a marginesami zwyciƒôstwa wyborczego (o≈õ y). Ka≈ºdy punkt reprezentuje jedne wybory. Czerwona linia pokazuje ‚Äúliniƒô najlepszego dopasowania‚Äù z regresji liniowej, z szarym pasmem wskazujƒÖcym niepewno≈õƒá. Punkty powy≈ºej przerywnej linii poziomej (y=0) reprezentujƒÖ zwyciƒôstwa wyborcze.\n\n\nCzytanie wynik√≥w: ‚ÄúOszacowanie‚Äù (Estimate) dla popularno≈õci (oko≈Ço 0,60) oznacza, ≈ºe ka≈ºdy 1-punktowy wzrost oceny popularno≈õci jest zwiƒÖzany z 0,60-punktowym wzrostem marginesu zwyciƒôstwa. Warto≈õƒá p (&lt;0,001) wskazuje, ≈ºe ten zwiƒÖzek jest statystycznie istotny - bardzo ma≈Ço prawdopodobny by wynika≈Ç tylko z przypadku.\n\nCo w≈Ça≈õnie odkryli≈õmy: Ka≈ºdy 1-punktowy wzrost oceny popularno≈õci jest zwiƒÖzany z oko≈Ço 0.65-punktowym wzrostem marginesu zwyciƒôstwa. Przy ocenie popularno≈õci poni≈ºej 15.1% urzƒôdujƒÖcy zazwyczaj przegrywajƒÖ.\nJednak ocena popularno≈õci reprezentuje tylko jeden czynnik sukcesu wyborczego. Bardziej kompleksowa analiza wymaga jednoczesnego badania wielu zmiennych:\n\n# Model regresji wielokrotnej\nfull_model &lt;- lm(victory_margin ~ approval + econ_growth + spending_100k, data = election_data)\n\n# Przejrzyste przedstawienie wynik√≥w\nmodel_results &lt;- tidy(full_model) %&gt;%\n  mutate(\n    estimate = round(estimate, 4),\n    p.value = round(p.value, 3),\n    significant = ifelse(p.value &lt; 0.05, \"Tak\", \"Nie\"),\n    term = recode(term,\n                  \"(Intercept)\" = \"Warto≈õƒá bazowa\",\n                  \"approval\" = \"Ocena popularno≈õci\",\n                  \"econ_growth\" = \"Wzrost gospodarczy (%)\",\n                  \"spending_100k\" = \"Wydatki kampanii (na $100k)\")\n  )\n\nkable(model_results, \n      col.names = c(\"Zmienna\", \"Rozmiar efektu\", \"B≈ÇƒÖd std\", \"Statystyka t\", \"Warto≈õƒá p\", \"Istotne?\"),\n      caption = \"Co naprawdƒô napƒôdza sukces wyborczy?\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nCo naprawdƒô napƒôdza sukces wyborczy?\n\n\nZmienna\nRozmiar efektu\nB≈ÇƒÖd std\nStatystyka t\nWarto≈õƒá p\nIstotne?\n\n\n\n\nWarto≈õƒá bazowa\n-18.8368\n3.7738929\n-4.991343\n0.00\nTak\n\n\nOcena popularno≈õci\n0.6541\n0.0397821\n16.441115\n0.00\nTak\n\n\nWzrost gospodarczy (%)\n1.9619\n0.4004247\n4.899426\n0.00\nTak\n\n\nWydatki kampanii (na $100k)\n0.4897\n0.3054328\n1.603246\n0.11\nNie\n\n\n\n\n\n\n\nGdy uwzglƒôdniamy jednocze≈õnie wiele czynnik√≥w, widzimy, ≈ºe:\n\nOcena popularno≈õci pozostaje najsilniejszym predyktorem (0,6 punktu na 1% popularno≈õci)\nWzrost gospodarczy r√≥wnie≈º ma istotne znaczenie (2,5 punktu na 1% wzrostu PKB)\nWydatki kampanii majƒÖ skromny efekt (0,3 punktu na $100 000 wydanych)\n\nTo jest si≈Ça analizy regresji - pomaga nam rozwik≈Çaƒá z≈Ço≈ºone zwiƒÖzki i zrozumieƒá, co naprawdƒô ma znaczenie w polityce.\n\n\n\n\n\n\nTypowe pu≈Çapki statystyczne w politologii\n\n\n\n\nB≈ÇƒÖd ekologiczny: Zak≈Çadanie, ≈ºe wzorce na poziomie grupy dotyczƒÖ jednostek\nStronniczo≈õƒá selekcji: Nielosowe pr√≥by, kt√≥re systematycznie wykluczajƒÖ pewne grupy\n\nZmienne zak≈Ç√≥cajƒÖce: Brak uwzglƒôdnienia zmiennych wp≈ÇywajƒÖcych zar√≥wno na X, jak i Y\nHakowanie p: Testowanie wielu hipotez a≈º do znalezienia istotno≈õci\nNadmierne uog√≥lnianie: Rozszerzanie wynik√≥w poza badanƒÖ populacjƒô\n\n\n\nPod koniec tego kursu zrozumiesz:\n\nJak ta analiza dzia≈Ça i jakie wymaga za≈Ço≈ºe≈Ñ\nKiedy mo≈ºemy interpretowaƒá te zwiƒÖzki jako przyczynowe vs.¬†jedynie korelacyjne\nJak oceniaƒá wiarygodno≈õƒá i praktyczne znaczenie naszych ustale≈Ñ\nCo mo≈ºe p√≥j≈õƒá nie tak i jak unikaƒá typowych pu≈Çapek\n\nTeraz zbudujmy fundament do zrozumienia, jak uzyskali≈õmy te wyniki i co naprawdƒô oznaczajƒÖ.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#≈õwiat-polityki-jest-pe≈Çen-danych",
    "href": "rozdzial1.html#≈õwiat-polityki-jest-pe≈Çen-danych",
    "title": "2¬† Wprowadzenie do statystyki i analizy danych dla politologii",
    "section": "2.9 ≈öwiat polityki jest pe≈Çen danych",
    "text": "2.9 ≈öwiat polityki jest pe≈Çen danych\nPolitologia ewoluowa≈Ça z dyscypliny g≈Ç√≥wnie teoretycznej do takiej, kt√≥ra coraz bardziej opiera siƒô na dowodach empirycznych. Niezale≈ºnie od tego, czy badamy:\n\nWyniki wybor√≥w: Dlaczego ludzie g≈ÇosujƒÖ tak, jak g≈ÇosujƒÖ?\nOpiniƒô publicznƒÖ: Co kszta≈Çtuje postawy wobec imigracji lub polityki klimatycznej?\nStosunki miƒôdzynarodowe: Jakie czynniki przewidujƒÖ konflikt miƒôdzy narodami?\nSkuteczno≈õƒá polityk: Czy nowa polityka edukacyjna rzeczywi≈õcie poprawi≈Ça wyniki?\n\nPotrzebujemy systematycznych sposob√≥w analizowania danych i wyciƒÖgania wniosk√≥w, kt√≥re wykraczajƒÖ poza anegdoty i osobiste wra≈ºenia.\nRozwa≈º to pytanie: ‚ÄúCzy demokracja prowadzi do wzrostu gospodarczego?‚Äù\nTwoja intuicja mo≈ºe sugerowaƒá, ≈ºe tak - kraje demokratyczne sƒÖ zazwyczaj bogatsze. Ale czy to przyczynowo≈õƒá, czy korelacja? Czy sƒÖ wyjƒÖtki? Jak pewni mo≈ºemy byƒá naszych wniosk√≥w?\nStatystyka dostarcza narzƒôdzi do przej≈õcia od przeczuƒá do odpowiedzi opartych na dowodach, pomagajƒÖc nam rozr√≥≈ºniƒá miƒôdzy tym, co wydaje siƒô prawdziwe, a tym, co rzeczywi≈õcie jest prawdziwe.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Wprowadzenie do statystyki i analizy danych dla politologii</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "3¬† Understanding Data Types in Social Sciences",
    "section": "",
    "text": "3.1 Foundations in Number Sets\nIn social science research, understanding the nature of our data is crucial for selecting appropriate analysis methods and drawing valid conclusions.\nBefore diving into data types, it‚Äôs essential to understand the basic number sets that form the foundation of our understanding of data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#foundations-in-number-sets",
    "href": "chapter2.html#foundations-in-number-sets",
    "title": "3¬† Understanding Data Types in Social Sciences",
    "section": "",
    "text": "Basic Number Sets\n\nNatural Numbers (‚Ñï): The counting numbers {0, 1, 2, 3, ‚Ä¶}\nIntegers (‚Ñ§): Includes natural numbers, their negatives, and zero {‚Ä¶, -2, -1, 0, 1, 2, ‚Ä¶}\nRational Numbers (‚Ñö): Numbers that can be expressed as a fraction of two integers\nReal Numbers (‚Ñù): All numbers on the number line, including rationals and irrationals\n\n\n\nProperties of Sets\n\nCountable Sets: Sets whose elements can be put in a one-to-one correspondence with the natural numbers. For example, the set of integers is countable.\nUncountable Sets: Sets that are not countable. The set of real numbers is uncountable.\nDiscrete Sets: Sets where each element is separated from other elements by a finite gap. The integers form a discrete set.\nDense Sets: Sets where between any two elements, there is always another element of the set. The rational numbers and real numbers are dense sets.\n\n\n\n\n\n\n\nNote\n\n\n\nUnderstanding these set properties is crucial for grasping the nature of different data types in social sciences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#discrete-vs.-continuous-data",
    "href": "chapter2.html#discrete-vs.-continuous-data",
    "title": "3¬† Understanding Data Types in Social Sciences",
    "section": "3.2 Discrete vs.¬†Continuous Data",
    "text": "3.2 Discrete vs.¬†Continuous Data\nIn data science and statistics, we often categorize variables as either discrete or continuous. However, the distinction is not always clear-cut, and some variables exhibit characteristics of both types. This section explores the concepts of discrete and continuous data, their differences, and the interesting cases of variables that can be treated as both or challenge our intuitive understanding.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDiscrete Data\nDiscrete data can only take on specific, countable values. These values are often (but not always) integers.\n\nCharacteristics of Discrete Data:\n\nCountable\nOften represented by integers\nCan be finite or infinite\nNo values between two adjacent data points\n\n\n\nExamples:\n\nNumber of students in a class\nNumber of cars sold by a dealership\nShoe sizes\n\n\n\n\nContinuous Data\nContinuous data can take on any value within a given range, including fractional and decimal values. It‚Äôs important to note that continuity is not solely determined by uncountability, but also by density.\n\nCharacteristics of Continuous Data:\n\nCan be uncountable (like real numbers) or dense (like rational numbers)\nCan be measured to any level of precision (theoretically)\nRepresented by real numbers or dense subsets of real numbers\nThere are always values between any two data points\n\n\n\nExamples:\n\nHeight\nWeight\nTemperature\nPercentages (explained further below)\n\n\n\n\nThe Discrete-Continuous Spectrum\nIn practice, some variables that are mathematically discrete are often treated as if they are continuous. This dual nature provides flexibility in how these variables can be analyzed and interpreted.\n\nReasons for Treating Discrete Data as Continuous:\n\nDense Granularity\n\nWhen a discrete variable has a large number of possible values within a range, it can approximate continuity.\nExample: Income measured in individual cents. While technically discrete, the large number of possible values makes it behave similarly to a continuous variable.\n\nAnalytical Convenience\n\nContinuous methods often yield reasonable and useful results even for dense discrete variables.\nIt‚Äôs often easier to use existing statistical tools if continuity is assumed, as this allows the use of calculus-based methods.\n\nApproximation of Underlying Phenomena\n\nIn some cases, a discrete measurement might be an approximation of an underlying continuous process.\nExample: While we measure time in discrete units (seconds, minutes, hours), time itself is continuous.\n\n\n\n\nExamples of Variables with Dual Discrete-Continuous Nature:\n\nAge\n\nDiscrete: Typically measured in whole years\nContinuous: Can be considered as a continuous variable in many analyses, especially when dealing with large populations\n\nPrice and Income\n\nDiscrete: Prices and incomes are actually measured in discrete units (e.g., cents or smallest currency unit)\nContinuous: In economic models and many analyses, prices and incomes are treated as continuous variables due to their dense nature and analytical convenience\n\nTest Scores\n\nDiscrete: Often given as whole numbers\nContinuous: In statistical analyses, test scores might be treated as continuous, especially when the range of possible scores is large\n\n\n\n\n\nSpecial Case: Percentages and Rational Numbers\nPercentages present an interesting case in the discrete-continuous spectrum:\n\nRational Nature: Percentages are essentially fractions (m/100), making them rational numbers.\nDense but Countable: The set of rational numbers is dense (between any two rationals, there‚Äôs another rational) but also countable.\nPractical Continuity: In most practical applications, percentages are treated as continuous due to their dense nature.\nFinite Precision: In reality, percentages are often reported to a limited number of decimal places, creating a finite set of possible values.\n\n\n\n\n\n\n\nPercentages: Bridging Discrete and Continuous\n\n\n\nVariables measured in percentages, such as unemployment rates or voter turnout, challenge our intuitive understanding of discreteness and continuity:\n\nThey are rational numbers (fractions with denominator 100), which are technically countable.\nThey form a dense set within their range (0% to 100%), allowing for values between any two percentages.\nIn practice, they are often treated as continuous variables due to their dense nature and analytical convenience.\nThe precision of measurement (e.g., reporting to one or two decimal places) can impose a discrete structure on what is conceptually a dense set.\n\nThis duality allows for flexible analytical approaches, depending on the specific research context and required precision.\n\n\n\n\nImplications for Data Analysis\nUnderstanding the nuanced nature of variables as discrete, continuous, or somewhere in between has important implications for data analysis:\n\nFlexibility in Modeling: It allows for the use of a wider range of statistical techniques.\nSimplified Calculations: Treating dense discrete data as continuous can simplify calculations and make certain analyses more tractable.\nImproved Interpretability: In some cases, treating discrete data as continuous can lead to more intuitive or useful interpretations of results.\nPotential for Error: It‚Äôs important to be aware of when approximations are appropriate and when they might lead to misleading results.\nTheoretical vs.¬†Practical Considerations: While the mathematical nature of the data is important, practical considerations in measurement and analysis often guide how we treat variables.\n\n\n\nConclusion\nThe distinction between discrete and continuous data is not always rigid in social sciences. Many variables, including those involving money, percentages, or dense measurements, can be viewed through both discrete and continuous lenses. The choice of treatment should be guided by the nature of the data, the goals of the analysis, and the potential implications of the choice. This flexibility, when used thoughtfully, provides powerful tools for social science researchers to gain insights from their data.\n\n\n\n\n\n\nDiscrete vs.¬†Continuous Numerical Data: A Language-Based Analogies\n\n\n\n\nThe Language Connection\nThink about how you naturally ask questions about quantities:\n\n‚ÄúHow many cookies are in the jar?‚Äù (counting)\n‚ÄúHow much water is in the glass?‚Äù (measuring)\n\nThis natural language distinction reflects the two fundamental types of numerical data:\n\n\nDiscrete Data = ‚ÄúHow Many?‚Äù Questions\n\nLike counting whole objects (countable nouns)\nTakes specific values with gaps between them\nExamples:\n\nNumber of pets: 0, 1, 2, 3‚Ä¶ (can‚Äôt have 2.5 pets)\nDice rolls: 1, 2, 3, 4, 5, 6\nStudents in a class: 20, 21, 22‚Ä¶\n\n\nü§î Self-Check: Can you find a value between 2 and 3 students? Why not?\n\n\nContinuous Data = ‚ÄúHow Much?‚Äù Questions\n\nLike measuring quantities (uncountable nouns)\nCan take any value within a range\nExamples:\n\nHeight: 1.7231‚Ä¶ meters\nTemperature: 36.8325‚Ä¶ ¬∞C\nTime: 3.5792‚Ä¶ hours\n\n\nü§î Self-Check: Write down three different values between 1.72 and 1.73 meters\n\n\nQuick Recognition Guide\n\nIf you naturally ask ‚ÄúHow many?‚Äù ‚Üí Discrete\nIf you naturally ask ‚ÄúHow much?‚Äù ‚Üí Continuous\nIf you can measure it more precisely ‚Üí Continuous\nIf you can only use whole numbers ‚Üí Discrete\n\n‚úçÔ∏è Practice: Classify these quantities as discrete or continuous\n\nYour age in years: _____\nYour height: _____\nNumber of songs in a playlist: _____\nVolume of water: _____",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#introduction-to-stevens-data-typology",
    "href": "chapter2.html#introduction-to-stevens-data-typology",
    "title": "3¬† Understanding Data Types in Social Sciences",
    "section": "3.3 Introduction to Stevens‚Äô Data Typology",
    "text": "3.3 Introduction to Stevens‚Äô Data Typology\nStanley S. Stevens, an American psychologist, introduced a classification system for scales of measurement in his 1946 paper ‚ÄúOn the Theory of Scales of Measurement.‚Äù This system, known as Stevens‚Äô data typology or levels of measurement, has become fundamental in understanding how different types of data should be analyzed and interpreted.\nStevens proposed four levels of measurement:\n\nNominal\nOrdinal\nInterval\nRatio\n\nEach level has specific properties and allows for different types of statistical operations and analyses.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\nNominal Scale\n\nDefinition\nThe nominal scale is the most basic level of measurement. It uses labels or categories to classify data without any quantitative value or order.\n\n\nProperties\n\nCategories are mutually exclusive\nNo inherent order among categories\nNo meaningful arithmetic operations can be performed\n\n\n\nExamples\n\nNationality (Polish, English, ‚Ä¶)\nBlood types (A, B, AB, O)\nEye color (Blue, Brown, Green, Hazel)\nBinary variables (‚ÄúSuccess‚Äù versus ‚ÄúFailure‚Äù)\n\n\n\n\nOrdinal Scale\n\nDefinition\nThe ordinal scale categorizes data into ordered categories, but the intervals between categories are not necessarily equal or meaningful.\n\n\nProperties\n\nCategories have a defined order\nDifferences between categories are not quantifiable\nArithmetic operations on the numbers are not meaningful\n\n\n\nExamples\n\nEducation levels (High School, Bachelor‚Äôs, Master‚Äôs, PhD)\nLikert scales (Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree)\nSocioeconomic status (Low, Medium, High)\n\n\n\n\nInterval Scale\n\nDefinition\nThe interval scale has ordered categories with equal intervals between adjacent categories. However, it lacks a true zero point.\n\n\nProperties\n\nEqual intervals between adjacent categories\nNo true zero point (zero is arbitrary)\nRatios between values are not meaningful\n\n\n\nExamples\n\nTemperature in Celsius or Fahrenheit\nCalendar years\npH scale (the difference between pH 4 and 5 represents the same change in hydrogen ion concentration as between pH 6 and 7)\nElevation above sea level\n\n\n\n\nRatio Scale\n\nDefinition\nThe ratio scale is the highest level of measurement. It has all the properties of the interval scale plus a true zero point, making ratios between values meaningful.\n\n\nProperties\n\nAll properties of interval scales\nTrue zero point\nRatios between values are meaningful\n\n\n\nExamples\n\nHeight\nWeight\nAge\nIncome\n\n\n\n\n\n\n\nWhy Some Statistics Work (and Others Don‚Äôt) for Interval Scales\n\n\n\n\nKey Idea\nAn interval scale is one where the distances between values are meaningful, but the zero point is arbitrary. For interval scales (e.g., temperature):\n\nAllowed: Addition/subtraction of values and multiplication/division by constants.\nNot allowed: Multiplication/division of values from the scale by each other, as this leads to results without physical interpretation.\n\n\n\nProperties of Interval Scales\n\nEqual intervals represent the same differences:\n\nThe difference between 20¬∞C and 25¬∞C (5¬∞C) represents the same change as between 30¬∞C and 35¬∞C.\nProportions of differences are preserved: 10¬∞C is twice the change of 5¬∞C.\n\nThe zero point is arbitrary:\n\n0¬∞C is the freezing point of water, not the absence of temperature.\nThe same physical state has different values in different scales: 0¬∞C = 32¬∞F.\n\nLinear transformation:\n\nGeneral formula: y = ax + b, where a \\neq 0.\nFor temperature: F = C \\times \\frac{9}{5} + 32.\n\n\n\n\nWhy the Arithmetic Mean Works\nThe arithmetic mean works because it relies on addition and division by a constant, which are allowed on interval scales. Example:\nData: 20¬∞C and 30¬∞C\n\nMethod 1: Mean in Celsius, then conversion\n1. Mean: (20¬∞C + 30¬∞C) √∑ 2 = 25¬∞C\n2. Conversion: 25¬∞C √ó (9/5) + 32 = 77¬∞F\n\nMethod 2: Conversion to ¬∞F, then mean\n1. Conversion: 20¬∞C ‚Üí 68¬∞F, 30¬∞C ‚Üí 86¬∞F\n2. Mean: (68¬∞F + 86¬∞F) √∑ 2 = 77¬∞F\n\nBoth methods give the same result! ‚úì\nMathematical proof of correctness: \\begin{align}\n\\bar{F} &= \\frac{F_1 + F_2}{2} \\\\\n&= \\frac{(C_1 \\times \\frac{9}{5} + 32) + (C_2 \\times \\frac{9}{5} + 32)}{2} \\\\\n&= \\frac{(C_1 + C_2) \\times \\frac{9}{5} + 64}{2} \\\\\n&= \\left(\\frac{C_1 + C_2}{2}\\right) \\times \\frac{9}{5} + 32 \\\\\n&= \\bar{C} \\times \\frac{9}{5} + 32\n\\end{align}\n\n\nWhy Variance is Problematic\nVariance is problematic because it relies on squared differences, leading to squared units (e.g., ¬∞C¬≤) without clear physical interpretation. Example:\nSame temperatures: 20¬∞C and 30¬∞C\n\nMethod 1: Variance in Celsius\n1. Mean: 25¬∞C\n2. Deviations: (20 - 25)¬∞C = -5¬∞C, (30 - 25)¬∞C = 5¬∞C\n3. Squared deviations: (-5¬∞C)¬≤ = 25(¬∞C)¬≤, (5¬∞C)¬≤ = 25(¬∞C)¬≤\n4. Mean: (25 + 25)(¬∞C)¬≤ √∑ 2 = 25(¬∞C)¬≤\n\nMethod 2: Variance in Fahrenheit\n1. Conversion: 20¬∞C ‚Üí 68¬∞F, 30¬∞C ‚Üí 86¬∞F\n2. Mean: 77¬∞F\n3. Deviations: (68 - 77)¬∞F = -9¬∞F, (86 - 77)¬∞F = 9¬∞F\n4. Squared deviations: (-9¬∞F)¬≤ = 81(¬∞F)¬≤, (9¬∞F)¬≤ = 81(¬∞F)¬≤\n5. Mean: (81 + 81)(¬∞F)¬≤ √∑ 2 = 81(¬∞F)¬≤\n\nProblem: 25(¬∞C)¬≤ and 81(¬∞F)¬≤ are not equivalent!\nMathematical analysis of the problem: \\begin{align}\n(F_i - \\bar{F})^2 &= [(C_i \\times \\frac{9}{5} + 32) - (\\bar{C} \\times \\frac{9}{5} + 32)]^2 \\\\\n&= [(C_i - \\bar{C}) \\times \\frac{9}{5}]^2 \\\\\n&= (C_i - \\bar{C})^2 \\times \\left(\\frac{9}{5}\\right)^2\n\\end{align}\n\n\nTheoretical Conclusions\n\nAllowed operations:\n\nAddition/subtraction (preserves differences).\nMultiplication/division by constants (scaling).\nArithmetic means.\nComparing temperature differences.\n\nNot allowed operations:\n\nMultiplying temperatures by each other.\nDividing temperatures by each other.\nGeometric means.\nCoefficient of variation.\n\nPractical implications:\n\nVariance and standard deviation require careful interpretation.\nBetter to use measures based on differences (e.g., MAD - mean absolute deviation).\nWhen comparing variability, it is advisable to standardize the data.\n\n\n\n\nPractical Rule\nIf your calculations involve multiplying values from an interval scale by each other, be particularly cautious in interpreting the results!\n\n\n\n\n\n\n\n\n\nProportions in Measurement Scales: The Case of Temperature\n\n\n\n\nTwo Types of Proportions\n\nProportions of values (DO NOT hold for interval scales):\nTake 80¬∞C and 20¬∞C:\nIn Celsius: 80¬∞C/20¬∞C = 4\nIn Fahrenheit: 176¬∞F/68¬∞F ‚âà 2.59\nIn Kelvin: 353.15K/293.15K ‚âà 1.20\n\nThe same temperatures give different proportions! \n‚Üí Proportions of values DO NOT make sense on interval scales; they only make sense on ratio scales.\n\n\nProportions of differences (hold for interval scales):\nTake two pairs of differences:\nPair 1: 30¬∞C - 20¬∞C = 10¬∞C\nPair 2: 80¬∞C - 60¬∞C = 20¬∞C\n\nProportion of differences in Celsius:\n20¬∞C/10¬∞C = 2\n\nThe same temperatures in Fahrenheit:\nPair 1: 86¬∞F - 68¬∞F = 18¬∞F\nPair 2: 176¬∞F - 140¬∞F = 36¬∞F\n\nProportion of differences in Fahrenheit:\n36¬∞F/18¬∞F = 2\n\nThe proportion of differences is the same! ‚úì\n\n\n\nMathematical Explanation\nFor the transformation F = \\frac{9}{5}C + 32:\n\nProportions of values DO NOT hold: [ = ]\nProportions of differences hold: [ = = ]\n\n\n\nWhy This Matters\n\nFor values:\n\nIn Celsius: 40¬∞C is not ‚Äútwice as hot‚Äù as 20¬∞C.\nIn Fahrenheit: 100¬∞F is not ‚Äútwice as hot‚Äù as 50¬∞F.\nOnly in Kelvin do proportions of values have physical meaning.\n\nFor differences:\n\nAn increase of 20¬∞C is always twice as large as an increase of 10¬∞C.\nAn increase of 36¬∞F is always twice as large as an increase of 18¬∞F.\nProportions of differences are scale-independent.\n\n\n\n\nImplications for Statistics\n\nOperations based on differences (WORK):\n\nArithmetic mean.\nMean absolute deviation.\nRange.\n\nOperations based on proportions of values (DO NOT WORK):\n\nGeometric mean.\nCoefficient of variation.\nVariance (because it uses squared values).\n\n\n\n\n\n\n\n\nImportance in Research and Analysis\nUnderstanding Stevens‚Äô data typology is crucial for several reasons:\n\nChoosing appropriate statistical tests: The level of measurement determines which statistical analyses are appropriate for a given dataset.\nInterpreting results: The meaning of statistical results depends on the level of measurement of the variables involved.\nDesigning measurement instruments: When creating surveys or other measurement tools, researchers must consider the level of measurement they want to achieve.\nData transformation: Sometimes, data can be transformed from one level to another, but this must be done carefully to avoid misinterpretation.\n\n\n\nControversies and Limitations\nWhile Stevens‚Äô typology is widely used, it has faced some criticisms:\n\nRigidity: Some argue that the typology is too rigid and that many real-world measurements fall between these categories.\nTreatment of ordinal data: There‚Äôs ongoing debate about when it‚Äôs appropriate to treat ordinal data as interval for certain analyses.\nPsychological scaling: Some psychological constructs (like intelligence) are difficult to categorize definitively within this system.\n\n\n\nConclusion\nStevens‚Äô data typology provides a fundamental framework for understanding different types of data and their properties. By recognizing the level of measurement of their variables, researchers can make informed decisions about data collection, analysis, and interpretation. However, it‚Äôs important to remember that while this typology is a useful guide, real-world data often requires nuanced consideration and may not always fit neatly into these categories.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not ‚Äútwice as acidic‚Äù as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#common-ordinal-scales-in-behavioural-research",
    "href": "chapter2.html#common-ordinal-scales-in-behavioural-research",
    "title": "3¬† Understanding Data Types in Social Sciences",
    "section": "3.4 Common Ordinal Scales in Behavioural Research",
    "text": "3.4 Common Ordinal Scales in Behavioural Research\n\nLikert Scales\nLikert scales are widely used in psychology and social sciences to measure attitudes, opinions, and perceptions. Named after psychologist Rensis Likert, these scales typically consist of a series of statements or questions that respondents rate on a scale, often from ‚ÄúStrongly Disagree‚Äù to ‚ÄúStrongly Agree.‚Äù\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nWhy Likert Scales are Ordinal Variables\nLikert scales are considered ordinal variables for several reasons:\n\nOrder without equal intervals: While the responses have a clear order (e.g., ‚ÄúStrongly Disagree‚Äù &lt; ‚ÄúDisagree‚Äù &lt; ‚ÄúNeutral‚Äù &lt; ‚ÄúAgree‚Äù &lt; ‚ÄúStrongly Agree‚Äù), the intervals between these categories are not necessarily equal.\nSubjective interpretation: The difference between ‚ÄúStrongly Disagree‚Äù and ‚ÄúDisagree‚Äù may not be the same as the difference between ‚ÄúAgree‚Äù and ‚ÄúStrongly Agree‚Äù for all respondents.\nLack of true zero point: Likert scales typically don‚Äôt have a true zero point, which is a characteristic of interval or ratio scales.\n\n\n\n\nIQ and Other Psychological Variables as Ordinal Measures\nMany psychological measures, including IQ, are often treated as interval scales but are, in fact, ordinal. Here‚Äôs why:\n\nIQ Scores:\n\nWhile IQ scores are presented as numbers, the difference between an IQ of 100 and 110 may not represent the same cognitive difference as between 130 and 140.\nThe scale is normalized and adjusted over time, making it difficult to claim true interval properties.\n\nOther Psychological Measures:\n\nDepression scales (e.g., Beck Depression Inventory)\nAnxiety measures (e.g., State-Trait Anxiety Inventory)\nPersonality assessments (e.g., Big Five Inventory)\n\n\nThese measures often use summed Likert-type items or other scoring methods that don‚Äôt guarantee equal intervals between scores.\n\n\nImplications for Analysis\nRecognizing these measures as ordinal has important implications for data analysis:\n\nAppropriate statistical tests: Use non-parametric tests (e.g., Mann-Whitney U, Kruskal-Wallis) instead of parametric ones.\nCorrelation analysis: Use Spearman‚Äôs rank correlation instead of Pearson‚Äôs correlation.\nCentral tendency: Report median and mode rather than mean.\nData visualization: Use methods appropriate for ordinal data, such as bar plots or stacked bar charts.\n\n\n\nConclusion\nWhile Likert scales and many behavioural measures are often treated as interval data for practical reasons, it‚Äôs crucial to remember their ordinal nature.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n\n\n\n\nExercise: Identifying Measurement Scales\n\n\n\nFor each of the following variables, determine the most appropriate scale of measurement (Nominal, Ordinal, Interval, or Ratio). Also evaluate whether the variable is discrete or continuous.\n\nGender: nominal level of measurement, and discrete;\nCustomer satisfaction: Poor, Fair, Good, Excellent\nHeight (questionnaire): ‚ÄúI am: very short, short, average, tall, very tall‚Äù\nHeight (inches)\nReaction time (milliseconds)\nPostal codes: e.g., 61548, 61761, 62461, 47424, 65233\nAge (years)\nNationality\nStreet addresses\nMilitary ranks\nLeft-Right political scale placement\nFamily size: 1 child, 2 children, 3 children, ‚Ä¶\nIQ score\nShirt size (S, M, L, ‚Ä¶)\nMovie ratings (1 star, 2 stars, 3 stars)\nTemperature (Celsius)\nTemperature (Kelvin)\nBlood types: A, B, AB, O\nIncome categories: low, medium, high\nVoter turnout\nPolitical party affiliation\nElectoral district magnitude\n\nRemember to justify your choices for each variable.\nFor instance: In Stevens‚Äô typology of measurement scales, street addresses are nominal data. This is because:\nThey serve purely as labels/identifiers. They have no inherent ordering (123 Main St isn‚Äôt ‚Äúmore than‚Äù 23 Oak St). You can‚Äôt perform meaningful mathematical operations on them.The only valid operation is testing for equality/inequality (is this the same address or different?)\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nMedian\nMediana\n-\n‚úì\n‚úì\n‚úì\n\n\nArithmetic Mean\n≈örednia arytmetyczna\n-\n-\n‚úì\n‚úì\n\n\nGeometric Mean\n≈örednia geometryczna\n-\n-\n-\n‚úì\n\n\nHarmonic Mean\n≈örednia harmoniczna\n-\n-\n-\n‚úì\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstƒôp\n-\n‚úì\n‚úì\n‚úì\n\n\nInterquartile Range\nRozstƒôp miƒôdzykwartylowy\n-\n‚úì\n‚úì\n‚úì\n\n\nMean Absolute Deviation\n≈örednie odchylenie bezwzglƒôdne\n-\n-\n‚úì\n‚úì\n\n\nVariance\nWariancja\n-\n-\n‚úì*\n‚úì\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n‚úì*\n‚úì\n\n\nCoefficient of Variation\nWsp√≥≈Çczynnik zmienno≈õci\n-\n-\n-\n‚úì\n\n\nAssociation / Wsp√≥≈Çzale≈ºno≈õƒá:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n‚úì\n‚úì\n‚úì\n\n\nKendall‚Äôs Tau\nTau Kendalla\n-\n‚úì\n‚úì\n‚úì\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n‚úì*\n‚úì\n\n\nCovariance\nKowariancja\n-\n-\n‚úì*\n‚úì\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\nNotes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporzƒÖdkowania\nOrdinal: Ordered categories / Kategorie uporzƒÖdkowane\nInterval: Equal intervals, arbitrary zero / R√≥wne interwa≈Çy, umowne zero\nRatio: Equal intervals, absolute zero / R√≥wne interwa≈Çy, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ‚úì* are commonly used for interval data despite theoretical issues / Niekt√≥re miary oznaczone ‚úì* sƒÖ powszechnie stosowane dla danych przedzia≈Çowych pomimo problem√≥w teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wyb√≥r miary powinien uwzglƒôdniaƒá zar√≥wno poprawno≈õƒá teoretycznƒÖ jak i u≈ºyteczno≈õƒá praktycznƒÖ\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalajƒÖ na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html",
    "href": "rozdzial2.html",
    "title": "4¬† Typy Danych w Naukach Spo≈Çecznych",
    "section": "",
    "text": "4.1 Podstawy Zbior√≥w Liczbowych\nW badaniach z obszaru nauk spo≈Çecznych zrozumienie natury danych jest kluczowe dla wyboru odpowiednich metod analizy i wyciƒÖgania prawid≈Çowych wniosk√≥w.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Typy Danych w Naukach Spo≈Çecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#podstawy-zbior√≥w-liczbowych",
    "href": "rozdzial2.html#podstawy-zbior√≥w-liczbowych",
    "title": "4¬† Typy Danych w Naukach Spo≈Çecznych",
    "section": "",
    "text": "Note\n\n\n\nZrozumienie w≈Ça≈õciwo≈õci zbior√≥w liczbowych jest kluczowe dla uchwycenia natury r√≥≈ºnych typ√≥w danych w naukach spo≈Çecznych.\n\n\n\nPodstawowe Zbiory Liczbowe\n\nLiczby Naturalne (‚Ñï): Liczby u≈ºywane do liczenia obiekt√≥w {0, 1, 2, 3, ‚Ä¶}\nLiczby Ca≈Çkowite (‚Ñ§): ObejmujƒÖ liczby naturalne, ich przeciwno≈õci i zero {‚Ä¶, -2, -1, 0, 1, 2, ‚Ä¶}\nLiczby Wymierne (‚Ñö): Liczby, kt√≥re mo≈ºna wyraziƒá jako u≈Çamek dw√≥ch liczb ca≈Çkowitych\nLiczby Rzeczywiste (‚Ñù): Wszystkie liczby na osi liczbowej, w tym wymierne i niewymierne\n\n\n\nW≈Ça≈õciwo≈õci Zbior√≥w\n\nZbiory Przeliczalne: Zbiory, kt√≥rych elementy mo≈ºna ustawiƒá w relacji jeden do jednego z liczbami naturalnymi. Na przyk≈Çad, zbi√≥r liczb ca≈Çkowitych jest przeliczalny.\nZbiory Nieprzeliczalne: Zbiory, kt√≥re nie sƒÖ przeliczalne. Zbi√≥r liczb rzeczywistych jest nieprzeliczalny.\nZbiory Dyskretne: Zbiory, w kt√≥rych ka≈ºdy element jest oddzielony od innych element√≥w sko≈ÑczonƒÖ przerwƒÖ. Liczby ca≈Çkowite tworzƒÖ zbi√≥r dyskretny.\nZbiory Gƒôste: Zbiory, w kt√≥rych miƒôdzy dowolnymi dwoma elementami zawsze znajduje siƒô inny element zbioru. Liczby wymierne i rzeczywiste sƒÖ zbiorami gƒôstymi.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Typy Danych w Naukach Spo≈Çecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#dane-dyskretne-vs.-ciƒÖg≈Çe",
    "href": "rozdzial2.html#dane-dyskretne-vs.-ciƒÖg≈Çe",
    "title": "4¬† Typy Danych w Naukach Spo≈Çecznych",
    "section": "4.2 Dane Dyskretne vs.¬†CiƒÖg≈Çe",
    "text": "4.2 Dane Dyskretne vs.¬†CiƒÖg≈Çe\nW nauce o danych i statystyce czƒôsto kategoryzujemy zmienne jako dyskretne lub ciƒÖg≈Çe. Jednak rozr√≥≈ºnienie to nie zawsze jest jednoznaczne, a niekt√≥re zmienne wykazujƒÖ cechy obu typ√≥w. Ta sekcja bada koncepcje danych dyskretnych i ciƒÖg≈Çych, ich r√≥≈ºnice oraz interesujƒÖce przypadki zmiennych, kt√≥re mo≈ºna traktowaƒá jako oba typy lub kt√≥re kwestionujƒÖ nasze intuicyjne rozumienie.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDane Dyskretne\nDane dyskretne mogƒÖ przyjmowaƒá tylko okre≈õlone, przeliczalne warto≈õci. Te warto≈õci czƒôsto (ale nie zawsze) sƒÖ liczbami ca≈Çkowitymi.\n\nCechy Danych Dyskretnych:\n\nPrzeliczalne\nCzƒôsto reprezentowane przez liczby ca≈Çkowite\nMogƒÖ byƒá sko≈Ñczone lub niesko≈Ñczone\nBrak warto≈õci miƒôdzy dwoma sƒÖsiednimi punktami danych\n\n\n\nPrzyk≈Çady:\n\nLiczba student√≥w w klasie\nLiczba samochod√≥w sprzedanych przez dealera\nRozmiary but√≥w\n\n\n\n\nDane CiƒÖg≈Çe\nDane ciƒÖg≈Çe mogƒÖ przyjmowaƒá dowolnƒÖ warto≈õƒá w danym zakresie, w tym warto≈õci u≈Çamkowe i dziesiƒôtne. Wa≈ºne jest, aby zauwa≈ºyƒá, ≈ºe ciƒÖg≈Ço≈õƒá nie jest okre≈õlona wy≈ÇƒÖcznie przez nieprzeliczalno≈õƒá, ale r√≥wnie≈º przez gƒôsto≈õƒá.\n\nCechy Danych CiƒÖg≈Çych:\n\nMogƒÖ byƒá nieprzeliczalne (jak liczby rzeczywiste) lub gƒôste (jak liczby wymierne)\nMogƒÖ byƒá mierzone z dowolnƒÖ precyzjƒÖ (teoretycznie)\nReprezentowane przez liczby rzeczywiste lub gƒôste podzbiory liczb rzeczywistych\nZawsze istniejƒÖ warto≈õci miƒôdzy dowolnymi dwoma punktami danych\n\n\n\nPrzyk≈Çady:\n\nWzrost\nWaga\nTemperatura\nProcenty (wyja≈õnione dalej poni≈ºej)\n\n\n\n\nSpektrum Dyskretno-CiƒÖg≈Çe\nW praktyce niekt√≥re zmienne, kt√≥re matematycznie sƒÖ dyskretne, czƒôsto sƒÖ traktowane tak, jakby by≈Çy ciƒÖg≈Çe. Ta dwoista natura zapewnia elastyczno≈õƒá w analizie i interpretacji tych zmiennych.\n\nPowody Traktowania Danych Dyskretnych jako CiƒÖg≈Çych:\n\nGƒôsta Granularno≈õƒá\n\nGdy zmienna dyskretna ma du≈ºƒÖ liczbƒô mo≈ºliwych warto≈õci w danym zakresie, mo≈ºe przybli≈ºaƒá ciƒÖg≈Ço≈õƒá.\nPrzyk≈Çad: Doch√≥d mierzony w pojedynczych groszach. Choƒá technicznie dyskretny, du≈ºa liczba mo≈ºliwych warto≈õci sprawia, ≈ºe zachowuje siƒô podobnie do zmiennej ciƒÖg≈Çej.\n\nWygoda Analityczna\n\nMetody ciƒÖg≈Çe czƒôsto dajƒÖ rozsƒÖdne i u≈ºyteczne wyniki nawet dla gƒôstych zmiennych dyskretnych.\nCzƒôsto ≈Çatwiej jest u≈ºywaƒá istniejƒÖcych narzƒôdzi statystycznych, je≈õli za≈Ço≈ºymy ciƒÖg≈Ço≈õƒá, poniewa≈º pozwala to na stosowanie metod opartych na rachunku r√≥≈ºniczkowym.\n\nPrzybli≈ºenie Zjawisk Bazowych\n\nW niekt√≥rych przypadkach dyskretny pomiar mo≈ºe byƒá przybli≈ºeniem bazowego procesu ciƒÖg≈Çego.\nPrzyk≈Çad: Chocia≈º mierzymy czas w dyskretnych jednostkach (sekundy, minuty, godziny), sam czas jest ciƒÖg≈Çy.\n\n\n\n\nPrzyk≈Çady Zmiennych o Dwoistej Naturze Dyskretno-CiƒÖg≈Çej:\n\nWiek\n\nDyskretny: Typowo mierzony w pe≈Çnych latach\nCiƒÖg≈Çy: Mo≈ºe byƒá uznany za zmiennƒÖ ciƒÖg≈ÇƒÖ w wielu analizach, szczeg√≥lnie przy du≈ºych populacjach\n\nCena i Doch√≥d\n\nDyskretne: Ceny i dochody sƒÖ w rzeczywisto≈õci mierzone w dyskretnych jednostkach (np. grosze lub najmniejsza jednostka waluty)\nCiƒÖg≈Çe: W modelach ekonomicznych i wielu analizach ceny i dochody sƒÖ traktowane jako zmienne ciƒÖg≈Çe ze wzglƒôdu na ich gƒôstƒÖ naturƒô i wygodƒô analitycznƒÖ\n\nWyniki Test√≥w\n\nDyskretne: Czƒôsto podawane jako liczby ca≈Çkowite\nCiƒÖg≈Çe: W analizach statystycznych wyniki test√≥w mogƒÖ byƒá traktowane jako ciƒÖg≈Çe, szczeg√≥lnie gdy zakres mo≈ºliwych wynik√≥w jest du≈ºy\n\n\n\n\n\nPrzypadek Szczeg√≥lny: Procenty i Liczby Wymierne\nProcenty przedstawiajƒÖ interesujƒÖcy przypadek w spektrum dyskretno-ciƒÖg≈Çym:\n\nNatura Wymierna: Procenty sƒÖ zasadniczo u≈Çamkami (m/100), co czyni je liczbami wymiernymi.\nGƒôste, ale Przeliczalne: Zbi√≥r liczb wymiernych jest gƒôsty (miƒôdzy dowolnymi dwoma wymiernymi jest inny wymierny), ale tak≈ºe przeliczalny.\nPraktyczna CiƒÖg≈Ço≈õƒá: W wiƒôkszo≈õci praktycznych zastosowa≈Ñ procenty sƒÖ traktowane jako ciƒÖg≈Çe ze wzglƒôdu na ich gƒôstƒÖ naturƒô.\nSko≈Ñczona Precyzja: W rzeczywisto≈õci procenty sƒÖ czƒôsto podawane z ograniczonƒÖ liczbƒÖ miejsc po przecinku, tworzƒÖc sko≈Ñczony zbi√≥r mo≈ºliwych warto≈õci.\n\n\n\n\n\n\n\nProcenty: ≈ÅƒÖczenie Dyskretnego i CiƒÖg≈Çego\n\n\n\nZmienne mierzone w procentach, takie jak stopy bezrobocia czy frekwencja wyborcza, kwestionujƒÖ nasze intuicyjne rozumienie dyskretno≈õci i ciƒÖg≈Ço≈õci:\n\nSƒÖ liczbami wymiernymi (u≈Çamki z mianownikiem 100), kt√≥re technicznie sƒÖ przeliczalne.\nTworzƒÖ zbi√≥r gƒôsty w swoim zakresie (od 0% do 100%), pozwalajƒÖc na warto≈õci miƒôdzy dowolnymi dwoma procentami.\nW praktyce sƒÖ czƒôsto traktowane jako zmienne ciƒÖg≈Çe ze wzglƒôdu na ich gƒôstƒÖ naturƒô i wygodƒô analitycznƒÖ.\nPrecyzja pomiaru (np. podawanie do jednego lub dw√≥ch miejsc po przecinku) mo≈ºe narzuciƒá dyskretnƒÖ strukturƒô na to, co koncepcyjnie jest zbiorem gƒôstym.\n\nTa dwoisto≈õƒá pozwala na elastyczne podej≈õcia analityczne, w zale≈ºno≈õci od konkretnego kontekstu badawczego i wymaganej precyzji.\n\n\n\n\nImplikacje dla Analizy Danych\nZrozumienie zniuansowanej natury zmiennych jako dyskretnych, ciƒÖg≈Çych lub gdzie≈õ pomiƒôdzy ma wa≈ºne implikacje dla analizy danych:\n\nElastyczno≈õƒá w Modelowaniu: Pozwala na wykorzystanie szerszego zakresu technik statystycznych.\nUproszczone Obliczenia: Traktowanie gƒôstych danych dyskretnych jako ciƒÖg≈Çych mo≈ºe upro≈õciƒá obliczenia i uczyniƒá niekt√≥re analizy bardziej wykonalnymi.\nLepsza Interpretowalno≈õƒá: W niekt√≥rych przypadkach traktowanie danych dyskretnych jako ciƒÖg≈Çych mo≈ºe prowadziƒá do bardziej intuicyjnych lub u≈ºytecznych interpretacji wynik√≥w.\nPotencja≈Ç B≈Çƒôdu: Wa≈ºne jest, aby byƒá ≈õwiadomym, kiedy przybli≈ºenia sƒÖ odpowiednie, a kiedy mogƒÖ prowadziƒá do mylƒÖcych wynik√≥w.\nRozwa≈ºania Teoretyczne vs.¬†Praktyczne: Choƒá matematyczna natura danych jest wa≈ºna, praktyczne wzglƒôdy w pomiarze i analizie czƒôsto kierujƒÖ tym, jak traktujemy zmienne.\n\n\n\nWnioski\nRozr√≥≈ºnienie miƒôdzy danymi dyskretnymi a ciƒÖg≈Çymi nie zawsze jest sztywne w naukach spo≈Çecznych. Wiele zmiennych, w tym te dotyczƒÖce pieniƒôdzy, procent√≥w czy gƒôstych pomiar√≥w, mo≈ºna oglƒÖdaƒá przez pryzmat zar√≥wno dyskretny, jak i ciƒÖg≈Çy. Wyb√≥r sposobu traktowania powinien byƒá kierowany naturƒÖ danych, celami analizy i potencjalnymi implikacjami tego wyboru. Ta elastyczno≈õƒá, gdy jest u≈ºywana rozwa≈ºnie, zapewnia potƒô≈ºne narzƒôdzia dla badaczy nauk spo≈Çecznych do uzyskiwania wglƒÖdu w ich dane.\n\n\n\n\n\n\nDane Dyskretne vs.¬†CiƒÖg≈Çe: Analogia Jƒôzykowa\n\n\n\n\nKluczowe Rozr√≥≈ºnienie Jƒôzykowe\nW jƒôzyku polskim mamy precyzyjne rozr√≥≈ºnienie:\n\n‚ÄúLiczba‚Äù ‚Üí u≈ºywamy dla rzeczy policzalnych\n‚ÄúIlo≈õƒá‚Äù ‚Üí u≈ºywamy dla rzeczy niepoliczalnych\n\nTo rozr√≥≈ºnienie doskonale odzwierciedla dwa podstawowe typy danych liczbowych:\n\n\nDane Dyskretne = ‚ÄúLiczba czego≈õ‚Äù\n\nU≈ºywamy s≈Çowa ‚Äúliczba‚Äù (tak jak m√≥wimy ‚Äúliczba student√≥w‚Äù)\nWarto≈õci sƒÖ rozdzielone jak pojedyncze elementy\nPrzyk≈Çady:\n\nLiczba ksiƒÖ≈ºek: 0, 1, 2, 3‚Ä¶\nLiczba punkt√≥w w te≈õcie: 0, 1, 2‚Ä¶\nLiczba mieszka≈Ñc√≥w: 100, 101, 102‚Ä¶\n\n\nü§î Czy poprawne jest powiedzenie ‚Äúilo≈õƒá student√≥w‚Äù czy ‚Äúliczba student√≥w‚Äù? (Poprawna forma pomo≈ºe Ci rozpoznaƒá typ danych)\n\n\nDane CiƒÖg≈Çe = ‚ÄúIlo≈õƒá czego≈õ‚Äù\n\nU≈ºywamy s≈Çowa ‚Äúilo≈õƒá‚Äù (tak jak m√≥wimy ‚Äúilo≈õƒá wody‚Äù)\nWarto≈õci p≈Çynnie przechodzƒÖ jedna w drugƒÖ\nPrzyk≈Çady:\n\nIlo≈õƒá cieczy: 1,5231‚Ä¶ litra\nIlo≈õƒá czasu: 2,3891‚Ä¶ godziny\nIlo≈õƒá energii: 5,7123‚Ä¶ kWh\n\n\nü§î Czy m√≥wimy ‚Äúilo≈õƒá wody‚Äù czy ‚Äúliczba wody‚Äù? (Poprawna forma wskazuje na typ danych)\n\n\nSpos√≥b Rozpoznawania\n\nCzy u≈ºy≈Çby≈õ s≈Çowa ‚Äúliczba‚Äù? ‚Üí Dane dyskretne\nCzy u≈ºy≈Çby≈õ s≈Çowa ‚Äúilo≈õƒá‚Äù? ‚Üí Dane ciƒÖg≈Çe\n\n‚úçÔ∏è ƒÜwiczenie: Uzupe≈Çnij poprawnym s≈Çowem i okre≈õl typ danych\n\n_____ uczni√≥w w klasie (liczba/ilo≈õƒá): typ _____\n_____ deszczu (liczba/ilo≈õƒá): typ _____\n_____ piosenek (liczba/ilo≈õƒá): typ _____\n_____ temperatury (liczba/ilo≈õƒá): typ _____",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Typy Danych w Naukach Spo≈Çecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "href": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "title": "4¬† Typy Danych w Naukach Spo≈Çecznych",
    "section": "4.3 Wprowadzenie do Typologii Danych Stevensa",
    "text": "4.3 Wprowadzenie do Typologii Danych Stevensa\nStanley S. Stevens, ameryka≈Ñski psycholog, wprowadzi≈Ç system klasyfikacji skal pomiarowych w swoim artykule z 1946 roku ‚ÄúOn the Theory of Scales of Measurement‚Äù. Ten system, znany jako typologia danych Stevensa lub poziomy pomiaru, sta≈Ç siƒô fundamentalny dla zrozumienia, jak r√≥≈ºne typy danych powinny byƒá analizowane i interpretowane.\nStevens zaproponowa≈Ç cztery poziomy pomiaru:\n\nNominalny\nPorzƒÖdkowy\nInterwa≈Çowy\nIlorazowy\n\nKa≈ºdy poziom ma specyficzne w≈Ça≈õciwo≈õci i pozwala na r√≥≈ºne rodzaje operacji statystycznych i analiz.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\nSkala Nominalna\n\nDefinicja\nSkala nominalna jest najbardziej podstawowym poziomem pomiaru. U≈ºywa etykiet lub kategorii do klasyfikacji danych bez ≈ºadnej warto≈õci ilo≈õciowej ani porzƒÖdku.\n\n\nW≈Ça≈õciwo≈õci\n\nKategorie sƒÖ wzajemnie wykluczajƒÖce siƒô\nBrak inherentnego porzƒÖdku miƒôdzy kategoriami\nNie mo≈ºna wykonywaƒá znaczƒÖcych operacji arytmetycznych\n\n\n\nPrzyk≈Çady\n\nNarodowo≈õƒá (Polak, Niemiec, ‚Ä¶)\nGrupy krwi (A, B, AB, O)\nKolor oczu (Niebieskie, BrƒÖzowe, Zielone, Piwne)\nZmienne binarne (‚ÄúSukces‚Äù versus ‚ÄúNiepowodzenie‚Äù)\n\n\n\n\nSkala PorzƒÖdkowa\n\nDefinicja\nSkala porzƒÖdkowa kategoryzuje dane w uporzƒÖdkowane kategorie, ale odstƒôpy miƒôdzy kategoriami niekoniecznie sƒÖ r√≥wne lub znaczƒÖce.\n\n\nW≈Ça≈õciwo≈õci\n\nKategorie majƒÖ zdefiniowany porzƒÖdek\nR√≥≈ºnice miƒôdzy kategoriami nie sƒÖ kwantyfikowalne\nOperacje arytmetyczne na liczbach nie sƒÖ znaczƒÖce\n\n\n\nPrzyk≈Çady\n\nPoziomy wykszta≈Çcenia (Szko≈Ça ≈örednia, Licencjat, Magister, Doktorat)\nSkale Likerta (Zdecydowanie siƒô nie zgadzam, Nie zgadzam siƒô, Neutralnie, Zgadzam siƒô, Zdecydowanie siƒô zgadzam)\nStatus spo≈Çeczno-ekonomiczny (Niski, ≈öredni, Wysoki)\n\n\n\n\nSkala Interwa≈Çowa\n\nDefinicja\nSkala interwa≈Çowa ma uporzƒÖdkowane kategorie z r√≥wnymi odstƒôpami miƒôdzy sƒÖsiednimi kategoriami. Jednak brakuje jej prawdziwego punktu zerowego.\n\n\nW≈Ça≈õciwo≈õci\n\nR√≥wne odstƒôpy miƒôdzy sƒÖsiednimi kategoriami\nBrak prawdziwego punktu zerowego (zero jest umowne)\nStosunki miƒôdzy warto≈õciami nie sƒÖ znaczƒÖce\n\n\n\nPrzyk≈Çady\n\nTemperatura w stopniach Celsjusza lub Fahrenheita\nLata kalendarzowe\nSkala pH\nWysoko≈õƒá nad poziomem morza\n\n\n\n\nSkala Ilorazowa\n\nDefinicja\nSkala ilorazowa jest najwy≈ºszym poziomem pomiaru. Ma wszystkie w≈Ça≈õciwo≈õci skali interwa≈Çowej plus prawdziwy punkt zerowy, co sprawia, ≈ºe stosunki miƒôdzy warto≈õciami sƒÖ znaczƒÖce.\n\n\nW≈Ça≈õciwo≈õci\n\nWszystkie w≈Ça≈õciwo≈õci skal interwa≈Çowych\nPrawdziwy punkt zerowy\nStosunki miƒôdzy warto≈õciami sƒÖ znaczƒÖce\n\n\n\nPrzyk≈Çady\n\nWzrost\nWaga\nWiek\nDoch√≥d\n\n\n\n\nZnaczenie w Badaniach i Analizie\nZrozumienie typologii danych Stevensa jest kluczowe z kilku powod√≥w:\n\nWyb√≥r odpowiednich test√≥w statystycznych: Poziom pomiaru determinuje, kt√≥re analizy statystyczne sƒÖ odpowiednie dla danego zbioru danych.\nInterpretacja wynik√≥w: Znaczenie wynik√≥w statystycznych zale≈ºy od poziomu pomiaru zaanga≈ºowanych zmiennych.\nProjektowanie narzƒôdzi pomiarowych: Przy tworzeniu ankiet lub innych narzƒôdzi pomiarowych badacze muszƒÖ wziƒÖƒá pod uwagƒô poziom pomiaru, kt√≥ry chcƒÖ osiƒÖgnƒÖƒá.\nTransformacja danych: Czasami dane mogƒÖ byƒá przekszta≈Çcane z jednego poziomu na drugi, ale musi to byƒá robione ostro≈ºnie, aby uniknƒÖƒá b≈Çƒôdnej interpretacji.\n\n\n\nKontrowersje i Ograniczenia\nChocia≈º typologia Stevensa jest szeroko stosowana, spotka≈Ça siƒô z pewnymi krytykami:\n\nSztywno≈õƒá: Niekt√≥rzy twierdzƒÖ, ≈ºe typologia jest zbyt sztywna i ≈ºe wiele rzeczywistych pomiar√≥w mie≈õci siƒô pomiƒôdzy tymi kategoriami.\nTraktowanie danych porzƒÖdkowych: Trwa debata na temat tego, kiedy w≈Ça≈õciwe jest traktowanie danych porzƒÖdkowych jako interwa≈Çowych dla pewnych analiz.\nSkalowanie psychologiczne: Niekt√≥re konstrukty psychologiczne (jak inteligencja) sƒÖ trudne do jednoznacznego skategoryzowania w ramach tego systemu.\n\n\n\nPodsumowanie\nTypologia danych Stevensa dostarcza fundamentalnych ram dla zrozumienia r√≥≈ºnych rodzaj√≥w danych i ich w≈Ça≈õciwo≈õci. RozpoznajƒÖc poziom pomiaru swoich zmiennych, badacze mogƒÖ podejmowaƒá ≈õwiadome decyzje dotyczƒÖce gromadzenia danych, analizy i interpretacji. Jednak wa≈ºne jest, aby pamiƒôtaƒá, ≈ºe chocia≈º ta typologia jest u≈ºytecznym przewodnikiem, rzeczywiste dane czƒôsto wymagajƒÖ niuansowego podej≈õcia i nie zawsze pasujƒÖ idealnie do tych kategorii.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not ‚Äútwice as acidic‚Äù as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Typy Danych w Naukach Spo≈Çecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#popularne-skale-porzƒÖdkowe-w-badaniach-behawioralnych",
    "href": "rozdzial2.html#popularne-skale-porzƒÖdkowe-w-badaniach-behawioralnych",
    "title": "4¬† Typy Danych w Naukach Spo≈Çecznych",
    "section": "4.4 Popularne Skale PorzƒÖdkowe w Badaniach Behawioralnych",
    "text": "4.4 Popularne Skale PorzƒÖdkowe w Badaniach Behawioralnych\n\nSkale Likerta\nSkale Likerta sƒÖ szeroko stosowane w psychologii i naukach spo≈Çecznych do pomiaru postaw, opinii i percepcji. Nazwane na cze≈õƒá psychologa Rensisa Likerta, skale te zazwyczaj sk≈ÇadajƒÖ siƒô z serii stwierdze≈Ñ lub pyta≈Ñ, kt√≥re respondenci oceniajƒÖ na skali, czƒôsto od ‚ÄúZdecydowanie siƒô nie zgadzam‚Äù do ‚ÄúZdecydowanie siƒô zgadzam‚Äù.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDlaczego Skale Likerta sƒÖ Zmiennymi PorzƒÖdkowymi\nSkale Likerta sƒÖ uwa≈ºane za zmienne porzƒÖdkowe z kilku powod√≥w:\n\nPorzƒÖdek bez r√≥wnych odstƒôp√≥w: Chocia≈º odpowiedzi majƒÖ wyra≈∫nƒÖ kolejno≈õƒá (np. ‚ÄúZdecydowanie siƒô nie zgadzam‚Äù &lt; ‚ÄúNie zgadzam siƒô‚Äù &lt; ‚ÄúNeutralnie‚Äù &lt; ‚ÄúZgadzam siƒô‚Äù &lt; ‚ÄúZdecydowanie siƒô zgadzam‚Äù), odstƒôpy miƒôdzy tymi kategoriami niekoniecznie sƒÖ r√≥wne.\nSubiektywna interpretacja: R√≥≈ºnica miƒôdzy ‚ÄúZdecydowanie siƒô nie zgadzam‚Äù a ‚ÄúNie zgadzam siƒô‚Äù mo≈ºe nie byƒá taka sama jak r√≥≈ºnica miƒôdzy ‚ÄúZgadzam siƒô‚Äù a ‚ÄúZdecydowanie siƒô zgadzam‚Äù dla wszystkich respondent√≥w.\nBrak prawdziwego punktu zerowego: Skale Likerta zazwyczaj nie majƒÖ prawdziwego punktu zerowego, co jest cechƒÖ charakterystycznƒÖ skal interwa≈Çowych lub ilorazowych.\n\n\n\n\n\n\n\nSkale pomiarowe wed≈Çug typologii Stevensa: zmienne ilo≈õciowe vs.¬†porzƒÖdkowe\n\n\n\n\nCzym sƒÖ zmienne ilo≈õciowe (numeryczne)?\nW typologii Stevensa wyr√≥≈ºniamy cztery skale pomiarowe. Zmienne ilo≈õciowe to te, kt√≥re mierzone sƒÖ na skalach interwa≈Çowych lub ilorazowych:\n\nSkala interwa≈Çowa: Posiada r√≥wne odstƒôpy miƒôdzy jednostkami, ale brak naturalnego punktu zerowego\nSkala ilorazowa: Posiada r√≥wne odstƒôpy miƒôdzy jednostkami oraz naturalny punkt zerowy\n\nZmienne ilo≈õciowe charakteryzujƒÖ siƒô nastƒôpujƒÖcymi w≈Ça≈õciwo≈õciami:\n\nR√≥wne interwa≈Çy: R√≥≈ºnica miƒôdzy 5 a 6 reprezentuje takƒÖ samƒÖ wielko≈õƒá jak r√≥≈ºnica miƒôdzy 95 a 96\nSp√≥jne jednostki: Ka≈ºdy przyrost reprezentuje takƒÖ samƒÖ ilo≈õƒá mierzonej cechy\nObiektywny pomiar: MierzƒÖ rzeczywiste ilo≈õci, a nie tylko wzglƒôdne pozycje\nDopuszczalne operacje matematyczne: Mo≈ºna wykonywaƒá operacje arytmetyczne\n\nPrzyk≈Çady zmiennych mierzonych na skali ilorazowej:\n\nWzrost: 170 cm jest dok≈Çadnie o 10 cm wy≈ºsze ni≈º 160 cm, a 170 cm jest dok≈Çadnie dwa razy wy≈ºsze ni≈º 85 cm\nWaga: R√≥≈ºnica miƒôdzy 50 kg a 60 kg to taka sama ilo≈õƒá wagi jak miƒôdzy 80 kg a 90 kg\nCzas: 4 godziny to dwa razy d≈Çu≈ºej ni≈º 2 godziny, a r√≥≈ºnica miƒôdzy 3 a 4 godzinami jest taka sama jak miƒôdzy 9 a 10 godzinami\nTemperatura w Kelwinach: 200K jest dwa razy cieplejsza ni≈º 100K (poniewa≈º 0K to zero absolutne)\n\nPrzyk≈Çad skali interwa≈Çowej:\n\nTemperatura w stopniach Celsjusza: R√≥≈ºnica miƒôdzy 20¬∞C a 30¬∞C jest taka sama jak miƒôdzy 70¬∞C a 80¬∞C, ale 40¬∞C nie jest ‚Äúdwa razy cieplejsze‚Äù ni≈º 20¬∞C (brak naturalnego zera)\nRok kalendarzowy: R√≥≈ºnica miƒôdzy 2020 a 2021 jest taka sama jak miƒôdzy 1950 a 1951, ale rok 2000 nie jest ‚Äúdwa razy starszy‚Äù ni≈º rok 1000\n\n\n\nDlaczego wyniki IQ sƒÖ zmiennƒÖ porzƒÖdkowƒÖ, a nie ilo≈õciowƒÖ\nMimo liczbowego zapisu, wyniki IQ sƒÖ w istocie zmiennƒÖ porzƒÖdkowƒÖ (w typologii Stevensa), a nie zmiennƒÖ interwa≈ÇowƒÖ, poniewa≈º:\n\nBrak jednolitej jednostki miary: Nie istnieje naturalna jednostka mierzƒÖca ‚Äúinteligencjƒô‚Äù\nKonstrukcja oparta na rangach: Skala powstaje przez uszeregowanie ludzi wzglƒôdem siebie, a nastƒôpnie przekszta≈Çcenie tych rang w wyniki liczbowe\nNier√≥wne interwa≈Çy: R√≥≈ºnica miƒôdzy IQ 100 a 110 niekoniecznie reprezentuje takƒÖ samƒÖ r√≥≈ºnicƒô poznawczƒÖ jak miƒôdzy 130 a 140\nBrak absolutnego zera: Nie istnieje znaczƒÖce pojƒôcie ‚Äúzerowej inteligencji‚Äù\nZale≈ºno≈õƒá od testu: R√≥≈ºne testy IQ mogƒÖ daƒá r√≥≈ºne wyniki dla tej samej osoby\n\nPrzyk≈Çad: Rozwa≈ºmy trzy osoby z wynikami IQ 85, 100 i 115. Choƒá mogliby≈õmy chcieƒá powiedzieƒá, ≈ºe r√≥≈ºnica miƒôdzy pierwszƒÖ a drugƒÖ osobƒÖ r√≥wna siƒô r√≥≈ºnicy miƒôdzy drugƒÖ a trzeciƒÖ, nie jest to faktycznie znaczƒÖce‚Äîzdolno≈õci poznawcze reprezentowane przez te wyniki nie wzrastajƒÖ w r√≥wnych krokach, mimo ≈ºe cyfry sugerujƒÖ r√≥wne odstƒôpy. Wyniki te informujƒÖ nas g≈Ç√≥wnie o pozycji osoby wzglƒôdem innych, co jest cechƒÖ skali porzƒÖdkowej.\nInny przyk≈Çad: Osoba z IQ 140 nie jest ‚Äúdwa razy inteligentniejsza‚Äù ni≈º osoba z IQ 70, mimo ≈ºe stosunek liczb wynosi 2:1. Takie por√≥wnanie nie ma sensu na skali porzƒÖdkowej.\n\n\nPunkty egzaminacyjne - pomiƒôdzy skalƒÖ porzƒÖdkowƒÖ a interwa≈ÇowƒÖ\nChoƒá traktujemy punkty egzaminacyjne jak zmienne ilo≈õciowe (interwa≈Çowe), czƒôsto majƒÖ one cechy zmiennych porzƒÖdkowych:\n\nNier√≥wna trudno≈õƒá pyta≈Ñ: Pytanie za 10 punkt√≥w z fizyki kwantowej nie mierzy tej samej ilo≈õci wiedzy co pytanie za 10 punkt√≥w z podstawowej arytmetyki\nR√≥≈ºne rodzaje kompetencji: R√≥≈ºne pytania testujƒÖ r√≥≈ºne umiejƒôtno≈õci (zapamiƒôtywanie, rozumienie, zastosowanie, analiza)\nSubiektywne przydzielanie punkt√≥w: Punktacja zale≈ºy od oceny egzaminatora, a nie obiektywnych jednostek miary\nBrak addytywno≈õci: Student zdobywajƒÖcy 90 punkt√≥w niekoniecznie jest ‚Äúdwa razy bardziej wykszta≈Çcony‚Äù ni≈º student zdobywajƒÖcy 45 punkt√≥w\n\nPrzyk≈Çad: Wyobra≈∫my sobie dw√≥ch student√≥w:\n\nStudent A: Odpowiada prawid≈Çowo na wszystkie ≈Çatwe i ≈õrednie pytania (zdobywa 75 punkt√≥w)\nStudent B: Odpowiada prawid≈Çowo na wszystkie trudne pytania, ale ≈ºadne ≈Çatwe (zdobywa 75 punkt√≥w)\n\nMimo r√≥wnej punktacji, ich wiedza jest jako≈õciowo r√≥≈ºna. Punkty sugerujƒÖ r√≥wno≈õƒá, ale w rzeczywisto≈õci sƒÖ to r√≥≈ºne profile kompetencji ‚Äî typowy problem ze zmiennymi, kt√≥re nie sƒÖ w pe≈Çni ilo≈õciowe.\n\n\nTraktowanie zmiennych porzƒÖdkowych jako ilo≈õciowych w praktyce\nZe wzglƒôd√≥w praktycznych czƒôsto traktujemy zmienne porzƒÖdkowe jak zmienne ilo≈õciowe, poniewa≈º:\n\nUmo≈ºliwia to stosowanie znanych operacji matematycznych (≈õrednie, odchylenia)\nUpraszcza komunikacjƒô i interpretacjƒô wynik√≥w (‚Äú≈õredni wynik 78%‚Äù)\nPozwala na stosowanie bardziej zaawansowanych metod statystycznych\n\nPrzyk≈Çad: ≈örednia ocen\n\nObliczamy ≈õredniƒÖ, przypisujƒÖc warto≈õci liczbowe ocenom (5, 4, 3, 2, 1)\nTraktujemy te warto≈õci jak zmienne ilo≈õciowe, obliczajƒÖc np. ≈õredniƒÖ 4,5\nAle czy r√≥≈ºnica miƒôdzy ocenƒÖ 5 a 4 (5-4=1) reprezentuje takƒÖ samƒÖ r√≥≈ºnicƒô wiedzy jak miƒôdzy ocenƒÖ 2 a 1 (2-1=1)?\nI czy ≈õrednia 5.0 jest naprawdƒô ‚Äúdwa razy lepsza‚Äù ni≈º ≈õrednia 2.5?\n\nInny przyk≈Çad: Skale Likerta\n\nW ankietach czƒôsto stosujemy skale typu: 1 = ‚Äúzdecydowanie nie zgadzam siƒô‚Äù, 5 = ‚Äúzdecydowanie zgadzam siƒô‚Äù\nObliczamy ≈õrednie odpowiedzi, zak≈ÇadajƒÖc r√≥wne odstƒôpy miƒôdzy kategoriami\nAle czy odleg≈Ço≈õƒá miƒôdzy ‚Äúzdecydowanie nie zgadzam siƒô‚Äù a ‚Äúraczej nie zgadzam siƒô‚Äù jest naprawdƒô taka sama jak miƒôdzy ‚Äúraczej zgadzam siƒô‚Äù a ‚Äúzdecydowanie zgadzam siƒô‚Äù?\n\n\n\nZnaczenie rozr√≥≈ºnienia skal pomiarowych\nRozumienie typologii skal pomiarowych Stevensa ma istotne konsekwencje praktyczne:\n\nZmienne jako≈õciowe porzƒÖdkowe: PozwalajƒÖ na stwierdzenie, ≈ºe co≈õ jest ‚Äúwiƒôksze/lepsze‚Äù lub ‚Äúmniejsze/gorsze‚Äù, ale nie okre≈õlajƒÖ ‚Äúo ile‚Äù (dopuszczalne por√≥wnania typu &gt;, &lt;, =)\nZmienne ilo≈õciowe: PozwalajƒÖ na okre≈õlenie dok≈Çadnych r√≥≈ºnic i proporcji (dopuszczalne operacje +, -, √ó, √∑)\n\n≈öwiadomo≈õƒá ogranicze≈Ñ skali pomiarowej pomaga w poprawnej interpretacji danych i doborze odpowiednich metod analizy.\nPrzyk≈Çad: Je≈õli Anna uzyska≈Ça 75 punkt√≥w na te≈õcie z historii i 85 punkt√≥w na te≈õcie z matematyki, nie mo≈ºemy jednoznacznie stwierdziƒá, ≈ºe jest ‚Äúlepsza z matematyki o 10 jednostek umiejƒôtno≈õci‚Äù. Punkty z r√≥≈ºnych test√≥w nie sƒÖ bezpo≈õrednio por√≥wnywalne, a interwa≈Çy mogƒÖ nie byƒá r√≥wnowa≈ºne.\nPoprawniejsze podej≈õcie: Lepiej por√≥wnaƒá jej wyniki z rozk≈Çadem wynik√≥w innych uczni√≥w. Je≈õli w historii 75 punkt√≥w plasuje jƒÖ w 50. percentylu, a 85 punkt√≥w z matematyki w 90. percentylu, to mo≈ºemy powiedzieƒá, ≈ºe wzglƒôdnie rzecz biorƒÖc, radzi sobie lepiej z matematykƒÖ ni≈º historiƒÖ - co jest wnioskiem opartym na skali porzƒÖdkowej.\n\n\n\n\n\n\nPodsumowanie\nChocia≈º skale Likerta i wiele miar psychologicznych jest czƒôsto traktowanych jako dane interwa≈Çowe ze wzglƒôd√≥w praktycznych, wa≈ºne jest, aby pamiƒôtaƒá o ich porzƒÖdkowym charakterze.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n\n\n\n\nƒÜwiczenie: Identyfikacja Skal Pomiarowych\n\n\n\nDla ka≈ºdej z poni≈ºszych zmiennych okre≈õl najbardziej odpowiedniƒÖ skalƒô pomiaru (Nominalna, PorzƒÖdkowa, Przedzia≈Çowa lub Stosunkowa). Czy zmienna jest dyskretna, czy ciƒÖg≈Ça?\n\nP≈Çeƒá: skala nominalna; zmienna dyskretna;\nSatysfakcja klienta: Niska, ≈örednia, Dobra, Doskona≈Ça\nWzrost (ankieta): ‚ÄúJestem: bardzo niski, niski, przeciƒôtnego wzrostu, wysoki, bardzo wysoki‚Äù\nWzrost mierzony w centymetrach\nCzas reakcji (w milisekundach)\nKody pocztowe: np. 00-001, 00-950, 80-452, 31-072\nWiek (w latach)\nMarki samochod√≥w\nNarodowo≈õƒá\nLiczba dzieci w rodzinie: 1 dziecko, 2 dzieci, 3 dzieci, ‚Ä¶\nWynik testu IQ\nTemperatura (skala Celsjusza)\nTemperatura (skala Kelvina)\nFrekwencja wyborcza\nPrzynale≈ºno≈õƒá partyjna\nWielko≈õƒá okrƒôgu wyborczego\nWsp√≥≈Çrzƒôdne w uk≈Çadzie kartezja≈Ñskim\nData (wzglƒôdem okre≈õlonej epoki, np. n.e.)\nWysoko≈õƒá nad poziomem morza\nGrupy krwi: A, B, AB, 0\nKategorie dochod√≥w: niskie, ≈õrednie, wysokie\nStopnie wojskowe\n\nPamiƒôtaj, aby uzasadniƒá sw√≥j wyb√≥r skali dla ka≈ºdej zmiennej.\nDla przyk≈Çadu: W typologii skal pomiarowych Stevensa, adresy uliczne sƒÖ danymi nominalnymi. Dlaczego?\nPe≈ÇniƒÖ wy≈ÇƒÖcznie funkcjƒô etykiet/identyfikator√≥w Nie majƒÖ naturalnego uporzƒÖdkowania (ul. Mickiewicza 5 nie jest ‚Äúwiƒôksza‚Äù ni≈º ul. S≈Çowackiego 10) Nie mo≈ºna wykonywaƒá na nich sensownych operacji matematycznych Jedyna dozwolona operacja to sprawdzanie r√≥wno≈õci/nier√≥wno≈õci (czy to ten sam adres czy inny?)\nMimo ≈ºe numery dom√≥w sƒÖ liczbami, w systemie adresowym funkcjonujƒÖ jako etykiety, a nie warto≈õci ilo≈õciowe. Liczba 100 w adresie ‚Äúul. Kili≈Ñskiego 100‚Äù nie jest u≈ºywana matematycznie - r√≥wnie dobrze mog≈Çaby to byƒá ‚Äúul. Jab≈Çkowa‚Äù czy ‚Äúul. Zeusa‚Äù, je≈õli chodzi o jej funkcjƒô w adresie.\n\n\n\n\n\n\n\n\nDlaczego Niekt√≥re Statystyki Dzia≈ÇajƒÖ (a Inne Nie) dla Skal Interwa≈Çowych\n\n\n\nSkala interwa≈Çowa to taka, w kt√≥rej odleg≈Ço≈õci miƒôdzy warto≈õciami sƒÖ znaczƒÖce, ale punkt zerowy jest umowny. W przypadku skal interwa≈Çowych (np. temperatury):\n\nDozwolone jest dodawanie/odejmowanie warto≈õci oraz mno≈ºenie/dzielenie przez sta≈Çe.\nNiedozwolone jest mno≈ºenie/dzielenie warto≈õci ze skali przez siebie, poniewa≈º prowadzi to do wynik√≥w bez interpretacji fizycznej.\n\n\nW≈Çasno≈õci Skali Interwa≈Çowej\n\nR√≥wne interwa≈Çy reprezentujƒÖ takie same r√≥≈ºnice:\n\nR√≥≈ºnica miƒôdzy 20¬∞C a 25¬∞C (5¬∞C) reprezentuje takƒÖ samƒÖ zmianƒô jak miƒôdzy 30¬∞C a 35¬∞C.\nProporcje r√≥≈ºnic sƒÖ zachowane: 10¬∞C to dwa razy wiƒôksza zmiana ni≈º 5¬∞C.\n\nPunkt zero jest umowny:\n\n0¬∞C to punkt zamarzania wody, a nie brak temperatury.\nTen sam stan fizyczny ma r√≥≈ºne warto≈õci w r√≥≈ºnych skalach: 0¬∞C = 32¬∞F.\n\nTransformacja liniowa:\n\nWz√≥r og√≥lny: y = ax + b, gdzie a \\neq 0.\nDla temperatury: F = C \\times \\frac{9}{5} + 32.\n\n\n\n\nDlaczego ≈örednia Arytmetyczna Dzia≈Ça\n≈örednia arytmetyczna dzia≈Ça, poniewa≈º opiera siƒô na dodawaniu i dzieleniu przez sta≈ÇƒÖ, kt√≥re sƒÖ dozwolone w skali interwa≈Çowej. Przyk≈Çad:\nDane: 20¬∞C i 30¬∞C\n\nMetoda 1: ≈örednia w Celsjuszach, potem konwersja\n1. ≈örednia: (20¬∞C + 30¬∞C) √∑ 2 = 25¬∞C\n2. Konwersja: 25¬∞C √ó (9/5) + 32 = 77¬∞F\n\nMetoda 2: Konwersja na ¬∞F, potem ≈õrednia\n1. Konwersja: 20¬∞C ‚Üí 68¬∞F, 30¬∞C ‚Üí 86¬∞F\n2. ≈örednia: (68¬∞F + 86¬∞F) √∑ 2 = 77¬∞F\n\nObie metody dajƒÖ ten sam wynik! ‚úì\nMatematyczny dow√≥d poprawno≈õci: \\begin{align}\n\\bar{F} &= \\frac{F_1 + F_2}{2} \\\\\n&= \\frac{(C_1 \\times \\frac{9}{5} + 32) + (C_2 \\times \\frac{9}{5} + 32)}{2} \\\\\n&= \\frac{(C_1 + C_2) \\times \\frac{9}{5} + 64}{2} \\\\\n&= \\left(\\frac{C_1 + C_2}{2}\\right) \\times \\frac{9}{5} + 32 \\\\\n&= \\bar{C} \\times \\frac{9}{5} + 32\n\\end{align}\n\n\nDlaczego Wariancja Jest Problematyczna\nWariancja jest problematyczna, poniewa≈º opiera siƒô na kwadratach r√≥≈ºnic, co prowadzi do jednostek kwadratowych (np. ¬∞C¬≤) bez jasnej interpretacji fizycznej. Przyk≈Çad:\nTe same temperatury: 20¬∞C i 30¬∞C\n\nMetoda 1: Wariancja w Celsjuszach\n1. ≈örednia: 25¬∞C\n2. Odchylenia: (20 - 25)¬∞C = -5¬∞C, (30 - 25)¬∞C = 5¬∞C\n3. Kwadraty odchyle≈Ñ: (-5¬∞C)¬≤ = 25(¬∞C)¬≤, (5¬∞C)¬≤ = 25(¬∞C)¬≤\n4. ≈örednia: (25 + 25)(¬∞C)¬≤ √∑ 2 = 25(¬∞C)¬≤\n\nMetoda 2: Wariancja w Fahrenheitach\n1. Konwersja: 20¬∞C ‚Üí 68¬∞F, 30¬∞C ‚Üí 86¬∞F\n2. ≈örednia: 77¬∞F\n3. Odchylenia: (68 - 77)¬∞F = -9¬∞F, (86 - 77)¬∞F = 9¬∞F\n4. Kwadraty odchyle≈Ñ: (-9¬∞F)¬≤ = 81(¬∞F)¬≤, (9¬∞F)¬≤ = 81(¬∞F)¬≤\n5. ≈örednia: (81 + 81)(¬∞F)¬≤ √∑ 2 = 81(¬∞F)¬≤\n\nProblem: 25(¬∞C)¬≤ i 81(¬∞F)¬≤ nie sƒÖ r√≥wnowa≈ºne!\nMatematyczna analiza problemu: \\begin{align}\n(F_i - \\bar{F})^2 &= [(C_i \\times \\frac{9}{5} + 32) - (\\bar{C} \\times \\frac{9}{5} + 32)]^2 \\\\\n&= [(C_i - \\bar{C}) \\times \\frac{9}{5}]^2 \\\\\n&= (C_i - \\bar{C})^2 \\times \\left(\\frac{9}{5}\\right)^2\n\\end{align}\n\n\nWnioski Teoretyczne\n\nOperacje dozwolone:\n\nDodawanie/odejmowanie (zachowuje r√≥≈ºnice).\nMno≈ºenie/dzielenie przez sta≈Çe (skalowanie).\n≈örednie arytmetyczne.\nPor√≥wnywanie r√≥≈ºnic temperatur.\n\nOperacje niedozwolone:\n\nMno≈ºenie temperatur przez siebie.\nDzielenie temperatur przez siebie.\n≈örednie geometryczne.\nWsp√≥≈Çczynnik zmienno≈õci.\n\nImplikacje praktyczne:\n\nWariancja i odchylenie standardowe wymagajƒÖ ostro≈ºnej interpretacji.\nLepiej u≈ºywaƒá miar opartych na r√≥≈ºnicach (np. MAD - ≈õrednie odchylenie bezwzglƒôdne).\nPrzy por√≥wnywaniu zmienno≈õci warto standaryzowaƒá dane.\n\n\n\n\nZasada Praktyczna\nJe≈õli w obliczeniach pojawia siƒô mno≈ºenie warto≈õci ze skali interwa≈Çowej przez siebie, nale≈ºy zachowaƒá szczeg√≥lnƒÖ ostro≈ºno≈õƒá w interpretacji wynik√≥w!\n\n\n\n\n\n\n\n\n\nProporcje w Skalach Pomiarowych: Przypadek Temperatury\n\n\n\n\nDwa Rodzaje Proporcji\n\nProporcje warto≈õci (NIE zachowujƒÖ siƒô w skali interwa≈Çowej):\nWe≈∫my 80¬∞C i 20¬∞C:\nW Celsjuszach: 80¬∞C/20¬∞C = 4\nW Fahrenheitach: 176¬∞F/68¬∞F ‚âà 2.59\nW Kelwinach: 353.15K/293.15K ‚âà 1.20\n\nTe same temperatury dajƒÖ r√≥≈ºne proporcje! \n‚Üí Proporcje warto≈õci NIE majƒÖ sensu na skalach interwa≈Çowych; sens majƒÖ tylko na skali ilorazowej.\n\n\nProporcje r√≥≈ºnic (zachowujƒÖ siƒô w skali interwa≈Çowej):\nWe≈∫my dwie pary r√≥≈ºnic:\nPara 1: 30¬∞C - 20¬∞C = 10¬∞C\nPara 2: 80¬∞C - 60¬∞C = 20¬∞C\n\nProporcja r√≥≈ºnic w Celsjuszach:\n20¬∞C/10¬∞C = 2\n\nTe same temperatury w Fahrenheitach:\nPara 1: 86¬∞F - 68¬∞F = 18¬∞F\nPara 2: 176¬∞F - 140¬∞F = 36¬∞F\n\nProporcja r√≥≈ºnic w Fahrenheitach:\n36¬∞F/18¬∞F = 2\n\nProporcja r√≥≈ºnic jest taka sama! ‚úì\n\n\n\nMatematyczne Wyja≈õnienie\nDla transformacji F = \\frac{9}{5}C + 32:\n\nProporcje warto≈õci NIE zachowujƒÖ siƒô: [ = ]\nProporcje r√≥≈ºnic zachowujƒÖ siƒô: [ = = ]\n\n\n\nDlaczego To Jest Wa≈ºne?\n\nDla warto≈õci:\n\nW skali Celsjusza: 40¬∞C nie jest ‚Äúdwa razy cieplejsze‚Äù ni≈º 20¬∞C.\nW skali Fahrenheita: 100¬∞F nie jest ‚Äúdwa razy cieplejsze‚Äù ni≈º 50¬∞F.\nTylko w Kelwinach proporcje warto≈õci majƒÖ sens fizyczny.\n\nDla r√≥≈ºnic:\n\nWzrost o 20¬∞C jest zawsze dwa razy wiƒôkszy ni≈º wzrost o 10¬∞C.\nWzrost o 36¬∞F jest zawsze dwa razy wiƒôkszy ni≈º wzrost o 18¬∞F.\nProporcje r√≥≈ºnic sƒÖ niezale≈ºne od skali.\n\n\n\n\nImplikacje dla Statystyk\n\nOperacje bazujƒÖce na r√≥≈ºnicach (DZIA≈ÅAJƒÑ):\n\n≈örednia arytmetyczna.\nOdchylenie bezwzglƒôdne.\nRozstƒôp.\n\nOperacje bazujƒÖce na proporcjach warto≈õci (NIE DZIA≈ÅAJƒÑ):\n\n≈örednia geometryczna.\nWsp√≥≈Çczynnik zmienno≈õci.\nWariancja (bo u≈ºywa kwadratu warto≈õci).",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Typy Danych w Naukach Spo≈Çecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#appendix-a-miary-statystyczne-dla-r√≥≈ºnych-typ√≥w-zmiennych",
    "href": "rozdzial2.html#appendix-a-miary-statystyczne-dla-r√≥≈ºnych-typ√≥w-zmiennych",
    "title": "4¬† Typy Danych w Naukach Spo≈Çecznych",
    "section": "4.5 Appendix A: Miary Statystyczne dla R√≥≈ºnych Typ√≥w Zmiennych",
    "text": "4.5 Appendix A: Miary Statystyczne dla R√≥≈ºnych Typ√≥w Zmiennych\n\n\n\n\n\n\n\n\n\n\n\nTyp zmiennej\nTendencja centralna\nMiary rozproszenia\nWzory\nOdpowiednie testy\nWizualizacja\n\n\n\n\nNominalna\nDominanta (moda)\nWska≈∫nik zr√≥≈ºnicowania\nVR = 1 - \\frac{f_m}{n} gdzie f_m to czƒôsto≈õƒá modalnej, n to liczba obserwacji\nChi-kwadrat, testy dok≈Çadne Fishera\nWykresy s≈Çupkowe, ko≈Çowe\n\n\n\n\nIndeks r√≥≈ºnorodno≈õci (Simpsona)\nD = 1 - \\sum_{i=1}^{k} p_i^2 gdzie p_i to proporcja i-tej kategorii\nTest zgodno≈õci, testy niezale≈ºno≈õci\nMozaikowe wykresy\n\n\n\n\nEntropia Shannona\nH = -\\sum_{i=1}^{k} p_i \\log_2(p_i) gdzie p_i to proporcja i-tej kategorii\nTesty oparte na entropii\nWykresy entropii, dendrogram\n\n\nPorzƒÖdkowa\nMediana\nRozstƒôp\nR = \\max(X) - \\min(X)\nMann-Whitney, Kruskal-Wallis\nWykresy skumulowane, wykresy rozbie≈ºne\n\n\n\n\nRozstƒôp miƒôdzykwartylowy (IQR)\nIQR = Q_3 - Q_1 gdzie Q_1 i Q_3 to pierwszy i trzeci kwartyl\nTesty rangowe, testy mediany\nWykresy pude≈Çkowe\n\n\n\n\nOdchylenie ƒáwiartkowe\nQ_D = \\frac{Q_3 - Q_1}{2}\nTest Jonckheere-Terpstra\nWykresy porzƒÖdkowe, skumulowane histogramy\n\n\nInterwa≈Çowa/ Ilorazowa\n≈örednia arytmetyczna\nWariancja\n\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2 gdzie \\mu to ≈õrednia\nTesty t, ANOVA, regresja\nHistogramy, wykresy rozrzutu\n\n\n\n\nOdchylenie standardowe\n\\sigma = \\sqrt{\\sigma^2}\nF-test, test Levene‚Äôa\nWykresy pude≈Çkowe\n\n\n\n\nWsp√≥≈Çczynnik zmienno≈õci\nCV = \\frac{\\sigma}{\\mu} \\times 100\\%\nTesty r√≥wno≈õci wariancji\nWykresy QQ, wykresy przedzia≈Çowe\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nMedian\nMediana\n-\n‚úì\n‚úì\n‚úì\n\n\nArithmetic Mean\n≈örednia arytmetyczna\n-\n-\n‚úì\n‚úì\n\n\nGeometric Mean\n≈örednia geometryczna\n-\n-\n-\n‚úì\n\n\nHarmonic Mean\n≈örednia harmoniczna\n-\n-\n-\n‚úì\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstƒôp\n-\n‚úì\n‚úì\n‚úì\n\n\nInterquartile Range\nRozstƒôp miƒôdzykwartylowy\n-\n‚úì\n‚úì\n‚úì\n\n\nMean Absolute Deviation\n≈örednie odchylenie bezwzglƒôdne\n-\n-\n‚úì\n‚úì\n\n\nVariance\nWariancja\n-\n-\n‚úì*\n‚úì\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n‚úì*\n‚úì\n\n\nCoefficient of Variation\nWsp√≥≈Çczynnik zmienno≈õci\n-\n-\n-\n‚úì\n\n\nAssociation / Wsp√≥≈Çzale≈ºno≈õƒá:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n‚úì\n‚úì\n‚úì\n\n\nKendall‚Äôs Tau\nTau Kendalla\n-\n‚úì\n‚úì\n‚úì\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n‚úì*\n‚úì\n\n\nCovariance\nKowariancja\n-\n-\n‚úì*\n‚úì\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\nNotes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporzƒÖdkowania\nOrdinal: Ordered categories / Kategorie uporzƒÖdkowane\nInterval: Equal intervals, arbitrary zero / R√≥wne interwa≈Çy, umowne zero\nRatio: Equal intervals, absolute zero / R√≥wne interwa≈Çy, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ‚úì* are commonly used for interval data despite theoretical issues / Niekt√≥re miary oznaczone ‚úì* sƒÖ powszechnie stosowane dla danych przedzia≈Çowych pomimo problem√≥w teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wyb√≥r miary powinien uwzglƒôdniaƒá zar√≥wno poprawno≈õƒá teoretycznƒÖ jak i u≈ºyteczno≈õƒá praktycznƒÖ\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalajƒÖ na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Typy Danych w Naukach Spo≈Çecznych</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "5¬† From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "",
    "text": "5.1 Sampling Methods",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-methods",
    "href": "chapter3.html#sampling-methods",
    "title": "5¬† From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "",
    "text": "Probability Sampling\nProbability sampling methods involve random selection, giving each member of the population a known, non-zero chance of being selected. These methods allow researchers to calculate sampling error and make statistical inferences about the population.\n\nSimple Random Sampling (SRS)\n\nDefinition: Each member of the population has an equal chance of being selected.\nAdvantages: Minimizes selection bias; allows straightforward statistical analysis.\nDisadvantages: Requires a complete sampling frame; may not capture enough members of smaller subgroups.\nExample: To select 100 students from a university with 10,000 students, assign each student a number and use a random number generator to select 100 numbers.\nBest used when: The population is relatively homogeneous and a complete list of population members is available.\n\nStratified Random Sampling\n\nDefinition: The population is divided into mutually exclusive subgroups (strata) based on shared characteristics, then samples are randomly selected from each stratum.\nAdvantages: Ensures representation of key subgroups; can improve precision for same sample size as SRS; allows analysis within and between strata.\nDisadvantages: Requires prior knowledge of population characteristics for stratification; more complex analysis.\nExample: In a national political survey, divide the population into strata based on geographic regions (Northeast, Midwest, South, West) and then randomly sample from each region proportionally to their population size.\nBest used when: The population contains distinct subgroups that might respond differently to the research question.\n\nCluster Sampling\n\nDefinition: The population is divided into clusters (usually geographic), some clusters are randomly selected, and all members within those clusters are studied.\nAdvantages: More cost-effective when population is geographically dispersed; doesn‚Äôt require a complete list of population members.\nDisadvantages: Lower statistical precision than SRS or stratified sampling; clusters must be representative.\nExample: To study high school students‚Äô study habits, randomly select 20 high schools from across the country and survey all students in those schools.\nBest used when: The population is widely dispersed geographically and traveling to all units would be costly.\n\nSystematic Sampling\n\nDefinition: Selecting every kth item from a list after a random start.\nAdvantages: Simple to implement; often more practical than SRS; can avoid neighbor effects.\nDisadvantages: Can introduce bias if there‚Äôs a periodic pattern in the list.\nExample: At a busy shopping mall, survey every 20th person who enters, starting with a randomly chosen number between 1 and 20.\nBest used when: The population is ordered randomly or in a way unrelated to the study variables.\n\nMulti-stage Sampling\n\nDefinition: Combining multiple sampling methods in stages.\nAdvantages: Practical for large-scale surveys; balances cost and precision.\nDisadvantages: Complex design and analysis; multiple stages can compound sampling errors.\nExample: First randomly select counties (cluster sampling), then randomly select households within those counties (simple random sampling), and finally select one adult from each household (systematic sampling).\nBest used when: Studying large, complex populations across wide geographical areas.\n\n\n\n\nNon-probability Sampling\nNon-probability sampling doesn‚Äôt involve random selection, which means statistical inferences about the population must be made with caution. While it can introduce bias, it may be necessary in certain situations.\n\nConvenience Sampling\n\nDefinition: Selecting easily accessible subjects.\nAdvantages: Fast, inexpensive, and easy to implement.\nDisadvantages: High risk of selection bias; limits generalizability.\nExample: A researcher studying college students‚Äô sleep patterns might survey students in their own classes.\nBest used for: Pilot studies, exploratory research, or when resources are severely limited.\n\nPurposive Sampling\n\nDefinition: Selecting subjects based on specific characteristics relevant to the research question.\nAdvantages: Focuses on relevant cases; useful for in-depth studies of specific groups.\nDisadvantages: Researcher bias in selection; limited generalizability.\nExample: For a study on the experiences of CEOs in the tech industry, intentionally seek out and interview CEOs from various tech companies.\nBest used for: Qualitative research, case studies, or studying unique populations.\n\nSnowball Sampling\n\nDefinition: Participants recruit other participants from their networks.\nAdvantages: Access to hard-to-reach or hidden populations; builds on social networks.\nDisadvantages: Sample biased toward those in certain social networks; cannot calculate selection probabilities.\nExample: In a study of undocumented immigrants‚Äô access to healthcare, researchers ask initial participants to refer other potential participants.\nBest used for: Studying rare populations or sensitive topics where no sampling frame exists.\n\nQuota Sampling\n\nDefinition: Selecting participants to meet specific quotas for certain characteristics to match known population parameters.\nAdvantages: Ensures representation of key demographic groups; faster and cheaper than probability sampling; doesn‚Äôt require sampling frame.\nDisadvantages: Non-random selection within quotas can introduce bias; inference is limited.\nExample: In a market research study, researchers ensure they interview specific numbers of people from different age groups, genders, and income levels.\nBest used for: Commercial polling, market research, or when probability sampling is not feasible.\n\n\n\n\nWhy Pollsters Increasingly Use Quota Sampling\nIn recent years, many polling organizations have shifted toward quota sampling approaches for several key reasons:\n\nDeclining Response Rates: Traditional probability-based telephone polls have seen response rates drop from about 36% in the 1990s to less than 10% today. This increases costs and potentially introduces non-response bias that can be worse than selection bias from non-probability methods.\nCoverage Issues: Random digit dialing no longer reaches a representative sample of the population as many people have abandoned landlines for cell phones, and many don‚Äôt answer calls from unknown numbers.\nCost Efficiency: Probability-based polls have become prohibitively expensive as response rates decline, while online panels and quota sampling are more affordable.\nSpeed: In fast-moving political campaigns or rapidly evolving public opinion situations, quota sampling can deliver results much faster than probability methods.\nWeighting and Modeling Improvements: Modern statistical techniques allow pollsters to adjust quota samples to better represent the target population by weighting responses based on known population parameters.\nHybrid Approaches: Many pollsters now use hybrid methods that combine elements of probability and non-probability sampling, with sophisticated weighting and modeling to improve accuracy.\n\nThe 2016 US presidential election, where many polls failed to predict the outcome accurately, led to considerable soul-searching among pollsters. Rather than abandoning quota sampling, many organizations have refined their methods, focusing on better quota definitions, improved weighting techniques, and more transparent reporting of methodological limitations.\nDespite these trends, it‚Äôs important to note that probability sampling remains the gold standard for statistical inference. Well-designed probability samples still provide the most reliable foundation for generalizing from sample to population, especially for academic research where accuracy is prioritized over cost and speed.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#statistical-errors",
    "href": "chapter3.html#statistical-errors",
    "title": "5¬† From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.2 Statistical Errors",
    "text": "5.2 Statistical Errors\n\nSystematic Error vs.¬†Random Error\nSystematic errors and random errors represent fundamentally different ways measurements can deviate from true values:\n\nSystematic Error (Bias)\n\nDefinition: Consistent, predictable deviations from the true value in a specific direction.\nCharacteristics:\n\nResults in measurements that are consistently too high or too low\nDoes not diminish with increased sample size\nCan often be corrected if identified\n\nExamples:\n\nA scale that consistently reads 2 pounds heavier than actual weight\nSelection bias from excluding certain population groups\nLeading questions in a survey that push respondents toward particular answers\n\nDetection and Correction:\n\nCalibration against known standards\nComparison with measurements using different methods\nCareful research design and pretesting\n\n\nRandom Error\n\nDefinition: Unpredictable fluctuations in measurements due to chance factors.\nCharacteristics:\n\nVaries in both magnitude and direction across measurements\nFollows a probability distribution (often normal)\nDecreases as sample size increases (‚àön relationship)\n\nExamples:\n\nNatural biological variations between subjects\nMinor fluctuations in measuring instruments\nEnvironmental conditions that vary randomly during measurement\n\nDetection and Reduction:\n\nIncreasing sample size\nTaking repeated measurements\nImproving measurement precision\n\n\n\n\n\nSampling Errors vs.¬†Non-Sampling Errors\nThis distinction focuses on the source of errors in statistical studies:\n\nSampling Error\n\nDefinition: Difference between a sample statistic and the true population parameter due to random variation in selection.\nCharacteristics:\n\nUnavoidable in any sample-based study\nQuantifiable through statistical measures (standard error, confidence intervals)\nDecreases predictably as sample size increases\nAlways random in nature (not systematic)\n\nExamples:\n\nA random sample showing 52% support for a policy when the true population value is 50%\nVariation in mean income across different random samples from the same population\n\nControl Methods:\n\nIncreasing sample size\nUsing more efficient sampling designs (e.g., stratification)\nStatistical adjustments like weighting\n\n\nNon-Sampling Error\n\nDefinition: All errors not attributable to sampling fluctuation, occurring in both sample surveys and complete censuses.\nCharacteristics:\n\nCan be systematic or random\nOften harder to detect and quantify than sampling error\nDoesn‚Äôt necessarily decrease with larger samples\nPotentially more damaging to research validity than sampling error\n\nTypes and Examples:\n\nCoverage Error (systematic): Using a voter registration list that omits recent registrants\nNonresponse Error (can be systematic or random): Higher refusal rates among certain demographics\nMeasurement Error (can be systematic or random):\n\nSystematic: Poorly worded questions that consistently bias responses\nRandom: Occasional data entry mistakes\n\nProcessing Error (can be systematic or random):\n\nSystematic: Consistently miscoding certain responses\nRandom: Occasional transcription errors\n\n\n\n\n\n\nThe Total Survey Error Framework\nModern survey methodology approaches error through the Total Survey Error (TSE) framework, which considers all sources of error that can affect survey quality:\n\nRepresentation Errors (affecting who is included):\n\nCoverage error\nSampling error\nNonresponse error\nAdjustment error (from weighting/imputation)\n\nMeasurement Errors (affecting the accuracy of responses):\n\nSpecification error (measuring the wrong concept)\nMeasurement error (respondent, interviewer, questionnaire effects)\nProcessing error (data entry, coding, editing)\n\n\nUnderstanding the interplay between these error types is crucial for designing high-quality research and appropriately interpreting results. While sampling error receives considerable attention because it‚Äôs easier to quantify, non-sampling errors often pose greater threats to validity in modern research.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff\n\n\nFigure: The relationship between model complexity, bias, and variance. This illustrates how the balance between systematic error (bias) and random error (variance) affects overall model error.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html",
    "href": "rozdzial3.html",
    "title": "6¬† Od Pr√≥by do Populacji - Zrozumienie Losowo≈õci, Pobierania Pr√≥by i Wnioskowania",
    "section": "",
    "text": "Metody Doboru Pr√≥by\n\n\nDob√≥r Probabilistyczny (Losowy)\nMetody doboru probabilistycznego opierajƒÖ siƒô na losowym doborze, gdzie ka≈ºdy cz≈Çonek populacji ma znanƒÖ, niezerowƒÖ szansƒô na wyb√≥r. Metody te pozwalajƒÖ badaczom obliczyƒá b≈ÇƒÖd pr√≥by i wyciƒÖgaƒá wnioski statystyczne o populacji.\n\nProsty Dob√≥r Losowy (SRS)\n\nDefinicja: Ka≈ºdy cz≈Çonek populacji ma r√≥wnƒÖ szansƒô na wyb√≥r.\nZalety: Minimalizuje b≈ÇƒÖd selekcji; umo≈ºliwia prostƒÖ analizƒô statystycznƒÖ.\nWady: Wymaga kompletnej operatu losowania; mo≈ºe nie uchwyciƒá wystarczajƒÖcej liczby cz≈Çonk√≥w mniejszych podgrup.\nPrzyk≈Çad: Aby wybraƒá 100 student√≥w z uniwersytetu liczƒÖcego 10 000 student√≥w, przypisz ka≈ºdemu studentowi numer i u≈ºyj generatora liczb losowych do wybrania 100 numer√≥w.\nNajlepsze zastosowanie: Gdy populacja jest wzglƒôdnie jednorodna i dostƒôpna jest kompletna lista cz≈Çonk√≥w populacji.\n\nDob√≥r Losowy Warstwowy\n\nDefinicja: Populacja jest podzielona na wzajemnie wykluczajƒÖce siƒô podgrupy (warstwy) na podstawie wsp√≥lnych cech, a nastƒôpnie pr√≥bki sƒÖ losowo wybierane z ka≈ºdej warstwy.\nZalety: Zapewnia reprezentacjƒô kluczowych podgrup; mo≈ºe poprawiƒá precyzjƒô dla tej samej wielko≈õci pr√≥by co SRS; umo≈ºliwia analizƒô wewnƒÖtrz i miƒôdzy warstwami.\nWady: Wymaga wcze≈õniejszej znajomo≈õci cech populacji do stratyfikacji; bardziej z≈Ço≈ºona analiza.\nPrzyk≈Çad: W krajowym sonda≈ºu politycznym, podziel populacjƒô na warstwy wed≈Çug region√≥w geograficznych (P√≥≈Çnocny-Wsch√≥d, P√≥≈Çnocny-Zach√≥d, Po≈Çudnie, itp.) i nastƒôpnie losowo pobierz pr√≥bki z ka≈ºdego regionu proporcjonalnie do ich wielko≈õci populacji.\nNajlepsze zastosowanie: Gdy populacja zawiera wyra≈∫ne podgrupy, kt√≥re mogƒÖ reagowaƒá r√≥≈ºnie na pytanie badawcze.\n\nDob√≥r Grupowy (Klastrowy)\n\nDefinicja: Populacja jest podzielona na grupy (zwykle geograficzne), niekt√≥re grupy sƒÖ losowo wybierane, a wszyscy cz≈Çonkowie w tych grupach sƒÖ badani.\nZalety: Bardziej op≈Çacalny gdy populacja jest geograficznie rozproszona; nie wymaga kompletnej listy cz≈Çonk√≥w populacji.\nWady: Ni≈ºsza precyzja statystyczna ni≈º SRS lub dob√≥r warstwowy; klastry muszƒÖ byƒá reprezentatywne.\nPrzyk≈Çad: Aby zbadaƒá nawyki uczenia siƒô uczni√≥w szk√≥≈Ç ≈õrednich, losowo wybierz 20 szk√≥≈Ç ≈õrednich z ca≈Çego kraju i zbadaj wszystkich uczni√≥w w tych szko≈Çach.\nNajlepsze zastosowanie: Gdy populacja jest szeroko rozproszona geograficznie, a dotarcie do wszystkich jednostek by≈Çoby kosztowne.\n\nDob√≥r Systematyczny\n\nDefinicja: Wybieranie co k-tego elementu z listy po losowym starcie.\nZalety: Prosty w implementacji; czƒôsto bardziej praktyczny ni≈º SRS; mo≈ºe uniknƒÖƒá efekt√≥w sƒÖsiedztwa.\nWady: Mo≈ºe wprowadziƒá b≈ÇƒÖd, je≈õli w li≈õcie wystƒôpuje okresowy wzorzec.\nPrzyk≈Çad: W zat≈Çoczonym centrum handlowym, badaj co 20. osobƒô, kt√≥ra wchodzi, zaczynajƒÖc od losowo wybranego numeru miƒôdzy 1 a 20.\nNajlepsze zastosowanie: Gdy populacja jest uporzƒÖdkowana losowo lub w spos√≥b niezwiƒÖzany ze zmiennymi badania.\n\nDob√≥r Wielostopniowy\n\nDefinicja: ≈ÅƒÖczenie wielu metod pr√≥bkowania w etapach.\nZalety: Praktyczny dla bada≈Ñ na du≈ºƒÖ skalƒô; r√≥wnowa≈ºy koszty i precyzjƒô.\nWady: Z≈Ço≈ºony projekt i analiza; wiele etap√≥w mo≈ºe kumulowaƒá b≈Çƒôdy pr√≥bkowania.\nPrzyk≈Çad: Najpierw losowo wybierz powiaty (dob√≥r klastrowy), nastƒôpnie losowo wybierz gospodarstwa domowe w tych powiatach (prosty dob√≥r losowy), a na ko≈Ñcu wybierz jednego doros≈Çego z ka≈ºdego gospodarstwa (dob√≥r systematyczny).\nNajlepsze zastosowanie: Badanie du≈ºych, z≈Ço≈ºonych populacji na rozleg≈Çych obszarach geograficznych.\n\n\n\n\nDob√≥r Nieprobabilistyczny (Nielosowy)\nDob√≥r nieprobabilistyczny nie opiera siƒô na losowym wyborze, co oznacza, ≈ºe wnioskowanie statystyczne o populacji musi byƒá dokonywane z ostro≈ºno≈õciƒÖ. Chocia≈º mo≈ºe wprowadzaƒá b≈ÇƒÖd, w niekt√≥rych sytuacjach jest niezbƒôdny.\n\nDob√≥r Przypadkowy (Convenience Sampling)\n\nDefinicja: Wybieranie ≈Çatwo dostƒôpnych podmiot√≥w.\nZalety: Szybki, niedrogi i ≈Çatwy w implementacji.\nWady: Wysokie ryzyko b≈Çƒôdu selekcji; ograniczona mo≈ºliwo≈õƒá uog√≥lniania.\nPrzyk≈Çad: Badacz studiujƒÖcy wzorce snu student√≥w mo≈ºe ankietowaƒá student√≥w ze swoich w≈Çasnych zajƒôƒá.\nNajlepsze zastosowanie: Badania pilota≈ºowe, badania eksploracyjne lub gdy zasoby sƒÖ znacznie ograniczone.\n\nDob√≥r Celowy\n\nDefinicja: Wybieranie podmiot√≥w na podstawie okre≈õlonych cech istotnych dla pytania badawczego.\nZalety: Koncentruje siƒô na istotnych przypadkach; przydatny do dog≈Çƒôbnych bada≈Ñ okre≈õlonych grup.\nWady: B≈ÇƒÖd badacza w wyborze; ograniczona mo≈ºliwo≈õƒá uog√≥lniania.\nPrzyk≈Çad: W badaniu do≈õwiadcze≈Ñ dyrektor√≥w generalnych w bran≈ºy technologicznej, celowo poszukuj i przeprowadzaj wywiady z dyrektorami r√≥≈ºnych firm technologicznych.\nNajlepsze zastosowanie: Badania jako≈õciowe, studia przypadk√≥w lub badanie unikalnych populacji.\n\nDob√≥r Kuli ≈önie≈ºnej\n\nDefinicja: Uczestnicy rekrutujƒÖ innych uczestnik√≥w ze swoich sieci.\nZalety: Dostƒôp do trudno dostƒôpnych lub ukrytych populacji; bazuje na sieciach spo≈Çecznych.\nWady: Pr√≥ba stronnicza w kierunku os√≥b w okre≈õlonych sieciach spo≈Çecznych; nie mo≈ºna obliczyƒá prawdopodobie≈Ñstwa wyboru.\nPrzyk≈Çad: W badaniu dostƒôpu nielegalnych imigrant√≥w do opieki zdrowotnej, badacze proszƒÖ poczƒÖtkowych uczestnik√≥w o polecenie innych potencjalnych uczestnik√≥w.\nNajlepsze zastosowanie: Badanie rzadkich populacji lub wra≈ºliwych temat√≥w, gdzie nie istnieje operat losowania.\n\nDob√≥r Kwotowy\n\nDefinicja: Wybieranie uczestnik√≥w w celu spe≈Çnienia okre≈õlonych kwot dla pewnych cech, aby dopasowaƒá do znanych parametr√≥w populacji.\nZalety: Zapewnia reprezentacjƒô kluczowych grup demograficznych; szybszy i ta≈Ñszy ni≈º pr√≥bkowanie probabilistyczne; nie wymaga operatu losowania.\nWady: Nielosowy wyb√≥r w ramach kwot mo≈ºe wprowadziƒá b≈ÇƒÖd; wnioskowanie jest ograniczone.\nPrzyk≈Çad: W badaniu rynkowym, badacze upewniajƒÖ siƒô, ≈ºe przeprowadzajƒÖ wywiady z okre≈õlonƒÖ liczbƒÖ os√≥b z r√≥≈ºnych grup wiekowych, p≈Çci i poziom√≥w dochod√≥w.\nNajlepsze zastosowanie: Komercyjne sonda≈ºe, badania rynkowe lub gdy pr√≥bkowanie probabilistyczne nie jest wykonalne.\n\n\n\n\nPr√≥by Kwotowe vs.¬†Pr√≥by Warstwowe: Por√≥wnanie i Praktyczne Zastosowania\nChoƒá na pierwszy rzut oka pr√≥bkowanie warstwowe i kwotowe mo≈ºe wydawaƒá siƒô podobne, istniejƒÖ miƒôdzy nimi fundamentalne r√≥≈ºnice:\n\nKluczowe r√≥≈ºnice miƒôdzy doborem warstwowym a kwotowym:\n\nPodstawa metodologiczna:\n\nDob√≥r warstwowy: Jest metodƒÖ probabilistycznƒÖ, gdzie po podziale na warstwy, jednostki w ka≈ºdej warstwie sƒÖ wybierane losowo.\nDob√≥r kwotowy: Jest metodƒÖ nieprobabilistycznƒÖ, gdzie badacz lub ankieter ma swobodƒô wyboru konkretnych jednostek, o ile spe≈Çnione sƒÖ za≈Ço≈ºone kwoty.\n\nMo≈ºliwo≈õƒá wnioskowania statystycznego:\n\nDob√≥r warstwowy: Pozwala na obliczenie b≈Çƒôdu pr√≥bkowania i przedzia≈Ç√≥w ufno≈õci, umo≈ºliwiajƒÖc formalne wnioskowanie statystyczne.\nDob√≥r kwotowy: Nie pozwala na obliczenie b≈Çƒôdu pr√≥bkowania, co ogranicza mo≈ºliwo≈õci formalnego wnioskowania statystycznego.\n\nKontrola procesu doboru:\n\nDob√≥r warstwowy: Ka≈ºdy etap procesu doboru jest kontrolowany przez badacza ‚Äì od definicji warstw po losowy wyb√≥r jednostek w warstwach.\nDob√≥r kwotowy: Ostateczny wyb√≥r respondent√≥w pozostaje w rƒôkach ankieter√≥w, co mo≈ºe wprowadzaƒá nie≈õwiadome obciƒÖ≈ºenia.\n\nPraktyczne wdro≈ºenie:\n\nDob√≥r warstwowy: Wymaga operatu losowania (kompletnej listy populacji) do przeprowadzenia losowania.\nDob√≥r kwotowy: Nie wymaga operatu losowania, a jedynie znajomo≈õci rozk≈Çadu kluczowych cech w populacji.\n\n\n\n\nJak dok≈Çadnie powstaje pr√≥ba kwotowa w badaniach CATI lub CAPI\nProces tworzenia pr√≥by kwotowej w badaniach CATI (Computer-Assisted Telephone Interviewing) lub CAPI (Computer-Assisted Personal Interviewing) obejmuje nastƒôpujƒÖce etapy:\n\nEtap planowania i przygotowania:\n\nOkre≈õlenie zmiennych kwotowych: Najczƒô≈õciej sƒÖ to podstawowe zmienne demograficzne: p≈Çeƒá, wiek, wykszta≈Çcenie, miejsce zamieszkania (miasto/wie≈õ), region.\nUstalenie wielko≈õci kwot: Na podstawie danych GUS lub innych wiarygodnych ≈∫r√≥de≈Ç danych (np. Diagnoza Spo≈Çeczna) okre≈õla siƒô, jaki procent populacji stanowiƒÖ poszczeg√≥lne kategorie.\nPrzygotowanie tabeli kwotowej: Tworzy siƒô wielowymiarowƒÖ macierz kwot, np. ile powinno byƒá kobiet w wieku 18-29 lat z wy≈ºszym wykszta≈Çceniem mieszkajƒÖcych na wsi w wojew√≥dztwie mazowieckim.\n\nPrzygotowanie operacyjne badania CATI:\n\nPrzygotowanie bazy telefonicznej: W przypadku CATI tworzy siƒô bazƒô numer√≥w telefonicznych (stacjonarnych i/lub kom√≥rkowych).\nLosowanie numer√≥w z puli: Czƒôsto stosuje siƒô metodƒô RDD (Random Digit Dialing) dla telefon√≥w kom√≥rkowych lub losowanie z ksiƒÖ≈ºek telefonicznych (coraz rzadziej) dla telefon√≥w stacjonarnych.\nPrzypisanie numer√≥w do zespo≈Ç√≥w ankieterskich: System CATI dystrybuuje numery do ankieter√≥w.\n\nRealizacja badania CATI:\n\nPytania filtrujƒÖce: Na poczƒÖtku rozmowy ankieter zadaje pytania o wiek, p≈Çeƒá i inne zmienne kwotowe.\nDecyzja o kontynuacji: System CATI na bie≈ºƒÖco monitoruje wype≈Çnienie kwot i decyduje, czy dana osoba kwalifikuje siƒô do badania (czy jej profil demograficzny jest jeszcze potrzebny w pr√≥bie).\nRealizacja wywiadu: Je≈õli respondent pasuje do wciƒÖ≈º otwartej kwoty, przeprowadzany jest wywiad.\nAutomatyczne zamykanie wype≈Çnionych kwot: Gdy dana kwota zostaje wype≈Çniona, system przestaje przyjmowaƒá nowych respondent√≥w o tym profilu.\nRejestracja odm√≥w udzia≈Çu w badaniu: System rejestruje odmowy wed≈Çug ich typu (odmowa na etapie wprowadzenia, odmowa po pytaniach filtrujƒÖcych, przerwanie wywiadu) oraz dane demograficzne, je≈õli zosta≈Çy zebrane przed odmowƒÖ.\n\nPrzygotowanie operacyjne badania CAPI:\n\nWyb√≥r lokalizacji: Wybiera siƒô punkty realizacji badania, czƒôsto stratyfikowane wed≈Çug region√≥w, wielko≈õci miejscowo≈õci itd.\nInstrukcje dla ankieter√≥w: Ankieterzy otrzymujƒÖ szczeg√≥≈Çowe instrukcje dotyczƒÖce kwot, kt√≥re muszƒÖ wype≈Çniƒá w swoim rejonie.\n\nRealizacja badania CAPI:\n\nScreener: Ankieter u≈ºywa kr√≥tkiego kwestionariusza selekcyjnego do okre≈õlenia, czy dana osoba spe≈Çnia kryteria kwotowe.\nDob√≥r respondenta: Ankieter sam decyduje, kogo zapytaƒá o udzia≈Ç w badaniu, kierujƒÖc siƒô wytycznymi kwotowymi.\nMonitorowanie realizacji kwot: Ankieterzy regularnie raportujƒÖ zrealizowane wywiady, a koordynator badania monitoruje wype≈Çnienie kwot.\n\nKontrola jako≈õci i analiza odm√≥w:\n\nWeryfikacja wywiad√≥w: Losowo wybrane wywiady sƒÖ weryfikowane przez ponowny kontakt z respondentem.\nKontrola pracy ankieter√≥w: W badaniach CAPI czƒôsto stosuje siƒô geolokalizacjƒô ankieter√≥w, ≈ºeby potwierdziƒá, ≈ºe faktycznie byli w deklarowanych lokalizacjach.\nKontrola ‚Äúefektu ankietera‚Äù: Analizuje siƒô, czy okre≈õleni ankieterzy nie majƒÖ systematycznie odmiennych wynik√≥w.\nAnaliza wska≈∫nika odpowiedzi (response rate): Oblicza siƒô stosunek zrealizowanych wywiad√≥w do wszystkich nawiƒÖzanych kontakt√≥w.\nAnaliza struktury odm√≥w: Sprawdza siƒô, czy odmowy nie sƒÖ systematycznie powiƒÖzane z okre≈õlonymi cechami demograficznymi, co mog≈Çoby wprowadziƒá b≈ÇƒÖd.\n\nWa≈ºenie ko≈Ñcowe (po realizacji badania):\n\nKorekta nier√≥wnomiernej realizacji kwot: Nawet przy najstaranniejszym doborze kwotowym rzadko udaje siƒô idealnie odwzorowaƒá strukturƒô populacji, dlatego stosuje siƒô wa≈ºenie danych po zako≈Ñczeniu zbierania wywiad√≥w.\nKalibracja do znanych parametr√≥w populacji: Pr√≥bƒô kalibruje siƒô do dok≈Çadnych danych z GUS lub innych wiarygodnych ≈∫r√≥de≈Ç, przypisujƒÖc odpowiednie wagi poszczeg√≥lnym respondentom.\nMetody wa≈ºenia: Najczƒô≈õciej stosuje siƒô wa≈ºenie brzegowe (rim weighting) lub iteracyjne dopasowywanie (raking), kt√≥re pozwalajƒÖ jednocze≈õnie dopasowaƒá pr√≥bƒô do wielu zmiennych demograficznych.\n\n\n\n\nPraktyczny przyk≈Çad realizacji badania CATI z doborem kwotowym w Polsce:\n\nCel badania: Og√≥lnopolski sonda≈º opinii na temat systemu edukacji, n=1000 wywiad√≥w.\nZmienne kwotowe:\n\nP≈Çeƒá: 52% kobiety, 48% mƒô≈ºczy≈∫ni\nWiek: 18-29 lat (18%), 30-44 lat (29%), 45-59 lat (25%), 60+ lat (28%)\nWykszta≈Çcenie: Podstawowe/gimnazjalne (18%), Zasadnicze zawodowe (23%), ≈örednie (35%), Wy≈ºsze (24%)\nWielko≈õƒá miejscowo≈õci: Wie≈õ (39%), Miasto do 50 tys. (23%), Miasto 50-200 tys. (16%), Miasto 200+ tys. (22%)\nRegion: Poszczeg√≥lne wojew√≥dztwa zgodnie z proporcjami GUS\n\nPrzebieg badania:\n\nSystem CATI losuje numery telefon√≥w i przydziela ankieterom\nAnkieter przeprowadza wywiad, je≈õli respondent spe≈Çnia kryteria kwotowe i siƒô zgadza\nSystem monitoruje wype≈Çnienie kwot i automatycznie zamyka te, kt√≥re osiƒÖgnƒô≈Çy zak≈ÇadanƒÖ liczebno≈õƒá\nW trakcie realizacji badania okazuje siƒô, ≈ºe najtrudniej dotrzeƒá do mƒô≈ºczyzn z wykszta≈Çceniem zasadniczym zawodowym w wieku 45-59 lat ‚Äì ankieterom przydzielane sƒÖ dodatkowe godziny na poszukiwanie respondent√≥w o tym profilu\nPo zako≈Ñczeniu zbierania danych, pr√≥ba jest wa≈ºona, aby skorygowaƒá niewielkie odchylenia od za≈Ço≈ºonych kwot\n\n\n\n\n\nDlaczego O≈õrodki Badania Opinii Coraz Czƒô≈õciej StosujƒÖ Dob√≥r Kwotowy\nW ostatnich latach wiele o≈õrodk√≥w badania opinii publicznej przesz≈Ço na metody doboru kwotowego z kilku kluczowych powod√≥w:\n\nSpadajƒÖce Wska≈∫niki Realizacji: Tradycyjne badania telefoniczne oparte na losowym doborze odnotowa≈Çy spadek wska≈∫nik√≥w odpowiedzi z oko≈Ço 36% w latach 90. do mniej ni≈º 10% obecnie. Zwiƒôksza to koszty i potencjalnie wprowadza b≈ÇƒÖd zwiƒÖzany z odmowami, kt√≥ry mo≈ºe byƒá gorszy ni≈º b≈ÇƒÖd selekcji wynikajƒÖcy z metod nielosowych.\nProblemy z Dotarciem do Respondent√≥w: Losowe wybieranie numer√≥w telefonicznych nie zapewnia ju≈º reprezentatywnej pr√≥by populacji, poniewa≈º wielu ludzi zrezygnowa≈Ço z telefon√≥w stacjonarnych na rzecz kom√≥rkowych, a wielu nie odbiera po≈ÇƒÖcze≈Ñ od nieznanych numer√≥w.\nEfektywno≈õƒá Kosztowa: Badania oparte na losowym doborze sta≈Çy siƒô niezwykle drogie wraz ze spadkiem wska≈∫nik√≥w odpowiedzi, podczas gdy panele internetowe i dob√≥r kwotowy sƒÖ bardziej przystƒôpne cenowo.\nSzybko≈õƒá: W szybko zmieniajƒÖcych siƒô kampaniach politycznych lub gwa≈Çtownie ewoluujƒÖcych sytuacjach spo≈Çecznych, dob√≥r kwotowy mo≈ºe dostarczyƒá wyniki znacznie szybciej ni≈º metody losowe.\nUdoskonalenia w Technikach Wa≈ºenia: Nowoczesne metody statystyczne pozwalajƒÖ badaczom dostosowaƒá pr√≥by kwotowe, aby lepiej reprezentowa≈Çy populacjƒô docelowƒÖ poprzez wa≈ºenie odpowiedzi w oparciu o znane parametry populacji.\nPodej≈õcia Mieszane: Wielu badaczy stosuje obecnie metody mieszane, kt√≥re ≈ÇƒÖczƒÖ elementy doboru losowego i nielosowego, z zaawansowanym wa≈ºeniem i modelowaniem w celu poprawy dok≈Çadno≈õci.\n\n\nWp≈Çyw odm√≥w na jako≈õƒá pr√≥by i praktyczne problemy realizacji bada≈Ñ\n\nProblem odm√≥w udzia≈Çu w badaniach\nOdmowy udzia≈Çu w badaniu stanowiƒÖ jedno z najwiƒôkszych wyzwa≈Ñ wsp√≥≈Çczesnej metodologii badawczej i majƒÖ istotny wp≈Çyw na jako≈õƒá uzyskiwanych wynik√≥w:\n\nSkala zjawiska:\n\nW klasycznych badaniach kwestionariuszowych poziom realizacji (response rate) w Polsce spad≈Ç z oko≈Ço 70-80% w latach 90. do 30-40% obecnie.\nW badaniach telefonicznych CATI wsp√≥≈Çczynnik odpowiedzi wynosi czƒôsto zaledwie 5-15%.\nW badaniach internetowych na panelach wska≈∫nik odpowiedzi mo≈ºe wynosiƒá 20-30%, ale w≈õr√≥d respondent√≥w dobieranych z og√≥≈Çu populacji spada nawet do 1-2%.\n\nTypy odm√≥w:\n\nOdmowy ‚Äútwarde‚Äù ‚Äì kategoryczna odmowa udzia≈Çu w jakimkolwiek badaniu.\nOdmowy ‚Äúmiƒôkkie‚Äù ‚Äì wym√≥wki typu ‚Äúnie mam czasu‚Äù, ‚Äújestem zajƒôty‚Äù, kt√≥re mogƒÖ wynikaƒá z niechƒôci do tematu badania.\nOdmowy selektywne ‚Äì odmowy udzia≈Çu w badaniach na okre≈õlone tematy (np. polityczne, dotyczƒÖce zdrowia).\nNiedostƒôpno≈õƒá ‚Äì niemo≈ºno≈õƒá nawiƒÖzania kontaktu z wylosowanƒÖ osobƒÖ mimo wielokrotnych pr√≥b.\n\nKonsekwencje dla jako≈õci bada≈Ñ:\n\nB≈ÇƒÖd systematyczny ‚Äì je≈õli osoby odmawiajƒÖce udzia≈Çu systematycznie r√≥≈ºniƒÖ siƒô od os√≥b uczestniczƒÖcych w badaniu pod wzglƒôdem kluczowych zmiennych.\nZawƒô≈ºenie pr√≥by do ‚Äúzawodowych respondent√≥w‚Äù ‚Äì szczeg√≥lnie w panelach internetowych, gdzie uczestniczƒÖ g≈Ç√≥wnie osoby chƒôtne do udzia≈Çu w wielu badaniach.\nNadreprezentacja okre≈õlonych grup ‚Äì np. emeryt√≥w i rencist√≥w, kt√≥rzy czƒô≈õciej majƒÖ czas i chƒôƒá na udzia≈Ç w badaniach osobistych.\n\nMetody radzenia sobie z odmowami:\n\nPonowne pr√≥by kontaktu ‚Äì standardem jest wykonanie minimum 3-4 pr√≥b kontaktu w r√≥≈ºnych dniach i porach.\nDostosowanie terminu ‚Äì oferowanie elastycznych termin√≥w realizacji wywiadu.\nZachƒôty materialne ‚Äì oferowanie drobnych gratyfikacji za udzia≈Ç w badaniu.\nAnaliza i wa≈ºenie ze wzglƒôdu na odmowy ‚Äì uwzglƒôdnianie struktury os√≥b odmawiajƒÖcych w procesie wa≈ºenia danych.\nTechniki konwersji odm√≥w ‚Äì specjalne szkolenia dla ankieter√≥w w zakresie przekonywania os√≥b poczƒÖtkowo odmawiajƒÖcych.\n\nDokumentacja odm√≥w:\n\nStandard AAPOR (American Association for Public Opinion Research) ‚Äì zaleca dok≈Çadne raportowanie wszystkich kontakt√≥w, odm√≥w i przyczyn braku realizacji wywiadu.\nKategorie dokumentacji ‚Äì liczba po≈ÇƒÖcze≈Ñ/wizyt, powody niedostƒôpno≈õci, typy i przyczyny odm√≥w.\nTransparentno≈õƒá metodologiczna ‚Äì raportowanie wska≈∫nika odpowiedzi oraz potencjalnego wp≈Çywu odm√≥w na wyniki.\n\n\n\n\n\nPraktyczne problemy realizacji bada≈Ñ probabilistycznych w Polsce:\n\nBrak dostƒôpu do operatu losowania: W Polsce nie ma ≈Çatwego dostƒôpu do pe≈Çnych i aktualnych rejestr√≥w populacji dla cel√≥w badawczych.\nProblem z realizacjƒÖ wywiad√≥w pod wylosowanymi adresami: Coraz trudniej przeprowadziƒá wywiady pod konkretnymi adresami ze wzglƒôdu na:\n\nZwiƒôkszonƒÖ liczbƒô zamkniƒôtych osiedli\nSpadek zaufania spo≈Çecznego i niechƒôƒá do wpuszczania ankieter√≥w do dom√≥w\nR√≥≈ºne godziny pracy potencjalnych respondent√≥w wymagajƒÖce wielokrotnych wizyt\n\nKoszt realizacji: Badania z prawdziwym losowym doborem pr√≥by (np. metodƒÖ random route) sƒÖ kilkukrotnie dro≈ºsze ni≈º badania kwotowe.\n\nWybory prezydenckie w USA w 2016 roku, gdzie wiele sonda≈ºy nie przewidzia≈Ço dok≈Çadnie wyniku, doprowadzi≈Çy do znacznych przemy≈õle≈Ñ w≈õr√≥d ankieter√≥w. Zamiast rezygnowaƒá z pr√≥bkowania kwotowego, wiele organizacji udoskonali≈Ço swoje metody, koncentrujƒÖc siƒô na lepszych definicjach kwot, ulepszonych technikach wa≈ºenia i bardziej przejrzystym raportowaniu ogranicze≈Ñ metodologicznych.\nPomimo tych trend√≥w, wa≈ºne jest, aby zauwa≈ºyƒá, ≈ºe pr√≥bkowanie probabilistyczne pozostaje z≈Çotym standardem wnioskowania statystycznego. Dobrze zaprojektowane pr√≥by probabilistyczne wciƒÖ≈º zapewniajƒÖ najbardziej niezawodnƒÖ podstawƒô do uog√≥lniania z pr√≥by na populacjƒô, szczeg√≥lnie w badaniach akademickich, gdzie dok≈Çadno≈õƒá jest priorytetem nad kosztem i szybko≈õciƒÖ.\n\n\n\nRodzaje B≈Çƒôd√≥w Statystycznych - Poprawiona Klasyfikacja\n\n\nB≈ÇƒÖd Systematyczny a B≈ÇƒÖd Losowy\nB≈Çƒôdy systematyczne i b≈Çƒôdy losowe reprezentujƒÖ fundamentalnie r√≥≈ºne sposoby, w jakie pomiary mogƒÖ odbiegaƒá od prawdziwych warto≈õci:\n\nB≈ÇƒÖd Systematyczny (ObciƒÖ≈ºenie)\n\nDefinicja: Konsekwentne, przewidywalne odchylenia od prawdziwej warto≈õci w okre≈õlonym kierunku.\nCechy:\n\nSkutkuje pomiarami, kt√≥re sƒÖ stale zbyt wysokie lub zbyt niskie\nNie zmniejsza siƒô wraz ze zwiƒôkszeniem wielko≈õci pr√≥by\nMo≈ºna go skorygowaƒá, je≈õli zostanie zidentyfikowany\n\nPrzyk≈Çady:\n\nWaga, kt√≥ra zawsze pokazuje o 1 kilogram wiƒôcej ni≈º rzeczywista waga\nB≈ÇƒÖd selekcji wynikajƒÖcy z pominiƒôcia niekt√≥rych grup populacji\nTendencyjne pytania w ankiecie, kt√≥re sk≈ÇaniajƒÖ respondent√≥w do konkretnych odpowiedzi\n\nWykrywanie i Korekta:\n\nKalibracja wobec znanych wzorc√≥w\nPor√≥wnanie z pomiarami przy u≈ºyciu r√≥≈ºnych metod\nStaranne projektowanie bada≈Ñ i wstƒôpne testowanie narzƒôdzi badawczych\n\n\nB≈ÇƒÖd Losowy\n\nDefinicja: Nieprzewidywalne wahania w pomiarach wynikajƒÖce z czynnik√≥w przypadkowych.\nCechy:\n\nZmienia siƒô zar√≥wno pod wzglƒôdem wielko≈õci, jak i kierunku w r√≥≈ºnych pomiarach\nZwykle podlega rozk≈Çadowi normalnemu\nZmniejsza siƒô wraz ze wzrostem wielko≈õci pr√≥by (zale≈ºno≈õƒá ‚àön)\n\nPrzyk≈Çady:\n\nNaturalne r√≥≈ºnice biologiczne miƒôdzy badanymi osobami\nDrobne wahania w przyrzƒÖdach pomiarowych\nWarunki ≈õrodowiskowe, kt√≥re przypadkowo zmieniajƒÖ siƒô podczas pomiaru\n\nSposoby ograniczania:\n\nZwiƒôkszanie wielko≈õci pr√≥by\nWykonywanie powt√≥rzonych pomiar√≥w\nPoprawa precyzji pomiar√≥w\n\n\n\n\n\nB≈Çƒôdy ZwiƒÖzane z Pr√≥bƒÖ a B≈Çƒôdy NiezwiƒÖzane z Pr√≥bƒÖ\nTo rozr√≥≈ºnienie koncentruje siƒô na ≈∫r√≥dle b≈Çƒôd√≥w w badaniach statystycznych:\n\nB≈ÇƒÖd ZwiƒÖzany z Pr√≥bƒÖ (B≈ÇƒÖd Pr√≥by)\n\nDefinicja: R√≥≈ºnica miƒôdzy statystykƒÖ pr√≥by a prawdziwym parametrem populacji wynikajƒÖca z losowych waha≈Ñ w doborze.\nCechy:\n\nNieunikniony w ka≈ºdym badaniu opartym na pr√≥bie\nMo≈ºna go obliczyƒá za pomocƒÖ miar statystycznych (b≈ÇƒÖd standardowy, przedzia≈Çy ufno≈õci)\nZmniejsza siƒô w przewidywalny spos√≥b wraz ze wzrostem wielko≈õci pr√≥by\nZawsze ma charakter losowy (nie systematyczny)\n\nPrzyk≈Çady:\n\nLosowa pr√≥ba pokazujƒÖca 52% poparcia dla polityki, gdy prawdziwa warto≈õƒá w populacji wynosi 50%\nZr√≥≈ºnicowanie ≈õredniego dochodu w r√≥≈ºnych losowych pr√≥bach z tej samej populacji\n\nMetody kontroli:\n\nZwiƒôkszanie wielko≈õci pr√≥by\nStosowanie bardziej efektywnych schemat√≥w doboru pr√≥by (np. warstwowanie)\nZastosowanie odpowiednich wag w analizie danych\n\n\nB≈ÇƒÖd NiezwiƒÖzany z Pr√≥bƒÖ (B≈ÇƒÖd Niesamplowy)\n\nDefinicja: Wszystkie b≈Çƒôdy, kt√≥re nie wynikajƒÖ z losowych waha≈Ñ w doborze pr√≥by, wystƒôpujƒÖce zar√≥wno w badaniach na pr√≥bie, jak i w pe≈Çnych spisach.\nCechy:\n\nMo≈ºe byƒá systematyczny lub losowy\nCzƒôsto trudniejszy do wykrycia i zmierzenia ni≈º b≈ÇƒÖd pr√≥by\nNiekoniecznie zmniejsza siƒô wraz ze zwiƒôkszeniem pr√≥by\nPotencjalnie bardziej szkodliwy dla trafno≈õci bada≈Ñ ni≈º b≈ÇƒÖd pr√≥by\n\nRodzaje i Przyk≈Çady:\n\nB≈ÇƒÖd Pokrycia (systematyczny): Korzystanie z listy wyborc√≥w, kt√≥ra nie obejmuje os√≥b niedawno zarejestrowanych\nB≈ÇƒÖd Braku Odpowiedzi (mo≈ºe byƒá systematyczny lub losowy): Wy≈ºszy wska≈∫nik odm√≥w w≈õr√≥d okre≈õlonych grup demograficznych\nB≈ÇƒÖd Pomiaru (mo≈ºe byƒá systematyczny lub losowy):\n\nSystematyczny: ≈πle sformu≈Çowane pytania, kt√≥re konsekwentnie zniekszta≈ÇcajƒÖ odpowiedzi\nLosowy: Sporadyczne b≈Çƒôdy przy wprowadzaniu danych\n\nB≈ÇƒÖd Przetwarzania (mo≈ºe byƒá systematyczny lub losowy):\n\nSystematyczny: Konsekwentne niew≈Ça≈õciwe kodowanie okre≈õlonych odpowiedzi\nLosowy: Przypadkowe b≈Çƒôdy w transkrypcji\n\n\n\n\n\n\nModel Ca≈Çkowitego B≈Çƒôdu Badania\nNowoczesna metodologia badawcza analizuje b≈Çƒôdy poprzez model Ca≈Çkowitego B≈Çƒôdu Badania (Total Survey Error), kt√≥ry uwzglƒôdnia wszystkie ≈∫r√≥d≈Ça b≈Çƒôd√≥w mogƒÖcych wp≈Çywaƒá na jako≈õƒá badania:\n\nB≈Çƒôdy Reprezentacji (wp≈ÇywajƒÖce na to, kto jest uwzglƒôdniony):\n\nB≈ÇƒÖd pokrycia\nB≈ÇƒÖd pr√≥by\nB≈ÇƒÖd braku odpowiedzi\nB≈ÇƒÖd wynikajƒÖcy z procedur wa≈ºenia danych\n\nB≈Çƒôdy Pomiaru (wp≈ÇywajƒÖce na dok≈Çadno≈õƒá odpowiedzi):\n\nB≈ÇƒÖd specyfikacji (mierzenie niew≈Ça≈õciwego zjawiska)\nB≈ÇƒÖd pomiaru (wp≈Çyw respondenta, ankietera, kwestionariusza)\nB≈ÇƒÖd przetwarzania (wprowadzanie danych, kodowanie, edytowanie)\n\n\nZrozumienie wzajemnego oddzia≈Çywania miƒôdzy tymi rodzajami b≈Çƒôd√≥w jest kluczowe dla projektowania wysokiej jako≈õci bada≈Ñ i w≈Ça≈õciwej interpretacji wynik√≥w. Chocia≈º b≈ÇƒÖd pr√≥by zwraca na siebie wiƒôkszƒÖ uwagƒô, poniewa≈º ≈Çatwiej go mierzyƒá, b≈Çƒôdy niezwiƒÖzane z pr√≥bƒÖ czƒôsto stanowiƒÖ wiƒôksze zagro≈ºenie dla trafno≈õci i rzetelno≈õci wsp√≥≈Çczesnych bada≈Ñ.\n\n\n\n≈πr√≥d≈Ço: https://scientistcafe.com/ids/vbtradeoff\n\n\nRysunek: ZwiƒÖzek miƒôdzy z≈Ço≈ºono≈õciƒÖ modelu, b≈Çƒôdem systematycznym (obciƒÖ≈ºeniem) i wariancjƒÖ. Ilustruje to, jak r√≥wnowaga miƒôdzy b≈Çƒôdem systematycznym a b≈Çƒôdem losowym (wariancjƒÖ) wp≈Çywa na og√≥lny b≈ÇƒÖd modelu.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Od Pr√≥by do Populacji - Zrozumienie Losowo≈õci, Pobierania Pr√≥by i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "chapter3b.html",
    "href": "chapter3b.html",
    "title": "7¬† Reliability and Validity in Data Science Research",
    "section": "",
    "text": "7.1 Defining Reliability and Validity\nIn data science and research, two crucial concepts that determine the quality of measurements and studies are reliability and validity. Understanding these concepts is essential for conducting robust research and drawing meaningful conclusions from data.\nReliability refers to the consistency of a measure. A reliable measurement or study produces similar results under consistent conditions.\nValidity refers to the accuracy of a measure. A valid measurement or study accurately represents what it claims to measure.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#the-four-combinations-of-reliability-and-validity",
    "href": "chapter3b.html#the-four-combinations-of-reliability-and-validity",
    "title": "7¬† Reliability and Validity in Data Science Research",
    "section": "7.2 The Four Combinations of Reliability and Validity",
    "text": "7.2 The Four Combinations of Reliability and Validity\nThere are four possible combinations of reliability and validity:\n\nHigh Reliability, High Validity\nHigh Reliability, Low Validity\nLow Reliability, High Validity\nLow Reliability, Low Validity\n\nLet‚Äôs explore each of these combinations with examples and visualizations.\n\n1. High Reliability, High Validity\nThis is the ideal scenario in research. Measurements are both consistent and accurate.\nExample: A well-calibrated digital scale used to measure weight. It consistently gives the same reading for the same object and accurately represents the true weight.\n\n\n2. High Reliability, Low Validity\nIn this case, measurements are consistent but not accurate.\nExample: A miscalibrated scale that always measures 5 kg too heavy. It gives consistent results (high reliability) but doesn‚Äôt represent the true weight (low validity).\n\n\n3. Low Reliability, High Validity\nHere, measurements are accurate on average but inconsistent.\nExample: A scale that fluctuates around the true weight. Sometimes it‚Äôs a bit over, sometimes a bit under, but on average, it‚Äôs correct.\n\n\n4. Low Reliability, Low Validity\nThis is the worst-case scenario, where measurements are neither consistent nor accurate.\nExample: A broken scale that gives random readings unrelated to the true weight.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#visualizing-reliability-and-validity",
    "href": "chapter3b.html#visualizing-reliability-and-validity",
    "title": "7¬† Reliability and Validity in Data Science Research",
    "section": "7.3 Visualizing Reliability and Validity",
    "text": "7.3 Visualizing Reliability and Validity\nTo better understand these concepts, let‚Äôs create visualizations using ggplot2 in R. We‚Äôll simulate measurement data for each scenario and plot them.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Generate data for each scenario\nn &lt;- 100\ntrue_value &lt;- 50\n\ndata &lt;- tibble(\n  high_rel_high_val = rnorm(n, mean = true_value, sd = 1),\n  high_rel_low_val = rnorm(n, mean = true_value + 5, sd = 1),\n  low_rel_high_val = rnorm(n, mean = true_value, sd = 5),\n  low_rel_low_val = runif(n, min = 0, max = 100)\n) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(cols = -id, names_to = \"scenario\", values_to = \"measurement\")\n\n# Create the scatterplot\nscatter_plot &lt;- ggplot(data, aes(x = id, y = measurement, color = scenario)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_hline(yintercept = true_value, linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~ scenario, ncol = 2, scales = \"free_y\",\n             labeller = labeller(scenario = c(\n               \"high_rel_high_val\" = \"High Reliability, High Validity\",\n               \"high_rel_low_val\" = \"High Reliability, Low Validity\",\n               \"low_rel_high_val\" = \"Low Reliability, High Validity\",\n               \"low_rel_low_val\" = \"Low Reliability, Low Validity\"\n             ))) +\n  labs(title = \"Scatterplots of Measurements\",\n       subtitle = \"Dashed line represents the true value\",\n       x = \"Measurement ID\",\n       y = \"Measured Value\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Create the histogram\nhist_plot &lt;- ggplot(data, aes(x = measurement, fill = scenario)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  geom_vline(xintercept = true_value, color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ scenario, ncol = 2, scales = \"free\",\n             labeller = labeller(scenario = c(\n               \"high_rel_high_val\" = \"High Reliability, High Validity\",\n               \"high_rel_low_val\" = \"High Reliability, Low Validity\",\n               \"low_rel_high_val\" = \"Low Reliability, High Validity\",\n               \"low_rel_low_val\" = \"Low Reliability, Low Validity\"\n             ))) +\n  labs(title = \"Histograms of Measurements\",\n       subtitle = \"Red dashed line represents the true value\",\n       x = \"Measured Value\",\n       y = \"Count\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Combine the plots\ncombined_plot &lt;- scatter_plot / hist_plot +\n  plot_layout(heights = c(1, 1)) +\n  plot_annotation(\n    title = \"Reliability and Validity in Measurements\",\n    theme = theme(plot.title = element_text(hjust = 0.5, size = 24, face = \"bold\"))\n  )\n\n# Display the combined plot\ncombined_plot\n\n\n\n\n\n\n\n\n\nInterpreting the Visualizations\n\nHigh Reliability, High Validity: Points cluster tightly around the true value (dashed line).\nHigh Reliability, Low Validity: Points cluster tightly, but consistently above the true value.\nLow Reliability, High Validity: Points scatter widely but center around the true value.\nLow Reliability, Low Validity: Points scatter randomly with no clear pattern or relation to the true value.\n\nUnderstanding reliability and validity is crucial in data science and research. High reliability ensures consistent measurements, while high validity ensures accurate representations of what we intend to measure. By considering both aspects, researchers can design more robust studies and draw more meaningful conclusions from their data.\nWhen conducting your own research or analyzing others‚Äô work, always consider: - How reliable are the measurements? - How valid is the approach for measuring the intended concept? - Do the methods used support both reliability and validity?\nBy keeping these questions in mind, you‚Äôll be better equipped to produce and interpret high-quality research in data science.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#types-of-reliability",
    "href": "chapter3b.html#types-of-reliability",
    "title": "7¬† Reliability and Validity in Data Science Research",
    "section": "7.4 Types of Reliability",
    "text": "7.4 Types of Reliability\nReliability can be assessed in several ways, each focusing on a different aspect of consistency:\n\nTest-Retest Reliability: This measures the consistency of a test over time. It involves administering the same test to the same group of participants at different times and comparing the results.\nInter-Rater Reliability: This assesses the degree of agreement among different raters or observers. It‚Äôs crucial when subjective judgments are involved in data collection.\nInternal Consistency: This evaluates how well different items on a test or scale measure the same construct. Cronbach‚Äôs alpha is a common measure of internal consistency.\nParallel Forms Reliability: This involves creating two equivalent forms of a test and administering them to the same group. The correlation between the two sets of scores indicates reliability.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#types-of-validity",
    "href": "chapter3b.html#types-of-validity",
    "title": "7¬† Reliability and Validity in Data Science Research",
    "section": "7.5 Types of Validity",
    "text": "7.5 Types of Validity\nValidity is a multifaceted concept, with several types that researchers need to consider:\n\nContent Validity: This ensures that a measure covers all aspects of the construct it aims to measure. It‚Äôs often assessed by expert judgment.\nConstruct Validity: This evaluates whether a test measures the intended theoretical construct. It includes:\n\nConvergent Validity: The degree to which the measure correlates with other measures of the same construct.\nDiscriminant Validity: The extent to which the measure does not correlate with measures of different constructs.\n\nCriterion Validity: This assesses how well a measure predicts an outcome. It includes:\n\nConcurrent Validity: How well the measure correlates with other measures of the same construct at the same time.\nPredictive Validity: How well the measure predicts future outcomes.\n\nFace Validity: Face validity describes how test subjects perceive the test and whether - from their point of view - it is adequate for the purpose it is supposed to serve. A lack of face validity, even though the test may be valid from the perspective of a specific purpose, can contribute to a decrease in motivation among test subjects, which directly affects the results achieved or may lead to rejection of the test. While not a scientific measure, it can be important for participant buy-in.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#internal-vs.-external-validity",
    "href": "chapter3b.html#internal-vs.-external-validity",
    "title": "7¬† Reliability and Validity in Data Science Research",
    "section": "7.6 Internal vs.¬†External Validity",
    "text": "7.6 Internal vs.¬†External Validity\nThese concepts are crucial in experimental design and the generalizability of research findings:\n\nInternal Validity\nInternal validity refers to the extent to which a study establishes a causal relationship between the independent and dependent variables. It answers the question: ‚ÄúDid the experimental treatment actually cause the observed effects?‚Äù\nFactors that can threaten internal validity include: - History: External events occurring between pre-test and post-test - Maturation: Natural changes in participants over time - Testing effects: Changes due to taking a pre-test - Instrumentation: Changes in the measurement tool or observers - Selection bias: Non-random assignment to groups - Attrition: Loss of participants during the study\n\n\nExternal Validity\nExternal validity refers to the extent to which the results of a study can be generalized to other situations, populations, or settings. It addresses the question: ‚ÄúTo what extent can the findings be applied beyond the specific context of the study?‚Äù\nFactors that can affect external validity include: - Population validity: How well the sample represents the larger population - Ecological validity: How well the study setting represents real-world conditions - Temporal validity: Whether the results hold true across time",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#consistency-in-research",
    "href": "chapter3b.html#consistency-in-research",
    "title": "7¬† Reliability and Validity in Data Science Research",
    "section": "7.7 Consistency in Research",
    "text": "7.7 Consistency in Research\nConsistency is closely related to reliability but extends beyond just measurement. In research, consistency refers to the overall coherence and stability of results across different contexts, methods, or studies.\nKey aspects of consistency in research include:\n\nReplicability: The ability to reproduce study results using the same methods and data.\nRobustness: The stability of findings across different analytical approaches or slight variations in methodology.\nConvergence: The alignment of results from different studies or methods investigating the same phenomenon.\nLongitudinal Consistency: The stability of findings over time, especially important in longitudinal studies.\n\nEnsuring consistency in research involves: - Using standardized procedures and measures - Thoroughly documenting methods and analytical decisions - Conducting replication studies - Meta-analyses to synthesize findings across multiple studies",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#balancing-reliability-validity-and-consistency",
    "href": "chapter3b.html#balancing-reliability-validity-and-consistency",
    "title": "7¬† Reliability and Validity in Data Science Research",
    "section": "7.8 Balancing Reliability, Validity, and Consistency",
    "text": "7.8 Balancing Reliability, Validity, and Consistency\nWhile reliability, validity, and consistency are all crucial for high-quality research, they sometimes involve trade-offs:\n\nA highly reliable measure might lack validity if it consistently measures the wrong thing.\nStriving for perfect internal validity (e.g., in tightly controlled lab experiments) might reduce external validity.\nEnsuring high consistency across diverse contexts might require sacrificing some degree of precision or depth in specific situations.\n\nResearchers must carefully balance these aspects based on their research questions and the nature of their study. A comprehensive understanding of reliability, validity, and consistency helps in designing robust studies, interpreting results accurately, and contributing meaningfully to the body of scientific knowledge.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#bias-variance-tradeoff",
    "href": "chapter3b.html#bias-variance-tradeoff",
    "title": "7¬† Reliability and Validity in Data Science Research",
    "section": "7.9 Bias-Variance Tradeoff",
    "text": "7.9 Bias-Variance Tradeoff\nThe concepts of reliability and validity are closely related to the statistical notion of the bias-variance tradeoff. This tradeoff is fundamental in machine learning and statistical modeling.\n\nBias refers to the error introduced by approximating a real-world problem with a simplified model. High bias can lead to underfitting.\nVariance refers to the error introduced by the model‚Äôs sensitivity to small fluctuations in the training set. High variance can lead to overfitting.\n\nLet‚Äôs visualize this concept with a simplified plot:\n\nx &lt;- seq(0, 10, length.out = 100)\ny_true &lt;- sin(x)\ny_low_bias_high_var &lt;- y_true + rnorm(100, 0, 0.3)\ny_high_bias_low_var &lt;- 0.5 * x\n\ndf &lt;- data.frame(x = rep(x, 3),\n                 y = c(y_true, y_low_bias_high_var, y_high_bias_low_var),\n                 type = rep(c(\"True Function\", \"Low Bias, High Variance\", \"High Bias, Low Variance\"), each = 100))\n\nggplot(df, aes(x = x, y = y, color = type)) +\n  geom_line() +\n  geom_point(data = subset(df, type != \"True Function\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"black\", \"blue\", \"red\")) +\n  labs(title = \"Bias-Variance Tradeoff\",\n       x = \"X\",\n       y = \"Y\",\n       color = \"Model Type\") +\n  theme_minimal()\n\n\n\n\nVisualization of Bias-Variance Tradeoff\n\n\n\n\nIn this plot: - The black line represents the true underlying function. - The blue points represent a model with low bias but high variance. It follows the true function closely on average but has a lot of noise. - The red line represents a model with high bias but low variance. It consistently underestimates the true function but has less noise.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#accuracy-and-precision",
    "href": "chapter3b.html#accuracy-and-precision",
    "title": "7¬† Reliability and Validity in Data Science Research",
    "section": "7.10 Accuracy and Precision",
    "text": "7.10 Accuracy and Precision\nThe concepts of accuracy and precision are closely related to validity and reliability:\n\nAccuracy refers to how close a measurement is to the true value (similar to validity).\nPrecision refers to how consistent or reproducible the measurements are (similar to reliability).\n\nWe can visualize these concepts using a simplified target analogy:\n\nlibrary(ggplot2)\nlibrary(ggforce)\n\ncreate_points &lt;- function(n, x_center, y_center, spread) {\n  data.frame(\n    x = rnorm(n, x_center, spread),\n    y = rnorm(n, y_center, spread)\n  )\n}\n\nset.seed(101)\npoints &lt;- rbind(\n  cbind(create_points(20, 0, 0, 0.1), type = \"High Accuracy\\nHigh Precision\"),\n  cbind(create_points(20, 0.5, 0.5, 0.1), type = \"Low Accuracy\\nHigh Precision\"),\n  cbind(create_points(20, 0, 0, 0.3), type = \"High Accuracy\\nLow Precision\"),\n  cbind(create_points(20, 0.5, 0.5, 0.3), type = \"Low Accuracy\\nLow Precision\")\n)\n\nggplot(points, aes(x, y)) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"black\", fill = NA) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 0.5), color = \"black\", fill = NA) +\n  geom_point(color = \"red\", size = 2) +\n  facet_wrap(~type) +\n  coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Accuracy vs Precision\")\n\n\n\n\nVisualization of Accuracy vs Precision\n\n\n\n\nIn this visualization: - High accuracy means the points are close to the center (bullseye). - High precision means the points are tightly clustered. - Each panel represents a different combination of accuracy and precision.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#conclusion",
    "href": "chapter3b.html#conclusion",
    "title": "7¬† Reliability and Validity in Data Science Research",
    "section": "7.11 Conclusion",
    "text": "7.11 Conclusion\nUnderstanding reliability and validity is crucial for conducting robust research. These concepts help us ensure that our measurements are both consistent and accurate. By relating them to ideas like the bias-variance tradeoff and accuracy-precision, we gain a deeper appreciation of the challenges involved in measurement and modeling in scientific research. As researchers, we must strive to develop measures and models that are both reliable and valid, balancing the tradeoffs between bias and variance, and between accuracy and precision. This requires careful design of research methodologies, rigorous testing of our measurement instruments, and thoughtful interpretation of our results.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#understanding-bias-vs.-variance-in-statistical-measurement",
    "href": "chapter3b.html#understanding-bias-vs.-variance-in-statistical-measurement",
    "title": "7¬† Reliability and Validity in Data Science Research",
    "section": "7.12 Understanding Bias vs.¬†Variance in Statistical Measurement",
    "text": "7.12 Understanding Bias vs.¬†Variance in Statistical Measurement\n\nIntroduction\nIn statistics and machine learning, two important concepts that affect the performance of our models are bias and variance. Understanding these concepts is crucial for building effective predictive models and avoiding common pitfalls like overfitting and underfitting.\n\nBias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can lead to underfitting.\n\nThink of bias as how far off our predictions are from the true values on average.\nIn terms of validity, high bias means our model isn‚Äôt capturing the true relationship in the data.\n\nVariance refers to the amount by which our model would change if we estimated it using a different training dataset. High variance can lead to overfitting.\n\nThink of variance as how much our predictions would fluctuate if we used different datasets.\nIn terms of reliability, high variance means our model is too sensitive to the specific data it was trained on.\n\n\nWe‚Äôll explore four scenarios to illustrate different combinations of bias and variance using synthetic data and regression models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#data-generation-and-model-fitting-function",
    "href": "chapter3b.html#data-generation-and-model-fitting-function",
    "title": "7¬† Reliability and Validity in Data Science Research",
    "section": "7.13 Data Generation and Model Fitting Function",
    "text": "7.13 Data Generation and Model Fitting Function\nFirst, let‚Äôs create a function that will help us generate data and fit models for each scenario:\n\ngenerate_and_fit &lt;- function(n, intercept, slope, noise_sd, model_degree) {\n  # Generate synthetic data\n  x &lt;- runif(n, 0, 10)\n  y &lt;- intercept + slope * x + rnorm(n, 0, noise_sd)\n  \n  # Fit model\n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", model_degree, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  # Generate predictions\n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  # Plot\n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    geom_abline(intercept = intercept, slope = slope, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Model Degree:\", model_degree),\n         subtitle = paste(\"Noise SD:\", noise_sd),\n         x = \"X (Input Variable)\",\n         y = \"Y (Target Variable)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nThis function does the following: 1. Generates synthetic data based on our parameters 2. Fits a polynomial regression model 3. Creates a plot showing the true relationship (blue dashed line), our model‚Äôs predictions (red solid line), and the data points\nNow, let‚Äôs explore our four scenarios!\n\nScenario 1: Low Bias, Low Variance\nIn this ideal scenario, we use a linear model to fit linear data with low noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 1, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The model (red line) closely follows the true relationship (blue dashed line). - Data points are clustered tightly around the line, indicating low noise. - This scenario represents a good fit: the model captures the underlying trend without being overly complex.\n\n\nScenario 2: Low Bias, High Variance\nHere, we use a linear model to fit linear data, but with high noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 5, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The model still captures the general trend, but data points are more scattered. - This high variance means our model‚Äôs predictions would be less reliable. - In real-world terms, this might represent a situation where our measurements are correct on average but have a lot of random error.\n\n\nScenario 3: High Bias, Low Variance\nIn this case, we use a linear model to fit quadratic (curved) data with low noise.\n\nquadratic_data &lt;- function(n, intercept, slope, noise_sd, model_degree) {\n  x &lt;- runif(n, 0, 10)\n  y &lt;- intercept + slope * x^2 + rnorm(n, 0, noise_sd)\n  \n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", model_degree, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    stat_function(fun = function(x) intercept + slope * x^2, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Model Degree:\", model_degree),\n         subtitle = paste(\"Noise SD:\", noise_sd),\n         x = \"X (Input Variable)\",\n         y = \"Y (Target Variable)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nquadratic_data(n = 100, intercept = 1, slope = 0.2, noise_sd = 1, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The linear model (red line) fails to capture the curvature of the true relationship (blue dashed line). - This high bias means our model is consistently off in its predictions. - In real-world terms, this might represent using an overly simplistic model for a complex phenomenon.\n\n\nScenario 4: High Bias, High Variance\nFinally, we use a high-degree polynomial to fit linear data with high noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 5, model_degree = 5)\n\n\n\n\n\n\n\n\nExplanation: - The model (red line) is overly complex, trying to fit the noise rather than the underlying trend. - This combination of high bias and high variance leads to poor generalization. - In real-world terms, this might represent overcomplicating our analysis and drawing false conclusions from random fluctuations in our data.\n\n\nConclusion\nUnderstanding the bias-variance trade-off is crucial in statistical modeling:\n\nLow Bias, Low Variance: The ideal scenario, where our model accurately captures the underlying relationship without being overly sensitive to noise.\nLow Bias, High Variance: Our model is correct on average but unreliable due to high sensitivity to individual data points.\nHigh Bias, Low Variance: Our model is consistently wrong due to oversimplification but gives stable predictions.\nHigh Bias, High Variance: The worst-case scenario, where our model is both inaccurate and unreliable.\n\nIn practice, we often need to balance bias and variance. Techniques like cross-validation, regularization, and ensemble methods can help find this balance.\nRemember: - A model with high bias is too simple and misses important patterns in the data. - A model with high variance is too complex and fits noise in the training data. - The goal is to find a sweet spot that captures true patterns without overfitting to noise.\nBy understanding these concepts, you‚Äôll be better equipped to choose appropriate models, avoid overfitting and underfitting, and build more effective predictive models in your future statistical analyses!",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html",
    "href": "rozdzial3b.html",
    "title": "8¬† Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych",
    "section": "",
    "text": "8.1 Definiowanie Rzetelno≈õci i Trafno≈õci\nW naukach o danych i badaniach naukowych, dwa kluczowe pojƒôcia, kt√≥re okre≈õlajƒÖ jako≈õƒá pomiar√≥w i bada≈Ñ, to rzetelno≈õƒá i trafno≈õƒá. Zrozumienie tych pojƒôƒá jest niezbƒôdne do prowadzenia solidnych bada≈Ñ i wyciƒÖgania znaczƒÖcych wniosk√≥w z danych.\nRzetelno≈õƒá odnosi siƒô do sp√≥jno≈õci pomiaru. Rzetelny pomiar lub badanie daje podobne wyniki w sp√≥jnych warunkach.\nTrafno≈õƒá odnosi siƒô do dok≈Çadno≈õci pomiaru. Trafny pomiar lub badanie dok≈Çadnie reprezentuje to, co twierdzi, ≈ºe mierzy.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#cztery-kombinacje-rzetelno≈õci-i-trafno≈õci",
    "href": "rozdzial3b.html#cztery-kombinacje-rzetelno≈õci-i-trafno≈õci",
    "title": "8¬† Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych",
    "section": "8.2 Cztery Kombinacje Rzetelno≈õci i Trafno≈õci",
    "text": "8.2 Cztery Kombinacje Rzetelno≈õci i Trafno≈õci\nIstniejƒÖ cztery mo≈ºliwe kombinacje rzetelno≈õci i trafno≈õci:\n\nWysoka Rzetelno≈õƒá, Wysoka Trafno≈õƒá\nWysoka Rzetelno≈õƒá, Niska Trafno≈õƒá\nNiska Rzetelno≈õƒá, Wysoka Trafno≈õƒá\nNiska Rzetelno≈õƒá, Niska Trafno≈õƒá\n\nPrzyjrzyjmy siƒô ka≈ºdej z tych kombinacji z przyk≈Çadami i wizualizacjami.\n\n1. Wysoka Rzetelno≈õƒá, Wysoka Trafno≈õƒá\nTo idealny scenariusz w badaniach. Pomiary sƒÖ zar√≥wno sp√≥jne, jak i dok≈Çadne.\nPrzyk≈Çad: Dobrze skalibrowana waga cyfrowa u≈ºywana do pomiaru wagi. Konsekwentnie daje ten sam odczyt dla tego samego obiektu i dok≈Çadnie reprezentuje prawdziwƒÖ wagƒô.\n\n\n2. Wysoka Rzetelno≈õƒá, Niska Trafno≈õƒá\nW tym przypadku pomiary sƒÖ sp√≥jne, ale niedok≈Çadne.\nPrzyk≈Çad: ≈πle skalibrowana waga, kt√≥ra zawsze mierzy 5 kg za ciƒô≈ºko. Daje sp√≥jne wyniki (wysoka rzetelno≈õƒá), ale nie reprezentuje prawdziwej wagi (niska trafno≈õƒá).\n\n\n3. Niska Rzetelno≈õƒá, Wysoka Trafno≈õƒá\nTutaj pomiary sƒÖ dok≈Çadne ≈õrednio, ale niesp√≥jne.\nPrzyk≈Çad: Waga, kt√≥ra waha siƒô wok√≥≈Ç prawdziwej wagi. Czasami pokazuje trochƒô wiƒôcej, czasami trochƒô mniej, ale ≈õrednio jest poprawna.\n\n\n4. Niska Rzetelno≈õƒá, Niska Trafno≈õƒá\nTo najgorszy scenariusz, gdzie pomiary nie sƒÖ ani sp√≥jne, ani dok≈Çadne.\nPrzyk≈Çad: Zepsuta waga, kt√≥ra daje losowe odczyty niezwiƒÖzane z prawdziwƒÖ wagƒÖ.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#wizualizacja-rzetelno≈õci-i-trafno≈õci",
    "href": "rozdzial3b.html#wizualizacja-rzetelno≈õci-i-trafno≈õci",
    "title": "8¬† Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych",
    "section": "8.3 Wizualizacja Rzetelno≈õci i Trafno≈õci",
    "text": "8.3 Wizualizacja Rzetelno≈õci i Trafno≈õci\nAby lepiej zrozumieƒá te pojƒôcia, stw√≥rzmy wizualizacje przy u≈ºyciu ggplot2 w R. Zasymulujemy dane pomiarowe dla ka≈ºdego scenariusza i narysujemy je.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Generowanie danych dla ka≈ºdego scenariusza\nn &lt;- 100\nprawdziwa_wartosc &lt;- 50\n\ndane &lt;- tibble(\n  wysoka_rz_wysoka_tr = rnorm(n, mean = prawdziwa_wartosc, sd = 1),\n  wysoka_rz_niska_tr = rnorm(n, mean = prawdziwa_wartosc + 5, sd = 1),\n  niska_rz_wysoka_tr = rnorm(n, mean = prawdziwa_wartosc, sd = 5),\n  niska_rz_niska_tr = runif(n, min = 0, max = 100)\n) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(cols = -id, names_to = \"scenariusz\", values_to = \"pomiar\")\n\n# Tworzenie wykresu punktowego\nwykres_punktowy &lt;- ggplot(dane, aes(x = id, y = pomiar, color = scenariusz)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_hline(yintercept = prawdziwa_wartosc, linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~ scenariusz, ncol = 2, scales = \"free_y\",\n             labeller = labeller(scenariusz = c(\n               \"wysoka_rz_wysoka_tr\" = \"Wysoka Rzetelno≈õƒá, Wysoka Trafno≈õƒá\",\n               \"wysoka_rz_niska_tr\" = \"Wysoka Rzetelno≈õƒá, Niska Trafno≈õƒá\",\n               \"niska_rz_wysoka_tr\" = \"Niska Rzetelno≈õƒá, Wysoka Trafno≈õƒá\",\n               \"niska_rz_niska_tr\" = \"Niska Rzetelno≈õƒá, Niska Trafno≈õƒá\"\n             ))) +\n  labs(title = \"Wykresy punktowe pomiar√≥w\",\n       subtitle = \"Przerywana linia reprezentuje prawdziwƒÖ warto≈õƒá\",\n       x = \"ID pomiaru\",\n       y = \"Zmierzona warto≈õƒá\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Tworzenie histogramu\nwykres_hist &lt;- ggplot(dane, aes(x = pomiar, fill = scenariusz)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  geom_vline(xintercept = prawdziwa_wartosc, color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ scenariusz, ncol = 2, scales = \"free\",\n             labeller = labeller(scenariusz = c(\n               \"wysoka_rz_wysoka_tr\" = \"Wysoka Rzetelno≈õƒá, Wysoka Trafno≈õƒá\",\n               \"wysoka_rz_niska_tr\" = \"Wysoka Rzetelno≈õƒá, Niska Trafno≈õƒá\",\n               \"niska_rz_wysoka_tr\" = \"Niska Rzetelno≈õƒá, Wysoka Trafno≈õƒá\",\n               \"niska_rz_niska_tr\" = \"Niska Rzetelno≈õƒá, Niska Trafno≈õƒá\"\n             ))) +\n  labs(title = \"Histogramy pomiar√≥w\",\n       subtitle = \"Czerwona przerywana linia reprezentuje prawdziwƒÖ warto≈õƒá\",\n       x = \"Zmierzona warto≈õƒá\",\n       y = \"Liczba\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# ≈ÅƒÖczenie wykres√≥w\nwykres_polaczony &lt;- wykres_punktowy / wykres_hist +\n  plot_layout(heights = c(1, 1)) +\n  plot_annotation(\n    title = \"Rzetelno≈õƒá i Trafno≈õƒá w Pomiarach\",\n    theme = theme(plot.title = element_text(hjust = 0.5, size = 24, face = \"bold\"))\n  )\n\n# Wy≈õwietlanie po≈ÇƒÖczonego wykresu\nwykres_polaczony\n\n\n\n\n\n\n\n\n\nInterpretacja Wizualizacji\n\nWysoka Rzetelno≈õƒá, Wysoka Trafno≈õƒá: Punkty grupujƒÖ siƒô ciasno wok√≥≈Ç prawdziwej warto≈õci (przerywana linia).\nWysoka Rzetelno≈õƒá, Niska Trafno≈õƒá: Punkty grupujƒÖ siƒô ciasno, ale konsekwentnie powy≈ºej prawdziwej warto≈õci.\nNiska Rzetelno≈õƒá, Wysoka Trafno≈õƒá: Punkty rozpraszajƒÖ siƒô szeroko, ale centrujƒÖ siƒô wok√≥≈Ç prawdziwej warto≈õci.\nNiska Rzetelno≈õƒá, Niska Trafno≈õƒá: Punkty rozpraszajƒÖ siƒô losowo bez wyra≈∫nego wzoru lub relacji do prawdziwej warto≈õci.\n\nZrozumienie rzetelno≈õci i trafno≈õci jest kluczowe w naukach o danych i badaniach. Wysoka rzetelno≈õƒá zapewnia sp√≥jne pomiary, podczas gdy wysoka trafno≈õƒá zapewnia dok≈Çadne reprezentacje tego, co zamierzamy zmierzyƒá. BiorƒÖc pod uwagƒô oba aspekty, badacze mogƒÖ projektowaƒá bardziej solidne badania i wyciƒÖgaƒá bardziej znaczƒÖce wnioski ze swoich danych.\nProwadzƒÖc w≈Çasne badania lub analizujƒÖc pracƒô innych, zawsze nale≈ºy rozwa≈ºyƒá: - Jak rzetelne sƒÖ pomiary? - Jak trafne jest podej≈õcie do pomiaru zamierzonego pojƒôcia? - Czy stosowane metody wspierajƒÖ zar√≥wno rzetelno≈õƒá, jak i trafno≈õƒá?\nMajƒÖc na uwadze te pytania, bƒôdziesz lepiej przygotowany do prowadzenia i interpretowania wysokiej jako≈õci bada≈Ñ w naukach o danych.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#rodzaje-rzetelno≈õci",
    "href": "rozdzial3b.html#rodzaje-rzetelno≈õci",
    "title": "8¬† Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych",
    "section": "8.4 Rodzaje Rzetelno≈õci",
    "text": "8.4 Rodzaje Rzetelno≈õci\nRzetelno≈õƒá mo≈ºna oceniaƒá na kilka sposob√≥w, ka≈ºdy skupiajƒÖcy siƒô na innym aspekcie sp√≥jno≈õci:\n\nRzetelno≈õƒá test-retest: Mierzy sp√≥jno≈õƒá testu w czasie. Polega na przeprowadzeniu tego samego testu na tej samej grupie uczestnik√≥w w r√≥≈ºnych momentach i por√≥wnaniu wynik√≥w.\nRzetelno≈õƒá miƒôdzy oceniajƒÖcymi: Ocenia stopie≈Ñ zgodno≈õci miƒôdzy r√≥≈ºnymi oceniajƒÖcymi lub obserwatorami. Jest kluczowa, gdy w zbieraniu danych biorƒÖ udzia≈Ç subiektywne osƒÖdy.\nSp√≥jno≈õƒá wewnƒôtrzna: Ocenia, jak dobrze r√≥≈ºne elementy testu lub skali mierzƒÖ ten sam konstrukt. Alfa Cronbacha jest powszechnƒÖ miarƒÖ sp√≥jno≈õci wewnƒôtrznej.\nRzetelno≈õƒá form r√≥wnoleg≈Çych: Polega na stworzeniu dw√≥ch r√≥wnowa≈ºnych form testu i przeprowadzeniu ich na tej samej grupie. Korelacja miƒôdzy dwoma zestawami wynik√≥w wskazuje na rzetelno≈õƒá.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#rodzaje-trafno≈õci",
    "href": "rozdzial3b.html#rodzaje-trafno≈õci",
    "title": "8¬† Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych",
    "section": "8.5 Rodzaje Trafno≈õci",
    "text": "8.5 Rodzaje Trafno≈õci\nTrafno≈õƒá jest pojƒôciem wieloaspektowym, z kilkoma rodzajami, kt√≥re badacze muszƒÖ wziƒÖƒá pod uwagƒô:\n\nTrafno≈õƒá tre≈õciowa: Zapewnia, ≈ºe pomiar obejmuje wszystkie aspekty konstruktu, kt√≥ry ma mierzyƒá. Czƒôsto jest oceniana przez osƒÖd ekspert√≥w.\nTrafno≈õƒá konstrukcyjna: Ocenia, czy test mierzy zamierzony konstrukt teoretyczny. Obejmuje:\n\nTrafno≈õƒá zbie≈ºnƒÖ: Stopie≈Ñ, w jakim pomiar koreluje z innymi pomiarami tego samego konstruktu.\nTrafno≈õƒá r√≥≈ºnicowƒÖ: Zakres, w jakim pomiar nie koreluje z pomiarami r√≥≈ºnych konstrukt√≥w.\n\nTrafno≈õƒá kryterialnƒÖ: Ocenia, jak dobrze pomiar przewiduje wynik. Obejmuje:\n\nTrafno≈õƒá wsp√≥≈Çbie≈ºnƒÖ: Jak dobrze pomiar koreluje z innymi pomiarami tego samego konstruktu w tym samym czasie.\nTrafno≈õƒá predykcyjnƒÖ: Jak dobrze pomiar przewiduje przysz≈Çe wyniki.\n\nTrafno≈õƒá fasadowa: Trafno≈õƒá fasadowa odnosi siƒô do tego, jak osoby badane postrzegajƒÖ test i czy uwa≈ºajƒÖ go za odpowiedni do celu, kt√≥remu ma s≈Çu≈ºyƒá. Brak trafno≈õci fasadowej mo≈ºe mieƒá negatywne konsekwencje, nawet je≈õli test jest faktycznie trafny (czyli mierzy to, co powinien mierzyƒá) z punktu widzenia jego zamierzonego celu. Choƒá nie jest to naukowa miara, mo≈ºe byƒá wa≈ºna dla zaanga≈ºowania uczestnik√≥w.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#trafno≈õƒá-wewnƒôtrzna-vs-zewnƒôtrzna",
    "href": "rozdzial3b.html#trafno≈õƒá-wewnƒôtrzna-vs-zewnƒôtrzna",
    "title": "8¬† Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych",
    "section": "8.6 Trafno≈õƒá Wewnƒôtrzna vs Zewnƒôtrzna",
    "text": "8.6 Trafno≈õƒá Wewnƒôtrzna vs Zewnƒôtrzna\nTe pojƒôcia sƒÖ kluczowe w projektowaniu eksperyment√≥w i mo≈ºliwo≈õci uog√≥lniania wynik√≥w bada≈Ñ:\n\nTrafno≈õƒá Wewnƒôtrzna\nTrafno≈õƒá wewnƒôtrzna odnosi siƒô do zakresu, w jakim badanie ustanawia zwiƒÖzek przyczynowy miƒôdzy zmiennymi niezale≈ºnymi a zale≈ºnymi. Odpowiada na pytanie: ‚ÄúCzy eksperymentalne traktowanie rzeczywi≈õcie spowodowa≈Ço zaobserwowane efekty?‚Äù\nCzynniki, kt√≥re mogƒÖ zagra≈ºaƒá trafno≈õci wewnƒôtrznej, obejmujƒÖ: - Historia: Zewnƒôtrzne wydarzenia wystƒôpujƒÖce miƒôdzy pre-testem a post-testem - Dojrzewanie: Naturalne zmiany u uczestnik√≥w w czasie - Efekty testowania: Zmiany wynikajƒÖce z przeprowadzenia pre-testu - Instrumentacja: Zmiany w narzƒôdziu pomiarowym lub obserwatorach - B≈ÇƒÖd selekcji: Nielosowy przydzia≈Ç do grup - Utrata: Utrata uczestnik√≥w podczas badania\n\n\nTrafno≈õƒá Zewnƒôtrzna\nTrafno≈õƒá zewnƒôtrzna odnosi siƒô do zakresu, w jakim wyniki badania mogƒÖ byƒá uog√≥lnione na inne sytuacje, populacje lub ustawienia. Odpowiada na pytanie: ‚ÄúW jakim stopniu wyniki mogƒÖ byƒá zastosowane poza konkretnym kontekstem badania?‚Äù\nCzynniki, kt√≥re mogƒÖ wp≈Çywaƒá na trafno≈õƒá zewnƒôtrznƒÖ, obejmujƒÖ: - Trafno≈õƒá populacyjna: Jak dobrze pr√≥ba reprezentuje szerszƒÖ populacjƒô - Trafno≈õƒá ekologiczna: Jak dobrze ustawienie badania reprezentuje warunki ≈õwiata rzeczywistego - Trafno≈õƒá czasowa: Czy wyniki pozostajƒÖ prawdziwe w czasie",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#sp√≥jno≈õƒá-w-badaniach",
    "href": "rozdzial3b.html#sp√≥jno≈õƒá-w-badaniach",
    "title": "8¬† Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych",
    "section": "8.7 Sp√≥jno≈õƒá w Badaniach",
    "text": "8.7 Sp√≥jno≈õƒá w Badaniach\nSp√≥jno≈õƒá jest ≈õci≈õle zwiƒÖzana z rzetelno≈õciƒÖ, ale wykracza poza sam pomiar. W badaniach sp√≥jno≈õƒá odnosi siƒô do og√≥lnej koherencji i stabilno≈õci wynik√≥w w r√≥≈ºnych kontekstach, metodach lub badaniach.\nKluczowe aspekty sp√≥jno≈õci w badaniach obejmujƒÖ:\n\nReplikowalno≈õƒá: Zdolno≈õƒá do odtworzenia wynik√≥w badania przy u≈ºyciu tych samych metod i danych.\nOdporno≈õƒá: Stabilno≈õƒá wynik√≥w w r√≥≈ºnych podej≈õciach analitycznych lub niewielkich zmianach w metodologii.\nKonwergencja: Zbie≈ºno≈õƒá wynik√≥w z r√≥≈ºnych bada≈Ñ lub metod badajƒÖcych to samo zjawisko.\nSp√≥jno≈õƒá d≈Çugoterminowa: Stabilno≈õƒá wynik√≥w w czasie, szczeg√≥lnie wa≈ºna w badaniach d≈Çugoterminowych.\n\nZapewnienie sp√≥jno≈õci w badaniach obejmuje: - Stosowanie standaryzowanych procedur i miar - Dok≈Çadne dokumentowanie metod i decyzji analitycznych - Przeprowadzanie bada≈Ñ replikacyjnych - Meta-analizy w celu syntezy wynik√≥w z wielu bada≈Ñ",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#r√≥wnowa≈ºenie-rzetelno≈õci-trafno≈õci-i-sp√≥jno≈õci",
    "href": "rozdzial3b.html#r√≥wnowa≈ºenie-rzetelno≈õci-trafno≈õci-i-sp√≥jno≈õci",
    "title": "8¬† Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych",
    "section": "8.8 R√≥wnowa≈ºenie Rzetelno≈õci, Trafno≈õci i Sp√≥jno≈õci",
    "text": "8.8 R√≥wnowa≈ºenie Rzetelno≈õci, Trafno≈õci i Sp√≥jno≈õci\nChocia≈º rzetelno≈õƒá, trafno≈õƒá i sp√≥jno≈õƒá sƒÖ kluczowe dla wysokiej jako≈õci bada≈Ñ, czasami wiƒÖ≈ºƒÖ siƒô z kompromisami:\n\nWysoce rzetelna miara mo≈ºe nie mieƒá trafno≈õci, je≈õli konsekwentnie mierzy niew≈Ça≈õciwƒÖ rzecz.\nDƒÖ≈ºenie do idealnej trafno≈õci wewnƒôtrznej (np. w ≈õci≈õle kontrolowanych eksperymentach laboratoryjnych) mo≈ºe zmniejszyƒá trafno≈õƒá zewnƒôtrznƒÖ.\nZapewnienie wysokiej sp√≥jno≈õci w r√≥≈ºnych kontekstach mo≈ºe wymagaƒá po≈õwiƒôcenia pewnego stopnia precyzji lub g≈Çƒôbi w konkretnych sytuacjach.\n\nBadacze muszƒÖ starannie r√≥wnowa≈ºyƒá te aspekty w oparciu o swoje pytania badawcze i charakter badania. Kompleksowe zrozumienie rzetelno≈õci, trafno≈õci i sp√≥jno≈õci pomaga w projektowaniu solidnych bada≈Ñ, dok≈Çadnej interpretacji wynik√≥w i znaczƒÖcym wk≈Çadzie do korpusu wiedzy naukowej.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#kompromis-miƒôdzy-obciƒÖ≈ºeniem-a-wariancjƒÖ",
    "href": "rozdzial3b.html#kompromis-miƒôdzy-obciƒÖ≈ºeniem-a-wariancjƒÖ",
    "title": "8¬† Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych",
    "section": "8.9 Kompromis miƒôdzy ObciƒÖ≈ºeniem a WariancjƒÖ",
    "text": "8.9 Kompromis miƒôdzy ObciƒÖ≈ºeniem a WariancjƒÖ\nPojƒôcia rzetelno≈õci i trafno≈õci sƒÖ ≈õci≈õle zwiƒÖzane ze statystycznym pojƒôciem kompromisu miƒôdzy obciƒÖ≈ºeniem a wariancjƒÖ. Ten kompromis jest fundamentalny w uczeniu maszynowym i modelowaniu statystycznym.\n\nObciƒÖ≈ºenie odnosi siƒô do b≈Çƒôdu wprowadzonego przez przybli≈ºenie problemu ze ≈õwiata rzeczywistego uproszczonym modelem. Wysokie obciƒÖ≈ºenie mo≈ºe prowadziƒá do niedopasowania.\nWariancja odnosi siƒô do b≈Çƒôdu wprowadzonego przez wra≈ºliwo≈õƒá modelu na ma≈Çe fluktuacje w zbiorze treningowym. Wysoka wariancja mo≈ºe prowadziƒá do przeuczenia.\n\nZobrazujmy to pojƒôcie za pomocƒÖ uproszczonego wykresu:\n\nx &lt;- seq(0, 10, length.out = 100)\ny_prawdziwa &lt;- sin(x)\ny_niskie_obciazenie_wysoka_wariancja &lt;- y_prawdziwa + rnorm(100, 0, 0.3)\ny_wysokie_obciazenie_niska_wariancja &lt;- 0.5 * x\n\ndf &lt;- data.frame(x = rep(x, 3),\n                 y = c(y_prawdziwa, y_niskie_obciazenie_wysoka_wariancja, y_wysokie_obciazenie_niska_wariancja),\n                 typ = rep(c(\"Prawdziwa Funkcja\", \"Niskie ObciƒÖ≈ºenie, Wysoka Wariancja\", \"Wysokie ObciƒÖ≈ºenie, Niska Wariancja\"), each = 100))\n\nggplot(df, aes(x = x, y = y, color = typ)) +\n  geom_line() +\n  geom_point(data = subset(df, typ != \"Prawdziwa Funkcja\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"black\", \"blue\", \"red\")) +\n  labs(title = \"Kompromis miƒôdzy ObciƒÖ≈ºeniem a WariancjƒÖ\",\n       x = \"X\",\n       y = \"Y\",\n       color = \"Typ Modelu\") +\n  theme_minimal()\n\n\n\n\nWizualizacja kompromisu miƒôdzy obciƒÖ≈ºeniem a wariancjƒÖ\n\n\n\n\nNa tym wykresie: - Czarna linia reprezentuje prawdziwƒÖ funkcjƒô bazowƒÖ. - Niebieskie punkty reprezentujƒÖ model z niskim obciƒÖ≈ºeniem, ale wysokƒÖ wariancjƒÖ. ≈örednio podƒÖ≈ºa blisko prawdziwej funkcji, ale ma du≈ºo szumu. - Czerwona linia reprezentuje model z wysokim obciƒÖ≈ºeniem, ale niskƒÖ wariancjƒÖ. Konsekwentnie niedoszacowuje prawdziwej funkcji, ale ma mniej szumu.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#dok≈Çadno≈õƒá-i-precyzja",
    "href": "rozdzial3b.html#dok≈Çadno≈õƒá-i-precyzja",
    "title": "8¬† Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych",
    "section": "8.10 Dok≈Çadno≈õƒá i Precyzja",
    "text": "8.10 Dok≈Çadno≈õƒá i Precyzja\nPojƒôcia dok≈Çadno≈õci i precyzji sƒÖ ≈õci≈õle zwiƒÖzane z trafno≈õciƒÖ i rzetelno≈õciƒÖ:\n\nDok≈Çadno≈õƒá odnosi siƒô do tego, jak blisko pomiar jest prawdziwej warto≈õci (podobnie do trafno≈õci).\nPrecyzja odnosi siƒô do tego, jak sp√≥jne lub powtarzalne sƒÖ pomiary (podobnie do rzetelno≈õci).\n\nMo≈ºemy zobrazowaƒá te pojƒôcia za pomocƒÖ uproszczonej analogii do tarczy:\n\nlibrary(ggplot2)\nlibrary(ggforce)\n\ncreate_points &lt;- function(n, x_center, y_center, spread) {\n  data.frame(\n    x = rnorm(n, x_center, spread),\n    y = rnorm(n, y_center, spread)\n  )\n}\n\nset.seed(101)\npoints &lt;- rbind(\n  cbind(create_points(20, 0, 0, 0.1), type = \"Wysoka Dok≈Çadno≈õƒá\\nWysoka Precyzja\"),\n  cbind(create_points(20, 0.5, 0.5, 0.1), type = \"Niska Dok≈Çadno≈õƒá\\nWysoka Precyzja\"),\n  cbind(create_points(20, 0, 0, 0.3), type = \"Wysoka Dok≈Çadno≈õƒá\\nNiska Precyzja\"),\n  cbind(create_points(20, 0.5, 0.5, 0.3), type = \"Niska Dok≈Çadno≈õƒá\\nNiska Precyzja\")\n)\n\nggplot(points, aes(x, y)) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"black\", fill = NA) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 0.5), color = \"black\", fill = NA) +\n  geom_point(color = \"red\", size = 2) +\n  facet_wrap(~type) +\n  coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Dok≈Çadno≈õƒá vs Precyzja\")\n\n\n\n\nWizualizacja Dok≈Çadno≈õci vs Precyzji\n\n\n\n\nW tej wizualizacji: - Wysoka dok≈Çadno≈õƒá oznacza, ≈ºe punkty sƒÖ blisko ≈õrodka (dziesiƒÖtki). - Wysoka precyzja oznacza, ≈ºe punkty sƒÖ ≈õci≈õle zgrupowane. - Ka≈ºdy panel reprezentuje innƒÖ kombinacjƒô dok≈Çadno≈õci i precyzji.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#podsumowanie",
    "href": "rozdzial3b.html#podsumowanie",
    "title": "8¬† Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych",
    "section": "8.11 Podsumowanie",
    "text": "8.11 Podsumowanie\nZrozumienie rzetelno≈õci i trafno≈õci jest kluczowe dla prowadzenia solidnych bada≈Ñ. Pojƒôcia te pomagajƒÖ nam zapewniƒá, ≈ºe nasze pomiary sƒÖ zar√≥wno sp√≥jne, jak i dok≈Çadne. ≈ÅƒÖczƒÖc je z ideami takimi jak kompromis miƒôdzy obciƒÖ≈ºeniem a wariancjƒÖ oraz dok≈Çadno≈õciƒÖ i precyzjƒÖ, zyskujemy g≈Çƒôbsze zrozumienie wyzwa≈Ñ zwiƒÖzanych z pomiarem i modelowaniem w badaniach naukowych. Jako badacze musimy dƒÖ≈ºyƒá do opracowania miar i modeli, kt√≥re sƒÖ zar√≥wno rzetelne, jak i trafne, r√≥wnowa≈ºƒÖc kompromisy miƒôdzy obciƒÖ≈ºeniem a wariancjƒÖ oraz miƒôdzy dok≈Çadno≈õciƒÖ a precyzjƒÖ. Wymaga to starannego projektowania metodologii bada≈Ñ, rygorystycznego testowania naszych instrument√≥w pomiarowych i przemy≈õlanej interpretacji naszych wynik√≥w.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#zrozumienie-obciƒÖ≈ºenia-vs.-wariancji-w-pomiarach-statystycznych",
    "href": "rozdzial3b.html#zrozumienie-obciƒÖ≈ºenia-vs.-wariancji-w-pomiarach-statystycznych",
    "title": "8¬† Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych",
    "section": "8.12 Zrozumienie ObciƒÖ≈ºenia vs.¬†Wariancji w Pomiarach Statystycznych",
    "text": "8.12 Zrozumienie ObciƒÖ≈ºenia vs.¬†Wariancji w Pomiarach Statystycznych\n\nWprowadzenie\nW statystyce i uczeniu maszynowym dwa wa≈ºne pojƒôcia, kt√≥re wp≈ÇywajƒÖ na wydajno≈õƒá naszych modeli, to obciƒÖ≈ºenie (bias) i wariancja (variance). Zrozumienie tych pojƒôƒá jest kluczowe dla budowania efektywnych modeli predykcyjnych i unikania typowych pu≈Çapek, takich jak przeuczenie i niedouczenie.\n\nObciƒÖ≈ºenie odnosi siƒô do b≈Çƒôdu wprowadzonego przez przybli≈ºenie rzeczywistego problemu, kt√≥ry mo≈ºe byƒá z≈Ço≈ºony, za pomocƒÖ uproszczonego modelu. Wysokie obciƒÖ≈ºenie mo≈ºe prowadziƒá do niedouczenia.\n\nWyobra≈∫ sobie obciƒÖ≈ºenie jako ≈õredniƒÖ odleg≈Ço≈õƒá naszych przewidywa≈Ñ od prawdziwych warto≈õci.\nW kontek≈õcie trafno≈õci, wysokie obciƒÖ≈ºenie oznacza, ≈ºe nasz model nie uchwyci≈Ç prawdziwej zale≈ºno≈õci w danych.\n\nWariancja odnosi siƒô do tego, jak bardzo nasz model zmieni≈Çby siƒô, gdyby≈õmy oszacowali go przy u≈ºyciu innego zbioru treningowego. Wysoka wariancja mo≈ºe prowadziƒá do przeuczenia.\n\nWyobra≈∫ sobie wariancjƒô jako to, jak bardzo nasze przewidywania waha≈Çyby siƒô, gdyby≈õmy u≈ºyli r√≥≈ºnych zbior√≥w danych.\nW kontek≈õcie rzetelno≈õci, wysoka wariancja oznacza, ≈ºe nasz model jest zbyt wra≈ºliwy na konkretne dane, na kt√≥rych zosta≈Ç wytrenowany.\n\n\nZbadamy cztery scenariusze, aby zilustrowaƒá r√≥≈ºne kombinacje obciƒÖ≈ºenia i wariancji przy u≈ºyciu syntetycznych danych i modeli regresji.\n\n\nFunkcja Generowania Danych i Dopasowywania Modelu\nNajpierw stw√≥rzmy funkcjƒô, kt√≥ra pomo≈ºe nam generowaƒá dane i dopasowywaƒá modele dla ka≈ºdego scenariusza:\n\ngeneruj_i_dopasuj &lt;- function(n, wyraz_wolny, nachylenie, odch_szumu, stopien_modelu) {\n  # Generowanie syntetycznych danych\n  x &lt;- runif(n, 0, 10)\n  y &lt;- wyraz_wolny + nachylenie * x + rnorm(n, 0, odch_szumu)\n  \n  # Dopasowanie modelu\n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", stopien_modelu, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  # Generowanie przewidywa≈Ñ\n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  # Wykres\n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    geom_abline(intercept = wyraz_wolny, slope = nachylenie, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Stopie≈Ñ Modelu:\", stopien_modelu),\n         subtitle = paste(\"Odchylenie Standardowe Szumu:\", odch_szumu),\n         x = \"X (Zmienna Wej≈õciowa)\",\n         y = \"Y (Zmienna Docelowa)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nTa funkcja wykonuje nastƒôpujƒÖce czynno≈õci: 1. Generuje syntetyczne dane na podstawie naszych parametr√≥w 2. Dopasowuje model regresji wielomianowej 3. Tworzy wykres pokazujƒÖcy prawdziwƒÖ zale≈ºno≈õƒá (niebieska przerywana linia), przewidywania naszego modelu (czerwona ciƒÖg≈Ça linia) i punkty danych\nTeraz zbadajmy nasze cztery scenariusze!\n\n\nScenariusz 1: Niskie ObciƒÖ≈ºenie, Niska Wariancja\nW tym idealnym scenariuszu u≈ºywamy modelu liniowego do dopasowania danych liniowych z niskim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 1, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyja≈õnienie: - Model (czerwona linia) ≈õci≈õle podƒÖ≈ºa za prawdziwƒÖ zale≈ºno≈õciƒÖ (niebieska przerywana linia). - Punkty danych sƒÖ skupione blisko linii, co wskazuje na niski szum. - Ten scenariusz reprezentuje dobre dopasowanie: model uchwyci≈Ç podstawowy trend bez nadmiernej z≈Ço≈ºono≈õci.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#scenariusz-2-niskie-obciƒÖ≈ºenie-wysoka-wariancja",
    "href": "rozdzial3b.html#scenariusz-2-niskie-obciƒÖ≈ºenie-wysoka-wariancja",
    "title": "8¬† Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych",
    "section": "8.13 Scenariusz 2: Niskie ObciƒÖ≈ºenie, Wysoka Wariancja",
    "text": "8.13 Scenariusz 2: Niskie ObciƒÖ≈ºenie, Wysoka Wariancja\nTutaj u≈ºywamy modelu liniowego do dopasowania danych liniowych, ale z wysokim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 5, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyja≈õnienie: - Model nadal uchwyci≈Ç og√≥lny trend, ale punkty danych sƒÖ bardziej rozproszone. - Ta wysoka wariancja oznacza, ≈ºe przewidywania naszego modelu by≈Çyby mniej wiarygodne. - W rzeczywistych warunkach mog≈Çoby to reprezentowaƒá sytuacjƒô, w kt√≥rej nasze pomiary sƒÖ ≈õrednio poprawne, ale majƒÖ du≈ºo losowego b≈Çƒôdu.\n\nScenariusz 3: Wysokie ObciƒÖ≈ºenie, Niska Wariancja\nW tym przypadku u≈ºywamy modelu liniowego do dopasowania danych kwadratowych (zakrzywionych) z niskim szumem.\n\ndane_kwadratowe &lt;- function(n, wyraz_wolny, nachylenie, odch_szumu, stopien_modelu) {\n  x &lt;- runif(n, 0, 10)\n  y &lt;- wyraz_wolny + nachylenie * x^2 + rnorm(n, 0, odch_szumu)\n  \n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", stopien_modelu, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    stat_function(fun = function(x) wyraz_wolny + nachylenie * x^2, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Stopie≈Ñ Modelu:\", stopien_modelu),\n         subtitle = paste(\"Odchylenie Standardowe Szumu:\", odch_szumu),\n         x = \"X (Zmienna Wej≈õciowa)\",\n         y = \"Y (Zmienna Docelowa)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\ndane_kwadratowe(n = 100, wyraz_wolny = 1, nachylenie = 0.2, odch_szumu = 1, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyja≈õnienie: - Model liniowy (czerwona linia) nie uchwyci≈Ç krzywizny prawdziwej zale≈ºno≈õci (niebieska przerywana linia). - To wysokie obciƒÖ≈ºenie oznacza, ≈ºe nasz model konsekwentnie myli siƒô w swoich przewidywaniach. - W rzeczywistych warunkach mog≈Çoby to reprezentowaƒá u≈ºycie zbyt uproszczonego modelu dla z≈Ço≈ºonego zjawiska.\n\n\nScenariusz 4: Wysokie ObciƒÖ≈ºenie, Wysoka Wariancja\nNa koniec u≈ºywamy wielomianu wysokiego stopnia do dopasowania danych liniowych z wysokim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 5, stopien_modelu = 5)\n\n\n\n\n\n\n\n\nWyja≈õnienie: - Model (czerwona linia) jest zbyt z≈Ço≈ºony, pr√≥bujƒÖc dopasowaƒá siƒô do szumu zamiast do podstawowego trendu. - Ta kombinacja wysokiego obciƒÖ≈ºenia i wysokiej wariancji prowadzi do s≈Çabej generalizacji. - W rzeczywistych warunkach mog≈Çoby to reprezentowaƒá nadmierne skomplikowanie naszej analizy i wyciƒÖganie fa≈Çszywych wniosk√≥w z losowych fluktuacji w naszych danych.\n\n\nPodsumowanie\nZrozumienie kompromisu miƒôdzy obciƒÖ≈ºeniem a wariancjƒÖ jest kluczowe w modelowaniu statystycznym:\n\nNiskie ObciƒÖ≈ºenie, Niska Wariancja: Idealny scenariusz, w kt√≥rym nasz model dok≈Çadnie uchwyci≈Ç podstawowƒÖ zale≈ºno≈õƒá bez nadmiernej wra≈ºliwo≈õci na szum.\nNiskie ObciƒÖ≈ºenie, Wysoka Wariancja: Nasz model jest ≈õrednio poprawny, ale niewiarygodny ze wzglƒôdu na wysokƒÖ wra≈ºliwo≈õƒá na pojedyncze punkty danych.\nWysokie ObciƒÖ≈ºenie, Niska Wariancja: Nasz model jest konsekwentnie b≈Çƒôdny z powodu nadmiernego uproszczenia, ale daje stabilne przewidywania.\nWysokie ObciƒÖ≈ºenie, Wysoka Wariancja: Najgorszy scenariusz, w kt√≥rym nasz model jest zar√≥wno niedok≈Çadny, jak i niewiarygodny.\n\nW praktyce czƒôsto musimy zr√≥wnowa≈ºyƒá obciƒÖ≈ºenie i wariancjƒô. Techniki takie jak walidacja krzy≈ºowa, regularyzacja i metody zespo≈Çowe mogƒÖ pom√≥c w znalezieniu tej r√≥wnowagi.\nPamiƒôtaj: - Model z wysokim obciƒÖ≈ºeniem jest zbyt prosty i pomija wa≈ºne wzorce w danych. - Model z wysokƒÖ wariancjƒÖ jest zbyt z≈Ço≈ºony i dopasowuje siƒô do szumu w danych treningowych. - Celem jest znalezienie z≈Çotego ≈õrodka, kt√≥ry uchwyci prawdziwe wzorce bez nadmiernego dopasowania do szumu.\nZrozumienie tych pojƒôƒá pomo≈ºe ci lepiej wybieraƒá odpowiednie modele, unikaƒá przeuczenia i niedouczenia oraz budowaƒá bardziej efektywne modele predykcyjne w przysz≈Çych analizach statystycznych!",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Rzetelno≈õƒá i Trafno≈õƒá w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "9¬† Research Designs: Experimental and Non-Experimental Approaches",
    "section": "",
    "text": "9.1 Introduction\nResearch designs are fundamental to the scientific process, providing structured approaches to investigate hypotheses and answer research questions. This chapter explores two main categories of research designs: experimental and non-experimental, with a focus on the Neyman-Rubin potential outcome framework. We‚Äôll delve into various design types, their characteristics, and provide practical examples using R for data analysis and visualization.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#experimental-designs",
    "href": "chapter4.html#experimental-designs",
    "title": "9¬† Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.2 Experimental Designs",
    "text": "9.2 Experimental Designs\nExperimental designs are characterized by the researcher‚Äôs control over the independent variable(s) and random assignment of subjects to different conditions. These designs are considered the gold standard for establishing causal relationships.\n\nRandomized Controlled Trials (RCTs)\nRCTs are the most rigorous form of experimental design. They involve:\n\nRandom assignment of subjects to treatment and control groups\nManipulation of the independent variable\nMeasurement of the dependent variable\n\nLet‚Äôs visualize a simple RCT design:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n# Create sample data\nn &lt;- 100\ndata &lt;- data.frame(\n  id = 1:n,\n  group = factor(rep(c(\"Control\", \"Treatment\"), each = n/2)),\n  pre_test = rnorm(n, mean = 50, sd = 10),\n  post_test = NA\n)\n\n# Simulate treatment effect\ndata$post_test &lt;- ifelse(data$group == \"Treatment\",\n                         data$pre_test + rnorm(n/2, mean = 10, sd = 5),\n                         data$pre_test + rnorm(n/2, mean = 0, sd = 5))\n\n# Reshape data for plotting\ndata_long &lt;- tidyr::pivot_longer(data, cols = c(pre_test, post_test),\n                                 names_to = \"time\", values_to = \"score\")\n\n# Create plot\nggplot(data_long, aes(x = time, y = score, color = group, group = interaction(id, group))) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.5) +\n  stat_summary(aes(group = group), fun = mean, geom = \"line\", size = 1.5) +\n  labs(title = \"Pre-test and Post-test Scores in RCT\",\n       x = \"Time\", y = \"Score\", color = \"Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\nRandomized Controlled Trial Design\n\n\n\n\nThis plot shows individual trajectories and group means for pre-test and post-test scores in a hypothetical RCT. The treatment group shows a clear increase in scores compared to the control group.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#ab-testing-an-example-and-comparison-with-rcts",
    "href": "chapter4.html#ab-testing-an-example-and-comparison-with-rcts",
    "title": "9¬† Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.3 A/B Testing: An Example and Comparison with RCTs",
    "text": "9.3 A/B Testing: An Example and Comparison with RCTs\nA/B testing is a widely used experimental method in digital marketing, user experience design, and product development. This chapter will present an example of A/B testing, explain its methodology, and discuss how it differs from Randomized Controlled Trials (RCTs).\n\nExample: Website Landing Page Conversion Rate\nLet‚Äôs consider an example where an e-commerce company wants to improve the conversion rate of their landing page. They decide to test two different layouts: the current layout (A) and a new layout (B).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(1234)\n\n# Simulate data\nn_visitors &lt;- 10000\ndata &lt;- data.frame(\n  Version = sample(c(\"A\", \"B\"), n_visitors, replace = TRUE),\n  Converted = rbinom(n_visitors, 1, ifelse(sample(c(\"A\", \"B\"), n_visitors, replace = TRUE) == \"A\", 0.10, 0.12))\n)\n\n# Calculate conversion rates\nconversion_rates &lt;- data %&gt;%\n  group_by(Version) %&gt;%\n  summarise(\n    Visitors = n(),\n    Conversions = sum(Converted),\n    ConversionRate = mean(Converted)\n  )\n\n# Visualize results\nggplot(conversion_rates, aes(x = Version, y = ConversionRate, fill = Version)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", ConversionRate * 100)), \n            vjust = -0.5, size = 4) +\n  theme_minimal() +\n  labs(title = \"A/B Test: Landing Page Conversion Rates\",\n       x = \"Page Version\", y = \"Conversion Rate\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure¬†9.1: A/B Test Results: Landing Page Conversion Rates\n\n\n\n\n\nIn this example, we simulated data for 10,000 visitors randomly assigned to either version A or B of the landing page. The results show that version B has a slightly higher conversion rate (11.44%) compared to version A (10.94%).\n\n\nA/B Testing Methodology\nA/B testing typically follows these steps:\n\nIdentify the element to be tested (e.g., landing page layout).\nCreate two versions: the control (A) and the variant (B).\nRandomly assign visitors to either version.\nCollect data on the metric of interest (e.g., conversion rate).\nAnalyze the results using statistical methods.\nMake a decision based on the results.\n\n\n\nDifferences between A/B Testing and RCTs\nWhile A/B testing and Randomized Controlled Trials (RCTs) share some similarities, they have several key differences:\n\nScope and Context:\n\nA/B Testing: Typically used in digital environments for quick, iterative improvements.\nRCTs: Used in various fields, including medicine, psychology, and social sciences, often for more complex interventions.\n\nDuration:\n\nA/B Testing: Usually shorter, often running for days or weeks.\nRCTs: Can last months or years, especially in medical research.\n\nSample Size:\n\nA/B Testing: Can involve very large sample sizes due to ease of implementation in digital platforms.\nRCTs: Sample sizes are often smaller due to practical and cost constraints.\n\nBlinding:\n\nA/B Testing: Participants are usually unaware they‚Äôre part of a test.\nRCTs: May involve single, double, or triple blinding to reduce bias.\n\nEthical Considerations:\n\nA/B Testing: Generally involves low-risk changes with minimal ethical concerns.\nRCTs: Often require extensive ethical review, especially in medical contexts.\n\nOutcome Measures:\n\nA/B Testing: Typically focuses on a single, easily measurable outcome (e.g., click-through rate).\nRCTs: Often measure multiple outcomes, including potential side effects or long-term impacts.\n\nGeneralizability:\n\nA/B Testing: Results are often specific to the platform or context tested.\nRCTs: Aim for broader generalizability, though this can vary.\n\nAnalysis Complexity:\n\nA/B Testing: Often uses simpler statistical analyses.\nRCTs: May involve more complex statistical methods to account for various factors.\n\n\nA/B testing is a powerful tool for making data-driven decisions in digital environments. While it shares the fundamental principle of randomization with RCTs, it is typically simpler, faster, and more focused on specific, measurable outcomes in digital contexts. Understanding these differences helps researchers and practitioners choose the most appropriate method for their specific needs and constraints.\n\n\nExample 1: Effect of Sleep Duration on Cognitive Performance\nResearch Question: Does increasing sleep duration improve cognitive performance in college students?\n\n# Generating sample data\nset.seed(456)\nn &lt;- 100\npre_experimental &lt;- rnorm(n, mean = 70, sd = 10)\npost_experimental &lt;- pre_experimental + rnorm(n, mean = 8, sd = 5)\npre_control &lt;- rnorm(n, mean = 70, sd = 10)\npost_control &lt;- pre_control + rnorm(n, mean = 1, sd = 5)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"Experimental\", \"Control\"), each = n*2),\n  Time = rep(rep(c(\"Pre\", \"Post\"), each = n), 2),\n  Score = c(pre_experimental, post_experimental, pre_control, post_control)\n)\n\n# Creating the plot\nggplot(data, aes(x = Time, y = Score, color = Group, group = Group)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Effect of Increased Sleep Duration on Cognitive Performance\") +\n  xlab(\"Time\") +\n  ylab(\"Cognitive Performance Score\")\n\n\n\n\n\n\n\nFigure¬†9.2: Effect of Sleep Duration on Cognitive Performance\n\n\n\n\n\n\nInterpretation\nThis plot demonstrates the effect of increased sleep duration on cognitive performance. The experimental group, which increased their sleep duration, shows a more substantial improvement in cognitive performance compared to the control group. This suggests that increasing sleep duration may positively impact cognitive abilities in college students.\n\n\n\nExample 2: Impact of Mindfulness Training on Stress Levels\nResearch Question: Can a short-term mindfulness training program reduce stress levels in healthcare workers?\n\n# Generating sample data\nset.seed(789)\nn &lt;- 120\npre_experimental &lt;- rnorm(n, mean = 60, sd = 15)\npost_experimental &lt;- pre_experimental + rnorm(n, mean = -12, sd = 8)\npre_control &lt;- rnorm(n, mean = 60, sd = 15)\npost_control &lt;- pre_control + rnorm(n, mean = -2, sd = 6)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"Mindfulness\", \"Control\"), each = n*2),\n  Time = rep(rep(c(\"Pre\", \"Post\"), each = n), 2),\n  StressScore = c(pre_experimental, post_experimental, pre_control, post_control)\n)\n\n# Creating the plot\nggplot(data, aes(x = Time, y = StressScore, color = Group, group = Group)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Impact of Mindfulness Training on Stress Levels\") +\n  xlab(\"Time\") +\n  ylab(\"Stress Score\")\n\n\n\n\n\n\n\nFigure¬†9.3: Impact of Mindfulness Training on Stress Levels\n\n\n\n\n\n\nInterpretation\nThis visualization illustrates the impact of a mindfulness training program on stress levels in healthcare workers. The mindfulness group shows a more significant decrease in stress scores compared to the control group. This suggests that the mindfulness training program may be effective in reducing stress levels among healthcare workers.\nWhen interpreting such results, it‚Äôs important to consider:\n\nThe magnitude of the change in each group\nThe difference in change between the experimental and control groups\nThe variability within each group\nAny potential confounding factors not accounted for in the experimental design\n\nThese examples provide a template for visualizing and interpreting similar experimental designs across different research contexts.\n\n\n\nFactorial Designs\nFactorial designs allow researchers to study the effects of multiple independent variables simultaneously. They are efficient and can reveal interaction effects between variables.\nExample of a 2x2 factorial design:\n\n# Create sample data for 2x2 factorial design\nset.seed(456)\nn_per_group &lt;- 25\n\nfactorial_data &lt;- data.frame(\n  factor_a = rep(rep(c(\"Low\", \"High\"), each = n_per_group), 2),\n  factor_b = rep(c(\"Control\", \"Treatment\"), each = n_per_group * 2),\n  outcome = NA\n)\n\n# Generate outcomes\nfactorial_data$outcome &lt;- ifelse(factorial_data$factor_a == \"Low\" & factorial_data$factor_b == \"Control\",\n                                 rnorm(n_per_group, 40, 5),\n                                 ifelse(factorial_data$factor_a == \"Low\" & factorial_data$factor_b == \"Treatment\",\n                                        rnorm(n_per_group, 45, 5),\n                                        ifelse(factorial_data$factor_a == \"High\" & factorial_data$factor_b == \"Control\",\n                                               rnorm(n_per_group, 50, 5),\n                                               rnorm(n_per_group, 60, 5))))\n\n# Create plot\nggplot(factorial_data, aes(x = factor_b, y = outcome, fill = factor_a)) +\n  geom_boxplot() +\n  facet_wrap(~factor_a, scales = \"free_x\") +\n  labs(title = \"2x2 Factorial Design\",\n       x = \"Factor B\", y = \"Outcome\", fill = \"Factor A\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n2x2 Factorial Design\n\n\n\n\nThis plot illustrates a 2x2 factorial design, showing the effects of two factors (A and B) on the outcome variable. We can observe main effects for both factors and a potential interaction effect.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#non-experimental-designs",
    "href": "chapter4.html#non-experimental-designs",
    "title": "9¬† Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.4 Non-Experimental Designs",
    "text": "9.4 Non-Experimental Designs\nNon-experimental designs are used when randomization or manipulation of variables is not possible or ethical. They include observational/descriptive studies and quasi-experimental designs.\n\nObservational Studies\nObservational studies involve collecting data without manipulating variables. They are useful for exploring relationships and generating hypotheses.\nExample: Correlation study\n\nset.seed(789)\nn &lt;- 100\nstudy_time &lt;- runif(n, 0, 10)\nexam_score &lt;- 50 + 5 * study_time + rnorm(n, 0, 10)\n\ncorrelation_data &lt;- data.frame(study_time, exam_score)\n\nggplot(correlation_data, aes(x = study_time, y = exam_score)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Correlation between Study Time and Exam Score\",\n       x = \"Study Time (hours)\", y = \"Exam Score\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCorrelation between Study Time and Exam Score\n\n\n\n\nThis scatter plot shows the relationship between study time and exam scores, illustrating a positive correlation typical in observational studies.\n\n\nQuasi-Experimental Designs\nQuasi-experimental designs lack random assignment but attempt to establish causal relationships. Common types include:\n\nDifference-in-Differences (DiD)\nRegression Discontinuity Design (RDD)\n\n\nDifference-in-Differences (DiD)\nDiD is used to estimate treatment effects by comparing the average change over time in the outcome variable for the treatment group to the average change over time for the control group.\nLet‚Äôs simulate a DiD analysis using the plm package:\n\nlibrary(plm)\n\nWarning: package 'plm' was built under R version 4.4.3\n\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(101)\n\n# Generate synthetic panel data\nn &lt;- 1000\ntime_periods &lt;- 5\nintervention_time &lt;- 3\n\npanel_data &lt;- data.frame(\n  id = rep(1:n, each = time_periods),\n  time = rep(1:time_periods, times = n),\n  treatment = rep(sample(c(0, 1), n, replace = TRUE), each = time_periods)\n)\n\n# Generate outcomes\npanel_data$outcome &lt;- with(panel_data,\n                           10 + 2 * time + 5 * treatment + \n                           3 * (time &gt;= intervention_time & treatment == 1) + \n                           rnorm(n * time_periods, 0, 2))\n\n# Create post-treatment indicator\npanel_data$post &lt;- as.integer(panel_data$time &gt;= intervention_time)\n\n# Estimate DiD model\ndid_model &lt;- plm(outcome ~ treatment * post, \n                 data = panel_data, \n                 index = c(\"id\", \"time\"), \n                 model = \"within\")\n\n# Summarize results\nsummary_did &lt;- summary(did_model)\n\n# Calculate group means for each time period\ngroup_means &lt;- aggregate(outcome ~ time + treatment, data = panel_data, FUN = mean)\n\n# Visualize DiD\nggplot(group_means, aes(x = time, y = outcome, color = factor(treatment), group = treatment)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = intervention_time, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Difference-in-Differences Analysis\",\n       subtitle = paste(\"Estimated treatment effect:\", round(coef(did_model)[\"treatment:post\"], 3)),\n       x = \"Time\", y = \"Outcome\", color = \"Treatment Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Control\", \"Treatment\")) +\n  scale_x_continuous(breaks = 1:time_periods)\n\n\n\n\nDifference-in-Differences Analysis\n\n\n\n# Print model summary\nprint(summary_did)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = outcome ~ treatment * post, data = panel_data, \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 1000, T = 5, N = 5000\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-7.509908 -1.625814  0.001753  1.610009  8.047479 \n\nCoefficients:\n               Estimate Std. Error t-value  Pr(&gt;|t|)    \npost            5.05692    0.10315  49.026 &lt; 2.2e-16 ***\ntreatment:post  2.89003    0.14935  19.351 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    78894\nResidual Sum of Squares: 26696\nR-Squared:      0.66163\nAdj. R-Squared: 0.57691\nF-statistic: 3908.68 on 2 and 3998 DF, p-value: &lt; 2.22e-16\n\n\nThe plot shows the average outcomes for treatment and control groups over time. The vertical dashed line indicates the intervention point. The DiD estimate is the difference between the two groups‚Äô changes from pre- to post-intervention periods.\nDiD Model:\n\nThe model outcome ~ treatment * post estimates:\nThe average treatment effect on the treated (ATT) after the intervention\nThe coefficient on treatment:post represents this effect\n\nInterpretation of Results: Looking at the model summary:\n\nThe coefficient for treatment:post is the DiD estimator. It represents the average treatment effect on the treated after the intervention.\nIf this coefficient is statistically significant, it suggests that the treatment had a causal effect on the outcome.\nThe magnitude of this coefficient tells us the size of the treatment effect.\n\nVisualization: The plot shows:\n\nSeparate trend lines for the treatment and control groups\nA vertical dashed line indicating the intervention time\nThe parallel trends assumption can be visually assessed by looking at the pre-intervention period\nThe divergence of the lines after the intervention represents the treatment effect\n\nAssumptions and Limitations:\nIt‚Äôs important to note some key assumptions of DiD:\n\nParallel trends: In the absence of treatment, the difference between the treatment and control groups would remain constant over time.\nNo spillover effects: The treatment does not affect the control group.\nNo compositional changes: The composition of treatment and control groups remains stable over time.\n\n\n\nRegression Discontinuity Design (RDD)\nRDD is used when treatment assignment is determined by a cutoff value on a continuous variable. It compares observations just above and below the cutoff to estimate the treatment effect.\nLet‚Äôs implement an RDD analysis using the rdrobust package:\n\nlibrary(rdrobust)\n\nWarning: package 'rdrobust' was built under R version 4.4.3\n\n# Generate synthetic RDD data\nset.seed(202)\nn &lt;- 1000\nx &lt;- runif(n, -1, 1)\ny &lt;- 3 + 2 * x + 4 * (x &gt;= 0) + rnorm(n, 0, 1)\n\nrdd_data &lt;- data.frame(x, y)\n\n# RDD analysis\nrdd_result &lt;- rdrobust(y, x, c = 0)\nsummary(rdd_result)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  499          501\nEff. Number of Obs.             182          175\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.362        0.362\nBW bias (b)                   0.575        0.575\nrho (h/b)                     0.630        0.630\nUnique Obs.                     499          501\n\n=====================================================================\n                   Point    Robust Inference\n                Estimate         z     P&gt;|z|      [ 95% C.I. ]       \n---------------------------------------------------------------------\n     RD Effect     4.092    15.013     0.000     [3.600 , 4.680]     \n=====================================================================\n\n# Visualize RDD\nggplot(rdd_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(data = subset(rdd_data, x &lt; 0), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(data = subset(rdd_data, x &gt;= 0), method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Regression Discontinuity Design\",\n       x = \"Running Variable\", y = \"Outcome\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nRegression Discontinuity Design Analysis\n\n\n\n\nThe plot shows the discontinuity at the cutoff point (x = 0), with separate regression lines fitted on either side. The treatment effect is estimated by the gap between these lines at the cutoff.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#the-neyman-rubin-potential-outcome-framework",
    "href": "chapter4.html#the-neyman-rubin-potential-outcome-framework",
    "title": "9¬† Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.5 The Neyman-Rubin Potential Outcome Framework",
    "text": "9.5 The Neyman-Rubin Potential Outcome Framework\nThe Neyman-Rubin potential outcome framework provides a formal approach to causal inference. It introduces the concept of potential outcomes: for each unit, we consider the outcome under treatment and the outcome under control, even though we can only observe one in reality.\nKey concepts:\n\nPotential Outcomes: Y_i(1) and Y_i(0) for treatment and control, respectively.\nObserved Outcome: Y_i = Y_i(1)T_i + Y_i(0)(1-T_i), where T_i is the treatment indicator.\nIndividual Treatment Effect: \\tau_i = Y_i(1) - Y_i(0)\nAverage Treatment Effect (ATE): E[\\tau_i] = E[Y_i(1) - Y_i(0)]\n\nThe framework emphasizes the ‚Äúfundamental problem of causal inference‚Äù: we can never observe both potential outcomes for a single unit simultaneously.\n\nExample: Estimating ATE in an RCT\nIn an RCT, random assignment ensures that treatment is independent of potential outcomes, allowing unbiased estimation of the ATE:\n\n\\hat{ATE} = \\frac{1}{n_1} \\sum_{i:T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i:T_i=0} Y_i\n\nWhere n_1 and n_0 are the numbers of treated and control units, respectively.\n\n# Using the RCT data from earlier\nate_estimate &lt;- mean(data$post_test[data$group == \"Treatment\"]) - \n                mean(data$post_test[data$group == \"Control\"])\n\nWarning in mean.default(data$post_test[data$group == \"Treatment\"]): argument is\nnot numeric or logical: returning NA\n\n\nWarning in mean.default(data$post_test[data$group == \"Control\"]): argument is\nnot numeric or logical: returning NA\n\ncat(\"Estimated Average Treatment Effect:\", round(ate_estimate, 2))\n\nEstimated Average Treatment Effect: NA\n\n\nThis estimate represents the causal effect of the treatment under the assumptions of the potential outcome framework.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#conclusion",
    "href": "chapter4.html#conclusion",
    "title": "9¬† Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.6 Conclusion",
    "text": "9.6 Conclusion\nThis chapter has explored various research designs, from experimental approaches like RCTs and factorial designs to non-experimental methods such as observational studies and quasi-experimental designs. We‚Äôve demonstrated how to implement and visualize these designs using R, and introduced the Neyman-Rubin potential outcome framework for causal inference.\nUnderstanding these designs and their appropriate use is crucial for conducting rigorous research and drawing valid causal conclusions. Each design has its strengths and limitations, and the choice of design should be guided by the research question, ethical considerations, and practical constraints.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#references",
    "href": "chapter4.html#references",
    "title": "9¬† Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.7 References",
    "text": "9.7 References\n\nImbens, G. W., & Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge University Press.\nAngrist, J. D., & Pischke, J. S. (2008). Mostly Harmless Econometrics: An Empiricist‚Äôs Companion. Princeton University Press.\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and Quasi-Experimental Designs for Generalized Causal Inference. Houghton Mifflin.\nCunningham, S. (2021). Causal Inference: The Mixtape. Yale University Press.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html",
    "href": "rozdzial4.html",
    "title": "10¬† Projekty Badawcze: Podej≈õcia Eksperymentalne i Nieeksperymentalne",
    "section": "",
    "text": "10.1 Wstƒôp\nProjekty badawcze stanowiƒÖ fundament procesu naukowego, zapewniajƒÖc ustrukturyzowane podej≈õcie do badania hipotez i odpowiadania na pytania badawcze. Ten rozdzia≈Ç analizuje dwie g≈Ç√≥wne kategorie projekt√≥w badawczych: eksperymentalne i nieeksperymentalne, ze szczeg√≥lnym uwzglƒôdnieniem modelu potencjalnych wynik√≥w Neymana-Rubina. Zag≈Çƒôbimy siƒô w r√≥≈ºne typy projekt√≥w, ich charakterystykƒô i przedstawimy praktyczne przyk≈Çady wykorzystania R do analizy danych i wizualizacji.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Projekty Badawcze: Podej≈õcia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#projekty-eksperymentalne",
    "href": "rozdzial4.html#projekty-eksperymentalne",
    "title": "10¬† Projekty Badawcze: Podej≈õcia Eksperymentalne i Nieeksperymentalne",
    "section": "10.2 Projekty Eksperymentalne",
    "text": "10.2 Projekty Eksperymentalne\nProjekty eksperymentalne charakteryzujƒÖ siƒô kontrolƒÖ badacza nad zmiennƒÖ(ymi) niezale≈ºnƒÖ(ymi) oraz losowym przydzia≈Çem uczestnik√≥w do r√≥≈ºnych warunk√≥w. Te projekty sƒÖ uwa≈ºane za z≈Çoty standard w ustalaniu zwiƒÖzk√≥w przyczynowych.\n\nRandomizowane Badania Kontrolowane (RCT)\nRCT sƒÖ najbardziej rygorystycznƒÖ formƒÖ projektu eksperymentalnego. ObejmujƒÖ one:\n\nLosowy przydzia≈Ç uczestnik√≥w do grup eksperymentalnej i kontrolnej\nManipulacjƒô zmiennƒÖ niezale≈ºnƒÖ\nPomiar zmiennej zale≈ºnej\n\nZobaczmy wizualizacjƒô prostego projektu RCT:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n# Tworzenie przyk≈Çadowych danych\nn &lt;- 100\ndata &lt;- data.frame(\n  id = 1:n,\n  grupa = factor(rep(c(\"Kontrolna\", \"Eksperymentalna\"), each = n/2)),\n  pre_test = rnorm(n, mean = 50, sd = 10),\n  post_test = NA\n)\n\n# Symulacja efektu leczenia\ndata$post_test &lt;- ifelse(data$grupa == \"Eksperymentalna\",\n                         data$pre_test + rnorm(n/2, mean = 10, sd = 5),\n                         data$pre_test + rnorm(n/2, mean = 0, sd = 5))\n\n# Przekszta≈Çcenie danych do formatu d≈Çugiego\ndata_long &lt;- tidyr::pivot_longer(data, cols = c(pre_test, post_test),\n                                 names_to = \"czas\", values_to = \"wynik\")\n\n# Tworzenie wykresu\nggplot(data_long, aes(x = czas, y = wynik, color = grupa, group = interaction(id, grupa))) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.5) +\n  stat_summary(aes(group = grupa), fun = mean, geom = \"line\", size = 1.5) +\n  labs(title = \"Wyniki Pre-test i Post-test w RCT\",\n       x = \"Czas\", y = \"Wynik\", color = \"Grupa\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_x_discrete(labels = c(\"pre_test\" = \"Pre-test\", \"post_test\" = \"Post-test\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\nProjekt Randomizowanego Badania Kontrolowanego\n\n\n\n\nTen wykres pokazuje indywidualne trajektorie i ≈õrednie grupowe dla wynik√≥w pre-test i post-test w hipotetycznym RCT. Grupa eksperymentalna wykazuje wyra≈∫ny wzrost wynik√≥w w por√≥wnaniu do grupy kontrolnej.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Projekty Badawcze: Podej≈õcia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#testy-ab-przyk≈Çad-i-por√≥wnanie-z-rct",
    "href": "rozdzial4.html#testy-ab-przyk≈Çad-i-por√≥wnanie-z-rct",
    "title": "10¬† Projekty Badawcze: Podej≈õcia Eksperymentalne i Nieeksperymentalne",
    "section": "10.3 Testy A/B: Przyk≈Çad i Por√≥wnanie z RCT",
    "text": "10.3 Testy A/B: Przyk≈Çad i Por√≥wnanie z RCT\nTesty A/B to szeroko stosowana metoda eksperymentalna w marketingu cyfrowym, projektowaniu do≈õwiadcze≈Ñ u≈ºytkownika i rozwoju produkt√≥w. Ten rozdzia≈Ç przedstawi przyk≈Çad testu A/B, wyja≈õni jego metodologiƒô i om√≥wi, czym r√≥≈ºni siƒô od Randomizowanych Bada≈Ñ Kontrolowanych (RCT).\n\nPrzyk≈Çad: Wsp√≥≈Çczynnik Konwersji Strony Docelowej\nRozwa≈ºmy przyk≈Çad, w kt√≥rym firma e-commerce chce poprawiƒá wsp√≥≈Çczynnik konwersji swojej strony docelowej. DecydujƒÖ siƒô przetestowaƒá dwa r√≥≈ºne uk≈Çady: obecny uk≈Çad (A) i nowy uk≈Çad (B).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(1234)\n\n# Symulacja danych\nn_odwiedzajacych &lt;- 10000\ndane &lt;- data.frame(\n  Wersja = sample(c(\"A\", \"B\"), n_odwiedzajacych, replace = TRUE),\n  Konwersja = rbinom(n_odwiedzajacych, 1, ifelse(sample(c(\"A\", \"B\"), n_odwiedzajacych, replace = TRUE) == \"A\", 0.10, 0.12))\n)\n\n# Obliczenie wsp√≥≈Çczynnik√≥w konwersji\nwspolczynniki_konwersji &lt;- dane %&gt;%\n  group_by(Wersja) %&gt;%\n  summarise(\n    Odwiedzajacy = n(),\n    Konwersje = sum(Konwersja),\n    WspolczynnikKonwersji = mean(Konwersja)\n  )\n\n# Wizualizacja wynik√≥w\nggplot(wspolczynniki_konwersji, aes(x = Wersja, y = WspolczynnikKonwersji, fill = Wersja)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", WspolczynnikKonwersji * 100)), \n            vjust = -0.5, size = 4) +\n  theme_minimal() +\n  labs(title = \"Test A/B: Wsp√≥≈Çczynniki Konwersji Strony Docelowej\",\n       x = \"Wersja Strony\", y = \"Wsp√≥≈Çczynnik Konwersji\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure¬†10.1: Wyniki Testu A/B: Wsp√≥≈Çczynniki Konwersji Strony Docelowej\n\n\n\n\n\nW tym przyk≈Çadzie zasymulowali≈õmy dane dla 10 000 odwiedzajƒÖcych losowo przypisanych do wersji A lub B strony docelowej. Wyniki pokazujƒÖ, ≈ºe wersja B ma nieco wy≈ºszy wsp√≥≈Çczynnik konwersji (11,44%) w por√≥wnaniu do wersji A (10,94%).\n\n\nMetodologia Test√≥w A/B\nTesty A/B zazwyczaj przebiegajƒÖ wed≈Çug nastƒôpujƒÖcych krok√≥w:\n\nZidentyfikowanie elementu do przetestowania (np. uk≈Çad strony docelowej).\nStworzenie dw√≥ch wersji: kontrolnej (A) i wariantu (B).\nLosowe przypisanie odwiedzajƒÖcych do jednej z wersji.\nZbieranie danych o interesujƒÖcej nas metryce (np. wsp√≥≈Çczynniku konwersji).\nAnaliza wynik√≥w przy u≈ºyciu metod statystycznych.\nPodjƒôcie decyzji na podstawie wynik√≥w.\n\n\n\nR√≥≈ºnice miƒôdzy Testami A/B a RCT\nChoƒá testy A/B i Randomizowane Badania Kontrolowane (RCT) majƒÖ pewne podobie≈Ñstwa, istnieje kilka kluczowych r√≥≈ºnic:\n\nZakres i Kontekst:\n\nTesty A/B: Zazwyczaj stosowane w ≈õrodowiskach cyfrowych do szybkich, iteracyjnych ulepsze≈Ñ.\nRCT: Stosowane w r√≥≈ºnych dziedzinach, w tym medycynie, psychologii i naukach spo≈Çecznych, czƒôsto dla bardziej z≈Ço≈ºonych interwencji.\n\nCzas Trwania:\n\nTesty A/B: Zwykle kr√≥tsze, czƒôsto trwajƒÖce dni lub tygodnie.\nRCT: MogƒÖ trwaƒá miesiƒÖce lub lata, szczeg√≥lnie w badaniach medycznych.\n\nWielko≈õƒá Pr√≥by:\n\nTesty A/B: MogƒÖ obejmowaƒá bardzo du≈ºe pr√≥by ze wzglƒôdu na ≈Çatwo≈õƒá implementacji na platformach cyfrowych.\nRCT: Wielko≈õci pr√≥b sƒÖ czƒôsto mniejsze ze wzglƒôdu na praktyczne i kosztowe ograniczenia.\n\nZa≈õlepienie:\n\nTesty A/B: Uczestnicy zazwyczaj nie sƒÖ ≈õwiadomi, ≈ºe biorƒÖ udzia≈Ç w te≈õcie.\nRCT: MogƒÖ obejmowaƒá pojedyncze, podw√≥jne lub potr√≥jne za≈õlepienie w celu zmniejszenia b≈Çƒôdu systematycznego.\n\nWzglƒôdy Etyczne:\n\nTesty A/B: Generalnie obejmujƒÖ zmiany niskiego ryzyka z minimalnymi obawami etycznymi.\nRCT: Czƒôsto wymagajƒÖ obszernej oceny etycznej, szczeg√≥lnie w kontek≈õcie medycznym.\n\nMiary Wynik√≥w:\n\nTesty A/B: Zazwyczaj skupiajƒÖ siƒô na pojedynczym, ≈Çatwo mierzalnym wyniku (np. wsp√≥≈Çczynnik klikalno≈õci).\nRCT: Czƒôsto mierzƒÖ wiele wynik√≥w, w tym potencjalne skutki uboczne lub d≈Çugoterminowe efekty.\n\nMo≈ºliwo≈õƒá Uog√≥lnienia:\n\nTesty A/B: Wyniki sƒÖ czƒôsto specyficzne dla testowanej platformy lub kontekstu.\nRCT: DƒÖ≈ºƒÖ do szerszej mo≈ºliwo≈õci uog√≥lnienia, choƒá mo≈ºe to siƒô r√≥≈ºniƒá.\n\nZ≈Ço≈ºono≈õƒá Analizy:\n\nTesty A/B: Czƒôsto wykorzystujƒÖ prostsze analizy statystyczne.\nRCT: MogƒÖ obejmowaƒá bardziej z≈Ço≈ºone metody statystyczne, aby uwzglƒôdniƒá r√≥≈ºne czynniki.\n\n\nTesty A/B sƒÖ potƒô≈ºnym narzƒôdziem do podejmowania decyzji opartych na danych w ≈õrodowiskach cyfrowych. Choƒá dzielƒÖ podstawowƒÖ zasadƒô randomizacji z RCT, sƒÖ zazwyczaj prostsze, szybsze i bardziej skoncentrowane na konkretnych, mierzalnych wynikach w kontekstach cyfrowych. Zrozumienie tych r√≥≈ºnic pomaga badaczom i praktykom wybraƒá najbardziej odpowiedniƒÖ metodƒô do ich konkretnych potrzeb i ogranicze≈Ñ.\nTesty A/B sƒÖ szczeg√≥lnie przydatne w optymalizacji stron internetowych, aplikacji mobilnych i kampanii marketingowych, gdzie szybkie iteracje i ciƒÖg≈Çe ulepszenia sƒÖ kluczowe. Z kolei RCT pozostajƒÖ z≈Çotym standardem w badaniach naukowych, szczeg√≥lnie w dziedzinach takich jak medycyna, gdzie rygorystyczna kontrola i d≈Çugoterminowa obserwacja sƒÖ niezbƒôdne.\nNiezale≈ºnie od wybranej metody, kluczowe jest staranne planowanie, precyzyjne wykonanie i ostro≈ºna interpretacja wynik√≥w. Zar√≥wno testy A/B, jak i RCT, gdy sƒÖ odpowiednio stosowane, mogƒÖ dostarczyƒá cennych informacji i przyczyniƒá siƒô do podejmowania lepszych decyzji opartych na danych.\n\n\nPrzyk≈Çad 1: Wp≈Çyw D≈Çugo≈õci Snu na Wydajno≈õƒá PoznawczƒÖ\nPytanie Badawcze: Czy zwiƒôkszenie d≈Çugo≈õci snu poprawia wydajno≈õƒá poznawczƒÖ u student√≥w?\n\n# Generowanie przyk≈Çadowych danych\nset.seed(456)\nn &lt;- 100\npre_eksperymentalna &lt;- rnorm(n, mean = 70, sd = 10)\npost_eksperymentalna &lt;- pre_eksperymentalna + rnorm(n, mean = 8, sd = 5)\npre_kontrolna &lt;- rnorm(n, mean = 70, sd = 10)\npost_kontrolna &lt;- pre_kontrolna + rnorm(n, mean = 1, sd = 5)\n\ndane &lt;- data.frame(\n  Grupa = rep(c(\"Eksperymentalna\", \"Kontrolna\"), each = n*2),\n  Czas = rep(rep(c(\"Przed\", \"Po\"), each = n), 2),\n  Wynik = c(pre_eksperymentalna, post_eksperymentalna, pre_kontrolna, post_kontrolna)\n)\n\n# Tworzenie wykresu\nggplot(dane, aes(x = Czas, y = Wynik, color = Grupa, group = Grupa)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Wp≈Çyw Zwiƒôkszonej D≈Çugo≈õci Snu na Wydajno≈õƒá PoznawczƒÖ\") +\n  xlab(\"Czas\") +\n  ylab(\"Wynik Wydajno≈õci Poznawczej\")\n\n\n\n\n\n\n\nFigure¬†10.2: Wp≈Çyw D≈Çugo≈õci Snu na Wydajno≈õƒá PoznawczƒÖ\n\n\n\n\n\n\nInterpretacja\nTen wykres pokazuje wp≈Çyw zwiƒôkszonej d≈Çugo≈õci snu na wydajno≈õƒá poznawczƒÖ. Grupa eksperymentalna, kt√≥ra zwiƒôkszy≈Ça d≈Çugo≈õƒá snu, wykazuje znacznie wiƒôkszƒÖ poprawƒô w wydajno≈õci poznawczej w por√≥wnaniu do grupy kontrolnej. Sugeruje to, ≈ºe zwiƒôkszenie d≈Çugo≈õci snu mo≈ºe pozytywnie wp≈Çywaƒá na zdolno≈õci poznawcze student√≥w.\n\n\n\nPrzyk≈Çad 2: Wp≈Çyw Treningu Uwa≈ºno≈õci na Poziom Stresu\nPytanie Badawcze: Czy kr√≥tkoterminowy program treningu uwa≈ºno≈õci mo≈ºe obni≈ºyƒá poziom stresu u pracownik√≥w s≈Çu≈ºby zdrowia?\n\n# Generowanie przyk≈Çadowych danych\nset.seed(789)\nn &lt;- 120\npre_eksperymentalna &lt;- rnorm(n, mean = 60, sd = 15)\npost_eksperymentalna &lt;- pre_eksperymentalna + rnorm(n, mean = -12, sd = 8)\npre_kontrolna &lt;- rnorm(n, mean = 60, sd = 15)\npost_kontrolna &lt;- pre_kontrolna + rnorm(n, mean = -2, sd = 6)\n\ndane &lt;- data.frame(\n  Grupa = rep(c(\"Uwa≈ºno≈õƒá\", \"Kontrolna\"), each = n*2),\n  Czas = rep(rep(c(\"Przed\", \"Po\"), each = n), 2),\n  PoziomStresu = c(pre_eksperymentalna, post_eksperymentalna, pre_kontrolna, post_kontrolna)\n)\n\n# Tworzenie wykresu\nggplot(dane, aes(x = Czas, y = PoziomStresu, color = Grupa, group = Grupa)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Wp≈Çyw Treningu Uwa≈ºno≈õci na Poziom Stresu\") +\n  xlab(\"Czas\") +\n  ylab(\"Poziom Stresu\")\n\n\n\n\n\n\n\nFigure¬†10.3: Wp≈Çyw Treningu Uwa≈ºno≈õci na Poziom Stresu\n\n\n\n\n\n\nInterpretacja\nTa wizualizacja ilustruje wp≈Çyw programu treningu uwa≈ºno≈õci na poziom stresu u pracownik√≥w s≈Çu≈ºby zdrowia. Grupa uwa≈ºno≈õci wykazuje znacznie wiƒôkszy spadek poziomu stresu w por√≥wnaniu do grupy kontrolnej. Sugeruje to, ≈ºe program treningu uwa≈ºno≈õci mo≈ºe byƒá skuteczny w redukcji poziomu stresu w≈õr√≥d pracownik√≥w s≈Çu≈ºby zdrowia.\n\n\n\nProjekty Czynnikowe\nProjekty czynnikowe pozwalajƒÖ badaczom na jednoczesne badanie efekt√≥w wielu zmiennych niezale≈ºnych. SƒÖ one efektywne i mogƒÖ ujawniaƒá efekty interakcji miƒôdzy zmiennymi.\nPrzyk≈Çad projektu czynnikowego 2x2:\n\n# Tworzenie przyk≈Çadowych danych dla projektu czynnikowego 2x2\nset.seed(456)\nn_per_group &lt;- 25\n\nfactorial_data &lt;- data.frame(\n  czynnik_a = rep(rep(c(\"Niski\", \"Wysoki\"), each = n_per_group), 2),\n  czynnik_b = rep(c(\"Kontrola\", \"Interwencja\"), each = n_per_group * 2),\n  wynik = NA\n)\n\n# Generowanie wynik√≥w\nfactorial_data$wynik &lt;- ifelse(factorial_data$czynnik_a == \"Niski\" & factorial_data$czynnik_b == \"Kontrola\",\n                               rnorm(n_per_group, 40, 5),\n                               ifelse(factorial_data$czynnik_a == \"Niski\" & factorial_data$czynnik_b == \"Interwencja\",\n                                      rnorm(n_per_group, 45, 5),\n                                      ifelse(factorial_data$czynnik_a == \"Wysoki\" & factorial_data$czynnik_b == \"Kontrola\",\n                                             rnorm(n_per_group, 50, 5),\n                                             rnorm(n_per_group, 60, 5))))\n\n# Tworzenie wykresu\nggplot(factorial_data, aes(x = czynnik_b, y = wynik, fill = czynnik_a)) +\n  geom_boxplot() +\n  facet_wrap(~czynnik_a, scales = \"free_x\") +\n  labs(title = \"Projekt Czynnikowy 2x2\",\n       x = \"Czynnik B\", y = \"Wynik\", fill = \"Czynnik A\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nProjekt Czynnikowy 2x2\n\n\n\n\nTen wykres ilustruje projekt czynnikowy 2x2, pokazujƒÖc efekty dw√≥ch czynnik√≥w (A i B) na zmiennƒÖ wynikowƒÖ. Mo≈ºemy zaobserwowaƒá g≈Ç√≥wne efekty dla obu czynnik√≥w oraz potencjalny efekt interakcji.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Projekty Badawcze: Podej≈õcia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#projekty-nieeksperymentalne",
    "href": "rozdzial4.html#projekty-nieeksperymentalne",
    "title": "10¬† Projekty Badawcze: Podej≈õcia Eksperymentalne i Nieeksperymentalne",
    "section": "10.4 Projekty Nieeksperymentalne",
    "text": "10.4 Projekty Nieeksperymentalne\nProjekty nieeksperymentalne sƒÖ stosowane, gdy randomizacja lub manipulacja zmiennymi nie jest mo≈ºliwa lub etyczna. ObejmujƒÖ one badania obserwacyjne/opisowe i quasi-eksperymentalne.\n\nBadania Obserwacyjne\nBadania obserwacyjne polegajƒÖ na zbieraniu danych bez manipulowania zmiennymi. SƒÖ one przydatne do eksploracji relacji i generowania hipotez.\nPrzyk≈Çad: Badanie korelacyjne\n\nset.seed(789)\nn &lt;- 100\nczas_nauki &lt;- runif(n, 0, 10)\nwynik_egzaminu &lt;- 50 + 5 * czas_nauki + rnorm(n, 0, 10)\n\ncorrelation_data &lt;- data.frame(czas_nauki, wynik_egzaminu)\n\nggplot(correlation_data, aes(x = czas_nauki, y = wynik_egzaminu)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Korelacja miƒôdzy Czasem Nauki a Wynikiem Egzaminu\",\n       x = \"Czas Nauki (godziny)\", y = \"Wynik Egzaminu\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nKorelacja miƒôdzy Czasem Nauki a Wynikiem Egzaminu\n\n\n\n\nTen wykres punktowy pokazuje relacjƒô miƒôdzy czasem nauki a wynikami egzaminu, ilustrujƒÖc pozytywnƒÖ korelacjƒô typowƒÖ dla bada≈Ñ obserwacyjnych.\n\n\nProjekty Quasi-Eksperymentalne\nProjekty quasi-eksperymentalne nie majƒÖ losowego przydzia≈Çu, ale pr√≥bujƒÖ ustaliƒá zwiƒÖzki przyczynowe. Popularne typy to:\n\nR√≥≈ºnica w R√≥≈ºnicach (DiD)\nRegresja NieciƒÖg≈Ça (RDD)\n\n\nR√≥≈ºnica w R√≥≈ºnicach (DiD)\nDiD jest u≈ºywana do oszacowania efekt√≥w interwencji poprzez por√≥wnanie ≈õredniej zmiany w czasie w zmiennej wynikowej dla grupy eksperymentalnej ze ≈õredniƒÖ zmianƒÖ w czasie dla grupy kontrolnej.\nPrzeprowad≈∫my symulacjƒô analizy DiD przy u≈ºyciu pakietu plm:\n\nlibrary(plm)\n\nWarning: package 'plm' was built under R version 4.4.3\n\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(101)\n\n# Generate synthetic panel data\nn &lt;- 1000\ntime_periods &lt;- 5\nintervention_time &lt;- 3\n\npanel_data &lt;- data.frame(\n  id = rep(1:n, each = time_periods),\n  time = rep(1:time_periods, times = n),\n  treatment = rep(sample(c(0, 1), n, replace = TRUE), each = time_periods)\n)\n\n# Generate outcomes\npanel_data$outcome &lt;- with(panel_data,\n                           10 + 2 * time + 5 * treatment + \n                           3 * (time &gt;= intervention_time & treatment == 1) + \n                           rnorm(n * time_periods, 0, 2))\n\n# Create post-treatment indicator\npanel_data$post &lt;- as.integer(panel_data$time &gt;= intervention_time)\n\n# Estimate DiD model\ndid_model &lt;- plm(outcome ~ treatment * post, \n                 data = panel_data, \n                 index = c(\"id\", \"time\"), \n                 model = \"within\")\n\n# Summarize results\nsummary_did &lt;- summary(did_model)\n\n# Calculate group means for each time period\ngroup_means &lt;- aggregate(outcome ~ time + treatment, data = panel_data, FUN = mean)\n\n# Visualize DiD\nggplot(group_means, aes(x = time, y = outcome, color = factor(treatment), group = treatment)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = intervention_time, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Difference-in-Differences Analysis\",\n       subtitle = paste(\"Estimated treatment effect:\", round(coef(did_model)[\"treatment:post\"], 3)),\n       x = \"Time\", y = \"Outcome\", color = \"Treatment Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Control\", \"Treatment\")) +\n  scale_x_continuous(breaks = 1:time_periods)\n\n\n\n\n\n\n\n# Print model summary\nprint(summary_did)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = outcome ~ treatment * post, data = panel_data, \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 1000, T = 5, N = 5000\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-7.509908 -1.625814  0.001753  1.610009  8.047479 \n\nCoefficients:\n               Estimate Std. Error t-value  Pr(&gt;|t|)    \npost            5.05692    0.10315  49.026 &lt; 2.2e-16 ***\ntreatment:post  2.89003    0.14935  19.351 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    78894\nResidual Sum of Squares: 26696\nR-Squared:      0.66163\nAdj. R-Squared: 0.57691\nF-statistic: 3908.68 on 2 and 3998 DF, p-value: &lt; 2.22e-16\n\n\nWykres pokazuje ≈õrednie wyniki dla grup interwencji i kontrolnej w czasie. Pionowa przerywana linia wskazuje punkt interwencji. Oszacowanie DiD to r√≥≈ºnica miƒôdzy zmianami obu grup od okresu przed do po interwencji.\nDiD Model:\n\nThe model outcome ~ treatment * post estimates:\nThe average treatment effect on the treated (ATT) after the intervention\nThe coefficient on treatment:post represents this effect\n\nInterpretation of Results: Looking at the model summary:\n\nThe coefficient for treatment:post is the DiD estimator. It represents the average treatment effect on the treated after the intervention.\nIf this coefficient is statistically significant, it suggests that the treatment had a causal effect on the outcome.\nThe magnitude of this coefficient tells us the size of the treatment effect.\n\nVisualization: The plot shows:\n\nSeparate trend lines for the treatment and control groups\nA vertical dashed line indicating the intervention time\nThe parallel trends assumption can be visually assessed by looking at the pre-intervention period\nThe divergence of the lines after the intervention represents the treatment effect\n\nAssumptions and Limitations:\nIt‚Äôs important to note some key assumptions of DiD:\n\nParallel trends: In the absence of treatment, the difference between the treatment and control groups would remain constant over time.\nNo spillover effects: The treatment does not affect the control group.\nNo compositional changes: The composition of treatment and control groups remains stable over time.\n\n\n\nRegresja NieciƒÖg≈Ça (RDD)\nRDD jest stosowana, gdy przydzia≈Ç do interwencji jest okre≈õlony przez warto≈õƒá granicznƒÖ na ciƒÖg≈Çej zmiennej. Por√≥wnuje obserwacje tu≈º powy≈ºej i poni≈ºej punktu granicznego, aby oszacowaƒá efekt interwencji.\nPrzeprowad≈∫my analizƒô RDD przy u≈ºyciu pakietu rdrobust:\n\nlibrary(rdrobust)\n\nWarning: package 'rdrobust' was built under R version 4.4.3\n\n# Generowanie syntetycznych danych RDD\nset.seed(202)\nn &lt;- 1000\nx &lt;- runif(n, -1, 1)\ny &lt;- 3 + 2 * x + 4 * (x &gt;= 0) + rnorm(n, 0, 1)\n\nrdd_data &lt;- data.frame(x, y)\n\n# Analiza RDD\nrdd_result &lt;- rdrobust(y, x, c = 0)\nsummary(rdd_result)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  499          501\nEff. Number of Obs.             182          175\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.362        0.362\nBW bias (b)                   0.575        0.575\nrho (h/b)                     0.630        0.630\nUnique Obs.                     499          501\n\n=====================================================================\n                   Point    Robust Inference\n                Estimate         z     P&gt;|z|      [ 95% C.I. ]       \n---------------------------------------------------------------------\n     RD Effect     4.092    15.013     0.000     [3.600 , 4.680]     \n=====================================================================\n\n# Wizualizacja RDD\nggplot(rdd_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(data = subset(rdd_data, x &lt; 0), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(data = subset(rdd_data, x &gt;= 0), method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Regresja NieciƒÖg≈Ça\",\n       x = \"Zmienna Bie≈ºƒÖca\", y = \"Wynik\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAnaliza Regresji NieciƒÖg≈Çej\n\n\n\n\nWykres pokazuje nieciƒÖg≈Ço≈õƒá w punkcie granicznym (x = 0), z oddzielnymi liniami regresji dopasowanymi po obu stronach. Efekt interwencji jest szacowany przez r√≥≈ºnicƒô miƒôdzy tymi liniami w punkcie granicznym.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Projekty Badawcze: Podej≈õcia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#model-potencjalnych-wynik√≥w-neymana-rubina",
    "href": "rozdzial4.html#model-potencjalnych-wynik√≥w-neymana-rubina",
    "title": "10¬† Projekty Badawcze: Podej≈õcia Eksperymentalne i Nieeksperymentalne",
    "section": "10.5 Model Potencjalnych Wynik√≥w Neymana-Rubina",
    "text": "10.5 Model Potencjalnych Wynik√≥w Neymana-Rubina\nModel potencjalnych wynik√≥w Neymana-Rubina zapewnia formalne podej≈õcie do wnioskowania przyczynowego. Wprowadza on koncepcjƒô potencjalnych wynik√≥w: dla ka≈ºdej jednostki rozwa≈ºamy wynik w warunkach interwencji i w warunkach kontrolnych, mimo ≈ºe w rzeczywisto≈õci mo≈ºemy zaobserwowaƒá tylko jeden z nich.\nKluczowe pojƒôcia:\n\nPotencjalne Wyniki: Y_i(1) i Y_i(0) odpowiednio dla interwencji i kontroli.\nObserwowany Wynik: Y_i = Y_i(1)T_i + Y_i(0)(1-T_i), gdzie T_i to wska≈∫nik interwencji.\nIndywidualny Efekt Interwencji: \\tau_i = Y_i(1) - Y_i(0)\nPrzeciƒôtny Efekt Interwencji (ATE): E[\\tau_i] = E[Y_i(1) - Y_i(0)]\n\nModel podkre≈õla ‚Äúfundamentalny problem wnioskowania przyczynowego‚Äù: nigdy nie mo≈ºemy zaobserwowaƒá obu potencjalnych wynik√≥w dla pojedynczej jednostki jednocze≈õnie.\n\nPrzyk≈Çad: Szacowanie ATE w RCT\nW RCT, losowy przydzia≈Ç zapewnia, ≈ºe interwencja jest niezale≈ºna od potencjalnych wynik√≥w, umo≈ºliwiajƒÖc nieobciƒÖ≈ºone oszacowanie ATE:\n\n\\hat{ATE} = \\frac{1}{n_1} \\sum_{i:T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i:T_i=0} Y_i\n\nGdzie n_1 i n_0 to odpowiednio liczby jednostek w grupie interwencji i kontrolnej.\n\n# U≈ºywajƒÖc danych RCT z wcze≈õniejszego przyk≈Çadu\nate_estimate &lt;- mean(data$post_test[data$grupa == \"Eksperymentalna\"]) - \n                mean(data$post_test[data$grupa == \"Kontrolna\"])\n\ncat(\"Oszacowany Przeciƒôtny Efekt Interwencji:\", round(ate_estimate, 2))\n\nOszacowany Przeciƒôtny Efekt Interwencji: 9.66",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Projekty Badawcze: Podej≈õcia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "11.1 Introduction to Sigma Notation (Œ£)\nDescriptive statistics are fundamental tools in social science research, providing a concise summary of data characteristics. They serve several crucial functions:",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction-to-sigma-notation-œÉ",
    "href": "chapter5.html#introduction-to-sigma-notation-œÉ",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "What is Sigma summation notation? Sigma (Œ£) is a mathematical operator that instructs us to sum (add) a sequence of terms - it functions as a directive to perform addition of all elements within a specified range.\nPurpose: Provides a concise way to write sums of many similar terms using a single symbol, avoiding lengthy addition expressions.\n\n\nBasic Formula\n\nThe general form of sigma notation is: \\sum_{i=a}^{b} f(i)\nSummation index: i\nLower bound: a\nUpper bound: b\nFunction: f(i)\n\n\n\nExamples of Sigma Notation Applications\n\nSimple Example: Sum of Natural Numbers\n\nSuppose you want to add the first five positive integers: \\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15\nThe above notation adds the first five positive integers.\n\n\n\nSum of Squares\n\nSuppose you want to sum the squares of the first four positive integers: \\sum_{i=1}^{4} i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 1 + 4 + 9 + 16 = 30\nThis is the sum of squares of the first four positive integers.\n\n\n\nSum of a Constant Value\n\nSumming a constant value c for n terms: \\sum_{i=1}^{n} c = c + c + c + ... + c \\text{ (n times)} = n \\cdot c\nExample: Sum of five fives: \\sum_{i=1}^{5} 5 = 5 + 5 + 5 + 5 + 5 = 5 \\cdot 5 = 25\n\n\n\n\nSimple Examples in Statistical Context\n\\sum_{i=1}^{n} x_i - Summation index: i (typically denotes a specific observation in a dataset) - Lower bound: 1 (we usually start from the first observation) - Upper bound: n (total number of observations in our dataset) - Expression: x_i (value of the ith observation)\n\nSumming Observation Values\n\nWe have a dataset: 5, 8, 12, 15, 20\nSum of all values: \\sum_{i=1}^{5} x_i = x_1 + x_2 + x_3 + x_4 + x_5 = 5 + 8 + 12 + 15 + 20 = 60\nThis sum is a key element when calculating the arithmetic mean.\n\n\n\nSum of Deviations from the Mean\n\nFor the same dataset (5, 8, 12, 15, 20), the mean is \\bar{x} = 60/5 = 12\nSum of deviations from the mean: \\sum_{i=1}^{5} (x_i - \\bar{x}) = (5-12) + (8-12) + (12-12) + (15-12) + (20-12) = -7 + (-4) + 0 + 3 + 8 = 0\nImportant observation: The sum of deviations from the mean always equals 0, which is a fundamental property of the arithmetic mean.\n\n\n\n\nSummary\n\nSigma Notation (Œ£) allows for concise expression of key statistical formulas\nThe most important applications include calculating:\n\nArithmetic mean\nVariance and standard deviation\nVarious sums of squares used in regression analysis\n\n\n\n\n\n\n\n\nSummation (Œ£) and Product (Œ†) Operators\n\n\n\n\nSigma (Œ£) Operator\n\\sum is a summation operator that instructs us to add terms:\n\\sum_{i=1}^{n} x_i = x_1 + x_2 + ... + x_n\nwhere: - i is the index variable - The lower value under Œ£ (here i=1) is the starting point - The upper value (here n) is the ending point\n\n\nPi (Œ†) Operator\n\\prod is a product operator that instructs us to multiply terms:\n\\prod_{i=1}^{n} x_i = x_1 \\times x_2 \\times ... \\times x_n\nwhere: - i is the index variable - The lower value under Œ† (here i=1) is the starting point - The upper value (here n) is the ending point\n\n\n\n\n\n\n\n\n\nExample of Œ£\n\n\n\n\\sum_{i=1}^{4} i = 1 + 2 + 3 + 4 = 10\n\n\n\n\n\n\n\n\nExample of Œ†\n\n\n\n\\prod_{i=1}^{4} i = 1 \\times 2 \\times 3 \\times 4 = 24\n\n\n\n\n\n\n\n\nKey Differences\n\n\n\n\nŒ£ represents repeated addition\nŒ† represents repeated multiplication",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#types-of-data-distributions",
    "href": "chapter5.html#types-of-data-distributions",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.2 Types of Data Distributions",
    "text": "11.2 Types of Data Distributions\n\n\n\n\n\n\nImportant\n\n\n\nData distribution informs what values a variable takes and how often.\n\n\nUnderstanding data distributions is crucial for data analysis and visualization. In this document, we‚Äôll explore various types of distributions and how to visualize them using ggplot2 in R.\n\nNormal Distribution\nThe normal distribution, also known as the Gaussian distribution, is symmetric and bell-shaped.\n\n# Generate normal distribution data\nnormal_data &lt;- data.frame(x = rnorm(1000))\n\n# Plot\nggplot(normal_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Normal Distribution\", x = \"Value\", y = \"Density\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\nUniform Distribution\nIn a uniform distribution, all values have an equal probability of occurrence.\n\n# Generate uniform distribution data\nuniform_data &lt;- data.frame(x = runif(1000))\n\n# Plot\nggplot(uniform_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Uniform Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\nSkewed Distributions\nSkewed distributions are asymmetric, with one tail longer than the other.\n\n# Generate right-skewed data\nright_skewed &lt;- data.frame(x = rlnorm(1000))\n\n# Plot\nggplot(right_skewed, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightyellow\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Right-Skewed Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\nBimodal Distribution\nA bimodal distribution has two peaks, indicating two distinct subgroups in the data.\n\n# Generate bimodal data\nbimodal_data &lt;- data.frame(x = c(rnorm(500, mean = -2), rnorm(500, mean = 2)))\n\n# Plot\nggplot(bimodal_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightpink\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Bimodal Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nKey Properties\nExamples\n\n\n\n\nSymmetric (Normal)\nSymmetric, bell-shaped, most values close to the mean\nAdult height in population, IQ test scores, measurement errors, standardized exam results\n\n\nUniform\nEqual probability across the entire range\nLast digit of phone numbers, random day of the week selection, position of pointer after spinning a wheel of fortune\n\n\nBimodal\nTwo distinct peaks, suggests presence of subgroups\nAge structure in university towns (students and permanent residents), opinions on strongly polarizing topics, traffic intensity hours (morning and afternoon peak)\n\n\nRight-skewed (Positively skewed)\nExtended ‚Äútail‚Äù on the right side, most values less than the mean\nQueue waiting time, commute time to work, age at first marriage\n\n\nHeavy-tailed skewed (Log-normal)\nStrong right asymmetry, values cannot be negative, long ‚Äúfat tail‚Äù\nPersonal income, housing prices, household size\n\n\nExtreme-tailed skewed (Power law)\nExtreme asymmetry, ‚Äúrich get richer‚Äù effect, no characteristic scale\nWealth of the richest individuals, city populations, number of followers on social media, number of citations of scientific publications",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#visualizing-real-world-data-distributions",
    "href": "chapter5.html#visualizing-real-world-data-distributions",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.3 Visualizing Real-World Data Distributions",
    "text": "11.3 Visualizing Real-World Data Distributions\nLet‚Äôs use the palmerpenguins dataset to explore data distributions.\n\nHistogram and Density Plot\n\n\n\n\n\n\nUnderstanding Histograms and Density\n\n\n\n‚≠ê A histogram is a special graph for numerical data where:\n\nData is grouped into ranges (called ‚Äúbins‚Äù)\nBars touch each other (unlike bar charts!) because the data is continuous\nEach bar‚Äôs height shows how many values fall into that range\n\nThink of density as showing how common or concentrated certain values are in your data:\n\nA higher point on a density curve (or taller bar in a histogram) means those values appear more frequently in your data\nA lower point means those values are less common\n\nJust like a crowded area has more people per space (higher density), a taller part of the graph shows values that appear more often in your dataset!\n\n\n\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Distribution of Penguin Flipper Lengths\", \n       x = \"Flipper Length (mm)\", \n       y = \"Density\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\nBox Plot\nBox plots are useful for comparing distributions across categories.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Distribution of Penguin Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nViolin Plot\nViolin plots combine box plot and density plot features.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Distribution of Penguin Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nRidgeline Plot\nRidgeline plots are useful for comparing multiple distributions.\n\nlibrary(ggridges)\n\nggplot(penguins, aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.6) +\n  labs(title = \"Distribution of Flipper Length by Penguin Species\",\n       x = \"Flipper Length (mm)\",\n       y = \"Species\")\n\nPicking joint bandwidth of 2.38\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nUnderstanding and visualizing data distributions is crucial in data analysis. ggplot2 provides a flexible and powerful toolkit for creating various types of distribution plots. By exploring different visualization techniques, we can gain insights into the underlying patterns and characteristics of our data.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-outliers",
    "href": "chapter5.html#understanding-outliers",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.4 Understanding Outliers",
    "text": "11.4 Understanding Outliers\nBefore diving into specific measures, it‚Äôs crucial to understand the concept of outliers, as they can significantly impact many descriptive statistics.\nOutliers are data points that differ significantly from other observations in the dataset. They can occur due to:\n\nMeasurement or recording errors\nGenuine extreme values in the population\n\nOutliers can have a substantial effect on many statistical measures, especially those based on means or sums of squared deviations. Therefore, it‚Äôs essential to:\n\nIdentify outliers through both statistical methods and domain knowledge\nInvestigate the cause of outliers\nMake informed decisions about whether to include or exclude them in analyses\n\nThroughout this guide, we‚Äôll discuss how different descriptive measures are affected by outliers.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#statistical-symbols-and-notations---summary",
    "href": "chapter5.html#statistical-symbols-and-notations---summary",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.5 Statistical Symbols and Notations - Summary",
    "text": "11.5 Statistical Symbols and Notations - Summary\n\n\n\n\n\n\n\n\n\n\nMeasure\nPopulation Parameter\nSample Statistic\nAlternative Notations\nUsage Notes\n\n\n\n\nSize\nN\nn\n-\nTotal count of observations\n\n\nMean\n\\mu\n\\bar{x}, m\nM, E(X)\nE(X) used in probability theory\n\n\nVariance\n\\sigma^2\ns^2\n\\text{Var}(X), V(X)\nSquared deviations from mean\n\n\nStandard Deviation\n\\sigma\ns\n\\text{SD}, \\text{std}\nSquare root of variance\n\n\nProportion\n\\pi, P\n\\hat{p}\n\\text{prop}\nRelative frequencies\n\n\nCorrelation\n\\rho\nr\n\\text{corr}(x,y)\nRanges from -1 to +1\n\n\nStandard Error\n\\sigma_{\\bar{x}}\ns_{\\bar{x}}\n\\text{SE}\nStandard error of mean\n\n\nSum\n\\sum\n\\sum\n\\sum_{i=1}^n\nWith indexing\n\n\nIndividual Value\nX_i\nx_i\n-\nith observation\n\n\nCovariance\n\\sigma_{xy}\ns_{xy}\n\\text{Cov}(X,Y)\nJoint variation\n\n\nMedian\n\\eta\n\\text{Med}\nM\nCentral value\n\n\nRange\nR\nr\n\\text{max}(X) - \\text{min}(X)\nSpread measure\n\n\nMode\n\\text{Mo}\n\\text{mo}\n\\text{mod}\nMost frequent value\n\n\nSkewness\n\\gamma_1\ng_1\n\\text{SK}\nDistribution asymmetry\n\n\nKurtosis\n\\gamma_2\ng_2\n\\text{KU}\nDistribution peakedness\n\n\n\nAdditional useful notations:\n\nSample moments: m_k = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^k\nPopulation moments: \\mu_k = E[(X - \\mu)^k]",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-central-tendency",
    "href": "chapter5.html#measures-of-central-tendency",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.6 Measures of Central Tendency",
    "text": "11.6 Measures of Central Tendency\nMeasures of central tendency aim to identify the ‚Äútypical‚Äù or ‚Äúcentral‚Äù value in a dataset. The three primary measures are mean, median, and mode.\n\nArithmetic Mean\nThe arithmetic mean is the sum of all values divided by the number of values.\nFormula: \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\nImportant Property: The mean is a balancing point in the data. The sum of deviations from the mean is always zero:\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0\nThis property makes the mean useful in many statistical calculations.\n\n\n\n\n\n\nUnderstanding Mean as a Balance Point üéØ\n\n\n\nLet‚Äôs consider a dataset X = \\{1, 2, 6, 7, 9\\} on a number line, imagining it as a seesaw:\n\n\n\nhttps://www.gastonsanchez.com/matrix4sl/mean-as-a-balancing-point.html\n\n\nThe mean (\\mu) acts as the perfect balance point of this seesaw. For our data:\n\\mu = \\frac{1 + 2 + 6 + 7 + 9}{5} = 5\n\nWhat happens at different support points? ü§î\n\nSupport point at 6 (too high):\n\nLeft side: Values (1, 2) are below\nRight side: Values (7, 9) are above\n\\sum distances from left = (6-1) + (6-2) = 9\n\\sum distances from right = (7-6) + (9-6) = 4\nThe seesaw tilts left! ‚¨ÖÔ∏è because 9 &gt; 4\n\nSupport point at 4 (too low):\n\nLeft side: Values (1, 2) are below\nRight side: Values (6, 7, 9) are above\n\\sum distances from left = (4-1) + (4-2) = 5\n\\sum distances from right = (6-4) + (7-4) + (9-4) = 10\nThe seesaw tilts right! ‚û°Ô∏è because 5 &lt; 10\n\nSupport point at mean (5) (perfect balance):\n\n\\sum distances below = \\sum distances above\n((5-1) + (5-2)) = ((6-5) + (7-5) + (9-5))\n7 = 7 ‚ú® Perfect balance!\n\n\nThis shows why the mean is the unique balance point, where:\n\\sum_{i=1}^n (x_i - \\mu) = 0\nThe seesaw will always tilt unless the support point is placed exactly at the mean! üé™\n\n\n\n\n\n\n\n\n\nMean as a Balance Point\n\n\n\nThis visualization shows how the arithmetic mean (5) acts as a balance point between clustered points on the left and dispersed points on the right:\nLeft side of the mean: - Points with values 2 and 3 - Close together (difference of 1 unit) - Distances from mean: 3 and 2 units - Sum of ‚Äúpull‚Äù = 5 units\nRight side of the mean: - Points with values 6 and 9 - More spread out (difference of 3 units) - Distances from mean: 1 and 4 units - Sum of ‚Äúpull‚Äù = 5 units\nKey observations:\n\nThe mean (5) is a balance point, even though:\n\nPoints on the left are clustered (2,3)\nPoints on the right are dispersed (6,9)\nGreen arrows show distances from the mean\n\nBalance is maintained because:\n\nSum of distances balances out: (5-2) + (5-3) = (6-5) + (9-5)\nTotal sum of distances = 5 units on each side\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual Calculation Example:\nLet‚Äôs calculate the mean for the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nSum all values\n2 + 4 + 4 + 5 + 5 + 7 + 9 = 36\n\n\n2\nCount the number of values\nn = 7\n\n\n3\nDivide the sum by n\n36 / 7 = 5.14\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(data)\n\n[1] 5.142857\n\n\nPros:\n\nEasy to calculate and understand\nUses all data points\nUseful for further statistical calculations\n\nCons:\n\nSensitive to outliers\nNot ideal for skewed distributions\n\nExample with outlier:\n\ndata_with_outlier &lt;- c(2, 4, 4, 5, 5, 7, 100)\nmean(data_with_outlier)\n\n[1] 18.14286\n\n\nAs we can see, the outlier (100) drastically affects the mean.\n\n\nMedian\nThe median is the middle value when the data is ordered.\nManual Calculation Example:\nUsing the same dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nResult\n\n\n\n\n1\nOrder the data\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nFind the middle value\n5\n\n\n\nFor even number of values, take the average of the two middle values.\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmedian(data)\n\n[1] 5\n\nmedian(data_with_outlier)\n\n[1] 5\n\n\nPros:\n\nNot affected by extreme outliers\nBetter for skewed distributions\n\nCons:\n\nDoesn‚Äôt use all data points\nLess useful for further statistical calculations\n\n\n\n\n\n\n\nWarning\n\n\n\nTo find the position of the median in a dataset:\n\nFirst sort the data in ascending order\nIf n is odd:\n\nMedian position = \\frac{n + 1}{2}\n\nIf n is even:\n\nFirst median position = \\frac{n}{2}\nSecond median position = \\frac{n}{2} + 1\nMedian = \\frac{\\text{value at }\\frac{n}{2} + \\text{value at }(\\frac{n}{2}+1)}{2}\n\n\nFor example:\n\nOdd n=7: position = \\frac{7+1}{2} = 4th value\nEven n=8: positions = \\frac{8}{2} = 4th and 4+1 = 5th value\n\n\n\n\n\nMode\nThe mode is the most frequently occurring value.\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nValue\nFrequency\n\n\n\n\n2\n1\n\n\n4\n2\n\n\n5\n2\n\n\n7\n1\n\n\n9\n1\n\n\n\nThe mode is 4 and 5 (bimodal).\nR calculation:\n\nlibrary(modeest)\nmfv(data)  # Most frequent value\n\n[1] 4 5\n\n\nPros:\n\nOnly measure of central tendency for nominal data\nCan identify multiple peaks in the data\n\nCons:\n\nNot always uniquely defined\nNot useful for continuous data\n\n\n\nWeighted (arithmetic) Mean (*)\nThe weighted mean is used when some data points are more important than others. There are two types of weighted means: with not normalized weights and with normalized weights.\n\nWeighted Mean with Not Normalized Weights\nThis is the standard form of the weighted mean, where weights can be any positive numbers representing the importance of each data point.\nFormula: \\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\nManual Calculation Example:\nLet‚Äôs calculate the weighted mean for the dataset: 2, 4, 5, 7 with weights 1, 2, 3, 1\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nMultiply each value by its weight\n(2 * 1) + (4 * 2) + (5 * 3) + (7 * 1) = 2 + 8 + 15 + 7 = 32\n\n\n2\nSum the weights\n1 + 2 + 3 + 1 = 7\n\n\n3\nDivide the result from step 1 by the result from step 2\n32 / 7 = 4.57\n\n\n\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\nWeighted Mean with Normalized Weights (Fractions)\nIn this case, the weights are fractions that sum to 1, representing the proportion of importance for each data point.\nFormula: \\bar{x}_w = \\sum_{i=1}^n w_i x_i, where \\sum_{i=1}^n w_i = 1\nManual Calculation Example:\nLet‚Äôs calculate the weighted mean for the dataset: 2, 4, 5, 7 with normalized weights 0.1, 0.3, 0.4, 0.2\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nMultiply each value by its weight\n(2 * 0.1) + (4 * 0.3) + (5 * 0.4) + (7 * 0.2)\n\n\n2\nSum the results\n0.2 + 1.2 + 2.0 + 1.4 = 4.8\n\n\n\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Note: these sum to 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nPros of Weighted Means:\n\nAccount for varying importance of data points\nUseful in survey analysis with different sample sizes or importance levels\nCan adjust for unequal probabilities in sampling designs\n\nCons of Weighted Means:\n\nRequire justification for weights\nCan be misused to manipulate results\nMay be less intuitive to interpret than simple arithmetic mean",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-variability",
    "href": "chapter5.html#measures-of-variability",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.7 Measures of Variability",
    "text": "11.7 Measures of Variability\nThese measures describe how spread out the data is. They are crucial for understanding the dispersion of data points around the central tendency.\n\n\n\n\n\n\nUnderstanding Variance\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†11.1: Three dot plots showing increasing variance with constant mean\n\n\n\n\n\nThe three dot plots above demonstrate how variance measures the spread of data around a central value:\n\nAll distributions have the same mean (Œº = 10), shown by the dashed line\nLow Variance (œÉ¬≤ = 1): Points cluster tightly around the mean\nMedium Variance (œÉ¬≤ = 4): Points show moderate spread\nHigh Variance (œÉ¬≤ = 9): Points spread widely around the mean\n\n\n\n\n\n\n\n\n\nUnderstanding Different Levels of Variability\n\n\n\n\n\n\n\n\n\n\n\n\nThis visualization shows three normal distributions with the same mean (Œº = 10) but different levels of variability:\n\nLow Variability (œÉ = 0.5)\n\nData points cluster tightly around the mean\nThe density curve is tall and narrow\nMost observations fall within ¬±0.5 units of the mean\n\nMedium Variability (œÉ = 2.0)\n\nData points spread out more from the mean\nThe density curve is lower and wider\nMost observations fall within ¬±2 units of the mean\n\nHigh Variability (œÉ = 4.0)\n\nData points spread widely from the mean\nThe density curve is much flatter and wider\nMost observations fall within ¬±4 units of the mean\n\n\n\n\n\nRange\nThe range is the difference between the maximum and minimum values.\nFormula: R = x_{max} - x_{min}\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nFind the maximum value\n9\n\n\n2\nFind the minimum value\n2\n\n\n3\nSubtract minimum from maximum\n9 - 2 = 7\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nrange(data)\n\n[1] 2 9\n\nmax(data) - min(data)\n\n[1] 7\n\n\nPros:\n\nSimple to calculate and understand\nGives an immediate sense of data spread\n\nCons:\n\nExtremely sensitive to outliers\nDoesn‚Äôt provide information about the distribution between extremes\n\n\n\nInterquartile Range (IQR)\nThe IQR is the difference between the 75th and 25th percentiles.\nFormula: IQR = Q_3 - Q_1\nTo find quartiles manually:\n\nFor odd number of values:\n\nQ2 (median) is the middle value\nQ1 is the median of the lower half (excluding the median of all observations)\nQ3 is the median of the upper half (excluding the median of all observations)\n\nFor even number of values:\n\nQ2 is the average of the two middle values\nQ1 is the median of the lower half (excluding the median of all observations)\nQ3 is the median of the upper half (excluding the median of all observations)\n\n\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nOrder the data\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nFind Q2 (median)\n5\n\n\n3\nFind Q1 (median of lower half)\n4\n\n\n4\nFind Q3 (median of upper half)\n7\n\n\n5\nCalculate IQR\nQ3 - Q1 = 7 - 4 = 3\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nprint(data)\n\n[1] 2 4 4 5 5 7 9\n\nquantile(data, type = 1)\n\n  0%  25%  50%  75% 100% \n   2    4    5    7    9 \n\nIQR(data, type = 1)\n\n[1] 3\n\n\nPros:\n\nRobust to outliers\nProvides information about the spread of the middle 50% of the data\n\nCons:\n\nIgnores the tails of the distribution\nLess efficient than standard deviation for normal distributions\n\n\n\nVariance\nVariance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n\n\n\n\n\n\nVariance: Understanding Average Squared Deviations\n\n\n\nWhat is Variance? Variance measures how ‚Äúspread out‚Äù numbers are from their mean - it‚Äôs the average of squared deviations from the mean.\nFormula: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nSimple Example: Consider numbers: 2, 4, 6, 8, 10 Mean (\\bar{x}) = 6\n\n\n\n\n\n\n\n\n\nCalculating Deviations:\n\n\n\n\n\n\n\n\n\n\n\n\nValue\nDeviation from mean\nSquare of deviation\n\n\n\n\n2\n-4\n16\n\n\n4\n-2\n4\n\n\n6\n0\n0\n\n\n8\n+2\n4\n\n\n10\n+4\n16\n\n\n\nVariance = \\frac{16 + 4 + 0 + 4 + 16}{4} = 10\nKey Points:\n\nMean acts as a reference line (blue dashed line)\nDeviations show distance from mean (red dotted lines)\nSquaring makes all deviations positive (blue bars)\nLarger deviations contribute more to variance\n\n\n\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the mean\n\\bar{x} = 5.14\n\n\n2\nSubtract the mean from each value and square the result\n(2 - 5.14)^2 = 9.86\n\n\n\n\n(4 - 5.14)^2 = 1.30\n\n\n\n\n(4 - 5.14)^2 = 1.30\n\n\n\n\n(5 - 5.14)^2 = 0.02\n\n\n\n\n(5 - 5.14)^2 = 0.02\n\n\n\n\n(7 - 5.14)^2 = 3.46\n\n\n\n\n(9 - 5.14)^2 = 14.90\n\n\n3\nSum the squared differences\n30.86\n\n\n4\nDivide by (n-1), i.e.¬†by the number of observations - 1\n30.86 / 6 = 5.14\n\n\n\nR calculation:\n\nvar(data)\n\n[1] 5.142857\n\n\nPros:\n\nUses all data points\nFoundation for many statistical tests\n\nCons:\n\nUnits are squared, making interpretation less intuitive\nSensitive to outliers\n\n\n\n\n\n\n\nBessel‚Äôs Correction: Why We Divide by (n-1) And Not by n\n\n\n\nThe Key Insight:\nWhen we calculate deviations from the mean, they must sum to zero. This is a mathematical fact: \\sum(x_i - \\bar{x}) = 0\nThink of it Like This:\nIf you have 5 numbers and their mean:\n\nOnce you calculate 4 deviations from the mean\nThe 5th deviation MUST be whatever makes the sum zero\nYou don‚Äôt really have 5 independent deviations\nYou only have 4 truly ‚Äúfree‚Äù deviations\n\nSimple Example:\nNumbers: 2, 4, 6, 8, 10\n\nMean = 6\nDeviations: -4, -2, 0, +2, +4\nNotice they sum to zero\nIf you know any 4 deviations, the 5th is predetermined!\n\nThis is Why:\n\nWhen calculating variance: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nWe divide by (n-1) not n\nBecause only (n-1) deviations are truly independent\nThe last one is determined by the others\n\nDegrees of Freedom:\n\nn = number of observations\n1 = constraint (deviations must sum to zero)\nn-1 = degrees of freedom = number of truly independent deviations\n\nWhen to Use It:\n\nWhen calculating sample variance\nWhen calculating sample standard deviation\n\nWhen NOT to Use It:\n\nPopulation calculations (when you have all data)\n\nRemember:\n\nIt‚Äôs not just a statistical trick\nDeviations from the mean must sum to zero\nThis constraint costs us one degree of freedom\n\n\n\n\n\nStandard Deviation\nThe standard deviation is the square root of the variance and measures the average dispersion of the data about their arithmetic mean. In contrast to the variance, it has the advantage of being expressed in the same units as the original measurements, making its interpretation more intuitive.\nFormula: s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the variance\ns^2 = 5.14 (from previous calculation)\n\n\n2\nTake the square root\ns = \\sqrt{5.14} = 2.27\n\n\n\nR calculation:\n\nsd(data)\n\n[1] 2.267787\n\n\nPros:\n\nIn same units as original data\nWidely used and understood\n\nCons:\n\nStill sensitive to outliers\nAssumes data is roughly ‚Äúnormally‚Äù distributed\n\n\n\nCoefficient of Variation (*)\nThe coefficient of variation is the standard deviation divided by the mean, often expressed as a percentage.\nFormula: CV = \\frac{s}{\\bar{x}} \\times 100\\%\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the mean\n\\bar{x} = 5.14\n\n\n2\nCalculate the standard deviation\ns = 2.27\n\n\n3\nDivide s by the mean and multiply by 100\n(2.27 / 5.14) * 100 = 44.16\\%\n\n\n\nR calculation:\n\n(sd(data) / mean(data)) * 100\n\n[1] 44.09586\n\n\nPros:\n\nAllows comparison of variability between datasets with different units or means\nUseful in fields like finance for risk assessment\n\nCons:\n\nNot meaningful for data with both positive and negative values\nCan be misleading when mean is close to zero\n\n\n\n\n\n\n\nLimitations of Coefficient of Variation (CV)\n\n\n\nThe coefficient of variation, calculated as (œÉ/Œº) √ó 100\\%, has two important limitations:\n\nNot meaningful for data with both positive and negative values\n\nThe mean could be close to zero due to positive and negative values cancelling out\nExample: Dataset {-5, -3, 2, 6} has mean = 0\n\nCV = (std dev / 0) √ó 100%\nThis leads to division by zero\nEven if mean isn‚Äôt exactly zero, the CV doesn‚Äôt represent true relative variability when data cross zero\n\nThe CV assumes a natural zero point and meaningful ratios between values\n\n\n\nMisleading when mean is close to zero\n\nSince CV = (œÉ/Œº) √ó 100\\%, as Œº approaches zero:\n\nThe denominator becomes very small\nResults in extremely large CV values\nThese large values don‚Äôt meaningfully represent relative variability\n\nExample:\n\nDataset A: {0.001, 0.002, 0.003} has mean = 0.002\nEven small standard deviations will produce very large CVs\nThe resulting large CV might suggest extreme variability when the data are actually quite close together\n\n\n\n\nBest Use Cases\nCV is most useful for:\n\nStrictly positive data\nData measured on a ratio scale\nData with means well above zero\nComparing variability between datasets with different units or scales",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-relative-position-standing",
    "href": "chapter5.html#measures-of-relative-position-standing",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.8 Measures of Relative Position (Standing)",
    "text": "11.8 Measures of Relative Position (Standing)\nUnderstanding where values sit within a dataset is crucial for data analysis. Let‚Äôs explore these concepts step by step.\n\nQuartiles (Q): The Basics\nThink of quartiles as special numbers that split your ordered data into four equal parts.\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\n\nWhat Are Quartiles?\nFirst Quartile (Q1):\n\nSeparates the lowest 25% of data from the rest\nAlso called the 25th percentile\nExample: If Q1 = 50 in a test score dataset, 25% of students scored below 50\n\nSecond Quartile (Q2):\n\nThe median - splits data in half\nAlso called the 50th percentile\nExample: If Q2 = 70, half the students scored below 70\n\nThird Quartile (Q3):\n\nSeparates the highest 25% of data from the rest\nAlso called the 75th percentile\nExample: If Q3 = 85, 75% of students scored below 85\n\n\n\nHow to Calculate Quartiles (Step by Step) - Two Methods\nLet‚Äôs examine student test scores using both common quartile calculation methods:\nExample 1: Odd Number Case (11 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 88, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 11 values (odd)\nMedian position = (n + 1)/2 = 6\nQ2 = 78\n\nStep 2: Find Q1\n\nTukey‚Äôs Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (n + 1)/4 = (11 + 1)/4 = 3\nQ1 = 70 (3rd value)\n\n\nStep 3: Find Q3\n\nTukey‚Äôs Method:\n\nLook at upper half: 80, 82, 85, 88, 90\nQ3 = median of upper half = 85\n\nInterpolation Method:\n\nPosition = 3(n + 1)/4 = 3(12)/4 = 9\nQ3 = 85 (9th value)\n\n\nExample 2: Even Number Case (10 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 10 values (even)\nMedian positions = 5 and 6\nQ2 = (75 + 78)/2 = 76.5\n\nStep 2: Find Q1\n\nTukey‚Äôs Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (10 + 1)/4 = 2.75\nQ1 = 65 + 0.75(70 - 65) = 68.75\n\n\nStep 3: Find Q3\n\nTukey‚Äôs Method:\n\nLook at upper half: 78, 80, 82, 85, 90\nQ3 = median of upper half = 82\n\nInterpolation Method:\n\nPosition = 3(10 + 1)/4 = 8.25\nQ3 = 82 + 0.25(85 - 82) = 82.75\n\n\nImportant Notes:\n\nTukey‚Äôs Method:\n\nFirst find the median (Q2)\nSplit the data into lower and upper halves\nFind Q1 as the median of the lower half\nFind Q3 as the median of the upper half\nWhen n is odd, the median is not included in either half\n\nInterpolation Method:\n\nUses positions (n+1)/4 for Q1 and 3(n+1)/4 for Q3\nWhen position falls between values, uses linear interpolation\nDoesn‚Äôt require splitting data into halves\n\n\nBoth methods give the same results for simple positions (Example 1) but can differ when interpolation is needed (Example 2).\n\n\n\n\n\n\nManual Construction of Tukey Boxplot\n\n\n\nStep 1: Calculate Key Components\n\nFind quartiles: Q_1, Q_2 (median), Q_3\nCalculate Interquartile Range: IQR = Q_3 - Q_1\n\nStep 2: Determine Whisker Boundaries\n\nLower fence: Q_1 - 1.5 \\times IQR\nUpper fence: Q_3 + 1.5 \\times IQR\n\nStep 3: Identify Outliers Data points are outliers if they are:\n\nBelow lower fence: x &lt; Q_1 - 1.5 \\times IQR\nAbove upper fence: x &gt; Q_3 + 1.5 \\times IQR\n\nExample: Given data: 2, 4, 6, 8, 9, 10, 11, 12, 14, 16, 50\n\nFind quartiles:\n\nQ_1 = 6\nQ_2 = 10\nQ_3 = 14\n\nCalculate IQR:\n\nIQR = 14 - 6 = 8\n\nCalculate fences:\n\nLower: 6 - (1.5 \\times 8) = -6\nUpper: 14 + (1.5 \\times 8) = 26\n\nIdentify outliers:\n\n50 &gt; 26, therefore 50 is an outlier\n\n\nGraphical Elements:\n\nBox: Draw from Q_1 to Q_3\nLine inside box: Draw at Q_2\nWhiskers: Extend to most extreme non-outlier points\nPoints: Plot outliers individually beyond whiskers\n\n\n\n\n\n\nPercentiles: A More Precise Measure of Relative Standing (*)\n\nWhat Are Percentiles?\nPercentiles give us a more detailed view by dividing data into 100 equal parts. Unlike quartiles, percentiles use linear interpolation for more precise measurements.\nKey Points:\n\nThe 25th percentile equals Q1\nThe 50th percentile equals Q2 (median)\nThe 75th percentile equals Q3\n\n\n\nCalculating Percentiles\nThe Formula: P_k = \\frac{k(n+1)}{100}\nWhere:\n\nP_k is the position for the kth percentile\nk is the percentile we want (1-100)\nn is the number of observations\n\nExample 3: Finding the 60th Percentile Let‚Äôs use student homework scores: 72, 75, 78, 80, 82, 85, 88, 90, 92, 95\nStep 1: Calculate position\n\nn = 10 scores\nFor 60th percentile: P_{60} = \\frac{60(10+1)}{100} = 6.6\n\nStep 2: Find surrounding values\n\nPosition 6: score of 85\nPosition 7: score of 88\n\nStep 3: Interpolate (important: percentiles use linear interpolation)\n\nWe need to go 0.6 of the way between 85 and 88 P_{60} = 85 + 0.6(88-85) P_{60} = 85 + 0.6(3) P_{60} = 85 + 1.8 = 86.8\n\nWhat this means: 60% of students scored 86.8 or below.\n\n\n\nPercentile Ranks (PR) (*)\n\nWhat is a Percentile Rank?\nWhile percentiles tell us the value at a certain position, percentile rank tells us what percentage of values fall below a specific score. Think of it as answering the question ‚ÄúWhat percentage of the class did I score higher than?‚Äù\nPR = \\frac{\\text{number of values below } + 0.5 \\times \\text{number of equal values}}{\\text{total number of values}} \\times 100\nExample 4: Finding a Percentile Rank Consider these exam scores:\n65, 70, 70, 75, 75, 75, 80, 85, 85, 90\nLet‚Äôs find the PR for a score of 75.\nStep 1: Count carefully\n\nValues below 75: 65, 70, 70 (3 values)\nValues equal to 75: 75, 75, 75 (3 values)\nTotal values: 10\n\nStep 2: Apply the formula\nPR = \\frac{3 + 0.5(3)}{10} \\times 100 PR = \\frac{3 + 1.5}{10} \\times 100 PR = \\frac{4.5}{10} \\times 100 = 45\\%\nInterpretation: A score of 75 is higher than 45% of the class scores.\nRemark:\nQ1: ‚ÄúWhy do we use 0.5 for equal values in PR?‚Äù\nA1: This is because we‚Äôre assuming people with the same score are evenly spread across that position. It‚Äôs like saying they share the position equally.\n\n\n\nUnderstanding and Interpreting Box Plots\nBox plots (also known as box-and-whisker plots) are powerful visualization tools for understanding data distributions. In this section, we‚Äôll explore how to construct and interpret box plots using height measurements from two groups.\n\nConstruction of the Tukey Box Plot\nThe box plot was introduced by John Tukey as part of his exploratory data analysis toolkit. It provides a standardized way of displaying the distribution of data based on a five-number summary.\n\nThe Five-Number Summary\nA box plot represents five key statistical values:\n\nMinimum: The smallest value in the dataset (excluding outliers)\nFirst Quartile (Q1): The 25th percentile, below which 25% of observations fall\nMedian (Q2): The 50th percentile, which divides the dataset into two equal halves\nThird Quartile (Q3): The 75th percentile, below which 75% of observations fall\nMaximum: The largest value in the dataset (excluding outliers)\n\n\n\nBox Plot Components\n\n\n\n\n\n\n\n\nFigure¬†11.2: Boxplot diagram showing its key components.\n\n\n\n\n\nThe components of a box plot include:\n\nThe Box:\n\nRepresents the interquartile range (IQR), containing the middle 50% of the data\nLower edge represents Q1\nUpper edge represents Q3\nLine inside the box represents the median (Q2)\n\nThe Whiskers:\n\nExtend from the box to show the range of non-outlier data\nIn a Tukey box plot, whiskers extend up to 1.5 √ó IQR from the box edges:\n\nLower whisker: extends to the minimum value ‚â• (Q1 - 1.5 √ó IQR)\nUpper whisker: extends to the maximum value ‚â§ (Q3 + 1.5 √ó IQR)\n\n\nOutliers:\n\nPoints that fall beyond the whiskers\nIndividually plotted as dots or symbols\nValues that are &lt; (Q1 - 1.5 √ó IQR) or &gt; (Q3 + 1.5 √ó IQR)\n\n\n\n\nKey Features to Observe\nWhen interpreting box plots, look for these characteristics:\n\nCentral Tendency: Location of the median line within the box\nDispersion: Width of the box (IQR) and length of the whiskers\nSkewness:\n\nSymmetrical data: median is approximately in the middle of the box, whiskers are roughly equal in length\nRight (positive) skew: median is closer to the bottom of the box, upper whisker is longer\nLeft (negative) skew: median is closer to the top of the box, lower whisker is longer\n\nOutliers: Presence of individual points beyond the whiskers\n\n\n\n\nCase Study: Comparing Heights Between Groups\nLet‚Äôs apply our understanding of box plots to a real dataset. We have height measurements (in centimeters) from two groups of 25 students each.\n\n# Create the dataset\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Transform dataset from wide to long format\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Display the first few rows\nhead(data_height_l)\n\n  Group_number height\n1      group_1    150\n2      group_1    160\n3      group_1    165\n4      group_1    168\n5      group_1    172\n6      group_1    173\n\n\nLet‚Äôs calculate some summary statistics for each group:\n\n# Calculate summary statistics for each group\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Calculate IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Create a comparison table\nstats_table &lt;- rbind(\n  group1_stats,\n  group2_stats\n)\nrownames(stats_table) &lt;- c(\"Group 1\", \"Group 2\")\n\n# Display the table\nstats_table\n\n        Min. 1st Qu. Median Mean 3rd Qu. Max.\nGroup 1  150     175    180  179     183  200\nGroup 2  138     165    175  172     182  210\n\n# Display IQR values\ncat(\"IQR for Group 1:\", group1_iqr, \"\\n\")\n\nIQR for Group 1: 8 \n\ncat(\"IQR for Group 2:\", group2_iqr, \"\\n\")\n\nIQR for Group 2: 17 \n\n\n\n\nVisualizing the Height Data\nNow, let‚Äôs visualize the data using box plots and density plots:\n\n# Create horizontal boxplots\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Height Distribution by Group\",\n       x = \"Group\",\n       y = \"Height (cm)\")\n\n\n\n\n\n\n\nFigure¬†11.3: Box plots comparing height distributions between groups.\n\n\n\n\n\nTo complement our box plots, let‚Äôs also look at the density distributions:\n\n# Create density plots\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Height Density by Group\",\n       x = \"Height (cm)\",\n       y = \"Density\")\n\n\n\n\n\n\n\nFigure¬†11.4: Density plots showing the height distributions for each group.\n\n\n\n\n\n\n\nBox Plot Interpretation Exercise\nBased on the box plots and density plots above, determine whether each of the following statements is True or False. For each statement, provide a brief explanation based on evidence from the visualizations.\n\n\n\n\n\n\nExercise Questions\n\n\n\n\nStudents from group 2 (G2) in the studied sample are, on average, taller than those from group 1 (G1).\nGroup 1 (G1) height measurements are more dispersed/spread out than group 2 (G2).\nThe lowest person is in group 2 (G2).\nBoth data sets are negatively (left) skewed.\nHalf of the students in group 2 (G2) measure at least 175 cm.\n\n\n\n\nHints for Interpretation\nWhen answering these questions, consider:\n\nThe position of the median line within each box\nThe relative sizes of the boxes (IQR)\nThe positions of the minimum and maximum values\nThe symmetry of the distributions (balanced or skewed)\nThe lengths of the whiskers\n\nFor each statement, determine whether it is True or False and provide your explanation:\n\n\n\n\n\n\nAnswer Template\n\n\n\n\n\n\nStudents from G2 are, on average, taller than G1: [True/False]\n\nExplanation:\n\nG1 height is more dispersed/spread out: [True/False]\n\nExplanation:\n\nThe lowest person is in G2: [True/False]\n\nExplanation:\n\nBoth data sets are negatively (left) skewed: [True/False]\n\nExplanation:\n\nHalf of G2 measure at least 175 cm: [True/False]\n\nExplanation:\n\n\n\n\n\nLet‚Äôs review the answers to our box plot interpretation questions:\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nStudents from G2 are, on average, taller than G1: False\n\nExplanation: The median height (middle line in the boxplot) for G1 is higher than G2.\n\nG1 height is more dispersed/spread out: False\n\nExplanation: G2 shows greater dispersion. This is visible in the boxplot where G2 has a larger interquartile range (IQR) of 17.5 cm compared to G1‚Äôs 9.5 cm. G2 also has a wider range from minimum to maximum values.\n\nThe lowest person is in G2: True\n\nExplanation: The minimum value in G2 is 138 cm, which is lower than the minimum value in G1 (150 cm).\n\nBoth data sets are negatively (left) skewed: True\n\nExplanation: In both groups, the median line is positioned toward the upper part of the box, and the lower whisker is longer than the upper whisker. This indicates that there‚Äôs a longer tail on the left side of the distribution, which means negative skewness.\n\nHalf of G2 measure at least 175 cm: True\n\nExplanation: The median (middle line in the boxplot) for G2 is 175 cm, which means that 50% of the values are greater than or equal to 175 cm.\n\n\n\n\n\n\n\n\nR Code Reference\nHere‚Äôs the complete R code used in this section:\n\n# Load required packages\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n# Set display options\noptions(scipen = 999, digits = 3)\n\n# Create the dataset\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Transform dataset from wide to long format\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Display the first few rows\nhead(data_height_l)\n\n# Calculate summary statistics for each group\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Calculate IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Create horizontal boxplots\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Height Distribution by Group\",\n       x = \"Group\",\n       y = \"Height (cm)\")\n\n# Create density plots\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Height Density by Group\",\n       x = \"Height (cm)\",\n       y = \"Density\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#shape-measures",
    "href": "chapter5.html#shape-measures",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.9 Shape Measures",
    "text": "11.9 Shape Measures\n\nSkewness\n\nDefinition\nSkewness quantifies the asymmetry of a data distribution. It indicates whether data tends to cluster more on one side of the mean than the other.\n\n\nMathematical Expression\nSK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3 where: - n is the sample size - x_i is the i-th observation - \\bar{x} is the sample mean - s is the sample standard deviation\n\n\nSimplified Numerical Example\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Three example datasets with different types of skewness\n# 1. Positive skewness (right tail)\npositive_skew_data &lt;- c(2, 3, 4, 4, 5, 5, 5, 6, 6, 7, 8, 12, 15, 20)\n# 2. Negative skewness (left tail)\nnegative_skew_data &lt;- c(1, 5, 10, 13, 14, 15, 16, 16, 17, 17, 18, 18, 19, 20)\n# 3. Near-zero skewness (symmetry)\nsymmetric_data &lt;- c(1, 3, 5, 7, 9, 10, 11, 12, 13, 15, 17, 19, 21)\n\n# Calculating skewness\npositive_skewness &lt;- skewness(positive_skew_data)\nnegative_skewness &lt;- skewness(negative_skew_data)\nsymmetric_skewness &lt;- skewness(symmetric_data)\n\n# Summary of results\nskewness_data &lt;- data.frame(\n  \"Distribution Type\" = c(\"Positive skewness\", \"Negative skewness\", \"Symmetric distribution\"),\n  \"Skewness value\" = round(c(positive_skewness, negative_skewness, symmetric_skewness), 3),\n  \"Interpretation\" = c(\n    \"Longer right tail (majority of data on the left side)\",\n    \"Longer left tail (majority of data on the right side)\",\n    \"Data distributed symmetrically\"\n  )\n)\n\n# Display table\nskewness_data\n\n       Distribution.Type Skewness.value\n1      Positive skewness           1.42\n2      Negative skewness          -1.33\n3 Symmetric distribution           0.00\n                                         Interpretation\n1 Longer right tail (majority of data on the left side)\n2 Longer left tail (majority of data on the right side)\n3                        Data distributed symmetrically\n\n\n\n\nVisualizations of Skewness Types\n\n# Create a data frame for all sets\ndf_skewness &lt;- rbind(\n  data.frame(value = positive_skew_data, type = \"Positive skewness\", \n             skewness = round(positive_skewness, 2)),\n  data.frame(value = negative_skew_data, type = \"Negative skewness\", \n             skewness = round(negative_skewness, 2)),\n  data.frame(value = symmetric_data, type = \"Symmetric distribution\", \n             skewness = round(symmetric_skewness, 2))\n)\n\n# Histograms for three types of skewness\np1 &lt;- ggplot(df_skewness, aes(x = value)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~type, scales = \"free_x\") +\n  geom_vline(data = df_skewness %&gt;% group_by(type) %&gt;% summarise(mean = mean(value)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_skewness %&gt;% group_by(type) %&gt;% summarise(median = median(value)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = unique(df_skewness[, c(\"type\", \"skewness\")]),\n           aes(x = Inf, y = Inf, label = paste(\"SK =\", skewness)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histograms showing different types of skewness\",\n    subtitle = \"Red line: mean, Green line: median\",\n    x = \"Value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n# Box plots\np2 &lt;- ggplot(df_skewness, aes(x = type, y = value, fill = type)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"skyblue\", \"lightgreen\", \"lightsalmon\")) +\n  labs(\n    title = \"Box plots for different types of skewness\",\n    x = \"Distribution type\",\n    y = \"Value\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Display plots\ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nExample: Voter Turnout Analysis\n\n# Generate three datasets reflecting different types of skewness\nset.seed(123)\n\n# 1. Positive skewness - typical for turnout in regions with low engagement\npositive_turnout &lt;- c(\n  runif(50, min = 20, max = 30),  # Small group with low turnout\n  rbeta(200, shape1 = 2, shape2 = 5) * 50 + 30  # Majority of results shifted to the left\n)\n\n# 2. Negative skewness - typical for regions with high political engagement\nnegative_turnout &lt;- c(\n  rbeta(200, shape1 = 5, shape2 = 2) * 30 + 50,  # Majority of results shifted to the right\n  runif(50, min = 40, max = 50)  # Small group with lower turnout\n)\n\n# 3. Symmetric distribution - typical for regions with uniform engagement\nsymmetric_turnout &lt;- rnorm(250, mean = 65, sd = 8)\n\n# Create data frame\ndf_turnout &lt;- rbind(\n  data.frame(turnout = positive_turnout, region = \"Region A: Positive skewness\"),\n  data.frame(turnout = negative_turnout, region = \"Region B: Negative skewness\"),\n  data.frame(turnout = symmetric_turnout, region = \"Region C: Symmetric distribution\")\n)\n\n# Calculate skewness for each region\nregion_skewness &lt;- df_turnout %&gt;%\n  group_by(region) %&gt;%\n  summarise(skewness = round(skewness(turnout), 2))\n\n# Histogram of turnout by region\np3 &lt;- ggplot(df_turnout, aes(x = turnout)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~region, ncol = 1) +\n  geom_vline(data = df_turnout %&gt;% group_by(region) %&gt;% summarise(mean = mean(turnout)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_turnout %&gt;% group_by(region) %&gt;% summarise(median = median(turnout)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = region_skewness,\n           aes(x = 25, y = 20, label = paste(\"SK =\", skewness)),\n           size = 3.5) +\n  labs(\n    title = \"Voter turnout in different regions\",\n    subtitle = \"Showing three types of skewness\",\n    x = \"Voter turnout (%)\",\n    y = \"Number of districts\"\n  ) +\n  theme_minimal()\n\n# Box plot\np4 &lt;- ggplot(df_turnout, aes(x = region, y = turnout, fill = region)) +\n  geom_boxplot() +\n  labs(\n    title = \"Comparison of turnout distributions across regions\",\n    x = \"Region\",\n    y = \"Voter turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Display plots\ngrid.arrange(p3, p4, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nInterpretation Guide\n\nPositive Skewness (&gt; 0): Distribution has a longer right tail - most values are concentrated on the left side\nNegative Skewness (&lt; 0): Distribution has a longer left tail - most values are concentrated on the right side\nZero Skewness: Distribution is approximately symmetric - values are evenly distributed around the mean\n\n\n\n\nKurtosis\n\nDefinition\nKurtosis measures the ‚Äútailedness‚Äù of a distribution, indicating the presence of extreme values compared to a normal distribution.\n\n\nMathematical Expression\nK = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\n\n\nSimplified Numerical Example\n\n# Three example datasets with different levels of kurtosis\n# 1. Leptokurtic distribution (high kurtosis, \"heavy tails\")\nleptokurtic_data &lt;- c(\n  rnorm(80, mean = 50, sd = 5),  # Most data clustered around the mean\n  c(20, 25, 30, 70, 75, 80)      # A few extreme values\n)\n\n# 2. Platykurtic distribution (low kurtosis, \"flat\")\nplatykurtic_data &lt;- c(\n  runif(50, min = 30, max = 70)  # Uniform distribution of values\n)\n\n# 3. Mesokurtic distribution (normal kurtosis)\nmesokurtic_data &lt;- rnorm(50, mean = 50, sd = 10)\n\n# Calculate kurtosis\nkurtosis_lepto &lt;- kurtosis(leptokurtic_data)\nkurtosis_platy &lt;- kurtosis(platykurtic_data)\nkurtosis_meso &lt;- kurtosis(mesokurtic_data)\n\n# Summary of results\nkurtosis_data &lt;- data.frame(\n  \"Distribution Type\" = c(\"Leptokurtic\", \"Platykurtic\", \"Mesokurtic\"),\n  \"Kurtosis value\" = round(c(kurtosis_lepto, kurtosis_platy, kurtosis_meso), 3),\n  \"Interpretation\" = c(\n    \"Many values near the mean, but also more extreme values\",\n    \"Values more uniformly distributed - flat distribution\",\n    \"Similar to normal distribution\"\n  )\n)\n\n# Display table\nkurtosis_data\n\n  Distribution.Type Kurtosis.value\n1       Leptokurtic           7.39\n2       Platykurtic           1.85\n3        Mesokurtic           2.25\n                                           Interpretation\n1 Many values near the mean, but also more extreme values\n2   Values more uniformly distributed - flat distribution\n3                          Similar to normal distribution\n\n\n\n\nVisualizations of Kurtosis Levels\n\n# Create a data frame for all sets\ndf_kurtosis &lt;- rbind(\n  data.frame(value = leptokurtic_data, type = \"Leptokurtic (K &gt; 3)\", \n             kurtosis = round(kurtosis_lepto, 2)),\n  data.frame(value = platykurtic_data, type = \"Platykurtic (K &lt; 3)\", \n             kurtosis = round(kurtosis_platy, 2)),\n  data.frame(value = mesokurtic_data, type = \"Mesokurtic (K ‚âà 3)\", \n             kurtosis = round(kurtosis_meso, 2))\n)\n\n# Histograms for three types of kurtosis\np5 &lt;- ggplot(df_kurtosis, aes(x = value)) +\n  geom_histogram(bins = 15, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~type, scales = \"free_y\") +\n  geom_text(data = unique(df_kurtosis[, c(\"type\", \"kurtosis\")]),\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtosis)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histograms showing different levels of kurtosis\",\n    x = \"Value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n# Box plots\np6 &lt;- ggplot(df_kurtosis, aes(x = type, y = value, fill = type)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"lightgreen\", \"lightsalmon\", \"skyblue\")) +\n  labs(\n    title = \"Box plots for different levels of kurtosis\",\n    x = \"Distribution type\",\n    y = \"Value\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Display plots\ngrid.arrange(p5, p6, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nExample: Parliamentary Voting Analysis\n\n# Generate three datasets reflecting different levels of kurtosis\nset.seed(456)\n\n# 1. Leptokurtic distribution - typical for votes with strong party discipline\nlepto_voting &lt;- c(\n  rnorm(150, mean = 75, sd = 3),  # Most votes with high agreement\n  c(20, 25, 30, 35, 40, 95, 96, 97, 98, 99)  # A few outlier votes\n)\n\n# 2. Platykurtic distribution - typical for controversial votes\nplaty_voting &lt;- c(\n  runif(80, min = 40, max = 60),  # Votes with moderate agreement\n  runif(80, min = 60, max = 80)   # Votes with higher agreement\n)\n\n# 3. Mesokurtic distribution - typical for normal votes\nmeso_voting &lt;- rnorm(160, mean = 65, sd = 10)\n\n# Create data frame\ndf_voting &lt;- rbind(\n  data.frame(agreement = lepto_voting, bill_type = \"Bills A: Leptokurtic\"),\n  data.frame(agreement = platy_voting, bill_type = \"Bills B: Platykurtic\"),\n  data.frame(agreement = meso_voting, bill_type = \"Bills C: Mesokurtic\")\n)\n\n# Calculate kurtosis for each bill type\nbill_kurtosis &lt;- df_voting %&gt;%\n  group_by(bill_type) %&gt;%\n  summarise(kurtosis = round(kurtosis(agreement), 2))\n\n# Histogram of voting agreement\np7 &lt;- ggplot(df_voting, aes(x = agreement)) +\n  geom_histogram(bins = 20, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~bill_type, ncol = 1) +\n  geom_text(data = bill_kurtosis,\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtosis)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Voting agreement for different types of bills\",\n    subtitle = \"Showing three levels of kurtosis\",\n    x = \"Voting agreement index (%)\",\n    y = \"Number of votes\"\n  ) +\n  theme_minimal()\n\n# Box plot\np8 &lt;- ggplot(df_voting, aes(x = bill_type, y = agreement, fill = bill_type)) +\n  geom_boxplot() +\n  labs(\n    title = \"Comparison of voting agreement distributions\",\n    x = \"Bill type\",\n    y = \"Voting agreement index (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Display plots\ngrid.arrange(p7, p8, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nInterpretation Guide\n\nLeptokurtic (K &gt; 3): ‚ÄúSlender‚Äù distribution with heavy tails - more extreme values than in a normal distribution\nPlatykurtic (K &lt; 3): ‚ÄúFlat‚Äù distribution - fewer extreme values than in a normal distribution\nMesokurtic (K ‚âà 3): Distribution similar to normal in terms of extreme values",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "href": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.10 Exercise 1. Center and dispersion of data",
    "text": "11.10 Exercise 1. Center and dispersion of data\n\nData\nWe have salary data (in thousands of euros) from two small European companies:\n\n\n\nIndex\nCompany X\nCompany Y\n\n\n\n\n1\n2\n3\n\n\n2\n2\n3\n\n\n3\n2\n4\n\n\n4\n3\n4\n\n\n5\n3\n4\n\n\n6\n3\n4\n\n\n7\n3\n4\n\n\n8\n3\n4\n\n\n9\n3\n5\n\n\n10\n4\n5\n\n\n11\n4\n5\n\n\n12\n4\n5\n\n\n13\n4\n5\n\n\n14\n4\n5\n\n\n15\n5\n6\n\n\n16\n5\n6\n\n\n17\n5\n6\n\n\n18\n5\n7\n\n\n19\n20\n7\n\n\n20\n35\n8\n\n\n\nThis table presents the data for both Company X and Company Y side by side, with an index column for easy reference.\n\n\nMeasures of Central Tendency\n\nMean\nThe mean is the average of all values in a dataset.\nFormula: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\nMo≈ºna te≈º zapisaƒá ten wz√≥r w postaci:\n\\bar{x} = \\frac{\\sum_{i=1}^{k} x_i f_i}{n}\ngdzie f_i to czƒôsto≈õƒá bezwzglƒôdna (liczba wystƒÖpie≈Ñ, waga bezwzglƒôdna) i-tej warto≈õci, a k to liczba r√≥≈ºnych warto≈õci cechy (liczba warto≈õci wyr√≥≈ºnionych).\nZ u≈ºyciem czƒôsto≈õci wzglƒôdnych:\n\\bar{x} = \\sum_{i=1}^{k} x_i p_i\ngdzie p_i to czƒôsto≈õƒá wzglƒôdna (frakcja, waga znormalizowana) i-tej warto≈õci, a k to liczba r√≥≈ºnych warto≈õci cechy (liczba warto≈õci wyr√≥≈ºnionych).\n\nManual Calculation for Company X\n\n\n\nValue (x_i)\nFrequency (f_i)\nx_i \\cdot f_i\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nTotal\nn = 20\nSum = 119\n\n\n\n\\bar{x} = \\frac{119}{20} = 5.95\n\n\nManual Calculation for Company Y\n\n\n\nValue (x_i)\nFrequency (f_i)\nx_i \\cdot f_i\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nTotal\nn = 20\nSum = 100\n\n\n\n\\bar{y} = \\frac{100}{20} = 5\n\n\nR Verification\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\nMedian\nThe median is the middle value when the data is ordered.\n\nManual Calculation for Company X\nOrdered data: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\frac{4 + 4}{2} = 4\n\n\nManual Calculation for Company Y\nOrdered data: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\frac{5 + 5}{2} = 5\n\n\nR Verification\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\nMode\nThe mode is the most frequent value in the dataset.\nFor Company X, the mode is 3 (appears 6 times). For Company Y, there are two modes: 4 and 5 (both appear 6 times).\n\n# Function to calculate mode\nget_mode &lt;- function(x) {\n  unique_x &lt;- unique(x)\n  unique_x[which.max(tabulate(match(x, unique_x)))]\n}\n\nget_mode(X)\n\n[1] 3\n\nget_mode(Y)\n\n[1] 4\n\n\n\n\n\nMeasures of Dispersion\n\nVariance\nThe variance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\nPoprawka Bessela jest stosowana przy obliczaniu wariancji z pr√≥by, aby uzyskaƒá nieobciƒÖ≈ºony estymator wariancji populacji. W standardowym wzorze na wariancjƒô z pr√≥by dzielimy przez (n-1) zamiast przez n.\nModyfikacje wzoru dla danych pogrupowanych (szereg czƒôsto≈õci):\nMo≈ºna te≈º zapisaƒá ten wz√≥r w postaci:\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{k} f_i(x_i - \\bar{x})^2\ngdzie f_i to czƒôsto≈õƒá bezwzglƒôdna (liczba wystƒÖpie≈Ñ) i-tej warto≈õci.\nGdy w obliczeniach stosujemy czƒôsto≈õci wzglƒôdne p = f_i/n, gdzie:\n\nf_i to czƒôsto≈õƒá (liczba wystƒÖpie≈Ñ)\nn to ca≈Çkowita liczebno≈õƒá pr√≥by\n\nWz√≥r na wariancjƒô z uwzglƒôdnieniem poprawki Bessela przyjmuje postaƒá:\ns^2 = \\frac{n}{n-1} \\sum_{i=1}^{k} p_i(x_i - \\bar{x})^2\ngdzie:\n\ns^2 to wariancja z pr√≥by\nn to liczebno≈õƒá pr√≥by\np_i to czƒôsto≈õƒá wzglƒôdna i-tej warto≈õci\nx_i to i-ta warto≈õƒá cechy\n\\bar{x} to ≈õrednia arytmetyczna\nk to liczba r√≥≈ºnych warto≈õci cechy\n\nKluczowe jest to, ≈ºe przy stosowaniu czƒôsto≈õci wzglƒôdnych mno≈ºymy ca≈Çe wyra≈ºenie przez czynnik \\frac{n}{n-1}, kt√≥ry wprowadza poprawkƒô Bessela.\n\nManual Calculation for Company X\n\n\n\n\n\n\n\n\n\n\nx_i\nf_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\nf_i(x_i - \\bar{x})^2\n\n\n\n\n2\n3\n-3.95\n15.6025\n46.8075\n\n\n3\n6\n-2.95\n8.7025\n52.215\n\n\n4\n5\n-1.95\n3.8025\n19.0125\n\n\n5\n4\n-0.95\n0.9025\n3.61\n\n\n20\n1\n14.05\n197.4025\n197.4025\n\n\n35\n1\n29.05\n843.9025\n843.9025\n\n\nTotal\n20\n\n\n1162.95\n\n\n\ns^2 = \\frac{1162.95}{19} = 61.21\n\n\nManual Calculation for Company Y\n\n\n\n\n\n\n\n\n\n\ny_i\nf_i\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\nf_i(y_i - \\bar{y})^2\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nTotal\n20\n\n\n34\n\n\n\ns^2 = \\frac{34}{19} = 1.79\n\n\nR Verification\n\nvar(X)\n\n[1] 61.2\n\nvar(Y)\n\n[1] 1.79\n\n\n\n\n\nStandard Deviation\nThe standard deviation is the square root of the variance.\nFormula: s = \\sqrt{s^2}\n\nFor Company X: s = \\sqrt{61.21} = 7.82\nFor Company Y: s = \\sqrt{1.79} = 1.34\n\n\nR Verification\n\nsd(X)\n\n[1] 7.82\n\nsd(Y)\n\n[1] 1.34\n\n\n\n\n\n\nQuartiles\nQuartiles divide the dataset into four equal parts.\n\nManual Calculation for Company X\nOrdered data: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\n\nQ1 (25th percentile): median of first 10 numbers = 3\nQ2 (50th percentile, median): 4\nQ3 (75th percentile): median of last 10 numbers = 5\n\n\n\nManual Calculation for Company Y\nOrdered data: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\n\nQ1 (25th percentile): median of first 10 numbers = 4\nQ2 (50th percentile, median): 5\nQ3 (75th percentile): median of last 10 numbers = 6\n\n\n\nR Verification\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\nIQR\n\nIQR_x = 5 - 3 = 2\nIQR_y = 6 - 4 = 2\n\n\n\n\nTukey Box Plot\nA Tukey box plot visually represents the distribution of data based on quartiles. We‚Äôll use ggplot2 to create the plot.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Prepare the data\ndata &lt;- data.frame(\n  Company = rep(c(\"X\", \"Y\"), each = 20),\n  Salary = c(X, Y)\n)\n\n# Create the box plot\nggplot(data, aes(x = Company, y = Salary, fill = Company)) +\n  geom_boxplot() +\n  labs(title = \"Salary Distribution in Companies X and Y\",\n       x = \"Company\",\n       y = \"Salary (thousands of euros)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n# Create the box plot\nggplot(data, aes(x = Company, y = Salary, fill = Company)) +\n  geom_boxplot(outliers = F) +\n  labs(title = \"Salary Distribution in Companies X and Y\",\n       x = \"Company\",\n       y = \"Salary (thousands of euros)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\nInterpreting the Box Plot\n\nThe box represents the interquartile range (IQR) from Q1 to Q3.\nThe line inside the box is the median (Q2).\nWhiskers extend to the smallest and largest values within 1.5 * IQR.\nPoints beyond the whiskers are considered outliers.\n\n\n\n\nComparison of Results\n\n\n\nMeasure\nCompany X\nCompany Y\n\n\n\n\nMean\n5.95\n5.00\n\n\nMedian\n4\n5\n\n\nMode\n3\n4 and 5\n\n\nVariance\n61.21\n1.79\n\n\nStandard Deviation\n7.82\n1.34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\nKey Observations:\n\nCentral Tendency: Company X has a higher mean but lower median than Company Y, indicating a right-skewed distribution for Company X.\nDispersion: Company X shows much higher variance and standard deviation, suggesting greater salary disparities.\nDistribution Shape: Company Y‚Äôs salaries are more tightly clustered, while Company X has extreme values (potential outliers) that significantly affect its mean and variance.\nQuartiles: Company Y‚Äôs interquartile range (Q3 - Q1) is slightly larger, but its overall range is much smaller than Company X‚Äôs.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-2.-comparing-electoral-district-size-variation-between-countries",
    "href": "chapter5.html#exercise-2.-comparing-electoral-district-size-variation-between-countries",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.11 Exercise 2. Comparing Electoral District Size Variation Between Countries",
    "text": "11.11 Exercise 2. Comparing Electoral District Size Variation Between Countries\n\nData\nWe have electoral district size data from two countries:\n\nx &lt;- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)  # Country high variance\ny &lt;- c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13)  # Country low variance\n\nkable(data.frame(\n  \"Country X (High var.)\" = x,\n  \"Country Y (Low var.)\" = y\n))\n\n\n\n\nCountry.X..High.var..\nCountry.Y..Low.var..\n\n\n\n\n1\n8\n\n\n3\n9\n\n\n5\n9\n\n\n7\n10\n\n\n9\n10\n\n\n11\n11\n\n\n13\n11\n\n\n15\n12\n\n\n17\n12\n\n\n19\n13\n\n\n\n\n\n\n\nMeasures of Central Tendency\n\nArithmetic Mean\nFormula: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\nCalculations for Country X\n\n\n\nElement\nValue\n\n\n\n\n1\n1\n\n\n2\n3\n\n\n3\n5\n\n\n4\n7\n\n\n5\n9\n\n\n6\n11\n\n\n7\n13\n\n\n8\n15\n\n\n9\n17\n\n\n10\n19\n\n\nSum\n100\n\n\n\n\\bar{x} = \\frac{100}{10} = 10\n\nmean_x &lt;- mean(x)\nc(\"Manual\" = 10, \"R\" = mean_x)\n\nManual      R \n    10     10 \n\n\n\n\nCalculations for Country Y\n\n\n\nElement\nValue\n\n\n\n\n1\n8\n\n\n2\n9\n\n\n3\n9\n\n\n4\n10\n\n\n5\n10\n\n\n6\n11\n\n\n7\n11\n\n\n8\n12\n\n\n9\n12\n\n\n10\n13\n\n\nSum\n105\n\n\n\n\\bar{y} = \\frac{105}{10} = 10.5\n\nmean_y &lt;- mean(y)\nc(\"Manual\" = 10.5, \"R\" = mean_y)\n\nManual      R \n  10.5   10.5 \n\n\n\n\n\nMedian\nThe median is the middle value in an ordered dataset.\n\nCalculations for Country X\nOrdered data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nFor n = 10 (even number of observations): Middle positions: 5 and 6 Middle values: 9 and 11\nMedian = \\frac{9 + 11}{2} = 10\n\nmedian_x &lt;- median(x)\nc(\"Manual\" = 10, \"R\" = median_x)\n\nManual      R \n    10     10 \n\n\n\n\nCalculations for Country Y\nOrdered data: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nFor n = 10 (even number of observations): Middle positions: 5 and 6 Middle values: 10 and 11\nMedian = \\frac{10 + 11}{2} = 10.5\n\nmedian_y &lt;- median(y)\nc(\"Manual\" = 10.5, \"R\" = median_y)\n\nManual      R \n  10.5   10.5 \n\n\n\n\n\nMode\n\nCalculations for Country X\n\n\n\nValue\nFrequency\n\n\n\n\n1\n1\n\n\n3\n1\n\n\n5\n1\n\n\n7\n1\n\n\n9\n1\n\n\n11\n1\n\n\n13\n1\n\n\n15\n1\n\n\n17\n1\n\n\n19\n1\n\n\n\nConclusion: No mode (all values occur once)\n\n\nCalculations for Country Y\n\n\n\nValue\nFrequency\n\n\n\n\n8\n1\n\n\n9\n2\n\n\n10\n2\n\n\n11\n2\n\n\n12\n2\n\n\n13\n1\n\n\n\nConclusion: Four modes: 9, 10, 11, 12 (each occurs twice)\n\n# Frequency tables\ntable_x &lt;- table(x)\ntable_y &lt;- table(y)\n\nlist(\n  \"Country X\" = table_x,\n  \"Country Y\" = table_y\n)\n\n$`Country X`\nx\n 1  3  5  7  9 11 13 15 17 19 \n 1  1  1  1  1  1  1  1  1  1 \n\n$`Country Y`\ny\n 8  9 10 11 12 13 \n 1  2  2  2  2  1 \n\n\n\n\n\nVariance\nVariance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\nCalculations for Country X\n\n\n\nx_i\n(x_i - \\bar{x})\n(x_i - \\bar{x})^2\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSum\n\n330\n\n\n\ns^2_X = \\frac{330}{9} = 36.67\n\nvar_x &lt;- var(x)\nc(\"Manual\" = 36.67, \"R\" = var_x)\n\nManual      R \n 36.67  36.67 \n\n\n\n\nCalculations for Country Y\n\n\n\nx_i\n(y_i - \\bar{y})\n(y_i - \\bar{y})^2\n\n\n\n\n8\n-2.5\n6.25\n\n\n9\n-1.5\n2.25\n\n\n9\n-1.5\n2.25\n\n\n10\n-0.5\n0.25\n\n\n10\n-0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n12\n1.5\n2.25\n\n\n12\n1.5\n2.25\n\n\n13\n2.5\n6.25\n\n\nSum\n\n22.5\n\n\n\ns^2_Y = \\frac{22.5}{9} = 2.5\n\nvar_y &lt;- var(y)\nc(\"Manual\" = 2.5, \"R\" = var_y)\n\nManual      R \n   2.5    2.5 \n\n\n\n\n\nStandard Deviation\nStandard deviation is the square root of variance. It measures variability in the same units as the data.\nFormula: s = \\sqrt{s^2}\n\nCalculations for Country X\nUsing previously calculated variance: s^2_X = 36.67\nCalculate square root: s_X = \\sqrt{36.67} \\approx 6.06\n\n\n\nStep\nCalculation\nResult\n\n\n\n\n1. Variance\ns^2_X\n36.67\n\n\n2. Square root\n\\sqrt{36.67}\n6.06\n\n\n\n\nsd_x &lt;- sd(x)\nc(\"Manual\" = 6.06, \"R\" = sd_x)\n\nManual      R \n 6.060  6.055 \n\n\n\n\nCalculations for Country Y\nUsing previously calculated variance: s^2_Y = 2.5\nCalculate square root: s_Y = \\sqrt{2.5} \\approx 1.58\n\n\n\nStep\nCalculation\nResult\n\n\n\n\n1. Variance\ns^2_Y\n2.5\n\n\n2. Square root\n\\sqrt{2.5}\n1.58\n\n\n\n\nsd_y &lt;- sd(y)\nc(\"Manual\" = 1.58, \"R\" = sd_y)\n\nManual      R \n 1.580  1.581 \n\n\nInterpretation:\n\nCountry X: Average deviation from the mean is about 6 seats\nCountry Y: Average deviation from the mean is about 1.6 seats\n\n\n\n\n\nCoefficient of Variation (CV)\nThe coefficient of variation is the ratio of standard deviation to mean, expressed as a percentage.\nFormula: CV = \\frac{s}{\\bar{x}} \\times 100\\%\n\nCalculations for Country X\nCV_X = \\frac{6.06}{10} \\times 100\\% = 60.6\\%\n\n\n\nComponent\nValue\n\n\n\n\nStandard deviation (s)\n6.06\n\n\nMean (\\bar{x})\n10\n\n\nCV\n60.6%\n\n\n\n\ncv_x &lt;- sd(x) / mean(x) * 100\nc(\"Manual\" = 60.6, \"R\" = cv_x)\n\nManual      R \n 60.60  60.55 \n\n\n\n\nCalculations for Country Y\nCV_Y = \\frac{1.58}{10.5} \\times 100\\% = 15.0\\%\n\n\n\nComponent\nValue\n\n\n\n\nStandard deviation (s)\n1.58\n\n\nMean (\\bar{x})\n10.5\n\n\nCV\n15.0%\n\n\n\n\ncv_y &lt;- sd(y) / mean(y) * 100\nc(\"Manual\" = 15.0, \"R\" = cv_y)\n\nManual      R \n 15.00  15.06 \n\n\n\n\n\nQuartiles and Interquartile Range (IQR)\n\nMethods for Calculating Quartiles\nThere are different methods for calculating quartiles. In our manual calculations, we‚Äôll use the median-excluding method:\n\nSplit the series at the median\nMedian is not included in quartile calculations\nCalculate median of each part - these will be Q1 and Q3 respectively\n\n\n\nCalculations for Country X\nOrdered data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMedian = 10 (not included in quartile calculations)\nLower half: 1, 3, 5, 7, 9 Q1 = median of lower half = 5\nUpper half: 11, 13, 15, 17, 19 Q3 = median of upper half = 15\nIQR = Q3 - Q1 = 15 - 5 = 10\n\n\nCalculations for Country Y\nOrdered data: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMedian = 10.5 (not included in quartile calculations)\nLower half: 8, 9, 9, 10, 10 Q1 = median of lower half = 9\nUpper half: 11, 11, 12, 12, 13 Q3 = median of upper half = 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n# Comparison of different quartile calculation methods in R\nmethods_comparison &lt;- data.frame(\n  Method = c(\"Manual (excl. median)\", \n             \"R type=1\", \"R type=2\", \"R type=7 (default)\"),\n  \"Q1 Country X\" = c(5, \n                    quantile(x, 0.25, type=1),\n                    quantile(x, 0.25, type=2),\n                    quantile(x, 0.25, type=7)),\n  \"Q3 Country X\" = c(15,\n                    quantile(x, 0.75, type=1),\n                    quantile(x, 0.75, type=2),\n                    quantile(x, 0.75, type=7)),\n  \"Q1 Country Y\" = c(9,\n                    quantile(y, 0.25, type=1),\n                    quantile(y, 0.25, type=2),\n                    quantile(y, 0.25, type=7)),\n  \"Q3 Country Y\" = c(12,\n                    quantile(y, 0.75, type=1),\n                    quantile(y, 0.75, type=2),\n                    quantile(y, 0.75, type=7))\n)\n\nkable(methods_comparison, digits = 2,\n      caption = \"Comparison of different quartile calculation methods\")\n\n\nComparison of different quartile calculation methods\n\n\n\n\n\n\n\n\n\nMethod\nQ1.Country.X\nQ3.Country.X\nQ1.Country.Y\nQ3.Country.Y\n\n\n\n\nManual (excl. median)\n5.0\n15.0\n9.00\n12.00\n\n\nR type=1\n5.0\n15.0\n9.00\n12.00\n\n\nR type=2\n5.0\n15.0\n9.00\n12.00\n\n\nR type=7 (default)\n5.5\n14.5\n9.25\n11.75\n\n\n\n\n\n\n\nExplanation of Different Quartile Calculation Methods\n\nManual method (excluding median):\n\nSplits data into two parts\nExcludes median\nFinds median of each part\n\nR type=1:\n\nFirst method in R\nUses whole positions\nNo interpolation\n\nR type=2:\n\nSecond method in R\nUses whole positions\nInterpolates when position is not whole\n\nR type=7 (default):\n\nDefault method in R\nUses quantile()[5] from SAS\nInterpolates according to Hyndman and Fan method\n\n\n\n\n\nResults Comparison\n\nsummary_df &lt;- data.frame(\n  Measure = c(\"Mean\", \"Median\", \"Mode\", \"Range\", \"Variance\", \n              \"Std. Dev.\", \"Q1\", \"Q3\", \"IQR\", \"CV (%)\"),\n  \"Country X\" = c(10, 10, \"none\", 18, 36.67, 6.06, 5, 15, 10, 60.6),\n  \"Country Y\" = c(10.5, 10.5, \"9,10,11,12\", 5, 2.5, 1.58, 9, 12, 3, 15.0)\n)\n\nkable(summary_df, \n      caption = \"Summary of all statistical measures\",\n      align = c('l', 'r', 'r'))\n\n\nSummary of all statistical measures\n\n\nMeasure\nCountry.X\nCountry.Y\n\n\n\n\nMean\n10\n10.5\n\n\nMedian\n10\n10.5\n\n\nMode\nnone\n9,10,11,12\n\n\nRange\n18\n5\n\n\nVariance\n36.67\n2.5\n\n\nStd. Dev.\n6.06\n1.58\n\n\nQ1\n5\n9\n\n\nQ3\n15\n12\n\n\nIQR\n10\n3\n\n\nCV (%)\n60.6\n15\n\n\n\n\n\n\n\nComparison using Box Plot\n\ndf_long &lt;- data.frame(\n  country = rep(c(\"X\", \"Y\"), each = 10),\n  size = c(x, y)\n)\n\n# Basic plot\np &lt;- ggplot(df_long, aes(x = country, y = size, fill = country)) +\n  geom_boxplot(outlier.shape = NA) +  # Disable default outlier points\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Add points with transparency\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\")) +\n  labs(\n    title = \"Comparison of Electoral District Size Variation\",\n    subtitle = paste(\"CV: Country X =\", round(cv_x, 1), \"%, Country Y =\", round(cv_y, 1), \"%\"),\n    x = \"Country\",\n    y = \"District Size\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Add quartile annotations\np + annotate(\n  \"text\", \n  x = c(1, 1, 1, 2, 2, 2), \n  y = c(max(x)+1, mean(x), min(x)-1, max(y)+1, mean(y), min(y)-1),\n  label = c(\n    paste(\"Q3 =\", quantile(x, 0.75, type=1)),\n    paste(\"M =\", median(x)),\n    paste(\"Q1 =\", quantile(x, 0.25, type=1)),\n    paste(\"Q3 =\", quantile(y, 0.75, type=1)),\n    paste(\"M =\", median(y)),\n    paste(\"Q1 =\", quantile(y, 0.25, type=1))\n  ),\n  size = 3\n)\n\n\n\n\n\n\n\n\n\n\nMethodological Notes\n\nQuartile Calculations:\n\nThe median-excluding method used may give different results than R‚Äôs default functions\nDifferences in calculation methods don‚Äôt affect overall conclusions\nAlways important to specify the method used in reports\n\nVisualization:\n\nBox plot effectively shows differences in distributions\nAdditional points show actual values\nAnnotations facilitate interpretation\n\n\n\n\nApplication Notes\n\nUsing the Analysis:\n\nAll calculations can be reproduced using the provided R code\nCode chunks are self-contained and documented\nData format requirements are clearly specified\n\nCustomization:\n\nAnalysis can be adapted for different district size datasets\nVisualization parameters can be adjusted for different presentation needs\nStatistical methods can be modified based on specific requirements\n\n\n\n\nConclusion\n\nSummary Statistics Comparison\n\n\n\nMeasure\nCountry X\nCountry Y\nRelative Difference\n\n\n\n\nMean\n10.0\n10.5\nSimilar\n\n\nMedian\n10.0\n10.5\nSimilar\n\n\nMode\nNone\nMultiple (9,10,11,12)\n-\n\n\nRange\n18\n5\n3.6√ó larger in X\n\n\nVariance\n36.67\n2.5\n14.7√ó larger in X\n\n\nIQR\n10\n3\n3.3√ó larger in X\n\n\nCV\n60.6%\n15.0%\n4.0√ó larger in X\n\n\n\n\n\nDistribution Characteristics\nCountry X:\n\nUniform distribution pattern\nNo dominant district size (no mode)\nWide range: 1 to 19 seats\nHigh variability (CV = 60.6%) - Even spread of values across range\n\nCountry Y:\n\nClustered distribution pattern\nMultiple common sizes (four modes)\nNarrow range: 8 to 13 seats\nLow variability (CV = 15.0%) - Values concentrated around mean\n\n\n\nBox Plot Interpretation\nThe box plot visualization reveals:\nStructure Elements:\n\nBox: Shows interquartile range (IQR)\nLower edge: First quartile (Q1)\nUpper edge: Third quartile (Q3)\nInternal line: Median (Q2)\nWhiskers: Extend to ¬±1.5 IQR - Points: Individual district sizes\n\nKey Visual Findings:\n\nBox Size:\n\n\nCountry X: Large box indicates wide spread of middle 50%\nCountry Y: Small box shows tight clustering of middle values\n\n\nWhisker Length:\n\nCountry X: Long whiskers indicate broad overall distribution\nCountry Y: Short whiskers show limited total spread\n\nPoint Distribution:\n\nCountry X: Points widely dispersed\nCountry Y: Points densely clustered\n\n\n\n\nKey Observations\n\nCentral Tendency:\n\nSimilar average district sizes\nDifferent distribution patterns\nDistinct approaches to standardization\n\nVariability Measures:\n\nAll metrics show Country X with 3-15 times more variation\nConsistent pattern across different statistical measures\nSystematic difference in district design\n\nSystem Design:\n\nCountry X: Flexible, varied approach\nCountry Y: Standardized, uniform approach\nDifferent philosophical approaches to representation\n\nRepresentative Implications:\n\nCountry X: Variable voter-to-representative ratios\nCountry Y: More consistent representation levels\nDifferent approaches to democratic representation\n\n\nThis analysis demonstrates fundamental differences in electoral system design between the two countries, with Country X adopting a more varied approach and Country Y maintaining greater uniformity in district sizes.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-3.-voter-participation-and-economic-prosperity",
    "href": "chapter5.html#exercise-3.-voter-participation-and-economic-prosperity",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.12 Exercise 3. Voter Participation and Economic Prosperity",
    "text": "11.12 Exercise 3. Voter Participation and Economic Prosperity\nAnaliza zwiƒÖzku miƒôdzy dobrobytem ekonomicznym a frekwencjƒÖ wyborczƒÖ w dzielnicach Amsterdamu na podstawie danych z wybor√≥w samorzƒÖdowych 2022.\n\nDane\nPr√≥ba obejmuje piƒôƒá reprezentatywnych dzielnic:\n\n\n\nDzielnica\nDoch√≥d (tys. ‚Ç¨)\nFrekwencja (%)\n\n\n\n\nA\n50\n60\n\n\nB\n45\n56\n\n\nC\n56\n70\n\n\nD\n40\n50\n\n\nE\n60\n75\n\n\n\n\n# Wczytanie bibliotek\nlibrary(tidyverse)\n\n# Utworzenie zbioru danych\ndane &lt;- data.frame(\n  dzielnica = LETTERS[1:5],\n  dochod = c(50, 45, 56, 40, 60),\n  frekwencja = c(60, 56, 70, 50, 75)\n)\n\n\n\nCzƒô≈õƒá 1: Statystyki opisowe\n\n# Statystyki dla dochodu\nmean(dane$dochod)\n\n[1] 50.2\n\nmedian(dane$dochod)\n\n[1] 50\n\nsd(dane$dochod)\n\n[1] 8.075\n\nrange(dane$dochod)\n\n[1] 40 60\n\n# Statystyki dla frekwencji\nmean(dane$frekwencja)\n\n[1] 62.2\n\nmedian(dane$frekwencja)\n\n[1] 60\n\nsd(dane$frekwencja)\n\n[1] 10.21\n\nrange(dane$frekwencja)\n\n[1] 50 75\n\n\n\n\nCzƒô≈õƒá 2: Analiza korelacji\n\n# Korelacja Pearsona\ncor.test(dane$dochod, dane$frekwencja)\n\n\n    Pearson's product-moment correlation\n\ndata:  dane$dochod and dane$frekwencja\nt = 16, df = 3, p-value = 0.0005\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9117 0.9996\nsample estimates:\n   cor \n0.9942 \n\n\n\n\nCzƒô≈õƒá 3: Model regresji OLS\n\n# Dopasowanie modelu OLS\nmodel &lt;- lm(frekwencja ~ dochod, data = dane)\n\n# Podsumowanie modelu\nsummary(model)\n\n\nCall:\nlm(formula = frekwencja ~ dochod, data = dane)\n\nResiduals:\n     1      2      3      4      5 \n-1.949  0.336  0.510  0.620  0.482 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.8965     3.9673   -0.23  0.83575    \ndochod        1.2569     0.0782   16.07  0.00052 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.26 on 3 degrees of freedom\nMultiple R-squared:  0.989, Adjusted R-squared:  0.985 \nF-statistic:  258 on 1 and 3 DF,  p-value: 0.000524\n\n\n\n\nWizualizacja\n\n# Wykres rozrzutu z liniƒÖ regresji\nggplot(dane, aes(x = dochod, y = frekwencja)) +\n  geom_point(size = 4, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  geom_text(aes(label = dzielnica), vjust = -1) +\n  labs(\n    title = \"Doch√≥d vs frekwencja wyborcza\",\n    x = \"Doch√≥d (tys. ‚Ç¨)\",\n    y = \"Frekwencja wyborcza (%)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nWnioski\nAnaliza wykaza≈Ça silny dodatni zwiƒÖzek miƒôdzy dobrobytem ekonomicznym dzielnicy a frekwencjƒÖ wyborczƒÖ. Mieszka≈Ñcy dzielnic o wy≈ºszych dochodach czƒô≈õciej uczestniczƒÖ w wyborach samorzƒÖdowych.\nUwaga: Ma≈Ça liczebno≈õƒá pr√≥by (n=5) ogranicza mo≈ºliwo≈õƒá generalizacji wynik√≥w.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-4.-understanding-boxplots-through-life-expectancy-data",
    "href": "chapter5.html#exercise-4.-understanding-boxplots-through-life-expectancy-data",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.13 Exercise 4. Understanding Boxplots Through Life Expectancy Data",
    "text": "11.13 Exercise 4. Understanding Boxplots Through Life Expectancy Data\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# Prepare data\ndata_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007)",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction-to-boxplots",
    "href": "chapter5.html#introduction-to-boxplots",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.14 Introduction to Boxplots",
    "text": "11.14 Introduction to Boxplots\nA boxplot (also known as a box-and-whisker plot) reveals key statistics about your data:\n\nMedian: The middle line in the box (50th percentile)\nFirst quartile (Q1): Bottom of the box (25th percentile)\nThird quartile (Q3): Top of the box (75th percentile)\nInterquartile Range (IQR): The height of the box (Q3 - Q1)\nWhiskers: Extend to the most extreme non-outlier values (Tukey‚Äôs method: 1.5 √ó IQR)\nOutliers: Individual points beyond the whiskers\n\n\nVisualizing Life Expectancy\n\nggplot(data_2007, aes(x = reorder(continent, lifeExp, FUN = median), y = lifeExp)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 24, \n               outlier.fill = \"red\", outlier.alpha = 0.6, outlier.size = 4) +\n  geom_jitter(width = 0.2, alpha = 0.4, color = \"darkblue\") +\n  labs(title = \"Life Expectancy by Continent (2007)\",\n       subtitle = \"Individual points show raw data; red points indicate outliers\",\n       x = \"Continent\",\n       y = \"Life Expectancy (years)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 14)\n  ) +\n  scale_y_continuous(breaks = seq(40, 85, by = 5))",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-the-data",
    "href": "chapter5.html#understanding-the-data",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.15 Understanding the Data",
    "text": "11.15 Understanding the Data\n\nMedian and Distribution\nAnswer True or False:\n\n50% of African countries have life expectancy below 54 years\nThe median life expectancy in Europe is approximately 78 years\nMore than 75% of countries in Oceania have life expectancy above 74 years\n25% of Asian countries have life expectancy below 65 years\nThe middle 50% of life expectancies in Europe fall between 74 and 80 years\n\n\n\nSpread and Variation\nAnswer True or False:\n\nAsia shows the largest spread (IQR) in life expectancy\nEurope has the smallest IQR among all continents\nThe variation in Africa‚Äôs life expectancy is greater than in the Americas\nOceania shows the least variation in life expectancy\nThe range (excluding outliers) in Asia is approximately 20 years\n\n\n\nOutliers and Extremes\nAnswer True or False:\n\nAfrica has two countries with unusually low life expectancy\nThere are no outliers in Oceania‚Äôs distribution\nAsia has both high and low outliers",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#changes-over-time",
    "href": "chapter5.html#changes-over-time",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.16 Changes Over Time",
    "text": "11.16 Changes Over Time\n\ntime_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  mutate(year = factor(year))\n\nggplot(time_comparison, aes(x = continent, y = lifeExp, fill = year)) +\n  geom_boxplot(alpha = 0.7, position = \"dodge\", outlier.shape = 21,\n               outlier.alpha = 0.6) +\n  labs(title = \"Life Expectancy: 1957 vs 2007\",\n       subtitle = \"Comparing distribution changes over 50 years\",\n       x = \"Continent\",\n       y = \"Life Expectancy (years)\",\n       fill = \"Year\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(breaks = seq(30, 85, by = 5))\n\n\n\n\n\n\n\n\n\nTime Comparison Questions\nAnswer True or False:\n\nThe median life expectancy increased in all continents between 1957 and 2007\nThe variation in life expectancy (IQR) decreased in most continents over time\nAfrica showed the smallest improvement in median life expectancy\nThe spread of life expectancies in Asia decreased substantially from 1957 to 2007\nOceania maintained the highest median life expectancy in both time periods\n\n\n\nStatistical Summary\n\n# Calculate summary statistics\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    median = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    n_outliers = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Summary Statistics by Continent and Year\")\n\n\nSummary Statistics by Continent and Year\n\n\ncontinent\nyear\nmedian\nq1\nq3\niqr\nn_outliers\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n0",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#key-learning-points",
    "href": "chapter5.html#key-learning-points",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.17 Key Learning Points",
    "text": "11.17 Key Learning Points\n\nDistribution Center:\n\nMedian shows the typical life expectancy\nChanges in median reflect overall improvements\n\nSpread and Variation:\n\nIQR (box height) indicates data dispersion\nWider boxes suggest more inequality in life expectancy\n\nOutliers and Extremes:\n\nOutliers often represent countries with unique circumstances\n\nTime Comparison:\n\nShows both absolute improvements and changes in variation\nHighlights persistent regional disparities\nReveals different rates of progress across continents",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "href": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "title": "11¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "11.18 Appendix: Summary Tables for Data Types and Applicable Statistical Measures",
    "text": "11.18 Appendix: Summary Tables for Data Types and Applicable Statistical Measures\n\nTable 1: Pros and Cons of Various Statistical Measures\n\nMeasures of Center\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nMean\n- Uses all data points- Allows for further statistical calculations- Ideal for normally distributed data\n- Sensitive to outliers- Not ideal for skewed distributions- Not meaningful for nominal data\nInterval, Ratio, some Discrete, Continuous\n\n\nMedian\n- Not affected by outliers- Good for skewed distributions- Can be used with ordinal data\n- Ignores the actual values of most data points- Less useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nMode\n- Can be used with any data type- Good for finding most common category\n- May not be unique (multimodal)- Not useful for many types of analyses- Ignores magnitude of differences between values\nAll types\n\n\n\n\n\nMeasures of Variability\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nRange\n- Simple to calculate and understand- Gives quick idea of data spread\n- Very sensitive to outliers- Ignores all data between extremes- Not useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nInterquartile Range (IQR)\n- Not affected by outliers- Good for skewed distributions\n- Ignores 50% of the data- Less intuitive than range\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nVariance\n- Uses all data points- Basis for many statistical procedures\n- Sensitive to outliers- Units are squared (less intuitive)\nInterval, Ratio, some Discrete, Continuous\n\n\nStandard Deviation\n- Uses all data points- Same units as original data- Widely used and understood\n- Sensitive to outliers- Assumes roughly normal distribution for interpretation\nInterval, Ratio, some Discrete, Continuous\n\n\nCoefficient of Variation\n- Allows comparison between datasets with different units or means\n- Can be misleading when means are close to zero- Not meaningful for data with negative values\nRatio, some Interval\n\n\n\n\n\nMeasures of Correlation/Association\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nPearson‚Äôs r\n- Measures linear relationship- Widely used and understood\n- Assumes normal distribution- Sensitive to outliers- Only captures linear relationships\nInterval, Ratio, Continuous\n\n\nSpearman‚Äôs rho\n- Can be used with ordinal data- Captures monotonic relationships- Less sensitive to outliers\n- Loses information by converting to ranks- May miss some types of relationships\nOrdinal, Interval, Ratio\n\n\nKendall‚Äôs tau\n- Can be used with ordinal data- More robust than Spearman‚Äôs for small samples- Has nice interpretation (probability of concordance)\n- Loses information by only considering order- Computationally more intensive\nOrdinal, Interval, Ratio\n\n\nChi-square\n- Can be used with nominal data- Tests independence of categorical variables\n- Requires large sample sizes- Sensitive to sample size- Doesn‚Äôt measure strength of association\nNominal, Ordinal\n\n\nCram√©r‚Äôs V\n- Can be used with nominal data- Provides measure of strength of association- Normalized to [0,1] range\n- Interpretation can be subjective- May overestimate association in small samples\nNominal, Ordinal\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nMedian\nMediana\n-\n‚úì\n‚úì\n‚úì\n\n\nArithmetic Mean\n≈örednia arytmetyczna\n-\n-\n‚úì*\n‚úì\n\n\nGeometric Mean\n≈örednia geometryczna\n-\n-\n-\n‚úì\n\n\nHarmonic Mean\n≈örednia harmoniczna\n-\n-\n-\n‚úì\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstƒôp\n-\n‚úì\n‚úì\n‚úì\n\n\nInterquartile Range\nRozstƒôp miƒôdzykwartylowy\n-\n‚úì\n‚úì\n‚úì\n\n\nMean Absolute Deviation\n≈örednie odchylenie bezwzglƒôdne\n-\n-\n‚úì\n‚úì\n\n\nVariance\nWariancja\n-\n-\n‚úì*\n‚úì\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n‚úì*\n‚úì\n\n\nCoefficient of Variation\nWsp√≥≈Çczynnik zmienno≈õci\n-\n-\n-\n‚úì\n\n\nAssociation / Wsp√≥≈Çzale≈ºno≈õƒá:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n‚úì\n‚úì\n‚úì\n\n\nKendall‚Äôs Tau\nTau Kendalla\n-\n‚úì\n‚úì\n‚úì\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n‚úì*\n‚úì\n\n\nCovariance\nKowariancja\n-\n-\n‚úì*\n‚úì\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\nNotes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporzƒÖdkowania\nOrdinal: Ordered categories / Kategorie uporzƒÖdkowane\nInterval: Equal intervals, arbitrary zero / R√≥wne interwa≈Çy, umowne zero\nRatio: Equal intervals, absolute zero / R√≥wne interwa≈Çy, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ‚úì* are commonly used for interval data despite theoretical issues / Niekt√≥re miary oznaczone ‚úì* sƒÖ powszechnie stosowane dla danych przedzia≈Çowych pomimo problem√≥w teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wyb√≥r miary powinien uwzglƒôdniaƒá zar√≥wno poprawno≈õƒá teoretycznƒÖ jak i u≈ºyteczno≈õƒá praktycznƒÖ\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalajƒÖ na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html",
    "href": "rozdzial5.html",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "12.1 Wprowadzenie do Notacji Sigma (Œ£)\nStatystyki opisowe sƒÖ fundamentalnymi narzƒôdziami w badaniach nauk spo≈Çecznych, zapewniajƒÖcymi zwiƒôz≈Çe podsumowanie charakterystyk danych. Pe≈ÇniƒÖ kilka kluczowych funkcji:",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wprowadzenie-do-notacji-sigma-œÉ",
    "href": "rozdzial5.html#wprowadzenie-do-notacji-sigma-œÉ",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "Co to jest notacja sumacyjna Sigma? Sigma (Œ£) to operator matematyczny, kt√≥ry nakazuje nam zsumowaƒá (dodaƒá) sekwencjƒô wyraz√≥w - dzia≈Ça jak instrukcja wykonania dodawania wszystkich element√≥w w okre≈õlonym zakresie.\nCel: Zapewnia zwiƒôz≈Çy spos√≥b zapisu sum wielu podobnych wyraz√≥w za pomocƒÖ jednego symbolu, unikajƒÖc d≈Çugich wyra≈ºe≈Ñ dodawania.\n\n\nPodstawowa formu≈Ça\n\nOg√≥lna forma notacji sigma to:\n\n\\sum_{i=a}^{b} f(i)\n\nIndeks sumowania: i\nDolna granica: a\nG√≥rna granica: b\nFunkcja: f(i)\n\n\n\nPrzyk≈Çady zastosowania notacji Sigma\n\nProsty przyk≈Çad: Suma liczb naturalnych\n\nZa≈Ç√≥≈ºmy, ≈ºe chcesz dodaƒá pierwsze piƒôƒá dodatnich liczb ca≈Çkowitych:\n\n\\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15\n\nPowy≈ºszy zapis dodaje pierwsze piƒôƒá dodatnich liczb ca≈Çkowitych.\n\n\n\nSuma kwadrat√≥w\n\nZa≈Ç√≥≈ºmy, ≈ºe chcesz zsumowaƒá kwadraty pierwszych czterech dodatnich liczb ca≈Çkowitych:\n\n\\sum_{i=1}^{4} i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 1 + 4 + 9 + 16 = 30\n\nJest to suma kwadrat√≥w pierwszych czterech dodatnich liczb ca≈Çkowitych.\n\n\n\nSuma warto≈õci sta≈Çej\n\nSumowanie sta≈Çej warto≈õci c dla n wyraz√≥w:\n\n\\sum_{i=1}^{n} c = c + c + c + ... + c \\text{ (n razy)} = n \\cdot c\n\nPrzyk≈Çad: Suma piƒôciu piƒÖtek:\n\n\\sum_{i=1}^{5} 5 = 5 + 5 + 5 + 5 + 5 = 5 \\cdot 5 = 25\n\n\n\nProste przyk≈Çady w kontek≈õcie statystyki\n\\sum_{i=1}^{n} x_i\n\nIndeks sumowania: i (zazwyczaj oznacza konkretnƒÖ obserwacjƒô w zbiorze danych)\nDolna granica: 1 (zwykle zaczynamy od pierwszej obserwacji)\nG√≥rna granica: n (ca≈Çkowita liczba obserwacji w naszym zbiorze danych)\nWyra≈ºenie: x_i (warto≈õƒá i-tej obserwacji)\n\n\nSumowanie warto≈õci obserwacji\n\nMamy zbi√≥r danych: 5, 8, 12, 15, 20\nSuma wszystkich warto≈õci:\n\n\\sum_{i=1}^{5} x_i = x_1 + x_2 + x_3 + x_4 + x_5 = 5 + 8 + 12 + 15 + 20 = 60\n\nTa suma jest kluczowym elementem przy obliczaniu ≈õredniej arytmetycznej.\n\n\n\nSuma odchyle≈Ñ od ≈õredniej\n\nDla tego samego zbioru danych (5, 8, 12, 15, 20), ≈õrednia wynosi \\bar{x} = 60/5 = 12\nSuma odchyle≈Ñ od ≈õredniej:\n\n\\sum_{i=1}^{5} (x_i - \\bar{x}) = (5-12) + (8-12) + (12-12) + (15-12) + (20-12) = -7 + (-4) + 0 + 3 + 8 = 0\n\nWa≈ºna obserwacja: Suma odchyle≈Ñ od ≈õredniej zawsze wynosi 0, co jest podstawowƒÖ w≈Ça≈õciwo≈õciƒÖ ≈õredniej arytmetycznej.\n\n\n\n\nPodsumowanie\n\nNotacja Sigma (Œ£) pozwala na zwiƒôz≈Çy zapis kluczowych wzor√≥w statystycznych\nNajwa≈ºniejsze zastosowania obejmujƒÖ obliczanie:\n\n≈öredniej arytmetycznej\nWariancji i odchylenia standardowego\nR√≥≈ºnych sum kwadrat√≥w u≈ºywanych w analizie regresji\n\n\n\n\n\n\n\n\nOperatory Sumy (Œ£) i Iloczynu (Œ†)\n\n\n\n\nOperator Sigma (Œ£)\n\\sum to operator sumowania, kt√≥ry nakazuje nam dodaƒá wyrazy:\n\\sum_{i=1}^{n} x_i = x_1 + x_2 + ... + x_n\ngdzie: - i to zmienna indeksowa - Dolna warto≈õƒá pod Œ£ (tutaj i=1) to punkt poczƒÖtkowy - G√≥rna warto≈õƒá (tutaj n) to punkt ko≈Ñcowy\n\n\nOperator Pi (Œ†)\n\\prod to operator iloczynu, kt√≥ry nakazuje nam pomno≈ºyƒá wyrazy:\n\\prod_{i=1}^{n} x_i = x_1 \\times x_2 \\times ... \\times x_n\ngdzie: - i to zmienna indeksowa - Dolna warto≈õƒá pod Œ† (tutaj i=1) to punkt poczƒÖtkowy - G√≥rna warto≈õƒá (tutaj n) to punkt ko≈Ñcowy\n\n\n\n\n\n\n\n\n\nPrzyk≈Çad Œ£\n\n\n\n\\sum_{i=1}^{4} i = 1 + 2 + 3 + 4 = 10\n\n\n\n\n\n\n\n\nPrzyk≈Çad Œ†\n\n\n\n\\prod_{i=1}^{4} i = 1 \\times 2 \\times 3 \\times 4 = 24\n\n\n\n\n\n\n\n\nKluczowe R√≥≈ºnice\n\n\n\n\nŒ£ oznacza wielokrotne dodawanie\nŒ† oznacza wielokrotne mno≈ºenie",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#typy-rozk≈Çad√≥w-danych",
    "href": "rozdzial5.html#typy-rozk≈Çad√≥w-danych",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.2 Typy rozk≈Çad√≥w danych",
    "text": "12.2 Typy rozk≈Çad√≥w danych\n\n\n\n\n\n\nImportant\n\n\n\nRozk≈Çad danych informuje o tym, jakie warto≈õci przyjmuje zmienna i jak czƒôsto.\n\n\nZrozumienie rozk≈Çad√≥w danych jest kluczowe dla analizy i wizualizacji danych. W tym dokumencie przyjrzymy siƒô r√≥≈ºnym typom rozk≈Çad√≥w i sposobom ich wizualizacji przy u≈ºyciu ggplot2 w R.\n\nRozk≈Çad normalny\nRozk≈Çad normalny, znany r√≥wnie≈º jako rozk≈Çad Gaussa, jest symetryczny i ma kszta≈Çt dzwonu.\n\n# Generowanie danych o rozk≈Çadzie normalnym\ndane_normalne &lt;- data.frame(x = rnorm(1000))\n\n# Wykres\nggplot(dane_normalne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozk≈Çad normalny\", x = \"Warto≈õƒá\", y = \"Gƒôsto≈õƒá\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\nRozk≈Çad jednostajny\nW rozk≈Çadzie jednostajnym wszystkie warto≈õci majƒÖ r√≥wne prawdopodobie≈Ñstwo wystƒÖpienia.\n\n# Generowanie danych o rozk≈Çadzie jednostajnym\ndane_jednostajne &lt;- data.frame(x = runif(1000))\n\n# Wykres\nggplot(dane_jednostajne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozk≈Çad jednostajny\", x = \"Warto≈õƒá\", y = \"Gƒôsto≈õƒá\")\n\n\n\n\n\n\n\n\n\n\nRozk≈Çady sko≈õne\nRozk≈Çady sko≈õne sƒÖ asymetryczne, z jednym ogonem d≈Çu≈ºszym ni≈º drugi.\n\n# Generowanie danych o rozk≈Çadzie prawosko≈õnym\ndane_prawoskosne &lt;- data.frame(x = rlnorm(1000))\n\n# Wykres\nggplot(dane_prawoskosne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightyellow\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozk≈Çad prawosko≈õny\", x = \"Warto≈õƒá\", y = \"Gƒôsto≈õƒá\")\n\n\n\n\n\n\n\n\n\n\nRozk≈Çad bimodalny\nRozk≈Çad bimodalny ma dwa szczyty (dwie dominanty), wskazujƒÖce na dwie odrƒôbne podgrupy w danych.\n\n# Generowanie danych bimodalnych\ndane_bimodalne &lt;- data.frame(x = c(rnorm(500, mean = -2), rnorm(500, mean = 2)))\n\n# Wykres\nggplot(dane_bimodalne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightpink\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozk≈Çad bimodalny\", x = \"Warto≈õƒá\", y = \"Gƒôsto≈õƒá\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRozk≈Çad\nKluczowe w≈Ça≈õciwo≈õci\nPrzyk≈Çady\n\n\n\n\nSymetryczny (Normalny)\nSymetryczny, kszta≈Çt dzwonu, wiƒôkszo≈õƒá warto≈õci blisko ≈õredniej\nWzrost doros≈Çych w populacji, wyniki test√≥w IQ, b≈Çƒôdy pomiarowe, wyniki egzamin√≥w standaryzowanych\n\n\nR√≥wnomierny (Jednostajny)\nJednakowe prawdopodobie≈Ñstwo w ca≈Çym zakresie\nOstatnia cyfra numeru telefonu, wyb√≥r losowego dnia tygodnia, pozycja wskaz√≥wki po zakrƒôceniu ko≈Çem fortuny\n\n\nDwumodalny (Bimodalny)\nDwa wyra≈∫ne szczyty, sugeruje istnienie podgrup\nStruktura wieku w miastach uniwersyteckich (studenci i stali mieszka≈Ñcy), opinie na tematy silnie polaryzujƒÖce spo≈Çecze≈Ñstwo, godziny natƒô≈ºenia ruchu drogowego (poranny i popo≈Çudniowy szczyt)\n\n\nSko≈õny w prawo (Prawostronnie asymetryczny)\nWyd≈Çu≈ºony ‚Äúogon‚Äù po prawej stronie, wiƒôkszo≈õƒá warto≈õci mniejsza od ≈õredniej\nCzas oczekiwania w kolejce, czas dojazdu do pracy, wiek zawarcia pierwszego ma≈Ç≈ºe≈Ñstwa\n\n\nSko≈õny z grubym ogonem (Log-normalny)\nSilna asymetria w prawo, warto≈õci nie mogƒÖ byƒá ujemne, d≈Çugi ‚Äúgruby ogon‚Äù\nDochody osobiste, ceny mieszka≈Ñ, wielko≈õƒá gospodarstw domowych\n\n\nSko≈õny o ekstremalnym ogonie (Potƒôgowy)\nEkstremalna asymetria, efekt ‚Äúbogaty staje siƒô bogatszym‚Äù, brak charakterystycznej skali\nMajƒÖtek najbogatszych os√≥b, populacja miast, liczba obserwujƒÖcych w mediach spo≈Çeczno≈õciowych, liczba cytowa≈Ñ publikacji naukowych",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wizualizacja-rozk≈Çad√≥w-danych-rzeczywistych",
    "href": "rozdzial5.html#wizualizacja-rozk≈Çad√≥w-danych-rzeczywistych",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.3 Wizualizacja rozk≈Çad√≥w danych rzeczywistych",
    "text": "12.3 Wizualizacja rozk≈Çad√≥w danych rzeczywistych\nU≈ºyjemy zbioru danych palmerpenguins do wizualizacji rozk≈Çad√≥w danych.\n\nHistogram i wykres gƒôsto≈õci\n\n\n\n\n\n\nUnderstanding Histograms and Density\n\n\n\n‚≠ê A histogram is a special graph for numerical data where:\n\nData is grouped into ranges (called ‚Äúbins‚Äù)\nBars touch each other (unlike bar charts!) because the data is continuous\nEach bar‚Äôs height shows how many values fall into that range\n\nThink of density as showing how common or concentrated certain values are in your data:\n\nA higher point on a density curve (or taller bar in a histogram) means those values appear more frequently in your data\nA lower point means those values are less common\n\nJust like a crowded area has more people per space (higher density), a taller part of the graph shows values that appear more often in your dataset!\n\n\n\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozk≈Çad d≈Çugo≈õci p≈Çetw pingwin√≥w\", \n       x = \"D≈Çugo≈õƒá p≈Çetwy (mm)\", \n       y = \"Gƒôsto≈õƒá\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\nWykres pude≈Çkowy\nWykresy pude≈Çkowe sƒÖ przydatne do por√≥wnywania rozk≈Çad√≥w miƒôdzy kategoriami.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Rozk≈Çad masy cia≈Ça pingwin√≥w wed≈Çug gatunku\", \n       x = \"Gatunek\", \n       y = \"Masa cia≈Ça (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nWykres skrzypcowy\nWykresy skrzypcowe ≈ÇƒÖczƒÖ cechy wykresu pude≈Çkowego i wykresu gƒôsto≈õci.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Rozk≈Çad masy cia≈Ça pingwin√≥w wed≈Çug gatunku\", \n       x = \"Gatunek\", \n       y = \"Masa cia≈Ça (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nWykres grzbietowy\nWykresy grzbietowe sƒÖ przydatne do por√≥wnywania wielu rozk≈Çad√≥w.\n\nlibrary(ggridges)\n\nggplot(penguins, aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.6) +\n  labs(title = \"Rozk≈Çad d≈Çugo≈õci p≈Çetw wed≈Çug gatunku pingwina\",\n       x = \"D≈Çugo≈õƒá p≈Çetwy (mm)\",\n       y = \"Gatunek\")\n\nPicking joint bandwidth of 2.38\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n\n\nPodsumowanie\nZrozumienie i wizualizacja rozk≈Çad√≥w danych sƒÖ kluczowe w analizie danych. ggplot2 zapewnia elastyczny i potƒô≈ºny zestaw narzƒôdzi do tworzenia r√≥≈ºnych typ√≥w wykres√≥w rozk≈Çad√≥w. BadajƒÖc r√≥≈ºne techniki wizualizacji, mo≈ºemy uzyskaƒá wglƒÖd w podstawowe wzorce i charakterystyki naszych danych.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#warto≈õci-odstajƒÖce-outliers",
    "href": "rozdzial5.html#warto≈õci-odstajƒÖce-outliers",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.4 Warto≈õci OdstajƒÖce (Outliers)",
    "text": "12.4 Warto≈õci OdstajƒÖce (Outliers)\nPrzed zag≈Çƒôbieniem siƒô w konkretne miary, kluczowe jest zrozumienie pojƒôcia warto≈õci odstajƒÖcych, poniewa≈º mogƒÖ one znaczƒÖco wp≈Çywaƒá na wiele statystyk opisowych.\nWarto≈õci odstajƒÖce to punkty danych, kt√≥re znacznie r√≥≈ºniƒÖ siƒô od innych obserwacji w zbiorze danych. MogƒÖ wystƒÖpiƒá z powodu:\n\nB≈Çƒôd√≥w pomiaru lub zapisu\nPrawdziwych ekstremalnych warto≈õci w populacji\n\nWarto≈õci odstajƒÖce mogƒÖ mieƒá istotny wp≈Çyw na wiele miar statystycznych, szczeg√≥lnie tych opartych na ≈õrednich lub sumach kwadrat√≥w odchyle≈Ñ. Dlatego wa≈ºne jest, aby:\n\nIdentyfikowaƒá warto≈õci odstajƒÖce zar√≥wno poprzez metody statystyczne, jak i wiedzƒô dziedzinowƒÖ\nBadaƒá przyczyny warto≈õci odstajƒÖcych\nPodejmowaƒá ≈õwiadome decyzje o tym, czy w≈ÇƒÖczaƒá je do analiz, czy nie",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#symbole-stosowane-w-statystyce---podsumowanie",
    "href": "rozdzial5.html#symbole-stosowane-w-statystyce---podsumowanie",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.5 Symbole Stosowane w Statystyce - podsumowanie",
    "text": "12.5 Symbole Stosowane w Statystyce - podsumowanie\n\n\n\n\n\n\n\n\n\n\nMiara\nParametr Populacji\nStatystyka z Pr√≥by\nAlternatywne Oznaczenia\nUwagi\n\n\n\n\nLiczebno≈õƒá\nN\nn\n-\nCa≈Çkowita liczba obserwacji\n\n\n≈örednia\n\\mu\n\\bar{x}\nE(X), M\nE(X) stosowane w rachunku prawdopodobie≈Ñstwa\n\n\nWariancja\n\\sigma^2\ns^2\n\\text{Var}(X), V(X)\nKwadrat odchyle≈Ñ od ≈õredniej\n\n\nOdchylenie standardowe\n\\sigma\ns\n\\text{OS}, \\text{std}\nPierwiastek z wariancji\n\n\nFrakcja/Proporcja\n\\pi, P\n\\hat{p}\n\\text{fr}\nCzƒôsto≈õci wzglƒôdne\n\n\nWsp√≥≈Çczynnik korelacji\n\\rho\nr\n\\text{kor}(x,y)\nWarto≈õci od -1 do +1\n\n\nB≈ÇƒÖd standardowy\n\\sigma_{\\bar{x}}\ns_{\\bar{x}}\n\\text{BS}\nB≈ÇƒÖd standardowy ≈õredniej\n\n\nSuma\n\\sum\n\\sum\n\\sum_{i=1}^n\nZ indeksowaniem\n\n\nPojedyncza obserwacja\nX_i\nx_i\n-\ni-ta obserwacja\n\n\nKowariancja\n\\sigma_{xy}\ns_{xy}\n\\text{Cov}(X,Y)\nWsp√≥lna zmienno≈õƒá\n\n\nMediana\n\\eta\n\\text{Me}\nM\nWarto≈õƒá ≈õrodkowa\n\n\nRozstƒôp\nR\nr\n\\text{max}(X) - \\text{min}(X)\nMiara rozproszenia\n\n\nDominanta\n\\text{Mo}\n\\text{mo}\n\\text{mod}\nWarto≈õƒá najczƒôstsza\n\n\nSko≈õno≈õƒá\n\\gamma_1\ng_1\n\\text{SK}\nAsymetria rozk≈Çadu\n\n\nKurtoza\n\\gamma_2\ng_2\n\\text{KU}\nSp≈Çaszczenie rozk≈Çadu\n\n\n\nDodatkowe wa≈ºne wzory:\n\nMomenty z pr√≥by: m_k = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^k\nMomenty populacji: \\mu_k = E[(X - \\mu)^k]",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-tendencji-centralnej",
    "href": "rozdzial5.html#miary-tendencji-centralnej",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.6 Miary Tendencji Centralnej",
    "text": "12.6 Miary Tendencji Centralnej\nMiary tendencji centralnej majƒÖ na celu identyfikacjƒô ‚Äútypowej‚Äù lub ‚Äúcentralnej‚Äù warto≈õci w zbiorze danych. Trzy podstawowe miary to ≈õrednia, mediana i moda.\n\n≈örednia Arytmetyczna\n≈örednia arytmetyczna to suma wszystkich warto≈õci podzielona przez liczbƒô warto≈õci.\nWz√≥r: \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\nWa≈ºna W≈Ça≈õciwo≈õƒá: ≈örednia jest punktem r√≥wnowagi w danych. Suma odchyle≈Ñ od ≈õredniej zawsze wynosi zero:\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0\nTa w≈Ça≈õciwo≈õƒá sprawia, ≈ºe ≈õrednia jest u≈ºyteczna w wielu obliczeniach statystycznych.\n\n\n\n\n\n\nZrozumienie ≈õredniej jako punktu r√≥wnowagi üéØ\n\n\n\nRozwa≈ºmy zbi√≥r danych X = \\{1, 2, 6, 7, 9\\} na osi liczbowej, wyobra≈ºajƒÖc go sobie jako hu≈õtawkƒô:\n\n\n\nhttps://www.gastonsanchez.com/matrix4sl/mean-as-a-balancing-point.html\n\n\n≈örednia (\\mu) dzia≈Ça jak idealny punkt r√≥wnowagi tej hu≈õtawki. Dla naszych danych:\n\\mu = \\frac{1 + 2 + 6 + 7 + 9}{5} = 5\n\nCo siƒô dzieje przy r√≥≈ºnych punktach podparcia? ü§î\n\nPunkt podparcia w 6 (za wysoko):\n\nLewa strona: Warto≈õci (1, 2) sƒÖ poni≈ºej\nPrawa strona: Warto≈õci (7, 9) sƒÖ powy≈ºej\n\\sum odleg≈Ço≈õci z lewej = (6-1) + (6-2) = 9\n\\sum odleg≈Ço≈õci z prawej = (7-6) + (9-6) = 4\nHu≈õtawka przechyla siƒô w lewo! ‚¨ÖÔ∏è bo 9 &gt; 4\n\nPunkt podparcia w 4 (za nisko):\n\nLewa strona: Warto≈õci (1, 2) sƒÖ poni≈ºej\nPrawa strona: Warto≈õci (6, 7, 9) sƒÖ powy≈ºej\n\\sum odleg≈Ço≈õci z lewej = (4-1) + (4-2) = 5\n\\sum odleg≈Ço≈õci z prawej = (6-4) + (7-4) + (9-4) = 10\nHu≈õtawka przechyla siƒô w prawo! ‚û°Ô∏è bo 5 &lt; 10\n\nPunkt podparcia w ≈õredniej (5) (idealna r√≥wnowaga):\n\n\\sum odleg≈Ço≈õci poni≈ºej = \\sum odleg≈Ço≈õci powy≈ºej\n((5-1) + (5-2)) = ((6-5) + (7-5) + (9-5))\n7 = 7 ‚ú® Idealna r√≥wnowaga!\n\n\nTo pokazuje, dlaczego ≈õrednia jest unikalnym punktem r√≥wnowagi, gdzie:\n\\sum_{i=1}^n (x_i - \\mu) = 0\nHu≈õtawka zawsze bƒôdzie siƒô przechylaƒá, chyba ≈ºe punkt podparcia zostanie umieszczony dok≈Çadnie w ≈õredniej! üé™\n\n\n\n\n\n\n\n\n\n≈örednia jako punkt r√≥wnowagi\n\n\n\nTa wizualizacja pokazuje, jak ≈õrednia arytmetyczna (5) dzia≈Ça jako punkt r√≥wnowagi pomiƒôdzy skupionymi punktami z lewej strony a rozproszonymi punktami z prawej strony:\nLewa strona ≈õredniej:\n\nPunkty o warto≈õciach 2 i 3\nBlisko siebie (r√≥≈ºnica 1 jednostka)\nOdleg≈Ço≈õci od ≈õredniej: 3 i 2 jednostki\nSuma ‚ÄúciƒÖ≈ºenia‚Äù = 5 jednostek\n\nPrawa strona ≈õredniej:\n\nPunkty o warto≈õciach 6 i 9\nBardziej oddalone (r√≥≈ºnica 3 jednostki)\nOdleg≈Ço≈õci od ≈õredniej: 1 i 4 jednostki\nSuma ‚ÄúciƒÖ≈ºenia‚Äù = 5 jednostek\n\nKluczowe obserwacje:\n\n≈örednia (5) jest punktem r√≥wnowagi, mimo ≈ºe:\n\nPunkty po lewej sƒÖ skupione (2,3)\nPunkty po prawej sƒÖ rozproszone (6,9)\nZielone strza≈Çki pokazujƒÖ odleg≈Ço≈õci od ≈õredniej\n\nR√≥wnowaga jest zachowana poniewa≈º:\n\nSuma odleg≈Ço≈õci siƒô r√≥wnowa≈ºy: (5-2) + (5-3) = (6-5) + (9-5)\nCa≈Çkowita suma odleg≈Ço≈õci = 5 jednostek po ka≈ºdej stronie\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrzyk≈Çad Rƒôcznego Obliczenia:\nObliczmy ≈õredniƒÖ dla zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nSumuj wszystkie warto≈õci\n2 + 4 + 4 + 5 + 5 + 7 + 9 = 36\n\n\n2\nPolicz liczbƒô warto≈õci\nn = 7\n\n\n3\nPodziel sumƒô przez n\n36 / 7 = 5,14\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(dane)\n\n[1] 5.142857\n\n\nZalety:\n\n≈Åatwa do obliczenia i zrozumienia\nWykorzystuje wszystkie punkty danych\n\nWady:\n\nWra≈ºliwa na warto≈õci odstajƒÖce\nMo≈ºe nie byƒá dobrƒÖ miarƒÖ dla silnie asymetrycznych rozk≈Çad√≥w danych\n\n\n\nMediana\nMediana to ≈õrodkowa warto≈õƒá, gdy dane sƒÖ uporzƒÖdkowane.\nPrzyk≈Çad Rƒôcznego Obliczenia:\nU≈ºywajƒÖc tego samego zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nWynik\n\n\n\n\n1\nUporzƒÖdkuj dane\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nZnajd≈∫ ≈õrodkowƒÖ warto≈õƒá\n5\n\n\n\nDla parzystej liczby warto≈õci, we≈∫ ≈õredniƒÖ z dw√≥ch ≈õrodkowych warto≈õci.\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmedian(dane)\n\n[1] 5\n\n\nZalety:\n\nNie jest zniekszta≈Çcona przez skrajne warto≈õci odstajƒÖce (outliers)\nLepsza dla rozk≈Çad√≥w sko≈õnych\n\nWady:\n\nNie wykorzystuje wszystkich punkt√≥w danych\n\n\n\n\n\n\n\nWarning\n\n\n\nJak znale≈∫ƒá pozycjƒô mediany w zbiorze danych:\n\nNajpierw posortuj dane rosnƒÖco\nGdy n jest nieparzyste:\n\nPozycja mediany = \\frac{n + 1}{2}\n\nGdy n jest parzyste:\n\nPierwsza pozycja mediany = \\frac{n}{2}\nDruga pozycja mediany = \\frac{n}{2} + 1\nMediana = \\frac{\\text{warto≈õƒá na pozycji }\\frac{n}{2} + \\text{warto≈õƒá na pozycji }(\\frac{n}{2}+1)}{2}\n\n\nPrzyk≈Çady:\n\nNieparzyste n=7: pozycja = \\frac{7+1}{2} = 4-ta warto≈õƒá\nParzyste n=8: pozycje = \\frac{8}{2} = 4-ta i 4+1 = 5-ta warto≈õƒá\n\n\n\n\n\nModa (Dominanta)\nModa to najczƒô≈õciej wystƒôpujƒÖca warto≈õƒá.\nPrzyk≈Çad Rƒôcznego Obliczenia:\nU≈ºywajƒÖc zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nWarto≈õƒá\nCzƒôsto≈õƒá\n\n\n\n\n2\n1\n\n\n4\n2\n\n\n5\n2\n\n\n7\n1\n\n\n9\n1\n\n\n\nModa to 4 i 5 (rozk≈Çad bimodalny).\nObliczenie w R:\n\nlibrary(modeest)\nmfv(dane)  # Najczƒô≈õciej wystƒôpujƒÖca warto≈õƒá\n\n[1] 4 5\n\n\nZalety:\n\nJedyna miara tendencji centralnej dla danych nominalnych\nMo≈ºe identyfikowaƒá wiele punkt√≥w szczytowych (dominujƒÖcych) w danych\n\nWady:\n\nNie zawsze jednoznacznie zdefiniowana\nNie jest odpowiednia dla danych ciƒÖg≈Çych\n\n\n\n≈örednia (arytmetyczna) Wa≈ºona (*)\n≈örednia wa≈ºona jest u≈ºywana, gdy niekt√≥re punkty danych sƒÖ wa≈ºniejsze ni≈º inne. WystƒôpujƒÖ dwa typy ≈õrednich wa≈ºonych: z wagami nienormalizowanymi i z wagami znormalizowanymi.\n\n≈örednia Wa≈ºona z Wagami Nienormalizowanymi\nJest to standardowa forma ≈õredniej wa≈ºonej, gdzie wagi mogƒÖ byƒá dowolnymi liczbami dodatnimi reprezentujƒÖcymi wa≈ºno≈õƒá ka≈ºdego punktu danych.\nWz√≥r: \\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\nPrzyk≈Çad Oblicze≈Ñ Rƒôcznych: Obliczmy ≈õredniƒÖ wa≈ºonƒÖ dla zbioru danych: 2, 4, 5, 7 z wagami 1, 2, 3, 1\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPomn√≥≈º ka≈ºdƒÖ warto≈õƒá przez jej wagƒô\n(2 * 1) + (4 * 2) + (5 * 3) + (7 * 1) = 2 + 8 + 15 + 7 = 32\n\n\n2\nZsumuj wagi\n1 + 2 + 3 + 1 = 7\n\n\n3\nPodziel wynik z kroku 1 przez wynik z kroku 2\n32 / 7 = 4.57\n\n\n\nObliczenia w R:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\n≈örednia Wa≈ºona z Wagami Znormalizowanymi (U≈Çamki)\nW tym przypadku wagi sƒÖ u≈Çamkami sumujƒÖcymi siƒô do 1, reprezentujƒÖcymi proporcjƒô wa≈ºno≈õci dla ka≈ºdego punktu danych.\nWz√≥r: \\bar{x}_w = \\sum_{i=1}^n w_i x_i, gdzie \\sum_{i=1}^n w_i = 1\nPrzyk≈Çad Oblicze≈Ñ Rƒôcznych:\nObliczmy ≈õredniƒÖ wa≈ºonƒÖ dla zbioru danych: 2, 4, 5, 7 z wagami znormalizowanymi 0.1, 0.3, 0.4, 0.2\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPomn√≥≈º ka≈ºdƒÖ warto≈õƒá przez jej wagƒô\n(2 * 0.1) + (4 * 0.3) + (5 * 0.4) + (7 * 0.2)\n\n\n2\nZsumuj wyniki\n0.2 + 1.2 + 2.0 + 1.4 = 4.8\n\n\n\nObliczenia w R:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Uwaga: sumujƒÖ siƒô do 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nZalety ≈örednich Wa≈ºonych:\n\nUwzglƒôdniajƒÖ r√≥≈ºnƒÖ wa≈ºno≈õƒá punkt√≥w danych\n\nWady ≈örednich Wa≈ºonych:\n\nWymagajƒÖ uzasadnienia dla wag\nMogƒÖ byƒá niew≈Ça≈õciwie wykorzystane w celu manipulacji wynikami\nMogƒÖ byƒá mniej intuicyjne w interpretacji ni≈º prosta ≈õrednia arytmetyczna",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-zmienno≈õci-rozproszenia",
    "href": "rozdzial5.html#miary-zmienno≈õci-rozproszenia",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.7 Miary Zmienno≈õci (Rozproszenia)",
    "text": "12.7 Miary Zmienno≈õci (Rozproszenia)\nTe miary opisujƒÖ, jak bardzo rozproszone sƒÖ dane.\n\n\n\n\n\n\nZrozumienie Wariancji\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†12.1: Trzy wykresy punktowe pokazujƒÖce rosnƒÖcƒÖ wariancjƒô przy sta≈Çej ≈õredniej\n\n\n\n\n\nPowy≈ºsze trzy wykresy punktowe pokazujƒÖ, w jaki spos√≥b wariancja mierzy rozproszenie danych wok√≥≈Ç warto≈õci centralnej:\n\nWszystkie rozk≈Çady majƒÖ tƒô samƒÖ ≈õredniƒÖ (Œº = 10), oznaczonƒÖ liniƒÖ przerywanƒÖ\nMa≈Ça Wariancja (œÉ¬≤ = 1): Punkty sƒÖ skupione blisko ≈õredniej\n≈örednia Wariancja (œÉ¬≤ = 4): Punkty wykazujƒÖ umiarkowane rozproszenie\nDu≈ºa Wariancja (œÉ¬≤ = 9): Punkty sƒÖ szeroko rozproszone wok√≥≈Ç ≈õredniej\n\n\n\n\n\n\n\n\n\nR√≥≈ºne Poziomy Zmienno≈õci\n\n\n\n\n\n\n\n\n\n\n\n\nTa wizualizacja przedstawia trzy rozk≈Çady normalne o tej samej ≈õredniej (Œº = 10), ale r√≥≈ºnych poziomach zmienno≈õci:\n\nMa≈Ça zmienno≈õƒá (œÉ = 0.5)\n\nPunkty danych grupujƒÖ siƒô ≈õci≈õle wok√≥≈Ç ≈õredniej\nKrzywa gƒôsto≈õci jest wysoka i wƒÖska\nWiƒôkszo≈õƒá obserwacji mie≈õci siƒô w przedziale ¬±0.5 jednostki (odchylenia stand.) od ≈õredniej\n\n≈örednia zmienno≈õƒá (œÉ = 2.0)\n\nPunkty danych sƒÖ bardziej rozproszone wok√≥≈Ç ≈õredniej\nKrzywa gƒôsto≈õci jest ni≈ºsza i szersza\nWiƒôkszo≈õƒá obserwacji mie≈õci siƒô w przedziale ¬±2 jednostki od ≈õredniej\n\nDu≈ºa zmienno≈õƒá (œÉ = 4.0)\n\nPunkty danych sƒÖ szeroko rozproszone wok√≥≈Ç ≈õredniej\nKrzywa gƒôsto≈õci jest znacznie bardziej p≈Çaska i szeroka\nWiƒôkszo≈õƒá obserwacji mie≈õci siƒô w przedziale ¬±4 jednostki od ≈õredniej\n\n\nZwr√≥ƒá uwagƒô, jak odchylenie standardowe (œÉ) bezpo≈õrednio powiƒÖzane jest z rozproszeniem rozk≈Çadu - wiƒôksze warto≈õci œÉ wskazujƒÖ na wiƒôkszƒÖ zmienno≈õƒá danych, podczas gdy mniejsze warto≈õci oznaczajƒÖ, ≈ºe punkty danych majƒÖ tendencjƒô do grupowania siƒô bli≈ºej ≈õredniej.\n\n\n\nRozstƒôp\nRozstƒôp to r√≥≈ºnica miƒôdzy warto≈õciƒÖ maksymalnƒÖ a minimalnƒÖ.\nWz√≥r: R = x_{max} - x_{min}\nPrzyk≈Çad Rƒôcznego Obliczenia:\nU≈ºywajƒÖc zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nZnajd≈∫ warto≈õƒá maksymalnƒÖ\n9\n\n\n2\nZnajd≈∫ warto≈õƒá minimalnƒÖ\n2\n\n\n3\nOdejmij minimum od maksimum\n9 - 2 = 7\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nrange(dane)\n\n[1] 2 9\n\nmax(dane) - min(dane)\n\n[1] 7\n\n\nZalety:\n\nProsty do obliczenia i zrozumienia\nSzybka informacja o og√≥lnym rozproszeniu danych\n\nWady:\n\nBardzo wra≈ºliwy na warto≈õci odstajƒÖce\nNie dostarcza informacji o rozk≈Çadzie miƒôdzy skrajno≈õciami\n\n\n\nRozstƒôp Miƒôdzykwartylowy (IQR)\nIQR to r√≥≈ºnica miƒôdzy 75. a 25. percentylem (3. a 1. kwartylem).\nWz√≥r: IQR = Q_3 - Q_1\nAby znale≈∫ƒá kwartyle rƒôcznie:\n\nDla nieparzystej liczby warto≈õci:\n\nQ2 (mediana) to ≈õrodkowa warto≈õƒá\nQ1 to mediana dolnej po≈Çowy (wy≈ÇƒÖczajƒÖc medianƒô dla wszystkich obserwacji)\nQ3 to mediana g√≥rnej po≈Çowy (wy≈ÇƒÖczajƒÖc medianƒô dla wszystkich obserwacji)\n\nDla parzystej liczby warto≈õci:\n\nQ2 to ≈õrednia z dw√≥ch ≈õrodkowych warto≈õci\nQ1 to mediana dolnej po≈Çowy (wy≈ÇƒÖczajƒÖc medianƒô dla wszystkich obserwacji)\nQ3 to mediana g√≥rnej po≈Çowy (wy≈ÇƒÖczajƒÖc medianƒô dla wszystkich obserwacji)\n\n\nPrzyk≈Çad Rƒôcznego Obliczenia:\nU≈ºywajƒÖc zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nUporzƒÖdkuj dane\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nZnajd≈∫ Q2 (medianƒô)\n5\n\n\n3\nZnajd≈∫ Q1 (medianƒô dolnej po≈Çowy)\n4\n\n\n4\nZnajd≈∫ Q3 (medianƒô g√≥rnej po≈Çowy)\n7\n\n\n5\nOblicz IQR\nQ3 - Q1 = 7 - 4 = 3\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nprint(dane)\n\n[1] 2 4 4 5 5 7 9\n\nquantile(dane, type = 1)\n\n  0%  25%  50%  75% 100% \n   2    4    5    7    9 \n\nIQR(dane, type = 1)\n\n[1] 3\n\n\nZalety:\n\nOdporny na warto≈õci odstajƒÖce\nDostarcza informacji o rozproszeniu ≈õrodkowych 50% danych\n\nWady:\n\nIgnoruje ogony rozk≈Çadu\nMniej efektywny ni≈º odchylenie standardowe dla rozk≈Çad√≥w normalnych\n\n\n\nWariancja\nWariancja mierzy ≈õrednie kwadratowe odchylenie od ≈õredniej.\nWz√≥r: s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n\n\n\n\n\n\nWariancja: Zrozumienie ≈öredniego Odchylenia Kwadratowego\n\n\n\nCzym jest Wariancja? Wariancja mierzy, jak bardzo punkty danych sƒÖ ‚Äúrozrzucone‚Äù wok√≥≈Ç ≈õredniej - jest ≈õredniƒÖ kwadrat√≥w odchyle≈Ñ od ≈õredniej.\nWz√≥r: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nProsty Przyk≈Çad: Rozwa≈ºmy liczby: 2, 4, 6, 8, 10 ≈örednia (\\bar{x}) = 6\n\n\n\n\n\n\n\n\n\nObliczanie Odchyle≈Ñ:\n\n\n\n\n\n\n\n\n\n\n\n\nWarto≈õƒá\nOdchylenie od ≈õredniej\nKwadrat odchylenia\n\n\n\n\n2\n-4\n16\n\n\n4\n-2\n4\n\n\n6\n0\n0\n\n\n8\n+2\n4\n\n\n10\n+4\n16\n\n\n\nWariancja = \\frac{16 + 4 + 0 + 4 + 16}{4} = 10\nKluczowe Punkty:\n\n≈örednia s≈Çu≈ºy jako punkt odniesienia (niebieska przerywana linia)\nOdchylenia pokazujƒÖ odleg≈Ço≈õƒá od ≈õredniej (czerwone kropkowane linie)\nPodniesienie do kwadratu sprawia, ≈ºe wszystkie odchylenia sƒÖ dodatnie (niebieskie s≈Çupki)\nWiƒôksze odchylenia majƒÖ wiƒôkszy wp≈Çyw na wariancjƒô\n\n\n\nPrzyk≈Çad Rƒôcznego Obliczenia:\nU≈ºywajƒÖc zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz ≈õredniƒÖ\n\\bar{x} = 5,14\n\n\n2\nOdejmij ≈õredniƒÖ od ka≈ºdej obserwacji i podnie≈õ wynik do kwadratu\n(2 - 5,14)^2 = 9,86\n\n\n\n\n(4 - 5,14)^2 = 1,30\n\n\n\n\n(4 - 5,14)^2 = 1,30\n\n\n\n\n(5 - 5,14)^2 = 0,02\n\n\n\n\n(5 - 5,14)^2 = 0,02\n\n\n\n\n(7 - 5,14)^2 = 3,46\n\n\n\n\n(9 - 5,14)^2 = 14,90\n\n\n3\nSumuj kwadraty r√≥≈ºnic\n30,86\n\n\n4\nPodziel przez (n-1), czyli przez liczbƒô obserwacji - 1\n30,86 / 6 = 5,14\n\n\n\nObliczenie w R:\n\nvar(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nWykorzystuje wszystkie punkty danych\nPodstawa dla wielu test√≥w statystycznych*\n\nWady:\n\nJednostki sƒÖ podniesione do kwadratu, co utrudnia interpretacjƒô\nWra≈ºliwa na warto≈õci odstajƒÖce\n\n\n\n\n\n\n\nPoprawka Bessela: Dlaczego Dzielimy przez (n-1), a nie po prostu przez n\n\n\n\nGdy obliczamy odchylenia od ≈õredniej, ich suma musi wynosiƒá zero. To matematyczny fakt: \\sum(x_i - \\bar{x}) = 0\nPomy≈õl o tym Tak:\nJe≈õli masz 5 liczb i ich ≈õredniƒÖ:\n\nPo obliczeniu 4 odchyle≈Ñ od ≈õredniej\n5-te odchylenie MUSI byƒá takie, ≈ºeby suma by≈Ça zero\nNie masz tak naprawdƒô 5 niezale≈ºnych odchyle≈Ñ\nMasz tylko 4 prawdziwie ‚Äúswobodne‚Äù odchylenia\n\nProsty Przyk≈Çad:\nLiczby: 2, 4, 6, 8, 10\n\n≈örednia = 6\nOdchylenia: -4, -2, 0, +2, +4\nZauwa≈º, ≈ºe sumujƒÖ siƒô do zera\nJe≈õli znasz dowolne 4 odchylenia, 5-te jest z g√≥ry okre≈õlone!\n\nDlatego W≈Ça≈õnie:\n\nPrzy obliczaniu wariancji: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nDzielimy przez (n-1), a nie n\nPoniewa≈º tylko (n-1) odchyle≈Ñ jest naprawdƒô niezale≈ºnych\nOstatnie jest okre≈õlone przez pozosta≈Çe\n\nStopnie Swobody:\n\nn = liczba obserwacji\n1 = ograniczenie (odchylenia muszƒÖ sumowaƒá siƒô do zera)\nn-1 = stopnie swobody = liczba prawdziwie niezale≈ºnych odchyle≈Ñ\n\nKiedy Stosowaƒá:\n\nPrzy obliczaniu wariancji z pr√≥by\nPrzy obliczaniu odchylenia standardowego z pr√≥by\n\nKiedy NIE Stosowaƒá:\n\nW obliczeniach dla ca≈Çej populacji (gdy mamy wszystkie dane)\nPrzy obliczaniu odchylenia od ustalonej, znanej warto≈õci parametru populacji statystycznej\n\nPamiƒôtaj:\n\nTo nie jest tylko statystyczny trik\nOdchylenia od ≈õredniej muszƒÖ sumowaƒá siƒô do zera\nTo ograniczenie kosztuje nas jeden stopie≈Ñ swobody\n\n\n\n\n\nOdchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji i mierzy przeciƒôtne rozproszenie danych wzglƒôdem ich ≈õredniej arytmetycznej. W przeciwie≈Ñstwie do wariancji, jest to miara mianowana i interpretowana w jednostkach bdanej zmiennej.\nWz√≥r: s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\nPrzyk≈Çad Rƒôcznego Obliczenia:\nU≈ºywajƒÖc zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz wariancjƒô\ns^2 = 5,14 (z poprzedniego obliczenia)\n\n\n2\nWyciƒÖgnij pierwiastek kwadratowy\ns = \\sqrt{5,14} = 2,27\n\n\n\nObliczenie w R:\n\nsd(dane)\n\n[1] 2.267787\n\n\nZalety:\n\nW tych samych jednostkach co oryginalne dane\nSzeroko stosowane i zrozumia≈Çe\n\nWady:\n\nNadal wra≈ºliwe na warto≈õci odstajƒÖce\nZak≈Çada, ≈ºe dane sƒÖ w przybli≈ºeniu ‚Äúnormalnie‚Äù roz≈Ço≈ºone\n\n\n\nWsp√≥≈Çczynnik zmienno≈õci (*)\nWsp√≥≈Çczynnik zmienno≈õci to odchylenie standardowe podzielone przez ≈õredniƒÖ arytmetycznƒÖ, czƒôsto wyra≈ºany jako warto≈õƒá procentowa.\nWz√≥r: CV = \\frac{s}{\\bar{x}} \\times 100\\%\nPrzyk≈Çad oblicze≈Ñ rƒôcznych:\nDla zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nOblicz ≈õredniƒÖ arytmetycznƒÖ\n\\bar{x} = 5,14\n\n\n2\nOblicz odchylenie standardowe\ns = 2,27\n\n\n3\nPodziel s przez ≈õredniƒÖ i pomn√≥≈º przez 100\n(2,27 / 5,14) * 100 = 44,16\\%\n\n\n\nObliczenia w R:\n\n(sd(dane) / mean(dane)) * 100\n\n[1] 44.09586\n\n\nZalety:\n- Umo≈ºliwia por√≥wnanie zmienno≈õci miƒôdzy zbiorami danych o r√≥≈ºnych jednostkach lub ≈õrednich\n- Przydatny w dziedzinach takich jak finanse do oceny ryzyka\nWady:\n- Nie ma znaczenia dla danych zawierajƒÖcych zar√≥wno warto≈õci dodatnie, jak i ujemne\n- Mo≈ºe byƒá mylƒÖcy, gdy ≈õrednia jest bliska zeru\n\n\n\n\n\n\nOgraniczenia Wsp√≥≈Çczynnika Zmienno≈õci (CV)\n\n\n\nWsp√≥≈Çczynnik zmienno≈õci, obliczany jako (œÉ/Œº) √ó 100\\%, ma dwa istotne ograniczenia:\n\nNie ma interpretacji dla danych zawierajƒÖcych warto≈õci dodatnie i ujemne\n\n≈örednia mo≈ºe byƒá bliska zeru ze wzglƒôdu na wzajemne znoszenie siƒô warto≈õci dodatnich i ujemnych\nPrzyk≈Çad: Zbi√≥r danych {-5, -3, 2, 6} ma ≈õredniƒÖ = 0\n\nCV = (odch. std. / 0) √ó 100%\nProwadzi to do dzielenia przez zero\nNawet gdy ≈õrednia nie jest dok≈Çadnie zero, CV nie reprezentuje prawdziwej wzglƒôdnej zmienno≈õci, gdy dane przechodzƒÖ przez zero\n\nCV zak≈Çada naturalny punkt zerowy i sensowne proporcje miƒôdzy warto≈õciami\n\n\n\nMylƒÖcy gdy ≈õrednia jest bliska zeru\n\nPoniewa≈º CV = (œÉ/Œº) √ó 100\\%, gdy Œº zbli≈ºa siƒô do zera:\n\nMianownik staje siƒô bardzo ma≈Çy\nSkutkuje to ekstremalnie du≈ºymi warto≈õciami CV\nTe du≈ºe warto≈õci nie reprezentujƒÖ sensownie wzglƒôdnej zmienno≈õci\n\nPrzyk≈Çad:\n\nZbi√≥r danych A: {0.001, 0.002, 0.003} ma ≈õredniƒÖ = 0.002\nNawet ma≈Çe odchylenia standardowe dadzƒÖ bardzo du≈ºe CV\nWynikajƒÖcy z tego du≈ºy CV mo≈ºe sugerowaƒá ekstremalne zr√≥≈ºnicowanie, gdy w rzeczywisto≈õci dane sƒÖ do≈õƒá skoncentrowane\n\n\n\n\nNajlepsze zastosowania\nCV jest najbardziej u≈ºyteczny dla:\n\nDanych ≈õci≈õle dodatnich\nDanych mierzonych na skali ilorazowej\nDanych ze ≈õredniƒÖ znacznie powy≈ºej zera\nPor√≥wnywania zmienno≈õci miƒôdzy zbiorami danych o r√≥≈ºnych jednostkach lub skalach",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-po≈Ço≈ºenia-wzglƒôdnego-wzglƒôdnej-pozycji",
    "href": "rozdzial5.html#miary-po≈Ço≈ºenia-wzglƒôdnego-wzglƒôdnej-pozycji",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.8 Miary Po≈Ço≈ºenia Wzglƒôdnego (Wzglƒôdnej Pozycji)",
    "text": "12.8 Miary Po≈Ço≈ºenia Wzglƒôdnego (Wzglƒôdnej Pozycji)\nZrozumienie relatywnej (wzglƒôdnej) pozycji warto≈õci w zbiorze danych.\n\nKwartyle (Q): Podstawy\nKwartyle to specjalne liczby, kt√≥re dzielƒÖ uporzƒÖdkowane dane na cztery r√≥wne czƒô≈õci.\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\n\nCzym sƒÖ Kwartyle?\nPierwszy Kwartyl (Q1):\n\nOddziela najni≈ºsze 25% danych od reszty\nNazywany r√≥wnie≈º 25-tym percentylem\nPrzyk≈Çad: Je≈õli Q1 = 50 w zbiorze wynik√≥w testu, 25% uczni√≥w uzyska≈Ço wynik poni≈ºej 50\n\nDrugi Kwartyl (Q2):\n\nMediana - dzieli dane na p√≥≈Ç\nNazywany r√≥wnie≈º 50-tym percentylem\nPrzyk≈Çad: Je≈õli Q2 = 70, po≈Çowa uczni√≥w uzyska≈Ça wynik poni≈ºej 70\n\nTrzeci Kwartyl (Q3):\n\nOddziela najwy≈ºsze 25% danych od reszty\nNazywany r√≥wnie≈º 75-tym percentylem\nPrzyk≈Çad: Je≈õli Q3 = 85, 75% uczni√≥w uzyska≈Ço wynik poni≈ºej 85\n\nZadanie 1: Kwartyle\nDane: 10, 12, 15, 15, 18, 20, 22, 25, 25 Znajd≈∫: Q1, Q2, Q3\nRozwiƒÖzanie:\n\nQ2 (n = 9, nieparzyste)\n\nPozycja = (9 + 1)/2 = 5\nQ2 = 18\n\nQ1\n\nPozycja = (9 + 1)/4 = 2.5\nMiƒôdzy 12 a 15\nQ1 = (12 + 15)/2 = 13.5\n\nQ3\n\nPozycja = 3(9 + 1)/4 = 7.5\nMiƒôdzy 22 a 25\nQ3 = (22 + 25)/2 = 23.5\n\n\n\n\nJak Obliczaƒá Kwartyle (Krok po Kroku) - Dwie Metody\nPrzeanalizujmy wyniki test√≥w uczni√≥w u≈ºywajƒÖc obu popularnych metod wyznaczania kwartyli:\nPrzyk≈Çad 1: Przypadek Nieparzystej Liczby Wynik√≥w (11 wynik√≥w)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 88, 90\nKrok 1: Znajd≈∫ Q2 (medianƒô) - Tak samo dla obu metod\n\nPrzy n = 11 warto≈õciach (nieparzyste)\nPozycja mediany = 2(n + 1)/4 = (n + 1)/2 = 6\nQ2 = 78\n\nKrok 2: Znajd≈∫ Q1\n\nMetoda Tukeya:\n\nSp√≥jrz na dolnƒÖ po≈Çowƒô: 60, 65, 70, 72, 75\nQ1 = mediana dolnej po≈Çowy = 70\n\nMetoda Interpolacji:\n\nPozycja = (n + 1)/4 = (11 + 1)/4 = 3\nQ1 = 70 (3-cia warto≈õƒá)\n\n\nKrok 3: Znajd≈∫ Q3\n\nMetoda Tukeya:\n\nSp√≥jrz na g√≥rnƒÖ po≈Çowƒô: 80, 82, 85, 88, 90\nQ3 = mediana g√≥rnej po≈Çowy = 85\n\nMetoda Interpolacji:\n\nPozycja = 3(n + 1)/4 = 3(12)/4 = 9\nQ3 = 85 (9-ta warto≈õƒá)\n\n\nPrzyk≈Çad 2: Przypadek Parzystej Liczby (10 wynik√≥w)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 90\nKrok 1: Znajd≈∫ Q2 (medianƒô) - Tak samo dla obu metod\n\nPrzy n = 10 warto≈õciach (parzyste)\nPozycje mediany = 5 i 6\nQ2 = (75 + 78)/2 = 76.5\n\nKrok 2: Znajd≈∫ Q1\n\nMetoda Tukeya:\n\nSp√≥jrz na dolnƒÖ po≈Çowƒô: 60, 65, 70, 72, 75\nQ1 = mediana dolnej po≈Çowy = 70\n\nMetoda Interpolacji:\n\nPozycja = (10 + 1)/4 = 2.75\nQ1 = 65 + 0.75(70 - 65) = 68.75\n\n\nKrok 3: Znajd≈∫ Q3\n\nMetoda Tukeya:\n\nSp√≥jrz na g√≥rnƒÖ po≈Çowƒô: 78, 80, 82, 85, 90\nQ3 = mediana g√≥rnej po≈Çowy = 82\n\nMetoda Interpolacji:\n\nPozycja = 3(10 + 1)/4 = 8.25\nQ3 = 82 + 0.25(85 - 82) = 82.75\n\n\nWa≈ºne Uwagi:\n\nMetoda Tukeya:\n\nNajpierw znajd≈∫ medianƒô (Q2)\nPodziel dane na dolnƒÖ i g√≥rnƒÖ po≈Çowƒô\nZnajd≈∫ Q1 jako medianƒô dolnej po≈Çowy\nZnajd≈∫ Q3 jako medianƒô g√≥rnej po≈Çowy\nGdy n jest nieparzyste, mediana nie jest uwzglƒôdniana w ≈ºadnej po≈Çowie\n\nMetoda Interpolacji:\n\nU≈ºywa pozycji (n+1)/4 dla Q1 i 3(n+1)/4 dla Q3\nGdy pozycja wypada miƒôdzy warto≈õciami, stosuje interpolacjƒô liniowƒÖ\nNie wymaga podzia≈Çu danych na po≈Çowy\n\n\n\n\n\n\n\n\nManual Construction of Tukey Boxplot\n\n\n\nStep 1: Calculate Key Components\n\nFind quartiles: Q_1, Q_2 (median), Q_3\nCalculate Interquartile Range: IQR = Q_3 - Q_1\n\nStep 2: Determine Whisker Boundaries\n\nLower fence: Q_1 - 1.5 \\times IQR\nUpper fence: Q_3 + 1.5 \\times IQR\n\nStep 3: Identify Outliers Data points are outliers if they are:\n\nBelow lower fence: x &lt; Q_1 - 1.5 \\times IQR\nAbove upper fence: x &gt; Q_3 + 1.5 \\times IQR\n\nExample: Given data: 2, 4, 6, 8, 9, 10, 11, 12, 14, 16, 50\n\nFind quartiles:\n\nQ_1 = 6\nQ_2 = 10\nQ_3 = 14\n\nCalculate IQR:\n\nIQR = 14 - 6 = 8\n\nCalculate fences:\n\nLower: 6 - (1.5 \\times 8) = -6\nUpper: 14 + (1.5 \\times 8) = 26\n\nIdentify outliers:\n\n50 &gt; 26, therefore 50 is an outlier\n\n\nGraphical Elements:\n\nBox: Draw from Q_1 to Q_3\nLine inside box: Draw at Q_2\nWhiskers: Extend to most extreme non-outlier points\nPoints: Plot outliers individually beyond whiskers\n\n\n\n\n\n\nPercentyle: Bardziej Precyzyjna Miara Wzglƒôdnej Pozycji (*)\n\nCzym sƒÖ Percentyle?\nPercentyle dajƒÖ nam bardziej szczeg√≥≈Çowy obraz, dzielƒÖc dane na 100 r√≥wnych czƒô≈õci. W przeciwie≈Ñstwie do kwartyli, percentyle u≈ºywajƒÖ interpolacji liniowej.\nKluczowe Punkty:\n\n25-ty percentyl r√≥wna siƒô Q1\n50-ty percentyl r√≥wna siƒô Q2 (mediana)\n75-ty percentyl r√≥wna siƒô Q3\n\n\n\nObliczanie Percentyli\nWz√≥r: P_k = \\frac{k(n+1)}{100}\nGdzie:\n\nP_k to pozycja dla k-tego percentyla\nk to percentyl, kt√≥ry chcemy znale≈∫ƒá (1-100)\nn to liczba obserwacji\n\nPrzyk≈Çad 3: Znajdowanie 60-tego Percentyla U≈ºyjmy wynik√≥w zada≈Ñ domowych uczni√≥w: 72, 75, 78, 80, 82, 85, 88, 90, 92, 95\nKrok 1: Oblicz pozycjƒô\n\nn = 10 wynik√≥w\nDla 60-tego percentyla: P_{60} = \\frac{60(10+1)}{100} = 6.6\n\nKrok 2: Znajd≈∫ otaczajƒÖce warto≈õci\n\nPozycja 6: wynik 85\nPozycja 7: wynik 88\n\nKrok 3: Interpoluj (wa≈ºne: percentyle u≈ºywajƒÖ interpolacji liniowej)\n\nMusimy przej≈õƒá 0.6 drogi miƒôdzy 85 a 88 P_{60} = 85 + 0.6(88-85) P_{60} = 85 + 0.6(3) P_{60} = 85 + 1.8 = 86.8\n\nCo to oznacza: 60% uczni√≥w uzyska≈Ço wynik 86.8 lub ni≈ºszy.\n\n\n\nRangi Percentylowe (PR) (*)\n\nCzym jest Ranga Percentylowa?\nPodczas gdy percentyle m√≥wiƒÖ nam o warto≈õci na okre≈õlonej pozycji, ranga percentylowa m√≥wi nam, jaki procent warto≈õci znajduje siƒô poni≈ºej okre≈õlonego wyniku. Mo≈ºna to traktowaƒá jako odpowied≈∫ na pytanie ‚ÄúJaki procent klasy uzyska≈Ç wynik ni≈ºszy ni≈º ja?‚Äù\nPR = \\frac{\\text{liczba warto≈õci poni≈ºej } + 0.5 \\times \\text{liczba r√≥wnych warto≈õci}}{\\text{ca≈Çkowita liczba warto≈õci}} \\times 100\nPrzyk≈Çad 4: Znajdowanie Rangi Percentylowej Rozwa≈ºmy te wyniki egzaminu:\n65, 70, 70, 75, 75, 75, 80, 85, 85, 90\nZnajd≈∫my PR dla wyniku 75.\nKrok 1: Dok≈Çadnie policz\n\nWarto≈õci poni≈ºej 75: 65, 70, 70 (3 warto≈õci)\nWarto≈õci r√≥wne 75: 75, 75, 75 (3 warto≈õci)\nCa≈Çkowita liczba warto≈õci: 10\n\nKrok 2: Zastosuj wz√≥r\nPR = \\frac{3 + 0.5(3)}{10} \\times 100 PR = \\frac{3 + 1.5}{10} \\times 100 PR = \\frac{4.5}{10} \\times 100 = 45\\%\nInterpretacja: Wynik 75 jest wy≈ºszy ni≈º 45% wynik√≥w w klasie.\nUwaga:\nP1: ‚ÄúDlaczego u≈ºywamy 0.5 dla r√≥wnych warto≈õci w PR?‚Äù\nO1: Jest tak, poniewa≈º zak≈Çadamy, ≈ºe osoby z tym samym wynikiem sƒÖ r√≥wnomiernie roz≈Ço≈ºone na tej pozycji. To jak powiedzenie, ≈ºe dzielƒÖ pozycjƒô po r√≥wno.\n\n\n\n\n\n\nPodw√≥jna Rola Mediany\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†12.2: Wizualizacja podw√≥jnej roli mediany\n\n\n\n\n\nMediana pe≈Çni dwie odrƒôbne, ale powiƒÖzane ze sobƒÖ role:\nA. Jako Miara Centrum:\n\nReprezentuje ≈õrodkowy punkt danych\nR√≥wnowa≈ºy liczbƒô obserwacji po obu stronach\nJest odporna na warto≈õci odstajƒÖce (w przeciwie≈Ñstwie do ≈õredniej arytmetycznej)\n\nB. Jako Miara Pozycji Wzglƒôdnej:\n\nWyznacza 50-ty percentyl\nDzieli dane na dwie r√≥wne czƒô≈õci\nKa≈ºdƒÖ warto≈õƒá mo≈ºna do niej odnie≈õƒá:\n\nPoni≈ºej mediany: dolne 50%\nPowy≈ºej mediany: g√≥rne 50%\n\n\nTa podw√≥jna natura sprawia, ≈ºe mediana jest szczeg√≥lnie przydatna do:\n\nOpisywania warto≈õci typowych (tendencja centralna)\nZrozumienia pozycji w rozk≈Çadzie (pozycja wzglƒôdna)\nDokonywania por√≥wna≈Ñ miƒôdzy r√≥≈ºnymi zbiorami danych\n\n\n\n\n\n\nWykres pude≈Çkowy\nWykresy pude≈Çkowe (znane r√≥wnie≈º jako wykresy skrzynkowe lub box-and-whisker plots) sƒÖ u≈ºytecznymi narzƒôdziami wizualizacji rozk≈Çad√≥w danych.\n\nKonstrukcja wykresu pude≈Çkowego Tukeya\nWykres pude≈Çkowy zosta≈Ç wprowadzony przez Johna Tukeya jako czƒô≈õƒá jego zestawu narzƒôdzi eksploracyjnej analizy danych. Wykres wizualizuje rozk≈Çad danych na podstawie piƒôciu podstawowych statystyk.\n\nPodsumowanie piƒôciu liczb\nWykres pude≈Çkowy reprezentuje piƒôƒá kluczowych warto≈õci statystycznych:\n\nMinimum: Najmniejsza warto≈õƒá w zbiorze danych (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych)\nPierwszy kwartyl (Q1): 25. percentyl, poni≈ºej kt√≥rego znajduje siƒô 25% obserwacji\nMediana (Q2): 50. percentyl, kt√≥ry dzieli zbi√≥r danych na dwie r√≥wne po≈Çowy\nTrzeci kwartyl (Q3): 75. percentyl, poni≈ºej kt√≥rego znajduje siƒô 75% obserwacji\nMaksimum: Najwiƒôksza warto≈õƒá w zbiorze danych (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych)\n\n\n\nKomponenty wykresu pude≈Çkowego\n\n\n\n\n\n\n\n\nFigure¬†12.3: Diagram wykresu pude≈Çkowego pokazujƒÖcy jego kluczowe komponenty.\n\n\n\n\n\nKomponenty wykresu pude≈Çkowego obejmujƒÖ:\n\nPude≈Çko:\n\nReprezentuje rozstƒôp miƒôdzykwartylowy (IQR), zawierajƒÖcy ≈õrodkowe 50% danych\nDolna krawƒôd≈∫ reprezentuje Q1\nG√≥rna krawƒôd≈∫ reprezentuje Q3\nLinia wewnƒÖtrz pude≈Çka reprezentuje medianƒô (Q2)\n\nWƒÖsy:\n\nRozciƒÖgajƒÖ siƒô od pude≈Çka, aby pokazaƒá zakres danych niebƒôdƒÖcych warto≈õciami odstajƒÖcymi\nW wykresie pude≈Çkowym Tukeya wƒÖsy rozciƒÖgajƒÖ siƒô do 1,5 √ó IQR od krawƒôdzi pude≈Çka:\n\nDolny wƒÖs: rozciƒÖga siƒô do minimalnej warto≈õci ‚â• (Q1 - 1,5 √ó IQR)\nG√≥rny wƒÖs: rozciƒÖga siƒô do maksymalnej warto≈õci ‚â§ (Q3 + 1,5 √ó IQR)\n\n\nWarto≈õci odstajƒÖce:\n\nPunkty, kt√≥re wykraczajƒÖ poza wƒÖsy\nIndywidualnie zaznaczone jako kropki lub inne symbole\nWarto≈õci, kt√≥re sƒÖ &lt; (Q1 - 1,5 √ó IQR) lub &gt; (Q3 + 1,5 √ó IQR)\n\n\n\n\nKluczowe cechy do obserwacji\nInterpretujƒÖc wykresy pude≈Çkowe, zwr√≥ƒá uwagƒô na nastƒôpujƒÖce cechy:\n\nTendencja centralna: Po≈Ço≈ºenie linii mediany wewnƒÖtrz pude≈Çka\nRozproszenie: Szeroko≈õƒá pude≈Çka (IQR) i d≈Çugo≈õƒá wƒÖs√≥w\nSko≈õno≈õƒá:\n\nDane symetryczne: mediana znajduje siƒô w przybli≈ºeniu na ≈õrodku pude≈Çka, wƒÖsy majƒÖ podobnƒÖ d≈Çugo≈õƒá\nSko≈õno≈õƒá prawostronna (dodatnia): mediana jest bli≈ºej dolnej czƒô≈õci pude≈Çka, g√≥rny wƒÖs jest d≈Çu≈ºszy\nSko≈õno≈õƒá lewostronna (ujemna): mediana jest bli≈ºej g√≥rnej czƒô≈õci pude≈Çka, dolny wƒÖs jest d≈Çu≈ºszy\n\nWarto≈õci odstajƒÖce: Obecno≈õƒá pojedynczych punkt√≥w poza wƒÖsami\n\n\n\n\nStudium przypadku: Por√≥wnanie wzrostu miƒôdzy grupami\nZastosujmy nasze zrozumienie wykres√≥w pude≈Çkowych do rzeczywistego zbioru danych. Mamy pomiary wzrostu (w centymetrach) z dw√≥ch grup, ka≈ºda po 25 student√≥w.\n\n# Utworzenie zbioru danych\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Przekszta≈Çcenie zbioru danych z formatu szerokiego na d≈Çugi\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Wy≈õwietlenie pierwszych kilku wierszy\nhead(data_height_l)\n\n  Group_number height\n1      group_1    150\n2      group_1    160\n3      group_1    165\n4      group_1    168\n5      group_1    172\n6      group_1    173\n\n\nObliczmy kilka statystyk podsumowujƒÖcych dla ka≈ºdej grupy:\n\n# Obliczenie statystyk podsumowujƒÖcych dla ka≈ºdej grupy\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Obliczenie IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Utworzenie tabeli por√≥wnawczej\nstats_table &lt;- rbind(\n  group1_stats,\n  group2_stats\n)\nrownames(stats_table) &lt;- c(\"Grupa 1\", \"Grupa 2\")\n\n# Wy≈õwietlenie tabeli\nstats_table\n\n        Min. 1st Qu. Median Mean 3rd Qu. Max.\nGrupa 1  150     175    180  179     183  200\nGrupa 2  138     165    175  172     182  210\n\n# Wy≈õwietlenie warto≈õci IQR\ncat(\"IQR dla Grupy 1:\", group1_iqr, \"\\n\")\n\nIQR dla Grupy 1: 8 \n\ncat(\"IQR dla Grupy 2:\", group2_iqr, \"\\n\")\n\nIQR dla Grupy 2: 17 \n\n\n\n\nWizualizacja danych dotyczƒÖcych wzrostu\nTeraz zwizualizujmy dane za pomocƒÖ wykres√≥w pude≈Çkowych i wykres√≥w gƒôsto≈õci:\n\n# Tworzenie poziomych wykres√≥w pude≈Çkowych\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Rozk≈Çad wzrostu wed≈Çug grupy\",\n       x = \"Grupa\",\n       y = \"Wzrost (cm)\")\n\n\n\n\n\n\n\nFigure¬†12.4: Wykresy pude≈Çkowe por√≥wnujƒÖce rozk≈Çady wzrostu miƒôdzy grupami.\n\n\n\n\n\nAby uzupe≈Çniƒá nasze wykresy pude≈Çkowe, przyjrzyjmy siƒô r√≥wnie≈º rozk≈Çadom gƒôsto≈õci:\n\n# Tworzenie wykres√≥w gƒôsto≈õci\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Gƒôsto≈õƒá wzrostu wed≈Çug grupy\",\n       x = \"Wzrost (cm)\",\n       y = \"Gƒôsto≈õƒá\")\n\n\n\n\n\n\n\nFigure¬†12.5: Wykresy gƒôsto≈õci pokazujƒÖce rozk≈Çady wzrostu dla ka≈ºdej grupy.\n\n\n\n\n\n\n\nƒÜwiczenie z interpretacji wykres√≥w pude≈Çkowych\nNa podstawie powy≈ºszych wykres√≥w pude≈Çkowych i wykres√≥w gƒôsto≈õci okre≈õl, czy ka≈ºde z poni≈ºszych stwierdze≈Ñ jest Prawdziwe czy Fa≈Çszywe. Dla ka≈ºdego stwierdzenia podaj kr√≥tkie wyja≈õnienie oparte na dowodach z wizualizacji.\n\n\n\n\n\n\nPytania ƒáwiczeniowe\n\n\n\n\nStudenci z grupy 2 (G2) w badanej pr√≥bie sƒÖ, ≈õrednio, wy≈ºsi ni≈º ci z grupy 1 (G1).\nWzrost w grupie 1 (G1) jest bardziej rozproszony/roz≈Ço≈ºony ni≈º w grupie 2 (G2).\nNajni≈ºsza osoba jest w grupie 2 (G2).\nOba zbiory danych majƒÖ sko≈õno≈õƒá ujemnƒÖ (lewostronnƒÖ).\nPo≈Çowa student√≥w w grupie 2 (G2) ma wzrost co najmniej 175 cm.\n\n\n\n\nWskaz√≥wki do interpretacji\nOdpowiadajƒÖc na te pytania, we≈∫ pod uwagƒô:\n\nPozycjƒô linii mediany w ka≈ºdym pude≈Çku\nWzglƒôdne rozmiary pude≈Çek (IQR)\nPozycje warto≈õci minimalnych i maksymalnych\nSymetriƒô rozk≈Çad√≥w (zr√≥wnowa≈ºone czy z sko≈õno≈õciƒÖ)\nD≈Çugo≈õci wƒÖs√≥w\n\nDla ka≈ºdego stwierdzenia ustal, czy jest Prawdziwe czy Fa≈Çszywe i podaj swoje wyja≈õnienie:\n\n\n\n\n\n\nSzablon odpowiedzi\n\n\n\n\n\n\nStudenci z G2 sƒÖ, ≈õrednio, wy≈ºsi ni≈º z G1: [Prawda/Fa≈Çsz]\n\nWyja≈õnienie:\n\nWzrost G1 jest bardziej rozproszony/roz≈Ço≈ºony: [Prawda/Fa≈Çsz]\n\nWyja≈õnienie:\n\nNajni≈ºsza osoba jest w G2: [Prawda/Fa≈Çsz]\n\nWyja≈õnienie:\n\nOba zbiory danych majƒÖ sko≈õno≈õƒá ujemnƒÖ (lewostronnƒÖ): [Prawda/Fa≈Çsz]\n\nWyja≈õnienie:\n\nPo≈Çowa G2 ma wzrost co najmniej 175 cm: [Prawda/Fa≈Çsz]\n\nWyja≈õnienie:\n\n\n\n\n\nPrzeanalizujmy odpowiedzi na nasze pytania dotyczƒÖce interpretacji wykres√≥w pude≈Çkowych:\n\n\n\n\n\n\nRozwiƒÖzania\n\n\n\n\n\n\nStudenci z G2 sƒÖ, ≈õrednio, wy≈ºsi ni≈º z G1: Fa≈Çsz\n\nWyja≈õnienie: Mediana wzrostu (≈õrodkowa linia w wykresie pude≈Çkowym) dla G1 jest wy≈ºsza ni≈º dla G2.\n\nWzrost G1 jest bardziej rozproszony/roz≈Ço≈ºony: Fa≈Çsz\n\nWyja≈õnienie: G2 wykazuje wiƒôksze rozproszenie. Jest to widoczne na wykresie pude≈Çkowym, gdzie G2 ma wiƒôkszy rozstƒôp miƒôdzykwartylowy (IQR) wynoszƒÖcy 17,5 cm w por√≥wnaniu z 9,5 cm dla G1. G2 ma r√≥wnie≈º szerszy zakres od warto≈õci minimalnej do maksymalnej.\n\nNajni≈ºsza osoba jest w G2: Prawda\n\nWyja≈õnienie: Warto≈õƒá minimalna w G2 wynosi 138 cm, co jest ni≈ºsze ni≈º warto≈õƒá minimalna w G1 (150 cm).\n\nOba zbiory danych majƒÖ sko≈õno≈õƒá ujemnƒÖ (lewostronnƒÖ): Prawda\n\nWyja≈õnienie: W obu grupach linia mediany jest umieszczona w kierunku g√≥rnej czƒô≈õci pude≈Çka, a dolny wƒÖs jest d≈Çu≈ºszy ni≈º g√≥rny. Wskazuje to na d≈Çu≈ºszy ogon po lewej stronie rozk≈Çadu, co oznacza sko≈õno≈õƒá ujemnƒÖ.\n\nPo≈Çowa G2 ma wzrost co najmniej 175 cm: Prawda\n\nWyja≈õnienie: Mediana (≈õrodkowa linia w wykresie pude≈Çkowym) dla G2 wynosi 175 cm, co oznacza, ≈ºe 50% warto≈õci jest wiƒôkszych lub r√≥wnych 175 cm.\n\n\n\n\n\n\n\n\nKod R\n\n# Wczytanie wymaganych pakiet√≥w\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n# Ustawienie opcji wy≈õwietlania\noptions(scipen = 999, digits = 3)\n\n# Utworzenie zbioru danych\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Przekszta≈Çcenie zbioru danych z formatu szerokiego na d≈Çugi\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Wy≈õwietlenie pierwszych kilku wierszy\nhead(data_height_l)\n\n# Obliczenie statystyk podsumowujƒÖcych dla ka≈ºdej grupy\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Obliczenie IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Tworzenie poziomych wykres√≥w pude≈Çkowych\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Rozk≈Çad wzrostu wed≈Çug grupy\",\n       x = \"Grupa\",\n       y = \"Wzrost (cm)\")\n\n# Tworzenie wykres√≥w gƒôsto≈õci\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Gƒôsto≈õƒá wzrostu wed≈Çug grupy\",\n       x = \"Wzrost (cm)\",\n       y = \"Gƒôsto≈õƒá\")",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wykresy-pude≈Çkowe-na-przyk≈Çadzie-danych-o-d≈Çugo≈õci-≈ºycia",
    "href": "rozdzial5.html#wykresy-pude≈Çkowe-na-przyk≈Çadzie-danych-o-d≈Çugo≈õci-≈ºycia",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.9 Wykresy Pude≈Çkowe na Przyk≈Çadzie Danych o D≈Çugo≈õci ≈ªycia",
    "text": "12.9 Wykresy Pude≈Çkowe na Przyk≈Çadzie Danych o D≈Çugo≈õci ≈ªycia\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# Przygotowanie danych\ndata_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007)\n\nWykres pude≈Çkowy (ang. box-and-whisker plot) przedstawia piƒôƒá kluczowych statystyk opisowych danych:\n\nMediana: ≈örodkowa linia w pude≈Çku (50. percentyl)\nPierwszy kwartyl (Q1): Dolna krawƒôd≈∫ pude≈Çka (25. percentyl)\nTrzeci kwartyl (Q3): G√≥rna krawƒôd≈∫ pude≈Çka (75. percentyl)\nRozstƒôp miƒôdzykwartylowy (IQR): Wysoko≈õƒá pude≈Çka (Q3 - Q1)\nWƒÖsy: RozciƒÖgajƒÖ siƒô do najbardziej skrajnych warto≈õci niebƒôdƒÖcych obserwacjami odstajƒÖcymi (metoda Tukeya: 1.5 √ó IQR)\nObserwacje odstajƒÖce: Pojedyncze punkty poza wƒÖsami\n\n\nWizualizacja D≈Çugo≈õci ≈ªycia\n\nggplot(data_2007, aes(x = reorder(continent, lifeExp, FUN = median), y = lifeExp)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 24, \n               outlier.fill = \"red\", outlier.alpha = 0.6, outlier.size = 4) +\n  geom_jitter(width = 0.2, alpha = 0.4, color = \"darkblue\") +\n  labs(title = \"D≈Çugo≈õƒá ≈ªycia wed≈Çug Kontynent√≥w (2007)\",\n       subtitle = \"Pojedyncze punkty pokazujƒÖ surowe dane; czerwone punkty oznaczajƒÖ warto≈õci odstajƒÖce\",\n       x = \"Kontynent\",\n       y = \"D≈Çugo≈õƒá ≈ºycia (w latach)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_y_continuous(breaks = seq(40, 85, by = 5))\n\n\n\n\n\n\n\n\n\n\nAnaliza Danych\n\n\nMediana i Rozk≈Çad\nOdpowiedz Prawda lub Fa≈Çsz:\n\n50% kraj√≥w afryka≈Ñskich ma d≈Çugo≈õƒá ≈ºycia poni≈ºej 52 lat\nMediana d≈Çugo≈õci ≈ºycia w Europie wynosi oko≈Ço 78 lat\nPonad 75% kraj√≥w Oceanii ma d≈Çugo≈õƒá ≈ºycia powy≈ºej 75 lat\n25% kraj√≥w azjatyckich ma d≈Çugo≈õƒá ≈ºycia poni≈ºej 68 lat\n≈örodkowe 50% d≈Çugo≈õci ≈ºycia w Europie mie≈õci siƒô miƒôdzy 76 a 80 lat\n\n\n\nRozrzut i Zmienno≈õƒá\nOdpowiedz Prawda lub Fa≈Çsz:\n\nAzja wykazuje najwiƒôkszy rozrzut (IQR) w d≈Çugo≈õci ≈ºycia\nEuropa ma najmniejszy IQR w≈õr√≥d wszystkich kontynent√≥w\nZmienno≈õƒá d≈Çugo≈õci ≈ºycia w Afryce jest wiƒôksza ni≈º w obu Amerykach\nOceania wykazuje najmniejszƒÖ zmienno≈õƒá w d≈Çugo≈õci ≈ºycia\nWƒÖsy dla Azji rozciƒÖgajƒÖ siƒô w przybli≈ºeniu od 58 do 82 lat (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych)\n\n\n\nWarto≈õci OdstajƒÖce i Ekstrema\nOdpowiedz Prawda lub Fa≈Çsz:\n\nAfryka ma dwa kraje z wyjƒÖtkowo niskƒÖ d≈Çugo≈õciƒÖ ≈ºycia\nW rozk≈Çadzie dla Oceanii nie ma warto≈õci odstajƒÖcych\nAzja ma kilka niskich warto≈õci odstajƒÖcych (poni≈ºej 55 lat)\n\n\n\nZmiany w Czasie\n\ntime_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  mutate(year = factor(year))\n\nggplot(time_comparison, aes(x = continent, y = lifeExp, fill = year)) +\n  geom_boxplot(alpha = 0.7, position = \"dodge\", outlier.shape = 21,\n               outlier.alpha = 0.6) +\n  labs(title = \"D≈Çugo≈õƒá ≈ªycia: 1957 vs 2007\",\n       subtitle = \"Por√≥wnanie zmian rozk≈Çadu na przestrzeni 50 lat\",\n       x = \"Kontynent\",\n       y = \"D≈Çugo≈õƒá ≈ºycia (w latach)\",\n       fill = \"Rok\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 14)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(breaks = seq(30, 85, by = 5))\n\n\n\n\n\n\n\n\n\n\nPytania dotyczƒÖce Zmian w Czasie\nOdpowiedz Prawda lub Fa≈Çsz:\n\nMediana d≈Çugo≈õci ≈ºycia wzros≈Ça na wszystkich kontynentach miƒôdzy 1957 a 2007 rokiem\nZmienno≈õƒá d≈Çugo≈õci ≈ºycia (IQR) zmniejszy≈Ça siƒô na wiƒôkszo≈õci kontynent√≥w w czasie\nAfryka wykaza≈Ça najmniejszƒÖ poprawƒô mediany d≈Çugo≈õci ≈ºycia\nRozrzut d≈Çugo≈õci ≈ºycia w Azji znaczƒÖco siƒô zmniejszy≈Ç od 1957 do 2007 roku\nOceania utrzyma≈Ça najwy≈ºszƒÖ medianƒô d≈Çugo≈õci ≈ºycia w obu okresach\n\n\n\nPodsumowanie Statystyczne\n\n# Obliczenie statystyk opisowych\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    mediana = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    min = min(lifeExp),\n    max = max(lifeExp),\n    liczba_odstajƒÖcych = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Statystyki Opisowe wed≈Çug Kontynentu i Roku\")\n\n\nStatystyki Opisowe wed≈Çug Kontynentu i Roku\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontinent\nyear\nmediana\nq1\nq3\niqr\nmin\nmax\nliczba_odstajƒÖcych\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n31.6\n58.1\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n39.6\n76.4\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n40.7\n70.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n60.9\n80.7\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n30.3\n67.8\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n43.8\n82.6\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n48.1\n73.5\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n71.8\n81.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n70.3\n70.3\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n80.2\n81.2\n0\n\n\n\n\n\n\n\nNajwa≈ºniejsze Wnioski\n\nCentrum Rozk≈Çadu:\n\nMediana pokazuje typowƒÖ d≈Çugo≈õƒá ≈ºycia\nZmiany mediany odzwierciedlajƒÖ og√≥lnƒÖ poprawƒô\n\nRozrzut i Zmienno≈õƒá:\n\nIQR (wysoko≈õƒá pude≈Çka) wskazuje na rozproszenie danych\nSzersze pude≈Çka sugerujƒÖ wiƒôksze nier√≥wno≈õci w d≈Çugo≈õci ≈ºycia\n\nWarto≈õci OdstajƒÖce i Ekstrema:\n\nWarto≈õci odstajƒÖce czƒôsto reprezentujƒÖ kraje o wyjƒÖtkowej sytuacji\n\nPor√≥wnanie w Czasie:\n\nPokazuje zar√≥wno bezwzglƒôdnƒÖ poprawƒô, jak i zmiany w wariancji\nUwydatnia utrzymujƒÖce siƒô r√≥≈ºnice regionalne\nUjawnia r√≥≈ºne tempo postƒôpu na poszczeg√≥lnych kontynentach\n\n\n\n\nStatistical Summary\n\n# Calculate summary statistics\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    median = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    min = min(lifeExp),\n    max = max(lifeExp),\n    n_outliers = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Summary Statistics by Continent and Year\")\n\n\nSummary Statistics by Continent and Year\n\n\ncontinent\nyear\nmedian\nq1\nq3\niqr\nmin\nmax\nn_outliers\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n31.6\n58.1\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n39.6\n76.4\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n40.7\n70.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n60.9\n80.7\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n30.3\n67.8\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n43.8\n82.6\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n48.1\n73.5\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n71.8\n81.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n70.3\n70.3\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n80.2\n81.2\n0",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-kszta≈Çtu",
    "href": "rozdzial5.html#miary-kszta≈Çtu",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.10 Miary Kszta≈Çtu",
    "text": "12.10 Miary Kszta≈Çtu\n\nSko≈õno≈õƒá\n\nDefinicja\nSko≈õno≈õƒá kwantyfikuje asymetriƒô rozk≈Çadu danych. Wskazuje, czy dane grupujƒÖ siƒô bardziej po jednej stronie ≈õredniej ni≈º po drugiej.\n\n\nWyra≈ºenie Matematyczne\nSK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3 gdzie: - n to wielko≈õƒá pr√≥by - x_i to i-ta obserwacja - \\bar{x} to ≈õrednia z pr√≥by - s to odchylenie standardowe z pr√≥by\n\n\nUproszczony Przyk≈Çad Numeryczny\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Trzy przyk≈Çadowe zestawy danych z r√≥≈ºnymi typami sko≈õno≈õci\n# 1. Sko≈õno≈õƒá dodatnia (prawy ogon)\ndane_skosnosc_dodatnia &lt;- c(2, 3, 4, 4, 5, 5, 5, 6, 6, 7, 8, 12, 15, 20)\n# 2. Sko≈õno≈õƒá ujemna (lewy ogon)\ndane_skosnosc_ujemna &lt;- c(1, 5, 10, 13, 14, 15, 16, 16, 17, 17, 18, 18, 19, 20)\n# 3. Sko≈õno≈õƒá bliska zeru (symetria)\ndane_skosnosc_symetryczna &lt;- c(1, 3, 5, 7, 9, 10, 11, 12, 13, 15, 17, 19, 21)\n\n# Obliczenie sko≈õno≈õci\nskosnosc_dodatnia &lt;- skewness(dane_skosnosc_dodatnia)\nskosnosc_ujemna &lt;- skewness(dane_skosnosc_ujemna)\nskosnosc_symetryczna &lt;- skewness(dane_skosnosc_symetryczna)\n\n# Zestawienie wynik√≥w\ndane_skosnosci &lt;- data.frame(\n  \"Typ rozk≈Çadu\" = c(\"Sko≈õno≈õƒá dodatnia\", \"Sko≈õno≈õƒá ujemna\", \"Rozk≈Çad symetryczny\"),\n  \"Warto≈õƒá sko≈õno≈õci\" = round(c(skosnosc_dodatnia, skosnosc_ujemna, skosnosc_symetryczna), 3),\n  \"Interpretacja\" = c(\n    \"D≈Çu≈ºszy prawy ogon (wiƒôkszo≈õƒá danych po lewej stronie)\",\n    \"D≈Çu≈ºszy lewy ogon (wiƒôkszo≈õƒá danych po prawej stronie)\",\n    \"Dane roz≈Ço≈ºone symetrycznie\"\n  )\n)\n\n# Wy≈õwietlenie tabeli\ndane_skosnosci\n\n         Typ.rozk≈Çadu Warto≈õƒá.sko≈õno≈õci\n1   Sko≈õno≈õƒá dodatnia              1.42\n2     Sko≈õno≈õƒá ujemna             -1.33\n3 Rozk≈Çad symetryczny              0.00\n                                           Interpretacja\n1 D≈Çu≈ºszy prawy ogon (wiƒôkszo≈õƒá danych po lewej stronie)\n2 D≈Çu≈ºszy lewy ogon (wiƒôkszo≈õƒá danych po prawej stronie)\n3                            Dane roz≈Ço≈ºone symetrycznie\n\n\n\n\nWizualizacje Typ√≥w Sko≈õno≈õci\n\n# Tworzymy ramkƒô danych dla wszystkich zestaw√≥w\ndf_skosnosc &lt;- rbind(\n  data.frame(wartosc = dane_skosnosc_dodatnia, typ = \"Sko≈õno≈õƒá dodatnia\", \n             skosnosc = round(skosnosc_dodatnia, 2)),\n  data.frame(wartosc = dane_skosnosc_ujemna, typ = \"Sko≈õno≈õƒá ujemna\", \n             skosnosc = round(skosnosc_ujemna, 2)),\n  data.frame(wartosc = dane_skosnosc_symetryczna, typ = \"Rozk≈Çad symetryczny\", \n             skosnosc = round(skosnosc_symetryczna, 2))\n)\n\n# Histogramy dla trzech typ√≥w sko≈õno≈õci\np1 &lt;- ggplot(df_skosnosc, aes(x = wartosc)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~typ, scales = \"free_x\") +\n  geom_vline(data = df_skosnosc %&gt;% group_by(typ) %&gt;% summarise(mean = mean(wartosc)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_skosnosc %&gt;% group_by(typ) %&gt;% summarise(median = median(wartosc)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = unique(df_skosnosc[, c(\"typ\", \"skosnosc\")]),\n           aes(x = Inf, y = Inf, label = paste(\"SK =\", skosnosc)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histogramy pokazujƒÖce r√≥≈ºne typy sko≈õno≈õci\",\n    subtitle = \"Czerwona linia: ≈õrednia, Zielona linia: mediana\",\n    x = \"Warto≈õƒá\",\n    y = \"Czƒôsto≈õƒá\"\n  ) +\n  theme_minimal()\n\n# Wykresy pude≈Çkowe\np2 &lt;- ggplot(df_skosnosc, aes(x = typ, y = wartosc, fill = typ)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"skyblue\", \"lightgreen\", \"lightsalmon\")) +\n  labs(\n    title = \"Wykresy pude≈Çkowe dla r√≥≈ºnych typ√≥w sko≈õno≈õci\",\n    x = \"Typ rozk≈Çadu\",\n    y = \"Warto≈õƒá\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Wy≈õwietlenie wykres√≥w\ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nPrzyk≈Çad: Analiza Frekwencji Wyborczej\n\n# Generujemy trzy zestawy danych odzwierciedlajƒÖce r√≥≈ºne typy sko≈õno≈õci\nset.seed(123)\n\n# 1. Sko≈õno≈õƒá dodatnia - typowa dla frekwencji w regionach o niskim zaanga≈ºowaniu\nfrekwencja_dodatnia &lt;- c(\n  runif(50, min = 20, max = 30),  # Ma≈Ça grupa z niskƒÖ frekwencjƒÖ\n  rbeta(200, shape1 = 2, shape2 = 5) * 50 + 30  # Wiƒôkszo≈õƒá wynik√≥w przesuniƒôtych w lewo\n)\n\n# 2. Sko≈õno≈õƒá ujemna - typowa dla region√≥w z wysokim zaanga≈ºowaniem politycznym\nfrekwencja_ujemna &lt;- c(\n  rbeta(200, shape1 = 5, shape2 = 2) * 30 + 50,  # Wiƒôkszo≈õƒá wynik√≥w przesuniƒôtych w prawo\n  runif(50, min = 40, max = 50)  # Ma≈Ça grupa z ni≈ºszƒÖ frekwencjƒÖ\n)\n\n# 3. Rozk≈Çad symetryczny - typowy dla region√≥w z r√≥wnomiernym zaanga≈ºowaniem\nfrekwencja_symetryczna &lt;- rnorm(250, mean = 65, sd = 8)\n\n# Tworzymy ramkƒô danych\ndf_frekwencja &lt;- rbind(\n  data.frame(frekwencja = frekwencja_dodatnia, region = \"Region A: Sko≈õno≈õƒá dodatnia\"),\n  data.frame(frekwencja = frekwencja_ujemna, region = \"Region B: Sko≈õno≈õƒá ujemna\"),\n  data.frame(frekwencja = frekwencja_symetryczna, region = \"Region C: Rozk≈Çad symetryczny\")\n)\n\n# Obliczamy sko≈õno≈õƒá dla ka≈ºdego regionu\nskosnosci_regionow &lt;- df_frekwencja %&gt;%\n  group_by(region) %&gt;%\n  summarise(skosnosc = round(skewness(frekwencja), 2))\n\n# Histogram frekwencji wed≈Çug region√≥w\np3 &lt;- ggplot(df_frekwencja, aes(x = frekwencja)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~region, ncol = 1) +\n  geom_vline(data = df_frekwencja %&gt;% group_by(region) %&gt;% summarise(mean = mean(frekwencja)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_frekwencja %&gt;% group_by(region) %&gt;% summarise(median = median(frekwencja)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = skosnosci_regionow,\n           aes(x = 25, y = 20, label = paste(\"SK =\", skosnosc)),\n           size = 3.5) +\n  labs(\n    title = \"Frekwencja wyborcza w r√≥≈ºnych regionach\",\n    subtitle = \"Pokazuje trzy rodzaje sko≈õno≈õci\",\n    x = \"Frekwencja wyborcza (%)\",\n    y = \"Liczba obwod√≥w\"\n  ) +\n  theme_minimal()\n\n# Wykres pude≈Çkowy\np4 &lt;- ggplot(df_frekwencja, aes(x = region, y = frekwencja, fill = region)) +\n  geom_boxplot() +\n  labs(\n    title = \"Por√≥wnanie rozk≈Çad√≥w frekwencji w regionach\",\n    x = \"Region\",\n    y = \"Frekwencja wyborcza (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Wy≈õwietlenie wykres√≥w\ngrid.arrange(p3, p4, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nPrzewodnik Interpretacji\n\nSko≈õno≈õƒá Dodatnia (&gt; 0): Rozk≈Çad ma d≈Çu≈ºszy ogon prawy - wiƒôkszo≈õƒá warto≈õci jest skupiona po lewej stronie\nSko≈õno≈õƒá Ujemna (&lt; 0): Rozk≈Çad ma d≈Çu≈ºszy ogon lewy - wiƒôkszo≈õƒá warto≈õci jest skupiona po prawej stronie\nSko≈õno≈õƒá Zero: Rozk≈Çad w przybli≈ºeniu symetryczny - warto≈õci roz≈Ço≈ºone r√≥wnomiernie wok√≥≈Ç ≈õredniej\n\n\n\n\nKurtoza\n\nDefinicja\nKurtoza mierzy ‚Äúogoniasto≈õƒá‚Äù rozk≈Çadu, wskazujƒÖc na obecno≈õƒá warto≈õci ekstremalnych w por√≥wnaniu z rozk≈Çadem normalnym.\n\n\nWyra≈ºenie Matematyczne\nK = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\n\n\nUproszczony Przyk≈Çad Numeryczny\n\n# Trzy przyk≈Çadowe zestawy danych z r√≥≈ºnymi poziomami kurtozy\n# 1. Rozk≈Çad leptokurtyczny (wysoka kurtoza, \"ciƒô≈ºkie ogony\")\ndane_leptokurtyczne &lt;- c(\n  rnorm(80, mean = 50, sd = 5),  # Wiƒôkszo≈õƒá danych skupiona wok√≥≈Ç ≈õredniej\n  c(20, 25, 30, 70, 75, 80)      # Kilka warto≈õci ekstremalnych\n)\n\n# 2. Rozk≈Çad platykurtyczny (niska kurtoza, \"p≈Çaski\")\ndane_platykurtyczne &lt;- c(\n  runif(50, min = 30, max = 70)  # R√≥wnomierny rozk≈Çad warto≈õci\n)\n\n# 3. Rozk≈Çad mezokurtyczny (normalna kurtoza)\ndane_mezokurtyczne &lt;- rnorm(50, mean = 50, sd = 10)\n\n# Obliczenie kurtozy\nkurtoza_lepto &lt;- kurtosis(dane_leptokurtyczne)\nkurtoza_platy &lt;- kurtosis(dane_platykurtyczne)\nkurtoza_mezo &lt;- kurtosis(dane_mezokurtyczne)\n\n# Zestawienie wynik√≥w\ndane_kurtozy &lt;- data.frame(\n  \"Typ rozk≈Çadu\" = c(\"Leptokurtyczny\", \"Platykurtyczny\", \"Mezokurtyczny\"),\n  \"Warto≈õƒá kurtozy\" = round(c(kurtoza_lepto, kurtoza_platy, kurtoza_mezo), 3),\n  \"Interpretacja\" = c(\n    \"Wiele warto≈õci blisko ≈õredniej, ale te≈º wiƒôcej warto≈õci ekstremalnych\",\n    \"Warto≈õci roz≈Ço≈ºone bardziej r√≥wnomiernie - p≈Çaski rozk≈Çad\",\n    \"Podobny do rozk≈Çadu normalnego\"\n  )\n)\n\n# Wy≈õwietlenie tabeli\ndane_kurtozy\n\n    Typ.rozk≈Çadu Warto≈õƒá.kurtozy\n1 Leptokurtyczny            7.39\n2 Platykurtyczny            1.85\n3  Mezokurtyczny            2.25\n                                                          Interpretacja\n1 Wiele warto≈õci blisko ≈õredniej, ale te≈º wiƒôcej warto≈õci ekstremalnych\n2             Warto≈õci roz≈Ço≈ºone bardziej r√≥wnomiernie - p≈Çaski rozk≈Çad\n3                                        Podobny do rozk≈Çadu normalnego\n\n\n\n\nWizualizacje Poziom√≥w Kurtozy\n\n# Tworzymy ramkƒô danych dla wszystkich zestaw√≥w\ndf_kurtoza &lt;- rbind(\n  data.frame(wartosc = dane_leptokurtyczne, typ = \"Leptokurtyczny (K &gt; 3)\", \n             kurtoza = round(kurtoza_lepto, 2)),\n  data.frame(wartosc = dane_platykurtyczne, typ = \"Platykurtyczny (K &lt; 3)\", \n             kurtoza = round(kurtoza_platy, 2)),\n  data.frame(wartosc = dane_mezokurtyczne, typ = \"Mezokurtyczny (K ‚âà 3)\", \n             kurtoza = round(kurtoza_mezo, 2))\n)\n\n# Histogramy dla trzech typ√≥w kurtozy\np5 &lt;- ggplot(df_kurtoza, aes(x = wartosc)) +\n  geom_histogram(bins = 15, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~typ, scales = \"free_y\") +\n  geom_text(data = unique(df_kurtoza[, c(\"typ\", \"kurtoza\")]),\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtoza)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histogramy pokazujƒÖce r√≥≈ºne poziomy kurtozy\",\n    x = \"Warto≈õƒá\",\n    y = \"Czƒôsto≈õƒá\"\n  ) +\n  theme_minimal()\n\n# Wykresy pude≈Çkowe\np6 &lt;- ggplot(df_kurtoza, aes(x = typ, y = wartosc, fill = typ)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"lightgreen\", \"lightsalmon\", \"skyblue\")) +\n  labs(\n    title = \"Wykresy pude≈Çkowe dla r√≥≈ºnych poziom√≥w kurtozy\",\n    x = \"Typ rozk≈Çadu\",\n    y = \"Warto≈õƒá\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Wy≈õwietlenie wykres√≥w\ngrid.arrange(p5, p6, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nPrzyk≈Çad: Analiza G≈Çosowa≈Ñ Parlamentarnych\n\n# Generujemy trzy zestawy danych odzwierciedlajƒÖce r√≥≈ºne poziomy kurtozy\nset.seed(456)\n\n# 1. Rozk≈Çad leptokurtyczny - typowy dla g≈Çosowa≈Ñ z silnƒÖ dyscyplinƒÖ partyjnƒÖ\nglosowania_lepto &lt;- c(\n  rnorm(150, mean = 75, sd = 3),  # Wiƒôkszo≈õƒá g≈Çosowa≈Ñ z wysokƒÖ zgodno≈õciƒÖ\n  c(20, 25, 30, 35, 40, 95, 96, 97, 98, 99)  # Kilka g≈Çosowa≈Ñ odstajƒÖcych\n)\n\n# 2. Rozk≈Çad platykurtyczny - typowy dla g≈Çosowa≈Ñ kontrowersyjnych\nglosowania_platy &lt;- c(\n  runif(80, min = 40, max = 60),  # G≈Çosowania z umiarkowanƒÖ zgodno≈õciƒÖ\n  runif(80, min = 60, max = 80)   # G≈Çosowania z wy≈ºszƒÖ zgodno≈õciƒÖ\n)\n\n# 3. Rozk≈Çad mezokurtyczny - typowy dla normalnych g≈Çosowa≈Ñ\nglosowania_mezo &lt;- rnorm(160, mean = 65, sd = 10)\n\n# Tworzymy ramkƒô danych\ndf_glosowania &lt;- rbind(\n  data.frame(zgodnosc = glosowania_lepto, typ_ustawy = \"Ustawy A: Leptokurtyczne\"),\n  data.frame(zgodnosc = glosowania_platy, typ_ustawy = \"Ustawy B: Platykurtyczne\"),\n  data.frame(zgodnosc = glosowania_mezo, typ_ustawy = \"Ustawy C: Mezokurtyczne\")\n)\n\n# Obliczamy kurtozƒô dla ka≈ºdego typu ustaw\nkurtozy_ustaw &lt;- df_glosowania %&gt;%\n  group_by(typ_ustawy) %&gt;%\n  summarise(kurtoza = round(kurtosis(zgodnosc), 2))\n\n# Histogram zgodno≈õci g≈Çosowa≈Ñ\np7 &lt;- ggplot(df_glosowania, aes(x = zgodnosc)) +\n  geom_histogram(bins = 20, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~typ_ustawy, ncol = 1) +\n  geom_text(data = kurtozy_ustaw,\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtoza)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Zgodno≈õƒá g≈Çosowa≈Ñ dla r√≥≈ºnych typ√≥w ustaw\",\n    subtitle = \"Pokazuje trzy poziomy kurtozy\",\n    x = \"Wska≈∫nik zgodno≈õci g≈Çosowa≈Ñ (%)\",\n    y = \"Liczba g≈Çosowa≈Ñ\"\n  ) +\n  theme_minimal()\n\n# Wykres pude≈Çkowy\np8 &lt;- ggplot(df_glosowania, aes(x = typ_ustawy, y = zgodnosc, fill = typ_ustawy)) +\n  geom_boxplot() +\n  labs(\n    title = \"Por√≥wnanie rozk≈Çad√≥w zgodno≈õci g≈Çosowa≈Ñ\",\n    x = \"Typ ustawy\",\n    y = \"Wska≈∫nik zgodno≈õci g≈Çosowa≈Ñ (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Wy≈õwietlenie wykres√≥w\ngrid.arrange(p7, p8, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nPrzewodnik Interpretacji\n\nLeptokurtyczny (K &gt; 3): ‚ÄúWysmuk≈Çy‚Äù rozk≈Çad z ciƒô≈ºkimi ogonami - wiƒôcej warto≈õci skrajnych ni≈º w rozk≈Çadzie normalnym\nPlatykurtyczny (K &lt; 3): ‚ÄúP≈Çaski‚Äù rozk≈Çad - mniej warto≈õci skrajnych ni≈º w rozk≈Çadzie normalnym\nMezokurtyczny (K ‚âà 3): Rozk≈Çad podobny do normalnego pod wzglƒôdem warto≈õci ekstremalnych",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ƒáwiczenie-1.-por√≥wnanie-wynagrodze≈Ñ",
    "href": "rozdzial5.html#ƒáwiczenie-1.-por√≥wnanie-wynagrodze≈Ñ",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.11 ƒÜwiczenie 1. Por√≥wnanie wynagrodze≈Ñ",
    "text": "12.11 ƒÜwiczenie 1. Por√≥wnanie wynagrodze≈Ñ\n\nDane\nMamy dane o wynagrodzeniach (w tysiƒÖcach euro) z dw√≥ch ma≈Çych firm europejskich:\n\n\n\nIndex\nFirma X\nFirma Y\n\n\n\n\n1\n2\n3\n\n\n2\n2\n3\n\n\n3\n2\n4\n\n\n4\n3\n4\n\n\n5\n3\n4\n\n\n6\n3\n4\n\n\n7\n3\n4\n\n\n8\n3\n4\n\n\n9\n3\n5\n\n\n10\n4\n5\n\n\n11\n4\n5\n\n\n12\n4\n5\n\n\n13\n4\n5\n\n\n14\n4\n5\n\n\n15\n5\n6\n\n\n16\n5\n6\n\n\n17\n5\n6\n\n\n18\n5\n7\n\n\n19\n20\n7\n\n\n20\n35\n8\n\n\n\n\n\nMiary tendencji centralnej\n\n≈örednia arytmetyczna\n≈örednia arytmetyczna to suma wszystkich warto≈õci podzielona przez ich liczbƒô.\nWz√≥r: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\nMo≈ºna te≈º zapisaƒá ten wz√≥r w postaci:\n\\bar{x} = \\frac{\\sum_{i=1}^{k} x_i f_i}{n}\ngdzie f_i to czƒôsto≈õƒá bezwzglƒôdna (liczba wystƒÖpie≈Ñ, waga bezwzglƒôdna) i-tej warto≈õci, a k to liczba r√≥≈ºnych warto≈õci cechy (liczba warto≈õci wyr√≥≈ºnionych).\nZ u≈ºyciem czƒôsto≈õci wzglƒôdnych:\n\\bar{x} = \\sum_{i=1}^{k} x_i p_i\ngdzie p_i to czƒôsto≈õƒá wzglƒôdna (frakcja, waga znormalizowana) i-tej warto≈õci, a k to liczba r√≥≈ºnych warto≈õci cechy (liczba warto≈õci wyr√≥≈ºnionych).\n\nObliczenia rƒôczne dla Firmy X\n\n\n\nWarto≈õƒá (x_i)\nCzƒôsto≈õƒá (f_i)\nx_i \\cdot f_i\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nSuma\nn = 20\nSuma = 119\n\n\n\n\\bar{x} = \\frac{119}{20} = 5,95\n\n\nObliczenia rƒôczne dla Firmy Y\n\n\n\nWarto≈õƒá (x_i)\nCzƒôsto≈õƒá (f_i)\nx_i \\cdot f_i\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nSuma\nn = 20\nSuma = 100\n\n\n\n\\bar{y} = \\frac{100}{20} = 5\n\n\nWeryfikacja w R\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\nMediana\nMediana to warto≈õƒá ≈õrodkowa w uporzƒÖdkowanym zbiorze danych.\n\nObliczenia rƒôczne dla Firmy X\nUporzƒÖdkowane dane: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\nn = 20 (parzyste), wiƒôc bierzemy ≈õredniƒÖ z 10. i 11. warto≈õci:\nMediana = \\frac{4 + 4}{2} = 4\n\n\nObliczenia rƒôczne dla Firmy Y\nUporzƒÖdkowane dane: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\nn = 20 (parzyste), wiƒôc bierzemy ≈õredniƒÖ z 10. i 11. warto≈õci:\nMediana = \\frac{5 + 5}{2} = 5\n\n\nWeryfikacja w R\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\nDominanta (moda)\nDominanta to najczƒô≈õciej wystƒôpujƒÖca warto≈õƒá w zbiorze danych.\nDla Firmy X dominanta wynosi 3 (wystƒôpuje 6 razy). Dla Firmy Y sƒÖ dwie dominanty: 4 i 5 (obie wystƒôpujƒÖ 6 razy).\n\n# Funkcja do obliczania dominanty\nznajdz_dominante &lt;- function(x) {\n  unikalne_x &lt;- unique(x)\n  unikalne_x[which.max(tabulate(match(x, unikalne_x)))]\n}\n\nznajdz_dominante(X)\n\n[1] 3\n\nznajdz_dominante(Y)\n\n[1] 4\n\n\n\n\n\nMiary rozproszenia\n\nWariancja\nWariancja mierzy ≈õrednie kwadratowe odchylenie od ≈õredniej.\nWz√≥r: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\nPoprawka Bessela jest stosowana przy obliczaniu wariancji z pr√≥by, aby uzyskaƒá nieobciƒÖ≈ºony estymator wariancji populacji. W standardowym wzorze na wariancjƒô z pr√≥by dzielimy przez (n-1) zamiast przez n.\nModyfikacje wzoru dla danych pogrupowanych (szereg czƒôsto≈õci):\nMo≈ºna te≈º zapisaƒá ten wz√≥r w postaci:\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{k} f_i(x_i - \\bar{x})^2\ngdzie f_i to czƒôsto≈õƒá bezwzglƒôdna (liczba wystƒÖpie≈Ñ) i-tej warto≈õci.\nGdy w obliczeniach stosujemy czƒôsto≈õci wzglƒôdne p = f_i/n, gdzie:\n\nf_i to czƒôsto≈õƒá (liczba wystƒÖpie≈Ñ)\nn to ca≈Çkowita liczebno≈õƒá pr√≥by\n\nWz√≥r na wariancjƒô z uwzglƒôdnieniem poprawki Bessela przyjmuje postaƒá:\ns^2 = \\frac{n}{n-1} \\sum_{i=1}^{k} p_i(x_i - \\bar{x})^2\ngdzie:\n\ns^2 to wariancja z pr√≥by\nn to liczebno≈õƒá pr√≥by\np_i = f_i/n to czƒôsto≈õƒá wzglƒôdna i-tej warto≈õci\nx_i to i-ta warto≈õƒá cechy\n\\bar{x} to ≈õrednia arytmetyczna\nk to liczba r√≥≈ºnych warto≈õci cechy\n\nKluczowe jest to, ≈ºe przy stosowaniu czƒôsto≈õci wzglƒôdnych mno≈ºymy ca≈Çe wyra≈ºenie przez czynnik \\frac{n}{n-1}, kt√≥ry wprowadza poprawkƒô Bessela.\n\nObliczenia rƒôczne dla Firmy X\n\n\n\n\n\n\n\n\n\n\nx_i\nf_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\nf_i(x_i - \\bar{x})^2\n\n\n\n\n2\n3\n-3,95\n15,6025\n46,8075\n\n\n3\n6\n-2,95\n8,7025\n52,215\n\n\n4\n5\n-1,95\n3,8025\n19,0125\n\n\n5\n4\n-0,95\n0,9025\n3,61\n\n\n20\n1\n14,05\n197,4025\n197,4025\n\n\n35\n1\n29,05\n843,9025\n843,9025\n\n\nSuma\n20\n\n\n1162,95\n\n\n\ns^2 = \\frac{1162,95}{19} = 61,21\n\n\nObliczenia rƒôczne dla Firmy Y\n\n\n\n\n\n\n\n\n\n\ny_i\nf_i\ny_i - \\bar{x}\n(y_i - \\bar{y})^2\nf_i(y_i - \\bar{y})^2\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nSuma\n20\n\n\n34\n\n\n\ns^2 = \\frac{34}{19} = 1,79\n\n\nWeryfikacja w R\n\nvar(X)\n\n[1] 61.2\n\nvar(Y)\n\n[1] 1.79\n\n\n\n\n\nOdchylenie standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji.\nWz√≥r: s = \\sqrt{s^2}\n\nDla Firmy X: s = \\sqrt{61,21} = 7,82\nDla Firmy Y: s = \\sqrt{1,79} = 1,34\n\n\nWeryfikacja w R\n\nsd(X)\n\n[1] 7.82\n\nsd(Y)\n\n[1] 1.34\n\n\n\n\n\n\nKwartyle\nKwartyle dzielƒÖ zbi√≥r danych na cztery r√≥wne czƒô≈õci.\n\nObliczenia rƒôczne dla Firmy X\nUporzƒÖdkowane dane: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 3\nQ2 (50. percentyl, mediana): 4\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 5\n\n\n\nObliczenia rƒôczne dla Firmy Y\nUporzƒÖdkowane dane: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 4\nQ2 (50. percentyl, mediana): 5\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 6\n\n\n\nWeryfikacja w R\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\nIQR\n\nIQR_x = 5 - 3 = 2\nIQR_y = 6 - 4 = 2\n\n\n\n\nWykres pude≈Çkowy Tukeya\nWykres pude≈Çkowy Tukeya wizualnie przedstawia rozk≈Çad danych na podstawie kwartyli. U≈ºyjemy biblioteki ggplot2 do stworzenia wykresu.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Przygotowanie danych\ndane &lt;- data.frame(\n  Firma = rep(c(\"X\", \"Y\"), each = 20),\n  Wynagrodzenie = c(X, Y)\n)\n\n# Tworzenie wykresu pude≈Çkowego\nggplot(dane, aes(x = Firma, y = Wynagrodzenie, fill = Firma)) +\n  geom_boxplot() +\n  labs(title = \"Rozk≈Çad wynagrodze≈Ñ w firmach X i Y\",\n       x = \"Firma\",\n       y = \"Wynagrodzenie (tysiƒÖce euro)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n# Tworzenie wykresu pude≈Çkowego\nggplot(dane, aes(x = Firma, y = Wynagrodzenie, fill = Firma)) +\n  geom_boxplot(outliers = F) +\n  labs(title = \"Rozk≈Çad wynagrodze≈Ñ w firmach X i Y\",\n       x = \"Firma\",\n       y = \"Wynagrodzenie (tysiƒÖce euro)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\nInterpretacja wykresu pude≈Çkowego\n\nPude≈Çko reprezentuje rozstƒôp miƒôdzykwartylowy (IQR) od Q1 do Q3.\nLinia wewnƒÖtrz pude≈Çka to mediana (Q2).\nWƒÖsy rozciƒÖgajƒÖ siƒô do najmniejszych i najwiƒôkszych warto≈õci w granicach 1,5 * IQR.\nPunkty poza wƒÖsami sƒÖ uznawane za warto≈õci odstajƒÖce.\n\n\n\n\nPor√≥wnanie wynik√≥w\n\n\n\nMiara\nFirma X\nFirma Y\n\n\n\n\n≈örednia\n5,95\n5,00\n\n\nMediana\n4\n5\n\n\nDominanta\n3\n4 i 5\n\n\nWariancja\n61,21\n1,79\n\n\nOdchylenie standard.\n7,82\n1,34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\nKluczowe obserwacje:\n\nTendencja centralna: Firma X ma wy≈ºszƒÖ ≈õredniƒÖ, ale ni≈ºszƒÖ medianƒô ni≈º Firma Y, co wskazuje na prawostronnie sko≈õny rozk≈Çad dla Firmy X.\n\nRozproszenie: Firma X wykazuje znacznie wy≈ºszƒÖ wariancjƒô i odchylenie standardowe, sugerujƒÖc wiƒôksze dysproporcje w wynagrodzeniach.\nKszta≈Çt rozk≈Çadu: Wynagrodzenia w Firmie Y sƒÖ bardziej skupione, podczas gdy Firma X ma warto≈õci ekstremalne (potencjalne warto≈õci odstajƒÖce), kt√≥re znaczƒÖco wp≈ÇywajƒÖ na jej ≈õredniƒÖ i wariancjƒô.\nKwartyle: Rozstƒôp miƒôdzykwartylowy (Q3 - Q1) Firmy Y jest nieznacznie wiƒôkszy, ale jej og√≥lny zakres jest znacznie mniejszy ni≈º Firmy X.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ƒáwiczenie-2.-por√≥wnanie-zmienno≈õci-wielko≈õci-okrƒôg√≥w-wyborczych-miƒôdzy-krajami",
    "href": "rozdzial5.html#ƒáwiczenie-2.-por√≥wnanie-zmienno≈õci-wielko≈õci-okrƒôg√≥w-wyborczych-miƒôdzy-krajami",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.12 ƒÜwiczenie 2. Por√≥wnanie Zmienno≈õci Wielko≈õci Okrƒôg√≥w Wyborczych Miƒôdzy Krajami",
    "text": "12.12 ƒÜwiczenie 2. Por√≥wnanie Zmienno≈õci Wielko≈õci Okrƒôg√≥w Wyborczych Miƒôdzy Krajami\n\nDane\nMamy dane o wielko≈õci okrƒôg√≥w wyborczych z dw√≥ch kraj√≥w:\n\nx &lt;- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)  # Kraj wysoka zmienno≈õƒá\ny &lt;- c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13)  # Kraj niska zmienno≈õƒá\n\nkable(data.frame(\n  \"Kraj X (Wysoka zm.)\" = x,\n  \"Kraj Y (Niska zm.)\" = y\n))\n\n\n\n\nKraj.X..Wysoka.zm..\nKraj.Y..Niska.zm..\n\n\n\n\n1\n8\n\n\n3\n9\n\n\n5\n9\n\n\n7\n10\n\n\n9\n10\n\n\n11\n11\n\n\n13\n11\n\n\n15\n12\n\n\n17\n12\n\n\n19\n13\n\n\n\n\n\n\n\nMiary Tendencji Centralnej\n\n≈örednia Arytmetyczna\nWz√≥r: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\nObliczenia dla Kraju X\n\n\n\nElement\nWarto≈õƒá\n\n\n\n\n1\n1\n\n\n2\n3\n\n\n3\n5\n\n\n4\n7\n\n\n5\n9\n\n\n6\n11\n\n\n7\n13\n\n\n8\n15\n\n\n9\n17\n\n\n10\n19\n\n\nSuma\n100\n\n\n\n\\bar{x} = \\frac{100}{10} = 10\n\nmean_x &lt;- mean(x)\nc(\"Rƒôcznie\" = 10, \"R\" = mean_x)\n\nRƒôcznie       R \n     10      10 \n\n\n\n\nObliczenia dla Kraju Y\n\n\n\nElement\nWarto≈õƒá\n\n\n\n\n1\n8\n\n\n2\n9\n\n\n3\n9\n\n\n4\n10\n\n\n5\n10\n\n\n6\n11\n\n\n7\n11\n\n\n8\n12\n\n\n9\n12\n\n\n10\n13\n\n\nSuma\n105\n\n\n\n\\bar{y} = \\frac{105}{10} = 10,5\n\nmean_y &lt;- mean(y)\nc(\"Rƒôcznie\" = 10.5, \"R\" = mean_y)\n\nRƒôcznie       R \n   10.5    10.5 \n\n\n\n\n\nMediana\nMediana to warto≈õƒá ≈õrodkowa w uporzƒÖdkowanym zbiorze danych.\n\nObliczenia dla Kraju X\nUporzƒÖdkowane dane: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nDla n = 10 (parzysta liczba obserwacji): Pozycje ≈õrodkowe: 5 i 6 Warto≈õci ≈õrodkowe: 9 i 11\nMediana = \\frac{9 + 11}{2} = 10\n\nmedian_x &lt;- median(x)\nc(\"Rƒôcznie\" = 10, \"R\" = median_x)\n\nRƒôcznie       R \n     10      10 \n\n\n\n\nObliczenia dla Kraju Y\nUporzƒÖdkowane dane: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nDla n = 10 (parzysta liczba obserwacji): Pozycje ≈õrodkowe: 5 i 6 Warto≈õci ≈õrodkowe: 10 i 11\nMediana = \\frac{10 + 11}{2} = 10,5\n\nmedian_y &lt;- median(y)\nc(\"Rƒôcznie\" = 10.5, \"R\" = median_y)\n\nRƒôcznie       R \n   10.5    10.5 \n\n\n\n\n\nDominanta\n\nObliczenia dla Kraju X\n\n\n\nWarto≈õƒá\nCzƒôsto≈õƒá\n\n\n\n\n1\n1\n\n\n3\n1\n\n\n5\n1\n\n\n7\n1\n\n\n9\n1\n\n\n11\n1\n\n\n13\n1\n\n\n15\n1\n\n\n17\n1\n\n\n19\n1\n\n\n\nWniosek: Brak dominanty (wszystkie warto≈õci wystƒôpujƒÖ jednokrotnie)\n\n\nObliczenia dla Kraju Y\n\n\n\nWarto≈õƒá\nCzƒôsto≈õƒá\n\n\n\n\n8\n1\n\n\n9\n2\n\n\n10\n2\n\n\n11\n2\n\n\n12\n2\n\n\n13\n1\n\n\n\nWniosek: Cztery dominanty: 9, 10, 11, 12 (ka≈ºda wystƒôpuje dwukrotnie)\n\n# Tabele czƒôsto≈õci\ntable_x &lt;- table(x)\ntable_y &lt;- table(y)\n\nlist(\n  \"Kraj X\" = table_x,\n  \"Kraj Y\" = table_y\n)\n\n$`Kraj X`\nx\n 1  3  5  7  9 11 13 15 17 19 \n 1  1  1  1  1  1  1  1  1  1 \n\n$`Kraj Y`\ny\n 8  9 10 11 12 13 \n 1  2  2  2  2  1 \n\n\n\n\n\nWariancja\nWariancja mierzy ≈õrednie kwadratowe odchylenie od ≈õredniej.\nWz√≥r: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\nObliczenia dla Kraju X\n\n\n\nx_i\n(x_i - \\bar{x})\n(x_i - \\bar{x})^2\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSuma\n\n330\n\n\n\ns^2_X = \\frac{330}{9} = 36,67\n\nvar_x &lt;- var(x)\nc(\"Rƒôcznie\" = 36.67, \"R\" = var_x)\n\nRƒôcznie       R \n  36.67   36.67 \n\n\n\n\nObliczenia dla Kraju Y\n\n\n\nx_i\n(y_i - \\bar{y})\n(y_i - \\bar{y})^2\n\n\n\n\n8\n-2,5\n6,25\n\n\n9\n-1,5\n2,25\n\n\n9\n-1,5\n2,25\n\n\n10\n-0,5\n0,25\n\n\n10\n-0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n12\n1,5\n2,25\n\n\n12\n1,5\n2,25\n\n\n13\n2,5\n6,25\n\n\nSuma\n\n22,5\n\n\n\ns^2_Y = \\frac{22,5}{9} = 2,5\n\nvar_y &lt;- var(y)\nc(\"Rƒôcznie\" = 2.5, \"R\" = var_y)\n\nRƒôcznie       R \n    2.5     2.5 \n\n\n\n\n\nOdchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji. Jest miarƒÖ zmienno≈õci wyra≈ºonƒÖ w tych samych jednostkach co dane.\nWz√≥r: s = \\sqrt{s^2}\n\nObliczenia dla Kraju X\nWykorzystujemy wcze≈õniej obliczonƒÖ wariancjƒô: s^2_X = 36,67\nObliczamy pierwiastek: s_X = \\sqrt{36,67} \\approx 6,06\n\n\n\nKrok\nObliczenie\nWynik\n\n\n\n\n1. Wariancja\ns^2_X\n36,67\n\n\n2. Pierwiastek\n\\sqrt{36,67}\n6,06\n\n\n\n\nsd_x &lt;- sd(x)\nc(\"Rƒôcznie\" = 6.06, \"R\" = sd_x)\n\nRƒôcznie       R \n  6.060   6.055 \n\n\n\n\nObliczenia dla Kraju Y\nWykorzystujemy wcze≈õniej obliczonƒÖ wariancjƒô: s^2_Y = 2,5\nObliczamy pierwiastek: s_Y = \\sqrt{2,5} \\approx 1,58\n\n\n\nKrok\nObliczenie\nWynik\n\n\n\n\n1. Wariancja\ns^2_Y\n2,5\n\n\n2. Pierwiastek\n\\sqrt{2,5}\n1,58\n\n\n\n\nsd_y &lt;- sd(y)\nc(\"Rƒôcznie\" = 1.58, \"R\" = sd_y)\n\nRƒôcznie       R \n  1.580   1.581 \n\n\nInterpretacja:\n\nKraj X: Przeciƒôtne odchylenie wielko≈õci okrƒôgu od ≈õredniej wynosi oko≈Ço 6 mandat√≥w\nKraj Y: Przeciƒôtne odchylenie wielko≈õci okrƒôgu od ≈õredniej wynosi oko≈Ço 1,6 mandatu\n\n\n\n\n\nWsp√≥≈Çczynnik Zmienno≈õci (CV)\nWsp√≥≈Çczynnik zmienno≈õci to stosunek odchylenia standardowego do ≈õredniej, wyra≈ºony w procentach.\nWz√≥r: CV = \\frac{s}{\\bar{x}} \\times 100\\%\n\nObliczenia dla Kraju X\nCV_X = \\frac{6,06}{10} \\times 100\\% = 60,6\\%\n\n\n\nSk≈Çadowa\nWarto≈õƒá\n\n\n\n\nOdchylenie standardowe (s)\n6,06\n\n\n≈örednia (\\bar{x})\n10\n\n\nCV\n60,6%\n\n\n\n\ncv_x &lt;- sd(x) / mean(x) * 100\nc(\"Rƒôcznie\" = 60.6, \"R\" = cv_x)\n\nRƒôcznie       R \n  60.60   60.55 \n\n\n\n\nObliczenia dla Kraju Y\nCV_Y = \\frac{1,58}{10,5} \\times 100\\% = 15,0\\%\n\n\n\nSk≈Çadowa\nWarto≈õƒá\n\n\n\n\nOdchylenie standardowe (s)\n1,58\n\n\n≈örednia (\\bar{x})\n10,5\n\n\nCV\n15,0%\n\n\n\n\ncv_y &lt;- sd(y) / mean(y) * 100\nc(\"Rƒôcznie\" = 15.0, \"R\" = cv_y)\n\nRƒôcznie       R \n  15.00   15.06 \n\n\n\n\n\nKwartyle i Rozstƒôp Miƒôdzykwartylowy (IQR)\n\nMetody obliczania kwartyli\nIstniejƒÖ r√≥≈ºne metody obliczania kwartyli. W naszych obliczeniach rƒôcznych zastosujemy metodƒô wy≈ÇƒÖczajƒÖcƒÖ medianƒô:\n\nDzielimy szereg na dwie czƒô≈õci wzglƒôdem mediany\nMediana nie jest uwzglƒôdniana w obliczeniach kwartyli\nDla ka≈ºdej czƒô≈õci obliczamy jej medianƒô - bƒôdzie to odpowiednio Q1 i Q3\n\n\n\nObliczenia dla Kraju X\nUporzƒÖdkowane dane: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMediana = 10 (nie uwzglƒôdniamy w obliczeniach kwartyli)\nDolna po≈Çowa: 1, 3, 5, 7, 9 Q1 = mediana dolnej po≈Çowy = 5\nG√≥rna po≈Çowa: 11, 13, 15, 17, 19 Q3 = mediana g√≥rnej po≈Çowy = 15\nIQR = Q3 - Q1 = 15 - 5 = 10\n\n\nObliczenia dla Kraju Y\nUporzƒÖdkowane dane: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMediana = 10.5 (nie uwzglƒôdniamy w obliczeniach kwartyli)\nDolna po≈Çowa: 8, 9, 9, 10, 10 Q1 = mediana dolnej po≈Çowy = 9\nG√≥rna po≈Çowa: 11, 11, 12, 12, 13 Q3 = mediana g√≥rnej po≈Çowy = 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n# Por√≥wnanie r√≥≈ºnych metod obliczania kwartyli w R\nmethods_comparison &lt;- data.frame(\n  Metoda = c(\"Rƒôcznie (bez mediany)\", \n             \"R type=1\", \"R type=2\", \"R type=7 (domy≈õlna)\"),\n  \"Q1 Kraj X\" = c(5, \n                  quantile(x, 0.25, type=1),\n                  quantile(x, 0.25, type=2),\n                  quantile(x, 0.25, type=7)),\n  \"Q3 Kraj X\" = c(15,\n                  quantile(x, 0.75, type=1),\n                  quantile(x, 0.75, type=2),\n                  quantile(x, 0.75, type=7)),\n  \"Q1 Kraj Y\" = c(9,\n                  quantile(y, 0.25, type=1),\n                  quantile(y, 0.25, type=2),\n                  quantile(y, 0.25, type=7)),\n  \"Q3 Kraj Y\" = c(12,\n                  quantile(y, 0.75, type=1),\n                  quantile(y, 0.75, type=2),\n                  quantile(y, 0.75, type=7))\n)\n\nkable(methods_comparison, digits = 2,\n      caption = \"Por√≥wnanie r√≥≈ºnych metod obliczania kwartyli\")\n\n\nPor√≥wnanie r√≥≈ºnych metod obliczania kwartyli\n\n\nMetoda\nQ1.Kraj.X\nQ3.Kraj.X\nQ1.Kraj.Y\nQ3.Kraj.Y\n\n\n\n\nRƒôcznie (bez mediany)\n5.0\n15.0\n9.00\n12.00\n\n\nR type=1\n5.0\n15.0\n9.00\n12.00\n\n\nR type=2\n5.0\n15.0\n9.00\n12.00\n\n\nR type=7 (domy≈õlna)\n5.5\n14.5\n9.25\n11.75\n\n\n\n\n\n\n\nWyja≈õnienie r√≥≈ºnic w metodach obliczania kwartyli\n\nMetoda rƒôczna (bez mediany):\n\nDzieli dane na dwie czƒô≈õci\nNie uwzglƒôdnia mediany\nZnajduje medianƒô ka≈ºdej czƒô≈õci\n\nR type=1:\n\nMetoda pierwsza w R\nU≈ºywa pozycji ca≈Çkowitych\nNie interpoluje\n\nR type=2:\n\nMetoda druga w R\nU≈ºywa pozycji ca≈Çkowitych\nInterpoluje gdy pozycja nie jest ca≈Çkowita\n\nR type=7 (domy≈õlna):\n\nMetoda domy≈õlna w R\nU≈ºywa quantile()[5] z SAS\nInterpoluje wed≈Çug metody opisanej przez Hyndmana i Fana\n\n\n\n\n\nPor√≥wnanie Wynik√≥w\n\nsummary_df &lt;- data.frame(\n  Miara = c(\"≈örednia\", \"Mediana\", \"Dominanta\", \"Rozstƒôp\", \"Wariancja\", \n            \"Odch. Stand.\", \"Q1\", \"Q3\", \"IQR\", \"CV (%)\"),\n  \"Kraj X\" = c(10, 10, \"brak\", 18, 36.67, 6.06, 5, 15, 10, 60.6),\n  \"Kraj Y\" = c(10.5, 10.5, \"9,10,11,12\", 5, 2.5, 1.58, 9, 12, 3, 15.0)\n)\n\nkable(summary_df, \n      caption = \"Zestawienie wszystkich miar statystycznych\",\n      align = c('l', 'r', 'r'))\n\n\nZestawienie wszystkich miar statystycznych\n\n\nMiara\nKraj.X\nKraj.Y\n\n\n\n\n≈örednia\n10\n10.5\n\n\nMediana\n10\n10.5\n\n\nDominanta\nbrak\n9,10,11,12\n\n\nRozstƒôp\n18\n5\n\n\nWariancja\n36.67\n2.5\n\n\nOdch. Stand.\n6.06\n1.58\n\n\nQ1\n5\n9\n\n\nQ3\n15\n12\n\n\nIQR\n10\n3\n\n\nCV (%)\n60.6\n15\n\n\n\n\n\n\n\nPor√≥wnanie za pomocƒÖ Wykresu Pude≈Çkowego\n\ndf_long &lt;- data.frame(\n  kraj = rep(c(\"X\", \"Y\"), each = 10),\n  wielkosc = c(x, y)\n)\n\n# Wykres podstawowy\np &lt;- ggplot(df_long, aes(x = kraj, y = wielkosc, fill = kraj)) +\n  geom_boxplot(outlier.shape = NA) +  # Wy≈ÇƒÖczamy domy≈õlne punkty odstajƒÖce\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Dodajemy punkty z przezroczysto≈õciƒÖ\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\")) +\n  labs(\n    title = \"Por√≥wnanie Zmienno≈õci Wielko≈õci Okrƒôg√≥w Wyborczych\",\n    subtitle = paste(\"CV: Kraj X =\", round(cv_x, 1), \"%, Kraj Y =\", round(cv_y, 1), \"%\"),\n    x = \"Kraj\",\n    y = \"Wielko≈õƒá Okrƒôgu\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Dodajemy adnotacje z kwartylami\np + annotate(\n  \"text\", \n  x = c(1, 1, 1, 2, 2, 2), \n  y = c(max(x)+1, mean(x), min(x)-1, max(y)+1, mean(y), min(y)-1),\n  label = c(\n    paste(\"Q3 =\", quantile(x, 0.75, type=1)),\n    paste(\"M =\", median(x)),\n    paste(\"Q1 =\", quantile(x, 0.25, type=1)),\n    paste(\"Q3 =\", quantile(y, 0.75, type=1)),\n    paste(\"M =\", median(y)),\n    paste(\"Q1 =\", quantile(y, 0.25, type=1))\n  ),\n  size = 3\n)\n\n\n\n\n\n\n\n\n\n\nUwagi Metodologiczne\n\nObliczenia kwartyli:\n\nZastosowana metoda wy≈ÇƒÖczajƒÖca medianƒô mo≈ºe dawaƒá inne wyniki ni≈º domy≈õlne funkcje R\nR√≥≈ºnice w metodach obliczeniowych nie wp≈ÇywajƒÖ na og√≥lne wnioski\nWarto zawsze zaznaczyƒá stosowanƒÖ metodƒô w raportach\n\nWizualizacja:\n\nWykres pude≈Çkowy skutecznie pokazuje r√≥≈ºnice w rozk≈Çadach\nDodatkowe punkty pokazujƒÖ rzeczywiste warto≈õci\nAdnotacje u≈ÇatwiajƒÖ interpretacjƒô\n\n\n\n\nPodsumowanie\n\nPor√≥wnanie Miar Statystycznych\n\n\n\nMiara\nKraj X\nKraj Y\nR√≥≈ºnica wzglƒôdna\n\n\n\n\n≈örednia\n10,0\n10,5\nPodobna\n\n\nMediana\n10,0\n10,5\nPodobna\n\n\nDominanta\nBrak\nWielokrotna (9,10,11,12)\n-\n\n\nRozstƒôp\n18\n5\n3,6√ó wiƒôkszy w X\n\n\nWariancja\n36,67\n2,5\n14,7√ó wiƒôksza w X\n\n\nIQR\n10\n3\n3,3√ó wiƒôkszy w X\n\n\nCV\n60,6%\n15,0%\n4,0√ó wiƒôkszy w X\n\n\n\n\n\nCharakterystyka Rozk≈Çad√≥w\nKraj X:\n\nRozk≈Çad r√≥wnomierny\nBrak dominujƒÖcej wielko≈õci okrƒôgu (brak dominanty)\nSzeroki zakres: od 1 do 19 mandat√≥w\nWysoka zmienno≈õƒá (CV = 60,6%)\nR√≥wnomierne roz≈Ço≈ºenie warto≈õci w zakresie\n\nKraj Y:\n\nRozk≈Çad skupiony\nWiele typowych wielko≈õci (cztery dominanty)\nWƒÖski zakres: od 8 do 13 mandat√≥w\nNiska zmienno≈õƒá (CV = 15,0%)\nWarto≈õci skoncentrowane wok√≥≈Ç ≈õredniej\n\n\n\nInterpretacja Wykresu Pude≈Çkowego\nWizualizacja w formie wykresu pude≈Çkowego pokazuje:\nElementy Struktury:\n\nPude≈Çko: Pokazuje rozstƒôp miƒôdzykwartylowy (IQR)\nDolna krawƒôd≈∫: Pierwszy kwartyl (Q1)\nG√≥rna krawƒôd≈∫: Trzeci kwartyl (Q3)\nLinia wewnƒôtrzna: Mediana (Q2)\nWƒÖsy: RozciƒÖgajƒÖ siƒô do ¬±1,5 IQR - Punkty: Pojedyncze wielko≈õci okrƒôg√≥w\n\nG≈Ç√≥wne Wnioski Wizualne:\n\nRozmiar Pude≈Çka:\n\n\nKraj X: Du≈ºe pude≈Çko wskazuje na szeroki rozrzut ≈õrodkowych 50%\nKraj Y: Ma≈Çe pude≈Çko pokazuje skupienie warto≈õci ≈õrodkowych\n\n\nD≈Çugo≈õƒá WƒÖs√≥w:\n\nKraj X: D≈Çugie wƒÖsy wskazujƒÖ na szeroki rozk≈Çad ca≈Çkowity\nKraj Y: Kr√≥tkie wƒÖsy pokazujƒÖ ograniczony rozrzut\n\nRozk≈Çad Punkt√≥w:\n\nKraj X: Punkty szeroko rozproszone\nKraj Y: Punkty gƒôsto skupione\n\n\n\n\nKluczowe Obserwacje\n\nTendencja Centralna:\n\nPodobne ≈õrednie wielko≈õci okrƒôg√≥w\nR√≥≈ºne wzorce rozk≈Çadu\nOdmienne podej≈õcia do standaryzacji\n\nMiary Zmienno≈õci:\n\nWszystkie miary pokazujƒÖ 3-15 razy wiƒôkszƒÖ zmienno≈õƒá w Kraju X\nSp√≥jny wzorzec w r√≥≈ºnych miarach statystycznych\nSystematyczna r√≥≈ºnica w projekcie okrƒôg√≥w\n\nProjekt Systemu:\n\nKraj X: Elastyczne, zr√≥≈ºnicowane podej≈õcie\nKraj Y: Ustandaryzowane, jednolite podej≈õcie\nR√≥≈ºne filozoficzne podej≈õcia do reprezentacji\n\nImplikacje Reprezentatywno≈õci:\n\nKraj X: Zmienna proporcja wyborc√≥w do przedstawicieli\nKraj Y: Bardziej sp√≥jne poziomy reprezentacji\nR√≥≈ºne podej≈õcia do reprezentacji demokratycznej\n\n\nAnaliza ta pokazuje fundamentalne r√≥≈ºnice w projektowaniu system√≥w wyborczych miƒôdzy dwoma krajami, gdzie Kraj X przyjmuje bardziej zr√≥≈ºnicowane podej≈õcie, a Kraj Y utrzymuje wiƒôkszƒÖ jednolito≈õƒá w wielko≈õci okrƒôg√≥w wyborczych.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ƒáwiczenie-3.-voter-participation-and-economic-prosperity",
    "href": "rozdzial5.html#ƒáwiczenie-3.-voter-participation-and-economic-prosperity",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.13 ƒÜwiczenie 3. Voter Participation and Economic Prosperity",
    "text": "12.13 ƒÜwiczenie 3. Voter Participation and Economic Prosperity\nAnaliza zwiƒÖzku miƒôdzy dobrobytem ekonomicznym a frekwencjƒÖ wyborczƒÖ w dzielnicach Amsterdamu na podstawie danych z wybor√≥w samorzƒÖdowych 2022.\n\nDane\nPr√≥ba obejmuje piƒôƒá reprezentatywnych dzielnic:\n\n\n\nDzielnica\nDoch√≥d (tys. ‚Ç¨)\nFrekwencja (%)\n\n\n\n\nA\n50\n60\n\n\nB\n45\n56\n\n\nC\n56\n70\n\n\nD\n40\n50\n\n\nE\n60\n75\n\n\n\n\n# Wczytanie bibliotek\nlibrary(tidyverse)\n\n# Utworzenie zbioru danych\ndane &lt;- data.frame(\n  dzielnica = LETTERS[1:5],\n  dochod = c(50, 45, 56, 40, 60),\n  frekwencja = c(60, 56, 70, 50, 75)\n)\n\n\n\nCzƒô≈õƒá 1: Statystyki opisowe\n\n# Statystyki dla dochodu\nmean(dane$dochod)\n\n[1] 50.2\n\nmedian(dane$dochod)\n\n[1] 50\n\nsd(dane$dochod)\n\n[1] 8.075\n\nrange(dane$dochod)\n\n[1] 40 60\n\n# Statystyki dla frekwencji\nmean(dane$frekwencja)\n\n[1] 62.2\n\nmedian(dane$frekwencja)\n\n[1] 60\n\nsd(dane$frekwencja)\n\n[1] 10.21\n\nrange(dane$frekwencja)\n\n[1] 50 75\n\n\n\n\nCzƒô≈õƒá 2: Analiza korelacji\n\n# Korelacja Pearsona\ncor.test(dane$dochod, dane$frekwencja)\n\n\n    Pearson's product-moment correlation\n\ndata:  dane$dochod and dane$frekwencja\nt = 16, df = 3, p-value = 0.0005\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9117 0.9996\nsample estimates:\n   cor \n0.9942 \n\n\n\n\nCzƒô≈õƒá 3: Model regresji OLS\n\n# Dopasowanie modelu OLS\nmodel &lt;- lm(frekwencja ~ dochod, data = dane)\n\n# Podsumowanie modelu\nsummary(model)\n\n\nCall:\nlm(formula = frekwencja ~ dochod, data = dane)\n\nResiduals:\n     1      2      3      4      5 \n-1.949  0.336  0.510  0.620  0.482 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.8965     3.9673   -0.23  0.83575    \ndochod        1.2569     0.0782   16.07  0.00052 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.26 on 3 degrees of freedom\nMultiple R-squared:  0.989, Adjusted R-squared:  0.985 \nF-statistic:  258 on 1 and 3 DF,  p-value: 0.000524\n\n\n\n\nWizualizacja\n\n# Wykres rozrzutu z liniƒÖ regresji\nggplot(dane, aes(x = dochod, y = frekwencja)) +\n  geom_point(size = 4, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  geom_text(aes(label = dzielnica), vjust = -1) +\n  labs(\n    title = \"Doch√≥d vs frekwencja wyborcza\",\n    x = \"Doch√≥d (tys. ‚Ç¨)\",\n    y = \"Frekwencja wyborcza (%)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nWnioski\nAnaliza wykaza≈Ça silny dodatni zwiƒÖzek miƒôdzy dobrobytem ekonomicznym dzielnicy a frekwencjƒÖ wyborczƒÖ. Mieszka≈Ñcy dzielnic o wy≈ºszych dochodach czƒô≈õciej uczestniczƒÖ w wyborach samorzƒÖdowych.\nUwaga: Ma≈Ça liczebno≈õƒá pr√≥by (n=5) ogranicza mo≈ºliwo≈õƒá generalizacji wynik√≥w.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#appendix-tabele-podsumowujƒÖce-typy-danych-i-odpowiednie-miary-statystyczne",
    "href": "rozdzial5.html#appendix-tabele-podsumowujƒÖce-typy-danych-i-odpowiednie-miary-statystyczne",
    "title": "12¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.14 Appendix: Tabele PodsumowujƒÖce Typy Danych i Odpowiednie Miary Statystyczne",
    "text": "12.14 Appendix: Tabele PodsumowujƒÖce Typy Danych i Odpowiednie Miary Statystyczne\n\nZalety i Wady R√≥≈ºnych Miar Statystycznych\n\nMiary Tendencji Centralnej\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\n≈örednia\n- Wykorzystuje wszystkie punkty danych- Pozwala na dalsze obliczenia statystyczne- Idealna dla danych o rozk≈Çadzie normalnym\n- Wra≈ºliwa na warto≈õci odstajƒÖce- Nieodpowiednia dla rozk≈Çad√≥w sko≈õnych- Bez znaczenia dla danych nominalnych\nInterwa≈Çowe, Ilorazowe, niekt√≥re Dyskretne, CiƒÖg≈Çe\n\n\nMediana\n- Niewra≈ºliwa na warto≈õci odstajƒÖce- Dobra dla rozk≈Çad√≥w sko≈õnych- Mo≈ºe byƒá stosowana do danych porzƒÖdkowych\n- Ignoruje rzeczywiste warto≈õci wiƒôkszo≈õci punkt√≥w danych- Mniej u≈ºyteczna do dalszych analiz statystycznych\nPorzƒÖdkowe, Interwa≈Çowe, Ilorazowe, Dyskretne, CiƒÖg≈Çe\n\n\nModa\n- Mo≈ºe byƒá stosowana do ka≈ºdego typu danych- Dobra do znajdowania najczƒôstszej kategorii\n- Mo≈ºe nie byƒá unikalna (rozk≈Çady multimodalne)- Nieprzydatna do wielu typ√≥w analiz- Ignoruje wielko≈õƒá r√≥≈ºnic miƒôdzy warto≈õciami\nWszystkie typy\n\n\n\n\n\nMiary Zmienno≈õci\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nZakres\n- Prosty do obliczenia i zrozumienia- Daje szybki obraz rozproszenia danych\n- Bardzo wra≈ºliwy na warto≈õci odstajƒÖce- Ignoruje wszystkie dane miƒôdzy ekstremami- Nieprzydatny do dalszych analiz statystycznych\nPorzƒÖdkowe, Interwa≈Çowe, Ilorazowe, Dyskretne, CiƒÖg≈Çe\n\n\nRozstƒôp miƒôdzykwartylowy (IQR)\n- Niewra≈ºliwy na warto≈õci odstajƒÖce- Dobry dla rozk≈Çad√≥w sko≈õnych\n- Ignoruje 50% danych- Mniej intuicyjny ni≈º zakres\nPorzƒÖdkowe, Interwa≈Çowe, Ilorazowe, Dyskretne, CiƒÖg≈Çe\n\n\nWariancja\n- Wykorzystuje wszystkie punkty danych- Podstawa wielu procedur statystycznych\n- Wra≈ºliwa na warto≈õci odstajƒÖce- Jednostki sƒÖ podniesione do kwadratu (mniej intuicyjne)\nInterwa≈Çowe, Ilorazowe, niekt√≥re Dyskretne, CiƒÖg≈Çe\n\n\nOdchylenie standardowe\n- Wykorzystuje wszystkie punkty danych- Te same jednostki co oryginalne dane- Szeroko stosowane i zrozumia≈Çe\n- Wra≈ºliwe na warto≈õci odstajƒÖce- Zak≈Çada w przybli≈ºeniu rozk≈Çad normalny dla interpretacji\nInterwa≈Çowe, Ilorazowe, niekt√≥re Dyskretne, CiƒÖg≈Çe\n\n\nWsp√≥≈Çczynnik zmienno≈õci\n- Pozwala na por√≥wnanie miƒôdzy zbiorami danych o r√≥≈ºnych jednostkach lub ≈õrednich\n- Mo≈ºe byƒá mylƒÖcy, gdy ≈õrednie sƒÖ bliskie zeru- Bez znaczenia dla danych z warto≈õciami ujemnymi\nIlorazowe, niekt√≥re Interwa≈Çowe\n\n\n\n\n\nMiary Korelacji/Asocjacji\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nr Pearsona\n- Mierzy zale≈ºno≈õƒá liniowƒÖ- Szeroko stosowany i zrozumia≈Çy\n- Zak≈Çada rozk≈Çad normalny- Wra≈ºliwy na warto≈õci odstajƒÖce- Uchwytuje tylko zale≈ºno≈õci liniowe\nInterwa≈Çowe, Ilorazowe, CiƒÖg≈Çe\n\n\nRho Spearmana\n- Mo≈ºe byƒá stosowany do danych porzƒÖdkowych- Uchwytuje zale≈ºno≈õci monotoniczne- Mniej wra≈ºliwy na warto≈õci odstajƒÖce\n- Traci informacje przez konwersjƒô na rangi- Mo≈ºe pominƒÖƒá niekt√≥re typy zale≈ºno≈õci\nPorzƒÖdkowe, Interwa≈Çowe, Ilorazowe\n\n\nTau Kendalla\n- Mo≈ºe byƒá stosowany do danych porzƒÖdkowych- Bardziej odporny ni≈º Spearman dla ma≈Çych pr√≥bek- Ma ≈ÇadnƒÖ interpretacjƒô (prawdopodobie≈Ñstwo zgodno≈õci)\n- Traci informacje, biorƒÖc pod uwagƒô tylko porzƒÖdek- Bardziej intensywny obliczeniowo\nPorzƒÖdkowe, Interwa≈Çowe, Ilorazowe\n\n\nChi-kwadrat\n- Mo≈ºe byƒá stosowany do danych nominalnych- Testuje niezale≈ºno≈õƒá zmiennych kategorycznych\n- Wymaga du≈ºych rozmiar√≥w pr√≥bek- Wra≈ºliwy na rozmiar pr√≥bki- Nie mierzy si≈Çy asocjacji\nNominalne, PorzƒÖdkowe\n\n\nV Cram√©ra\n- Mo≈ºe byƒá stosowany do danych nominalnych- Dostarcza miarƒô si≈Çy asocjacji- Znormalizowany do zakresu [0,1]\n- Interpretacja mo≈ºe byƒá subiektywna- Mo≈ºe przeszacowaƒá asocjacjƒô w ma≈Çych pr√≥bkach\nNominalne, PorzƒÖdkowe\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nMedian\nMediana\n-\n‚úì\n‚úì\n‚úì\n\n\nArithmetic Mean\n≈örednia arytmetyczna\n-\n-\n‚úì*\n‚úì\n\n\nGeometric Mean\n≈örednia geometryczna\n-\n-\n-\n‚úì\n\n\nHarmonic Mean\n≈örednia harmoniczna\n-\n-\n-\n‚úì\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstƒôp\n-\n‚úì\n‚úì\n‚úì\n\n\nInterquartile Range\nRozstƒôp miƒôdzykwartylowy\n-\n‚úì\n‚úì\n‚úì\n\n\nMean Absolute Deviation\n≈örednie odchylenie bezwzglƒôdne\n-\n-\n‚úì\n‚úì\n\n\nVariance\nWariancja\n-\n-\n‚úì*\n‚úì\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n‚úì*\n‚úì\n\n\nCoefficient of Variation\nWsp√≥≈Çczynnik zmienno≈õci\n-\n-\n-\n‚úì\n\n\nAssociation / Wsp√≥≈Çzale≈ºno≈õƒá:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n‚úì\n‚úì\n‚úì\n\n\nKendall‚Äôs Tau\nTau Kendalla\n-\n‚úì\n‚úì\n‚úì\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n‚úì*\n‚úì\n\n\nCovariance\nKowariancja\n-\n-\n‚úì*\n‚úì\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\nNotes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporzƒÖdkowania\nOrdinal: Ordered categories / Kategorie uporzƒÖdkowane\nInterval: Equal intervals, arbitrary zero / R√≥wne interwa≈Çy, umowne zero\nRatio: Equal intervals, absolute zero / R√≥wne interwa≈Çy, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ‚úì* are commonly used for interval data despite theoretical issues / Niekt√≥re miary oznaczone ‚úì* sƒÖ powszechnie stosowane dla danych przedzia≈Çowych pomimo problem√≥w teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wyb√≥r miary powinien uwzglƒôdniaƒá zar√≥wno poprawno≈õƒá teoretycznƒÖ jak i u≈ºyteczno≈õƒá praktycznƒÖ\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalajƒÖ na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "13¬† Data Visualization: with examples in R",
    "section": "",
    "text": "13.1 Introduction to Data Types and Visualization\nThis chapter explores fundamental types of data visualizations: bar plots, histograms, and box plots, in particular.\nBefore diving into specific visualization techniques, it‚Äôs crucial to understand the different types of data you might encounter and how they influence your choice of visualization method. We‚Äôll explore these concepts with practical examples using the ggplot2 library in R.\nFirst, let‚Äôs load the necessary libraries:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#bar-plots",
    "href": "chapter6.html#bar-plots",
    "title": "13¬† Data Visualization: with examples in R",
    "section": "13.2 Bar Plots",
    "text": "13.2 Bar Plots\nBar plots are excellent for displaying categorical data or summarizing continuous data by groups.\n\nUnderstanding Bar Plots\nA bar plot represents data using rectangular bars with heights proportional to the values they represent. They are used to compare different categories or groups.\nKey components of a bar plot: 1. X-axis: Represents categories 2. Y-axis: Represents values (can be counts, percentages, or any numerical value) 3. Bars: Rectangle for each category, height corresponds to its value\n\nExample Data\nLet‚Äôs use a simple dataset of fruit sales:\n\nfruits &lt;- c(\"Apple\", \"Banana\", \"Orange\", \"Grape\")\nsales &lt;- c(120, 85, 70, 100)\n\n# Create a data frame\ndf &lt;- data.frame(fruit = fruits, sales = sales)\n\n\n\n\nHand-Drawn Bar Plot\nTo create a bar plot by hand:\n\nDraw a horizontal line (x-axis) and a vertical line (y-axis) perpendicular to each other.\nLabel the x-axis with your categories (fruits), evenly spaced.\nLabel the y-axis with a suitable scale for your values (sales, 0 to 120 in increments of 20).\nFor each category, draw a rectangle (bar) whose height corresponds to its value on the y-axis scale.\nColor or shade each bar if desired.\nAdd a title and labels for both axes.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen drawing by hand, use graph paper for more precise measurements and straighter lines. Choose a scale that allows all your data to fit while maximizing the use of space.\n\n\n\n\nBar Plot in Base R\n\n# Create bar plot\nbarplot(sales, names.arg = fruits, \n        main = \"Fruit Sales\",\n        xlab = \"Fruit Types\", ylab = \"Sales\")\n\n\n\n\n\n\n\n\n\n\nBar Plot with ggplot2\n\n# Create bar plot with ggplot2\nggplot(df, aes(x = fruit, y = sales)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Fruit Sales\",\n       x = \"Fruit Types\", y = \"Sales\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpreting Bar Plots\nWhen interpreting a bar plot, consider the following:\n\nRelative Heights: Compare the heights of the bars to understand which categories have higher or lower values.\nOrdering: Sometimes, bars are ordered by height to make comparisons easier.\nPatterns: Look for any patterns or trends across categories.\nOutliers: Identify any bars that are much taller or shorter than the others.\n\n\nExample Interpretation\nFor our fruit sales data:\n\nApples have the highest sales (120), followed by Grapes (100).\nOranges have the lowest sales (70).\nThere‚Äôs a considerable difference between the highest (Apples) and lowest (Oranges) sales.\nBananas and Grapes have similar sales figures, in the middle range.\n\nThis information could be useful for inventory management or marketing strategies in a fruit shop.\n\n\n\n\n\n\nNote\n\n\n\nBar plots are great for comparing categories, but they don‚Äôt show the distribution within each category. For that, you might need other plot types like box plots.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#histograms",
    "href": "chapter6.html#histograms",
    "title": "13¬† Data Visualization: with examples in R",
    "section": "13.3 Histograms",
    "text": "13.3 Histograms\nHistograms visualize the distribution of a continuous variable by dividing it into intervals (bins) and showing the frequency or density of data points in each bin.\n\nUnderstanding Histograms\nKey components of a histogram: 1. X-axis: Represents the variable‚Äôs values, divided into bins 2. Y-axis: Represents frequency, relative frequency, or density 3. Bars: Rectangle for each bin, height corresponds to the y-axis measure\nThere are three main types of histograms:\n\nFrequency Histogram: The y-axis shows the count of data points in each bin.\nRelative Frequency Histogram: The y-axis shows the proportion of data points in each bin (frequency divided by total number of data points).\nDensity Histogram: The y-axis shows the density, which is the relative frequency divided by the bin width. The total area of all bars sums to 1.\n\n\nExample Data\nLet‚Äôs use a dataset of 50 student exam scores (out of 100):\n\nset.seed(123)  # for reproducibility\nscores &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\nHand-Drawn Histogram\nTo create a frequency histogram by hand:\n\nFind the range of your data.\nChoose a number of bins (let‚Äôs use 7 bins).\nCreate a frequency table.\nDraw x and y axes.\nLabel x-axis with bin ranges and y-axis with frequency.\nDraw a rectangle for each bin, with height corresponding to its frequency.\nAdd a title and labels for both axes.\n\nFor a relative frequency histogram, divide each frequency by the total number of data points before drawing the bars.\nFor a density histogram, divide the relative frequency by the bin width before drawing the bars.\n\n\n\n\n\n\nTip\n\n\n\nThe number of bins can affect the interpretation. Too few bins may obscure important features, while too many may introduce noise. A common rule of thumb is to use the square root of the number of data points as the number of bins.\n\n\n\n\nHistograms in Base R\n\n# Frequency Histogram\nhist(scores, breaks = 7, \n     main = \"Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Relative Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Relative Frequency\")\n\n\n\n\n\n\n\n# Density Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Density Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Density\")\nlines(density(scores), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\nHistograms with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(score = scores)\n\n# Frequency Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nggplot(df, aes(x = score, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Relative Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Relative Frequency\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Density Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Density Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Density\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nInterpreting Histograms\nWhen interpreting a histogram, consider:\n\nCentral Tendency: Where is the peak of the distribution?\nSpread: How wide is the distribution?\nShape: Is it symmetric, skewed, or multi-modal?\nOutliers: Are there any unusual values far from the main distribution?",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#box-plots-and-tukey-box-plots",
    "href": "chapter6.html#box-plots-and-tukey-box-plots",
    "title": "13¬† Data Visualization: with examples in R",
    "section": "13.4 Box Plots and Tukey Box Plots",
    "text": "13.4 Box Plots and Tukey Box Plots\nBox plots, also known as box-and-whisker plots, provide a concise summary of a distribution. We‚Äôll focus on the Tukey-style box plot, named after the statistician John Tukey who popularized this type of plot.\n\nUnderstanding Box Plots\nA box plot represents five key statistics:\n\nMinimum value (excluding outliers)\nFirst quartile (Q1)\nMedian\nThird quartile (Q3)\nMaximum value (excluding outliers)\n\nAdditionally, box plots show:\n\nWhiskers: Lines extending from the box to the minimum and maximum values (excluding outliers)\nOutliers: Individual points beyond the whiskers\n\n\nCalculating Quartiles and Outliers\nTo create a box plot, follow these steps:\n\nOrder your data from smallest to largest.\nFind the median (middle value if odd number of data points, average of two middle values if even).\nFind Q1 (median of lower half of data) and Q3 (median of upper half of data).\nCalculate the Interquartile Range (IQR) = Q3 - Q1\nDetermine outliers using Tukey‚Äôs rule:\n\nLower outliers: &lt; Q1 - 1.5 * IQR\nUpper outliers: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe factor 1.5 in Tukey‚Äôs outlier rule is based on the properties of the normal distribution. For normally distributed data, this rule identifies about 0.7% of the data as potential outliers.\n\n\n\n\nExample Data\nLet‚Äôs use a small dataset to illustrate:\n\ndata &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\nHand-Drawn Tukey Box Plot\nTo create a Tukey box plot by hand:\n\nDraw a vertical line representing the range from minimum to maximum (2 to 15 in our example, excluding the outlier).\nDraw a box from Q1 to Q3.\nDraw a horizontal line through the box at the median.\nDraw whiskers from the box to the minimum and maximum values (excluding outliers).\nRepresent the outlier (50) as an individual point beyond the whisker.\nAdd a scale to the vertical axis and label it.\n\n\n\nBox Plot in Base R\n\n# Create box plot\nboxplot(data, main = \"Box Plot of Sample Data\",\n        ylab = \"Values\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\nTukey Box Plot with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(value = data)\n\n# Create Tukey box plot with ggplot2\nggplot(df, aes(x = \"\", y = value)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Tukey Box Plot of Sample Data\",\n       x = \"\", y = \"Values\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\nInterpreting Box Plots\nWhen interpreting a box plot, consider the following:\n\nCentral Tendency: The median shows the center of the distribution.\nSpread: The box (IQR) represents the middle 50% of the data.\nSkewness: If the median line is closer to one end of the box, the distribution is skewed.\nOutliers: Points beyond the whiskers are potential outliers.\nComparisons: When comparing multiple box plots, look at relative positions of medians, box sizes, and presence of outliers.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#conclusion",
    "href": "chapter6.html#conclusion",
    "title": "13¬† Data Visualization: with examples in R",
    "section": "13.5 Conclusion",
    "text": "13.5 Conclusion\nIn this chapter, we explored three fundamental types of data visualizations: bar plots, histograms, and box plots. We demonstrated how to create these plots by hand, using R‚Äôs base plotting system, and using the ggplot2 library.\nEach type of plot serves a different purpose: - Bar plots are excellent for comparing categories. - Histograms show the distribution of a continuous variable. - Box plots provide a concise summary of a distribution, highlighting central tendency, spread, and outliers.\nRemember, the choice of visualization depends on your data type and the insights you want to convey. Always consider your audience and the story you want to tell with your data when selecting and designing your visualizations.\nPractice creating these plots by hand to deepen your understanding of their construction and interpretation. Then, leverage the power of R and ggplot2 to quickly create and customize these visualizations for larger datasets and more complex analyses.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html",
    "href": "rozdzial6.html",
    "title": "14¬† Wizualizacja Danych: z przyk≈Çadami w R",
    "section": "",
    "text": "14.1 Wprowadzenie do Typ√≥w Danych i Wizualizacji\nW tym rozdziale poznamy podstawowe typy wizualizacji danych: wykresy s≈Çupkowe, histogramy i wykresy pude≈Çkowe. Om√≥wimy ich tworzenie zar√≥wno rƒôcznie, jak i przy u≈ºyciu R.\nPrzed zag≈Çƒôbieniem siƒô w konkretne techniki wizualizacji, wa≈ºne jest zrozumienie r√≥≈ºnych typ√≥w danych i ich wp≈Çywu na wyb√≥r metody wizualizacji. Przeanalizujemy te koncepcje na praktycznych przyk≈Çadach z u≈ºyciem biblioteki ggplot2 w R.\nNajpierw za≈Çadujmy niezbƒôdne biblioteki:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Wizualizacja Danych: z przyk≈Çadami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-s≈Çupkowe",
    "href": "rozdzial6.html#wykresy-s≈Çupkowe",
    "title": "14¬† Wizualizacja Danych: z przyk≈Çadami w R",
    "section": "14.2 Wykresy S≈Çupkowe",
    "text": "14.2 Wykresy S≈Çupkowe\nWykresy s≈Çupkowe doskonale nadajƒÖ siƒô do prezentacji danych kategorycznych lub podsumowania danych ciƒÖg≈Çych w grupach.\n\nZrozumienie Wykres√≥w S≈Çupkowych\nWykres s≈Çupkowy przedstawia dane za pomocƒÖ prostokƒÖtnych s≈Çupk√≥w, kt√≥rych wysoko≈õƒá jest proporcjonalna do reprezentowanych przez nie warto≈õci. S≈Çu≈ºƒÖ do por√≥wnywania r√≥≈ºnych kategorii lub grup.\nG≈Ç√≥wne elementy wykresu s≈Çupkowego: 1. O≈õ X: Reprezentuje kategorie 2. O≈õ Y: Reprezentuje warto≈õci (mogƒÖ to byƒá liczebno≈õci, procenty lub dowolne warto≈õci numeryczne) 3. S≈Çupki: ProstokƒÖt dla ka≈ºdej kategorii, wysoko≈õƒá odpowiada jej warto≈õci\n\nPrzyk≈Çadowe Dane\nU≈ºyjmy prostego zestawu danych dotyczƒÖcego sprzeda≈ºy owoc√≥w:\n\nowoce &lt;- c(\"Jab≈Çko\", \"Banan\", \"Pomara≈Ñcza\", \"Winogrono\")\nsprzedaz &lt;- c(120, 85, 70, 100)\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(owoc = owoce, sprzedaz = sprzedaz)\n\n\n\n\nRƒôcznie Rysowany Wykres S≈Çupkowy\nAby stworzyƒá wykres s≈Çupkowy rƒôcznie:\n\nNarysuj liniƒô poziomƒÖ (o≈õ X) i pionowƒÖ (o≈õ Y) prostopad≈Çe do siebie.\nOznacz o≈õ X swoimi kategoriami (owocami), r√≥wnomiernie rozmieszczonymi.\nOznacz o≈õ Y odpowiedniƒÖ skalƒÖ dla Twoich warto≈õci (sprzeda≈º, od 0 do 120 z przyrostami co 20).\nDla ka≈ºdej kategorii narysuj prostokƒÖt (s≈Çupek), kt√≥rego wysoko≈õƒá odpowiada jej warto≈õci na skali osi Y.\nJe≈õli chcesz, pokoloruj lub zacienuj ka≈ºdy s≈Çupek.\nDodaj tytu≈Ç i etykiety dla obu osi.\n\n\n\n\n\n\n\nTip\n\n\n\nPrzy rysowaniu rƒôcznym u≈ºyj papieru milimetrowego dla dok≈Çadniejszych pomiar√≥w i prostszych linii. Wybierz skalƒô, kt√≥ra pozwoli zmie≈õciƒá wszystkie dane, maksymalnie wykorzystujƒÖc dostƒôpnƒÖ przestrze≈Ñ.\n\n\n\n\nWykres S≈Çupkowy w Podstawowym R\n\n# Tworzenie wykresu s≈Çupkowego\nbarplot(sprzedaz, names.arg = owoce, \n        main = \"Sprzeda≈º Owoc√≥w\",\n        xlab = \"Rodzaje Owoc√≥w\", ylab = \"Sprzeda≈º\")\n\n\n\n\n\n\n\n\n\n\nWykres S≈Çupkowy z ggplot2\n\n# Tworzenie wykresu s≈Çupkowego z ggplot2\nggplot(df, aes(x = owoc, y = sprzedaz)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Sprzeda≈º Owoc√≥w\",\n       x = \"Rodzaje Owoc√≥w\", y = \"Sprzeda≈º\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpretacja Wykres√≥w S≈Çupkowych\nPodczas interpretacji wykresu s≈Çupkowego zwr√≥ƒá uwagƒô na:\n\nWzglƒôdne Wysoko≈õci: Por√≥wnaj wysoko≈õci s≈Çupk√≥w, aby zrozumieƒá, kt√≥re kategorie majƒÖ wy≈ºsze lub ni≈ºsze warto≈õci.\nKolejno≈õƒá: Czasami s≈Çupki sƒÖ uporzƒÖdkowane wed≈Çug wysoko≈õci, aby u≈Çatwiƒá por√≥wnania.\nWzorce: Poszukaj wzorc√≥w lub trend√≥w miƒôdzy kategoriami.\nWarto≈õci OdstajƒÖce: Zidentyfikuj s≈Çupki, kt√≥re sƒÖ znacznie wy≈ºsze lub ni≈ºsze od pozosta≈Çych.\n\n\nPrzyk≈Çadowa Interpretacja\nDla naszych danych o sprzeda≈ºy owoc√≥w:\n\nJab≈Çka majƒÖ najwy≈ºszƒÖ sprzeda≈º (120), nastƒôpnie Winogrona (100).\nPomara≈Ñcze majƒÖ najni≈ºszƒÖ sprzeda≈º (70).\nIstnieje znaczna r√≥≈ºnica miƒôdzy najwy≈ºszƒÖ (Jab≈Çka) a najni≈ºszƒÖ (Pomara≈Ñcze) sprzeda≈ºƒÖ.\nBanany i Winogrona majƒÖ podobne warto≈õci sprzeda≈ºy, w ≈õrednim zakresie.\n\nTa informacja mo≈ºe byƒá przydatna dla zarzƒÖdzania zapasami lub strategii marketingowych w sklepie owocowym.\n\n\n\n\n\n\nNote\n\n\n\nWykresy s≈Çupkowe sƒÖ ≈õwietne do por√≥wnywania kategorii, ale nie pokazujƒÖ rozk≈Çadu wewnƒÖtrz ka≈ºdej kategorii. Do tego mogƒÖ byƒá potrzebne inne typy wykres√≥w, jak wykresy pude≈Çkowe.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Wizualizacja Danych: z przyk≈Çadami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#histogramy",
    "href": "rozdzial6.html#histogramy",
    "title": "14¬† Wizualizacja Danych: z przyk≈Çadami w R",
    "section": "14.3 Histogramy",
    "text": "14.3 Histogramy\nHistogramy wizualizujƒÖ rozk≈Çad zmiennej ciƒÖg≈Çej poprzez podzielenie jej na przedzia≈Çy (bins) i pokazanie czƒôsto≈õci lub gƒôsto≈õci punkt√≥w danych w ka≈ºdym przedziale.\n\nZrozumienie Histogram√≥w\nG≈Ç√≥wne elementy histogramu: 1. O≈õ X: Reprezentuje warto≈õci zmiennej, podzielone na przedzia≈Çy 2. O≈õ Y: Reprezentuje czƒôsto≈õƒá, wzglƒôdnƒÖ czƒôsto≈õƒá lub gƒôsto≈õƒá 3. S≈Çupki: ProstokƒÖt dla ka≈ºdego przedzia≈Çu, wysoko≈õƒá odpowiada mierze na osi Y\nIstniejƒÖ trzy g≈Ç√≥wne typy histogram√≥w:\n\nHistogram Czƒôsto≈õci: O≈õ Y pokazuje liczbƒô punkt√≥w danych w ka≈ºdym przedziale.\nHistogram Czƒôsto≈õci Wzglƒôdnej: O≈õ Y pokazuje proporcjƒô punkt√≥w danych w ka≈ºdym przedziale (czƒôsto≈õƒá podzielona przez ca≈ÇkowitƒÖ liczbƒô punkt√≥w danych).\nHistogram Gƒôsto≈õci: O≈õ Y pokazuje gƒôsto≈õƒá, kt√≥ra jest czƒôsto≈õciƒÖ wzglƒôdnƒÖ podzielonƒÖ przez szeroko≈õƒá przedzia≈Çu. Ca≈Çkowita powierzchnia wszystkich s≈Çupk√≥w sumuje siƒô do 1.\n\n\nPrzyk≈Çadowe Dane\nU≈ºyjmy zbioru 50 wynik√≥w egzamin√≥w student√≥w (na 100 punkt√≥w):\n\nset.seed(123)  # dla powtarzalno≈õci\nwyniki &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\nRƒôcznie Rysowany Histogram\nAby stworzyƒá histogram czƒôsto≈õci rƒôcznie:\n\nZnajd≈∫ zakres danych.\nWybierz liczbƒô przedzia≈Ç√≥w (u≈ºyjmy 7 przedzia≈Ç√≥w).\nUtw√≥rz tabelƒô czƒôsto≈õci.\nNarysuj osie X i Y.\nOznacz o≈õ X zakresami przedzia≈Ç√≥w, a o≈õ Y czƒôsto≈õciƒÖ.\nNarysuj prostokƒÖt dla ka≈ºdego przedzia≈Çu, z wysoko≈õciƒÖ odpowiadajƒÖcƒÖ jego czƒôsto≈õci.\nDodaj tytu≈Ç i etykiety dla obu osi.\n\nDla histogramu czƒôsto≈õci wzglƒôdnej, podziel ka≈ºdƒÖ czƒôsto≈õƒá przez ca≈ÇkowitƒÖ liczbƒô punkt√≥w danych przed narysowaniem s≈Çupk√≥w.\nDla histogramu gƒôsto≈õci, podziel czƒôsto≈õƒá wzglƒôdnƒÖ przez szeroko≈õƒá przedzia≈Çu przed narysowaniem s≈Çupk√≥w.\n\n\n\n\n\n\nTip\n\n\n\nLiczba przedzia≈Ç√≥w mo≈ºe wp≈ÇynƒÖƒá na interpretacjƒô. Zbyt ma≈Ço przedzia≈Ç√≥w mo≈ºe ukryƒá wa≈ºne cechy, podczas gdy zbyt wiele mo≈ºe wprowadziƒá szum. PowszechnƒÖ regu≈ÇƒÖ jest u≈ºycie pierwiastka kwadratowego z liczby punkt√≥w danych jako liczby przedzia≈Ç√≥w.\n\n\n\n\nHistogramy w Podstawowym R\n\n# Histogram Czƒôsto≈õci\nhist(wyniki, breaks = 7, \n     main = \"Histogram Czƒôsto≈õci Wynik√≥w Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Czƒôsto≈õƒá\")\n\n\n\n\n\n\n\n# Histogram Czƒôsto≈õci Wzglƒôdnej\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Czƒôsto≈õci Wzglƒôdnej Wynik√≥w Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Czƒôsto≈õƒá Wzglƒôdna\")\n\n\n\n\n\n\n\n# Histogram Gƒôsto≈õci\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Gƒôsto≈õci Wynik√≥w Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Gƒôsto≈õƒá\")\nlines(density(wyniki), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\nHistogramy z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wynik = wyniki)\n\n# Histogram Czƒôsto≈õci\nggplot(df, aes(x = wynik)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Czƒôsto≈õci Wynik√≥w Egzaminu\",\n       x = \"Wyniki\", y = \"Czƒôsto≈õƒá\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram Czƒôsto≈õci Wzglƒôdnej\nggplot(df, aes(x = wynik, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Czƒôsto≈õci Wzglƒôdnej Wynik√≥w Egzaminu\",\n       x = \"Wyniki\", y = \"Czƒôsto≈õƒá Wzglƒôdna\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Histogram Gƒôsto≈õci\nggplot(df, aes(x = wynik)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Histogram Gƒôsto≈õci Wynik√≥w Egzaminu\",\n       x = \"Wyniki\", y = \"Gƒôsto≈õƒá\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nInterpretacja Histogram√≥w\nPodczas interpretacji histogramu zwr√≥ƒá uwagƒô na:\n\nTendencjƒô CentralnƒÖ: Gdzie znajduje siƒô szczyt rozk≈Çadu?\nRozrzut: Jak szeroki jest rozk≈Çad?\nKszta≈Çt: Czy jest symetryczny, sko≈õny, czy wielomodalny?\nWarto≈õci OdstajƒÖce: Czy sƒÖ nietypowe warto≈õci daleko od g≈Ç√≥wnego rozk≈Çadu?",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Wizualizacja Danych: z przyk≈Çadami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-pude≈Çkowe-i-wykresy-pude≈Çkowe-tukeya",
    "href": "rozdzial6.html#wykresy-pude≈Çkowe-i-wykresy-pude≈Çkowe-tukeya",
    "title": "14¬† Wizualizacja Danych: z przyk≈Çadami w R",
    "section": "14.4 Wykresy Pude≈Çkowe i Wykresy Pude≈Çkowe Tukeya",
    "text": "14.4 Wykresy Pude≈Çkowe i Wykresy Pude≈Çkowe Tukeya\nWykresy pude≈Çkowe, znane r√≥wnie≈º jako wykresy skrzynkowe, dostarczajƒÖ zwiƒôz≈Çego podsumowania rozk≈Çadu. Skupimy siƒô na wykresie pude≈Çkowym w stylu Tukeya, nazwanym na cze≈õƒá statystyka Johna Tukeya, kt√≥ry spopularyzowa≈Ç ten typ wykresu.\n\nZrozumienie Wykres√≥w Pude≈Çkowych\nWykres pude≈Çkowy przedstawia piƒôƒá kluczowych statystyk:\n\nWarto≈õƒá minimalna (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych)\nPierwszy kwartyl (Q1)\nMediana\nTrzeci kwartyl (Q3)\nWarto≈õƒá maksymalna (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych)\n\nDodatkowo wykresy pude≈Çkowe pokazujƒÖ:\n\nWƒÖsy: Linie rozciƒÖgajƒÖce siƒô od pude≈Çka do warto≈õci minimalnej i maksymalnej (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych)\nWarto≈õci odstajƒÖce: Indywidualne punkty poza wƒÖsami\n\n\nObliczanie Kwartyli i Warto≈õci OdstajƒÖcych\nAby stworzyƒá wykres pude≈Çkowy, postƒôpuj zgodnie z tymi krokami:\n\nUporzƒÖdkuj dane od najmniejszej do najwiƒôkszej warto≈õci.\nZnajd≈∫ medianƒô (≈õrodkowa warto≈õƒá dla nieparzystej liczby punkt√≥w danych, ≈õrednia z dw√≥ch ≈õrodkowych warto≈õci dla parzystej).\nZnajd≈∫ Q1 (mediana dolnej po≈Çowy danych) i Q3 (mediana g√≥rnej po≈Çowy danych).\nOblicz Rozstƒôp Miƒôdzykwartylowy (IQR) = Q3 - Q1\nOkre≈õl warto≈õci odstajƒÖce u≈ºywajƒÖc regu≈Çy Tukeya:\n\nDolne warto≈õci odstajƒÖce: &lt; Q1 - 1.5 * IQR\nG√≥rne warto≈õci odstajƒÖce: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nWsp√≥≈Çczynnik 1.5 w regule Tukeya dla warto≈õci odstajƒÖcych opiera siƒô na w≈Ça≈õciwo≈õciach rozk≈Çadu normalnego. Dla danych o rozk≈Çadzie normalnym, ta regu≈Ça identyfikuje oko≈Ço 0.7% danych jako potencjalne warto≈õci odstajƒÖce.\n\n\n\n\nPrzyk≈Çadowe Dane\nU≈ºyjmy ma≈Çego zbioru danych do ilustracji:\n\ndane &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\nRƒôcznie Rysowany Wykres Pude≈Çkowy Tukeya\nAby stworzyƒá wykres pude≈Çkowy Tukeya rƒôcznie:\n\nNarysuj liniƒô pionowƒÖ reprezentujƒÖcƒÖ zakres od minimum do maksimum (2 do 15 w naszym przyk≈Çadzie, z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcej).\nNarysuj pude≈Çko od Q1 do Q3.\nNarysuj poziomƒÖ liniƒô przez pude≈Çko na poziomie mediany.\nNarysuj wƒÖsy od pude≈Çka do warto≈õci minimalnej i maksymalnej (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych).\nPrzedstaw warto≈õƒá odstajƒÖcƒÖ (50) jako indywidualny punkt poza wƒÖsem.\nDodaj skalƒô do osi pionowej i oznacz jƒÖ.\n\n\n\nWykres Pude≈Çkowy w Podstawowym R\n\n# Tworzenie wykresu pude≈Çkowego\nboxplot(dane, main = \"Wykres Pude≈Çkowy Przyk≈Çadowych Danych\",\n        ylab = \"Warto≈õci\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\nWykres Pude≈Çkowy Tukeya z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wartosc = dane)\n\n# Tworzenie wykresu pude≈Çkowego Tukeya z ggplot2\nggplot(df, aes(x = \"\", y = wartosc)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Wykres Pude≈Çkowy Tukeya Przyk≈Çadowych Danych\",\n       x = \"\", y = \"Warto≈õci\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\nInterpretacja Wykres√≥w Pude≈Çkowych\nPodczas interpretacji wykresu pude≈Çkowego zwr√≥ƒá uwagƒô na nastƒôpujƒÖce elementy:\n\nTendencja Centralna: Mediana pokazuje ≈õrodek rozk≈Çadu.\nRozrzut: Pude≈Çko (IQR) reprezentuje ≈õrodkowe 50% danych.\nSko≈õno≈õƒá: Je≈õli linia mediany jest bli≈ºej jednego ko≈Ñca pude≈Çka, rozk≈Çad jest sko≈õny.\nWarto≈õci OdstajƒÖce: Punkty poza wƒÖsami sƒÖ potencjalnymi warto≈õciami odstajƒÖcymi.\nPor√≥wnania: Przy por√≥wnywaniu wielu wykres√≥w pude≈Çkowych, zwr√≥ƒá uwagƒô na wzglƒôdne po≈Ço≈ºenie median, rozmiary pude≈Çek i obecno≈õƒá warto≈õci odstajƒÖcych.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Wizualizacja Danych: z przyk≈Çadami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "href": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "title": "14¬† Wizualizacja Danych: z przyk≈Çadami w R",
    "section": "14.5 Zaawansowane Techniki Wizualizacji",
    "text": "14.5 Zaawansowane Techniki Wizualizacji\nOpr√≥cz podstawowych typ√≥w wykres√≥w, warto poznaƒá kilka bardziej zaawansowanych technik wizualizacji, kt√≥re mogƒÖ byƒá przydatne w analizie danych.\n\nWykresy Skrzypcowe\nWykresy skrzypcowe ≈ÇƒÖczƒÖ cechy wykres√≥w pude≈Çkowych i wykres√≥w gƒôsto≈õci, dajƒÖc bardziej kompletny obraz rozk≈Çadu danych.\n\n# Tworzenie wykresu skrzypcowego\nggplot(df, aes(x = \"\", y = wartosc)) +\n  geom_violin(fill = \"lightblue\") +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Wykres Skrzypcowy Przyk≈Çadowych Danych\",\n       x = \"\", y = \"Warto≈õci\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWykresy Rozrzutu z Marginesami\n≈ÅƒÖczenie wykres√≥w rozrzutu z histogramami na marginesach mo≈ºe dostarczyƒá wiƒôcej informacji o rozk≈Çadzie danych w dw√≥ch wymiarach.\n\n# Generowanie danych do wykresu rozrzutu\nset.seed(123)\ndf_scatter &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100)\n)\n\n# Tworzenie wykresu rozrzutu z marginesami\nlibrary(ggExtra)\np &lt;- ggplot(df_scatter, aes(x = x, y = y)) +\n  geom_point() +\n  theme_minimal()\nggMarginal(p, type = \"histogram\", fill = \"lightblue\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Wizualizacja Danych: z przyk≈Çadami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wnioski",
    "href": "rozdzial6.html#wnioski",
    "title": "14¬† Wizualizacja Danych: z przyk≈Çadami w R",
    "section": "14.6 Wnioski",
    "text": "14.6 Wnioski\nW tym rozdziale poznali≈õmy trzy podstawowe typy wizualizacji danych: wykresy s≈Çupkowe, histogramy i wykresy pude≈Çkowe. Pokazali≈õmy, jak tworzyƒá te wykresy rƒôcznie, u≈ºywajƒÖc podstawowego systemu wykres√≥w R oraz biblioteki ggplot2.\nKa≈ºdy typ wykresu s≈Çu≈ºy innemu celowi: - Wykresy s≈Çupkowe doskonale nadajƒÖ siƒô do por√≥wnywania kategorii. - Histogramy pokazujƒÖ rozk≈Çad zmiennej ciƒÖg≈Çej. - Wykresy pude≈Çkowe dostarczajƒÖ zwiƒôz≈Çego podsumowania rozk≈Çadu, podkre≈õlajƒÖc tendencjƒô centralnƒÖ, rozrzut i warto≈õci odstajƒÖce.\nPamiƒôtaj, ≈ºe wyb√≥r wizualizacji zale≈ºy od typu danych i wniosk√≥w, kt√≥re chcesz przekazaƒá. Zawsze bierz pod uwagƒô swojƒÖ docelowƒÖ grupƒô odbiorc√≥w i historiƒô, kt√≥rƒÖ chcesz opowiedzieƒá za pomocƒÖ swoich danych, wybierajƒÖc i projektujƒÖc wizualizacje.\nƒÜwicz tworzenie tych wykres√≥w rƒôcznie, aby pog≈Çƒôbiƒá zrozumienie ich konstrukcji i interpretacji. Nastƒôpnie wykorzystaj moc R i ggplot2, aby szybko tworzyƒá i dostosowywaƒá te wizualizacje dla wiƒôkszych zbior√≥w danych i bardziej z≈Ço≈ºonych analiz.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Wizualizacja Danych: z przyk≈Çadami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#ƒáwiczenia-praktyczne",
    "href": "rozdzial6.html#ƒáwiczenia-praktyczne",
    "title": "14¬† Wizualizacja Danych: z przyk≈Çadami w R",
    "section": "14.7 ƒÜwiczenia Praktyczne",
    "text": "14.7 ƒÜwiczenia Praktyczne\n\nZbierz dane o popularno≈õci r√≥≈ºnych gatunk√≥w muzycznych w≈õr√≥d Twoich znajomych. Stw√≥rz wykres s≈Çupkowy przedstawiajƒÖcy te dane.\nZmierz czas reakcji 30 os√≥b na bodziec d≈∫wiƒôkowy (w milisekundach). Utw√≥rz histogram tych danych.\nZbierz dane o wzro≈õcie 50 os√≥b w Twojej spo≈Çeczno≈õci. Stw√≥rz wykres pude≈Çkowy dla tych danych, osobno dla mƒô≈ºczyzn i kobiet.\nZnajd≈∫ zestaw danych online (np. na Kaggle) i stw√≥rz trzy r√≥≈ºne wizualizacje dla tych danych. Opisz, jakie wnioski mo≈ºna wyciƒÖgnƒÖƒá z ka≈ºdej wizualizacji.\nStw√≥rz wykres skrzypcowy dla danych o cenach dom√≥w w r√≥≈ºnych dzielnicach miasta. Por√≥wnaj go z wykresem pude≈Çkowym tych samych danych. Jakie dodatkowe informacje dostarcza wykres skrzypcowy?\n\nPamiƒôtaj, ≈ºe praktyka jest kluczem do opanowania sztuki wizualizacji danych. Eksperymentuj z r√≥≈ºnymi typami wykres√≥w i parametrami, aby znale≈∫ƒá najlepszy spos√≥b przedstawienia swoich danych.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Wizualizacja Danych: z przyk≈Çadami w R</span>"
    ]
  },
  {
    "objectID": "correg_en.html",
    "href": "correg_en.html",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "",
    "text": "15.1 Introduction\nThe distinction between correlation and causation represents a fundamental challenge in statistical analysis. While correlation measures the statistical association between variables, causation implies a direct influence of one variable on another.\nStatistical relationships form the backbone of data-driven decision making across disciplines‚Äîfrom economics and public health to psychology and environmental science. Understanding when a relationship indicates mere association versus genuine causality is crucial for valid inference and effective policy recommendations.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#covariance",
    "href": "correg_en.html#covariance",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.2 Covariance",
    "text": "15.2 Covariance\nCovariance measures how two variables vary together, indicating both the direction and magnitude of their linear relationship.\nFormula: \\text{cov}(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\nWhere:\n\nx_i and y_i are individual data points\n\\bar{x} and \\bar{y} are the means of variables X and Y\nn is the number of observations\nWe divide by (n-1) for sample covariance (Bessel‚Äôs correction)\n\n\nStep-by-Step Manual Calculation Process\nExample 1: Student Study Hours vs.¬†Test Scores\nData:\n\nStudy Hours (X): 2, 4, 6, 8, 10\nTest Scores (Y): 65, 70, 80, 85, 95\n\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate means\n\\bar{x} = \\frac{2+4+6+8+10}{5} = 6 hours\n\n\n\n\n\\bar{y} = \\frac{65+70+80+85+95}{5} = 79 points\n\n\n2\nCalculate deviations\n(x_i - \\bar{x}): -4, -2, 0, 2, 4\n\n\n\n\n(y_i - \\bar{y}): -14, -9, 1, 6, 16\n\n\n3\nCalculate products\n(x_i - \\bar{x})(y_i - \\bar{y}):\n\n\n\n\n(-4)(-14) = 56\n\n\n\n\n(-2)(-9) = 18\n\n\n\n\n(0)(1) = 0\n\n\n\n\n(2)(6) = 12\n\n\n\n\n(4)(16) = 64\n\n\n4\nSum the products\n\\sum = 56 + 18 + 0 + 12 + 64 = 150\n\n\n5\nDivide by (n-1)\n\\text{cov}(X,Y) = \\frac{150}{5-1} = \\frac{150}{4} = 37.5\n\n\n\nR Verification:\n\n# Define the data\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Calculate covariance\ncov_manual &lt;- cov(study_hours, test_scores)\nprint(paste(\"Covariance:\", round(cov_manual, 2)))\n\n[1] \"Covariance: 37.5\"\n\n# Verify step by step\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\nproducts &lt;- x_dev * y_dev\ncov_calculated &lt;- sum(products) / (length(study_hours) - 1)\n\n# Display calculation steps\ndata.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_deviation = x_dev,\n  Y_deviation = y_dev,\n  Product = products\n)\n\n   X  Y X_deviation Y_deviation Product\n1  2 65          -4         -14      56\n2  4 70          -2          -9      18\n3  6 80           0           1       0\n4  8 85           2           6      12\n5 10 95           4          16      64\n\n\nInterpretation: The positive covariance (37.5) indicates that study hours and test scores tend to increase together.\n\n\nPractice Problem with Solution\nCalculate covariance manually for:\n\nTemperature (¬∞F): 32, 50, 68, 86, 95\nIce Cream Sales ($): 100, 200, 400, 600, 800\n\nSolution:\n\n\n\nStep\nCalculation\n\n\n\n\n1. Means\n\\bar{x} = \\frac{32+50+68+86+95}{5} = 66.2¬∞F\n\n\n\n\\bar{y} = \\frac{100+200+400+600+800}{5} = 420\n\n\n2. Deviations\nX: -34.2, -16.2, 1.8, 19.8, 28.8\n\n\n\nY: -320, -220, -20, 180, 380\n\n\n3. Products\n10944, 3564, -36, 3564, 10944\n\n\n4. Sum\n28980\n\n\n5. Covariance\n\\frac{28980}{4} = 7245\n\n\n\n\n# Verify practice problem\ntemp &lt;- c(32, 50, 68, 86, 95)\nsales &lt;- c(100, 200, 400, 600, 800)\nprint(paste(\"Covariance for practice problem:\", round(cov(temp, sales), 2)))\n\n[1] \"Covariance for practice problem: 7245\"",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#correlation-coefficient",
    "href": "correg_en.html#correlation-coefficient",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.3 Correlation Coefficient",
    "text": "15.3 Correlation Coefficient\nThe correlation coefficient standardizes covariance to eliminate scale dependency, producing values between -1 and +1.\n\nInterpretation Guidelines\n\n\n\n\n\n\n\n\n\nCorrelation Value\nStrength\nInterpretation\nExample\n\n\n\n\n¬±0.90 to ¬±1.00\nVery Strong\nAlmost perfect relationship\nHeight of parents and children\n\n\n¬±0.70 to ¬±0.89\nStrong\nHighly related variables\nStudy time and grades\n\n\n¬±0.50 to ¬±0.69\nModerate\nModerately related\nExercise and weight loss\n\n\n¬±0.30 to ¬±0.49\nWeak\nWeakly related\nShoe size and reading ability\n\n\n¬±0.00 to ¬±0.29\nVery Weak/None\nLittle to no relationship\nBirth month and intelligence\n\n\n\n\n\nTypes of Correlations Visualization\n\n# Generate sample data with different correlation patterns\nn &lt;- 100\n\n# Positive linear correlation\nset.seed(123)\nyears_education &lt;- rnorm(n, 14, 3)\nannual_income &lt;- 15000 + 3500 * years_education + rnorm(n, 0, 10000)\npos_cor &lt;- round(cor(years_education, annual_income), 3)\n\n# Negative linear correlation\nscreen_time &lt;- runif(n, 1, 8)\nsleep_hours &lt;- 9 - 0.5 * screen_time + rnorm(n, 0, 0.5)\nneg_cor &lt;- round(cor(screen_time, sleep_hours), 3)\n\n# No correlation\nx_random &lt;- rnorm(n, 50, 10)\ny_random &lt;- rnorm(n, 50, 10)\nno_cor &lt;- round(cor(x_random, y_random), 3)\n\n# Non-linear correlation (quadratic)\nhours_studied &lt;- runif(n, 0, 12)\ntest_score &lt;- -2 * (hours_studied - 6)^2 + 90 + rnorm(n, 0, 5)\nnonlin_cor &lt;- round(cor(hours_studied, test_score), 3)\n\n# Create data frames with correlation values\npositive_data &lt;- data.frame(\n  x = years_education, \n  y = annual_income,\n  label = paste0(\"Positive Linear (r = \", pos_cor, \")\")\n)\n\nnegative_data &lt;- data.frame(\n  x = screen_time, \n  y = sleep_hours,\n  label = paste0(\"Negative Linear (r = \", neg_cor, \")\")\n)\n\nno_corr_data &lt;- data.frame(\n  x = x_random, \n  y = y_random,\n  label = paste0(\"No Correlation (r = \", no_cor, \")\")\n)\n\nnonlinear_data &lt;- data.frame(\n  x = hours_studied, \n  y = test_score,\n  label = paste0(\"Non-linear (Pearson r = \", nonlin_cor, \")\")\n)\n\n# Combine data\nall_data &lt;- rbind(positive_data, negative_data, no_corr_data, nonlinear_data)\n\n# Create faceted plot\nggplot(all_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE, alpha = 0.3) +\n  facet_wrap(~ label, scales = \"free\", ncol = 2) +\n  labs(\n    title = \"Different Types of Correlations\",\n    subtitle = \"Linear regression line shown in red with confidence band\",\n    x = \"\", \n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, color = \"gray50\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#pearson-correlation",
    "href": "correg_en.html#pearson-correlation",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.4 Pearson Correlation",
    "text": "15.4 Pearson Correlation\nFormula: r = \\frac{\\text{cov}(X,Y)}{s_X s_Y} = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}}\n\nComplete Manual Calculation Example\nUsing our study hours example:\n\nStudy Hours (X): 2, 4, 6, 8, 10\nTest Scores (Y): 65, 70, 80, 85, 95\n\nDetailed Calculation Steps:\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate covariance\nFrom above: \\text{cov}(X,Y) = 37.5\n\n\n2\nCalculate deviations squared\n\n\n\n\nFor X\n(x_i - \\bar{x})^2: 16, 4, 0, 4, 16\n\n\n\n\nSum = 40\n\n\n\nFor Y\n(y_i - \\bar{y})^2: 196, 81, 1, 36, 256\n\n\n\n\nSum = 570\n\n\n3\nCalculate standard deviations\n\n\n\n\ns_X\ns_X = \\sqrt{\\frac{40}{4}} = \\sqrt{10} = 3.162\n\n\n\ns_Y\ns_Y = \\sqrt{\\frac{570}{4}} = \\sqrt{142.5} = 11.937\n\n\n4\nCalculate correlation\nr = \\frac{37.5}{3.162 \\times 11.937}\n\n\n\n\nr = \\frac{37.5}{37.73} = 0.994\n\n\n\n\n# Manual calculation verification\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Calculate Pearson correlation\nr_value &lt;- cor(study_hours, test_scores, method = \"pearson\")\nprint(paste(\"Pearson correlation coefficient:\", round(r_value, 3)))\n\n[1] \"Pearson correlation coefficient: 0.993\"\n\n# Detailed calculation\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\n\n# Show calculation table\ncalc_table &lt;- data.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_dev = x_dev,\n  Y_dev = y_dev,\n  X_dev_sq = x_dev^2,\n  Y_dev_sq = y_dev^2,\n  Product = x_dev * y_dev\n)\nprint(calc_table)\n\n   X  Y X_dev Y_dev X_dev_sq Y_dev_sq Product\n1  2 65    -4   -14       16      196      56\n2  4 70    -2    -9        4       81      18\n3  6 80     0     1        0        1       0\n4  8 85     2     6        4       36      12\n5 10 95     4    16       16      256      64\n\n# Summary statistics\ncat(\"\\nSummary:\")\n\n\nSummary:\n\ncat(\"\\nSum of (X-mean)¬≤:\", sum(x_dev^2))\n\n\nSum of (X-mean)¬≤: 40\n\ncat(\"\\nSum of (Y-mean)¬≤:\", sum(y_dev^2))\n\n\nSum of (Y-mean)¬≤: 570\n\ncat(\"\\nSum of products:\", sum(x_dev * y_dev))\n\n\nSum of products: 150\n\ncat(\"\\nStandard deviation of X:\", sd(study_hours))\n\n\nStandard deviation of X: 3.162278\n\ncat(\"\\nStandard deviation of Y:\", sd(test_scores))\n\n\nStandard deviation of Y: 11.93734\n\n# Calculate confidence interval and p-value\ncor_test &lt;- cor.test(study_hours, test_scores, method = \"pearson\")\nprint(cor_test)\n\n\n    Pearson's product-moment correlation\n\ndata:  study_hours and test_scores\nt = 15, df = 3, p-value = 0.0006431\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8994446 0.9995859\nsample estimates:\n      cor \n0.9933993 \n\n\nInterpretation: r = 0.994 indicates an almost perfect positive linear relationship between study hours and test scores. The p-value &lt; 0.05 suggests this relationship is statistically significant.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#spearman-rank-correlation",
    "href": "correg_en.html#spearman-rank-correlation",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.5 Spearman Rank Correlation",
    "text": "15.5 Spearman Rank Correlation\nSpearman correlation measures monotonic relationships using ranks instead of raw values.\nFormula: \\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2 - 1)}\nWhere d_i is the difference between ranks for observation i.\n\nComplete Manual Example\nData: Math and English Scores\n\n\n\nStudent\nMath Score\nEnglish Score\n\n\n\n\nA\n78\n82\n\n\nB\n85\n90\n\n\nC\n88\n88\n\n\nD\n92\n94\n\n\nE\n95\n96\n\n\n\nRanking and Calculation:\n\n\n\n\n\n\n\n\n\n\n\n\nStudent\nMath Score\nMath Rank\nEnglish Score\nEnglish Rank\nd = (Math Rank - English Rank)\nd¬≤\n\n\n\n\nA\n78\n1\n82\n1\n0\n0\n\n\nB\n85\n2\n90\n3\n-1\n1\n\n\nC\n88\n3\n88\n2\n1\n1\n\n\nD\n92\n4\n94\n4\n0\n0\n\n\nE\n95\n5\n96\n5\n0\n0\n\n\n\n\n\n\n\nSum:\n2\n\n\n\nCalculation: \\rho = 1 - \\frac{6 \\times 2}{5(25-1)} = 1 - \\frac{12}{120} = 1 - 0.1 = 0.9\n\n# Data\nmath_scores &lt;- c(78, 85, 88, 92, 95)\nenglish_scores &lt;- c(82, 90, 88, 94, 96)\n\n# Show ranks\nrank_table &lt;- data.frame(\n  Student = LETTERS[1:5],\n  Math_Score = math_scores,\n  Math_Rank = rank(math_scores),\n  English_Score = english_scores,\n  English_Rank = rank(english_scores)\n)\n\nrank_table$d &lt;- rank_table$Math_Rank - rank_table$English_Rank\nrank_table$d_squared &lt;- rank_table$d^2\n\nprint(rank_table)\n\n  Student Math_Score Math_Rank English_Score English_Rank  d d_squared\n1       A         78         1            82            1  0         0\n2       B         85         2            90            3 -1         1\n3       C         88         3            88            2  1         1\n4       D         92         4            94            4  0         0\n5       E         95         5            96            5  0         0\n\ncat(\"\\nSum of d¬≤:\", sum(rank_table$d_squared))\n\n\nSum of d¬≤: 2\n\n# Calculate Spearman correlation\nrho &lt;- cor(math_scores, english_scores, method = \"spearman\")\ncat(\"\\nSpearman correlation (R):\", round(rho, 3))\n\n\nSpearman correlation (R): 0.9\n\n# Manual calculation\nn &lt;- length(math_scores)\nrho_manual &lt;- 1 - (6 * sum(rank_table$d_squared)) / (n * (n^2 - 1))\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.9",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#cross-tabulation-and-categorical-data",
    "href": "correg_en.html#cross-tabulation-and-categorical-data",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.6 Cross-tabulation and Categorical Data",
    "text": "15.6 Cross-tabulation and Categorical Data\nCross-tabulation shows relationships between categorical variables.\n\n# Create more realistic sample data\nset.seed(123)\nn_total &lt;- 120\n\n# Create education and employment data with realistic relationship\neducation &lt;- factor(\n  c(rep(\"High School\", 40), \n    rep(\"College\", 50), \n    rep(\"Graduate\", 30)),\n  levels = c(\"High School\", \"College\", \"Graduate\")\n)\n\n# Employment status with education-related probabilities\nemployment &lt;- factor(\n  c(# High School: 75% employed\n    sample(c(rep(\"Employed\", 30), rep(\"Unemployed\", 10)), 40),\n    # College: 90% employed  \n    sample(c(rep(\"Employed\", 45), rep(\"Unemployed\", 5)), 50),\n    # Graduate: 95% employed\n    sample(c(rep(\"Employed\", 28), rep(\"Unemployed\", 2)), 30)),\n  levels = c(\"Employed\", \"Unemployed\")\n)\n\n# Create contingency table\ncontingency_table &lt;- table(education, employment)\nprint(\"Contingency Table:\")\n\n[1] \"Contingency Table:\"\n\nprint(contingency_table)\n\n             employment\neducation     Employed Unemployed\n  High School       30         10\n  College           45          5\n  Graduate          28          2\n\n# Calculate row percentages\nprint(\"\\nRow Percentages (% within each education level):\")\n\n[1] \"\\nRow Percentages (% within each education level):\"\n\nprint(round(prop.table(contingency_table, 1) * 100, 1))\n\n             employment\neducation     Employed Unemployed\n  High School     75.0       25.0\n  College         90.0       10.0\n  Graduate        93.3        6.7\n\n# Chi-square test for independence\nchi_test &lt;- chisq.test(contingency_table)\n\nWarning in chisq.test(contingency_table): Chi-squared approximation may be\nincorrect\n\nprint(chi_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 5.9623, df = 2, p-value = 0.05073",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#practical-exercises-with-solutions",
    "href": "correg_en.html#practical-exercises-with-solutions",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.7 Practical Exercises with Solutions",
    "text": "15.7 Practical Exercises with Solutions\n\nExercise 1: Calculate Pearson Correlation Manually\nData:\n\nHeight (inches): 66, 68, 70, 72, 74\nWeight (pounds): 140, 155, 170, 185, 200\n\nSolution:\n\n# Data\nheight &lt;- c(66, 68, 70, 72, 74)\nweight &lt;- c(140, 155, 170, 185, 200)\n\n# Step 1: Calculate means\nh_mean &lt;- mean(height)  # 70\nw_mean &lt;- mean(weight)  # 170\n\n# Step 2: Calculate deviations and products\ncalculation_df &lt;- data.frame(\n  Height = height,\n  Weight = weight,\n  H_dev = height - h_mean,\n  W_dev = weight - w_mean,\n  H_dev_sq = (height - h_mean)^2,\n  W_dev_sq = (weight - w_mean)^2,\n  Product = (height - h_mean) * (weight - w_mean)\n)\n\nprint(calculation_df)\n\n  Height Weight H_dev W_dev H_dev_sq W_dev_sq Product\n1     66    140    -4   -30       16      900     120\n2     68    155    -2   -15        4      225      30\n3     70    170     0     0        0        0       0\n4     72    185     2    15        4      225      30\n5     74    200     4    30       16      900     120\n\n# Step 3: Calculate correlation\ncov_hw &lt;- sum(calculation_df$Product) / (length(height) - 1)\nsd_h &lt;- sqrt(sum(calculation_df$H_dev_sq) / (length(height) - 1))\nsd_w &lt;- sqrt(sum(calculation_df$W_dev_sq) / (length(weight) - 1))\nr_manual &lt;- cov_hw / (sd_h * sd_w)\n\ncat(\"\\nManual calculation:\")\n\n\nManual calculation:\n\ncat(\"\\nCovariance:\", round(cov_hw, 2))\n\n\nCovariance: 75\n\ncat(\"\\nSD Height:\", round(sd_h, 2))\n\n\nSD Height: 3.16\n\ncat(\"\\nSD Weight:\", round(sd_w, 2))\n\n\nSD Weight: 23.72\n\ncat(\"\\nCorrelation:\", round(r_manual, 3))\n\n\nCorrelation: 1\n\n# Verify with R\ncat(\"\\n\\nR calculation:\", round(cor(height, weight), 3))\n\n\n\nR calculation: 1\n\n\n\n\nExercise 2: Calculate Spearman Correlation Manually\nData:\n\nStudent rankings in Math: 1, 3, 2, 5, 4\nStudent rankings in Science: 2, 4, 1, 5, 3\n\nSolution:\n\n# Rankings (already ranked)\nmath_rank &lt;- c(1, 3, 2, 5, 4)\nscience_rank &lt;- c(2, 4, 1, 5, 3)\n\n# Calculate differences\nd &lt;- math_rank - science_rank\nd_squared &lt;- d^2\n\n# Create table\nrank_df &lt;- data.frame(\n  Student = 1:5,\n  Math_Rank = math_rank,\n  Science_Rank = science_rank,\n  d = d,\n  d_squared = d_squared\n)\n\nprint(rank_df)\n\n  Student Math_Rank Science_Rank  d d_squared\n1       1         1            2 -1         1\n2       2         3            4 -1         1\n3       3         2            1  1         1\n4       4         5            5  0         0\n5       5         4            3  1         1\n\n# Calculate Spearman correlation\nn &lt;- length(math_rank)\nsum_d_sq &lt;- sum(d_squared)\nrho_manual &lt;- 1 - (6 * sum_d_sq) / (n * (n^2 - 1))\n\ncat(\"\\nSum of d¬≤:\", sum_d_sq)\n\n\nSum of d¬≤: 4\n\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.8\n\ncat(\"\\nSpearman correlation (R):\", round(cor(math_rank, science_rank, method = \"spearman\"), 3))\n\n\nSpearman correlation (R): 0.8\n\n\n\n\nExercise 3: Interpretation Practice\nInterpret these correlation values:\n\nr = 0.85 between hours of practice and performance score\n\nAnswer: Strong positive relationship. As practice hours increase, performance scores tend to increase substantially.\n\nr = -0.72 between outside temperature and heating costs\n\nAnswer: Strong negative relationship. As temperature increases, heating costs decrease substantially.\n\nr = 0.12 between shoe size and intelligence\n\nAnswer: Very weak/no meaningful relationship. Shoe size and intelligence are essentially unrelated.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#important-points-to-remember",
    "href": "correg_en.html#important-points-to-remember",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.8 Important Points to Remember",
    "text": "15.8 Important Points to Remember\n\nCorrelation measures relationship strength: Values range from -1 to +1\nCorrelation ‚â† Causation: High correlation doesn‚Äôt prove one variable causes another\nChoose the right method:\n\nPearson: For linear relationships in continuous data\nSpearman: For monotonic relationships or ranked data\n\nCheck assumptions:\n\nPearson assumes linear relationship and normal distribution\nSpearman only assumes monotonic relationship\n\nWatch for outliers: Extreme values can greatly affect Pearson correlation\nVisualize your data: Always plot before calculating correlation",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-decision-tree-for-correlation-analysis",
    "href": "correg_en.html#summary-decision-tree-for-correlation-analysis",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.9 Summary: Decision Tree for Correlation Analysis",
    "text": "15.9 Summary: Decision Tree for Correlation Analysis\n\n\n\nCHOOSING THE RIGHT CORRELATION METHOD:\n\nIs your data numerical?\n‚îú‚îÄ YES ‚Üí Is the relationship linear?\n‚îÇ   ‚îú‚îÄ YES ‚Üí Use PEARSON correlation\n‚îÇ   ‚îî‚îÄ NO ‚Üí Is it monotonic?\n‚îÇ       ‚îú‚îÄ YES ‚Üí Use SPEARMAN correlation\n‚îÇ       ‚îî‚îÄ NO ‚Üí Consider non-linear methods\n‚îî‚îÄ NO ‚Üí Is it ordinal (ranked)?\n    ‚îú‚îÄ YES ‚Üí Use SPEARMAN correlation\n    ‚îî‚îÄ NO ‚Üí Use CROSS-TABULATION for categorical data",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#additional-practice-problems",
    "href": "correg_en.html#additional-practice-problems",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.10 Additional Practice Problems",
    "text": "15.10 Additional Practice Problems\n\nProblem Set A: Manual Calculations\n\nCalculate covariance and Pearson correlation for:\n\nX: 10, 20, 30, 40, 50\nY: 15, 25, 35, 45, 55\n\nSolution: Cov(X,Y) = 250, r = 1.0 (perfect positive correlation)\nCalculate Spearman correlation for movie ratings:\n\nMovie A ratings: 8, 6, 9, 7, 5\nMovie B ratings: 7, 8, 9, 6, 4\n\nSolution: œÅ = 0.3 (weak positive correlation)\n\n\n\nProblem Set B: Interpretation\n\nA study finds r = 0.91 between hours of sleep and test performance.\n\nInterpretation: Very strong positive relationship suggesting that more sleep is associated with better test performance. However, this doesn‚Äôt prove causation‚Äîother factors might be involved.\n\nAnother study finds r = -0.03 between birth month and IQ scores.\n\nInterpretation: No meaningful relationship. Birth month and IQ are essentially unrelated.\n\n\n\n\nQuick Reference Card\n\n\n\n\n\n\n\n\n\nMeasure\nUse When\nFormula\nRange\n\n\n\n\nCovariance\nInitial exploration of relationship\n\\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n-‚àû to +‚àû\n\n\nPearson r\nLinear relationships, continuous data\n\\frac{\\text{cov}(X,Y)}{s_X s_Y}\n-1 to +1\n\n\nSpearman œÅ\nMonotonic relationships, ranked data\n1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n-1 to +1\n\n\nCross-tabs\nCategorical variables\nFrequency counts\nN/A",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#understanding-ordinary-least-squares-ols-a-quick-start-guide",
    "href": "correg_en.html#understanding-ordinary-least-squares-ols-a-quick-start-guide",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.11 Understanding Ordinary Least Squares (OLS): A Quick-start Guide",
    "text": "15.11 Understanding Ordinary Least Squares (OLS): A Quick-start Guide\n\n\n\n\n\n\nUnderstanding Ordinary Least Squares (OLS): A Quick-start Guide\n\n\n\n\nIntroduction: What is Regression Analysis?\nRegression analysis helps us understand and measure relationships between things we can observe. It provides mathematical tools to identify patterns in data that help us make predictions.\nConsider these research questions:\n\nHow does study time affect test scores?\nHow does experience affect salary?\nHow does advertising spending influence sales?\n\nRegression gives us systematic methods to answer these questions with real data.\n\n\nThe Starting Point: A Simple Example\nLet‚Äôs begin with something concrete. You‚Äôve collected data from 20 students in your class:\n\n\n\nStudent\nStudy Hours\nExam Score\n\n\n\n\nAlex\n2\n68\n\n\nBeth\n4\n74\n\n\nCarlos\n6\n85\n\n\nDiana\n8\n91\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\nWhen you plot this data, you get a scatter plot with dots all over. Your goal: find the straight line that best describes the relationship between study hours and exam scores.\nBut what does ‚Äúbest‚Äù mean? That‚Äôs what we‚Äôll discover.\n\n\nWhy Real Data Doesn‚Äôt Fall on a Perfect Line\nBefore diving into the math, let‚Äôs understand why data points don‚Äôt line up perfectly.\n\nDeterministic vs.¬†Stochastic Models\nDeterministic Models describe relationships with no uncertainty. Think of physics equations: \\text{Distance} = \\text{Speed} √ó \\text{Time}\nIf you drive at exactly 60 mph for exactly 2 hours, you‚Äôll always travel exactly 120 miles. No variation, no exceptions.\nStochastic Models acknowledge that real-world data contains natural variation. The fundamental structure is: Y = f(X) + \\epsilon\nWhere:\n\nY is what we‚Äôre trying to predict (exam scores)\nf(X) is the systematic pattern (how study hours typically affect scores)\n\\epsilon (epsilon) represents all the random stuff we can‚Äôt measure\n\nIn our example, two students might study for 5 hours but get different scores because:\n\nOne slept better the night before\nOne is naturally better at test-taking\nOne had a noisy roommate during the exam\nPure chance in which questions were asked\n\nThis randomness is natural and expected - that‚Äôs what \\epsilon captures.\n\n\n\nThe Simple Linear Regression Model\nWe express the relationship between study hours and exam scores as: Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\nLet‚Äôs decode this:\n\nY_i = exam score for student i\nX_i = study hours for student i\n\\beta_0 = the intercept (baseline score with zero study hours)\n\\beta_1 = the slope (points gained per study hour)\n\\epsilon_i = everything else affecting student i‚Äôs score\n\nKey insight: We never know the true values of \\beta_0 and \\beta_1. Instead, we use our data to estimate them, calling our estimates \\hat{\\beta}_0 and \\hat{\\beta}_1 (the ‚Äúhats‚Äù mean ‚Äúestimated‚Äù).\n\n\nUnderstanding Residuals: How Wrong Are Our Predictions?\nOnce we draw a line through our data, we can make predictions. For each student:\n\nActual score (y_i): What they really got\nPredicted score (\\hat{y}_i): What our line says they should have gotten\nResidual (e_i): The difference = Actual - Predicted\n\nVisual Example:\nDiana: Studied 8 hours, scored 91\nOur line predicts: 88 points\nResidual: 91 - 88 = +3 points (we underestimated)\n\nEric: Studied 5 hours, scored 70\nOur line predicts: 79 points  \nResidual: 70 - 79 = -9 points (we overestimated)\n\n\nThe Key Insight: Why Square the Residuals?\nHere‚Äôs a puzzle. Consider these residuals from four students:\n\nStudent A: +5 points\nStudent B: -5 points\nStudent C: +3 points\nStudent D: -3 points\n\nIf we just add them: (+5) + (-5) + (+3) + (-3) = 0\nThis suggests perfect predictions, but every prediction was wrong! The positive and negative errors canceled out.\nThe solution: Square each residual before adding:\n\nStudent A: (+5)^2 = 25\nStudent B: (-5)^2 = 25\nStudent C: (+3)^2 = 9\nStudent D: (-3)^2 = 9\nTotal squared error: 68\n\nWhy squaring works:\n\nNo more cancellation: All squared values are positive\nBigger errors matter more: A 10-point error counts 4√ó as much as a 5-point error\nMathematical convenience: Squared functions are smooth and differentiable\n\n\n\nThe Ordinary Least Squares Method\nOLS finds the line that minimizes the Sum of Squared Errors (SSE):\n\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\nExpanding this: \\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1x_i))^2\nIn plain English: ‚ÄúFind the intercept and slope that make the total squared prediction error as small as possible.‚Äù\n\n\nThe Mathematical Solution (Formal Derivation)\nTo minimize SSE, we use calculus. Taking partial derivatives and setting them to zero:\n\\frac{\\partial SSE}{\\partial \\beta_0} = -2\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i) = 0\n\\frac{\\partial SSE}{\\partial \\beta_1} = -2\\sum_{i=1}^n x_i(y_i - \\beta_0 - \\beta_1x_i) = 0\nSolving this system of equations yields:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\nWhere \\bar{x} and \\bar{y} are the sample means.\nWhat this tells us:\n\nThe slope depends on how X and Y vary together (covariance) relative to how much X varies alone (variance)\nThe line always passes through the center point (\\bar{x}, \\bar{y})\n\n\n\nMaking Sense of Variation: How Good Is Our Line?\nTo evaluate our model‚Äôs performance, we break down the variation in exam scores:\n\nTotal Sum of Squares (SST)\n‚ÄúHow much do exam scores vary overall?‚Äù SST = \\sum_{i=1}^n(y_i - \\bar{y})^2\nThis measures how spread out the scores are from the class average.\n\n\nRegression Sum of Squares (SSR)\n‚ÄúHow much variation does our line explain?‚Äù SSR = \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2\nThis measures how much better our predictions are than just guessing the average for everyone.\n\n\nError Sum of Squares (SSE)\n‚ÄúHow much variation is left unexplained?‚Äù SSE = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\nThis is the variation our model couldn‚Äôt capture (the squared residuals).\n\n\nThe Fundamental Equation\nSST = SSR + SSE \\text{Total Variation} = \\text{Explained} + \\text{Unexplained}\n\n\n\nR-Squared: The Model Report Card\nThe coefficient of determination (R¬≤) tells us what percentage of variation our model explains:\nR^2 = \\frac{\\text{Explained Variation}}{\\text{Total Variation}} = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nHow to interpret R¬≤:\n\nR¬≤ = 0.75: ‚ÄúStudy hours explain 75% of the variation in exam scores‚Äù\nR¬≤ = 0.30: ‚ÄúOur model captures 30% of what makes scores different‚Äù\nR¬≤ = 1.00: Perfect prediction (never happens with real data)\nR¬≤ = 0.00: Our line is no better than guessing the average\n\nImportant reality check: In social sciences, R¬≤ = 0.30 might be excellent. In engineering, R¬≤ = 0.95 might be the minimum acceptable. Context matters.\n\n\nInterpreting Your Results\nWhen you run OLS and get \\hat{\\beta}_0 = 60 and \\hat{\\beta}_1 = 4:\nThe Slope (\\hat{\\beta}_1 = 4):\n\n‚ÄúEach additional hour of study is associated with 4 more points on the exam‚Äù\nThis is an average effect across all students\nIt‚Äôs not a guarantee for any individual student\n\nThe Intercept (\\hat{\\beta}_0 = 60):\n\n‚ÄúA student who studies 0 hours is predicted to score 60‚Äù\nOften this is just a mathematical anchor point\nMay not make practical sense (who studies 0 hours?)\n\nThe Prediction Equation: \\text{Predicted Score} = 60 + 4 \\times \\text{Study Hours}\nSo a student studying 5 hours: Predicted score = 60 + 4(5) = 80 points\n\n\nEffect Size and Practical Significance\nStatistical significance tells us whether an effect exists. Practical significance tells us whether it matters. Understanding both is crucial for proper interpretation.\n\nCalculating and Interpreting Raw Effect Sizes\nThe raw (unstandardized) effect size is simply your slope coefficient \\hat{\\beta}_1.\nExample: If \\hat{\\beta}_1 = 4 points per hour:\n\nThis is the raw effect size\nInterpretation: ‚ÄúOne hour of additional study yields 4 exam points‚Äù\n\nTo assess practical significance, consider:\n\nScale of the outcome: 4 points on a 100-point exam (4%) vs.¬†4 points on a 500-point exam (0.8%)\nCost of the intervention: Is one hour of study time worth 4 points?\nContext-specific thresholds: Does 4 points change a letter grade?\n\n\n\nCalculating Standardized Effect Sizes\nStandardized effects allow comparison across different scales and studies.\nFormula for standardized coefficient (beta weight): \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y}\nWhere:\n\ns_X = standard deviation of X (study hours)\ns_Y = standard deviation of Y (exam scores)\n\nStep-by-step calculation:\n\nCalculate the standard deviation of X: s_X = \\sqrt{\\frac{\\sum(x_i - \\bar{x})^2}{n-1}}\nCalculate the standard deviation of Y: s_Y = \\sqrt{\\frac{\\sum(y_i - \\bar{y})^2}{n-1}}\nMultiply: \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y}\n\nExample calculation:\n\nSuppose s_X = 2.5 hours and s_Y = 12 points\nWith \\hat{\\beta}_1 = 4: \\beta_{std} = 4 \\times \\frac{2.5}{12} = 0.83\nInterpretation: ‚ÄúA one standard deviation increase in study hours (2.5 hours) is associated with 0.83 standard deviations increase in exam score‚Äù\n\n\n\nCohen‚Äôs Guidelines for Effect Sizes\nFor standardized regression coefficients:\n\nSmall effect: |Œ≤| ‚âà 0.10 (explains ~1% of variance)\nMedium effect: |Œ≤| ‚âà 0.30 (explains ~9% of variance)\nLarge effect: |Œ≤| ‚âà 0.50 (explains ~25% of variance)\n\nFor R¬≤ (proportion of variance explained):\n\nSmall effect: R¬≤ ‚âà 0.02\nMedium effect: R¬≤ ‚âà 0.13\nLarge effect: R¬≤ ‚âà 0.26\n\nImportant: These are general benchmarks. Field-specific standards often differ:\n\nPsychology/Education: R¬≤ = 0.10 might be meaningful\nPhysics/Engineering: R¬≤ &lt; 0.90 might be unacceptable\nEconomics: R¬≤ = 0.30 might be excellent\n\n\n\nCalculating Confidence Intervals for Effect Sizes\nTo quantify uncertainty in your effect size:\nFor the raw coefficient: CI = \\hat{\\beta}_1 \\pm t_{critical} \\times SE(\\hat{\\beta}_1)\nWhere:\n\nt_{critical} = critical value from t-distribution (usually ‚âà 2 for 95% CI)\nSE(\\hat{\\beta}_1) = standard error of the slope\n\nPractical interpretation: If 95% CI = [3.2, 4.8], we can say: ‚ÄúWe‚Äôre 95% confident that each study hour adds between 3.2 and 4.8 exam points.‚Äù\n\n\nMaking Decisions About Practical Significance\nTo determine if an effect is practically significant, consider:\n\nMinimum meaningful difference: What‚Äôs the smallest effect that would matter?\n\nIn education: Often 0.25 standard deviations\nIn medicine: Determined by clinical relevance\nIn business: Based on cost-benefit analysis\n\nNumber needed to treat (NNT) analog: How much X must change for meaningful Y change?\n\nIf passing requires 10 more points and \\hat{\\beta}_1 = 4\nStudents need 2.5 more study hours to pass\n\nCost-effectiveness ratio: \\text{Efficiency} = \\frac{\\text{Effect Size}}{\\text{Cost of Intervention}}\n\nExample practical significance assessment:\n\nEffect: 4 points per study hour\nPassing threshold: 70 points\nCurrent average: 68 points\nConclusion: 30 minutes of extra study could change fail to pass\nDecision: Practically significant for borderline students\n\n\n\n\nUnderstanding Uncertainty: Nothing Is Perfect\nYour estimates come from a sample, not the entire population. This creates uncertainty.\n\nWhy We Have Uncertainty\n\nYou studied 20 students, not all students ever\nYour sample might be slightly unusual by chance\nMeasurement isn‚Äôt perfect (did students report hours accurately?)\n\n\n\nConfidence Intervals: Being Honest About Uncertainty\nInstead of saying ‚Äúthe effect is exactly 4 points per hour,‚Äù we say:\n\n‚ÄúWe estimate 4 points per hour‚Äù\n‚ÄúWe‚Äôre 95% confident the true effect is between 3.2 and 4.8 points‚Äù\n\nThis range (3.2 to 4.8) is called a 95% confidence interval.\nWhat it means: If we repeated this study many times with different samples, 95% of the intervals we calculate would contain the true effect.\nWhat it doesn‚Äôt mean: There‚Äôs a 95% chance the true value is in this specific interval (it either is or isn‚Äôt).\n\n\nTesting If There‚Äôs Really a Relationship\nThe big question: ‚ÄúIs there actually a relationship, or did we just get lucky with our sample?‚Äù\nWe test this by asking: ‚ÄúIf study hours truly had zero effect on scores, how likely would we be to see a pattern this strong just by chance?‚Äù\nThe process (simplified):\n\nAssume there‚Äôs no relationship (the ‚Äúnull hypothesis‚Äù)\nCalculate how unlikely our data would be if that were true\nIf it‚Äôs very unlikely (typically less than 5% chance), we conclude there probably is a relationship\n\nP-values in plain English:\n\np = 0.03: ‚ÄúIf study hours didn‚Äôt matter at all, there‚Äôs only a 3% chance we‚Äôd see a pattern this strong by luck‚Äù\np = 0.40: ‚ÄúThis pattern could easily happen by chance even if there‚Äôs no real relationship‚Äù\n\nRule of thumb: p &lt; 0.05 ‚Üí ‚Äústatistically significant‚Äù (probably a real relationship)\n\n\n\nWhen Things Go Wrong: Model Diagnostics\n\nQuick Visual Checks\n\nPlot your data first: Does it look roughly linear?\nPlot residuals vs.¬†predicted values: Should look like a random cloud\nLook for outliers: Any points way off from the others?\n\n\n\nWarning Signs Your Model Might Be Misleading\nPattern in residuals: If residuals show a curve or trend, you‚Äôre missing something\nIncreasing spread: If residuals get more spread out as predictions increase, standard errors might be wrong\nInfluential outliers: One or two weird points can drag your whole line off\nMissing variables: If you forgot something important (like prior knowledge), your estimates might be biased\n\n\n\nKey Assumptions: When OLS Works Well\n\nLinearity: The true relationship is approximately straight\n\nCheck: Look at your scatter plot\n\nIndependence: Each observation is separate\n\nCheck: Make sure students didn‚Äôt work together or copy\n\nConstant variance: The spread of residuals is similar everywhere\n\nCheck: Residual plot shouldn‚Äôt fan out\n\nNo perfect multicollinearity: (For multiple regression) Predictors aren‚Äôt perfectly related\n\nCheck: Make sure you didn‚Äôt include the same variable twice\n\nRandom sampling: Your data represents the population you care about\n\nCheck: Did you sample fairly?\n\n\n\n\nSummary: Your OLS Toolkit\nWhat OLS Does:\n\nFinds the straight line that minimizes squared prediction errors\nEstimates how much Y changes when X changes by one unit\nTells you how much variation your model explains (R¬≤)\nQuantifies uncertainty in your estimates\n\nYour Step-by-Step Process:\n\nPlot your data - does a line make sense?\nRun OLS to get \\hat{\\beta}_0 and \\hat{\\beta}_1\nCheck R¬≤ - how much variation do you explain?\nCalculate effect sizes (raw and standardized)\nAssess practical significance using context-specific criteria\nLook at confidence intervals - how uncertain are you?\nCheck residuals - any obvious problems?\nMake decisions based on both statistical and practical significance\n\nKey Interpretations:\n\nSlope: ‚ÄúWhen X increases by 1, Y changes by [slope] on average‚Äù\nStandardized slope: ‚ÄúWhen X increases by 1 SD, Y changes by [Œ≤_std] SDs‚Äù\nR¬≤: ‚ÄúX explains [R¬≤√ó100]% of the variation in Y‚Äù\nP-value: ‚ÄúIf there were no relationship, we‚Äôd see this pattern [p√ó100]% of the time by chance‚Äù\nConfidence interval: ‚ÄúWe‚Äôre 95% confident the true effect is between [lower] and [upper]‚Äù\n\nCritical Reminders:\n\nAssociation does not imply causation\nStatistical significance does not guarantee practical importance\nEvery model is wrong, but some are useful\nAlways visualize your data and residuals\nConsider both effect size and uncertainty when making decisions\n\nOLS provides a principled, mathematical approach to finding patterns in real-world data. While it cannot provide perfect predictions, it offers the best linear approximation possible along with honest assessments of that approximation‚Äôs quality and uncertainty.\n\n\n15.12 Complete Manual OLS Calculation: A Step-by-Step Example\nA professor wants to understand the relationship between hours spent studying and exam scores. She collects data from 6 students:\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nOur goal: Find the best-fitting line \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X using OLS.\n\n\n15.13 Step 1: Calculate the Means\nFirst, we need the mean of X and Y.\nFor X (study hours): \\bar{X} = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5\nFor Y (exam scores): \\bar{Y} = \\frac{65 + 70 + 75 + 85 + 88 + 95}{6} = \\frac{478}{6} = 79.67\n\n\n15.14 Step 2: Calculate Deviations from Means\nFor each observation, calculate (X_i - \\bar{X}) and (Y_i - \\bar{Y}):\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n1 - 3.5 = -2.5\n65 - 79.67 = -14.67\n\n\nB\n2\n70\n2 - 3.5 = -1.5\n70 - 79.67 = -9.67\n\n\nC\n3\n75\n3 - 3.5 = -0.5\n75 - 79.67 = -4.67\n\n\nD\n4\n85\n4 - 3.5 = 0.5\n85 - 79.67 = 5.33\n\n\nE\n5\n88\n5 - 3.5 = 1.5\n88 - 79.67 = 8.33\n\n\nF\n6\n95\n6 - 3.5 = 2.5\n95 - 79.67 = 15.33\n\n\n\n\n\n15.15 Step 3: Calculate Products and Squares\nNow calculate (X_i - \\bar{X})(Y_i - \\bar{Y}) and (X_i - \\bar{X})^2:\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n(-2.5)(-14.67) = 36.68\n(-2.5)¬≤ = 6.25\n\n\nB\n(-1.5)(-9.67) = 14.51\n(-1.5)¬≤ = 2.25\n\n\nC\n(-0.5)(-4.67) = 2.34\n(-0.5)¬≤ = 0.25\n\n\nD\n(0.5)(5.33) = 2.67\n(0.5)¬≤ = 0.25\n\n\nE\n(1.5)(8.33) = 12.50\n(1.5)¬≤ = 2.25\n\n\nF\n(2.5)(15.33) = 38.33\n(2.5)¬≤ = 6.25\n\n\nSum\n107.03\n17.50\n\n\n\n\n\n15.16 Step 4: Calculate the Slope (\\hat{\\beta}_1)\nUsing the OLS formula: \\hat{\\beta}_1 = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} = \\frac{107.03}{17.50} = 6.12\nInterpretation: Each additional hour of study is associated with a 6.12-point increase in exam score.\n\n\n15.17 Step 5: Calculate the Intercept (\\hat{\\beta}_0)\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X} = 79.67 - (6.12 \\times 3.5) = 79.67 - 21.42 = 58.25\nInterpretation: A student who studies 0 hours is predicted to score 58.25 points.\n\n\n15.18 Step 6: Write the Regression Equation\n\\hat{Y} = 58.25 + 6.12X\n\n\n15.19 Step 7: Calculate Predicted Values and Residuals\nUsing our equation to predict each student‚Äôs score:\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nResidual e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n58.25 + 6.12(1) = 64.37\n65 - 64.37 = 0.63\n\n\nB\n2\n70\n58.25 + 6.12(2) = 70.49\n70 - 70.49 = -0.49\n\n\nC\n3\n75\n58.25 + 6.12(3) = 76.61\n75 - 76.61 = -1.61\n\n\nD\n4\n85\n58.25 + 6.12(4) = 82.73\n85 - 82.73 = 2.27\n\n\nE\n5\n88\n58.25 + 6.12(5) = 88.85\n88 - 88.85 = -0.85\n\n\nF\n6\n95\n58.25 + 6.12(6) = 94.97\n95 - 94.97 = 0.03\n\n\n\nCheck: Sum of residuals = 0.63 - 0.49 - 1.61 + 2.27 - 0.85 + 0.03 ‚âà 0 ‚úì\n\n\n15.20 Step 8: Calculate Sum of Squares\n\nTotal Sum of Squares (SST)\nHow much total variation exists in exam scores?\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n(65 - 79.67)¬≤ = (-14.67)¬≤ = 215.21\n\n\nB\n70\n(70 - 79.67)¬≤ = (-9.67)¬≤ = 93.51\n\n\nC\n75\n(75 - 79.67)¬≤ = (-4.67)¬≤ = 21.81\n\n\nD\n85\n(85 - 79.67)¬≤ = (5.33)¬≤ = 28.41\n\n\nE\n88\n(88 - 79.67)¬≤ = (8.33)¬≤ = 69.39\n\n\nF\n95\n(95 - 79.67)¬≤ = (15.33)¬≤ = 235.01\n\n\nSum\n\nSST = 663.34\n\n\n\n\n\nRegression Sum of Squares (SSR)\nHow much variation does our model explain?\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n(64.37 - 79.67)¬≤ = (-15.30)¬≤ = 234.09\n\n\nB\n70.49\n(70.49 - 79.67)¬≤ = (-9.18)¬≤ = 84.27\n\n\nC\n76.61\n(76.61 - 79.67)¬≤ = (-3.06)¬≤ = 9.36\n\n\nD\n82.73\n(82.73 - 79.67)¬≤ = (3.06)¬≤ = 9.36\n\n\nE\n88.85\n(88.85 - 79.67)¬≤ = (9.18)¬≤ = 84.27\n\n\nF\n94.97\n(94.97 - 79.67)¬≤ = (15.30)¬≤ = 234.09\n\n\nSum\n\nSSR = 655.44\n\n\n\n\n\nError Sum of Squares (SSE)\nHow much variation is unexplained?\n\n\n\nStudent\nResidual e_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n-0.49\n0.24\n\n\nC\n-1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n-0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSum\n\nSSE = 9.10\n\n\n\nVerification: SST = SSR + SSE 663.34 ‚âà 655.44 + 9.10 = 664.54 ‚úì (small rounding difference)\n\n\n\n15.21 Step 9: Calculate R-Squared\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternative formula: R^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 1 - 0.014 = 0.986\n(Small difference due to rounding)\nInterpretation: Study hours explain 98.8% of the variation in exam scores. This is an extremely strong relationship.\n\n\n15.22 Step 10: Calculate Effect Sizes\n\nRaw Effect Size\nThe raw effect size is simply the slope: 6.12 points per hour\n\n\nStandardized Effect Size\nFirst, calculate standard deviations:\nFor X: s_X = \\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{n-1}} = \\sqrt{\\frac{17.50}{5}} = \\sqrt{3.50} = 1.87\nFor Y: s_Y = \\sqrt{\\frac{\\sum(Y_i - \\bar{Y})^2}{n-1}} = \\sqrt{\\frac{663.34}{5}} = \\sqrt{132.67} = 11.52\nStandardized coefficient: \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y} = 6.12 \\times \\frac{1.87}{11.52} = 6.12 \\times 0.162 = 0.99\nInterpretation: A one standard deviation increase in study hours (1.87 hours) is associated with a 0.99 standard deviation increase in exam score.\nAccording to Cohen‚Äôs guidelines:\n\nSmall effect: |Œ≤| = 0.10\nMedium effect: |Œ≤| = 0.30\nLarge effect: |Œ≤| = 0.50\n\nOur standardized effect of 0.99 is nearly twice Cohen‚Äôs ‚Äúlarge effect‚Äù threshold.\n\n\n\n15.23 Step 11: Practical Significance Assessment\n\nContext Analysis\n\nScale consideration:\n\nEffect: 6.12 points per hour\nExam scale: 0-100 points\nPercentage impact: 6.12% per hour\n\nPractical thresholds:\n\nLetter grade difference: Often 10 points\nTime to improve one letter grade: 10/6.12 = 1.63 hours\nConclusion: Less than 2 hours of extra study could change a letter grade\n\nCost-benefit analysis:\n\nBenefit: 6.12 points per hour\nCost: 1 hour of time\nDecision: Highly favorable return on investment\n\n\n\n\n\n15.24 Summary of Results\nRegression equation: \\hat{Y} = 58.25 + 6.12X\nKey statistics:\n\nSlope (\\hat{\\beta}_1): 6.12 points/hour\nIntercept (\\hat{\\beta}_0): 58.25 points\nR¬≤: 0.988 (98.8% of variance explained)\nStandardized effect: 0.99 (very large effect)\n\nPractical interpretation:\n\nEach hour of study adds about 6 points to exam score\nThe model fits extremely well (R¬≤ near 1)\nThe effect is both statistically and practically significant\nStudents can meaningfully improve grades with modest increases in study time\n\n\n\n15.25 Verification Check\nTo verify our calculations, let‚Äôs check that the regression line passes through (\\bar{X}, \\bar{Y}):\n\\hat{Y} = 58.25 + 6.12(3.5) = 58.25 + 21.42 = 79.67 = \\bar{Y} ‚úì\nThe calculation confirms our regression line passes through the point of means, as it should.\n\n\n15.26 R Code to Verify Manual Calculations\nBelow is R code that checks all our manual calculations. You can run this code to confirm every step.\n\n# Step 1: Create the data\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)  # X variable\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95)  # Y variable\nn &lt;- length(study_hours)  # Sample size\n\n# Create a data frame for easier handling\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Step 2: Calculate means\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Step 3: Calculate deviations from means\ndata$x_dev &lt;- data$X - x_bar  # X deviations\ndata$y_dev &lt;- data$Y - y_bar  # Y deviations\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Step 4: Calculate products and squares for OLS formula\ndata$xy_product &lt;- data$x_dev * data$y_dev  # (Xi - XÃÑ)(Yi - »≤)\ndata$x_dev_sq &lt;- data$x_dev^2  # (Xi - XÃÑ)¬≤\n\n# Sum of products and squares\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - XÃÑ)(Yi - »≤):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - XÃÑ)(Yi - »≤): 107 \n\ncat(\"Sum of (Xi - XÃÑ)¬≤:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - XÃÑ)¬≤: 17.5 \n\n# Step 5: Calculate slope (beta_1) manually\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (Œ≤‚ÇÅ) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (Œ≤‚ÇÅ) calculated manually: 6.11 \n\n# Step 6: Calculate intercept (beta_0) manually\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (Œ≤‚ÇÄ) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (Œ≤‚ÇÄ) calculated manually: 58.27 \n\n# Step 7: Compare with R's lm() function\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Step 8: Calculate predicted values and residuals\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X  # Predicted values\ndata$residual &lt;- data$Y - data$Y_hat  # Residuals\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ‚âà 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ‚âà 0): 0 \n\n# Step 9: Calculate Sum of Squares\n# Total Sum of Squares (SST)\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\n# Regression Sum of Squares (SSR)\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\n# Error Sum of Squares (SSE)\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\n# Verify that SST = SSR + SSE\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Step 10: Calculate R-squared\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R¬≤ (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR¬≤ (Method 1: SSR/SST): 0.9863 \n\ncat(\"R¬≤ (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR¬≤ (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R¬≤ (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR¬≤ (from lm function): 0.9863 \n\n# Step 11: Calculate Effect Sizes\n# Raw effect size (just the slope)\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\n# Standard deviations for standardized effect\nsd_x &lt;- sd(data$X)  # Standard deviation of X\nsd_y &lt;- sd(data$Y)  # Standard deviation of Y\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\n# Standardized effect size\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Correlation coefficient (should equal sqrt(R¬≤) for simple regression)\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R¬≤:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R¬≤: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n# Step 12: Create visualization\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\n# Plot the data and regression line\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5)\n\n# Add the regression line\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Add the mean point\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Add vertical lines for residuals\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\n# Add legend\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# Add the equation to the plot\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R¬≤ = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\n\n\n\n# Final summary\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )\n\n\n\nRunning the Code\nTo run this R code:\n\nCopy the entire code block above\nPaste it into RStudio or any R console\nExecute the code\nCompare the output with our manual calculations\n\nThe code will:\n\nRecreate all our manual calculations step by step\nVerify results using R‚Äôs built-in lm() function\nGenerate a visualization of the data with the regression line\nDisplay all intermediate calculations with clear labels\n\n\n\nExpected Output Highlights\nWhen you run this code, you should see:\n\nSlope: 6.12 (matching our manual calculation)\nIntercept: 58.25 (matching our manual calculation)\nR¬≤: 0.988 (matching our manual calculation)\nStandardized effect: 0.99 (matching our manual calculation)\nA plot showing the data points, regression line, and residuals\n\nThis verification confirms that our pen-and-paper calculations were correct!",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#complete-manual-ols-calculation-a-step-by-step-example",
    "href": "correg_en.html#complete-manual-ols-calculation-a-step-by-step-example",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.12 Complete Manual OLS Calculation: A Step-by-Step Example",
    "text": "15.12 Complete Manual OLS Calculation: A Step-by-Step Example\nA professor wants to understand the relationship between hours spent studying and exam scores. She collects data from 6 students:\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nOur goal: Find the best-fitting line \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X using OLS.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-1-calculate-the-means",
    "href": "correg_en.html#step-1-calculate-the-means",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.13 Step 1: Calculate the Means",
    "text": "15.13 Step 1: Calculate the Means\nFirst, we need the mean of X and Y.\nFor X (study hours): \\bar{X} = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5\nFor Y (exam scores): \\bar{Y} = \\frac{65 + 70 + 75 + 85 + 88 + 95}{6} = \\frac{478}{6} = 79.67",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-2-calculate-deviations-from-means",
    "href": "correg_en.html#step-2-calculate-deviations-from-means",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.14 Step 2: Calculate Deviations from Means",
    "text": "15.14 Step 2: Calculate Deviations from Means\nFor each observation, calculate (X_i - \\bar{X}) and (Y_i - \\bar{Y}):\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n1 - 3.5 = -2.5\n65 - 79.67 = -14.67\n\n\nB\n2\n70\n2 - 3.5 = -1.5\n70 - 79.67 = -9.67\n\n\nC\n3\n75\n3 - 3.5 = -0.5\n75 - 79.67 = -4.67\n\n\nD\n4\n85\n4 - 3.5 = 0.5\n85 - 79.67 = 5.33\n\n\nE\n5\n88\n5 - 3.5 = 1.5\n88 - 79.67 = 8.33\n\n\nF\n6\n95\n6 - 3.5 = 2.5\n95 - 79.67 = 15.33",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-3-calculate-products-and-squares",
    "href": "correg_en.html#step-3-calculate-products-and-squares",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.15 Step 3: Calculate Products and Squares",
    "text": "15.15 Step 3: Calculate Products and Squares\nNow calculate (X_i - \\bar{X})(Y_i - \\bar{Y}) and (X_i - \\bar{X})^2:\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n(-2.5)(-14.67) = 36.68\n(-2.5)¬≤ = 6.25\n\n\nB\n(-1.5)(-9.67) = 14.51\n(-1.5)¬≤ = 2.25\n\n\nC\n(-0.5)(-4.67) = 2.34\n(-0.5)¬≤ = 0.25\n\n\nD\n(0.5)(5.33) = 2.67\n(0.5)¬≤ = 0.25\n\n\nE\n(1.5)(8.33) = 12.50\n(1.5)¬≤ = 2.25\n\n\nF\n(2.5)(15.33) = 38.33\n(2.5)¬≤ = 6.25\n\n\nSum\n107.03\n17.50",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-4-calculate-the-slope-hatbeta_1",
    "href": "correg_en.html#step-4-calculate-the-slope-hatbeta_1",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.16 Step 4: Calculate the Slope (\\hat{\\beta}_1)",
    "text": "15.16 Step 4: Calculate the Slope (\\hat{\\beta}_1)\nUsing the OLS formula: \\hat{\\beta}_1 = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} = \\frac{107.03}{17.50} = 6.12\nInterpretation: Each additional hour of study is associated with a 6.12-point increase in exam score.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-5-calculate-the-intercept-hatbeta_0",
    "href": "correg_en.html#step-5-calculate-the-intercept-hatbeta_0",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.17 Step 5: Calculate the Intercept (\\hat{\\beta}_0)",
    "text": "15.17 Step 5: Calculate the Intercept (\\hat{\\beta}_0)\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X} = 79.67 - (6.12 \\times 3.5) = 79.67 - 21.42 = 58.25\nInterpretation: A student who studies 0 hours is predicted to score 58.25 points.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-6-write-the-regression-equation",
    "href": "correg_en.html#step-6-write-the-regression-equation",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.18 Step 6: Write the Regression Equation",
    "text": "15.18 Step 6: Write the Regression Equation\n\\hat{Y} = 58.25 + 6.12X",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-7-calculate-predicted-values-and-residuals",
    "href": "correg_en.html#step-7-calculate-predicted-values-and-residuals",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.19 Step 7: Calculate Predicted Values and Residuals",
    "text": "15.19 Step 7: Calculate Predicted Values and Residuals\nUsing our equation to predict each student‚Äôs score:\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nResidual e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n58.25 + 6.12(1) = 64.37\n65 - 64.37 = 0.63\n\n\nB\n2\n70\n58.25 + 6.12(2) = 70.49\n70 - 70.49 = -0.49\n\n\nC\n3\n75\n58.25 + 6.12(3) = 76.61\n75 - 76.61 = -1.61\n\n\nD\n4\n85\n58.25 + 6.12(4) = 82.73\n85 - 82.73 = 2.27\n\n\nE\n5\n88\n58.25 + 6.12(5) = 88.85\n88 - 88.85 = -0.85\n\n\nF\n6\n95\n58.25 + 6.12(6) = 94.97\n95 - 94.97 = 0.03\n\n\n\nCheck: Sum of residuals = 0.63 - 0.49 - 1.61 + 2.27 - 0.85 + 0.03 ‚âà 0 ‚úì",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-8-calculate-sum-of-squares",
    "href": "correg_en.html#step-8-calculate-sum-of-squares",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.20 Step 8: Calculate Sum of Squares",
    "text": "15.20 Step 8: Calculate Sum of Squares\n\nTotal Sum of Squares (SST)\nHow much total variation exists in exam scores?\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n(65 - 79.67)¬≤ = (-14.67)¬≤ = 215.21\n\n\nB\n70\n(70 - 79.67)¬≤ = (-9.67)¬≤ = 93.51\n\n\nC\n75\n(75 - 79.67)¬≤ = (-4.67)¬≤ = 21.81\n\n\nD\n85\n(85 - 79.67)¬≤ = (5.33)¬≤ = 28.41\n\n\nE\n88\n(88 - 79.67)¬≤ = (8.33)¬≤ = 69.39\n\n\nF\n95\n(95 - 79.67)¬≤ = (15.33)¬≤ = 235.01\n\n\nSum\n\nSST = 663.34\n\n\n\n\n\nRegression Sum of Squares (SSR)\nHow much variation does our model explain?\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n(64.37 - 79.67)¬≤ = (-15.30)¬≤ = 234.09\n\n\nB\n70.49\n(70.49 - 79.67)¬≤ = (-9.18)¬≤ = 84.27\n\n\nC\n76.61\n(76.61 - 79.67)¬≤ = (-3.06)¬≤ = 9.36\n\n\nD\n82.73\n(82.73 - 79.67)¬≤ = (3.06)¬≤ = 9.36\n\n\nE\n88.85\n(88.85 - 79.67)¬≤ = (9.18)¬≤ = 84.27\n\n\nF\n94.97\n(94.97 - 79.67)¬≤ = (15.30)¬≤ = 234.09\n\n\nSum\n\nSSR = 655.44\n\n\n\n\n\nError Sum of Squares (SSE)\nHow much variation is unexplained?\n\n\n\nStudent\nResidual e_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n-0.49\n0.24\n\n\nC\n-1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n-0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSum\n\nSSE = 9.10\n\n\n\nVerification: SST = SSR + SSE 663.34 ‚âà 655.44 + 9.10 = 664.54 ‚úì (small rounding difference)",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-9-calculate-r-squared",
    "href": "correg_en.html#step-9-calculate-r-squared",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.21 Step 9: Calculate R-Squared",
    "text": "15.21 Step 9: Calculate R-Squared\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternative formula: R^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 1 - 0.014 = 0.986\n(Small difference due to rounding)\nInterpretation: Study hours explain 98.8% of the variation in exam scores. This is an extremely strong relationship.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-10-calculate-effect-sizes",
    "href": "correg_en.html#step-10-calculate-effect-sizes",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.22 Step 10: Calculate Effect Sizes",
    "text": "15.22 Step 10: Calculate Effect Sizes\n\nRaw Effect Size\nThe raw effect size is simply the slope: 6.12 points per hour\n\n\nStandardized Effect Size\nFirst, calculate standard deviations:\nFor X: s_X = \\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{n-1}} = \\sqrt{\\frac{17.50}{5}} = \\sqrt{3.50} = 1.87\nFor Y: s_Y = \\sqrt{\\frac{\\sum(Y_i - \\bar{Y})^2}{n-1}} = \\sqrt{\\frac{663.34}{5}} = \\sqrt{132.67} = 11.52\nStandardized coefficient: \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y} = 6.12 \\times \\frac{1.87}{11.52} = 6.12 \\times 0.162 = 0.99\nInterpretation: A one standard deviation increase in study hours (1.87 hours) is associated with a 0.99 standard deviation increase in exam score.\nAccording to Cohen‚Äôs guidelines:\n\nSmall effect: |Œ≤| = 0.10\nMedium effect: |Œ≤| = 0.30\nLarge effect: |Œ≤| = 0.50\n\nOur standardized effect of 0.99 is nearly twice Cohen‚Äôs ‚Äúlarge effect‚Äù threshold.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-11-practical-significance-assessment",
    "href": "correg_en.html#step-11-practical-significance-assessment",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.23 Step 11: Practical Significance Assessment",
    "text": "15.23 Step 11: Practical Significance Assessment\n\nContext Analysis\n\nScale consideration:\n\nEffect: 6.12 points per hour\nExam scale: 0-100 points\nPercentage impact: 6.12% per hour\n\nPractical thresholds:\n\nLetter grade difference: Often 10 points\nTime to improve one letter grade: 10/6.12 = 1.63 hours\nConclusion: Less than 2 hours of extra study could change a letter grade\n\nCost-benefit analysis:\n\nBenefit: 6.12 points per hour\nCost: 1 hour of time\nDecision: Highly favorable return on investment",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-of-results",
    "href": "correg_en.html#summary-of-results",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.24 Summary of Results",
    "text": "15.24 Summary of Results\nRegression equation: \\hat{Y} = 58.25 + 6.12X\nKey statistics:\n\nSlope (\\hat{\\beta}_1): 6.12 points/hour\nIntercept (\\hat{\\beta}_0): 58.25 points\nR¬≤: 0.988 (98.8% of variance explained)\nStandardized effect: 0.99 (very large effect)\n\nPractical interpretation:\n\nEach hour of study adds about 6 points to exam score\nThe model fits extremely well (R¬≤ near 1)\nThe effect is both statistically and practically significant\nStudents can meaningfully improve grades with modest increases in study time",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#verification-check",
    "href": "correg_en.html#verification-check",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.25 Verification Check",
    "text": "15.25 Verification Check\nTo verify our calculations, let‚Äôs check that the regression line passes through (\\bar{X}, \\bar{Y}):\n\\hat{Y} = 58.25 + 6.12(3.5) = 58.25 + 21.42 = 79.67 = \\bar{Y} ‚úì\nThe calculation confirms our regression line passes through the point of means, as it should.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#r-code-to-verify-manual-calculations",
    "href": "correg_en.html#r-code-to-verify-manual-calculations",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.26 R Code to Verify Manual Calculations",
    "text": "15.26 R Code to Verify Manual Calculations\nBelow is R code that checks all our manual calculations. You can run this code to confirm every step.\n\n# Step 1: Create the data\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)  # X variable\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95)  # Y variable\nn &lt;- length(study_hours)  # Sample size\n\n# Create a data frame for easier handling\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Step 2: Calculate means\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Step 3: Calculate deviations from means\ndata$x_dev &lt;- data$X - x_bar  # X deviations\ndata$y_dev &lt;- data$Y - y_bar  # Y deviations\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Step 4: Calculate products and squares for OLS formula\ndata$xy_product &lt;- data$x_dev * data$y_dev  # (Xi - XÃÑ)(Yi - »≤)\ndata$x_dev_sq &lt;- data$x_dev^2  # (Xi - XÃÑ)¬≤\n\n# Sum of products and squares\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - XÃÑ)(Yi - »≤):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - XÃÑ)(Yi - »≤): 107 \n\ncat(\"Sum of (Xi - XÃÑ)¬≤:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - XÃÑ)¬≤: 17.5 \n\n# Step 5: Calculate slope (beta_1) manually\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (Œ≤‚ÇÅ) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (Œ≤‚ÇÅ) calculated manually: 6.11 \n\n# Step 6: Calculate intercept (beta_0) manually\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (Œ≤‚ÇÄ) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (Œ≤‚ÇÄ) calculated manually: 58.27 \n\n# Step 7: Compare with R's lm() function\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Step 8: Calculate predicted values and residuals\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X  # Predicted values\ndata$residual &lt;- data$Y - data$Y_hat  # Residuals\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ‚âà 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ‚âà 0): 0 \n\n# Step 9: Calculate Sum of Squares\n# Total Sum of Squares (SST)\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\n# Regression Sum of Squares (SSR)\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\n# Error Sum of Squares (SSE)\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\n# Verify that SST = SSR + SSE\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Step 10: Calculate R-squared\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R¬≤ (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR¬≤ (Method 1: SSR/SST): 0.9863 \n\ncat(\"R¬≤ (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR¬≤ (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R¬≤ (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR¬≤ (from lm function): 0.9863 \n\n# Step 11: Calculate Effect Sizes\n# Raw effect size (just the slope)\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\n# Standard deviations for standardized effect\nsd_x &lt;- sd(data$X)  # Standard deviation of X\nsd_y &lt;- sd(data$Y)  # Standard deviation of Y\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\n# Standardized effect size\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Correlation coefficient (should equal sqrt(R¬≤) for simple regression)\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R¬≤:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R¬≤: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n# Step 12: Create visualization\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\n# Plot the data and regression line\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5)\n\n# Add the regression line\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Add the mean point\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Add vertical lines for residuals\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\n# Add legend\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# Add the equation to the plot\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R¬≤ = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\n\n\n\n# Final summary\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )\n\n\n\nRunning the Code\nTo run this R code:\n\nCopy the entire code block above\nPaste it into RStudio or any R console\nExecute the code\nCompare the output with our manual calculations\n\nThe code will:\n\nRecreate all our manual calculations step by step\nVerify results using R‚Äôs built-in lm() function\nGenerate a visualization of the data with the regression line\nDisplay all intermediate calculations with clear labels\n\n\n\nExpected Output Highlights\nWhen you run this code, you should see:\n\nSlope: 6.12 (matching our manual calculation)\nIntercept: 58.25 (matching our manual calculation)\nR¬≤: 0.988 (matching our manual calculation)\nStandardized effect: 0.99 (matching our manual calculation)\nA plot showing the data points, regression line, and residuals\n\nThis verification confirms that our pen-and-paper calculations were correct!",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-linear-regression-model",
    "href": "correg_en.html#the-linear-regression-model",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.27 The Linear Regression Model",
    "text": "15.27 The Linear Regression Model\nRegression analysis provides a statistical framework for modeling relationships between a dependent variable and one or more independent variables. This methodology enables researchers to quantify relationships, test hypotheses, and make predictions based on observed data.\n\nSimple Linear Regression\nThe simple linear regression model expresses the relationship between a dependent variable and a single independent variable:\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\nWhere: - Y_i represents the dependent variable for observation i - X_i represents the independent variable for observation i - \\beta_0 is the intercept parameter - \\beta_1 is the slope parameter - \\varepsilon_i is the error term for observation i\n\n\nMultiple Linear Regression\nThe multiple linear regression model extends this framework to incorporate k independent variables:\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + ... + \\beta_k X_{ki} + \\varepsilon_i\nThis formulation allows for the simultaneous analysis of multiple predictors and their respective contributions to the dependent variable.\n\n\nOrdinary Least Squares Estimation\n\nDefining the Optimization Criterion\nThe estimation of regression parameters requires a criterion for determining the ‚Äúbest‚Äù fit. Consider three potential approaches for defining the optimal line through a set of data points:\n\n\nApproach 1: Minimizing the Sum of Residuals\n\\min \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i) = \\min \\sum_{i=1}^{n} e_i\nThis approach is fundamentally flawed. For any line passing through the data, we can always find another line where positive and negative residuals sum to zero. In fact, infinitely many lines satisfy \\sum e_i = 0. This criterion fails to uniquely identify an optimal solution. Moreover, a horizontal line through the mean of Y would achieve zero sum of residuals while ignoring the relationship with X entirely.\n\n\nApproach 2: Minimizing the Sum of Absolute Residuals\n\\min \\sum_{i=1}^{n} |Y_i - \\hat{Y}_i| = \\min \\sum_{i=1}^{n} |e_i|\nThis criterion, known as Least Absolute Deviations (LAD), addresses the cancellation problem by taking absolute values. It produces estimates that are more robust to outliers than OLS. However, this approach presents significant challenges:\n\nThe absolute value function is not differentiable at zero, complicating analytical solutions\nMultiple solutions may exist (the objective function may have multiple minima)\nNo closed-form solution exists; iterative numerical methods are required\nStatistical inference is more complex, lacking the elegant properties of OLS estimators\n\n\n\nApproach 3: Minimizing the Sum of Squared Residuals\n\\min \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\min \\sum_{i=1}^{n} e_i^2\nThe Ordinary Least Squares (OLS) approach minimizes the sum of squared residuals. This criterion offers several advantages:\n\nPrevents cancellation of positive and negative errors\nProvides a unique solution (except in cases of perfect multicollinearity)\nYields closed-form analytical solutions through differentiation\nProduces estimators with optimal statistical properties under classical assumptions\nFacilitates straightforward statistical inference\n\n\n\n\nVisualizing the Sum of Squared Errors\nThe OLS method can be understood geometrically through the following conceptual framework:\n\nEach error appears as a vertical line from the data point to the regression line\nEach of these vertical lines represents a residual (e_i)\nWe square each residual, which can be visualized as creating a square area\nThe sum of all these squared areas is what OLS minimizes\n\n\n# Visualization of squared errors as geometric areas\nlibrary(ggplot2)\n\n# Generate sample data with clear pattern\nset.seed(42)\nn &lt;- 8  # Small number for clarity\nx &lt;- seq(2, 16, length.out = n)\ny &lt;- 10 + 1.5*x + rnorm(n, 0, 3)\ndf &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, df)\ndf$fitted &lt;- fitted(model)\ndf$residual &lt;- residuals(model)\n\n# Create visualization with actual squares\np &lt;- ggplot(df, aes(x = x, y = y)) +\n  # Add regression line first (bottom layer)\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\", linewidth = 1.2) +\n  \n  # Add squares for each residual\n  # For positive residuals\n  geom_rect(data = subset(df, residual &gt; 0),\n            aes(xmin = x - abs(residual)/2, \n                xmax = x + abs(residual)/2, \n                ymin = fitted, \n                ymax = fitted + abs(residual)),\n            fill = \"red\", alpha = 0.25, color = \"red\", linewidth = 0.5) +\n  \n  # For negative residuals  \n  geom_rect(data = subset(df, residual &lt; 0),\n            aes(xmin = x - abs(residual)/2, \n                xmax = x + abs(residual)/2, \n                ymin = fitted - abs(residual), \n                ymax = fitted),\n            fill = \"red\", alpha = 0.25, color = \"red\", linewidth = 0.5) +\n  \n  # Add residual lines\n  geom_segment(aes(xend = x, yend = fitted), \n               color = \"red\", linewidth = 0.8, linetype = \"solid\") +\n  \n  # Add data points\n  geom_point(size = 3.5, color = \"black\") +\n  \n  # Add text annotations for selected squared values\n  geom_text(data = subset(df, abs(residual) &gt; 2),\n            aes(x = x, \n                y = fitted + sign(residual) * abs(residual)/2,\n                label = paste0(\"e¬≤=\", round(residual^2, 1))),\n            size = 3, color = \"darkred\", fontface = \"italic\") +\n  \n  # Styling\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(face = \"italic\")\n  ) +\n  coord_equal() +  # Ensures squares appear as squares\n  labs(\n    title = \"Geometric Visualization of Sum of Squared Errors\",\n    subtitle = paste(\"SSE =\", round(sum(df$residual^2), 1), \n                     \"- Red squares represent squared residuals\"),\n    x = \"Independent Variable (X)\",\n    y = \"Dependent Variable (Y)\"\n  ) +\n  # Add SSE annotation\n  annotate(\"rect\", \n           xmin = max(df$x) - 3, xmax = max(df$x) - 0.5,\n           ymin = min(df$y) - 2, ymax = min(df$y),\n           fill = \"lightyellow\", alpha = 0.8, color = \"gray40\") +\n  annotate(\"text\", \n           x = max(df$x) - 1.75, y = min(df$y) - 1,\n           label = paste(\"Œ£e¬≤ =\", round(sum(df$residual^2), 1)),\n           size = 4, fontface = \"bold\", color = \"darkred\")\n\nprint(p)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nMathematical Derivation of OLS Estimators\nFor simple linear regression, the OLS estimators are obtained by minimizing the sum of squared residuals:\nSSE = \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2\nTaking partial derivatives with respect to \\beta_0 and \\beta_1 and setting them equal to zero yields the normal equations. Solving this system produces:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} = \\frac{Cov(X,Y)}{Var(X)}\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\n\n\nProperties of OLS Estimators\nThe OLS procedure guarantees several important properties:\n\nZero sum of residuals: \\sum_{i=1}^{n} e_i = 0\nOrthogonality of residuals and predictors: \\sum_{i=1}^{n} X_i e_i = 0\nThe fitted regression line passes through the point (\\bar{X}, \\bar{Y})\nZero covariance between fitted values and residuals: \\sum_{i=1}^{n} \\hat{Y}_i e_i = 0\n\n\n\nClassical Linear Model Assumptions\n\nCore Assumptions\nFor OLS estimators to possess desirable statistical properties, the following assumptions must hold:\n\n\nAssumption 1: Linearity in Parameters\nThe relationship between the dependent and independent variables is linear in the parameters: Y_i = \\beta_0 + \\beta_1 X_{1i} + ... + \\beta_k X_{ki} + \\varepsilon_i\n\n\nAssumption 2: Strict Exogeneity\nThe error term has zero conditional expectation given all values of the independent variables: E[\\varepsilon_i | X] = 0\nThis assumption implies that the independent variables contain no information about the mean of the error term. It is stronger than contemporaneous exogeneity and rules out feedback from past errors to current regressors. This assumption is critical for unbiased estimation and is often violated in time series contexts with lagged dependent variables or in the presence of omitted variables.\nThis assumption is particularly important for our discussion of spurious correlations. Violations of the exogeneity assumption lead to endogeneity problems, which we will discuss later.\n\n\nAssumption 3: No Perfect Multicollinearity\nIn multiple regression, no independent variable can be expressed as a perfect linear combination of other independent variables. The matrix X'X must be invertible.\n\n\nAssumption 4: Homoscedasticity\nThe variance of the error term is constant across all observations: Var(\\varepsilon_i | X) = \\sigma^2\nThis assumption ensures that the precision of the regression does not vary systematically with the level of the independent variables.\n\n\nAssumption 5: No Autocorrelation\nThe error terms are uncorrelated with each other: Cov(\\varepsilon_i, \\varepsilon_j | X) = 0 \\text{ for } i \\neq j\n\n\nAssumption 6: Normality of Errors (for inference)\nThe error terms follow a normal distribution: \\varepsilon_i \\sim N(0, \\sigma^2)\nThis assumption is not required for the unbiasedness or consistency of OLS estimators but is necessary for exact finite-sample inference.\n\n\n\nGauss-Markov Theorem\nUnder Assumptions 1-5, the OLS estimators are BLUE (Best Linear Unbiased Estimators):\n\nBest: Minimum variance among the class of linear unbiased estimators\nLinear: The estimators are linear functions of the dependent variable\nUnbiased: E[\\hat{\\beta}] = \\beta\n\n\n\nVisualization of OLS Methodology\n\nGeometric Interpretation\n\n# Comprehensive visualization of OLS regression\nlibrary(ggplot2)\n\n# Generate sample data\nset.seed(42)\nn &lt;- 50\nx &lt;- runif(n, 0, 100)\nepsilon &lt;- rnorm(n, 0, 15)\ny &lt;- 20 + 0.8*x + epsilon\n\n# Create data frame\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, data = data)\ndata$fitted &lt;- fitted(model)\ndata$residuals &lt;- residuals(model)\n\n# Create comprehensive plot\nggplot(data, aes(x = x, y = y)) +\n  # Add confidence interval\n  geom_smooth(method = \"lm\", se = TRUE, alpha = 0.15, fill = \"blue\") +\n  # Add regression line\n  geom_line(aes(y = fitted), color = \"blue\", linewidth = 1.2) +\n  # Add residual segments\n  geom_segment(aes(xend = x, yend = fitted), \n               color = \"red\", alpha = 0.5, linewidth = 0.7) +\n  # Add observed points\n  geom_point(size = 2.5, alpha = 0.8) +\n  # Add fitted values\n  geom_point(aes(y = fitted), color = \"blue\", size = 1.5, alpha = 0.6) +\n  # Annotations\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(size = 11),\n    plot.title = element_text(size = 12, face = \"bold\")\n  ) +\n  labs(\n    title = \"Ordinary Least Squares Regression\",\n    subtitle = sprintf(\"Estimated equation: Y = %.2f + %.3f X  (R¬≤ = %.3f, RSE = %.2f)\",\n                      coef(model)[1], coef(model)[2], \n                      summary(model)$r.squared, \n                      summary(model)$sigma),\n    x = \"Independent Variable (X)\",\n    y = \"Dependent Variable (Y)\"\n  ) +\n  annotate(\"text\", x = min(x) + 5, y = max(y) - 5,\n           label = sprintf(\"SSE = %.1f\", sum(residuals(model)^2)),\n           hjust = 0, size = 3.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nVisualization of Squared Residuals\n\n# Demonstrate why squaring is necessary\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Generate example with clear pattern\nset.seed(123)\nn &lt;- 20\nx &lt;- seq(1, 10, length.out = n)\ny &lt;- 2 + 1.5*x + rnorm(n, 0, 2)\ndf &lt;- data.frame(x = x, y = y)\n\n# Fit model\nmodel &lt;- lm(y ~ x, df)\ndf$fitted &lt;- fitted(model)\ndf$residual &lt;- residuals(model)\n\n# Plot 1: Raw residuals\np1 &lt;- ggplot(df, aes(x = x)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(aes(y = 0, yend = residual, xend = x), \n               color = ifelse(df$residual &gt; 0, \"darkgreen\", \"darkred\"),\n               linewidth = 1) +\n  geom_point(aes(y = residual), size = 3,\n             color = ifelse(df$residual &gt; 0, \"darkgreen\", \"darkred\")) +\n  theme_minimal() +\n  labs(title = \"Residuals (ei)\",\n       subtitle = sprintf(\"Sum = %.2f (not meaningful)\", sum(df$residual)),\n       x = \"X\", y = \"Residual\") +\n  ylim(c(-6, 6))\n\n# Plot 2: Squared residuals\np2 &lt;- ggplot(df, aes(x = x)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(aes(y = 0, yend = residual^2, xend = x), \n               color = \"darkred\", linewidth = 1) +\n  geom_point(aes(y = residual^2), size = 3, color = \"darkred\") +\n  theme_minimal() +\n  labs(title = \"Squared Residuals (ei¬≤)\",\n       subtitle = sprintf(\"Sum = %.2f (minimized by OLS)\", sum(df$residual^2)),\n       x = \"X\", y = \"Squared Residual\") +\n  ylim(c(0, 36))\n\n# Combine plots\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\nDiagnostic Analysis\n\nResidual Diagnostics\nAssessment of model assumptions requires careful examination of residual patterns:\n\n# Generate diagnostic plots\npar(mfrow = c(2, 2))\n\n# Residuals vs Fitted Values\nplot(model, which = 1)\n# Tests linearity and homoscedasticity assumptions\n\n# Normal Q-Q Plot\nplot(model, which = 2)\n# Tests normality assumption\n\n# Scale-Location Plot\nplot(model, which = 3)\n# Tests homoscedasticity assumption\n\n# Cook's Distance\nplot(model, which = 4)\n\n\n\n\n\n\n\n# Identifies influential observations\n\n\n\nTesting Assumptions Formally\n\n# Formal statistical tests\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(car)\n\n# Test for heteroscedasticity\n# Breusch-Pagan test\nbptest(model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model\nBP = 0.37876, df = 1, p-value = 0.5383\n\n# Test for autocorrelation\n# Durbin-Watson test\ndwtest(model)\n\n\n    Durbin-Watson test\n\ndata:  model\nDW = 2.1781, p-value = 0.5599\nalternative hypothesis: true autocorrelation is greater than 0\n\n# Test for normality\n# Shapiro-Wilk test on residuals\nshapiro.test(residuals(model))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(model)\nW = 0.98221, p-value = 0.9593\n\n# Test for linearity\n# Rainbow test\nraintest(model)\n\n\n    Rainbow test\n\ndata:  model\nRain = 1.2139, df1 = 10, df2 = 8, p-value = 0.3995\n\n\n\n\n\nExtensions and Alternatives\n\nWhen OLS Assumptions Fail\nWhen classical assumptions are violated, alternative approaches may be necessary:\n\nHeteroscedasticity: Weighted Least Squares (WLS) or robust standard errors\nAutocorrelation: Generalized Least Squares (GLS) or Newey-West standard errors\nNon-normality: Bootstrap inference or robust regression methods\nMulticollinearity: Ridge regression or LASSO\nEndogeneity: Instrumental Variables (IV) or Two-Stage Least Squares (2SLS)\n\n\n\nRobust Regression Methods\nWhen outliers are present, robust alternatives to OLS include:\n\nM-estimators (Huber regression)\nLeast Trimmed Squares (LTS)\nMM-estimators\n\n\n\n\nPractical Implementation\n\nComplete Analysis Example\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(car)\n\n# Generate dataset\nset.seed(2024)\nn &lt;- 200\ndata &lt;- data.frame(\n  x1 = rnorm(n, 50, 10),\n  x2 = rnorm(n, 30, 5),\n  x3 = rbinom(n, 1, 0.5)\n)\ndata$y &lt;- 10 + 2*data$x1 + 3*data$x2 + 15*data$x3 + rnorm(n, 0, 10)\n\n# Fit multiple regression model\nfull_model &lt;- lm(y ~ x1 + x2 + x3, data = data)\n\n# Model summary\nsummary(full_model)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25.6507  -6.9674  -0.7472   6.3670  30.4959 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 13.59158    5.97565   2.274               0.024 *  \nx1           2.05837    0.06876  29.934 &lt;0.0000000000000002 ***\nx2           2.76233    0.15259  18.103 &lt;0.0000000000000002 ***\nx3          14.80771    1.42654  10.380 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.897 on 196 degrees of freedom\nMultiple R-squared:  0.8744,    Adjusted R-squared:  0.8725 \nF-statistic: 454.9 on 3 and 196 DF,  p-value: &lt; 0.00000000000000022\n\n# ANOVA table\nanova(full_model)\n\nAnalysis of Variance Table\n\nResponse: y\n           Df Sum Sq Mean Sq F value                Pr(&gt;F)    \nx1          1  82189   82189  839.04 &lt; 0.00000000000000022 ***\nx2          1  40936   40936  417.90 &lt; 0.00000000000000022 ***\nx3          1  10555   10555  107.75 &lt; 0.00000000000000022 ***\nResiduals 196  19200      98                                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Confidence intervals for parameters\nconfint(full_model, level = 0.95)\n\n                2.5 %    97.5 %\n(Intercept)  1.806741 25.376410\nx1           1.922758  2.193977\nx2           2.461401  3.063262\nx3          11.994367 17.621053\n\n# Variance Inflation Factors (multicollinearity check)\nvif(full_model)\n\n      x1       x2       x3 \n1.009487 1.044192 1.038735 \n\n# Model diagnostics\npar(mfrow = c(2, 2))\nplot(full_model)\n\n\n\n\n\n\n\n# Tidy output\ntidy(full_model, conf.int = TRUE)\n\n# A tibble: 4 √ó 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    13.6     5.98        2.27 2.40e- 2     1.81     25.4 \n2 x1              2.06    0.0688     29.9  4.90e-75     1.92      2.19\n3 x2              2.76    0.153      18.1  1.06e-43     2.46      3.06\n4 x3             14.8     1.43       10.4  2.14e-20    12.0      17.6 \n\nglance(full_model)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.874         0.872  9.90      455. 5.21e-88     3  -740. 1490. 1507.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\nConclusion\nOrdinary Least Squares regression remains a fundamental tool in statistical analysis. The method‚Äôs mathematical elegance, combined with its optimal properties under the classical assumptions, explains its widespread application. However, practitioners must carefully verify assumptions and consider alternatives when these conditions are not met. Understanding both the theoretical foundations and practical limitations of OLS is essential for proper statistical inference and prediction.\n\n\nVisualizing OLS Through Different Regression Lines\nA simple but effective way to visualize the concept of ‚Äúbest fit‚Äù is to compare multiple lines and their resulting SSE values:\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Create sample data\nset.seed(123)\nx &lt;- 1:20\ny &lt;- 2 + 3*x + rnorm(20, 0, 5)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, data = data)\ncoef &lt;- coefficients(model)\n\n# Define different lines: optimal and sub-optimal with clearer differences\nlines &lt;- data.frame(\n  label = c(\"Best Fit (OLS)\", \"Line A\", \"Line B\", \"Line C\"),\n  intercept = c(coef[1], coef[1] - 8, coef[1] + 8, coef[1] - 4),\n  slope = c(coef[2], coef[2] - 1.2, coef[2] + 0.8, coef[2] - 0.7)\n)\n\n# Calculate SSE for each line\nlines$sse &lt;- sapply(1:nrow(lines), function(i) {\n  predicted &lt;- lines$intercept[i] + lines$slope[i] * x\n  sum((y - predicted)^2)\n})\n\n# Add percentage increase over optimal SSE\nlines$pct_increase &lt;- round((lines$sse / lines$sse[1] - 1) * 100, 1)\nlines$pct_text &lt;- ifelse(lines$label == \"Best Fit (OLS)\", \n                         \"Optimal\", \n                         paste0(\"+\", lines$pct_increase, \"%\"))\n\n# Assign distinct colors for better visibility\nline_colors &lt;- c(\"Best Fit (OLS)\" = \"blue\", \n                \"Line A\" = \"red\", \n                \"Line B\" = \"darkgreen\", \n                \"Line C\" = \"purple\")\n\n# Create data for mini residual plots\nmini_data &lt;- data.frame()\nfor(i in 1:nrow(lines)) {\n  line_data &lt;- data.frame(\n    x = x,\n    y = y,\n    predicted = lines$intercept[i] + lines$slope[i] * x,\n    residuals = y - (lines$intercept[i] + lines$slope[i] * x),\n    line = lines$label[i]\n  )\n  mini_data &lt;- rbind(mini_data, line_data)\n}\n\n# Create main comparison plot with improved visibility\np1 &lt;- ggplot(data, aes(x = x, y = y)) +\n  # Add background grid for reference\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_line(color = \"gray90\"),\n    panel.grid.major = element_line(color = \"gray85\"),\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 13),\n    axis.title = element_text(size = 13, face = \"bold\"),\n    axis.text = element_text(size = 12)\n  ) +\n  # Add data points\n  geom_point(size = 2.5, alpha = 0.8) +\n  # Add lines with improved visibility\n  geom_abline(data = lines, \n              aes(intercept = intercept, slope = slope, \n                  color = label, linetype = label == \"Best Fit (OLS)\"),\n              size = 1.2) +\n  # Use custom colors\n  scale_color_manual(values = line_colors) +\n  scale_linetype_manual(values = c(\"TRUE\" = \"solid\", \"FALSE\" = \"dashed\"), guide = \"none\") +\n  # Better legends\n  labs(title = \"Comparing Different Regression Lines\",\n       subtitle = \"The OLS line minimizes the sum of squared errors\",\n       x = \"X\", y = \"Y\",\n       color = \"Regression Line\") +\n  guides(color = guide_legend(override.aes = list(size = 2)))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n# Create mini residual plots with improved visibility\np_mini &lt;- list()\n\nfor(i in 1:nrow(lines)) {\n  line_data &lt;- subset(mini_data, line == lines$label[i])\n  \n  p_mini[[i]] &lt;- ggplot(line_data, aes(x = x, y = residuals)) +\n    # Add reference line\n    geom_hline(yintercept = 0, linetype = \"dashed\", size = 0.8, color = \"gray50\") +\n    # Add residual points with line color\n    geom_point(color = line_colors[lines$label[i]], size = 2.5) +\n    # Add squares to represent squared errors\n    geom_rect(aes(xmin = x - 0.3, xmax = x + 0.3,\n                  ymin = 0, ymax = residuals),\n              fill = line_colors[lines$label[i]], alpha = 0.2) +\n    # Improved titles\n    labs(title = lines$label[i],\n         subtitle = paste(\"SSE =\", round(lines$sse[i], 1), \n                          ifelse(i == 1, \" (Optimal)\", \n                                 paste0(\" (+\", lines$pct_increase[i], \"%)\"))),\n         x = NULL, y = NULL) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(size = 12, face = \"bold\", color = line_colors[lines$label[i]]),\n      plot.subtitle = element_text(size = 10),\n      panel.grid.minor = element_blank()\n    )\n}\n\n# Create SSE comparison table with better visibility\nsse_df &lt;- data.frame(\n  x = rep(1, nrow(lines)),\n  y = nrow(lines):1,\n  label = paste0(lines$label, \": SSE = \", round(lines$sse, 1), \" (\", lines$pct_text, \")\"),\n  color = line_colors[lines$label]\n)\n\nsse_table &lt;- ggplot(sse_df, aes(x = x, y = y, label = label, color = color)) +\n  geom_text(hjust = 0, size = 5, fontface = \"bold\") +\n  scale_color_identity() +\n  theme_void() +\n  xlim(1, 10) +\n  ylim(0.5, nrow(lines) + 0.5) +\n  labs(title = \"Sum of Squared Errors (SSE) Comparison\") +\n  theme(plot.title = element_text(hjust = 0, face = \"bold\", size = 14))\n\n# Arrange the plots with better spacing\ngrid.arrange(\n  p1, \n  arrangeGrob(p_mini[[1]], p_mini[[2]], p_mini[[3]], p_mini[[4]], \n              ncol = 2, padding = unit(1, \"cm\")),\n  sse_table, \n  ncol = 1, \n  heights = c(4, 3, 1)\n)\n\n\n\n\nComparing different regression lines\n\n\n\n\n\n\nKey Learning Points\n\nThe Sum of Squared Errors (SSE) is what Ordinary Least Squares (OLS) regression minimizes\nEach residual contributes its squared value to the total SSE\nThe OLS line has a lower SSE than any other possible line\nLarge residuals contribute disproportionately to the SSE due to the squaring operation\nThis is why outliers can have such a strong influence on regression lines\n\n\nStep-by-Step SSE Minimization\nTo illustrate the process of finding the minimum SSE, we can create a sequence that passes through the optimal point, showing how the SSE first decreases to a minimum and then increases again:\n\n# Create sample data\nset.seed(123)\nx &lt;- 1:20\ny &lt;- 2 + 3*x + rnorm(20, 0, 5)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, data = data)\ncoef &lt;- coefficients(model)\n\n# Create a sequence of steps that passes through the optimal OLS line\nsteps &lt;- 9  # Use odd number to have a middle point at the optimum\nstep_seq &lt;- data.frame(\n  step = 1:steps,\n  intercept = seq(coef[1] - 8, coef[1] + 8, length.out = steps),\n  slope = seq(coef[2] - 1.5, coef[2] + 1.5, length.out = steps)\n)\n\n# Mark the middle step (optimal OLS solution)\noptimal_step &lt;- ceiling(steps/2)\n\n# Calculate SSE for each step\nstep_seq$sse &lt;- sapply(1:nrow(step_seq), function(i) {\n  predicted &lt;- step_seq$intercept[i] + step_seq$slope[i] * x\n  sum((y - predicted)^2)\n})\n\n# Create a \"journey through the SSE valley\" plot\np2 &lt;- ggplot(data, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_abline(data = step_seq, \n              aes(intercept = intercept, slope = slope, \n                  color = sse, group = step),\n              size = 1) +\n  # Highlight the optimal line\n  geom_abline(intercept = step_seq$intercept[optimal_step], \n              slope = step_seq$slope[optimal_step],\n              color = \"green\", size = 1.5) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  labs(title = \"Journey Through the SSE Valley\",\n       subtitle = \"The green line represents the OLS solution with minimum SSE\",\n       color = \"SSE Value\") +\n  theme_minimal()\n\n# Create an SSE valley plot\np3 &lt;- ggplot(step_seq, aes(x = step, y = sse)) +\n  geom_line(size = 1) +\n  geom_point(size = 3, aes(color = sse)) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  # Highlight the optimal point\n  geom_point(data = step_seq[optimal_step, ], aes(x = step, y = sse), \n             size = 5, color = \"green\") +\n  # Add annotation\n  annotate(\"text\", x = optimal_step, y = step_seq$sse[optimal_step] * 1.1, \n           label = \"Minimum SSE\", color = \"darkgreen\", fontface = \"bold\") +\n  labs(title = \"The SSE Valley: Decreasing Then Increasing\",\n       subtitle = \"The SSE reaches its minimum at the OLS solution\",\n       x = \"Step\",\n       y = \"Sum of Squared Errors\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Display both plots\ngrid.arrange(p2, p3, ncol = 1, heights = c(3, 2))\n\n\n\n\nSSE minimization visualization\n\n\n\n\nIn R, the lm() function fits linear regression models:\n\nmodel &lt;- lm(y ~ x, data = data_frame)\n\n\n\n\nModel Interpretation: A Beginner‚Äôs Guide\nLet‚Äôs create a simple dataset to understand regression output better. Imagine we‚Äôre studying how years of education affect annual income:\n\n# Create a simple dataset - this is our Data Generating Process (DGP)\nset.seed(123) # For reproducibility\neducation_years &lt;- 10:20  # Education from 10 to 20 years\nn &lt;- length(education_years)\n\n# True parameters in our model - using more realistic values for Poland\ntrue_intercept &lt;- 3000   # Base monthly income with no education (in PLN)\ntrue_slope &lt;- 250        # Each year of education increases monthly income by 250 PLN\n\n# Generate monthly incomes with some random noise\nincome &lt;- true_intercept + true_slope * education_years + rnorm(n, mean=0, sd=300)\n\n# Create our dataset\neducation_income &lt;- data.frame(\n  education = education_years,\n  income = income\n)\n\n# Let's visualize our data\nlibrary(ggplot2)\nggplot(education_income, aes(x = education, y = income)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  scale_y_continuous(limits = c(5000, 8500), \n                     breaks = seq(5000, 8500, by = 500),\n                     labels = scales::comma) +\n  scale_x_continuous(breaks = 10:20) +\n  labs(\n    title = \"Relationship between Education and Income in Poland\",\n    x = \"Years of Education\",\n    y = \"Monthly Income (PLN)\",\n    subtitle = \"Red line shows the estimated linear relationship\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\")\n  ) +\n  annotate(\"text\", x = 11, y = 8000, \n           label = \"Each point represents\\none person's data\", \n           hjust = 0, size = 4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nFitting the Model\nNow let‚Äôs fit a linear regression model to this data:\n\n# Fit a simple regression model\nedu_income_model &lt;- lm(income ~ education, data = education_income)\n\n# Display the results\nmodel_summary &lt;- summary(edu_income_model)\nmodel_summary\n\n\nCall:\nlm(formula = income ~ education, data = education_income)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-427.72 -206.04  -38.12  207.32  460.78 \n\nCoefficients:\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)   3095.3      447.6   6.915 0.0000695 ***\neducation      247.2       29.2   8.467 0.0000140 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 306.3 on 9 degrees of freedom\nMultiple R-squared:  0.8885,    Adjusted R-squared:  0.8761 \nF-statistic: 71.69 on 1 and 9 DF,  p-value: 0.00001403\n\n\n\n\nUnderstanding the Regression Output Step by Step\nLet‚Äôs break down what each part of this output means in simple terms:\n\n1. The Formula\nAt the top, you see income ~ education, which means we‚Äôre predicting income based on education.\n\n\n2. Residuals\nThese show how far our predictions are from the actual values. Ideally, they should be centered around zero.\n\n\n3. Coefficients Table\n\n\n\nCoefficient Estimates\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n3095.27\n447.63\n6.91\n0\n\n\neducation\n247.23\n29.20\n8.47\n0\n\n\n\n\n\nIntercept (\\beta_0):\n\nValue: Approximately 3095\nInterpretation: This is the predicted monthly income for someone with 0 years of education\nNote: Sometimes the intercept isn‚Äôt meaningful in real-world terms, especially if x=0 is outside your data range\n\nEducation (\\beta_1):\n\nValue: Approximately 247\nInterpretation: For each additional year of education, we expect monthly income to increase by this amount in PLN\nThis is our main coefficient of interest!\n\nStandard Error:\n\nMeasures how precise our estimates are\nSmaller standard errors mean more precise estimates\nThink of it as ‚Äúgive or take how much‚Äù for our coefficients\n\nt value:\n\nThis is the coefficient divided by its standard error\nIt tells us how many standard errors away from zero our coefficient is\nLarger absolute t values (above 2) suggest the effect is statistically significant\n\np-value:\n\nThe probability of seeing our result (or something more extreme) if there was actually no relationship\nTypically, p &lt; 0.05 is considered statistically significant\nFor education, p = 0.000014, which is significant!\n\n\n\n4. Model Fit Statistics\n\n\n\nModel Fit Statistics\n\n\nStatistic\nValue\n\n\n\n\nR-squared\n0.888\n\n\nAdjusted R-squared\n0.876\n\n\nF-statistic\n71.686\n\n\np-value\n0.000\n\n\n\n\n\nR-squared:\n\nValue: 0.888\nInterpretation: 89% of the variation in income is explained by education\nHigher is better, but be cautious of very high values (could indicate overfitting)\n\nF-statistic:\n\nTests whether the model as a whole is statistically significant\nA high F-statistic with a low p-value indicates a significant model\n\n\n\n\nVisualizing the Model Results\nLet‚Äôs visualize what our model actually tells us:\n\n# Predicted values\neducation_income$predicted &lt;- predict(edu_income_model)\neducation_income$residuals &lt;- residuals(edu_income_model)\n\n# Create a more informative plot\nggplot(education_income, aes(x = education, y = income)) +\n  # Actual data points\n  geom_point(size = 3, color = \"blue\") +\n  \n  # Regression line\n  geom_line(aes(y = predicted), color = \"red\", size = 1.2) +\n  \n  # Residual lines\n  geom_segment(aes(xend = education, yend = predicted), \n               color = \"darkgray\", linetype = \"dashed\") +\n  \n  # Set proper scales\n  scale_y_continuous(limits = c(5000, 8500), \n                     breaks = seq(5000, 8500, by = 500),\n                     labels = scales::comma) +\n  scale_x_continuous(breaks = 10:20) +\n  \n  # Annotations\n  annotate(\"text\", x = 19, y = 7850, \n           label = paste(\"Slope =\", round(coef(edu_income_model)[2]), \"PLN per year\"),\n           color = \"red\", hjust = 1, fontface = \"bold\") +\n  \n  annotate(\"text\", x = 10.5, y = 5500, \n           label = paste(\"Intercept =\", round(coef(edu_income_model)[1]), \"PLN\"),\n           color = \"red\", hjust = 0, fontface = \"bold\") +\n  \n  annotate(\"text\", x = 14, y = 8200, \n           label = paste(\"R¬≤ =\", round(model_summary$r.squared, 2)),\n           color = \"black\", fontface = \"bold\") +\n  \n  # Labels\n  labs(\n    title = \"Interpreting the Education-Income Regression Model\",\n    subtitle = \"Red line shows predicted income for each education level\",\n    x = \"Years of Education\",\n    y = \"Monthly Income (PLN)\",\n    caption = \"Gray dashed lines represent residuals (prediction errors)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\nReal-World Interpretation\n\nA person with 16 years of education (college graduate) would be predicted to earn about: \\hat{Y} = 3095 + 247 \\times 16 = 7051 \\text{ PLN monthly}\nThe model suggests that each additional year of education is associated with a 247 PLN increase in monthly income.\nOur model explains approximately 89% of the variation in income in our sample.\nThe relationship is statistically significant (p &lt; 0.001), meaning it‚Äôs very unlikely to observe this relationship if education truly had no effect on income.\n\n\n\nImportant Cautions for Beginners\n\nCorrelation ‚â† Causation: Our model shows association, not necessarily causation\nOmitted Variables: Other factors might influence both education and income\nExtrapolation: Be careful predicting outside the range of your data\nLinear Relationship: We‚Äôve assumed the relationship is linear, which may not always be true",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#regression-analysis-and-ordinary-least-squares",
    "href": "correg_en.html#regression-analysis-and-ordinary-least-squares",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.28 Regression Analysis and Ordinary Least Squares (*)",
    "text": "15.28 Regression Analysis and Ordinary Least Squares (*)\n\nFoundations of Regression Analysis\nRegression analysis constitutes a fundamental statistical methodology for examining relationships between variables. At its core, regression provides a systematic framework for understanding how changes in one or more independent variables influence a dependent variable.\nThe primary objectives of regression analysis include: - Quantifying relationships between variables - Making predictions based on observed patterns - Testing hypotheses about variable associations - Understanding the proportion of variation explained by predictors\n\n\nDeterministic versus Stochastic Models\nStatistical modeling encompasses two fundamental approaches:\nDeterministic models assume precise, invariant relationships between variables. Given specific inputs, these models yield identical outputs without variation. Consider the physics equation:\n\\text{Distance} = \\text{Speed} \\times \\text{Time}\nThis relationship exhibits no randomness; identical inputs always produce identical outputs.\nStochastic models, in contrast, acknowledge inherent variability in real-world phenomena. Regression analysis employs stochastic modeling through the fundamental equation:\nY = f(X) + \\epsilon\nWhere: - Y represents the outcome variable - f(X) captures the systematic relationship between predictors and outcome - \\epsilon represents random variation inherent in the data\nThis formulation recognizes that real-world relationships contain both systematic patterns and random variation.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#simple-linear-regression-model",
    "href": "correg_en.html#simple-linear-regression-model",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.29 Simple Linear Regression Model",
    "text": "15.29 Simple Linear Regression Model\n\nModel Specification\nSimple linear regression models the relationship between a single predictor variable and an outcome variable through a linear equation:\nY_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\nThe model components represent: - Y_i: The dependent variable for observation i - X_i: The independent variable for observation i - \\beta_0: The population intercept parameter - \\beta_1: The population slope parameter - \\epsilon_i: The random error term for observation i\n\n\nInterpretation of Parameters\nThe parameters possess specific interpretations:\n\nIntercept (\\beta_0): The expected value of Y when X = 0. This represents the baseline level of the outcome variable.\nSlope (\\beta_1): The expected change in Y for a one-unit increase in X. This quantifies the strength and direction of the linear relationship.\nError term (\\epsilon_i): Captures all factors affecting Y not explained by X, including measurement error, omitted variables, and inherent randomness.\n\n\n\nEstimation versus True Parameters\nThe distinction between population parameters and sample estimates proves crucial:\n\nPopulation parameters (\\beta_0, \\beta_1) represent true, unknown values\nSample estimates (\\hat{\\beta}_0, \\hat{\\beta}_1) represent our best approximations based on available data\nThe hat notation (^) consistently denotes estimated values\n\nThe fitted regression equation becomes:\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1X_i",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-ordinary-least-squares-method-1",
    "href": "correg_en.html#the-ordinary-least-squares-method-1",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.30 The Ordinary Least Squares Method",
    "text": "15.30 The Ordinary Least Squares Method\n\nThe Fundamental Challenge\nGiven a dataset with observations (X_i, Y_i), we need a systematic method to determine the ‚Äúbest‚Äù values for \\hat{\\beta}_0 and \\hat{\\beta}_1. The challenge lies in defining what constitutes ‚Äúbest‚Äù and developing a practical method to find these values.\nConsider that for any given line through the data, each observation will have a prediction error or residual:\ne_i = Y_i - \\hat{Y}_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1X_i)\nThese residuals represent how far our predictions deviate from actual values. A good fitting line should make these residuals as small as possible overall.\n\n\nWhy Minimize the Sum of Squared Residuals?\nThe Ordinary Least Squares method determines optimal parameter estimates by minimizing the sum of squared residuals. This choice requires justification, as we could conceivably minimize other quantities. The rationale for squaring residuals includes:\nMathematical tractability: Squaring creates a smooth, differentiable function that yields closed-form solutions through calculus. The derivatives of squared terms lead to linear equations that can be solved analytically.\nEqual treatment of positive and negative errors: Simply summing raw residuals would allow positive and negative errors to cancel, potentially yielding a sum of zero even when predictions are poor. Squaring ensures all deviations contribute positively to the total error measure.\nPenalization of large errors: Squaring gives progressively greater weight to larger errors. An error of 4 units contributes 16 to the sum, while an error of 2 units contributes only 4. This property encourages finding a line that avoids extreme prediction errors.\nStatistical optimality: Under certain assumptions (including normally distributed errors), OLS estimators possess desirable statistical properties, including being the Best Linear Unbiased Estimators (BLUE) according to the Gauss-Markov theorem.\nConnection to variance: The sum of squared deviations directly relates to variance, a fundamental measure of spread in statistics. Minimizing squared residuals thus minimizes the variance of prediction errors.\n\n\nThe OLS Optimization Problem\nThe OLS method formally seeks values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that minimize:\n\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1X_i))^2\nThis optimization problem can be solved using calculus by: 1. Taking partial derivatives with respect to \\hat{\\beta}_0 and \\hat{\\beta}_1 2. Setting these derivatives equal to zero 3. Solving the resulting system of equations\n\n\nDerivation of OLS Estimators\nThe minimization yields closed-form solutions:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n(X_i - \\bar{X})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\nThese formulas reveal that: - The slope estimate depends on the covariance between variables relative to the predictor‚Äôs variance - The intercept ensures the regression line passes through the point of means (\\bar{X}, \\bar{Y})\n\n\nProperties of OLS Estimators\nOLS estimators possess several desirable properties:\n\nUnbiasedness: Under appropriate conditions, E[\\hat{\\beta}_j] = \\beta_j\nEfficiency: OLS provides minimum variance among linear unbiased estimators\nConsistency: As sample size increases, estimates converge to true values\nThe regression line passes through the centroid: The point (\\bar{X}, \\bar{Y}) always lies on the fitted line\n\n\n\nExtension to Multiple Regression\nWhile this guide focuses on simple linear regression with one predictor, the OLS framework extends naturally to multiple regression with several predictors:\nY_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + ... + \\beta_kX_{ki} + \\epsilon_i\nThe same principle applies: we minimize the sum of squared residuals, though the mathematics involves matrix algebra rather than simple formulas. The fundamental logic‚Äîfinding parameter values that minimize prediction errors‚Äîremains unchanged.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#understanding-variance-decomposition",
    "href": "correg_en.html#understanding-variance-decomposition",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.31 Understanding Variance Decomposition",
    "text": "15.31 Understanding Variance Decomposition\n\nThe Baseline Model Concept\nBefore introducing predictors, consider the simplest possible model: predicting every observation using the overall mean \\bar{Y}. This baseline model represents our best prediction in the absence of additional information.\nThe baseline model‚Äôs predictions: \\hat{Y}_i^{\\text{baseline}} = \\bar{Y} \\text{ for all } i\nThis model serves as a reference point for evaluating improvement gained through incorporating predictors. The baseline model essentially asks: ‚ÄúIf we knew nothing about the relationship between X and Y, what would be our best constant prediction?‚Äù\n\n\nComponents of Total Variation\nThe total variation in the outcome variable decomposes into three fundamental components:\n\nTotal Sum of Squares (SST)\nSST quantifies the total variation in the outcome variable relative to its mean:\n\\text{SST} = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\nInterpretation: SST represents the total variance that requires explanation. It measures the prediction error when using only the mean as our model‚Äîessentially the variance explained by the baseline (zero) model. This is the starting point: the total amount of variation we hope to explain by introducing predictors.\n\n\nRegression Sum of Squares (SSR)\nSSR measures the variation explained by the regression model:\n\\text{SSR} = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\nInterpretation: SSR quantifies the improvement in prediction achieved by incorporating the predictor variable. It represents the reduction in prediction error relative to the baseline model‚Äîthe portion of total variation that our regression line successfully captures.\n\n\nError Sum of Squares (SSE)\nSSE captures the unexplained variation remaining after regression:\n\\text{SSE} = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\nInterpretation: SSE represents the residual variation that the model cannot explain, reflecting the inherent randomness and effects of omitted variables. This is the variation that remains even after using our best-fitting line.\n\n\n\nThe Fundamental Decomposition Identity\nThese components relate through the fundamental equation:\n\\text{SST} = \\text{SSR} + \\text{SSE}\nThis identity demonstrates that: - Total variation equals the sum of explained and unexplained components - The regression model partitions total variation into systematic and random parts - Model improvement can be assessed by comparing SSR to SST\n\n\nConceptual Framework for Variance Decomposition\n# Demonstration of Variance Decomposition\nlibrary(ggplot2)\n\n# Generate sample data\nset.seed(42)\nn &lt;- 50\nx &lt;- runif(n, 1, 10)\ny &lt;- 3 + 2*x + rnorm(n, 0, 2)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit model\nmodel &lt;- lm(y ~ x, data = data)\ny_mean &lt;- mean(y)\ny_pred &lt;- predict(model)\n\n# Calculate components\nSST &lt;- sum((y - y_mean)^2)\nSSR &lt;- sum((y_pred - y_mean)^2)\nSSE &lt;- sum((y - y_pred)^2)\n\n# Display decomposition\ncat(\"Variance Decomposition\\n\")\ncat(\"======================\\n\")\ncat(\"Total SS (SST):\", round(SST, 2), \n    \"- Total variation from mean\\n\")\ncat(\"Regression SS (SSR):\", round(SSR, 2), \n    \"- Variation explained by model\\n\")\ncat(\"Error SS (SSE):\", round(SSE, 2), \n    \"- Unexplained variation\\n\")\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\ncat(round(SST, 2), \"=\", round(SSR, 2), \"+\", \n    round(SSE, 2), \"\\n\")",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-coefficient-of-determination-r¬≤",
    "href": "correg_en.html#the-coefficient-of-determination-r¬≤",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.32 The Coefficient of Determination (R¬≤)",
    "text": "15.32 The Coefficient of Determination (R¬≤)\n\nDefinition and Calculation\nThe coefficient of determination, denoted R¬≤, quantifies the proportion of total variation explained by the regression model:\nR^2 = \\frac{\\text{SSR}}{\\text{SST}} = \\frac{\\text{Explained Variation}}{\\text{Total Variation}}\nAlternatively expressed as:\nR^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}} = 1 - \\frac{\\text{Unexplained Variation}}{\\text{Total Variation}}\nR¬≤ directly answers the question: ‚ÄúWhat proportion of the total variation in Y (relative to the baseline mean model) does our regression model explain?‚Äù\n\n\nInterpretation Guidelines\nR¬≤ values range from 0 to 1, with specific interpretations:\n\nR¬≤ = 0: The model explains no variation beyond the baseline mean model. The regression line provides no improvement over simply using \\bar{Y}.\nR¬≤ = 0.25: The model explains 25% of total variation. Three-quarters of the variation remains unexplained.\nR¬≤ = 0.75: The model explains 75% of total variation. This represents substantial explanatory power.\nR¬≤ = 1.00: The model explains all variation (perfect fit). All data points fall exactly on the regression line.\n\n\n\nContextual Considerations\nThe interpretation of R¬≤ requires careful consideration of context:\nField-specific standards: Acceptable R¬≤ values vary dramatically across disciplines - Physical sciences often expect R¬≤ &gt; 0.90 due to controlled conditions - Social sciences may consider R¬≤ = 0.30 meaningful given human complexity - Biological systems typically show intermediate values due to natural variation\nSample size effects: Small samples can artificially inflate R¬≤, leading to overly optimistic assessments of model fit.\nModel complexity: In multiple regression, additional predictors mechanically increase R¬≤, even if they lack true explanatory power.\nPractical significance: Statistical fit should align with substantive importance. A model with R¬≤ = 0.95 may be less useful than one with R¬≤ = 0.60 if the latter addresses more relevant questions.\n\n\nAdjusted R¬≤ for Multiple Regression\nWhen extending to multiple regression, adjusted R¬≤ accounts for the number of predictors:\nR^2_{\\text{adj}} = 1 - \\frac{\\text{SSE}/(n-p)}{\\text{SST}/(n-1)}\nWhere: - n = sample size - p = number of parameters (including intercept)\nAdjusted R¬≤ penalizes model complexity, providing a more conservative measure when comparing models with different numbers of predictors.\n\n\nMeasures of Fit\n\nR-squared (R^2): R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nRoot Mean Square Error (RMSE): RMSE = \\sqrt{\\frac{SSE}{n}}\nMean Absolute Error (MAE): MAE = \\frac{1}{n}\\sum_{i=1}^n |Y_i - \\hat{Y}_i|",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#practical-implementation-1",
    "href": "correg_en.html#practical-implementation-1",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.33 Practical Implementation",
    "text": "15.33 Practical Implementation\n\nComplete Regression Analysis Example\n\n# Comprehensive regression analysis\nlibrary(ggplot2)\n\n# Generate educational data\nset.seed(123)\nn &lt;- 100\nstudy_hours &lt;- runif(n, 0, 10)\nexam_scores &lt;- 50 + 4*study_hours + rnorm(n, 0, 5)\neducation_data &lt;- data.frame(\n  study_hours = study_hours,\n  exam_scores = exam_scores\n)\n\n# Fit regression model\nmodel &lt;- lm(exam_scores ~ study_hours, data = education_data)\n\n# Extract key statistics\ny_mean &lt;- mean(exam_scores)\ny_pred &lt;- predict(model)\n\n# Variance decomposition\nSST &lt;- sum((exam_scores - y_mean)^2)\nSSR &lt;- sum((y_pred - y_mean)^2)\nSSE &lt;- sum((exam_scores - y_pred)^2)\nr_squared &lt;- SSR/SST\n\n# Display results\ncat(\"OLS Regression Results\\n\")\n\nOLS Regression Results\n\ncat(\"======================\\n\")\n\n======================\n\ncat(\"\\nParameter Estimates:\\n\")\n\n\nParameter Estimates:\n\ncat(\"Intercept (Œ≤‚ÇÄ):\", round(coef(model)[1], 2), \"\\n\")\n\nIntercept (Œ≤‚ÇÄ): 49.96 \n\ncat(\"Slope (Œ≤‚ÇÅ):\", round(coef(model)[2], 2), \"\\n\")\n\nSlope (Œ≤‚ÇÅ): 3.96 \n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Expected score with 0 study hours:\", \n    round(coef(model)[1], 2), \"\\n\")\n\n- Expected score with 0 study hours: 49.96 \n\ncat(\"- Score increase per study hour:\", \n    round(coef(model)[2], 2), \"\\n\")\n\n- Score increase per study hour: 3.96 \n\ncat(\"\\nVariance Decomposition:\\n\")\n\n\nVariance Decomposition:\n\ncat(\"Total variation (SST):\", round(SST, 2), \"\\n\")\n\nTotal variation (SST): 14880.14 \n\ncat(\"Explained by model (SSR):\", round(SSR, 2), \"\\n\")\n\nExplained by model (SSR): 12578.3 \n\ncat(\"Unexplained (SSE):\", round(SSE, 2), \"\\n\")\n\nUnexplained (SSE): 2301.84 \n\ncat(\"\\nModel Performance:\\n\")\n\n\nModel Performance:\n\ncat(\"R-squared:\", round(r_squared, 4), \"\\n\")\n\nR-squared: 0.8453 \n\ncat(\"Interpretation: The model explains\", \n    round(r_squared * 100, 1), \n    \"% of the variation in exam scores\\n\")\n\nInterpretation: The model explains 84.5 % of the variation in exam scores\n\ncat(\"beyond what the mean alone could explain.\\n\")\n\nbeyond what the mean alone could explain.\n\n\n\n\nVisualization of Key Concepts\n\n# Create comprehensive visualization\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Plot 1: Baseline model (mean only)\np1 &lt;- ggplot(education_data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = y_mean, color = \"blue\", size = 1) +\n  geom_segment(aes(xend = study_hours, yend = y_mean), \n               alpha = 0.3, color = \"red\") +\n  labs(title = \"Baseline Model: Predicting with Mean\",\n       subtitle = paste(\"Total variation (SST) =\", round(SST, 1)),\n       x = \"Study Hours\", y = \"Exam Scores\") +\n  theme_minimal() +\n  annotate(\"text\", x = 8, y = y_mean + 1, \n           label = \"Mean of Y\", color = \"blue\")\n\n# Plot 2: Regression model\np2 &lt;- ggplot(education_data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_segment(aes(xend = study_hours, yend = y_pred), \n               alpha = 0.3, color = \"red\") +\n  labs(title = \"Regression Model: Improved Predictions\",\n       subtitle = paste(\"Unexplained variation (SSE) =\", \n                       round(SSE, 1), \n                       \"| R¬≤ =\", round(r_squared, 3)),\n       x = \"Study Hours\", y = \"Exam Scores\") +\n  theme_minimal() +\n  annotate(\"text\", x = 8, y = max(y_pred) + 1, \n           label = \"Regression Line\", color = \"blue\")\n\n# Plot 3: Variance components\nvariance_data &lt;- data.frame(\n  Component = c(\"Total (SST)\", \"Explained (SSR)\", \"Unexplained (SSE)\"),\n  Value = c(SST, SSR, SSE),\n  Type = c(\"Total\", \"Explained\", \"Unexplained\")\n)\n\np3 &lt;- ggplot(variance_data, aes(x = Component, y = Value, fill = Type)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"Total\" = \"gray50\", \n                               \"Explained\" = \"green4\", \n                               \"Unexplained\" = \"red3\")) +\n  labs(title = \"Variance Decomposition\",\n       y = \"Sum of Squares\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Combine plots\ngrid.arrange(p1, p2, p3, layout_matrix = rbind(c(1, 2), c(3, 3)))\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-and-key-insights",
    "href": "correg_en.html#summary-and-key-insights",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.34 Summary and Key Insights",
    "text": "15.34 Summary and Key Insights\n\nCore Concepts Review\nRegression analysis models relationships between variables using stochastic frameworks that acknowledge inherent variation. The simple linear regression model expresses this relationship as Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i.\nOrdinary Least Squares provides optimal parameter estimates by minimizing the sum of squared residuals. This choice of minimizing squared errors stems from mathematical tractability, equal treatment of positive and negative errors, appropriate penalization of large errors, and desirable statistical properties.\nVariance decomposition partitions total variation into: - Total variation from the baseline mean model (SST) - Variation explained by the regression model (SSR)\n- Unexplained residual variation (SSE)\nThe fundamental identity SST = SSR + SSE shows how regression improves upon the baseline model.\nR¬≤ quantifies model performance as the proportion of total variation explained, providing a standardized measure of how much better the regression model performs compared to simply using the mean.\n\n\nCritical Considerations\nThe baseline model (predicting with the mean) serves as the fundamental reference point. All regression improvement is measured relative to this simple model. SST represents the total variance requiring explanation when we start with no predictors.\nParameter estimates (\\hat{\\beta}_0, \\hat{\\beta}_1) represent sample-based approximations of unknown population values (\\beta_0, \\beta_1). The distinction between population parameters and sample estimates remains crucial for proper inference.\nModel assessment requires considering both statistical fit (R¬≤) and practical significance. A model with modest R¬≤ may still provide valuable insights, while high R¬≤ does not guarantee causation or practical utility.\n\n\nExtensions and Applications\nWhile this guide focuses on simple linear regression, the framework extends naturally to: - Multiple regression with several predictors - Polynomial regression for nonlinear relationships - Interaction terms to capture conditional effects - Categorical predictors through appropriate coding\nThe fundamental principles‚Äîminimizing prediction errors, decomposing variation, and assessing model fit‚Äîremain consistent across these extensions. The mathematical complexity increases, but the conceptual foundation established here continues to apply.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#key-assumptions-of-linear-regression",
    "href": "correg_en.html#key-assumptions-of-linear-regression",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.35 Key Assumptions of Linear Regression",
    "text": "15.35 Key Assumptions of Linear Regression\n\nStrict Exogeneity: The Fundamental Assumption\nThe most crucial assumption in regression is strict exogeneity:\nE[\\varepsilon|X] = 0\nThis means:\n\nThe error term has zero mean conditional on X\nX contains no information about the average error\nThere are no systematic patterns in how our predictions are wrong\n\nLet‚Äôs visualize when this assumption holds and when it doesn‚Äôt:\n\n# Generate data\nset.seed(789)\nx &lt;- seq(1, 10, by = 0.2)\n\n# Case 1: Exogenous errors\ny_exog &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\n\n# Case 2: Non-exogenous errors (error variance increases with x)\ny_nonexog &lt;- 2 + 3*x + 0.5*x*rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_exog &lt;- data.frame(\n  x = x,\n  y = y_exog,\n  type = \"Exogenous Errors\\n(Assumption Satisfied)\"\n)\n\ndata_nonexog &lt;- data.frame(\n  x = x,\n  y = y_nonexog,\n  type = \"Non-Exogenous Errors\\n(Assumption Violated)\"\n)\n\ndata_combined &lt;- rbind(data_exog, data_nonexog)\n\n# Create plots with residuals\nplot_residuals &lt;- function(data, title) {\n  model &lt;- lm(y ~ x, data = data)\n  data$predicted &lt;- predict(model)\n  data$residuals &lt;- residuals(model)\n  \n  p1 &lt;- ggplot(data, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    theme_minimal() +\n    labs(title = title)\n  \n  p2 &lt;- ggplot(data, aes(x = x, y = residuals)) +\n    geom_point() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n    theme_minimal() +\n    labs(y = \"Residuals\")\n  \n  list(p1, p2)\n}\n\n# Generate plots\nplots_exog &lt;- plot_residuals(data_exog, \"Exogenous Errors\")\nplots_nonexog &lt;- plot_residuals(data_nonexog, \"Non-Exogenous Errors\")\n\n# Arrange plots\ngridExtra::grid.arrange(\n  plots_exog[[1]], plots_exog[[2]],\n  plots_nonexog[[1]], plots_nonexog[[2]],\n  ncol = 2\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure¬†15.1: Exogeneity vs.¬†Non-Exogeneity Examples\n\n\n\n\n\n\n\nLinearity: The Form Assumption\nThe relationship between X and Y should be linear in parameters:\nE[Y|X] = \\beta_0 + \\beta_1X\nNote that this doesn‚Äôt mean X and Y must have a straight-line relationship - we can transform variables. Let‚Äôs see different types of relationships:\n\n# Generate data\nset.seed(101)\nx &lt;- seq(1, 10, by = 0.1)\n\n# Different relationships\ndata_relationships &lt;- data.frame(\n  x = rep(x, 3),\n  y = c(\n    # Linear\n    2 + 3*x + rnorm(length(x), 0, 2),\n    # Quadratic\n    2 + 0.5*x^2 + rnorm(length(x), 0, 2),\n    # Exponential\n    exp(0.3*x) + rnorm(length(x), 0, 2)\n  ),\n  type = rep(c(\"Linear\", \"Quadratic\", \"Exponential\"), each = length(x))\n)\n\n# Plot\nggplot(data_relationships, aes(x = x, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  facet_wrap(~type, scales = \"free_y\") +\n  theme_minimal() +\n  labs(subtitle = \"Red: linear fit, Blue: true relationship\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure¬†15.2: Linear and Nonlinear Relationships\n\n\n\n\n\n\n\nUnderstanding Violations and Solutions\nWhen linearity is violated:\n\nTransform variables:\n\nLog transformation: for exponential relationships\nSquare root: for moderate nonlinearity\nPower transformations: for more complex relationships\n\n\n\n# Generate exponential data\nset.seed(102)\nx &lt;- seq(1, 10, by = 0.2)\ny &lt;- exp(0.3*x) + rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_trans &lt;- data.frame(\n  x = x,\n  y = y,\n  log_y = log(y)\n)\n\nWarning in log(y): NaNs produced\n\n# Original scale plot\np1 &lt;- ggplot(data_trans, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Original Scale\")\n\n# Log scale plot\np2 &lt;- ggplot(data_trans, aes(x = x, y = log_y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Log-Transformed Y\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFigure¬†15.3: Effect of Variable Transformations",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#spurious-correlation-causes-and-examples",
    "href": "correg_en.html#spurious-correlation-causes-and-examples",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.36 Spurious Correlation: Causes and Examples",
    "text": "15.36 Spurious Correlation: Causes and Examples\nSpurious correlation occurs when variables appear related but the relationship is not causal. These misleading correlations arise from several sources:\n\nRandom coincidence (chance)\nConfounding variables (hidden third factors)\nSelection biases\nImproper statistical analysis\nReverse causality\nEndogeneity problems (including simultaneity)\n\n\nRandom Coincidence (Chance)\nWith sufficient data mining or small sample sizes, seemingly meaningful correlations can emerge purely by chance. This is especially problematic when researchers conduct multiple analyses without appropriate corrections for multiple comparisons, a practice known as ‚Äúp-hacking.‚Äù\n\n# Create a realistic example of spurious correlation based on actual country data\n# Using country data on chocolate consumption and Nobel prize winners\n# This example is inspired by a published correlation (Messerli, 2012)\nset.seed(123)\ncountries &lt;- c(\"Switzerland\", \"Sweden\", \"Denmark\", \"Belgium\", \"Austria\", \n               \"Norway\", \"Germany\", \"Netherlands\", \"United Kingdom\", \"Finland\", \n               \"France\", \"Italy\", \"Spain\", \"Poland\", \"Greece\", \"Portugal\")\n\n# Create realistic data: Chocolate consumption correlates with GDP per capita\n# Higher GDP countries tend to consume more chocolate and have better research funding\ngdp_per_capita &lt;- c(87097, 58977, 67218, 51096, 53879, 89154, 51860, 57534, \n                    46510, 53982, 43659, 35551, 30416, 17841, 20192, 24567)\n\n# Normalize GDP values to make them more manageable\ngdp_normalized &lt;- (gdp_per_capita - min(gdp_per_capita)) / \n                 (max(gdp_per_capita) - min(gdp_per_capita))\n\n# More realistic chocolate consumption - loosely based on real consumption patterns\n# plus some randomness, but influenced by GDP\nchocolate_consumption &lt;- 4 + 8 * gdp_normalized + rnorm(16, 0, 0.8)\n\n# Nobel prizes - also influenced by GDP (research funding) with noise\n# The relationship is non-linear, but will show up as correlated\nnobel_prizes &lt;- 2 + 12 * gdp_normalized^1.2 + rnorm(16, 0, 1.5)\n\n# Create dataframe\ncountry_data &lt;- data.frame(\n  country = countries,\n  chocolate = round(chocolate_consumption, 1),\n  nobel = round(nobel_prizes, 1),\n  gdp = gdp_per_capita\n)\n\n# Fit regression model - chocolate vs nobel without controlling for GDP\nchocolate_nobel_model &lt;- lm(nobel ~ chocolate, data = country_data)\n\n# Better model that reveals the confounding\nfull_model &lt;- lm(nobel ~ chocolate + gdp, data = country_data)\n\n# Plot the apparent relationship\nggplot(country_data, aes(x = chocolate, y = nobel)) +\n  geom_point(color = \"darkblue\", size = 3, alpha = 0.7) +\n  geom_text(aes(label = country), hjust = -0.2, vjust = 0, size = 3) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  labs(\n    title = \"Apparent Correlation: Chocolate Consumption vs. Nobel Prizes\",\n    subtitle = \"Demonstrates how confounding variables create spurious correlations\",\n    x = \"Chocolate Consumption (kg per capita)\",\n    y = \"Nobel Prizes per 10M Population\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Show regression results\nsummary(chocolate_nobel_model)\n\n\nCall:\nlm(formula = nobel ~ chocolate, data = country_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9080 -1.4228  0.0294  0.5962  3.2977 \n\nCoefficients:\n            Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept)  -4.0518     1.3633  -2.972     0.0101 *  \nchocolate     1.3322     0.1682   7.921 0.00000154 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.626 on 14 degrees of freedom\nMultiple R-squared:  0.8176,    Adjusted R-squared:  0.8045 \nF-statistic: 62.75 on 1 and 14 DF,  p-value: 0.000001536\n\n# Demonstrate multiple testing problem\np_values &lt;- numeric(100)\nfor(i in 1:100) {\n  # Generate two completely random variables with n=20\n  x &lt;- rnorm(20)\n  y &lt;- rnorm(20)\n  # Test for correlation and store p-value\n  p_values[i] &lt;- cor.test(x, y)$p.value\n}\n\n# How many \"significant\" results at alpha = 0.05?\nsum(p_values &lt; 0.05)\n\n[1] 3\n\n# Visualize the multiple testing phenomenon\nhist(p_values, breaks = 20, main = \"P-values from 100 Tests of Random Data\",\n     xlab = \"P-value\", col = \"lightblue\", border = \"white\")\nabline(v = 0.05, col = \"red\", lwd = 2, lty = 2)\ntext(0.15, 20, paste(\"Approximately\", sum(p_values &lt; 0.05),\n                     \"tests are 'significant'\\nby random chance alone!\"), \n     col = \"darkred\")\n\n\n\n\n\n\n\n\nThis example demonstrates how seemingly compelling correlations can emerge between unrelated variables due to confounding factors and chance. The correlation between chocolate consumption and Nobel prizes appears significant (p &lt; 0.05) when analyzed directly, even though it‚Äôs explained by a third variable - national wealth (GDP per capita).\nWealthier countries typically consume more chocolate and simultaneously invest more in education and research, leading to more Nobel prizes. Without controlling for this confounding factor, we would mistakenly conclude a direct relationship between chocolate and Nobel prizes.\nThe multiple testing demonstration further illustrates why spurious correlations appear so frequently in research. When conducting 100 statistical tests on completely random data, we expect approximately 5 ‚Äúsignificant‚Äù results at Œ± = 0.05 purely by chance. In real research settings where hundreds of variables might be analyzed, the probability of finding false positive correlations increases dramatically.\nThis example underscores three critical points:\n\nSmall sample sizes (16 countries) are particularly vulnerable to chance correlations\nConfounding variables can create strong apparent associations between unrelated factors\nMultiple testing without appropriate corrections virtually guarantees finding ‚Äúsignificant‚Äù but meaningless patterns\n\nSuch findings explain why replication is essential in research and why most initial ‚Äúdiscoveries‚Äù fail to hold up in subsequent studies.\n\n\nConfounding Variables (Hidden Third Factors)\nConfounding occurs when an external variable influences both the predictor and outcome variables, creating an apparent relationship that may disappear when the confounder is accounted for.\n\n# Create sample data\nn &lt;- 200\nability &lt;- rnorm(n, 100, 15)                       # Natural ability \neducation &lt;- 10 + 0.05 * ability + rnorm(n, 0, 2)  # Education affected by ability\nincome &lt;- 10000 + 2000 * education + 100 * ability + rnorm(n, 0, 5000)  # Income affected by both\n\nomitted_var_data &lt;- data.frame(\n  ability = ability,\n  education = education,\n  income = income\n)\n\n# Model without accounting for ability\nmodel_naive &lt;- lm(income ~ education, data = omitted_var_data)\n\n# Model accounting for ability\nmodel_full &lt;- lm(income ~ education + ability, data = omitted_var_data)\n\n# Show results\nsummary(model_naive)\n\n\nCall:\nlm(formula = income ~ education, data = omitted_var_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14422.9  -3362.1    142.7   3647.7  14229.6 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  18982.0     2410.5   7.875    0.000000000000221 ***\neducation     2050.9      158.7  12.926 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5066 on 198 degrees of freedom\nMultiple R-squared:  0.4576,    Adjusted R-squared:  0.4549 \nF-statistic: 167.1 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_full)\n\n\nCall:\nlm(formula = income ~ education + ability, data = omitted_var_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12739.9  -3388.7    -41.1   3572.1  14976.8 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 13203.84    3018.85   4.374            0.0000198 ***\neducation    1871.43     166.03  11.272 &lt; 0.0000000000000002 ***\nability        85.60      27.87   3.071              0.00243 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4961 on 197 degrees of freedom\nMultiple R-squared:  0.4824,    Adjusted R-squared:  0.4772 \nF-statistic: 91.81 on 2 and 197 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Create visualization with ability shown through color\nggplot(omitted_var_data, aes(x = education, y = income, color = ability)) +\n  geom_point(alpha = 0.8) +\n  scale_color_viridis_c(name = \"Ability Score\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) + \n  labs(\n    title = \"Income vs. Education, Colored by Ability\",\n    subtitle = \"Visualizing the confounding variable\",\n    x = \"Years of Education\",\n    y = \"Annual Income (PLN)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis example illustrates omitted variable bias: without accounting for ability, the estimated effect of education on income is exaggerated (2,423 PLN per year vs.¬†1,962 PLN per year). The confounding occurs because ability influences both education and income, creating a spurious component in the observed correlation.\n\nClassic Example: Ice Cream and Drownings\nA classic example of confounding involves the correlation between ice cream sales and drowning incidents, both influenced by temperature:\n\n# Create sample data\nn &lt;- 100\ntemperature &lt;- runif(n, 5, 35)  # Temperature in Celsius\n\n# Both ice cream sales and drownings are influenced by temperature\nice_cream_sales &lt;- 100 + 10 * temperature + rnorm(n, 0, 20)\ndrownings &lt;- 1 + 0.3 * temperature + rnorm(n, 0, 1)\n\nconfounding_data &lt;- data.frame(\n  temperature = temperature,\n  ice_cream_sales = ice_cream_sales,\n  drownings = drownings\n)\n\n# Model without controlling for temperature\nmodel_naive &lt;- lm(drownings ~ ice_cream_sales, data = confounding_data)\n\n# Model controlling for temperature\nmodel_full &lt;- lm(drownings ~ ice_cream_sales + temperature, data = confounding_data)\n\n# Show results\nsummary(model_naive)\n\n\nCall:\nlm(formula = drownings ~ ice_cream_sales, data = confounding_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8163 -0.7597  0.0118  0.7846  2.5797 \n\nCoefficients:\n                 Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)     -1.503063   0.370590  -4.056              0.0001 ***\nice_cream_sales  0.028074   0.001205  23.305 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.088 on 98 degrees of freedom\nMultiple R-squared:  0.8471,    Adjusted R-squared:  0.8456 \nF-statistic: 543.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_full)\n\n\nCall:\nlm(formula = drownings ~ ice_cream_sales + temperature, data = confounding_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.85074 -0.61169  0.01186  0.60556  2.01776 \n\nCoefficients:\n                 Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)      1.243785   0.530123   2.346         0.021 *  \nice_cream_sales -0.002262   0.004839  -0.467         0.641    \ntemperature      0.317442   0.049515   6.411 0.00000000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9169 on 97 degrees of freedom\nMultiple R-squared:  0.8926,    Adjusted R-squared:  0.8904 \nF-statistic: 403.2 on 2 and 97 DF,  p-value: &lt; 0.00000000000000022\n\n# Create visualization\nggplot(confounding_data, aes(x = ice_cream_sales, y = drownings, color = temperature)) +\n  geom_point(alpha = 0.8) +\n  scale_color_viridis_c(name = \"Temperature (¬∞C)\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"The Ice Cream and Drownings Correlation\",\n    subtitle = \"Temperature as a confounding variable\",\n    x = \"Ice Cream Sales\",\n    y = \"Drowning Incidents\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe naive model shows a statistically significant relationship between ice cream sales and drownings. However, once temperature is included in the model, the coefficient for ice cream sales decreases substantially and becomes statistically insignificant. This demonstrates how failing to account for confounding variables can lead to spurious correlations.\n\n\n\nReverse Causality\nReverse causality occurs when the assumed direction of causation is incorrect. Consider this example of anxiety and relaxation techniques:\n\n# Create sample data\nn &lt;- 200\nanxiety_level &lt;- runif(n, 1, 10)  # Anxiety level (1-10)\n\n# People with higher anxiety tend to use more relaxation techniques\nrelaxation_techniques &lt;- 1 + 0.7 * anxiety_level + rnorm(n, 0, 1)\n\nreverse_data &lt;- data.frame(\n  anxiety = anxiety_level,\n  relaxation = relaxation_techniques\n)\n\n# Fit models in both directions\nmodel_incorrect &lt;- lm(anxiety ~ relaxation, data = reverse_data)\nmodel_correct &lt;- lm(relaxation ~ anxiety, data = reverse_data)\n\n# Show regression results\nsummary(model_incorrect)\n\n\nCall:\nlm(formula = anxiety ~ relaxation, data = reverse_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9651 -0.7285 -0.0923  0.7247  3.7996 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) -0.09482    0.21973  -0.432               0.667    \nrelaxation   1.15419    0.04105  28.114 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.182 on 198 degrees of freedom\nMultiple R-squared:  0.7997,    Adjusted R-squared:  0.7987 \nF-statistic: 790.4 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_correct)\n\n\nCall:\nlm(formula = relaxation ~ anxiety, data = reverse_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.15178 -0.51571 -0.00222  0.55513  2.04334 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  1.05726    0.15286   6.917      0.0000000000624 ***\nanxiety      0.69284    0.02464  28.114 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9161 on 198 degrees of freedom\nMultiple R-squared:  0.7997,    Adjusted R-squared:  0.7987 \nF-statistic: 790.4 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(reverse_data, aes(x = relaxation, y = anxiety)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Anxiety and Relaxation Techniques\",\n    subtitle = \"Example of reverse causality\",\n    x = \"Use of Relaxation Techniques (frequency/week)\",\n    y = \"Anxiety Level (1-10 scale)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nBoth regression models show statistically significant relationships, but they imply different causal mechanisms. The incorrect model suggests that relaxation techniques increase anxiety, while the correct model reflects the true data generating process: anxiety drives the use of relaxation techniques.\n\n\nCollider Bias (Selection Bias)\nCollider bias occurs when conditioning on a variable that is affected by both the independent and dependent variables of interest, creating an artificial relationship between variables that are actually independent.\n\n# Create sample data\nn &lt;- 1000\n\n# Generate two independent variables (no relationship between them)\nintelligence &lt;- rnorm(n, 100, 15)  # IQ score\nfamily_wealth &lt;- rnorm(n, 50, 15)  # Wealth score (independent from intelligence)\n  \n# True data-generating process: admission depends on both intelligence and wealth\nadmission_score &lt;- 0.4 * intelligence + 0.4 * family_wealth + rnorm(n, 0, 10)\nadmitted &lt;- admission_score &gt; median(admission_score)  # Binary admission variable\n\n# Create full dataset\nfull_data &lt;- data.frame(\n  intelligence = intelligence,\n  wealth = family_wealth,\n  admission_score = admission_score,\n  admitted = admitted\n)\n\n# Regression in full population (true model)\nfull_model &lt;- lm(intelligence ~ wealth, data = full_data)\nsummary(full_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth, data = full_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-49.608 -10.115   0.119  10.832  55.581 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 101.42330    1.73139   58.58 &lt;0.0000000000000002 ***\nwealth       -0.02701    0.03334   -0.81               0.418    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.41 on 998 degrees of freedom\nMultiple R-squared:  0.0006569, Adjusted R-squared:  -0.0003444 \nF-statistic: 0.656 on 1 and 998 DF,  p-value: 0.4182\n\n# Get just the admitted students\nadmitted_only &lt;- full_data[full_data$admitted, ]\n\n# Regression in admitted students (conditioning on the collider)\nadmitted_model &lt;- lm(intelligence ~ wealth, data = admitted_only)\nsummary(admitted_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth, data = admitted_only)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.511  -9.064   0.721   8.965  48.267 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 115.4750     2.6165  44.133 &lt; 0.0000000000000002 ***\nwealth       -0.1704     0.0462  -3.689              0.00025 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.91 on 498 degrees of freedom\nMultiple R-squared:  0.0266,    Adjusted R-squared:  0.02464 \nF-statistic: 13.61 on 1 and 498 DF,  p-value: 0.0002501\n\n# Additional analysis - regression with the collider as a control variable\n# This demonstrates how controlling for a collider introduces bias\ncollider_control_model &lt;- lm(intelligence ~ wealth + admitted, data = full_data)\nsummary(collider_control_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth + admitted, data = full_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.729  -8.871   0.700   8.974  48.044 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  102.90069    1.56858  65.601 &lt; 0.0000000000000002 ***\nwealth        -0.19813    0.03224  -6.145        0.00000000116 ***\nadmittedTRUE  14.09944    0.94256  14.959 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.93 on 997 degrees of freedom\nMultiple R-squared:  0.1838,    Adjusted R-squared:  0.1822 \nF-statistic: 112.3 on 2 and 997 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Plot for full population\np1 &lt;- ggplot(full_data, aes(x = wealth, y = intelligence)) +\n  geom_point(alpha = 0.3, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Full Population\",\n    subtitle = paste(\"Correlation:\", round(cor(full_data$intelligence, full_data$wealth), 3)),\n    x = \"Family Wealth Score\",\n    y = \"Intelligence Score\"\n  ) +\n  theme_minimal()\n\n# Plot for admitted students\np2 &lt;- ggplot(admitted_only, aes(x = wealth, y = intelligence)) +\n  geom_point(alpha = 0.3, color = \"darkgreen\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Admitted Students Only\",\n    subtitle = paste(\"Correlation:\", round(cor(admitted_only$intelligence, admitted_only$wealth), 3)),\n    x = \"Family Wealth Score\",\n    y = \"Intelligence Score\"\n  ) +\n  theme_minimal()\n\n# Display plots side by side\nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis example demonstrates collider bias in three ways:\n\nIn the full population, intelligence and wealth have no relationship (coefficient near zero, p-value = 0.87)\nAmong admitted students (conditioning on the collider), a significant negative relationship appears (coefficient = -0.39, p-value &lt; 0.001)\nWhen controlling for admission status in a regression, a spurious relationship is introduced (coefficient = -0.16, p-value &lt; 0.001)\n\nThe collider bias creates relationships between variables that are truly independent. This can be represented in a directed acyclic graph (DAG):\n\\text{Intelligence} \\rightarrow \\text{Admission} \\leftarrow \\text{Wealth}\nWhen we condition on admission (the collider), we create a spurious association between intelligence and wealth.\n\n\nImproper Analysis\nInappropriate statistical methods can produce spurious correlations. Common issues include using linear models for non-linear relationships, ignoring data clustering, or mishandling time series data.\n\n# Generate data with a true non-linear relationship\nn &lt;- 100\nx &lt;- seq(-3, 3, length.out = n)\ny &lt;- x^2 + rnorm(n, 0, 1)  # Quadratic relationship\n\nimproper_data &lt;- data.frame(x = x, y = y)\n\n# Fit incorrect linear model\nwrong_model &lt;- lm(y ~ x, data = improper_data)\n\n# Fit correct quadratic model\ncorrect_model &lt;- lm(y ~ x + I(x^2), data = improper_data)\n\n# Show results\nsummary(wrong_model)\n\n\nCall:\nlm(formula = y ~ x, data = improper_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2176 -2.1477 -0.6468  2.4365  7.3457 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  3.14689    0.28951  10.870 &lt;0.0000000000000002 ***\nx            0.08123    0.16548   0.491               0.625    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.895 on 98 degrees of freedom\nMultiple R-squared:  0.002453,  Adjusted R-squared:  -0.007726 \nF-statistic: 0.2409 on 1 and 98 DF,  p-value: 0.6246\n\nsummary(correct_model)\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = improper_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81022 -0.65587  0.01935  0.61168  2.68894 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  0.12407    0.14498   0.856               0.394    \nx            0.08123    0.05524   1.470               0.145    \nI(x^2)       0.98766    0.03531  27.972 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9664 on 97 degrees of freedom\nMultiple R-squared:   0.89, Adjusted R-squared:  0.8877 \nF-statistic: 392.3 on 2 and 97 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(improper_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), color = \"green\", se = FALSE) +\n  labs(\n    title = \"Improper Analysis Example\",\n    subtitle = \"Linear model (red) vs. Quadratic model (green)\",\n    x = \"Variable X\",\n    y = \"Variable Y\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe linear model incorrectly suggests no relationship between x and y (coefficient near zero, p-value = 0.847), while the quadratic model reveals the true relationship (R^2 = 0.90). This demonstrates how model misspecification can create spurious non-correlations, masking real relationships that exist in different forms.\n\n\nEndogeneity and Its Sources\nEndogeneity occurs when an explanatory variable is correlated with the error term in a regression model. This violates the exogeneity assumption of OLS regression and leads to biased estimates. There are several sources of endogeneity:\n\nOmitted Variable Bias\nAs shown in the education-income example, when important variables are omitted from the model, their effects are absorbed into the error term, which becomes correlated with included variables.\n\n\nMeasurement Error\nWhen variables are measured with error, the observed values differ from true values, creating correlation between the error term and the predictors.\n\n\nSimultaneity (Bidirectional Causality)\nWhen the dependent variable also affects the independent variable, creating a feedback loop. Let‚Äôs demonstrate this:\n\n# Create sample data with mutual influence\nn &lt;- 100\n\n# Initialize variables\neconomic_growth &lt;- rnorm(n, 2, 1)\nemployment_rate &lt;- rnorm(n, 60, 5)\n\n# Create mutual influence through iterations\nfor(i in 1:3) {\n  economic_growth &lt;- 2 + 0.05 * employment_rate + rnorm(n, 0, 0.5)\n  employment_rate &lt;- 50 + 5 * economic_growth + rnorm(n, 0, 2)\n}\n\nsimultaneity_data &lt;- data.frame(\n  growth = economic_growth,\n  employment = employment_rate\n)\n\n# Model estimating effect of growth on employment\nmodel_growth_on_emp &lt;- lm(employment ~ growth, data = simultaneity_data)\n\n# Model estimating effect of employment on growth\nmodel_emp_on_growth &lt;- lm(growth ~ employment, data = simultaneity_data)\n\n# Show results\nsummary(model_growth_on_emp)\n\n\nCall:\nlm(formula = employment ~ growth, data = simultaneity_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.603 -1.500 -0.099  1.387  5.673 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  49.9665     2.0717   24.12 &lt;0.0000000000000002 ***\ngrowth        5.0151     0.3528   14.22 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.045 on 98 degrees of freedom\nMultiple R-squared:  0.6734,    Adjusted R-squared:  0.6701 \nF-statistic: 202.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_emp_on_growth)\n\n\nCall:\nlm(formula = growth ~ employment, data = simultaneity_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11417 -0.20626 -0.02185  0.22646  0.72941 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) -4.801257   0.749557  -6.405        0.00000000523 ***\nemployment   0.134283   0.009446  14.216 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3346 on 98 degrees of freedom\nMultiple R-squared:  0.6734,    Adjusted R-squared:  0.6701 \nF-statistic: 202.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(simultaneity_data, aes(x = growth, y = employment)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Simultaneity Between Economic Growth and Employment\",\n    x = \"Economic Growth (%)\",\n    y = \"Employment Rate (%)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe true data generating process is a system of simultaneous equations:\n\\text{Growth}_i = \\alpha_0 + \\alpha_1 \\text{Employment}_i + u_i \\text{Employment}_i = \\beta_0 + \\beta_1 \\text{Growth}_i + v_i\nStandard OLS regression cannot consistently estimate either equation because each explanatory variable is correlated with the error term in its respective equation.\n\n\nSelection Bias\nWhen the sample is not randomly selected from the population, the selection process can introduce correlation between the error term and the predictors. The collider bias example demonstrates a form of selection bias.\nThe consequences of endogeneity include: - Biased coefficient estimates - Incorrect standard errors - Invalid hypothesis tests - Misleading causal interpretations\nAddressing endogeneity requires specialized methods such as instrumental variables, system estimation, panel data methods, or experimental designs.\n\n\n\n\n\n\nUnderstanding Endogeneity in Regression\n\n\n\nEndogeneity is a critical concept in statistical analysis that occurs when an explanatory variable in a regression model is correlated with the error term. This creates challenges for accurately understanding cause-and-effect relationships in research. Let‚Äôs examine the three main types of endogeneity and how they affect research outcomes.\n\nOmitted Variable Bias (OVB)\nOmitted Variable Bias occurs when an important variable that affects both the dependent and independent variables is left out of the analysis. This omission leads to incorrect conclusions about the relationship between the variables we‚Äôre studying.\nConsider a study examining the relationship between education and income:\nExample: Education and Income The observed relationship shows that more education correlates with higher income. However, an individual‚Äôs inherent abilities affect both their educational attainment and their earning potential. Without accounting for ability, we may overestimate education‚Äôs direct effect on income.\nThe statistical representation shows why this matters:\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2z_i + \\epsilon_i (Complete model)\ny_i = \\beta_0 + \\beta_1x_i + u_i (Incomplete model)\nWhen we omit an important variable, our estimates of the remaining relationships become biased and unreliable.\n\n\nSimultaneity\nSimultaneity occurs when two variables simultaneously influence each other, making it difficult to determine the direction of causation. This creates a feedback loop that complicates statistical analysis.\nCommon Examples of Simultaneity:\nAcademic Performance and Study Habits represent a clear case of simultaneity. Academic performance influences how much time students dedicate to studying, while study time affects academic performance. This two-way relationship makes it challenging to measure the isolated effect of either variable.\nMarket Dynamics provide another example. Prices influence demand, while demand influences prices. This concurrent relationship requires special analytical approaches to understand the true relationships.\n\n\nMeasurement Error\nMeasurement error occurs when we cannot accurately measure our variables of interest. This imprecision can significantly impact our analysis and conclusions.\nCommon Sources of Measurement Error:\nSelf-Reported Data presents a significant challenge. When participants report their own behaviors or characteristics, such as study time, the reported values often differ from actual values. This discrepancy affects our ability to measure true relationships.\nTechnical Limitations also contribute to measurement error through imprecise measuring tools, inconsistent measurement conditions, and recording or data entry errors.\n\n\nAddressing Endogeneity in Research\n\nIdentification Strategies\n\n# Example of controlling for omitted variables\nmodel_simple &lt;- lm(income ~ education, data = df)\nmodel_full &lt;- lm(income ~ education + ability + experience + region, data = df)\n\n# Compare coefficients\nsummary(model_simple)\nsummary(model_full)\n\n\nInclude Additional Variables: Collect data on potentially important omitted variables and include relevant control variables in your analysis. For example, including measures of ability when studying education‚Äôs effect on income.\nUse Panel Data: Collect data across multiple time periods to control for unobserved fixed characteristics and analyze changes over time.\nInstrumental Variables: Find variables that affect your independent variable but not your dependent variable to isolate the relationship of interest.\n\n\n\nImproving Measurement\n\nMultiple Measurements: Take several measurements of key variables, use averaging to reduce random error, and compare different measurement methods.\nBetter Data Collection: Use validated measurement instruments, implement quality control procedures, and document potential sources of error.\n\n\n\n\nBest Practices for Researchers\nResearch Design fundamentally shapes your ability to address endogeneity. Plan for potential endogeneity issues before collecting data, include measures for potentially important control variables, and consider using multiple measurement approaches.\nAnalysis should include testing for endogeneity when possible, using appropriate statistical methods for your specific situation, and documenting assumptions and limitations.\nReporting must clearly describe potential endogeneity concerns, explain how you addressed these issues, and discuss implications for your conclusions.\n\n\n\n\n\n\n\n\n\nFormal Derivation of OLS Estimators: A Complete Mathematical Treatment\n\n\n\n\nObjective and Setup\nWe seek to find the values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that minimize the sum of squared residuals:\nSSE = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nThis is an unconstrained optimization problem where we treat SSE as a function of two variables: SSE(\\hat{\\beta}_0, \\hat{\\beta}_1).\n\n\nMathematical Prerequisites\nChain Rule for Composite Functions: For f(g(x)), the derivative is: \\frac{d}{dx}[f(g(x))] = f'(g(x)) \\cdot g'(x)\nIn our context:\n\nOuter function: f(u) = u^2 with derivative f'(u) = 2u\nInner function: u = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\n\nFirst-Order Conditions: At a minimum, both partial derivatives equal zero: \\frac{\\partial SSE}{\\partial \\hat{\\beta}_0} = 0 \\quad \\text{and} \\quad \\frac{\\partial SSE}{\\partial \\hat{\\beta}_1} = 0\n\n\nDerivation of \\hat{\\beta}_0\nStep 1: Take the partial derivative with respect to \\hat{\\beta}_0:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_0} = \\frac{\\partial}{\\partial \\hat{\\beta}_0} \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2\nStep 2: Apply the chain rule to each term: = \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot \\frac{\\partial}{\\partial \\hat{\\beta}_0}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\n= \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot (-1)\n= -2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\nStep 3: Set equal to zero and solve: -2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\nDividing by -2 and expanding: \\sum_{i=1}^n y_i - n\\hat{\\beta}_0 - \\hat{\\beta}_1\\sum_{i=1}^n x_i = 0\nStep 4: Isolate \\hat{\\beta}_0: n\\hat{\\beta}_0 = \\sum_{i=1}^n y_i - \\hat{\\beta}_1\\sum_{i=1}^n x_i\n\\boxed{\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}}\nInterpretation: The intercept adjusts to ensure the regression line passes through the point of means (\\bar{x}, \\bar{y}).\n\n\nDerivation of \\hat{\\beta}_1\nStep 1: Take the partial derivative with respect to \\hat{\\beta}_1:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_1} = \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot (-x_i)\n= -2\\sum_{i=1}^n x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\nStep 2: Substitute the expression for \\hat{\\beta}_0: = -2\\sum_{i=1}^n x_i(y_i - (\\bar{y} - \\hat{\\beta}_1\\bar{x}) - \\hat{\\beta}_1x_i)\n= -2\\sum_{i=1}^n x_i((y_i - \\bar{y}) - \\hat{\\beta}_1(x_i - \\bar{x}))\nStep 3: Set equal to zero and expand: \\sum_{i=1}^n x_i(y_i - \\bar{y}) - \\hat{\\beta}_1\\sum_{i=1}^n x_i(x_i - \\bar{x}) = 0\nStep 4: Use the algebraic identity \\sum_{i=1}^n x_i(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}):\nThis identity holds because: \\sum_{i=1}^n x_i(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x} + \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) + \\bar{x}\\sum_{i=1}^n(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) + 0\nSimilarly, \\sum_{i=1}^n x_i(x_i - \\bar{x}) = \\sum_{i=1}^n (x_i - \\bar{x})^2.\nStep 5: Solve for \\hat{\\beta}_1: \\hat{\\beta}_1\\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\n\\boxed{\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}}\n\n\nVerification of Minimum (Second-Order Conditions)\nTo confirm we have found a minimum (not a maximum or saddle point), we examine the Hessian matrix of second partial derivatives:\nSecond partial derivatives:\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_0^2} = 2n &gt; 0\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_1^2} = 2\\sum_{i=1}^n x_i^2 &gt; 0\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_0 \\partial \\hat{\\beta}_1} = 2\\sum_{i=1}^n x_i\nHessian matrix: \\mathbf{H} = 2\\begin{bmatrix} n & \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i & \\sum_{i=1}^n x_i^2 \\end{bmatrix}\nPositive definiteness check:\n\nFirst leading principal minor: 2n &gt; 0 ‚úì\nSecond leading principal minor (determinant): \\det(\\mathbf{H}) = 4\\left(n\\sum_{i=1}^n x_i^2 - \\left(\\sum_{i=1}^n x_i\\right)^2\\right) = 4n\\sum_{i=1}^n(x_i - \\bar{x})^2 &gt; 0 ‚úì\n\nSince the Hessian is positive definite, we have confirmed a minimum.\n\n\nGeometric Interpretation\n\n# Visualizing the optimization surface\nlibrary(tidyverse)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Generate sample data\nset.seed(42)\nx &lt;- runif(20, 1, 8)\ny &lt;- 2 + 3*x + rnorm(20, 0, 1)\n\n# Create grid of beta values\nbeta0_seq &lt;- seq(0, 4, length.out = 50)\nbeta1_seq &lt;- seq(2, 4, length.out = 50)\ngrid &lt;- expand.grid(beta0 = beta0_seq, beta1 = beta1_seq)\n\n# Calculate SSE for each combination\ngrid$SSE &lt;- apply(grid, 1, function(params) {\n  sum((y - (params[1] + params[2]*x))^2)\n})\n\n# Create contour plot\nggplot(grid, aes(x = beta0, y = beta1, z = SSE)) +\n  geom_contour_filled(aes(fill = after_stat(level))) +\n  geom_point(x = coef(lm(y ~ x))[1], \n             y = coef(lm(y ~ x))[2], \n             color = \"red\", size = 3) +\n  labs(title = \"SSE Surface in Parameter Space\",\n       subtitle = \"Red point shows the OLS minimum\",\n       x = expression(hat(beta)[0]),\n       y = expression(hat(beta)[1])) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.1-understanding-correlation-measures-a-self-study-tutorial-using-stress-level-and-cognitive-performance-data",
    "href": "correg_en.html#appendix-a.1-understanding-correlation-measures-a-self-study-tutorial-using-stress-level-and-cognitive-performance-data",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.37 Appendix A.1: Understanding Correlation Measures: A Self-Study Tutorial (Using Stress Level and Cognitive Performance Data)",
    "text": "15.37 Appendix A.1: Understanding Correlation Measures: A Self-Study Tutorial (Using Stress Level and Cognitive Performance Data)\n\nDataset Overview\n\ndata &lt;- data.frame(\n  anxiety_level = c(8, 5, 11, 14, 7, 10),\n  cognitive_performance = c(85, 90, 62, 55, 80, 65)\n)\n\n\n\n1. Covariance Calculation\n\nStep 1: Calculate Means\n\n\n\n\n\n\n\n\nVariable\nCalculation\nResult\n\n\n\n\nMean Anxiety (\\bar{x})\n(8 + 5 + 11 + 14 + 7 + 10) √∑ 6\n9.17\n\n\nMean Cognitive (\\bar{y})\n(85 + 90 + 62 + 55 + 80 + 65) √∑ 6\n72.83\n\n\n\n\n\nStep 2: Calculate Deviations and Products\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\n(x_i - \\bar{x})\n(y_i - \\bar{y})\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n1\n8\n85\n-1.17\n12.17\n-14.24\n\n\n2\n5\n90\n-4.17\n17.17\n-71.60\n\n\n3\n11\n62\n1.83\n-10.83\n-19.82\n\n\n4\n14\n55\n4.83\n-17.83\n-86.12\n\n\n5\n7\n80\n-2.17\n7.17\n-15.56\n\n\n6\n10\n65\n0.83\n-7.83\n-6.50\n\n\nSum\n55\n437\n0.00\n0.00\n-213.84\n\n\n\n\n\nStep 3: Calculate Covariance\n \\text{Cov}(X,Y) = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n-1} = \\frac{-213.84}{5} = -42.77 \n\n\n\n2. Pearson Correlation Coefficient\n\nStep 1: Calculate Squared Deviations\n\n\n\n\n\n\n\n\n\n\ni\n(x_i - \\bar{x})\n(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n-1.17\n12.17\n1.37\n148.11\n\n\n2\n-4.17\n17.17\n17.39\n294.81\n\n\n3\n1.83\n-10.83\n3.35\n117.29\n\n\n4\n4.83\n-17.83\n23.33\n317.91\n\n\n5\n-2.17\n7.17\n4.71\n51.41\n\n\n6\n0.83\n-7.83\n0.69\n61.31\n\n\nSum\n0.00\n0.00\n50.84\n990.84\n\n\n\n\n\nStep 2: Calculate Standard Deviations\n\n\n\n\n\n\n\n\n\nMeasure\nFormula\nCalculation\nResult\n\n\n\n\ns_x\n\\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n-1}}\n\\sqrt{\\frac{50.84}{5}}\n3.19\n\n\ns_y\n\\sqrt{\\frac{\\sum (y_i - \\bar{y})^2}{n-1}}\n\\sqrt{\\frac{990.84}{5}}\n14.08\n\n\n\n\n\nStep 3: Calculate Pearson Correlation\n r = \\frac{\\text{Cov}(X,Y)}{s_x s_y} = \\frac{-42.77}{3.19 \\times 14.08} = -0.95 \n\n\n\n3. Spearman Rank Correlation\n\nStep 1: Assign Ranks\n\n\n\ni\nx_i\ny_i\nRank x_i\nRank y_i\nd_i\nd_i^2\n\n\n\n\n1\n8\n85\n3\n2\n1\n1\n\n\n2\n5\n90\n1\n1\n0\n0\n\n\n3\n11\n62\n5\n5\n0\n0\n\n\n4\n14\n55\n6\n6\n0\n0\n\n\n5\n7\n80\n2\n3\n-1\n1\n\n\n6\n10\n65\n4\n4\n0\n0\n\n\nSum\n\n\n\n\n\n2\n\n\n\n\n\nStep 2: Calculate Spearman Correlation\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} = 1 - \\frac{6(2)}{6(36-1)} = 1 - \\frac{12}{210} = 0.94 \n\n\n\nVerification using R\n\n# Calculate correlations using R\ncor(data$anxiety_level, data$cognitive_performance, method = \"pearson\")\n\n[1] -0.9527979\n\ncor(data$anxiety_level, data$cognitive_performance, method = \"spearman\")\n\n[1] -0.9428571\n\n\n\n\nInterpretation\n\nThe strong negative Pearson correlation (r = -0.95) indicates a very strong negative linear relationship between anxiety level and cognitive performance.\nThe strong positive Spearman correlation (œÅ = 0.94) shows that the relationship is also strongly monotonic.\nThe difference between Pearson and Spearman correlations suggests that while there is a strong relationship, it might not be perfectly linear.\n\n\n\nExercise\n\nVerify each calculation step in the tables above.\nTry calculating these measures with a modified dataset:\n\nAdd one outlier and observe how it affects both correlation coefficients\nChange one pair of values and recalculate",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.2-calculating-covariance-pearson-spearman-correlation-and-ols---worked-example",
    "href": "correg_en.html#appendix-a.2-calculating-covariance-pearson-spearman-correlation-and-ols---worked-example",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.38 Appendix A.2: Calculating Covariance, Pearson, Spearman Correlation, and OLS - worked example",
    "text": "15.38 Appendix A.2: Calculating Covariance, Pearson, Spearman Correlation, and OLS - worked example\nA political science student is investigating the relationship between district magnitude (DM) and Gallagher‚Äôs disproportionality index (GH) in parliamentary elections across 10 randomly selected democracies.\nData on electoral district magnitudes (\\text{DM}) and Gallagher index:\n\n\n\n\\text{DM} (X)\nGallagher (Y)\n\n\n\n\n2\n18.2\n\n\n3\n16.7\n\n\n4\n15.8\n\n\n5\n15.3\n\n\n6\n15.0\n\n\n7\n14.8\n\n\n8\n14.7\n\n\n9\n14.6\n\n\n10\n14.55\n\n\n11\n14.52\n\n\n\n\nStep 1: Calculate Basic Statistics\nCalculation of means:\nFor \\text{DM} (X): \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\nDetailed calculation:\n2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 65 \\bar{x} = \\frac{65}{10} = 6.5\nFor Gallagher index (Y): \\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\nDetailed calculation:\n18.2 + 16.7 + 15.8 + 15.3 + 15.0 + 14.8 + 14.7 + 14.6 + 14.55 + 14.52 = 154.17 \\bar{y} = \\frac{154.17}{10} = 15.417\n\n\nStep 2: Detailed Covariance Calculations\nComplete working table showing all calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18.2\n-4.5\n2.783\n-12.5235\n20.25\n7.7451\n\n\n2\n3\n16.7\n-3.5\n1.283\n-4.4905\n12.25\n1.6461\n\n\n3\n4\n15.8\n-2.5\n0.383\n-0.9575\n6.25\n0.1467\n\n\n4\n5\n15.3\n-1.5\n-0.117\n0.1755\n2.25\n0.0137\n\n\n5\n6\n15.0\n-0.5\n-0.417\n0.2085\n0.25\n0.1739\n\n\n6\n7\n14.8\n0.5\n-0.617\n-0.3085\n0.25\n0.3807\n\n\n7\n8\n14.7\n1.5\n-0.717\n-1.0755\n2.25\n0.5141\n\n\n8\n9\n14.6\n2.5\n-0.817\n-2.0425\n6.25\n0.6675\n\n\n9\n10\n14.55\n3.5\n-0.867\n-3.0345\n12.25\n0.7517\n\n\n10\n11\n14.52\n4.5\n-0.897\n-4.0365\n20.25\n0.8047\n\n\nSum\n65\n154.17\n0\n0\n-28.085\n82.5\n12.8442\n\n\n\nCovariance calculation: \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\text{Cov}(X,Y) = \\frac{-28.085}{9} = -3.120556\n\n\nStep 3: Standard Deviation Calculations\nFor \\text{DM} (X): \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}}\n\\sigma_x = \\sqrt{\\frac{82.5}{9}} = \\sqrt{9.1667} = 3.026582\nFor Gallagher (Y): \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}}\n\\sigma_y = \\sqrt{\\frac{12.8442}{9}} = \\sqrt{1.4271} = 1.194612\n\n\nStep 4: Pearson Correlation Calculation\nr = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\nr = \\frac{-3.120556}{3.026582 \\times 1.194612} = \\frac{-3.120556}{3.615752} = -0.863044\n\n\nStep 5: Spearman Rank Correlation Calculation\nComplete ranking table with all calculations:\n\n\n\ni\nX_i\nY_i\nRank X_i\nRank Y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n18.2\n1\n10\n-9\n81\n\n\n2\n3\n16.7\n2\n9\n-7\n49\n\n\n3\n4\n15.8\n3\n8\n-5\n25\n\n\n4\n5\n15.3\n4\n7\n-3\n9\n\n\n5\n6\n15.0\n5\n6\n-1\n1\n\n\n6\n7\n14.8\n6\n5\n1\n1\n\n\n7\n8\n14.7\n7\n4\n3\n9\n\n\n8\n9\n14.6\n8\n3\n5\n25\n\n\n9\n10\n14.55\n9\n2\n7\n49\n\n\n10\n11\n14.52\n10\n1\n9\n81\n\n\nSum\n\n\n\n\n\n330\n\n\n\nSpearman correlation calculation: \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n\\rho = 1 - \\frac{6 \\times 330}{10(100 - 1)} = 1 - \\frac{1980}{990} = 1 - 2 = -1\n\n\nStep 6: R Verification\n\n# Create vectors\nDM &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nGH &lt;- c(18.2, 16.7, 15.8, 15.3, 15.0, 14.8, 14.7, 14.6, 14.55, 14.52)\n\n# Calculate covariance\ncov(DM, GH)\n\n[1] -3.120556\n\n# Calculate correlations\ncor(DM, GH, method = \"pearson\")\n\n[1] -0.8627742\n\ncor(DM, GH, method = \"spearman\")\n\n[1] -1\n\n\n\n\nStep 7: Basic Visualization\n\nlibrary(ggplot2)\n\n# Create data frame\ndata &lt;- data.frame(DM = DM, GH = GH)\n\n# Create scatter plot\nggplot(data, aes(x = DM, y = GH)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"District Magnitude vs Gallagher Index\",\n    x = \"District Magnitude (DM)\",\n    y = \"Gallagher Index (GH)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nOLS Estimation and Goodness-of-Fit Measures\n\n\nStep 1: Calculate OLS Estimates\nUsing previously calculated values:\n\n\\sum(X_i - \\bar{X})(Y_i - \\bar{Y}) = -28.085\n\\sum(X_i - \\bar{X})^2 = 82.5\n\\bar{X} = 6.5\n\\bar{Y} = 15.417\n\nCalculate slope (\\hat{\\beta_1}): \\hat{\\beta_1} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\hat{\\beta_1} = -28,085 √∑ 82,5 = -0,3404\nCalculate intercept (\\hat{\\beta_0}): \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 15,417 - (-0,3404 √ó 6,5)\n   = 15,417 + 2,2126\n   = 17,6296\nTherefore, the OLS regression equation is: \\hat{Y} = 17.6296 - 0.3404X\n\n\nStep 2: Calculate Fitted Values and Residuals\nComplete table showing all calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i\ne_i = Y_i - \\hat{Y}_i\ne_i^2\n(Y_i - \\bar{Y})^2\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18.2\n16.9488\n1.2512\n1.5655\n7.7451\n2.3404\n\n\n2\n3\n16.7\n16.6084\n0.0916\n0.0084\n1.6461\n1.4241\n\n\n3\n4\n15.8\n16.2680\n-0.4680\n0.2190\n0.1467\n0.7225\n\n\n4\n5\n15.3\n15.9276\n-0.6276\n0.3939\n0.0137\n0.2601\n\n\n5\n6\n15.0\n15.5872\n-0.5872\n0.3448\n0.1739\n0.0289\n\n\n6\n7\n14.8\n15.2468\n-0.4468\n0.1996\n0.3807\n0.0290\n\n\n7\n8\n14.7\n14.9064\n-0.2064\n0.0426\n0.5141\n0.2610\n\n\n8\n9\n14.6\n14.5660\n0.0340\n0.0012\n0.6675\n0.7241\n\n\n9\n10\n14.55\n14.2256\n0.3244\n0.1052\n0.7517\n1.4184\n\n\n10\n11\n14.52\n13.8852\n0.6348\n0.4030\n0.8047\n2.3439\n\n\nSum\n65\n154.17\n154.17\n0\n3.2832\n12.8442\n9.5524\n\n\n\nCalculations for fitted values:\nFor X = 2:\n≈∂ = 17.6296 + (-0.3404 √ó 2) = 16.9488\n\nFor X = 3:\n≈∂ = 17.6296 + (-0.3404 √ó 3) = 16.6084\n\n[... continue for all values]\n\n\nStep 3: Calculate Goodness-of-Fit Measures\nSum of Squared Errors (SSE): SSE = \\sum e_i^2\nSSE = 3.2832\nSum of Squared Total (SST): SST = \\sum(Y_i - \\bar{Y})^2\nSST = 12.8442\nSum of Squared Regression (SSR): SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\nSSR = 9.5524\nVerify decomposition: SST = SSR + SSE\n12.8442 = 9.5524 + 3.2832 (within rounding error)\nR-squared calculation: R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nR¬≤ = 9.5524 √∑ 12.8442\n   = 0.7438\n\n\nStep 4: R Verification\n\n# Fit linear model\nmodel &lt;- lm(GH ~ DM, data = data)\n\n# View summary statistics\nsummary(model)\n\n\nCall:\nlm(formula = GH ~ DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62764 -0.46274 -0.08615  0.26624  1.25109 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept) 17.62976    0.50121  35.174 0.000000000467 ***\nDM          -0.34042    0.07053  -4.827        0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6406 on 8 degrees of freedom\nMultiple R-squared:  0.7444,    Adjusted R-squared:  0.7124 \nF-statistic:  23.3 on 1 and 8 DF,  p-value: 0.00131\n\n# Calculate R-squared manually\nSST &lt;- sum((GH - mean(GH))^2)\nSSE &lt;- sum(residuals(model)^2)\nSSR &lt;- SST - SSE\nR2_manual &lt;- SSR/SST\nR2_manual\n\n[1] 0.7443793\n\n\n\n\nStep 5: Residual Analysis\n\n# Create residual plots\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\nStep 6: Predicted vs Actual Values Plot\n\n# Create predicted vs actual plot\nggplot(data.frame(\n  Actual = GH,\n  Predicted = fitted(model)\n), aes(x = Predicted, y = Actual)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Predicted vs Actual Values\",\n    x = \"Predicted Gallagher Index\",\n    y = \"Actual Gallagher Index\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nLog-Transformed Models\n\n\nStep 1: Data Transformation\nFirst, calculate natural logarithms of variables:\n\n\n\ni\nX_i\nY_i\n\\ln(X_i)\n\\ln(Y_i)\n\n\n\n\n1\n2\n18.2\n0.6931\n2.9014\n\n\n2\n3\n16.7\n1.0986\n2.8154\n\n\n3\n4\n15.8\n1.3863\n2.7600\n\n\n4\n5\n15.3\n1.6094\n2.7278\n\n\n5\n6\n15.0\n1.7918\n2.7081\n\n\n6\n7\n14.8\n1.9459\n2.6946\n\n\n7\n8\n14.7\n2.0794\n2.6878\n\n\n8\n9\n14.6\n2.1972\n2.6810\n\n\n9\n10\n14.55\n2.3026\n2.6777\n\n\n10\n11\n14.52\n2.3979\n2.6757\n\n\n\n\n\nStep 2: Compare Different Model Specifications\nWe estimate three alternative specifications:\n\nLog-linear model: \\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nLinear-log model: Y_i = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\nLog-log model: \\ln(Y_i) = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\n\n\n# Create transformed variables\ndata$log_DM &lt;- log(data$DM)\ndata$log_GH &lt;- log(data$GH)\n\n# Fit models\nmodel_linear &lt;- lm(GH ~ DM, data = data)\nmodel_loglinear &lt;- lm(log_GH ~ DM, data = data)\nmodel_linearlog &lt;- lm(GH ~ log_DM, data = data)\nmodel_loglog &lt;- lm(log_GH ~ log_DM, data = data)\n\n# Compare R-squared values\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Linear\", \"Log-linear\", \"Linear-log\", \"Log-log\"),\n  R_squared = c(\n    summary(model_linear)$r.squared,\n    summary(model_loglinear)$r.squared,\n    summary(model_linearlog)$r.squared,\n    summary(model_loglog)$r.squared\n  )\n)\n\n# Display comparison\nmodels_comparison\n\n       Model R_squared\n1     Linear 0.7443793\n2 Log-linear 0.7670346\n3 Linear-log 0.9141560\n4    Log-log 0.9288088\n\n\n\n\nStep 3: Visual Comparison\n\n# Create plots for each model\np1 &lt;- ggplot(data, aes(x = DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Linear Model\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Log-linear Model\") +\n  theme_minimal()\n\np3 &lt;- ggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Linear-log Model\") +\n  theme_minimal()\n\np4 &lt;- ggplot(data, aes(x = log_DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Log-log Model\") +\n  theme_minimal()\n\n# Arrange plots in a grid\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Residual Analysis for Best Model\nBased on R-squared values, analyze residuals for the best-fitting model:\n\n# Residual plots for best model\npar(mfrow = c(2, 2))\nplot(model_linearlog)\n\n\n\n\n\n\n\n\n\n\nStep 5: Interpretation of Best Model\nThe linear-log model coefficients:\n\nsummary(model_linearlog)\n\n\nCall:\nlm(formula = GH ~ log_DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40702 -0.30207 -0.04907  0.22905  0.60549 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)  19.0223     0.4079   46.64 0.0000000000494 ***\nlog_DM       -2.0599     0.2232   -9.23 0.0000153880425 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3712 on 8 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9034 \nF-statistic: 85.19 on 1 and 8 DF,  p-value: 0.00001539\n\n\nInterpretation: - \\hat{\\beta_0} represents the expected Gallagher Index when ln(DM) = 0 (i.e., when DM = 1) - \\hat{\\beta_1} represents the change in Gallagher Index associated with a one-unit increase in ln(DM)\n\n\nStep 6: Model Predictions\n\n# Create prediction plot for best model\nggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Linear-log Model: Gallagher Index vs ln(District Magnitude)\",\n    x = \"ln(District Magnitude)\",\n    y = \"Gallagher Index\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nStep 7: Elasticity Analysis\nFor the log-log model, coefficients represent elasticities directly. Calculate average elasticity for the linear-log model:\n\n# Calculate elasticity at means\nmean_DM &lt;- mean(data$DM)\nmean_GH &lt;- mean(data$GH)\nbeta1 &lt;- coef(model_linearlog)[2]\nelasticity &lt;- beta1 * (1/mean_GH)\nelasticity\n\n    log_DM \n-0.1336136 \n\n\nThis represents the percentage change in the Gallagher Index for a 1% change in District Magnitude.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.3-understanding-pearson-spearman-and-kendall",
    "href": "correg_en.html#appendix-a.3-understanding-pearson-spearman-and-kendall",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.39 Appendix A.3: Understanding Pearson, Spearman, and Kendall",
    "text": "15.39 Appendix A.3: Understanding Pearson, Spearman, and Kendall\n\nDataset\n\ndata &lt;- data.frame(\n  x = c(2, 4, 5, 3, 8),\n  y = c(3, 5, 4, 4, 7)\n)\n\n\n\nPearson Correlation\n r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}} \n\nStep-by-Step Calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n2\n3\n-2.4\n-1.6\n3.84\n5.76\n2.56\n\n\n2\n4\n5\n-0.4\n0.4\n-0.16\n0.16\n0.16\n\n\n3\n5\n4\n0.6\n-0.6\n-0.36\n0.36\n0.36\n\n\n4\n3\n4\n-1.4\n-0.6\n0.84\n1.96\n0.36\n\n\n5\n8\n7\n3.6\n2.4\n8.64\n12.96\n5.76\n\n\nSum\n22\n23\n0\n0\n12.8\n21.2\n9.2\n\n\n\n\\bar{x} = 4.4 \\bar{y} = 4.6\n r = \\frac{12.8}{\\sqrt{21.2 \\times 9.2}} = \\frac{12.8}{\\sqrt{195.04}} = \\frac{12.8}{13.97} = 0.92 \n\n\n\nSpearman Correlation\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} \n\nStep-by-Step Calculations:\n\n\n\ni\nx_i\ny_i\nRank x_i\nRank y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n3\n1\n1\n0\n0\n\n\n2\n4\n5\n3\n5\n-2\n4\n\n\n3\n5\n4\n4\n2.5\n1.5\n2.25\n\n\n4\n3\n4\n2\n2.5\n-0.5\n0.25\n\n\n5\n8\n7\n5\n4\n1\n1\n\n\nSum\n\n\n\n\n\n7.5\n\n\n\n \\rho = 1 - \\frac{6(7.5)}{5(25-1)} = 1 - \\frac{45}{120} = 0.82 \n\n\n\nKendall‚Äôs Tau\n \\tau = \\frac{\\text{number of concordant pairs} - \\text{number of discordant pairs}}{\\frac{1}{2}n(n-1)} \n\nStep-by-Step Calculations:\n\n\n\nPair (i,j)\nx_i,x_j\ny_i,y_j\nx_j-x_i\ny_j-y_i\nResult\n\n\n\n\n(1,2)\n2,4\n3,5\n+2\n+2\nC\n\n\n(1,3)\n2,5\n3,4\n+3\n+1\nC\n\n\n(1,4)\n2,3\n3,4\n+1\n+1\nC\n\n\n(1,5)\n2,8\n3,7\n+6\n+4\nC\n\n\n(2,3)\n4,5\n5,4\n+1\n-1\nD\n\n\n(2,4)\n4,3\n5,4\n-1\n-1\nC\n\n\n(2,5)\n4,8\n5,7\n+4\n+2\nC\n\n\n(3,4)\n5,3\n4,4\n-2\n0\nD\n\n\n(3,5)\n5,8\n4,7\n+3\n+3\nC\n\n\n(4,5)\n3,8\n4,7\n+5\n+3\nC\n\n\n\nNumber of concordant pairs = 8 Number of discordant pairs = 2  \\tau = \\frac{8-2}{10} = 0.74 \n\n\n\nVerification in R\n\ncat(\"Pearson:\", round(cor(data$x, data$y, method=\"pearson\"), 2), \"\\n\")\n\nPearson: 0.92 \n\ncat(\"Spearman:\", round(cor(data$x, data$y, method=\"spearman\"), 2), \"\\n\")\n\nSpearman: 0.82 \n\ncat(\"Kendall:\", round(cor(data$x, data$y, method=\"kendall\"), 2), \"\\n\")\n\nKendall: 0.74 \n\n\n\n\nInterpretation of Results\n\nPearson Correlation (r = 0.92)\n\nStrong positive linear correlation\nIndicates a very strong linear relationship between variables\n\nSpearman Correlation (œÅ = 0.82)\n\nAlso strong positive correlation\nSlightly lower than Pearson‚Äôs, suggesting some deviations from monotonicity\n\nKendall‚Äôs Tau (œÑ = 0.74)\n\nLowest of the three values, but still indicates strong association\nMore robust to outliers\n\n\n\n\nComparison of Measures\n\nDifferences in Values:\n\nPearson (0.92) - highest value, strong linearity\nSpearman (0.82) - considers only ranking\nKendall (0.74) - most conservative measure\n\nPractical Application:\n\nAll measures confirm strong positive association\nDifferences between measures indicate slight deviations from perfect linearity\nKendall provides the most conservative estimate of relationship strength\n\n\n\n\nExercises\n\nChange y[3] from 4 to 6 and recalculate all three correlations\nAdd an outlier (x=10, y=2) and recalculate correlations\nCompare which measure is most sensitive to changes in the data\n\n\n\nKey Points to Remember\n\nPearson Correlation:\n\nMeasures linear relationship\nMost sensitive to outliers\nRequires interval or ratio data\n\nSpearman Correlation:\n\nMeasures monotonic relationship\nLess sensitive to outliers\nWorks with ordinal data\n\nKendall‚Äôs Tau:\n\nMeasures ordinal association\nMost robust to outliers\nBest for small samples and tied ranks",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-b-bias-in-ols-estimation-with-endogenous-regressors",
    "href": "correg_en.html#appendix-b-bias-in-ols-estimation-with-endogenous-regressors",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.40 Appendix B: Bias in OLS Estimation with Endogenous Regressors",
    "text": "15.40 Appendix B: Bias in OLS Estimation with Endogenous Regressors\nIn this tutorial, we will explore the bias in Ordinary Least Squares (OLS) estimation when the error term is correlated with the explanatory variable, a situation known as endogeneity. We will first derive the bias mathematically and then illustrate it using a simulated dataset in R.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#theoretical-derivation",
    "href": "correg_en.html#theoretical-derivation",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.41 Theoretical Derivation",
    "text": "15.41 Theoretical Derivation\nConsider a data generating process (DGP) where the true relationship between x and y is:\n y = 2x + e \nHowever, there is an endogeneity problem because the error term e is correlated with x in the following way:\n e = 1x + u \nwhere u is an independent error term.\nIf we estimate the simple linear model y = \\hat{\\beta_0} + \\hat{\\beta_1}x + \\varepsilon using OLS, the OLS estimator of \\hat{\\beta_1} will be biased due to the endogeneity issue.\nTo understand the bias, let‚Äôs derive the expected value of the OLS estimator \\hat{\\beta}_1:\n\\begin{align*}\nE[\\hat{\\beta}_1] &= E[(X'X)^{-1}X'y] \\\\\n                 &= E[(X'X)^{-1}X'(2x + 1x + u)] \\\\\n                 &= E[(X'X)^{-1}X'(3x + u)] \\\\\n                 &= 3 + E[(X'X)^{-1}X'u]\n\\end{align*}\nIf the error term u is uncorrelated with x, then E[(X'X)^{-1}X'u] = 0, and the OLS estimator would be unbiased: E[\\hat{\\beta}_1] = 3. However, in this case, the original error term e is correlated with x, so u is also likely to be correlated with x.\nAssuming E[(X'X)^{-1}X'u] \\neq 0, the OLS estimator will be biased:\n\\begin{align*}\n\\text{Bias}(\\hat{\\beta}_1) &= E[\\hat{\\beta}_1] - \\beta_{1,\\text{true}} \\\\\n                           &= 3 + E[(X'X)^{-1}X'u] - 2 \\\\\n                           &= 1 + E[(X'X)^{-1}X'u]\n\\end{align*}\nThe direction and magnitude of the bias will depend on the correlation between x and u. If x and u are positively correlated, the bias will be positive, and the OLS estimator will overestimate the true coefficient. Conversely, if x and u are negatively correlated, the bias will be negative, and the OLS estimator will underestimate the true coefficient.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#simulation-in-r",
    "href": "correg_en.html#simulation-in-r",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.42 Simulation in R",
    "text": "15.42 Simulation in R\nLet‚Äôs create a simple dataset with 10 observations where x is in the interval 1:10, and generate y values based on the given DGP: y = 2x + e, where e = 1x + u, and u is a random error term.\n\nset.seed(123)  # for reproducibility\nx &lt;- 1:10\nu &lt;- rnorm(10, mean = 0, sd = 1)\ne &lt;- 1*x + u\n# e &lt;- 1*x\ny &lt;- 2*x + e\n\n# Generate the data frame\ndata &lt;- data.frame(x = x, y = y)\n\n# Estimate the OLS model\nmodel &lt;- lm(y ~ x, data = data)\n\n# Print the model summary\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1348 -0.5624 -0.1393  0.3854  1.6814 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)   0.5255     0.6673   0.787         0.454    \nx             2.9180     0.1075  27.134 0.00000000367 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9768 on 8 degrees of freedom\nMultiple R-squared:  0.9893,    Adjusted R-squared:  0.9879 \nF-statistic: 736.3 on 1 and 8 DF,  p-value: 0.000000003666\n\n\nIn this example, the true relationship is y = 2x + e, where e = 1x + u. However, when we estimate the OLS model, we get:\n \\hat{y} = 0.18376 + 3.05874x \nThe estimated coefficient for x is 3.05874, which is biased upward from the true value of 2. This bias is due to the correlation between the error term e and the explanatory variable x.\nTo visualize the bias using ggplot2, we can plot the true relationship (y = 2x) and the estimated OLS relationship:\n\nlibrary(ggplot2)\n\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 2, color = \"blue\", linewidth = 1, linetype = \"dashed\") +\n  geom_abline(intercept = coef(model)[1], slope = coef(model)[2], color = \"red\", linewidth = 1) +\n  labs(title = \"True vs. Estimated Relationship\", x = \"x\", y = \"y\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  scale_color_manual(name = \"Lines\", values = c(\"blue\", \"red\"), \n                     labels = c(\"True\", \"OLS\"))\n\n\n\n\n\n\n\n\nThe plot will show that the estimated OLS line (red) is steeper than the true relationship line (blue), illustrating the upward bias in the estimated coefficient.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#conclusion-1",
    "href": "correg_en.html#conclusion-1",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.43 Conclusion",
    "text": "15.43 Conclusion\nIn summary, when the error term is correlated with the explanatory variable (endogeneity), the OLS estimator will be biased. The direction and magnitude of the bias depend on the nature of the correlation between the error term and the explanatory variable. This tutorial demonstrated the bias both mathematically and through a simulated example in R, using ggplot2 for visualization.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-c.-worked-examples",
    "href": "correg_en.html#appendix-c.-worked-examples",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.44 Appendix C. Worked Examples",
    "text": "15.44 Appendix C. Worked Examples",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "href": "correg_en.html#descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.45 Descriptive Statistics and OLS Example - Income and Voter Turnout",
    "text": "15.45 Descriptive Statistics and OLS Example - Income and Voter Turnout\nBackground\nIn preparation for the 2024 municipal elections, the Amsterdam Electoral Commission conducted research on voter participation patterns across different city neighborhoods. A key question emerged:\nDoes economic prosperity of a neighborhood correlate with civic engagement, specifically voter turnout?\nData Collection\nSample: 5 representative neighborhoods in Amsterdam\nTime Period: Data from the 2022 municipal elections\nVariables:\n\nIncome: Average annual household income per capita (thousands ‚Ç¨)\nTurnout: Percentage of registered voters who voted in the election\n\n\nInitial R Output for Reference\n\n# Data\nincome &lt;- c(50, 45, 56, 40, 60)  # thousands ‚Ç¨\nturnout &lt;- c(60, 56, 70, 50, 75) # %\n\n# Full model check\nmodel &lt;- lm(turnout ~ income)\nsummary(model)\n\n\nCall:\nlm(formula = turnout ~ income)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \nincome       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n\n\n\nDispersion Measures\nMeans:\n\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n} = \\frac{50 + 45 + 56 + 40 + 60}{5} = \\frac{251}{5} = 50.2\n\\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n} = \\frac{60 + 56 + 70 + 50 + 75}{5} = \\frac{311}{5} = 62.2\n\n# Verification\nmean(income)  # 50.2\n\n[1] 50.2\n\nmean(turnout) # 62.2\n\n[1] 62.2\n\n\nVariances:\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X: (-0.2, -5.2, 5.8, -10.2, 9.8)\ns^2_X = \\frac{0.04 + 27.04 + 33.64 + 104.04 + 96.04}{4} = \\frac{260.8}{4} = 65.2\nDeviations for Y: (-2.2, -6.2, 7.8, -12.2, 12.8)\ns^2_Y = \\frac{4.84 + 38.44 + 60.84 + 148.84 + 163.84}{4} = \\frac{416.8}{4} = 104.2\n\n# Verification\nvar(income)  # 65.2\n\n[1] 65.2\n\nvar(turnout) # 104.2\n\n[1] 104.2\n\n\n\n\nCovariance and Correlation\nCovariance:\ns_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-0.2 \\times -2.2) = 0.44 (-5.2 \\times -6.2) = 32.24 (5.8 \\times 7.8) = 45.24 (-10.2 \\times -12.2) = 124.44 (9.8 \\times 12.8) = 125.44\ns_{XY} = \\frac{327.8}{4} = 81.95\n\n# Verification\ncov(income, turnout) # 81.95\n\n[1] 81.95\n\n\nCorrelation:\nr_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{81.95}{\\sqrt{65.2}\\sqrt{104.2}} = 0.994\n\n# Verification\ncor(income, turnout) # 0.994\n\n[1] 0.9942402\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{81.95}{65.2} = 1.2571429\nIntercept:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\nStep by step:\n\n1.2571429 \\times 50.2 = 63.1085714\n\\hat{\\beta_0} = 62.2 - 63.1085714 = -0.9085714\n\n\n# Verification\ncoef(model)  # Exact coefficients from R\n\n(Intercept)      income \n -0.8964724   1.2569018 \n\n\n\n\nDetailed Decomposition of Variance and R-squared\nStep 1: Calculate predicted values (\\hat{Y}):\n \\hat{Y} = -0.9085714 + 1.2571429X\nThe predicted values \\hat{Y} for each X value:\nFor X = 50:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (50)\n \\hat{Y} = -0.9085714 + 62.857145 \\hat{Y} = 61.9485736\nFor X = 45:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (45)\n \\hat{Y} = -0.9085714 + 56.5714305 \\hat{Y} = 55.6535591\nFor X = 56:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (56)\n \\hat{Y} = -0.9085714 + 70.4200024 \\hat{Y} = 69.5114310\nFor X = 40:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (40)\n \\hat{Y} = -0.9085714 + 50.2657160 \\hat{Y} = 49.3571446\nFor X = 60:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (60)\n \\hat{Y} = -0.9085714 + 75.4285740 \\hat{Y} = 74.5200026\n\n# Verification of predicted values\ny_hat &lt;- -0.9085714 + 1.2571429 * income\ndata.frame(\n  X = income,\n  Y = turnout,\n  Y_hat = y_hat,\n  row.names = 1:5\n)\n\n   X  Y    Y_hat\n1 50 60 61.94857\n2 45 56 55.66286\n3 56 70 69.49143\n4 40 50 49.37714\n5 60 75 74.52000\n\n\nStep 2: Calculate SST (Total Sum of Squares)\nSST = \\sum(Y_i - \\bar{Y})^2 \\text{ where } \\bar{Y} = 62.2\n(60 - 62.2)^2 = (-2.2)^2 = 4.84 (56 - 62.2)^2 = (-6.2)^2 = 38.44 (70 - 62.2)^2 = (7.8)^2 = 60.84 (50 - 62.2)^2 = (-12.2)^2 = 148.84 (75 - 62.2)^2 = (12.8)^2 = 163.84\nSST = 4.84 + 38.44 + 60.84 + 148.84 + 163.84 = 416.8\nStep 3: Calculate SSR (Regression Sum of Squares)\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\n(61.9485736 - 62.2)^2 = (-0.2514264)^2 = 0.0632151 (55.6535591 - 62.2)^2 = (-6.5464409)^2 = 42.8558689 (69.5114310 - 62.2)^2 = (7.3114310)^2 = 53.4570178 (49.3571446 - 62.2)^2 = (-12.8428554)^2 = 164.9389370 (74.5200026 - 62.2)^2 = (12.3200026)^2 = 151.7824640\nSSR = 413.0975028\nStep 4: Calculate SSE (Error Sum of Squares)\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n(60 - 61.9485736)^2 = (-1.9485736)^2 = 3.7969384 (56 - 55.6535591)^2 = (0.3464409)^2 = 0.1200212 (70 - 69.5114310)^2 = (0.4885690)^2 = 0.2387198 (50 - 49.3571446)^2 = (0.6428554)^2 = 0.4132631 (75 - 74.5200026)^2 = (0.4799974)^2 = 0.2303975\nSSE = 4.7024972\nStep 5: Verify decomposition\nSST = SSR + SSE 416.8 = 413.0975028 + 4.7024972\nStep 6: Calculate R-squared\nR^2 = \\frac{SSR}{SST} = \\frac{413.0975028}{416.8} = 0.9916\n\n# Verification\nsummary(model)$r.squared  # Should match our calculation\n\n[1] 0.9885135\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\ndf &lt;- data.frame(income = income, turnout = turnout)\n\nggplot(df, aes(x = income, y = turnout)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Voter Turnout vs Income per Capita\",\n    x = \"Income per Capita (thousands ‚Ç¨)\",\n    y = \"Voter Turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    text = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\nInterpretation\nThe analysis shows:\n\nA very strong positive correlation (r = 0.994) between income and voter turnout\nThe regression equation \\hat{Y} = -0.9085714 + 1.2571429X indicates that:\n\nFor each ‚Ç¨1,000 increase in income, turnout increases by about 1.26 percentage points\nThe intercept (-0.9086) has little practical meaning as income is never zero\n\nThe R-squared of 0.9916 indicates that 99.16% of the variance in turnout is explained by income",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "href": "correg_en.html#anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.46 Anxiety Levels and Cognitive Performance: A Laboratory Study",
    "text": "15.46 Anxiety Levels and Cognitive Performance: A Laboratory Study\n\nData and Context\nIn a psychology experiment, researchers measured the relationship between anxiety levels (measured by galvanic skin response, GSR) and cognitive performance (score on a working memory task).\n\n# Data\nanxiety &lt;- c(2.1, 3.4, 4.2, 5.1, 5.8, 6.4, 7.2, 8.0)  # GSR readings\nperformance &lt;- c(92, 88, 84, 78, 74, 70, 65, 62)      # Working memory scores\n\n# Initial model check\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8993 -0.6660  0.2162  0.6106  1.5262 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 105.3248     1.3189   79.86 0.00000000026 ***\nanxiety      -5.4407     0.2359  -23.06 0.00000043549 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 6 degrees of freedom\nMultiple R-squared:  0.9888,    Adjusted R-squared:  0.987 \nF-statistic: 531.9 on 1 and 6 DF,  p-value: 0.0000004355\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{2.1 + 3.4 + 4.2 + 5.1 + 5.8 + 6.4 + 7.2 + 8.0}{8} = \\frac{42.2}{8} = 5.275\n\\bar{Y} = \\frac{92 + 88 + 84 + 78 + 74 + 70 + 65 + 62}{8} = \\frac{613}{8} = 76.625\n\n# Verification\nmean(anxiety)\n\n[1] 5.275\n\nmean(performance)\n\n[1] 76.625\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(2.1 - 5.275) = -3.175\n(3.4 - 5.275) = -1.875\n(4.2 - 5.275) = -1.075\n(5.1 - 5.275) = -0.175\n(5.8 - 5.275) = 0.525\n(6.4 - 5.275) = 1.125\n(7.2 - 5.275) = 1.925\n(8.0 - 5.275) = 2.725\n\nSquared deviations:\n10.08063 + 3.51563 + 1.15563 + 0.03063 + 0.27563 + 1.26563 + 3.70563 +\n7.42563 = 27.45500\ns^2_X = \\frac{27.45500}{7} = 3.922143\nSimilarly for Y: Deviations:\n15.375, 11.375, 7.375, 1.375, -2.625, -6.625, -11.625, -14.625\ns^2_Y = \\frac{236.875 + 129.391 + 54.391 + 1.891 + 6.891 + 43.891 + 135.141 + 213.891}{7} = \\frac{822.362}{7} = 117.4803\n\n# Verification\nvar(anxiety)\n\n[1] 3.922143\n\nvar(performance)\n\n[1] 117.4107\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-3.175 √ó 15.375) = -48.815625\n(-1.875 √ó 11.375) = -21.328125\n(-1.075 √ó 7.375) = -7.928125\n(-0.175 √ó 1.375) = -0.240625\n(0.525 √ó -2.625) = -1.378125\n(1.125 √ó -6.625) = -7.453125\n(1.925 √ó -11.625) = -22.378125\n(2.725 √ó -14.625) = -39.853125\nSum = -149.375\ns_{XY} = \\frac{-149.375}{7} = -21.33929\n\n# Verification\ncov(anxiety, performance)\n\n[1] -21.33929\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-21.33929}{\\sqrt{3.922143}\\sqrt{117.4803}} = -0.9932\n\n# Verification\ncor(anxiety, performance)\n\n[1] -0.9944073\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-21.33929}{3.922143} = -5.4407\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-5.4407 √ó 5.275 = -28.6997\n\\hat{\\beta_0} = 76.625 - (-28.6997) = 105.3247\n\n\n# Verification\ncoef(model)\n\n(Intercept)     anxiety \n 105.324804   -5.440721 \n\n\n\n\n4. R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}): \\hat{Y} = 105.3247 - 5.4407X\n\n# Predicted values\ny_hat &lt;- 105.3247 - 5.4407 * anxiety\ndata.frame(\n  Anxiety = anxiety,\n  Performance = performance,\n  Predicted = y_hat,\n  row.names = 1:8\n)\n\n  Anxiety Performance Predicted\n1     2.1          92  93.89923\n2     3.4          88  86.82632\n3     4.2          84  82.47376\n4     5.1          78  77.57713\n5     5.8          74  73.76864\n6     6.4          70  70.50422\n7     7.2          65  66.15166\n8     8.0          62  61.79910\n\n\nStep 2: Sum of Squares\nSST = \\sum(Y_i - \\bar{Y})^2 = 822.362\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 816.3094\nSSE = \\sum(Y_i - \\hat{Y}_i)^2 = 6.0526\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{816.3094}{822.362} = 0.9926\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.9888459\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\n\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Cognitive Performance vs. Anxiety Levels\",\n    x = \"Anxiety (GSR)\",\n    y = \"Performance Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.993) between anxiety and cognitive performance\nFor each unit increase in GSR (anxiety), performance decreases by approximately 5.44 points\nThe model explains 99.26% of the variance in performance scores\nThe relationship appears to be strongly linear, suggesting a reliable anxiety-performance relationship\nThe high intercept (105.32) represents the theoretical maximum performance at zero anxiety\n\n\n\nStudy Limitations\n\nSmall sample size (n=8)\nPossible other confounding variables\nLimited range of anxiety levels\nCross-sectional rather than longitudinal data",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "href": "correg_en.html#district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "title": "15¬† Introduction to Correlation and Regression Analysis",
    "section": "15.47 District Magnitude and Electoral Disproportionality: A Comparative Analysis",
    "text": "15.47 District Magnitude and Electoral Disproportionality: A Comparative Analysis\n\nData Generating Process\nLet‚Äôs set up a DGP where:\n\\begin{aligned}\n& Y_{\\text{Gallagher}} = 12 - 0.8X_{\\text{DM}} + \\varepsilon \\\\\n& \\varepsilon \\sim \\mathcal{N}(0, 1) \\\\\n& X_{\\text{DM}} \\in \\{3, 5, 7, 10, 12, 15\\}\n\\end{aligned}\n\n# DGP\nmagnitude &lt;- c(3, 5, 7, 10, 12, 15)\nepsilon &lt;- rnorm(6, mean = 0, sd = 1)\ngallagher &lt;- 12 - 0.8 * magnitude + epsilon\n\n# Round (sampled from the DGP) Gallagher indices to one decimal place\ngallagher &lt;- round(c(9.0, 7.8, 9.2, 4.1, 2.5, 1.7), 1)\n\n# Show data\ndata.frame(\n  District_Magnitude = magnitude,\n  Gallagher_Index = gallagher\n)\n\n  District_Magnitude Gallagher_Index\n1                  3             9.0\n2                  5             7.8\n3                  7             9.2\n4                 10             4.1\n5                 12             2.5\n6                 15             1.7\n\n# Initial model check\nmodel &lt;- lm(gallagher ~ magnitude)\nsummary(model)\n\n\nCall:\nlm(formula = gallagher ~ magnitude)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.6516 -0.4628  2.3260 -0.6908 -0.9020  0.3813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7349     1.3034   9.003 0.000843 ***\nmagnitude    -0.6944     0.1359  -5.110 0.006934 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.368 on 4 degrees of freedom\nMultiple R-squared:  0.8672,    Adjusted R-squared:  0.834 \nF-statistic: 26.11 on 1 and 4 DF,  p-value: 0.006934\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{3 + 5 + 7 + 10 + 12 + 15}{6} = \\frac{52}{6} = 8.6667\n\\bar{Y} = \\frac{9.0 + 7.8 + 9.2 + 4.1 + 2.5 + 1.7}{6} = \\frac{34.3}{6} = 5.7167\n\n# Verification\nmean(magnitude)\n\n[1] 8.666667\n\nmean(gallagher)\n\n[1] 5.716667\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(3 - 8.6667) = -5.6667\n(5 - 8.6667) = -3.6667\n(7 - 8.6667) = -1.6667\n(10 - 8.6667) = 1.3333\n(12 - 8.6667) = 3.3333\n(15 - 8.6667) = 6.3333\n\nSquared deviations:\n\n32.1115 + 13.4445 + 2.7779 + 1.7777 + 11.1109 + 40.1107 = 101.3332\n\ns^2_X = \\frac{101.3332}{5} = 20.2666\nFor Y: Deviations: 3.2833, 2.0833, 3.4833, -1.6167, -3.2167, -4.0167\ns^2_Y = \\frac{56.3483}{5} = 11.2697\n\n# Verification\nvar(magnitude)\n\n[1] 20.26667\n\nvar(gallagher)\n\n[1] 11.26967\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n\n(-5.6667 √ó 3.2833) = -18.6057\n(-3.6667 √ó 2.0833) = -7.6387\n(-1.6667 √ó 3.4833) = -5.8056\n(1.3333 √ó -1.6167) = -2.1556\n(3.3333 √ó -3.2167) = -10.7223\n(6.3333 √ó -4.0167) = -25.4391\n\nSum = -70.3670\ns_{XY} = \\frac{-70.3670}{5} = -14.0734\n\n# Verification\ncov(magnitude, gallagher)\n\n[1] -14.07333\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-14.0734}{\\sqrt{20.2666}\\sqrt{11.2697}} = -0.9279\n\n# Verification\ncor(magnitude, gallagher)\n\n[1] -0.9312157\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-14.0734}{20.2666} = -0.6944\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-0.6944 √ó 8.6667 = -6.0181\n\\hat{\\beta_0} = 5.7167 - (-6.0181) = 11.7348\n\n\n# Verification\ncoef(model)\n\n(Intercept)   magnitude \n 11.7348684  -0.6944079 \n\n\n\n\nR-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = 11.7348 - 0.6944X\n\n# Predicted values\ny_hat &lt;- 11.7348 - 0.6944 * magnitude\ndata.frame(\n  Magnitude = magnitude,\n  Gallagher = gallagher,\n  Predicted = y_hat,\n  row.names = 1:6\n)\n\n  Magnitude Gallagher Predicted\n1         3       9.0    9.6516\n2         5       7.8    8.2628\n3         7       9.2    6.8740\n4        10       4.1    4.7908\n5        12       2.5    3.4020\n6        15       1.7    1.3188\n\n\nStep 2: Sum of Squares SST = \\sum(Y_i - \\bar{Y})^2 = 56.3483 SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 48.5271 SSE = \\sum(Y_i - \\hat{Y}_i)^2 = 7.8212\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{48.5271}{56.3483} = 0.8612\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.8671626\n\n\n\n\nVisualization - True vs.¬†Estimated Parameters\n\nTrue DGP: Y = 12 - 0.8X + Œµ\nEstimated Model: Y = 11.7348 - 0.6944X\n\n\nlibrary(ggplot2)\n\n# Create data frame with original data\ndf &lt;- data.frame(\n  magnitude = magnitude,\n  gallagher = gallagher\n)\n\n# Create sequence for smooth lines\nx_seq &lt;- seq(min(magnitude), max(magnitude), length.out = 100)\n\n# Calculate predicted values for both lines\ntrue_dgp &lt;- 12 - 0.8 * x_seq\nestimated &lt;- 11.7348 - 0.6944 * x_seq\n\n# Combine into a data frame for plotting\nlines_df &lt;- data.frame(\n  magnitude = rep(x_seq, 2),\n  value = c(true_dgp, estimated),\n  Model = rep(c(\"True DGP\", \"Estimated\"), each = length(x_seq))\n)\n\n# Create plot\nggplot() +\n  geom_line(data = lines_df, \n            aes(x = magnitude, y = value, color = Model, linetype = Model),\n            size = 1) +\n  geom_point(data = df, \n             aes(x = magnitude, y = gallagher),\n             color = \"black\", \n             size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  labs(\n    title = \"True DGP vs. Estimated Regression Line\",\n    subtitle = \"Black points show observed data with random noise\",\n    x = \"District Magnitude\",\n    y = \"Gallagher Index\",\n    caption = \"True DGP: Y = 12 - 0.8X + Œµ\\nEstimated: Y = 11.73 - 0.69X\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.caption = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\n\nObservations about Model Fit\n\nSlope Comparison\n\nTrue slope: -0.8\nEstimated slope: -0.69\nThe estimated slope is reasonably close to the true parameter\n\nIntercept Comparison\n\nTrue intercept: 12\nEstimated intercept: 11.73\nThe estimated intercept very closely approximates the true value\n\nVisual Patterns\n\nThe lines are nearly parallel, showing good slope recovery\nPoints scatter around both lines due to the random error term (Œµ)\nThe small sample size (n=6) leads to some imprecision in estimation\nThe estimated line (blue) provides a good approximation of the true DGP (red dashed)\n\nImpact of Random Error\n\nThe scatter of points around the true DGP line reflects the N(0,1) error term\nThis noise leads to the slight differences in estimated parameters\nWith a larger sample, we would expect even closer convergence to true parameters\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.93) between district magnitude and electoral disproportionality\nFor each unit increase in district magnitude, the Gallagher index decreases by approximately 0.69 points\nThe model explains 86.12% of the variance in disproportionality\nThe relationship appears strongly linear with moderate scatter\nThe intercept (11.73) represents the expected disproportionality in a hypothetical single-member district system\n\n\n\nStudy Context\n\nData represents simulated observations from a DGP with moderate noise\nSample shows how increasing district magnitude tends to reduce disproportionality\nRandom component reflects other institutional and political factors affecting disproportionality\n\n\n\nLimitations\n\nSmall sample size (n=6)\nSimulated rather than real-world data\nAssumes linear relationship\nDoes not account for other institutional features",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_pl.html",
    "href": "correg_pl.html",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "",
    "text": "16.1 Wprowadzenie (Introduction)\nR√≥≈ºnica miƒôdzy korelacjƒÖ (correlation) a przyczynowo≈õciƒÖ/kausalno≈õciƒÖ (causation) to jedno z podstawowych wyzwa≈Ñ w analizie statystycznej. Korelacja mierzy statystyczny zwiƒÖzek miƒôdzy zmiennymi, natomiast przyczynowo≈õƒá oznacza bezpo≈õredni wp≈Çyw jednej zmiennej na drugƒÖ.\nZale≈ºno≈õci statystyczne stanowiƒÖ fundament podejmowania decyzji opartych na danych w wielu dyscyplinach ‚Äî od ekonomii i zdrowia publicznego po psychologiƒô i nauki o ≈õrodowisku. Zrozumienie, kiedy zwiƒÖzek wskazuje jedynie na asocjacjƒô (association), a kiedy na prawdziwƒÖ kausalno≈õƒá (genuine causality), jest kluczowe dla poprawnych wniosk√≥w i skutecznych rekomendacji politycznych.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kowariancja-covariance",
    "href": "correg_pl.html#kowariancja-covariance",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.2 Kowariancja (Covariance)",
    "text": "16.2 Kowariancja (Covariance)\nKowariancja (covariance) mierzy, w jaki spos√≥b dwie zmienne wsp√≥≈ÇzmieniajƒÖ siƒô, wskazujƒÖc zar√≥wno kierunek, jak i si≈Çƒô ich liniowego zwiƒÖzku.\nWz√≥r: \\text{cov}(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\nGdzie:\n\n$x_i$ i $y_i$ to poszczeg√≥lne obserwacje,\n${x}$ i ${y}$ to ≈õrednie odpowiednio zmiennych $X$ i $Y$,\n$n$ to liczba obserwacji,\ndzielimy przez $(n-1)$, poniewa≈º liczymy kowariancjƒô z pr√≥by (tzw. poprawka Bessela; Bessel‚Äôs correction).\n\n\nObliczenia rƒôczne krok po kroku (Step-by-Step Manual Calculation Process)\nPrzyk≈Çad 1: Godziny nauki a wyniki testu\nDane:\n\nGodziny nauki (X): 2, 4, 6, 8, 10\nWyniki testu (Y): 65, 70, 80, 85, 95\n\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nOblicz ≈õrednie\n${x} = = 6$ godz.\n\n\n\n\n${y} = = 79$ pkt\n\n\n2\nOdchylenia od ≈õrednich\n$(x_i - {x})$: -4, -2, 0, 2, 4\n\n\n\n\n$(y_i - {y})$: -14, -9, 1, 6, 16\n\n\n3\nIloczyny odchyle≈Ñ\n$(x_i - {x})(y_i - {y})$:\n\n\n\n\n(-4)(-14) = 56\n\n\n\n\n(-2)(-9) = 18\n\n\n\n\n(0)(1) = 0\n\n\n\n\n(2)(6) = 12\n\n\n\n\n(4)(16) = 64\n\n\n4\nSuma iloczyn√≥w\n$= 56 + 18 + 0 + 12 + 64 = 150$\n\n\n5\nPodziel przez $(n-1)$\n$(X,Y) = = = 37{.}5$\n\n\n\nWeryfikacja w R (R Verification):\n\n# Definicja danych\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Kowariancja\ncov_manual &lt;- cov(study_hours, test_scores)\nprint(paste(\"Covariance:\", round(cov_manual, 2)))\n\n[1] \"Covariance: 37.5\"\n\n# Weryfikacja krok po kroku\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\nproducts &lt;- x_dev * y_dev\ncov_calculated &lt;- sum(products) / (length(study_hours) - 1)\n\n# Tabela oblicze≈Ñ\ndata.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_deviation = x_dev,\n  Y_deviation = y_dev,\n  Product = products\n)\n\n   X  Y X_deviation Y_deviation Product\n1  2 65          -4         -14      56\n2  4 70          -2          -9      18\n3  6 80           0           1       0\n4  8 85           2           6      12\n5 10 95           4          16      64\n\n\nInterpretacja: Dodatnia kowariancja (37,5) wskazuje, ≈ºe wraz ze wzrostem liczby godzin nauki rosnƒÖ tak≈ºe wyniki testu ‚Äî zmienne majƒÖ tendencjƒô do wsp√≥lnego wzrostu.\n\n\nZadanie ƒáwiczeniowe z rozwiƒÖzaniem (Practice Problem with Solution)\nPolicz rƒôcznie kowariancjƒô dla:\n\nTemperatura (¬∞F): 32, 50, 68, 86, 95\nSprzeda≈º lod√≥w ($): 100, 200, 400, 600, 800\n\nRozwiƒÖzanie:\n\n\n\n\n\n\n\nKrok\nObliczenie\n\n\n\n\n1. ≈örednie\n${x} = = 66{.}2^!F$\n\n\n\n${y} = = 420$\n\n\n2. Odchylenia\nX: -34.2, -16.2, 1.8, 19.8, 28.8\n\n\n\nY: -320, -220, -20, 180, 380\n\n\n3. Iloczyny\n10944, 3564, -36, 3564, 10944\n\n\n4. Suma\n28980\n\n\n5. Kowariancja\n$ = 7245$\n\n\n\n\n# Weryfikacja zadania\ntemp &lt;- c(32, 50, 68, 86, 95)\nsales &lt;- c(100, 200, 400, 600, 800)\nprint(paste(\"Covariance for practice problem:\", round(cov(temp, sales), 2)))\n\n[1] \"Covariance for practice problem: 7245\"",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#wsp√≥≈Çczynnik-korelacji-correlation-coefficient",
    "href": "correg_pl.html#wsp√≥≈Çczynnik-korelacji-correlation-coefficient",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.3 Wsp√≥≈Çczynnik korelacji (Correlation Coefficient)",
    "text": "16.3 Wsp√≥≈Çczynnik korelacji (Correlation Coefficient)\nWsp√≥≈Çczynnik korelacji (correlation coefficient) standaryzuje kowariancjƒô, usuwajƒÖc zale≈ºno≈õƒá od skali i przyjmujƒÖc warto≈õci od -1 do +1.\n\nWskaz√≥wki interpretacyjne (Interpretation Guidelines)\n\n\n\n\n\n\n\n\n\nWarto≈õƒá korelacji\nSi≈Ça\nInterpretacja\nPrzyk≈Çad\n\n\n\n\n¬±0.90 do ¬±1.00\nBardzo silna\nNiemal doskona≈Çy zwiƒÖzek\nWzrost rodzic√≥w i dzieci\n\n\n¬±0.70 do ¬±0.89\nSilna\nZmienne silnie powiƒÖzane\nCzas nauki i oceny\n\n\n¬±0.50 do ¬±0.69\nUmiarkowana\nUmiarkowany zwiƒÖzek\nƒÜwiczenia a spadek masy\n\n\n¬±0.30 do ¬±0.49\nS≈Çaba\nS≈Çaby zwiƒÖzek\nRozmiar buta a umiejƒôtno≈õƒá czytania\n\n\n¬±0.00 do ¬±0.29\nBardzo s≈Çaba/brak\nZnikomy lub brak zwiƒÖzku\nMiesiƒÖc urodzenia a inteligencja\n\n\n\n\n\nWizualizacja typ√≥w zale≈ºno≈õci korelacyjnych (Types of Correlations Visualization)\n\n# Generowanie przyk≈Çadowych danych dla r√≥≈ºnych wzorc√≥w korelacji\nn &lt;- 100\n\n# Dodatnia zale≈ºno≈õƒá liniowa\nset.seed(123)\nyears_education &lt;- rnorm(n, 14, 3)\nannual_income &lt;- 15000 + 3500 * years_education + rnorm(n, 0, 10000)\npos_cor &lt;- round(cor(years_education, annual_income), 3)\n\n# Ujemna zale≈ºno≈õƒá liniowa\nscreen_time &lt;- runif(n, 1, 8)\nsleep_hours &lt;- 9 - 0.5 * screen_time + rnorm(n, 0, 0.5)\nneg_cor &lt;- round(cor(screen_time, sleep_hours), 3)\n\n# Brak korelacji\nx_random &lt;- rnorm(n, 50, 10)\ny_random &lt;- rnorm(n, 50, 10)\nno_cor &lt;- round(cor(x_random, y_random), 3)\n\n# Nieliniowa zale≈ºno≈õƒá (kwadratowa)\nhours_studied &lt;- runif(n, 0, 12)\ntest_score &lt;- -2 * (hours_studied - 6)^2 + 90 + rnorm(n, 0, 5)\nnonlin_cor &lt;- round(cor(hours_studied, test_score), 3)\n\n# Ramki danych z warto≈õciami korelacji\npositive_data &lt;- data.frame(\n  x = years_education, \n  y = annual_income,\n  label = paste0(\"Dodatnia liniowa (r = \", pos_cor, \")\")\n)\n\nnegative_data &lt;- data.frame(\n  x = screen_time, \n  y = sleep_hours,\n  label = paste0(\"Ujemna liniowa (r = \", neg_cor, \")\")\n)\n\nno_corr_data &lt;- data.frame(\n  x = x_random, \n  y = y_random,\n  label = paste0(\"Brak korelacji (r = \", no_cor, \")\")\n)\n\nnonlinear_data &lt;- data.frame(\n  x = hours_studied, \n  y = test_score,\n  label = paste0(\"Nieliniowa (Pearson r = \", nonlin_cor, \")\")\n)\n\n# Po≈ÇƒÖczenie danych\nall_data &lt;- rbind(positive_data, negative_data, no_corr_data, nonlinear_data)\n\n# Wykres fasetowy\nggplot(all_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE, alpha = 0.3) +\n  facet_wrap(~ label, scales = \"free\", ncol = 2) +\n  labs(\n    title = \"R√≥≈ºne typy korelacji\",\n    subtitle = \"Linia regresji liniowej (na czerwono) z pasmem ufno≈õci\",\n    x = \"\", \n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, color = \"gray50\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#korelacja-pearsona-pearson-correlation",
    "href": "correg_pl.html#korelacja-pearsona-pearson-correlation",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.4 Korelacja Pearsona (Pearson Correlation)",
    "text": "16.4 Korelacja Pearsona (Pearson Correlation)\nWz√≥r: r = \\frac{\\text{cov}(X,Y)}{s_X s_Y} = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}}\n\nPe≈Çny przyk≈Çad oblicze≈Ñ rƒôcznych (Complete Manual Calculation Example)\nNa danych o godzinach nauki:\n\nGodziny nauki (X): 2, 4, 6, 8, 10\nWyniki testu (Y): 65, 70, 80, 85, 95\n\nSzczeg√≥≈Çowe kroki:\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nKowariancja\nZ powy≈ºej: $(X,Y) = 37{.}5$\n\n\n2\nKwadraty odchyle≈Ñ\n\n\n\n\nDla X\n$(x_i - {x})^2$: 16, 4, 0, 4, 16\n\n\n\n\nSuma = 40\n\n\n\nDla Y\n$(y_i - {y})^2$: 196, 81, 1, 36, 256\n\n\n\n\nSuma = 570\n\n\n3\nOdchylenia standardowe (standard deviations)\n\n\n\n\n$s_X$\n$s_X = = = 3{.}162$\n\n\n\n$s_Y$\n$s_Y = = = 11{.}937$\n\n\n4\nKorelacja\n$r = $\n\n\n\n\n$r = = 0{.}994$\n\n\n\n\n# Weryfikacja oblicze≈Ñ\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Wsp√≥≈Çczynnik korelacji Pearsona\nr_value &lt;- cor(study_hours, test_scores, method = \"pearson\")\nprint(paste(\"Pearson correlation coefficient:\", round(r_value, 3)))\n\n[1] \"Pearson correlation coefficient: 0.993\"\n\n# Obliczenia szczeg√≥≈Çowe\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\n\n# Tabela oblicze≈Ñ\ncalc_table &lt;- data.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_dev = x_dev,\n  Y_dev = y_dev,\n  X_dev_sq = x_dev^2,\n  Y_dev_sq = y_dev^2,\n  Product = x_dev * y_dev\n)\nprint(calc_table)\n\n   X  Y X_dev Y_dev X_dev_sq Y_dev_sq Product\n1  2 65    -4   -14       16      196      56\n2  4 70    -2    -9        4       81      18\n3  6 80     0     1        0        1       0\n4  8 85     2     6        4       36      12\n5 10 95     4    16       16      256      64\n\n# Statystyki podsumowujƒÖce\ncat(\"\\nSummary:\")\n\n\nSummary:\n\ncat(\"\\nSum of (X-mean)¬≤:\", sum(x_dev^2))\n\n\nSum of (X-mean)¬≤: 40\n\ncat(\"\\nSum of (Y-mean)¬≤:\", sum(y_dev^2))\n\n\nSum of (Y-mean)¬≤: 570\n\ncat(\"\\nSum of products:\", sum(x_dev * y_dev))\n\n\nSum of products: 150\n\ncat(\"\\nStandard deviation of X:\", sd(study_hours))\n\n\nStandard deviation of X: 3.162278\n\ncat(\"\\nStandard deviation of Y:\", sd(test_scores))\n\n\nStandard deviation of Y: 11.93734\n\n# Przedzia≈Ç ufno≈õci i p-value\ncor_test &lt;- cor.test(study_hours, test_scores, method = \"pearson\")\nprint(cor_test)\n\n\n    Pearson's product-moment correlation\n\ndata:  study_hours and test_scores\nt = 15, df = 3, p-value = 0.0006431\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8994446 0.9995859\nsample estimates:\n      cor \n0.9933993 \n\n\nInterpretacja: $r = 0{.}994$ wskazuje na niemal doskona≈Çy dodatni liniowy zwiƒÖzek miƒôdzy godzinami nauki a wynikiem testu. Warto≈õƒá p &lt; 0.05 sugeruje statystycznƒÖ istotno≈õƒá tej zale≈ºno≈õci.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#korelacja-rang-spearmana-spearman-rank-correlation",
    "href": "correg_pl.html#korelacja-rang-spearmana-spearman-rank-correlation",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.5 Korelacja rang Spearmana (Spearman Rank Correlation)",
    "text": "16.5 Korelacja rang Spearmana (Spearman Rank Correlation)\nKorelacja Spearmana mierzy monotoniczne zale≈ºno≈õci, u≈ºywajƒÖc rang zamiast surowych warto≈õci.\nWz√≥r: \\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2 - 1)}\ngdzie $d_i$ to r√≥≈ºnica rang dla obserwacji $i$.\n\nPe≈Çny przyk≈Çad oblicze≈Ñ rƒôcznych (Complete Manual Example)\nDane: Wyniki z matematyki i angielskiego\n\n\n\nUcze≈Ñ\nMatematyka\nAngielski\n\n\n\n\nA\n78\n82\n\n\nB\n85\n90\n\n\nC\n88\n88\n\n\nD\n92\n94\n\n\nE\n95\n96\n\n\n\nRangowanie i obliczenia:\n\n\n\n\n\n\n\n\n\n\n\n\nUcze≈Ñ\nWynik z mat.\nRanga mat.\nWynik z ang.\nRanga ang.\nd = (ranga mat. ‚àí ranga ang.)\nd¬≤\n\n\n\n\nA\n78\n1\n82\n1\n0\n0\n\n\nB\n85\n2\n90\n3\n-1\n1\n\n\nC\n88\n3\n88\n2\n1\n1\n\n\nD\n92\n4\n94\n4\n0\n0\n\n\nE\n95\n5\n96\n5\n0\n0\n\n\n\n\n\n\n\nSuma:\n2\n\n\n\nObliczenie: \\rho = 1 - \\frac{6 \\times 2}{5(25-1)} = 1 - \\frac{12}{120} = 1 - 0{.}1 = 0{.}9\n\n# Dane\nmath_scores &lt;- c(78, 85, 88, 92, 95)\nenglish_scores &lt;- c(82, 90, 88, 94, 96)\n\n# Rangi\nrank_table &lt;- data.frame(\n  Student = LETTERS[1:5],\n  Math_Score = math_scores,\n  Math_Rank = rank(math_scores),\n  English_Score = english_scores,\n  English_Rank = rank(english_scores)\n)\n\nrank_table$d &lt;- rank_table$Math_Rank - rank_table$English_Rank\nrank_table$d_squared &lt;- rank_table$d^2\n\nprint(rank_table)\n\n  Student Math_Score Math_Rank English_Score English_Rank  d d_squared\n1       A         78         1            82            1  0         0\n2       B         85         2            90            3 -1         1\n3       C         88         3            88            2  1         1\n4       D         92         4            94            4  0         0\n5       E         95         5            96            5  0         0\n\ncat(\"\\nSum of d¬≤:\", sum(rank_table$d_squared))\n\n\nSum of d¬≤: 2\n\n# Korelacja Spearmana\nrho &lt;- cor(math_scores, english_scores, method = \"spearman\")\ncat(\"\\nSpearman correlation (R):\", round(rho, 3))\n\n\nSpearman correlation (R): 0.9\n\n# Obliczenie rƒôczne\nn &lt;- length(math_scores)\nrho_manual &lt;- 1 - (6 * sum(rank_table$d_squared)) / (n * (n^2 - 1))\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.9",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#tabele-krzy≈ºowe-cross-tabulation-i-dane-kategoryczne",
    "href": "correg_pl.html#tabele-krzy≈ºowe-cross-tabulation-i-dane-kategoryczne",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.6 Tabele krzy≈ºowe (Cross-tabulation) i dane kategoryczne",
    "text": "16.6 Tabele krzy≈ºowe (Cross-tabulation) i dane kategoryczne\nTabela krzy≈ºowa (cross-tabulation, contingency table) pokazuje zale≈ºno≈õci miƒôdzy zmiennymi kategorycznymi.\n\n# Bardziej realistyczne dane przyk≈Çadowe\nset.seed(123)\nn_total &lt;- 120\n\n# Poziom edukacji a zatrudnienie\neducation &lt;- factor(\n  c(rep(\"High School\", 40), \n    rep(\"College\", 50), \n    rep(\"Graduate\", 30)),\n  levels = c(\"High School\", \"College\", \"Graduate\")\n)\n\n# Status zatrudnienia z prawdopodobie≈Ñstwami zale≈ºnymi od edukacji\nemployment &lt;- factor(\n  c(# High School: 75% employed\n    sample(c(rep(\"Employed\", 30), rep(\"Unemployed\", 10)), 40),\n    # College: 90% employed  \n    sample(c(rep(\"Employed\", 45), rep(\"Unemployed\", 5)), 50),\n    # Graduate: 95% employed\n    sample(c(rep(\"Employed\", 28), rep(\"Unemployed\", 2)), 30)),\n  levels = c(\"Employed\", \"Unemployed\")\n)\n\n# Tabela kontyngencji\ncontingency_table &lt;- table(education, employment)\nprint(\"Contingency Table:\")\n\n[1] \"Contingency Table:\"\n\nprint(contingency_table)\n\n             employment\neducation     Employed Unemployed\n  High School       30         10\n  College           45          5\n  Graduate          28          2\n\n# Procenty w wierszach\nprint(\"\\nRow Percentages (% within each education level):\")\n\n[1] \"\\nRow Percentages (% within each education level):\"\n\nprint(round(prop.table(contingency_table, 1) * 100, 1))\n\n             employment\neducation     Employed Unemployed\n  High School     75.0       25.0\n  College         90.0       10.0\n  Graduate        93.3        6.7\n\n# Test niezale≈ºno≈õci chi-kwadrat (Chi-square test)\nchi_test &lt;- chisq.test(contingency_table)\n\nWarning in chisq.test(contingency_table): Chi-squared approximation may be\nincorrect\n\nprint(chi_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 5.9623, df = 2, p-value = 0.05073",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#ƒáwiczenia-praktyczne-z-rozwiƒÖzaniami-practical-exercises-with-solutions",
    "href": "correg_pl.html#ƒáwiczenia-praktyczne-z-rozwiƒÖzaniami-practical-exercises-with-solutions",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.7 ƒÜwiczenia praktyczne z rozwiƒÖzaniami (Practical Exercises with Solutions)",
    "text": "16.7 ƒÜwiczenia praktyczne z rozwiƒÖzaniami (Practical Exercises with Solutions)\n\nƒÜwiczenie 1: Rƒôczne obliczenie korelacji Pearsona (Calculate Pearson Correlation Manually)\nDane:\n\nWzrost (cale): 66, 68, 70, 72, 74\nWaga (funty): 140, 155, 170, 185, 200\n\nRozwiƒÖzanie:\n\n# Dane\nheight &lt;- c(66, 68, 70, 72, 74)\nweight &lt;- c(140, 155, 170, 185, 200)\n\n# Krok 1: ≈örednie\nh_mean &lt;- mean(height)  # 70\nw_mean &lt;- mean(weight)  # 170\n\n# Krok 2: Odchylenia i iloczyny\ncalculation_df &lt;- data.frame(\n  Height = height,\n  Weight = weight,\n  H_dev = height - h_mean,\n  W_dev = weight - w_mean,\n  H_dev_sq = (height - h_mean)^2,\n  W_dev_sq = (weight - w_mean)^2,\n  Product = (height - h_mean) * (weight - w_mean)\n)\n\nprint(calculation_df)\n\n  Height Weight H_dev W_dev H_dev_sq W_dev_sq Product\n1     66    140    -4   -30       16      900     120\n2     68    155    -2   -15        4      225      30\n3     70    170     0     0        0        0       0\n4     72    185     2    15        4      225      30\n5     74    200     4    30       16      900     120\n\n# Krok 3: Korelacja\ncov_hw &lt;- sum(calculation_df$Product) / (length(height) - 1)\nsd_h &lt;- sqrt(sum(calculation_df$H_dev_sq) / (length(height) - 1))\nsd_w &lt;- sqrt(sum(calculation_df$W_dev_sq) / (length(weight) - 1))\nr_manual &lt;- cov_hw / (sd_h * sd_w)\n\ncat(\"\\nManual calculation:\")\n\n\nManual calculation:\n\ncat(\"\\nCovariance:\", round(cov_hw, 2))\n\n\nCovariance: 75\n\ncat(\"\\nSD Height:\", round(sd_h, 2))\n\n\nSD Height: 3.16\n\ncat(\"\\nSD Weight:\", round(sd_w, 2))\n\n\nSD Weight: 23.72\n\ncat(\"\\nCorrelation:\", round(r_manual, 3))\n\n\nCorrelation: 1\n\n# Weryfikacja w R\ncat(\"\\n\\nR calculation:\", round(cor(height, weight), 3))\n\n\n\nR calculation: 1\n\n\n\n\nƒÜwiczenie 2: Rƒôczne obliczenie korelacji Spearmana (Calculate Spearman Correlation Manually)\nDane:\n\nRangi uczni√≥w z matematyki: 1, 3, 2, 5, 4\nRangi uczni√≥w z nauk ≈õcis≈Çych (science): 2, 4, 1, 5, 3\n\nRozwiƒÖzanie:\n\n# Rangi (ju≈º zrankowane)\nmath_rank &lt;- c(1, 3, 2, 5, 4)\nscience_rank &lt;- c(2, 4, 1, 5, 3)\n\n# R√≥≈ºnice\nd &lt;- math_rank - science_rank\nd_squared &lt;- d^2\n\n# Tabela\nrank_df &lt;- data.frame(\n  Student = 1:5,\n  Math_Rank = math_rank,\n  Science_Rank = science_rank,\n  d = d,\n  d_squared = d_squared\n)\n\nprint(rank_df)\n\n  Student Math_Rank Science_Rank  d d_squared\n1       1         1            2 -1         1\n2       2         3            4 -1         1\n3       3         2            1  1         1\n4       4         5            5  0         0\n5       5         4            3  1         1\n\n# Korelacja Spearmana\nn &lt;- length(math_rank)\nsum_d_sq &lt;- sum(d_squared)\nrho_manual &lt;- 1 - (6 * sum_d_sq) / (n * (n^2 - 1))\n\ncat(\"\\nSum of d¬≤:\", sum_d_sq)\n\n\nSum of d¬≤: 4\n\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.8\n\ncat(\"\\nSpearman correlation (R):\", round(cor(math_rank, science_rank, method = \"spearman\"), 3))\n\n\nSpearman correlation (R): 0.8\n\n\n\n\nƒÜwiczenie 3: Interpretacja wynik√≥w (Interpretation Practice)\nZinterpretuj nastƒôpujƒÖce warto≈õci korelacji:\n\nr = 0.85 miƒôdzy godzinami treningu a wynikiem sprawdzianu sprawno≈õci\n\nOdpowied≈∫: Silny dodatni zwiƒÖzek. Wraz ze wzrostem liczby godzin treningu wyniki istotnie rosnƒÖ.\n\nr = -0.72 miƒôdzy temperaturƒÖ na zewnƒÖtrz a kosztami ogrzewania\n\nOdpowied≈∫: Silny ujemny zwiƒÖzek. Wraz ze wzrostem temperatury koszty ogrzewania wyra≈∫nie malejƒÖ.\n\nr = 0.12 miƒôdzy rozmiarem buta a inteligencjƒÖ\n\nOdpowied≈∫: Bardzo s≈Çaby/brak istotnego zwiƒÖzku. Zmienne sƒÖ praktycznie niezale≈ºne.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#najwa≈ºniejsze-rzeczy-do-zapamiƒôtania-important-points-to-remember",
    "href": "correg_pl.html#najwa≈ºniejsze-rzeczy-do-zapamiƒôtania-important-points-to-remember",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.8 Najwa≈ºniejsze rzeczy do zapamiƒôtania (Important Points to Remember)",
    "text": "16.8 Najwa≈ºniejsze rzeczy do zapamiƒôtania (Important Points to Remember)\n\nKorelacja mierzy si≈Çƒô zwiƒÖzku: Warto≈õci od -1 do +1.\nKorelacja ‚â† przyczynowo≈õƒá (Correlation ‚â† Causation): Wysoka korelacja nie dowodzi wp≈Çywu jednej zmiennej na drugƒÖ.\nDobierz w≈Ça≈õciwƒÖ metodƒô:\n\nPearson: ZwiƒÖzki liniowe dla danych ciƒÖg≈Çych.\nSpearman: ZwiƒÖzki monotoniczne lub dane rangowe.\n\nSprawd≈∫ za≈Ço≈ºenia:\n\nPearson: liniowo≈õƒá i (w praktyce) rozk≈Çad zbli≈ºony do normalnego.\nSpearman: wymagana jedynie monotoniczno≈õƒá.\n\nUwaga na obserwacje odstajƒÖce (outliers): MogƒÖ silnie wp≈Çywaƒá na korelacjƒô Pearsona.\nZawsze wizualizuj dane: Wykresy pomagajƒÖ oceniƒá kszta≈Çt zale≈ºno≈õci.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#podsumowanie-drzewko-decyzyjne-do-analizy-korelacji-summary-decision-tree-for-correlation-analysis",
    "href": "correg_pl.html#podsumowanie-drzewko-decyzyjne-do-analizy-korelacji-summary-decision-tree-for-correlation-analysis",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.9 Podsumowanie: drzewko decyzyjne do analizy korelacji (Summary: Decision Tree for Correlation Analysis)",
    "text": "16.9 Podsumowanie: drzewko decyzyjne do analizy korelacji (Summary: Decision Tree for Correlation Analysis)\n\n\n\nWYB√ìR W≈ÅA≈öCIWEJ MIARY KORELACJI:\n\nCzy dane sƒÖ liczbowe (numeryczne)?\n‚îú‚îÄ TAK ‚Üí Czy zwiƒÖzek jest liniowy?\n‚îÇ   ‚îú‚îÄ TAK ‚Üí U≈ºyj korelacji PEARSONA\n‚îÇ   ‚îî‚îÄ NIE ‚Üí Czy zwiƒÖzek jest monotoniczny?\n‚îÇ       ‚îú‚îÄ TAK ‚Üí U≈ºyj korelacji SPEARMANA\n‚îÇ       ‚îî‚îÄ NIE ‚Üí Rozwa≈º metody nieliniowe\n‚îî‚îÄ NIE ‚Üí Czy dane sƒÖ porzƒÖdkowe (rangi)?\n    ‚îú‚îÄ TAK ‚Üí U≈ºyj korelacji SPEARMANA\n    ‚îî‚îÄ NIE ‚Üí U≈ºyj TABEL KRZY≈ªOWYCH dla danych kategorycznych",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#dodatkowe-zadania-additional-practice-problems",
    "href": "correg_pl.html#dodatkowe-zadania-additional-practice-problems",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.10 Dodatkowe zadania (Additional Practice Problems)",
    "text": "16.10 Dodatkowe zadania (Additional Practice Problems)\n\nZestaw zada≈Ñ A: obliczenia rƒôczne (Problem Set A: Manual Calculations)\n\nPolicz kowariancjƒô i korelacjƒô Pearsona dla:\n\nX: 10, 20, 30, 40, 50\nY: 15, 25, 35, 45, 55\n\nRozwiƒÖzanie: Cov(X,Y) = 250, r = 1.0 (idealna dodatnia korelacja).\nPolicz korelacjƒô Spearmana dla ocen film√≥w:\n\nOceny filmu A: 8, 6, 9, 7, 5\nOceny filmu B: 7, 8, 9, 6, 4\n\nRozwiƒÖzanie: œÅ = 0.3 (s≈Çaba dodatnia korelacja).\n\n\n\nZestaw zada≈Ñ B: interpretacja (Problem Set B: Interpretation)\n\nBadanie wykazuje r = 0.91 miƒôdzy godzinami snu a wynikami test√≥w.\n\nInterpretacja: Bardzo silny dodatni zwiƒÖzek ‚Äî wiƒôcej snu wiƒÖ≈ºe siƒô z lepszymi wynikami. Nie dowodzi to jednak kausalno≈õci; mo≈ºliwe czynniki uboczne.\n\nInne badanie: r = -0.03 miƒôdzy miesiƒÖcem urodzenia a IQ.\n\nInterpretacja: Brak istotnego zwiƒÖzku. MiesiƒÖc urodzenia i IQ sƒÖ w praktyce niezale≈ºne.\n\n\n\n\n≈öciƒÖga (Quick Reference Card)\n\n\n\n\n\n\n\n\n\nMiara\nKiedy u≈ºywaƒá (Use When)\nWz√≥r (Formula)\nZakres (Range)\n\n\n\n\nKowariancja (Covariance)\nWstƒôpne badanie zwiƒÖzku\n\\frac{\\sum(x\\_i-\\bar{x})(y\\_i-\\bar{y})}{n-1}\n-\\infty do +\\infty\n\n\nPearson r\nZwiƒÖzki liniowe, dane ciƒÖg≈Çe\n\\frac{\\text{cov}(X,Y)}{s\\_X s\\_Y}\n-1 do +1\n\n\nSpearman œÅ\nZwiƒÖzki monotoniczne, rangi\n1 - \\frac{6\\sum d\\_i^2}{n(n^2-1)}\n-1 do +1\n\n\nTabele krzy≈ºowe (Cross-tabs)\nZmienne kategoryczne\nZliczenia czƒôsto≈õci\nn/d",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#analiza-regresji-ols-ordinary-least-squares-przewodnik-na-start-a-quick-start-guide",
    "href": "correg_pl.html#analiza-regresji-ols-ordinary-least-squares-przewodnik-na-start-a-quick-start-guide",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.11 Analiza regresji OLS (Ordinary Least Squares): przewodnik na start (A Quick-start Guide)",
    "text": "16.11 Analiza regresji OLS (Ordinary Least Squares): przewodnik na start (A Quick-start Guide)\n\n\n\n\n\n\nAnaliza regresji OLS: przewodnik na start\n\n\n\n\nWprowadzenie: czym jest analiza regresji?\nAnaliza regresji (regression analysis) pomaga zrozumieƒá i mierzyƒá zale≈ºno≈õci miƒôdzy obserwowalnymi wielko≈õciami. To zestaw narzƒôdzi matematycznych do identyfikowania wzorc√≥w w danych, kt√≥re umo≈ºliwiajƒÖ prognozowanie (prediction).\nRozwa≈º pytania badawcze:\n\nJak czas nauki wp≈Çywa na wynik testu?\nJak do≈õwiadczenie wp≈Çywa na wynagrodzenie?\nJak wydatki na reklamƒô oddzia≈ÇujƒÖ na sprzeda≈º?\n\nRegresja dostarcza systematycznych metod, by na te pytania odpowiadaƒá na podstawie realnych danych.\n\n\nPunkt wyj≈õcia: prosty przyk≈Çad\nZacznijmy od konkretu. Zebrano dane o 20 studentach z Twojej klasy:\n\n\n\nStudent\nStudy Hours\nExam Score\n\n\n\n\nAlex\n2\n68\n\n\nBeth\n4\n74\n\n\nCarlos\n6\n85\n\n\nDiana\n8\n91\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\nPo narysowaniu wykresu punktowego (scatter plot) chcesz znale≈∫ƒá prostƒÖ, kt√≥ra najlepiej opisuje zwiƒÖzek miƒôdzy godzinami nauki a wynikiem.\nAle co znaczy ‚Äûnajlepiej‚Äù? W≈Ça≈õnie to odkryjemy.\n\n\nDlaczego prawdziwe dane nie uk≈ÇadajƒÖ siƒô w idealnƒÖ liniƒô\nZanim przejdziemy do rachunk√≥w, zrozummy, dlaczego punkty zwykle nie le≈ºƒÖ na jednej prostej.\n\nModele deterministyczne vs.¬†stochastyczne\nModele deterministyczne (deterministic models) opisujƒÖ zwiƒÖzki bez niepewno≈õci. Przyk≈Çad z fizyki:\n\\text{Distance} = \\text{Speed} \\times \\text{Time}\nJedziesz dok≈Çadnie 60 mph przez 2 godziny ‚Üí zawsze 120 mil. Zero odchyle≈Ñ.\nModele stochastyczne (stochastic models) uznajƒÖ, ≈ºe w danych naturalnie wystƒôpuje losowo≈õƒá. Og√≥lna postaƒá to:\nY = f(X) + \\epsilon\nGdzie:\n\nY ‚Äî wielko≈õƒá, kt√≥rƒÖ prognozujemy (np. wynik testu),\nf(X) ‚Äî wzorzec systematyczny (jak godziny nauki typowo wp≈ÇywajƒÖ na wyniki),\n\\epsilon ‚Äî ‚Äûreszta‚Äù/szum: wszystko, czego nie mierzymy.\n\nW naszym przyk≈Çadzie dwoje student√≥w mo≈ºe uczyƒá siƒô po 5 godzin, a jednak dostaƒá r√≥≈ºne oceny, bo:\n\njedno lepiej spa≈Ço,\njedno ma talent do test√≥w,\njedno mia≈Ço ha≈Ças na sali,\npytania trafi≈Çy bardziej/mniej pod ich przygotowanie.\n\nTa losowo≈õƒá jest naturalna ‚Äî tym zajmuje siƒô \\epsilon.\n\n\n\nProsty model regresji liniowej\nZale≈ºno≈õƒá miƒôdzy godzinami nauki a wynikiem zapisujemy jako:\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nRozszyfrujmy:\n\nY_i ‚Äî wynik testu studenta i,\nX_i ‚Äî godziny nauki studenta i,\n\\beta_0 ‚Äî wyraz wolny (intercept, ‚Äûpoziom bazowy‚Äù przy 0 godzin),\n\\beta_1 ‚Äî nachylenie (slope, przyrost punkt√≥w na godzinƒô),\n\\epsilon_i ‚Äî ‚Äûwszystko inne‚Äù wp≈ÇywajƒÖce na wynik i.\n\nWa≈ºne: Prawdziwych warto≈õci \\beta_0, \\beta_1 nie znamy. Szacujemy je z danych i oznaczamy ‚Äûz daszkiem‚Äù: \\hat{\\beta}_0, \\hat{\\beta}_1.\n\n\nReszty: jak bardzo mylimy siƒô w przewidywaniach?\nPo dopasowaniu prostej mo≈ºemy przewidzieƒá wyniki. Dla ka≈ºdej obserwacji:\n\nWarto≈õƒá rzeczywista (y_i): faktyczny wynik,\nWarto≈õƒá przewidziana (\\hat{y}_i): co ‚Äûm√≥wi‚Äù nasza prosta,\nReszta (e_i): r√≥≈ºnica = Rzeczywista ‚àí Przewidziana.\n\nPrzyk≈Çad:\nDiana: 8 h nauki, wynik 91\nLinia przewiduje: 88\nReszta: 91 ‚àí 88 = +3 (zani≈ºyli≈õmy)\n\nEric: 5 h nauki, wynik 70\nLinia przewiduje: 79\nReszta: 70 ‚àí 79 = ‚àí9 (zawy≈ºyli≈õmy)\n\n\nKluczowy pomys≈Ç: dlaczego kwadratujemy reszty?\nZa≈Ç√≥≈ºmy reszty czterech student√≥w:\n\nA: +5\nB: ‚àí5\nC: +3\nD: ‚àí3\n\nSuma: (+5) + (-5) + (+3) + (-3) = 0.\nTo nie znaczy, ≈ºe przewidywania sƒÖ idealne ‚Äî b≈Çƒôdy siƒô znoszƒÖ.\nRozwiƒÖzanie: sumujemy kwadraty reszt:\n\n(+5)^2 = 25\n(-5)^2 = 25\n(+3)^2 = 9\n(-3)^2 = 9\nSuma kwadrat√≥w b≈Çƒôd√≥w = 68\n\nDlaczego to dzia≈Ça:\n\nBrak znoszenia znak√≥w, bo kwadraty sƒÖ dodatnie,\nDu≈ºe b≈Çƒôdy wa≈ºƒÖ mocniej (10 punkt√≥w to 4√ó wiƒôcej ni≈º 5 punkt√≥w),\nWygoda matematyczna: funkcje kwadratowe sƒÖ g≈Çadkie i r√≥≈ºniczkowalne.\n\n\n\nMetoda zwyk≈Çych najmniejszych kwadrat√≥w (OLS)\nOLS wybiera takƒÖ prostƒÖ, kt√≥ra minimalizuje sumƒô kwadrat√≥w reszt (SSE ‚Äî Sum of Squared Errors):\n\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\nCzyli:\n\\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n \\big(y_i - (\\beta_0 + \\beta_1 x_i)\\big)^2\n‚ÄûPo ludzku‚Äù: Znajd≈∫ takie \\beta_0 i \\beta_1, by ≈ÇƒÖczny b≈ÇƒÖd (w kwadracie) przewidywa≈Ñ by≈Ç jak najmniejszy.\n\n\nRozwiƒÖzanie matematyczne\nMinimalizujemy SSE rachunkiem r√≥≈ºniczkowym. Warunki pierwszego rzƒôdu:\n\\frac{\\partial SSE}{\\partial \\beta_0} = -2\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i) = 0\n\\frac{\\partial SSE}{\\partial \\beta_1} = -2\\sum_{i=1}^n x_i(y_i - \\beta_0 - \\beta_1x_i) = 0\nRozwiƒÖzanie uk≈Çadu:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\nWnioski:\n\nNachylenie zale≈ºy od tego, jak wsp√≥≈ÇzmieniajƒÖ siƒô X i Y (kowariancja) wzglƒôdem zmienno≈õci samego X (wariancja),\nLinia przechodzi przez punkt ≈õrednich (\\bar{x}, \\bar{y}).\n\n\n\nSkƒÖd wiemy, ≈ºe linia jest ‚Äûdobra‚Äù? Rozk≈Çad zmienno≈õci\nRozbijamy ca≈ÇkowitƒÖ zmienno≈õƒá wynik√≥w:\nCa≈Çkowita suma kwadrat√≥w (SST ‚Äî Total Sum of Squares)\n‚ÄûJak bardzo og√≥lnie r√≥≈ºniƒÖ siƒô wyniki?‚Äù\nSST = \\sum_{i=1}^n (y_i - \\bar{y})^2\nSuma kwadrat√≥w regresji (SSR ‚Äî Regression Sum of Squares)\n‚ÄûIle zmienno≈õci wyja≈õnia nasza linia?‚Äù\nSSR = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\nSuma kwadrat√≥w b≈Çƒôd√≥w (SSE ‚Äî Error Sum of Squares)\n‚ÄûIle nie wyja≈õniamy?‚Äù\nSSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\nTo≈ºsamo≈õƒá wariancyjna:\nSST = SSR + SSE \\quad\\Rightarrow\\quad \\text{Ca≈Çkowita} = \\text{Wyja≈õniona} + \\text{Niewyja≈õniona}\n\n\nR^2: ocena dopasowania\nWsp√≥≈Çczynnik determinacji (R^2) m√≥wi, jaki odsetek zmienno≈õci wyja≈õnia model:\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nInterpretacja:\n\nR^2 = 0.75: ‚ÄûGodziny nauki wyja≈õniajƒÖ 75% zr√≥≈ºnicowania wynik√≥w‚Äù,\nR^2 = 0.30: ‚ÄûModel wyja≈õnia 30% tego, czym r√≥≈ºniƒÖ siƒô wyniki‚Äù,\nR^2 = 1.00: perfekcyjne dopasowanie (w realu prawie nigdy),\nR^2 = 0.00: nie lepiej ni≈º zgadywanie ≈õredniej.\n\nUwaga kontekstowa: w naukach spo≈Çecznych 0.30 bywa ≈õwietne; w in≈ºynierii oczekuje siƒô bardzo wysokich R^2.\n\n\nInterpretacja wynik√≥w\nZa≈Ç√≥≈ºmy, ≈ºe oszacowali≈õmy \\hat{\\beta}_0 = 60 i \\hat{\\beta}_1 = 4.\nNachylenie (\\hat{\\beta}_1 = 4):\n\n‚ÄûKa≈ºda dodatkowa godzina nauki wiƒÖ≈ºe siƒô ≈õrednio z +4 punktami‚Äù,\nTo efekt przeciƒôtny, nie obietnica dla konkretnej osoby.\n\nWyraz wolny (\\hat{\\beta}_0 = 60):\n\n‚ÄûPrzy 0 godzinach nauki przewidujemy 60 punkt√≥w‚Äù,\nCzƒôsto to tylko kotwica matematyczna ‚Äî mo≈ºe nie mieƒá sensu praktycznego.\n\nR√≥wnanie predykcji:\n\\widehat{\\text{Wynik}} = 60 + 4 \\times \\text{Godziny nauki}\n5 godzin ‚Üí 60 + 4 \\cdot 5 = 80 punkt√≥w.\n\n\nWielko≈õƒá efektu i istotno≈õƒá praktyczna\nIstotno≈õƒá statystyczna m√≥wi, czy efekt istnieje; istotno≈õƒá praktyczna ‚Äî czy ma znaczenie. Potrzebujemy obu.\n\nSurowa wielko≈õƒá efektu\nTo po prostu nachylenie \\hat{\\beta}_1.\nPrzyk≈Çad: \\hat{\\beta}_1 = 4 pkt/godz.\nCzy to ‚Äûdu≈ºo‚Äù? Zale≈ºy od:\n\nSkali wyniku (4/100 = 4% vs 4/500 = 0,8%),\nKosztu interwencji (czy 1h nauki warta 4 pkt?),\nProg√≥w decyzyjnych (czy 4 pkt zmienia ocenƒô?).\n\n\n\nStandaryzowana wielko≈õƒá efektu\nU≈Çatwia por√≥wnania miƒôdzy badaniami:\n\\beta_{\\text{std}} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y}\ngdzie s_X i s_Y to odchylenia standardowe X i Y.\nPrzyk≈Çad: je≈õli s_X = 2.5 h, s_Y = 12 pkt i \\hat{\\beta}_1 = 4:\n\\beta_{\\text{std}} = 4 \\cdot \\frac{2.5}{12} = 0.83\n‚ÄûPrzy wzro≈õcie X o 1 SD, Y ro≈õnie o 0.83 SD‚Äù.\n\n\nWskaz√≥wki Cohena\nDla standaryzowanych wsp√≥≈Çczynnik√≥w:\n\nma≈Çy: |\\beta| \\approx 0.10 (~1% wariancji),\n≈õredni: |\\beta| \\approx 0.30 (~9%),\ndu≈ºy: |\\beta| \\approx 0.50 (~25%).\n\nDla R^2:\n\nma≈Çy: R^2 \\approx 0.02,\n≈õredni: R^2 \\approx 0.13,\ndu≈ºy: R^2 \\approx 0.26.\n\nUwaga: to og√≥lne progi ‚Äî normy r√≥≈ºniƒÖ siƒô miƒôdzy dziedzinami.\n\n\nPrzedzia≈Çy ufno≈õci dla wielko≈õci efektu\nDla surowego wsp√≥≈Çczynnika:\nCI = \\hat{\\beta}_1 \\pm t_{\\text{critical}} \\cdot SE(\\hat{\\beta}_1)\nJe≈õli 95% CI = [3.2, 4.8], m√≥wimy: ‚ÄûZ 95% pewno≈õciƒÖ prawdziwy efekt mie≈õci siƒô miƒôdzy 3.2 a 4.8 pkt/godz.‚Äù\n\n\nOcena istotno≈õci praktycznej\nWe≈∫ pod uwagƒô:\n\nMinimalnie istotnƒÖ r√≥≈ºnicƒô (MID),\nIle trzeba zmieniƒá X, by osiƒÖgnƒÖƒá sensownƒÖ zmianƒô Y,\nKoszt-efektywno≈õƒá:\n\n\\text{Efektywno≈õƒá} = \\frac{\\text{Wielko≈õƒá efektu}}{\\text{Koszt interwencji}}\nPrzyk≈Çad:\nEfekt: 4 pkt/godz.\nPr√≥g zaliczenia: 70; ≈õrednia: 68 ‚Üí 30 min nauki mo≈ºe zmieniƒá niezal na zal ‚Üí istotne praktycznie.\n\n\n\nNiepewno≈õƒá\nSzacunki pochodzƒÖ z pr√≥by, nie z ca≈Çej populacji ‚Üí niepewno≈õƒá.\n\nSkƒÖd niepewno≈õƒá?\n\nMasz 20 student√≥w, nie wszystkich,\nPr√≥ba mo≈ºe byƒá nietypowa,\nPomiary nie sƒÖ doskona≈Çe (raportowanie godzin nauki).\n\n\n\nPrzedzia≈Çy ufno≈õci\nZamiast ‚Äûefekt to dok≈Çadnie 4‚Äù, m√≥wimy:\n\n‚ÄûSzacujemy 4‚Äù,\n‚Äû95% CI: [3.2, 4.8]‚Äú.\n\nZnaczenie: w wielu powt√≥rzeniach 95% takich przedzia≈Ç√≥w zawiera prawdziwƒÖ warto≈õƒá.\n\n\nTestowanie istnienia zale≈ºno≈õci\nPytamy: ‚ÄûGdyby prawdziwie nie by≈Ço zwiƒÖzku, jak ma≈Ço prawdopodobny by≈Çby obserwowany wzorzec?‚Äù\n\np = 0.03: przy braku efektu tylko 3% szans na tak silny wzorzec ‚Äûz przypadku‚Äù,\np = 0.40: wzorzec ‚Äûm√≥g≈Çby siƒô ≈Çatwo zdarzyƒá‚Äù bez efektu.\n\nRegu≈Ça kciuka: p &lt; 0.05 ‚Üí ‚Äûstatystycznie istotne‚Äù.\n\n\n\nGdy co≈õ idzie ≈∫le: diagnostyka modelu\nSzybkie wizualizacje:\n\nWykres punktowy: czy zwiƒÖzek jest mniej wiƒôcej liniowy?\nReszty vs.¬†przewidywania: powinien byƒá losowy ob≈Çok,\nOutliery: punkty bardzo odleg≈Çe?\n\nSygna≈Çy ostrzegawcze:\n\nWzorzec w resztach ‚Üí brak liniowo≈õci lub zmienna pominiƒôta,\nRozszerzajƒÖcy siƒô wachlarz reszt ‚Üí heteroskedastyczno≈õƒá,\nWp≈Çywowe obserwacje (influential) ciƒÖgnƒÖ prostƒÖ,\nPominiƒôte zmienne ‚Üí obciƒÖ≈ºenia (bias).\n\n\n\nZa≈Ço≈ºenia: kiedy OLS dzia≈Ça dobrze\n\nLiniowo≈õƒá (linearity) ‚Äî zale≈ºno≈õƒá w przybli≈ºeniu prosta,\nNiezale≈ºno≈õƒá (independence) ‚Äî obserwacje od siebie niezale≈ºne,\nSta≈Ça wariancja (homoskedastyczno≈õƒá) ‚Äî rozrzut reszt podobny w ca≈Çym zakresie,\nBrak doskona≈Çej wsp√≥≈Çliniowo≈õci (no perfect multicollinearity) ‚Äî w regresji wielorakiej predyktory nie sƒÖ liniowo zale≈ºne ‚Äûna 100%‚Äú,\nLosowy dob√≥r pr√≥by (random sampling) ‚Äî dane reprezentatywne.\n\n\n\nPodsumowanie\nCo robi OLS:\n\nDopasowuje prostƒÖ minimalizujƒÖcƒÖ SSE,\nSzacuje, o ile ≈õrednio zmienia siƒô Y przy zmianie X o 1,\nPodaje R^2 ‚Äî ile zmienno≈õci wyja≈õniamy,\nKwantyfikuje niepewno≈õƒá (SE, CI, p-values).\n\nKroki praktyczne:\n\nWykres danych ‚Äî czy linia ma sens?\nUruchom OLS ‚Üí \\hat{\\beta}_0, \\hat{\\beta}_1,\nSprawd≈∫ R^2,\nOblicz wielko≈õci efektu (surowƒÖ i standaryzowanƒÖ),\nOce≈Ñ istotno≈õƒá praktycznƒÖ,\nSprawd≈∫ przedzia≈Çy ufno≈õci,\nObejrzyj reszty,\nDecyduj, ≈ÇƒÖczƒÖc istotno≈õƒá statystycznƒÖ i praktycznƒÖ.\n\nKluczowe interpretacje:\n\nNachylenie: ‚ÄûGdy X ro≈õnie o 1, Y zmienia siƒô ≈õrednio o [slope]‚Äú,\nStandaryzowane nachylenie: ‚Äû+1 SD w X ‚Üí [Œ≤_std] SD w Y‚Äú,\nR^2: ‚ÄûX wyja≈õnia R^2 \\times 100\\% zmienno≈õci Y‚Äú,\np-value: ‚ÄûPrzy braku efektu taki wzorzec mia≈Çby prawdopodobie≈Ñstwo p \\times 100\\%‚Äú,\nCI: ‚ÄûZ 95% pewno≈õciƒÖ prawdziwy efekt jest w [dolna, g√≥rna]‚Äú.\n\nPamiƒôtaj:\n\nAsocjacja ‚â† przyczynowo≈õƒá,\nIstotno≈õƒá statystyczna ‚â† istotno≈õƒá praktyczna,\n‚ÄûKa≈ºdy model jest b≈Çƒôdny, niekt√≥re sƒÖ u≈ºyteczne‚Äù,\nZawsze wizualizuj dane i reszty,\nDecyzje opieraj na wielko≈õci efektu i niepewno≈õci.\n\nOLS dostarcza uporzƒÖdkowany, matematyczny spos√≥b znajdowania wzorc√≥w w danych. Nie daje doskona≈Çych prognoz, ale zapewnia najlepszƒÖ liniowƒÖ aproksymacjƒô wraz z uczciwƒÖ ocenƒÖ jej jako≈õci i niepewno≈õci.\n\n\n16.12 Rƒôczne obliczenia OLS krok po kroku\nBadaczka chce zbadaƒá zale≈ºno≈õƒá miƒôdzy godzinami nauki a wynikiem testu (6 student√≥w):\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nCelem jest wyznaczyƒá \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X metodƒÖ OLS.\n\nKrok 1: ≈örednie\nDla X:\n\\bar{X} = \\frac{1+2+3+4+5+6}{6} = \\frac{21}{6} = 3.5\nDla Y:\n\\bar{Y} = \\frac{65+70+75+85+88+95}{6} = \\frac{478}{6} = 79.67\n\n\nKrok 2: Odchylenia od ≈õrednich\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n‚àí2.5\n‚àí14.67\n\n\nB\n2\n70\n‚àí1.5\n‚àí9.67\n\n\nC\n3\n75\n‚àí0.5\n‚àí4.67\n\n\nD\n4\n85\n0.5\n5.33\n\n\nE\n5\n88\n1.5\n8.33\n\n\nF\n6\n95\n2.5\n15.33\n\n\n\n\n\nKrok 3: Iloczyny i kwadraty\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n36.68\n6.25\n\n\nB\n14.51\n2.25\n\n\nC\n2.34\n0.25\n\n\nD\n2.67\n0.25\n\n\nE\n12.50\n2.25\n\n\nF\n38.33\n6.25\n\n\nSuma\n107.03\n17.50\n\n\n\n\n\nKrok 4: Nachylenie \\hat{\\beta}_1\n\\hat{\\beta}_1 = \\frac{107.03}{17.50} = 6.12\nInterpretacja: +6.12 punktu za ka≈ºdƒÖ dodatkowƒÖ godzinƒô nauki.\n\n\nKrok 5: Wyraz wolny \\hat{\\beta}_0\n\\hat{\\beta}_0 = 79.67 - 6.12 \\cdot 3.5 = 58.25\nInterpretacja: przy 0 godzinach przewidujemy 58.25.\n\n\nKrok 6: R√≥wnanie regresji\n\\hat{Y} = 58.25 + 6.12 X\n\n\nKrok 7: Predykcje i reszty\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nReszta e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n64.37\n0.63\n\n\nB\n2\n70\n70.49\n‚àí0.49\n\n\nC\n3\n75\n76.61\n‚àí1.61\n\n\nD\n4\n85\n82.73\n2.27\n\n\nE\n5\n88\n88.85\n‚àí0.85\n\n\nF\n6\n95\n94.97\n0.03\n\n\n\nKontrola: suma reszt ‚âà 0 ‚úì\n\n\nKrok 8: Sumy kwadrat√≥w\nSST ‚Äî ca≈Çkowita:\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n215.21\n\n\nB\n70\n93.51\n\n\nC\n75\n21.81\n\n\nD\n85\n28.41\n\n\nE\n88\n69.39\n\n\nF\n95\n235.01\n\n\nSuma\n\nSST = 663.34\n\n\n\nSSR ‚Äî wyja≈õniona:\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n234.09\n\n\nB\n70.49\n84.27\n\n\nC\n76.61\n9.36\n\n\nD\n82.73\n9.36\n\n\nE\n88.85\n84.27\n\n\nF\n94.97\n234.09\n\n\nSuma\n\nSSR = 655.44\n\n\n\nSSE ‚Äî b≈ÇƒÖd:\n\n\n\nStudent\ne_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n‚àí0.49\n0.24\n\n\nC\n‚àí1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n‚àí0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSuma\n\nSSE = 9.10\n\n\n\nWeryfikacja: SST \\approx SSR + SSE\n663.34 \\approx 655.44 + 9.10 = 664.54 (drobne r√≥≈ºnice zaokrƒÖgle≈Ñ).\n\n\nKrok 9: R^2\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternatywnie:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 0.986\nInterpretacja: ~98.8% zmienno≈õci wynik√≥w wyja≈õniajƒÖ godziny nauki ‚Äî bardzo silny zwiƒÖzek.\n\n\nKrok 10: Wielko≈õci efektu\nSurowa: 6.12 pkt/godz.\nStandaryzowana:\n\ns_X = \\sqrt{17.50/5} = 1.87,\ns_Y = \\sqrt{663.34/5} = 11.52,\n\\beta_{\\text{std}} = 6.12 \\cdot (1.87/11.52) = 0.99 ‚Üí bardzo du≈ºy efekt (wg Cohena).\n\n\n\nKrok 11: Istotno≈õƒá praktyczna\n\nSkala: 6.12% na 100-punktowej skali / godz.,\nProgi: zmiana oceny (10 pkt) ‚âà 1.63 h,\nKoszt-efekt: korzystny ‚Äî sensowna inwestycja czasu.\n\n\n\nPodsumowanie wynik√≥w\n\nR√≥wnanie: \\hat{Y} = 58.25 + 6.12 X\nNachylenie: 6.12 pkt/godz.\nWyraz wolny: 58.25 pkt\nR^2: 0.988\n\\beta_{\\text{std}}: 0.99\n\nW praktyce: ka≈ºda godzina nauki to ‚âà +6 pkt; dopasowanie znakomite; efekt istotny statystycznie i praktycznie.\n\n\nKontrola wyniku\nSprawd≈∫, ≈ºe linia przechodzi przez (\\bar{X}, \\bar{Y}):\n58.25 + 6.12 \\cdot 3.5 = 79.67 = \\bar{Y} ‚úì\n\n\n\n16.13 Kod R do weryfikacji oblicze≈Ñ\n\n# Krok 1: Dane\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)      # X\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95) # Y\nn &lt;- length(study_hours)\n\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Krok 2: ≈örednie\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Krok 3: Odchylenia od ≈õrednich\ndata$x_dev &lt;- data$X - x_bar\ndata$y_dev &lt;- data$Y - y_bar\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Krok 4: Iloczyny i kwadraty\ndata$xy_product &lt;- data$x_dev * data$y_dev\ndata$x_dev_sq &lt;- data$x_dev^2\n\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - XÃÑ)(Yi - »≤):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - XÃÑ)(Yi - »≤): 107 \n\ncat(\"Sum of (Xi - XÃÑ)¬≤:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - XÃÑ)¬≤: 17.5 \n\n# Krok 5: Nachylenie (beta_1)\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (Œ≤‚ÇÅ) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (Œ≤‚ÇÅ) calculated manually: 6.11 \n\n# Krok 6: Wyraz wolny (beta_0)\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (Œ≤‚ÇÄ) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (Œ≤‚ÇÄ) calculated manually: 58.27 \n\n# Krok 7: Por√≥wnanie z lm()\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Krok 8: Predykcje i reszty\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X\ndata$residual &lt;- data$Y - data$Y_hat\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ‚âà 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ‚âà 0): 0 \n\n# Krok 9: Sumy kwadrat√≥w\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Krok 10: R-kwadrat\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R¬≤ (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR¬≤ (Method 1: SSR/SST): 0.9863 \n\ncat(\"R¬≤ (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR¬≤ (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R¬≤ (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR¬≤ (from lm function): 0.9863 \n\n# Krok 11: Wielko≈õci efektu\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\nsd_x &lt;- sd(data$X)\nsd_y &lt;- sd(data$Y)\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Korelacja (dla regresji prostej |r| = sqrt(R^2))\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R¬≤:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R¬≤: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n\n\n# Krok 12: Wizualizacja\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5,\n     xlim = c(0, 7), ylim = c(60, 100))\n\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Punkt ≈õrednich\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Reszty jako odcinki pionowe\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# R√≥wnanie na wykresie\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R¬≤ = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\nAnaliza regresji OLS - godziny nauki vs.¬†wynik testu\n\n\n\n# Podsumowanie ko≈Ñcowe\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )\n\n\n\n\n16.14 Jak uruchomiƒá kod\n\nSkopiuj ca≈Çy blok kodu,\nWklej do RStudio,\nUruchom chunk po chunk lub ca≈Çy dokument,\nPor√≥wnaj wyniki z obliczeniami rƒôcznymi.\n\nCo zobaczysz:\n\nNachylenie: 6.12,\nWyraz wolny: 58.25,\nR^2: ‚âà 0.988,\nEfekt standaryzowany: ‚âà 0.99,\nWykres z punktami, liniƒÖ regresji i resztami.\n\nTo potwierdza poprawno≈õƒá oblicze≈Ñ manualnych.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#rƒôczne-obliczenia-ols-krok-po-kroku",
    "href": "correg_pl.html#rƒôczne-obliczenia-ols-krok-po-kroku",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.12 Rƒôczne obliczenia OLS krok po kroku",
    "text": "16.12 Rƒôczne obliczenia OLS krok po kroku\nBadaczka chce zbadaƒá zale≈ºno≈õƒá miƒôdzy godzinami nauki a wynikiem testu (6 student√≥w):\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nCelem jest wyznaczyƒá \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X metodƒÖ OLS.\n\nKrok 1: ≈örednie\nDla X:\n\\bar{X} = \\frac{1+2+3+4+5+6}{6} = \\frac{21}{6} = 3.5\nDla Y:\n\\bar{Y} = \\frac{65+70+75+85+88+95}{6} = \\frac{478}{6} = 79.67\n\n\nKrok 2: Odchylenia od ≈õrednich\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n‚àí2.5\n‚àí14.67\n\n\nB\n2\n70\n‚àí1.5\n‚àí9.67\n\n\nC\n3\n75\n‚àí0.5\n‚àí4.67\n\n\nD\n4\n85\n0.5\n5.33\n\n\nE\n5\n88\n1.5\n8.33\n\n\nF\n6\n95\n2.5\n15.33\n\n\n\n\n\nKrok 3: Iloczyny i kwadraty\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n36.68\n6.25\n\n\nB\n14.51\n2.25\n\n\nC\n2.34\n0.25\n\n\nD\n2.67\n0.25\n\n\nE\n12.50\n2.25\n\n\nF\n38.33\n6.25\n\n\nSuma\n107.03\n17.50\n\n\n\n\n\nKrok 4: Nachylenie \\hat{\\beta}_1\n\\hat{\\beta}_1 = \\frac{107.03}{17.50} = 6.12\nInterpretacja: +6.12 punktu za ka≈ºdƒÖ dodatkowƒÖ godzinƒô nauki.\n\n\nKrok 5: Wyraz wolny \\hat{\\beta}_0\n\\hat{\\beta}_0 = 79.67 - 6.12 \\cdot 3.5 = 58.25\nInterpretacja: przy 0 godzinach przewidujemy 58.25.\n\n\nKrok 6: R√≥wnanie regresji\n\\hat{Y} = 58.25 + 6.12 X\n\n\nKrok 7: Predykcje i reszty\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nReszta e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n64.37\n0.63\n\n\nB\n2\n70\n70.49\n‚àí0.49\n\n\nC\n3\n75\n76.61\n‚àí1.61\n\n\nD\n4\n85\n82.73\n2.27\n\n\nE\n5\n88\n88.85\n‚àí0.85\n\n\nF\n6\n95\n94.97\n0.03\n\n\n\nKontrola: suma reszt ‚âà 0 ‚úì\n\n\nKrok 8: Sumy kwadrat√≥w\nSST ‚Äî ca≈Çkowita:\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n215.21\n\n\nB\n70\n93.51\n\n\nC\n75\n21.81\n\n\nD\n85\n28.41\n\n\nE\n88\n69.39\n\n\nF\n95\n235.01\n\n\nSuma\n\nSST = 663.34\n\n\n\nSSR ‚Äî wyja≈õniona:\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n234.09\n\n\nB\n70.49\n84.27\n\n\nC\n76.61\n9.36\n\n\nD\n82.73\n9.36\n\n\nE\n88.85\n84.27\n\n\nF\n94.97\n234.09\n\n\nSuma\n\nSSR = 655.44\n\n\n\nSSE ‚Äî b≈ÇƒÖd:\n\n\n\nStudent\ne_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n‚àí0.49\n0.24\n\n\nC\n‚àí1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n‚àí0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSuma\n\nSSE = 9.10\n\n\n\nWeryfikacja: SST \\approx SSR + SSE\n663.34 \\approx 655.44 + 9.10 = 664.54 (drobne r√≥≈ºnice zaokrƒÖgle≈Ñ).\n\n\nKrok 9: R^2\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternatywnie:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 0.986\nInterpretacja: ~98.8% zmienno≈õci wynik√≥w wyja≈õniajƒÖ godziny nauki ‚Äî bardzo silny zwiƒÖzek.\n\n\nKrok 10: Wielko≈õci efektu\nSurowa: 6.12 pkt/godz.\nStandaryzowana:\n\ns_X = \\sqrt{17.50/5} = 1.87,\ns_Y = \\sqrt{663.34/5} = 11.52,\n\\beta_{\\text{std}} = 6.12 \\cdot (1.87/11.52) = 0.99 ‚Üí bardzo du≈ºy efekt (wg Cohena).\n\n\n\nKrok 11: Istotno≈õƒá praktyczna\n\nSkala: 6.12% na 100-punktowej skali / godz.,\nProgi: zmiana oceny (10 pkt) ‚âà 1.63 h,\nKoszt-efekt: korzystny ‚Äî sensowna inwestycja czasu.\n\n\n\nPodsumowanie wynik√≥w\n\nR√≥wnanie: \\hat{Y} = 58.25 + 6.12 X\nNachylenie: 6.12 pkt/godz.\nWyraz wolny: 58.25 pkt\nR^2: 0.988\n\\beta_{\\text{std}}: 0.99\n\nW praktyce: ka≈ºda godzina nauki to ‚âà +6 pkt; dopasowanie znakomite; efekt istotny statystycznie i praktycznie.\n\n\nKontrola wyniku\nSprawd≈∫, ≈ºe linia przechodzi przez (\\bar{X}, \\bar{Y}):\n58.25 + 6.12 \\cdot 3.5 = 79.67 = \\bar{Y} ‚úì",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kod-r-do-weryfikacji-oblicze≈Ñ",
    "href": "correg_pl.html#kod-r-do-weryfikacji-oblicze≈Ñ",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.13 Kod R do weryfikacji oblicze≈Ñ",
    "text": "16.13 Kod R do weryfikacji oblicze≈Ñ\n\n# Krok 1: Dane\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)      # X\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95) # Y\nn &lt;- length(study_hours)\n\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Krok 2: ≈örednie\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Krok 3: Odchylenia od ≈õrednich\ndata$x_dev &lt;- data$X - x_bar\ndata$y_dev &lt;- data$Y - y_bar\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Krok 4: Iloczyny i kwadraty\ndata$xy_product &lt;- data$x_dev * data$y_dev\ndata$x_dev_sq &lt;- data$x_dev^2\n\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - XÃÑ)(Yi - »≤):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - XÃÑ)(Yi - »≤): 107 \n\ncat(\"Sum of (Xi - XÃÑ)¬≤:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - XÃÑ)¬≤: 17.5 \n\n# Krok 5: Nachylenie (beta_1)\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (Œ≤‚ÇÅ) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (Œ≤‚ÇÅ) calculated manually: 6.11 \n\n# Krok 6: Wyraz wolny (beta_0)\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (Œ≤‚ÇÄ) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (Œ≤‚ÇÄ) calculated manually: 58.27 \n\n# Krok 7: Por√≥wnanie z lm()\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Krok 8: Predykcje i reszty\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X\ndata$residual &lt;- data$Y - data$Y_hat\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ‚âà 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ‚âà 0): 0 \n\n# Krok 9: Sumy kwadrat√≥w\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Krok 10: R-kwadrat\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R¬≤ (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR¬≤ (Method 1: SSR/SST): 0.9863 \n\ncat(\"R¬≤ (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR¬≤ (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R¬≤ (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR¬≤ (from lm function): 0.9863 \n\n# Krok 11: Wielko≈õci efektu\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\nsd_x &lt;- sd(data$X)\nsd_y &lt;- sd(data$Y)\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Korelacja (dla regresji prostej |r| = sqrt(R^2))\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R¬≤:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R¬≤: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n\n\n# Krok 12: Wizualizacja\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5,\n     xlim = c(0, 7), ylim = c(60, 100))\n\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Punkt ≈õrednich\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Reszty jako odcinki pionowe\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# R√≥wnanie na wykresie\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R¬≤ = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\nAnaliza regresji OLS - godziny nauki vs.¬†wynik testu\n\n\n\n# Podsumowanie ko≈Ñcowe\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#jak-uruchomiƒá-kod",
    "href": "correg_pl.html#jak-uruchomiƒá-kod",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.14 Jak uruchomiƒá kod",
    "text": "16.14 Jak uruchomiƒá kod\n\nSkopiuj ca≈Çy blok kodu,\nWklej do RStudio,\nUruchom chunk po chunk lub ca≈Çy dokument,\nPor√≥wnaj wyniki z obliczeniami rƒôcznymi.\n\nCo zobaczysz:\n\nNachylenie: 6.12,\nWyraz wolny: 58.25,\nR^2: ‚âà 0.988,\nEfekt standaryzowany: ‚âà 0.99,\nWykres z punktami, liniƒÖ regresji i resztami.\n\nTo potwierdza poprawno≈õƒá oblicze≈Ñ manualnych.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#appendix-a-obliczanie-kowariancji-korelacji-pearsona-i-spearmana-oraz-modelowanie-ols---przyk≈Çad-wprowadzajƒÖcy",
    "href": "correg_pl.html#appendix-a-obliczanie-kowariancji-korelacji-pearsona-i-spearmana-oraz-modelowanie-ols---przyk≈Çad-wprowadzajƒÖcy",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.15 Appendix A: Obliczanie Kowariancji, Korelacji Pearsona i Spearmana, oraz modelowanie OLS - przyk≈Çad wprowadzajƒÖcy",
    "text": "16.15 Appendix A: Obliczanie Kowariancji, Korelacji Pearsona i Spearmana, oraz modelowanie OLS - przyk≈Çad wprowadzajƒÖcy\nStudentka politologii bada zwiƒÖzek miƒôdzy wielko≈õciƒÖ okrƒôgu wyborczego (DM) a wska≈∫nikiem dysproporcjonalno≈õci Gallaghera (GH) w wyborach parlamentarnych w 10 losowo wybranych demokracjach.\nDane dotyczƒÖce wielko≈õci okrƒôgu wyborczego (\\text{DM}) i indeksu Gallaghera:\n\n\n\n\\text{DM} (X)\nGallagher (Y)\n\n\n\n\n2\n18,2\n\n\n3\n16,7\n\n\n4\n15,8\n\n\n5\n15,3\n\n\n6\n15,0\n\n\n7\n14,8\n\n\n8\n14,7\n\n\n9\n14,6\n\n\n10\n14,55\n\n\n11\n14,52\n\n\n\n\nKrok 1: Obliczanie Podstawowych Statystyk\nObliczanie ≈õrednich:\nDla \\text{DM} (X): \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\nSzczeg√≥≈Çowe obliczenia:\n2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 65 \\bar{x} = \\frac{65}{10} = 6,5\nDla indeksu Gallaghera (Y): \\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\nSzczeg√≥≈Çowe obliczenia:\n18,2 + 16,7 + 15,8 + 15,3 + 15,0 + 14,8 + 14,7 + 14,6 + 14,55 + 14,52 = 154,17 \\bar{y} = \\frac{154,17}{10} = 15,417\n\n\nKrok 2: Szczeg√≥≈Çowe Obliczenia Kowariancji\nPe≈Çna tabela robocza ze wszystkimi obliczeniami:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18,2\n-4,5\n2,783\n-12,5235\n20,25\n7,7451\n\n\n2\n3\n16,7\n-3,5\n1,283\n-4,4905\n12,25\n1,6461\n\n\n3\n4\n15,8\n-2,5\n0,383\n-0,9575\n6,25\n0,1467\n\n\n4\n5\n15,3\n-1,5\n-0,117\n0,1755\n2,25\n0,0137\n\n\n5\n6\n15,0\n-0,5\n-0,417\n0,2085\n0,25\n0,1739\n\n\n6\n7\n14,8\n0,5\n-0,617\n-0,3085\n0,25\n0,3807\n\n\n7\n8\n14,7\n1,5\n-0,717\n-1,0755\n2,25\n0,5141\n\n\n8\n9\n14,6\n2,5\n-0,817\n-2,0425\n6,25\n0,6675\n\n\n9\n10\n14,55\n3,5\n-0,867\n-3,0345\n12,25\n0,7517\n\n\n10\n11\n14,52\n4,5\n-0,897\n-4,0365\n20,25\n0,8047\n\n\nSuma\n65\n154,17\n0\n0\n-28,085\n82,5\n12,8442\n\n\n\nObliczanie kowariancji: \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\text{Cov}(X,Y) = \\frac{-28,085}{9} = -3,120556\n\n\nKrok 3: Obliczanie Odchylenia Standardowego\nDla \\text{DM} (X): \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}}\n\\sigma_x = \\sqrt{\\frac{82,5}{9}} = \\sqrt{9,1667} = 3,026582\nDla Gallaghera (Y): \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}}\n\\sigma_y = \\sqrt{\\frac{12,8442}{9}} = \\sqrt{1,4271} = 1,194612\n\n\nKrok 4: Obliczanie Korelacji Pearsona\nr = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\nr = \\frac{-3,120556}{3,026582 \\times 1,194612} = \\frac{-3,120556}{3,615752} = -0,863044\n\n\nKrok 5: Obliczanie Korelacji Rangowej Spearmana\nPe≈Çna tabela rangowa ze wszystkimi obliczeniami:\n\n\n\ni\nX_i\nY_i\nRanga X_i\nRanga Y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n18,2\n1\n10\n-9\n81\n\n\n2\n3\n16,7\n2\n9\n-7\n49\n\n\n3\n4\n15,8\n3\n8\n-5\n25\n\n\n4\n5\n15,3\n4\n7\n-3\n9\n\n\n5\n6\n15,0\n5\n6\n-1\n1\n\n\n6\n7\n14,8\n6\n5\n1\n1\n\n\n7\n8\n14,7\n7\n4\n3\n9\n\n\n8\n9\n14,6\n8\n3\n5\n25\n\n\n9\n10\n14,55\n9\n2\n7\n49\n\n\n10\n11\n14,52\n10\n1\n9\n81\n\n\nSuma\n\n\n\n\n\n330\n\n\n\nObliczanie korelacji Spearmana: \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n\\rho = 1 - \\frac{6 \\times 330}{10(100 - 1)} = 1 - \\frac{1980}{990} = 1 - 2 = -1\n\n\nKrok 6: Weryfikacja w R\n\n# Tworzenie wektor√≥w\nDM &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nGH &lt;- c(18.2, 16.7, 15.8, 15.3, 15.0, 14.8, 14.7, 14.6, 14.55, 14.52)\n\n# Obliczanie kowariancji\ncov(DM, GH)\n\n[1] -3.120556\n\n# Obliczanie korelacji\ncor(DM, GH, method = \"pearson\")\n\n[1] -0.8627742\n\ncor(DM, GH, method = \"spearman\")\n\n[1] -1\n\n\n\n\nKrok 7: Podstawowa Wizualizacja\n\nlibrary(ggplot2)\n\n# Tworzenie ramki danych\ndata &lt;- data.frame(DM = DM, GH = GH)\n\n# Tworzenie wykresu rozrzutu\nggplot(data, aes(x = DM, y = GH)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Wielko≈õƒá Okrƒôgu vs Indeks Gallaghera\",\n    x = \"Wielko≈õƒá Okrƒôgu (DM)\",\n    y = \"Indeks Gallaghera (GH)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nEstymacja OLS i Miary Dopasowania Modelu\n\n\nKrok 1: Obliczanie Estymator√≥w OLS\nKorzystajƒÖc z wcze≈õniej obliczonych warto≈õci:\n\n\\sum(X_i - \\bar{X})(Y_i - \\bar{Y}) = -28,085\n\\sum(X_i - \\bar{X})^2 = 82,5\n\\bar{X} = 6,5\n\\bar{Y} = 15,417\n\nObliczanie nachylenia (\\hat{\\beta_1}):\n\\hat{\\beta_1} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\hat{\\beta_1} = -28,085 √∑ 82,5 = -0,3404\nObliczanie wyrazu wolnego (\\hat{\\beta_0}): \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 15,417 - (-0,3404 √ó 6,5)\n   = 15,417 + 2,2126\n   = 17,6296\nZatem r√≥wnanie regresji OLS ma postaƒá: \\hat{Y} = 17,6296 - 0,3404X\n\n\nKrok 2: Obliczanie Warto≈õci Dopasowanych i Reszt\nPe≈Çna tabela ze wszystkimi obliczeniami:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i\ne_i = Y_i - \\hat{Y}_i\ne_i^2\n(Y_i - \\bar{Y})^2\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18,2\n16,9488\n1,2512\n1,5655\n7,7451\n2,3404\n\n\n2\n3\n16,7\n16,6084\n0,0916\n0,0084\n1,6461\n1,4241\n\n\n3\n4\n15,8\n16,2680\n-0,4680\n0,2190\n0,1467\n0,7225\n\n\n4\n5\n15,3\n15,9276\n-0,6276\n0,3939\n0,0137\n0,2601\n\n\n5\n6\n15,0\n15,5872\n-0,5872\n0,3448\n0,1739\n0,0289\n\n\n6\n7\n14,8\n15,2468\n-0,4468\n0,1996\n0,3807\n0,0290\n\n\n7\n8\n14,7\n14,9064\n-0,2064\n0,0426\n0,5141\n0,2610\n\n\n8\n9\n14,6\n14,5660\n0,0340\n0,0012\n0,6675\n0,7241\n\n\n9\n10\n14,55\n14,2256\n0,3244\n0,1052\n0,7517\n1,4184\n\n\n10\n11\n14,52\n13,8852\n0,6348\n0,4030\n0,8047\n2,3439\n\n\nSuma\n65\n154,17\n154,17\n0\n3,2832\n12,8442\n9,5524\n\n\n\nObliczenia dla warto≈õci dopasowanych:\nDla X = 2:\n≈∂ = 17,6296 + (-0,3404 √ó 2) = 16,9488\n\nDla X = 3:\n≈∂ = 17,6296 + (-0,3404 √ó 3) = 16,6084\n\n[... kontynuacja dla wszystkich warto≈õci]\n\n\nKrok 3: Obliczanie Miar Dopasowania\nSuma kwadrat√≥w reszt (SSE): SSE = \\sum e_i^2\nSSE = 3,2832\nCa≈Çkowita suma kwadrat√≥w (SST): SST = \\sum(Y_i - \\bar{Y})^2\nSST = 12,8442\nSuma kwadrat√≥w regresji (SSR): SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\nSSR = 9,5524\nWeryfikacja dekompozycji: SST = SSR + SSE\n12,8442 = 9,5524 + 3,2832 (w granicach b≈Çƒôdu zaokrƒÖglenia)\nObliczanie wsp√≥≈Çczynnika determinacji R-kwadrat: R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nR¬≤ = 9,5524 √∑ 12,8442\n   = 0,7438\n\n\nKrok 4: Weryfikacja w R\n\n# Dopasowanie modelu liniowego\nmodel &lt;- lm(GH ~ DM, data = data)\n\n# Podsumowanie statystyk\nsummary(model)\n\n\nCall:\nlm(formula = GH ~ DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62764 -0.46274 -0.08615  0.26624  1.25109 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept) 17.62976    0.50121  35.174 0.000000000467 ***\nDM          -0.34042    0.07053  -4.827        0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6406 on 8 degrees of freedom\nMultiple R-squared:  0.7444,    Adjusted R-squared:  0.7124 \nF-statistic:  23.3 on 1 and 8 DF,  p-value: 0.00131\n\n# Rƒôczne obliczenie R-kwadrat\nSST &lt;- sum((GH - mean(GH))^2)\nSSE &lt;- sum(residuals(model)^2)\nSSR &lt;- SST - SSE\nR2_manual &lt;- SSR/SST\nR2_manual\n\n[1] 0.7443793\n\n\n\n\nKrok 5: Analiza Reszt\n\n# Tworzenie wykres√≥w reszt\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\nKrok 6: Wykres Warto≈õci Przewidywanych vs Rzeczywistych\n\n# Tworzenie wykresu warto≈õci przewidywanych vs rzeczywistych\nggplot(data.frame(\n  Rzeczywiste = GH,\n  Przewidywane = fitted(model)\n), aes(x = Przewidywane, y = Rzeczywiste)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Warto≈õci Przewidywane vs Rzeczywiste\",\n    x = \"Przewidywany Indeks Gallaghera\",\n    y = \"Rzeczywisty Indeks Gallaghera\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nModele z TransformacjƒÖ LogarytmicznƒÖ\n\n\nKrok 1: Transformacja Danych\nNajpierw obliczamy logarytmy naturalne zmiennych:\n\n\n\ni\nX_i\nY_i\n\\ln(X_i)\n\\ln(Y_i)\n\n\n\n\n1\n2\n18,2\n0,6931\n2,9014\n\n\n2\n3\n16,7\n1,0986\n2,8154\n\n\n3\n4\n15,8\n1,3863\n2,7600\n\n\n4\n5\n15,3\n1,6094\n2,7278\n\n\n5\n6\n15,0\n1,7918\n2,7081\n\n\n6\n7\n14,8\n1,9459\n2,6946\n\n\n7\n8\n14,7\n2,0794\n2,6878\n\n\n8\n9\n14,6\n2,1972\n2,6810\n\n\n9\n10\n14,55\n2,3026\n2,6777\n\n\n10\n11\n14,52\n2,3979\n2,6757\n\n\n\n\n\nKrok 2: Por√≥wnanie R√≥≈ºnych Specyfikacji Modelu\nSzacujemy trzy alternatywne specyfikacje:\n\nModel log-liniowy: \\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nModel liniowo-logarytmiczny: Y_i = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\nModel log-log: \\ln(Y_i) = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\n\n\n# Tworzenie zmiennych transformowanych\ndata$log_DM &lt;- log(data$DM)\ndata$log_GH &lt;- log(data$GH)\n\n# Dopasowanie modeli\nmodel_linear &lt;- lm(GH ~ DM, data = data)\nmodel_loglinear &lt;- lm(log_GH ~ DM, data = data)\nmodel_linearlog &lt;- lm(GH ~ log_DM, data = data)\nmodel_loglog &lt;- lm(log_GH ~ log_DM, data = data)\n\n# Por√≥wnanie warto≈õci R-kwadrat\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Liniowy\", \"Log-liniowy\", \"Liniowo-logarytmiczny\", \"Log-log\"),\n  R_kwadrat = c(\n    summary(model_linear)$r.squared,\n    summary(model_loglinear)$r.squared,\n    summary(model_linearlog)$r.squared,\n    summary(model_loglog)$r.squared\n  )\n)\n\n# Wy≈õwietlenie por√≥wnania\nmodels_comparison\n\n                  Model R_kwadrat\n1               Liniowy 0.7443793\n2           Log-liniowy 0.7670346\n3 Liniowo-logarytmiczny 0.9141560\n4               Log-log 0.9288088\n\n\n\n\nKrok 3: Por√≥wnanie Wizualne\n\n# Tworzenie wykres√≥w dla ka≈ºdego modelu\np1 &lt;- ggplot(data, aes(x = DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Liniowy\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Log-liniowy\") +\n  theme_minimal()\n\np3 &lt;- ggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Liniowo-logarytmiczny\") +\n  theme_minimal()\n\np4 &lt;- ggplot(data, aes(x = log_DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Log-log\") +\n  theme_minimal()\n\n# Uk≈Çadanie wykres√≥w w siatkƒô\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nKrok 4: Analiza Reszt dla Najlepszego Modelu\nNa podstawie warto≈õci R-kwadrat, analiza reszt dla najlepiej dopasowanego modelu:\n\n# Wykresy reszt dla najlepszego modelu\npar(mfrow = c(2, 2))\nplot(model_linearlog)\n\n\n\n\n\n\n\n\n\n\nKrok 5: Interpretacja Najlepszego Modelu\nWsp√≥≈Çczynniki modelu liniowo-logarytmicznego:\n\nsummary(model_linearlog)\n\n\nCall:\nlm(formula = GH ~ log_DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40702 -0.30207 -0.04907  0.22905  0.60549 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)  19.0223     0.4079   46.64 0.0000000000494 ***\nlog_DM       -2.0599     0.2232   -9.23 0.0000153880425 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3712 on 8 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9034 \nF-statistic: 85.19 on 1 and 8 DF,  p-value: 0.00001539\n\n\nInterpretacja:\n\n\\hat{\\beta_0} reprezentuje oczekiwany Indeks Gallaghera, gdy ln(DM) = 0 (czyli gdy DM = 1)\n\\hat{\\beta_1} reprezentuje zmianƒô Indeksu Gallaghera zwiƒÖzanƒÖ z jednostkowym wzrostem ln(DM)\n\n\n\nKrok 6: Predykcje Modelu\n\n# Tworzenie wykresu predykcji dla najlepszego modelu\nggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Model Liniowo-logarytmiczny: Indeks Gallaghera vs ln(Wielko≈õƒá Okrƒôgu)\",\n    x = \"ln(Wielko≈õƒá Okrƒôgu)\",\n    y = \"Indeks Gallaghera\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nKrok 7: Analiza Elastyczno≈õci\nDla modelu log-log wsp√≥≈Çczynniki bezpo≈õrednio reprezentujƒÖ elastyczno≈õci. Obliczenie ≈õredniej elastyczno≈õci dla modelu liniowo-logarytmicznego:\n\n# Obliczenie elastyczno≈õci przy warto≈õciach ≈õrednich\nmean_DM &lt;- mean(data$DM)\nmean_GH &lt;- mean(data$GH)\nbeta1 &lt;- coef(model_linearlog)[2]\nelastycznosc &lt;- beta1 * (1/mean_GH)\nelastycznosc\n\n    log_DM \n-0.1336136 \n\n\nWarto≈õƒá ta reprezentuje procentowƒÖ zmianƒô Indeksu Gallaghera przy jednoprocentowej zmianie Wielko≈õci Okrƒôgu.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#appendix-b.-prosty-model-regresji-liniowej-mnkols---przyk≈Çady-r√≥≈ºne",
    "href": "correg_pl.html#appendix-b.-prosty-model-regresji-liniowej-mnkols---przyk≈Çady-r√≥≈ºne",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.16 Appendix B. Prosty Model Regresji Liniowej (MNK/OLS) - przyk≈Çady r√≥≈ºne",
    "text": "16.16 Appendix B. Prosty Model Regresji Liniowej (MNK/OLS) - przyk≈Çady r√≥≈ºne",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przyk≈Çad-1.-zwiƒÖzek-miƒôdzy-wielko≈õciƒÖ-okrƒôgu-a-dysproporcjonalno≈õciƒÖ-wyborczƒÖ-1",
    "href": "correg_pl.html#przyk≈Çad-1.-zwiƒÖzek-miƒôdzy-wielko≈õciƒÖ-okrƒôgu-a-dysproporcjonalno≈õciƒÖ-wyborczƒÖ-1",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.17 Przyk≈Çad 1. ZwiƒÖzek Miƒôdzy Wielko≈õciƒÖ Okrƒôgu a Dysproporcjonalno≈õciƒÖ WyborczƒÖ (1)",
    "text": "16.17 Przyk≈Çad 1. ZwiƒÖzek Miƒôdzy Wielko≈õciƒÖ Okrƒôgu a Dysproporcjonalno≈õciƒÖ WyborczƒÖ (1)\nTa analiza bada zwiƒÖzek miƒôdzy wielko≈õciƒÖ okrƒôgu wyborczego (DM) a wska≈∫nikiem dysproporcjonalno≈õci Loosemore-Hanby (LH) w wyborach parlamentarnych w 6 krajach. Indeks Loosemore-Hanby mierzy dysproporcjonalno≈õƒá wyborczƒÖ, gdzie wy≈ºsze warto≈õci wskazujƒÖ na wiƒôkszƒÖ dysproporcjonalno≈õƒá miƒôdzy g≈Çosami a mandatami.\n\nDane\n\n\nWarning: package 'knitr' was built under R version 4.4.3\n\n\n\nWielko≈õƒá Okrƒôgu i Indeks LH wed≈Çug Kraju\n\n\nCountry\nDM\nLH\n\n\n\n\nA\n3\n15.50\n\n\nB\n4\n14.25\n\n\nC\n5\n13.50\n\n\nD\n6\n13.50\n\n\nE\n7\n13.00\n\n\nF\n8\n12.75\n\n\n\n\n\n\n\nKrok 1: Obliczenie wariancji i odchylenia standardowego dla DM i LH\n\nObliczenia dla Wielko≈õci Okrƒôgu (DM)\nNajpierw obliczam ≈õredniƒÖ warto≈õci DM:\n\\bar{x}_{DM} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\frac{3 + 4 + 5 + 6 + 7 + 8}{6} = \\frac{33}{6} = 5.5\nNastƒôpnie obliczam wariancjƒô u≈ºywajƒÖc formu≈Çy z korektƒÖ Bessela:\n\\sigma^2_{DM} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\n\n\nKraj\nDM (x_i)\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\n\n\n\n\nA\n3\n3 - 5.5 = -2.5\n(-2.5)^2 = 6.25\n\n\nB\n4\n4 - 5.5 = -1.5\n(-1.5)^2 = 2.25\n\n\nC\n5\n5 - 5.5 = -0.5\n(-0.5)^2 = 0.25\n\n\nD\n6\n6 - 5.5 = 0.5\n(0.5)^2 = 0.25\n\n\nE\n7\n7 - 5.5 = 1.5\n(1.5)^2 = 2.25\n\n\nF\n8\n8 - 5.5 = 2.5\n(2.5)^2 = 6.25\n\n\nSuma\n\n\n17.5\n\n\n\n\\sigma^2_{DM} = \\frac{17.5}{6-1} = \\frac{17.5}{5} = 3.5\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji:\n\\sigma_{DM} = \\sqrt{\\sigma^2_{DM}} = \\sqrt{3.5} = 1.871\n\n\nObliczenia dla Indeksu LH\nNajpierw obliczam ≈õredniƒÖ warto≈õci LH:\n\\bar{x}_{LH} = \\frac{15.5 + 14.25 + 13.5 + 13.5 + 13 + 12.75}{6} = \\frac{82.5}{6} = 13.75\nNastƒôpnie obliczam wariancjƒô:\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n15.5\n15.5 - 13.75 = 1.75\n(1.75)^2 = 3.0625\n\n\nB\n14.25\n14.25 - 13.75 = 0.5\n(0.5)^2 = 0.25\n\n\nC\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nD\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nE\n13\n13 - 13.75 = -0.75\n(-0.75)^2 = 0.5625\n\n\nF\n12.75\n12.75 - 13.75 = -1\n(-1)^2 = 1\n\n\nSuma\n\n\n5\n\n\n\n\\sigma^2_{LH} = \\frac{5}{6-1} = \\frac{5}{5} = 1\nOdchylenie standardowe wynosi:\n\\sigma_{LH} = \\sqrt{\\sigma^2_{LH}} = \\sqrt{1} = 1\nPodsumowanie Zadania 1:\n\nWariancja DM (z korektƒÖ Bessela): 3.5\nOdchylenie Standardowe DM: 1.871\nWariancja LH (z korektƒÖ Bessela): 1\nOdchylenie Standardowe LH: 1\n\n\n\n\nKrok 2: Obliczenie kowariancji miƒôdzy DM i LH dla tej pr√≥by danych\nKowariancja jest obliczana przy u≈ºyciu formu≈Çy z korektƒÖ Bessela:\nCov(DM, LH) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\nA\n3\n15.5\n-2.5\n1.75\n(-2.5)(1.75) = -4.375\n\n\nB\n4\n14.25\n-1.5\n0.5\n(-1.5)(0.5) = -0.75\n\n\nC\n5\n13.5\n-0.5\n-0.25\n(-0.5)(-0.25) = 0.125\n\n\nD\n6\n13.5\n0.5\n-0.25\n(0.5)(-0.25) = -0.125\n\n\nE\n7\n13\n1.5\n-0.75\n(1.5)(-0.75) = -1.125\n\n\nF\n8\n12.75\n2.5\n-1\n(2.5)(-1) = -2.5\n\n\nSuma\n\n\n\n\n-8.75\n\n\n\nCov(DM, LH) = \\frac{-8.75}{5} = -1.75\nKowariancja miƒôdzy DM i LH: -1.75\nUjemna kowariancja wskazuje na odwrotnƒÖ zale≈ºno≈õƒá: gdy wielko≈õƒá okrƒôgu wzrasta, indeks dysproporcjonalno≈õci LH ma tendencjƒô do spadku.\n\n\nKrok 3: Obliczenie wsp√≥≈Çczynnika korelacji liniowej Pearsona miƒôdzy DM i LH\nWsp√≥≈Çczynnik korelacji Pearsona obliczany jest przy u≈ºyciu formu≈Çy:\nr = \\frac{Cov(DM, LH)}{\\sigma_{DM} \\cdot \\sigma_{LH}}\nMamy ju≈º obliczone:\n\nCov(DM, LH) = -1.75\n\\sigma_{DM} = 1.871\n\\sigma_{LH} = 1\n\nr = \\frac{-1.75}{1.871 \\cdot 1} = \\frac{-1.75}{1.871} = -0.935\nWsp√≥≈Çczynnik korelacji Pearsona: -0.935\n\nInterpretacja:\nWsp√≥≈Çczynnik korelacji -0.935 wskazuje:\n\nKierunek: Znak ujemny pokazuje odwrotnƒÖ zale≈ºno≈õƒá miƒôdzy wielko≈õciƒÖ okrƒôgu a indeksem LH.\nSi≈Ça: Warto≈õƒá bezwzglƒôdna 0.935 wskazuje na bardzo silnƒÖ korelacjƒô (blisko -1).\nInterpretacja praktyczna: Poniewa≈º wy≈ºsze warto≈õci indeksu LH wskazujƒÖ na wiƒôkszƒÖ dysproporcjonalno≈õƒá, ta silna ujemna korelacja sugeruje, ≈ºe gdy wielko≈õƒá okrƒôgu wzrasta, dysproporcjonalno≈õƒá wyborcza ma tendencjƒô do znacznego spadku. Innymi s≈Çowy, systemy wyborcze z wiƒôkszymi okrƒôgami (wiƒôcej przedstawicieli wybieranych z jednego okrƒôgu) zwykle dajƒÖ bardziej proporcjonalne wyniki (ni≈ºsza dysproporcjonalno≈õƒá).\n\nOdkrycie to jest zgodne z teoriƒÖ nauk politycznych, kt√≥ra sugeruje, ≈ºe wiƒôksze okrƒôgi zapewniajƒÖ wiƒôcej mo≈ºliwo≈õci mniejszym partiom, aby uzyskaƒá reprezentacjƒô, co prowadzi do wynik√≥w wyborczych, kt√≥re lepiej odzwierciedlajƒÖ rozk≈Çad g≈Ços√≥w miƒôdzy partiami.\n\n\n\nKrok 4: Skonstruowanie modelu regresji liniowej prostej i obliczenie R-kwadrat\nFormu≈Ça dla regresji liniowej prostej:\nLH = \\beta_0 + \\beta_1 \\cdot DM + \\varepsilon\nGdzie:\n\n\\beta_0 to wyraz wolny (przeciƒôcie)\n\\beta_1 to wsp√≥≈Çczynnik nachylenia dla DM\n\nFormu≈Ça do obliczenia \\beta_1 to:\n\\beta_1 = \\frac{Cov(DM, LH)}{\\sigma^2_{DM}} = \\frac{-1.75}{3.5} = -0.5\nAby obliczyƒá \\beta_0, u≈ºywam:\n\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x} = 13.75 - (-0.5) \\cdot 5.5 = 13.75 + 2.75 = 16.5\nZatem r√≥wnanie regresji to:\nLH = 16.5 - 0.5 \\cdot DM\n\nObliczanie warto≈õci przewidywanych i b≈Çƒôd√≥w\nU≈ºywajƒÖc naszego r√≥wnania regresji, obliczam przewidywane warto≈õci LH:\n\\hat{LH} = 16.5 - 0.5 \\cdot DM\n\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i = 16.5 - 0.5x_i)\nB≈ÇƒÖd (y_i - \\hat{y}_i)\nB≈ÇƒÖd bezwzglƒôdny (|y_i - \\hat{y}_i|)\nB≈ÇƒÖd kwadratowy ([y_i - \\hat{y}_i]^2)\n\n\n\n\nA\n3\n15.5\n16.5 - 0.5(3) = 16.5 - 1.5 = 15\n15.5 - 15 = 0.5\n|0.5| = 0.5\n(0.5)^2 = 0.25\n\n\nB\n4\n14.25\n16.5 - 0.5(4) = 16.5 - 2 = 14.5\n14.25 - 14.5 = -0.25\n|-0.25| = 0.25\n(-0.25)^2 = 0.0625\n\n\nC\n5\n13.5\n16.5 - 0.5(5) = 16.5 - 2.5 = 14\n13.5 - 14 = -0.5\n|-0.5| = 0.5\n(-0.5)^2 = 0.25\n\n\nD\n6\n13.5\n16.5 - 0.5(6) = 16.5 - 3 = 13.5\n13.5 - 13.5 = 0\n|0| = 0\n(0)^2 = 0\n\n\nE\n7\n13\n16.5 - 0.5(7) = 16.5 - 3.5 = 13\n13 - 13 = 0\n|0| = 0\n(0)^2 = 0\n\n\nF\n8\n12.75\n16.5 - 0.5(8) = 16.5 - 4 = 12.5\n12.75 - 12.5 = 0.25\n|0.25| = 0.25\n(0.25)^2 = 0.0625\n\n\nSuma\n\n\n\n\n1.5\n0.625\n\n\n\n\n\nObliczanie R-kwadrat\n\nSST (Ca≈Çkowita suma kwadrat√≥w)\n\nSST = \\sum_{i=1}^{n}(y_i - \\bar{y})^2\nObliczyli≈õmy ju≈º te odchylenia w Kroku 1:\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n15.5\n1.75\n3.0625\n\n\nB\n14.25\n0.5\n0.25\n\n\nC\n13.5\n-0.25\n0.0625\n\n\nD\n13.5\n-0.25\n0.0625\n\n\nE\n13\n-0.75\n0.5625\n\n\nF\n12.75\n-1\n1\n\n\nSuma\n\n\n5\n\n\n\nSST = 5\n\nSSR (Suma kwadrat√≥w regresji)\n\nSSR = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2\n\n\n\n\n\n\n\n\n\nKraj\nPrzewidywane LH (\\hat{y}_i)\n\\hat{y}_i - \\bar{y}\n(\\hat{y}_i - \\bar{y})^2\n\n\n\n\nA\n15\n15 - 13.75 = 1.25\n(1.25)^2 = 1.5625\n\n\nB\n14.5\n14.5 - 13.75 = 0.75\n(0.75)^2 = 0.5625\n\n\nC\n14\n14 - 13.75 = 0.25\n(0.25)^2 = 0.0625\n\n\nD\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nE\n13\n13 - 13.75 = -0.75\n(-0.75)^2 = 0.5625\n\n\nF\n12.5\n12.5 - 13.75 = -1.25\n(-1.25)^2 = 1.5625\n\n\nSuma\n\n\n4.375\n\n\n\nSSR = 4.375\n\nSSE (Suma kwadrat√≥w b≈Çƒôd√≥w)\n\nSSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\nZ tabeli powy≈ºej, suma kwadrat√≥w b≈Çƒôd√≥w wynosi:\nSSE = 0.625\n\nWeryfikacja\n\nMo≈ºemy zweryfikowaƒá nasze obliczenia sprawdzajƒÖc, czy SST = SSR + SSE:\n5 = 4.375 + 0.625 = 5 \\checkmark\n\nObliczanie R-kwadrat\n\nR^2 = \\frac{SSR}{SST} = \\frac{4.375}{5} = 0.875\n\n\nObliczanie RMSE (Root Mean Square Error)\nRMSE jest obliczane przy u≈ºyciu formu≈Çy:\nRMSE = \\sqrt{\\frac{SSE}{n}} = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}}\nU≈ºywajƒÖc naszego obliczonego SSE:\nRMSE = \\sqrt{\\frac{0.625}{6}} = \\sqrt{0.104} \\approx 0.323\n\n\nObliczanie MAE (Mean Absolute Error)\nMAE jest obliczane przy u≈ºyciu formu≈Çy:\nMAE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}\nU≈ºywajƒÖc sum z tabeli:\nMAE = \\frac{1.5}{6} = 0.25\nModel regresji: LH = 16.5 - 0.5 \\cdot DM\nR-kwadrat: 0.875\nRMSE: 0.323\nMAE: 0.25\n\n\nInterpretacja:\n\nR√≥wnanie regresji: Dla ka≈ºdego wzrostu jednostkowego wielko≈õci okrƒôgu, indeks dysproporcjonalno≈õci LH jest oczekiwany spadek o 0.5 jednostki. Wyraz wolny (16.5) reprezentuje oczekiwany indeks LH, gdy wielko≈õƒá okrƒôgu wynosi zero (choƒá nie ma to praktycznego znaczenia, poniewa≈º wielko≈õƒá okrƒôgu nie mo≈ºe wynosiƒá zero).\nR-kwadrat: 0.875 wskazuje, ≈ºe oko≈Ço 87.5% wariancji w dysproporcjonalno≈õci wyborczej (indeks LH) mo≈ºe byƒá wyja≈õnione przez wielko≈õƒá okrƒôgu. Jest to wysoka warto≈õƒá, sugerujƒÖca, ≈ºe wielko≈õƒá okrƒôgu jest rzeczywi≈õcie silnym predyktorem dysproporcjonalno≈õci wyborczej.\nRMSE i MAE: Niskie warto≈õci RMSE (0.323) i MAE (0.25) wskazujƒÖ, ≈ºe model dobrze dopasowuje siƒô do danych, z ma≈Çymi b≈Çƒôdami predykcji.\nImplikacje polityczne: Odkrycia sugerujƒÖ, ≈ºe zwiƒôkszanie wielko≈õci okrƒôgu mog≈Çoby byƒá skutecznƒÖ strategiƒÖ reformy wyborczej dla kraj√≥w dƒÖ≈ºƒÖcych do zmniejszenia dysproporcjonalno≈õci miƒôdzy g≈Çosami a mandatami. Jednak korzy≈õci marginalne wydajƒÖ siƒô zmniejszaƒá wraz ze wzrostem wielko≈õci okrƒôgu, jak widaƒá w wzorcu danych.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#example-2.-descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "href": "correg_pl.html#example-2.-descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.18 Example 2. Descriptive Statistics and OLS Example - Income and Voter Turnout",
    "text": "16.18 Example 2. Descriptive Statistics and OLS Example - Income and Voter Turnout\nBackground\nIn preparation for the 2024 municipal elections, the Amsterdam Electoral Commission conducted research on voter participation patterns across different city neighborhoods. A key question emerged:\nDoes economic prosperity of a neighborhood correlate with civic engagement, specifically voter turnout?\nData Collection\nSample: 5 representative neighborhoods in Amsterdam\nTime Period: Data from the 2022 municipal elections\nVariables:\n\nIncome: Average annual household income per capita (thousands ‚Ç¨)\nTurnout: Percentage of registered voters who voted in the election\n\n\nInitial R Output for Reference\n\n# Data\nincome &lt;- c(50, 45, 56, 40, 60)  # thousands ‚Ç¨\nturnout &lt;- c(60, 56, 70, 50, 75) # %\n\n# Full model check\nmodel &lt;- lm(turnout ~ income)\nsummary(model)\n\n\nCall:\nlm(formula = turnout ~ income)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \nincome       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n\n\n\nDispersion Measures\nMeans:\n\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n} = \\frac{50 + 45 + 56 + 40 + 60}{5} = \\frac{251}{5} = 50.2\n\\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n} = \\frac{60 + 56 + 70 + 50 + 75}{5} = \\frac{311}{5} = 62.2\n\n# Verification\nmean(income)  # 50.2\n\n[1] 50.2\n\nmean(turnout) # 62.2\n\n[1] 62.2\n\n\nVariances:\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X: (-0.2, -5.2, 5.8, -10.2, 9.8)\ns^2_X = \\frac{0.04 + 27.04 + 33.64 + 104.04 + 96.04}{4} = \\frac{260.8}{4} = 65.2\nDeviations for Y: (-2.2, -6.2, 7.8, -12.2, 12.8)\ns^2_Y = \\frac{4.84 + 38.44 + 60.84 + 148.84 + 163.84}{4} = \\frac{416.8}{4} = 104.2\n\n# Verification\nvar(income)  # 65.2\n\n[1] 65.2\n\nvar(turnout) # 104.2\n\n[1] 104.2\n\n\n\n\nCovariance and Correlation\nCovariance:\ns_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-0.2 \\times -2.2) = 0.44 (-5.2 \\times -6.2) = 32.24 (5.8 \\times 7.8) = 45.24 (-10.2 \\times -12.2) = 124.44 (9.8 \\times 12.8) = 125.44\ns_{XY} = \\frac{327.8}{4} = 81.95\n\n# Verification\ncov(income, turnout) # 81.95\n\n[1] 81.95\n\n\nCorrelation:\nr_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{81.95}{\\sqrt{65.2}\\sqrt{104.2}} = 0.994\n\n# Verification\ncor(income, turnout) # 0.994\n\n[1] 0.9942402\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{81.95}{65.2} = 1.2571429\nIntercept:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\nStep by step:\n\n1.2571429 \\times 50.2 = 63.1085714\n\\hat{\\beta_0} = 62.2 - 63.1085714 = -0.9085714\n\n\n# Verification\ncoef(model)  # Exact coefficients from R\n\n(Intercept)      income \n -0.8964724   1.2569018 \n\n\n\n\nDetailed Decomposition of Variance and R-squared\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = -0.9085714 + 1.2571429X\nThe predicted values \\hat{Y} for each X value:\nFor X = 50:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (50)\n \\hat{Y} = -0.9085714 + 62.857145 \\hat{Y} = 61.9485736\nFor X = 45:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (45)\n \\hat{Y} = -0.9085714 + 56.5714305 \\hat{Y} = 55.6535591\nFor X = 56:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (56)\n \\hat{Y} = -0.9085714 + 70.4200024 \\hat{Y} = 69.5114310\nFor X = 40:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (40)\n \\hat{Y} = -0.9085714 + 50.2657160 \\hat{Y} = 49.3571446\nFor X = 60:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (60)\n \\hat{Y} = -0.9085714 + 75.4285740 \\hat{Y} = 74.5200026\n\n# Verification of predicted values\ny_hat &lt;- -0.9085714 + 1.2571429 * income\ndata.frame(\n  X = income,\n  Y = turnout,\n  Y_hat = y_hat,\n  row.names = 1:5\n)\n\n   X  Y    Y_hat\n1 50 60 61.94857\n2 45 56 55.66286\n3 56 70 69.49143\n4 40 50 49.37714\n5 60 75 74.52000\n\n\nStep 2: Calculate SST (Total Sum of Squares)\nSST = \\sum(Y_i - \\bar{Y})^2 \\text{ where } \\bar{Y} = 62.2\n(60 - 62.2)^2 = (-2.2)^2 = 4.84 (56 - 62.2)^2 = (-6.2)^2 = 38.44 (70 - 62.2)^2 = (7.8)^2 = 60.84 (50 - 62.2)^2 = (-12.2)^2 = 148.84 (75 - 62.2)^2 = (12.8)^2 = 163.84\nSST = 4.84 + 38.44 + 60.84 + 148.84 + 163.84 = 416.8\nStep 3: Calculate SSR (Regression Sum of Squares)\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\n(61.9485736 - 62.2)^2 = (-0.2514264)^2 = 0.0632151 (55.6535591 - 62.2)^2 = (-6.5464409)^2 = 42.8558689 (69.5114310 - 62.2)^2 = (7.3114310)^2 = 53.4570178 (49.3571446 - 62.2)^2 = (-12.8428554)^2 = 164.9389370 (74.5200026 - 62.2)^2 = (12.3200026)^2 = 151.7824640\nSSR = 413.0975028\nStep 4: Calculate SSE (Error Sum of Squares)\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n(60 - 61.9485736)^2 = (-1.9485736)^2 = 3.7969384 (56 - 55.6535591)^2 = (0.3464409)^2 = 0.1200212 (70 - 69.5114310)^2 = (0.4885690)^2 = 0.2387198 (50 - 49.3571446)^2 = (0.6428554)^2 = 0.4132631 (75 - 74.5200026)^2 = (0.4799974)^2 = 0.2303975\nSSE = 4.7024972\nStep 5: Verify decomposition\nSST = SSR + SSE 416.8 = 413.0975028 + 4.7024972\nStep 6: Calculate R-squared\nR^2 = \\frac{SSR}{SST} = \\frac{413.0975028}{416.8} = 0.9916\n\n# Verification\nsummary(model)$r.squared  # Should match our calculation\n\n[1] 0.9885135\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\ndf &lt;- data.frame(income = income, turnout = turnout)\n\nggplot(df, aes(x = income, y = turnout)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Voter Turnout vs Income per Capita\",\n    x = \"Income per Capita (thousands ‚Ç¨)\",\n    y = \"Voter Turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    text = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\nInterpretation\nThe analysis shows:\n\nA very strong positive correlation (r = 0.994) between income and voter turnout\nThe regression equation \\hat{Y} = -0.9085714 + 1.2571429X indicates that:\n\nFor each ‚Ç¨1,000 increase in income, turnout increases by about 1.26 percentage points\nThe intercept (-0.9086) has little practical meaning as income is never zero\n\nThe R-squared of 0.9916 indicates that 99.16% of the variance in turnout is explained by income",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#example-3.-anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "href": "correg_pl.html#example-3.-anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.19 Example 3. Anxiety Levels and Cognitive Performance: A Laboratory Study",
    "text": "16.19 Example 3. Anxiety Levels and Cognitive Performance: A Laboratory Study\n\nData and Context\nIn a psychology experiment, researchers measured the relationship between anxiety levels (measured by galvanic skin response, GSR) and cognitive performance (score on a working memory task).\n\n# Data\nanxiety &lt;- c(2.1, 3.4, 4.2, 5.1, 5.8, 6.4, 7.2, 8.0)  # GSR readings\nperformance &lt;- c(92, 88, 84, 78, 74, 70, 65, 62)      # Working memory scores\n\n# Initial model check\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8993 -0.6660  0.2162  0.6106  1.5262 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 105.3248     1.3189   79.86 0.00000000026 ***\nanxiety      -5.4407     0.2359  -23.06 0.00000043549 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 6 degrees of freedom\nMultiple R-squared:  0.9888,    Adjusted R-squared:  0.987 \nF-statistic: 531.9 on 1 and 6 DF,  p-value: 0.0000004355\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{2.1 + 3.4 + 4.2 + 5.1 + 5.8 + 6.4 + 7.2 + 8.0}{8} = \\frac{42.2}{8} = 5.275\n\\bar{Y} = \\frac{92 + 88 + 84 + 78 + 74 + 70 + 65 + 62}{8} = \\frac{613}{8} = 76.625\n\n# Verification\nmean(anxiety)\n\n[1] 5.275\n\nmean(performance)\n\n[1] 76.625\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(2.1 - 5.275) = -3.175\n(3.4 - 5.275) = -1.875\n(4.2 - 5.275) = -1.075\n(5.1 - 5.275) = -0.175\n(5.8 - 5.275) = 0.525\n(6.4 - 5.275) = 1.125\n(7.2 - 5.275) = 1.925\n(8.0 - 5.275) = 2.725\n\nSquared deviations:\n10.08063 + 3.51563 + 1.15563 + 0.03063 + 0.27563 + 1.26563 + 3.70563 +\n7.42563 = 27.45500\ns^2_X = \\frac{27.45500}{7} = 3.922143\nSimilarly for Y: Deviations:\n15.375, 11.375, 7.375, 1.375, -2.625, -6.625, -11.625, -14.625\ns^2_Y = \\frac{236.875 + 129.391 + 54.391 + 1.891 + 6.891 + 43.891 + 135.141 + 213.891}{7} = \\frac{822.362}{7} = 117.4803\n\n# Verification\nvar(anxiety)\n\n[1] 3.922143\n\nvar(performance)\n\n[1] 117.4107\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-3.175 √ó 15.375) = -48.815625\n(-1.875 √ó 11.375) = -21.328125\n(-1.075 √ó 7.375) = -7.928125\n(-0.175 √ó 1.375) = -0.240625\n(0.525 √ó -2.625) = -1.378125\n(1.125 √ó -6.625) = -7.453125\n(1.925 √ó -11.625) = -22.378125\n(2.725 √ó -14.625) = -39.853125\nSum = -149.375\ns_{XY} = \\frac{-149.375}{7} = -21.33929\n\n# Verification\ncov(anxiety, performance)\n\n[1] -21.33929\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-21.33929}{\\sqrt{3.922143}\\sqrt{117.4803}} = -0.9932\n\n# Verification\ncor(anxiety, performance)\n\n[1] -0.9944073\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-21.33929}{3.922143} = -5.4407\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-5.4407 √ó 5.275 = -28.6997\n\\hat{\\beta_0} = 76.625 - (-28.6997) = 105.3247\n\n\n# Verification\ncoef(model)\n\n(Intercept)     anxiety \n 105.324804   -5.440721 \n\n\n\n\n4. R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}): \\hat{Y} = 105.3247 - 5.4407X\n\n# Predicted values\ny_hat &lt;- 105.3247 - 5.4407 * anxiety\ndata.frame(\n  Anxiety = anxiety,\n  Performance = performance,\n  Predicted = y_hat,\n  row.names = 1:8\n)\n\n  Anxiety Performance Predicted\n1     2.1          92  93.89923\n2     3.4          88  86.82632\n3     4.2          84  82.47376\n4     5.1          78  77.57713\n5     5.8          74  73.76864\n6     6.4          70  70.50422\n7     7.2          65  66.15166\n8     8.0          62  61.79910\n\n\nStep 2: Sum of Squares\nSST = \\sum(Y_i - \\bar{Y})^2 = 822.362\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 816.3094\nSSE = \\sum(Y_i - \\hat{Y}_i)^2 = 6.0526\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{816.3094}{822.362} = 0.9926\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.9888459\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\n\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Cognitive Performance vs. Anxiety Levels\",\n    x = \"Anxiety (GSR)\",\n    y = \"Performance Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.993) between anxiety and cognitive performance\nFor each unit increase in GSR (anxiety), performance decreases by approximately 5.44 points\nThe model explains 99.26% of the variance in performance scores\nThe relationship appears to be strongly linear, suggesting a reliable anxiety-performance relationship\nThe high intercept (105.32) represents the theoretical maximum performance at zero anxiety\n\n\n\nStudy Limitations\n\nSmall sample size (n=8)\nPossible other confounding variables\nLimited range of anxiety levels\nCross-sectional rather than longitudinal data",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przyk≈Çad-4.-analiza-zwiƒÖzku-miƒôdzy-wielko≈õciƒÖ-okrƒôgu-a-wska≈∫nikiem-dysproporcjonalno≈õci-wyborczej-2",
    "href": "correg_pl.html#przyk≈Çad-4.-analiza-zwiƒÖzku-miƒôdzy-wielko≈õciƒÖ-okrƒôgu-a-wska≈∫nikiem-dysproporcjonalno≈õci-wyborczej-2",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.20 Przyk≈Çad 4. Analiza zwiƒÖzku miƒôdzy wielko≈õciƒÖ okrƒôgu a wska≈∫nikiem dysproporcjonalno≈õci wyborczej (2)",
    "text": "16.20 Przyk≈Çad 4. Analiza zwiƒÖzku miƒôdzy wielko≈õciƒÖ okrƒôgu a wska≈∫nikiem dysproporcjonalno≈õci wyborczej (2)\nTa analiza bada zwiƒÖzek miƒôdzy wielko≈õciƒÖ okrƒôgu (DM) a wska≈∫nikiem dysproporcjonalno≈õci Loosemore-Hanby (LH) w wyborach parlamentarnych w 6 krajach. Wska≈∫nik Loosemore-Hanby mierzy dysproporcjonalno≈õƒá wyborczƒÖ, przy czym wy≈ºsze warto≈õci wskazujƒÖ na wiƒôkszƒÖ dysproporcjonalno≈õƒá miƒôdzy g≈Çosami a mandatami.\n\nDane\n\n\n\nWielko≈õƒá okrƒôgu i wska≈∫nik LH wed≈Çug kraju\n\n\nKraj\nDM\nLH\n\n\n\n\nA\n4\n12\n\n\nB\n10\n8\n\n\nC\n3\n15\n\n\nD\n8\n10\n\n\nE\n7\n6\n\n\nF\n4\n13\n\n\n\n\n\n\n\nKrok 1: Obliczenie wariancji i odchylenia standardowego dla DM i LH\n\nObliczenia dla wielko≈õci okrƒôgu (DM)\nNajpierw obliczƒô ≈õredniƒÖ warto≈õci DM:\n\\bar{x}_{DM} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\frac{4 + 10 + 3 + 8 + 7 + 4}{6} = \\frac{36}{6} = 6\nNastƒôpnie obliczƒô wariancjƒô z korektƒÖ Bessela, korzystajƒÖc z wzoru:\ns^2_{DM} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\n\n\nKraj\nDM (x_i)\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\n\n\n\n\nA\n4\n4 - 6 = -2\n(-2)^2 = 4\n\n\nB\n10\n10 - 6 = 4\n(4)^2 = 16\n\n\nC\n3\n3 - 6 = -3\n(-3)^2 = 9\n\n\nD\n8\n8 - 6 = 2\n(2)^2 = 4\n\n\nE\n7\n7 - 6 = 1\n(1)^2 = 1\n\n\nF\n4\n4 - 6 = -2\n(-2)^2 = 4\n\n\nSuma\n\n\n38\n\n\n\ns^2_{DM} = \\frac{38}{6-1} = \\frac{38}{5} = 7.6\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji:\ns_{DM} = \\sqrt{s^2_{DM}} = \\sqrt{7.6} = 2.757\n\n\nObliczenia dla wska≈∫nika LH\nNajpierw obliczƒô ≈õredniƒÖ warto≈õci LH:\n\\bar{y}_{LH} = \\frac{12 + 8 + 15 + 10 + 6 + 13}{6} = \\frac{64}{6} = 10.667\nNastƒôpnie obliczƒô wariancjƒô z korektƒÖ Bessela:\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n12\n12 - 10.667 = 1.333\n(1.333)^2 = 1.777\n\n\nB\n8\n8 - 10.667 = -2.667\n(-2.667)^2 = 7.113\n\n\nC\n15\n15 - 10.667 = 4.333\n(4.333)^2 = 18.775\n\n\nD\n10\n10 - 10.667 = -0.667\n(-0.667)^2 = 0.445\n\n\nE\n6\n6 - 10.667 = -4.667\n(-4.667)^2 = 21.781\n\n\nF\n13\n13 - 10.667 = 2.333\n(2.333)^2 = 5.443\n\n\nSuma\n\n\n55.334\n\n\n\ns^2_{LH} = \\frac{55.334}{6-1} = \\frac{55.334}{5} = 11.067\nOdchylenie standardowe to:\ns_{LH} = \\sqrt{s^2_{LH}} = \\sqrt{11.067} = 3.327\nPodsumowanie kroku 1:\n\nWariancja DM (z korektƒÖ Bessela): 7.6\nOdchylenie standardowe DM: 2.757\nWariancja LH (z korektƒÖ Bessela): 11.067\nOdchylenie standardowe LH: 3.327\n\n\n\n\nKrok 2: Obliczenie kowariancji miƒôdzy DM a LH dla tej pr√≥by danych\nKowariancja jest obliczana przy u≈ºyciu wzoru z korektƒÖ Bessela:\nCov(DM, LH) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\nA\n4\n12\n-2\n1.333\n(-2)(1.333) = -2.666\n\n\nB\n10\n8\n4\n-2.667\n(4)(-2.667) = -10.668\n\n\nC\n3\n15\n-3\n4.333\n(-3)(4.333) = -12.999\n\n\nD\n8\n10\n2\n-0.667\n(2)(-0.667) = -1.334\n\n\nE\n7\n6\n1\n-4.667\n(1)(-4.667) = -4.667\n\n\nF\n4\n13\n-2\n2.333\n(-2)(2.333) = -4.666\n\n\nSuma\n\n\n\n\n-37\n\n\n\nCov(DM, LH) = \\frac{-37}{6-1} = \\frac{-37}{5} = -7.4\nKowariancja miƒôdzy DM a LH: -7.4\nUjemna kowariancja wskazuje na odwrotnƒÖ zale≈ºno≈õƒá: wraz ze wzrostem wielko≈õci okrƒôgu wska≈∫nik dysproporcjonalno≈õci LH ma tendencjƒô do spadku.\n\n\nKrok 3: Obliczenie wsp√≥≈Çczynnika korelacji liniowej Pearsona miƒôdzy DM a LH\nWsp√≥≈Çczynnik korelacji Pearsona oblicza siƒô przy u≈ºyciu wzoru:\nr = \\frac{Cov(DM, LH)}{s_{DM} \\cdot s_{LH}}\nMamy ju≈º obliczone:\n\nCov(DM, LH) = -7.4\ns_{DM} = 2.757\ns_{LH} = 3.327\n\nr = \\frac{-7.4}{2.757 \\cdot 3.327} = \\frac{-7.4}{9.172} = -0.807\nWsp√≥≈Çczynnik korelacji Pearsona: -0.807\n\nInterpretacja:\nWsp√≥≈Çczynnik korelacji -0.807 wskazuje:\n\nKierunek: Ujemny znak pokazuje odwrotnƒÖ zale≈ºno≈õƒá miƒôdzy wielko≈õciƒÖ okrƒôgu a wska≈∫nikiem LH.\nSi≈Ça: Warto≈õƒá bezwzglƒôdna 0.807 wskazuje na silnƒÖ korelacjƒô (blisko -1).\nInterpretacja praktyczna: Poniewa≈º wy≈ºsze warto≈õci wska≈∫nika LH wskazujƒÖ na wiƒôkszƒÖ dysproporcjonalno≈õƒá, ta silna ujemna korelacja sugeruje, ≈ºe wraz ze wzrostem wielko≈õci okrƒôgu, dysproporcjonalno≈õƒá wyborcza ma tendencjƒô do znacznego spadku. Innymi s≈Çowy, systemy wyborcze z wiƒôkszymi okrƒôgami wyborczymi (wiƒôcej przedstawicieli wybieranych w okrƒôgu) majƒÖ tendencjƒô do generowania bardziej proporcjonalnych wynik√≥w (mniejsza dysproporcjonalno≈õƒá).\n\nUstalenie to jest zgodne z teoriƒÖ nauk politycznych, kt√≥ra sugeruje, ≈ºe wiƒôksze okrƒôgi wyborcze zapewniajƒÖ mniejszym partiom wiƒôcej mo≈ºliwo≈õci uzyskania reprezentacji, co prowadzi do wynik√≥w wyborczych, kt√≥re lepiej odzwierciedlajƒÖ rozk≈Çad g≈Ços√≥w miƒôdzy partiami.\n\n\n\nKrok 4: Skonstruowanie prostego modelu regresji liniowej i obliczenie R-kwadrat\nU≈ºyjƒô wzoru na prostƒÖ regresjƒô liniowƒÖ:\nLH = \\beta_0 + \\beta_1 \\cdot DM + \\varepsilon\nGdzie:\n\n\\beta_0 to wyraz wolny\n\\beta_1 to wsp√≥≈Çczynnik nachylenia dla DM\n\nWz√≥r na obliczenie \\beta_1 z korektƒÖ Bessela to:\n\\beta_1 = \\frac{Cov(DM, LH)}{s^2_{DM}} = \\frac{-7.4}{7.6} = -0.974\nAby obliczyƒá \\beta_0, u≈ºyjƒô:\n\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x} = 10.667 - (-0.974) \\cdot 6 = 10.667 + 5.844 = 16.511\nZatem r√≥wnanie regresji to:\nLH = 16.511 - 0.974 \\cdot DM\n\nObliczanie R-kwadrat\nAby w≈Ça≈õciwie obliczyƒá R-kwadrat, muszƒô obliczyƒá nastƒôpujƒÖce sumy kwadrat√≥w:\n\nSST (Ca≈Çkowita suma kwadrat√≥w): Mierzy ca≈ÇkowitƒÖ zmienno≈õƒá zmiennej zale≈ºnej (LH)\nSSR (Suma kwadrat√≥w regresji): Mierzy zmienno≈õƒá wyja≈õnionƒÖ przez model regresji\nSSE (Suma kwadrat√≥w b≈Çƒôd√≥w): Mierzy niewyja≈õnionƒÖ zmienno≈õƒá modelu\n\nNajpierw obliczƒô przewidywane warto≈õci LH, u≈ºywajƒÖc naszego r√≥wnania regresji:\n\\hat{LH} = 16.511 - 0.974 \\cdot DM\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i = 16.511 - 0.974x_i)\n\n\n\n\nA\n4\n12\n16.511 - 0.974(4) = 16.511 - 3.896 = 12.615\n\n\nB\n10\n8\n16.511 - 0.974(10) = 16.511 - 9.74 = 6.771\n\n\nC\n3\n15\n16.511 - 0.974(3) = 16.511 - 2.922 = 13.589\n\n\nD\n8\n10\n16.511 - 0.974(8) = 16.511 - 7.792 = 8.719\n\n\nE\n7\n6\n16.511 - 0.974(7) = 16.511 - 6.818 = 9.693\n\n\nF\n4\n13\n16.511 - 0.974(4) = 16.511 - 3.896 = 12.615\n\n\n\nTeraz, obliczƒô ka≈ºdƒÖ sumƒô kwadrat√≥w:\n\nSST (Ca≈Çkowita suma kwadrat√≥w)\n\nSST = \\sum_{i=1}^{n}(y_i - \\bar{y})^2\nJu≈º obliczyli≈õmy te odchylenia w Kroku 1:\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n12\n1.333\n1.777\n\n\nB\n8\n-2.667\n7.113\n\n\nC\n15\n4.333\n18.775\n\n\nD\n10\n-0.667\n0.445\n\n\nE\n6\n-4.667\n21.781\n\n\nF\n13\n2.333\n5.443\n\n\nSuma\n\n\n55.334\n\n\n\nSST = 55.334\n\nSSR (Suma kwadrat√≥w regresji)\n\nSSR = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2\n\n\n\n\n\n\n\n\n\nKraj\nPrzewidywane LH (\\hat{y}_i)\n\\hat{y}_i - \\bar{y}\n(\\hat{y}_i - \\bar{y})^2\n\n\n\n\nA\n12.615\n12.615 - 10.667 = 1.948\n(1.948)^2 = 3.795\n\n\nB\n6.771\n6.771 - 10.667 = -3.896\n(-3.896)^2 = 15.178\n\n\nC\n13.589\n13.589 - 10.667 = 2.922\n(2.922)^2 = 8.538\n\n\nD\n8.719\n8.719 - 10.667 = -1.948\n(-1.948)^2 = 3.795\n\n\nE\n9.693\n9.693 - 10.667 = -0.974\n(-0.974)^2 = 0.949\n\n\nF\n12.615\n12.615 - 10.667 = 1.948\n(1.948)^2 = 3.795\n\n\nSuma\n\n\n36.05\n\n\n\nSSR = 36.05\n\nSSE (Suma kwadrat√≥w b≈Çƒôd√≥w)\n\nSSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i)\ny_i - \\hat{y}_i\n(y_i - \\hat{y}_i)^2\n\n\n\n\nA\n12\n12.615\n12 - 12.615 = -0.615\n(-0.615)^2 = 0.378\n\n\nB\n8\n6.771\n8 - 6.771 = 1.229\n(1.229)^2 = 1.510\n\n\nC\n15\n13.589\n15 - 13.589 = 1.411\n(1.411)^2 = 1.991\n\n\nD\n10\n8.719\n10 - 8.719 = 1.281\n(1.281)^2 = 1.641\n\n\nE\n6\n9.693\n6 - 9.693 = -3.693\n(-3.693)^2 = 13.638\n\n\nF\n13\n12.615\n13 - 12.615 = 0.385\n(0.385)^2 = 0.148\n\n\nSuma\n\n\n\n19.306\n\n\n\nSSE = 19.306\n\nWeryfikacja\n\nMo≈ºemy zweryfikowaƒá nasze obliczenia, sprawdzajƒÖc czy SST = SSR + SSE:\n55.334 \\approx 36.05 + 19.306 = 55.356\nNiewielka r√≥≈ºnica (0.022) wynika z zaokrƒÖgle≈Ñ w obliczeniach.\n\nObliczanie R-kwadrat\n\nR^2 = \\frac{SSR}{SST} = \\frac{36.05}{55.334} = 0.652\nAlternatywnie, mo≈ºemy te≈º obliczyƒá:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{19.306}{55.334} = 1 - 0.349 = 0.651\nDrobna r√≥≈ºnica wynika z zaokrƒÖgle≈Ñ.\n\n\nObliczanie RMSE (Pierwiastek ≈õredniego b≈Çƒôdu kwadratowego)\nObliczanie RMSE (Pierwiastek ≈õredniego b≈Çƒôdu kwadratowego)\nRMSE oblicza siƒô przy u≈ºyciu wzoru:\nRMSE = \\sqrt{\\frac{SSE}{n}} = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}}\nKorzystajƒÖc z naszej obliczonej SSE:\nRMSE = \\sqrt{\\frac{19.306}{6}} = \\sqrt{3.218} = 1.794\nKorekta Bessela (dzielenie przez n-1 zamiast n) stosuje siƒô do estymacji wariancji pr√≥by, ale nie jest standardowo stosowana przy obliczaniu RMSE, gdy≈º RMSE jest miarƒÖ b≈Çƒôdu predykcji, a nie estymatorem parametru populacji.\n\n\nObliczanie MAE (≈öredni b≈ÇƒÖd bezwzglƒôdny)\nMAE oblicza siƒô przy u≈ºyciu wzoru:\nMAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i)\n|y_i - \\hat{y}_i|\n\n\n\n\nA\n12\n12.615\n|12 - 12.615| = 0.615\n\n\nB\n8\n6.771\n|8 - 6.771| = 1.229\n\n\nC\n15\n13.589\n|15 - 13.589| = 1.411\n\n\nD\n10\n8.719\n|10 - 8.719| = 1.281\n\n\nE\n6\n9.693\n|6 - 9.693| = 3.693\n\n\nF\n13\n12.615\n|13 - 12.615| = 0.385\n\n\nSuma\n\n\n8.614\n\n\n\nMAE = \\frac{8.614}{6} = 1.436\nModel regresji: LH = 16.511 - 0.974 \\cdot DM\nR-kwadrat: 0.651\nRMSE: 1.794\nMAE: 1.436\n\n\nInterpretacja:\n\nR√≥wnanie regresji: Dla ka≈ºdego jednostkowego wzrostu wielko≈õci okrƒôgu, wska≈∫nik dysproporcjonalno≈õci LH zmniejsza siƒô o 0.974 jednostki. Wyraz wolny (16.511) reprezentuje oczekiwany wska≈∫nik LH, gdy wielko≈õƒá okrƒôgu wynosi zero (choƒá nie ma to praktycznego znaczenia, poniewa≈º wielko≈õƒá okrƒôgu nie mo≈ºe wynosiƒá zero).\nR-kwadrat: 0.651 wskazuje, ≈ºe oko≈Ço 65.1% wariancji dysproporcjonalno≈õci wyborczej (wska≈∫nik LH) mo≈ºe byƒá wyja≈õnione przez wielko≈õƒá okrƒôgu. Jest to do≈õƒá wysoka warto≈õƒá, sugerujƒÖca, ≈ºe wielko≈õƒá okrƒôgu jest rzeczywi≈õcie silnym predyktorem dysproporcjonalno≈õci wyborczej, choƒá mniejszym ni≈º w poprzednim zestawie danych.\nRMSE: Warto≈õƒá 1.794 informuje nas o przeciƒôtnym b≈Çƒôdzie prognozy modelu. Jest to miara dok≈Çadno≈õci przewidywa≈Ñ modelu wyra≈ºona w jednostkach zmiennej zale≈ºnej (LH).\nMAE: Warto≈õƒá 1.436 informuje nas o przeciƒôtnym bezwzglƒôdnym b≈Çƒôdzie prognozy modelu. W por√≥wnaniu z RMSE, MAE jest mniej czu≈Çy na warto≈õci odstajƒÖce, co potwierdza, ≈ºe niekt√≥re obserwacje (np. dla kraju E) majƒÖ stosunkowo du≈ºy b≈ÇƒÖd predykcji.\nImplikacje polityczne: Wyniki sugerujƒÖ, ≈ºe zwiƒôkszenie wielko≈õci okrƒôgu mog≈Çoby byƒá skutecznƒÖ strategiƒÖ reform wyborczych dla kraj√≥w starajƒÖcych siƒô zmniejszyƒá dysproporcjonalno≈õƒá miƒôdzy g≈Çosami a mandatami. Jednak≈ºe, korzy≈õci marginalne wydajƒÖ siƒô zmniejszaƒá wraz ze wzrostem wielko≈õci okrƒôgu, jak widaƒá we wzorcu danych.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#example-5.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "href": "correg_pl.html#example-5.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "title": "16¬† Wprowadzenie do analizy korelacji i regresji (Correlation and Regression Analysis)",
    "section": "16.21 Example 5. District Magnitude and Electoral Disproportionality: A Comparative Analysis (*)",
    "text": "16.21 Example 5. District Magnitude and Electoral Disproportionality: A Comparative Analysis (*)\n\nData Generating Process\nLet‚Äôs set up a DGP where:\n\\begin{aligned}\n& Y_{\\text{Gallagher}} = 12 - 0.8X_{\\text{DM}} + \\varepsilon \\\\\n& \\varepsilon \\sim \\mathcal{N}(0, 1) \\\\\n& X_{\\text{DM}} \\in \\{3, 5, 7, 10, 12, 15\\}\n\\end{aligned}\n\n# DGP\nmagnitude &lt;- c(3, 5, 7, 10, 12, 15)\nepsilon &lt;- rnorm(6, mean = 0, sd = 1)\ngallagher &lt;- 12 - 0.8 * magnitude + epsilon\n\n# Round (sampled from the DGP) Gallagher indices to one decimal place\ngallagher &lt;- round(c(9.0, 7.8, 9.2, 4.1, 2.5, 1.7), 1)\n\n# Show data\ndata.frame(\n  District_Magnitude = magnitude,\n  Gallagher_Index = gallagher\n)\n\n  District_Magnitude Gallagher_Index\n1                  3             9.0\n2                  5             7.8\n3                  7             9.2\n4                 10             4.1\n5                 12             2.5\n6                 15             1.7\n\n# Initial model check\nmodel &lt;- lm(gallagher ~ magnitude)\nsummary(model)\n\n\nCall:\nlm(formula = gallagher ~ magnitude)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.6516 -0.4628  2.3260 -0.6908 -0.9020  0.3813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7349     1.3034   9.003 0.000843 ***\nmagnitude    -0.6944     0.1359  -5.110 0.006934 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.368 on 4 degrees of freedom\nMultiple R-squared:  0.8672,    Adjusted R-squared:  0.834 \nF-statistic: 26.11 on 1 and 4 DF,  p-value: 0.006934\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{3 + 5 + 7 + 10 + 12 + 15}{6} = \\frac{52}{6} = 8.6667\n\\bar{Y} = \\frac{9.0 + 7.8 + 9.2 + 4.1 + 2.5 + 1.7}{6} = \\frac{34.3}{6} = 5.7167\n\n# Verification\nmean(magnitude)\n\n[1] 8.666667\n\nmean(gallagher)\n\n[1] 5.716667\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(3 - 8.6667) = -5.6667\n(5 - 8.6667) = -3.6667\n(7 - 8.6667) = -1.6667\n(10 - 8.6667) = 1.3333\n(12 - 8.6667) = 3.3333\n(15 - 8.6667) = 6.3333\n\nSquared deviations:\n\n32.1115 + 13.4445 + 2.7779 + 1.7777 + 11.1109 + 40.1107 = 101.3332\n\ns^2_X = \\frac{101.3332}{5} = 20.2666\nFor Y: Deviations: 3.2833, 2.0833, 3.4833, -1.6167, -3.2167, -4.0167\ns^2_Y = \\frac{56.3483}{5} = 11.2697\n\n# Verification\nvar(magnitude)\n\n[1] 20.26667\n\nvar(gallagher)\n\n[1] 11.26967\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n\n(-5.6667 √ó 3.2833) = -18.6057\n(-3.6667 √ó 2.0833) = -7.6387\n(-1.6667 √ó 3.4833) = -5.8056\n(1.3333 √ó -1.6167) = -2.1556\n(3.3333 √ó -3.2167) = -10.7223\n(6.3333 √ó -4.0167) = -25.4391\n\nSum = -70.3670\ns_{XY} = \\frac{-70.3670}{5} = -14.0734\n\n# Verification\ncov(magnitude, gallagher)\n\n[1] -14.07333\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-14.0734}{\\sqrt{20.2666}\\sqrt{11.2697}} = -0.9279\n\n# Verification\ncor(magnitude, gallagher)\n\n[1] -0.9312157\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-14.0734}{20.2666} = -0.6944\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-0.6944 √ó 8.6667 = -6.0181\n\\hat{\\beta_0} = 5.7167 - (-6.0181) = 11.7348\n\n\n# Verification\ncoef(model)\n\n(Intercept)   magnitude \n 11.7348684  -0.6944079 \n\n\n\n\nR-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = 11.7348 - 0.6944X\n\n# Predicted values\ny_hat &lt;- 11.7348 - 0.6944 * magnitude\ndata.frame(\n  Magnitude = magnitude,\n  Gallagher = gallagher,\n  Predicted = y_hat,\n  row.names = 1:6\n)\n\n  Magnitude Gallagher Predicted\n1         3       9.0    9.6516\n2         5       7.8    8.2628\n3         7       9.2    6.8740\n4        10       4.1    4.7908\n5        12       2.5    3.4020\n6        15       1.7    1.3188\n\n\nStep 2: Sum of Squares SST = \\sum(Y_i - \\bar{Y})^2 = 56.3483 SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 48.5271 SSE = \\sum(Y_i - \\hat{Y}_i)^2 = 7.8212\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{48.5271}{56.3483} = 0.8612\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.8671626\n\n\n\n\nVisualization - True vs.¬†Estimated Parameters\n\nTrue DGP: Y = 12 - 0.8X + Œµ\nEstimated Model: Y = 11.7348 - 0.6944X\n\n\nlibrary(ggplot2)\n\n# Create data frame with original data\ndf &lt;- data.frame(\n  magnitude = magnitude,\n  gallagher = gallagher\n)\n\n# Create sequence for smooth lines\nx_seq &lt;- seq(min(magnitude), max(magnitude), length.out = 100)\n\n# Calculate predicted values for both lines\ntrue_dgp &lt;- 12 - 0.8 * x_seq\nestimated &lt;- 11.7348 - 0.6944 * x_seq\n\n# Combine into a data frame for plotting\nlines_df &lt;- data.frame(\n  magnitude = rep(x_seq, 2),\n  value = c(true_dgp, estimated),\n  Model = rep(c(\"True DGP\", \"Estimated\"), each = length(x_seq))\n)\n\n# Create plot\nggplot() +\n  geom_line(data = lines_df, \n            aes(x = magnitude, y = value, color = Model, linetype = Model),\n            size = 1) +\n  geom_point(data = df, \n             aes(x = magnitude, y = gallagher),\n             color = \"black\", \n             size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  labs(\n    title = \"True DGP vs. Estimated Regression Line\",\n    subtitle = \"Black points show observed data with random noise\",\n    x = \"District Magnitude\",\n    y = \"Gallagher Index\",\n    caption = \"True DGP: Y = 12 - 0.8X + Œµ\\nEstimated: Y = 11.73 - 0.69X\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.caption = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\n\nObservations about Model Fit\n\nSlope Comparison\n\nTrue slope: -0.8\nEstimated slope: -0.69\nThe estimated slope is reasonably close to the true parameter\n\nIntercept Comparison\n\nTrue intercept: 12\nEstimated intercept: 11.73\nThe estimated intercept very closely approximates the true value\n\nVisual Patterns\n\nThe lines are nearly parallel, showing good slope recovery\nPoints scatter around both lines due to the random error term (Œµ)\nThe small sample size (n=6) leads to some imprecision in estimation\nThe estimated line (blue) provides a good approximation of the true DGP (red dashed)\n\nImpact of Random Error\n\nThe scatter of points around the true DGP line reflects the N(0,1) error term\nThis noise leads to the slight differences in estimated parameters\nWith a larger sample, we would expect even closer convergence to true parameters\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.93) between district magnitude and electoral disproportionality\nFor each unit increase in district magnitude, the Gallagher index decreases by approximately 0.69 points\nThe model explains 86.12% of the variance in disproportionality\nThe relationship appears strongly linear with moderate scatter\nThe intercept (11.73) represents the expected disproportionality in a hypothetical single-member district system\n\n\n\nStudy Context\n\nData represents simulated observations from a DGP with moderate noise\nSample shows how increasing district magnitude tends to reduce disproportionality\nRandom component reflects other institutional and political factors affecting disproportionality\n\n\n\nLimitations\n\nSmall sample size (n=6)\nSimulated rather than real-world data\nAssumes linear relationship\nDoes not account for other institutional features",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Wprowadzenie do analizy korelacji i regresji *(Correlation and Regression Analysis)*</span>"
    ]
  },
  {
    "objectID": "probability_en.html",
    "href": "probability_en.html",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "",
    "text": "17.1 Probability: Preliminary Concepts\nImagine you‚Äôre trying to decide whether to bring an umbrella to class tomorrow. You check the weather forecast, which says there‚Äôs a 30% chance of rain. But what does this number really mean?\nThis is where probability comes in - it‚Äôs a mathematical way to measure how likely something is to happen.\nA probability represents the likelihood or chance of an event occurring, expressed as a number between 0 and 1 (or as a percentage between 0% and 100%).\nBefore we dive into probability theory, let‚Äôs establish some foundational concepts that we‚Äôll use throughout this course.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#probability-preliminary-concepts",
    "href": "probability_en.html#probability-preliminary-concepts",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "",
    "text": "Basic Set Concepts\nBefore we can understand probability, we need to grasp some fundamental concepts from set theory. A set is simply a collection of distinct objects.\nA set can be defined by:\n\nListing all elements: A = \\{1, 2, 3\\}\nDescribing a property: B = \\{\\text{x | x is a positive integer less than 4}\\}\n\nThe empty set \\emptyset contains no elements.\nIf A and B are sets:\n\nIf A is a subset of B, we write A \\subseteq B\nIf x is an element of A, we write x \\in A\n\nFor example, if B = \\{1, 2, 3\\}:\n\n\\{1, 2\\} is a subset of B (written \\{1, 2\\} \\subseteq B)\n1 is an element of B (written 1 \\in B)\n\n\n\n\n\n\n\nNote\n\n\n\nThe proper subset notation uses a strict subset symbol. If A is a proper subset of B, we write:\nA \\subset B\nThis means that A is a subset of B AND A \\neq B (A is not equal to B).\nIn contrast, A \\subseteq B allows for the possibility that A = B.\nA set is a fundamental mathematical concept - it‚Äôs a collection of distinct objects where order doesn‚Äôt matter and duplicates are not allowed. In other words, each element either belongs to the set or it doesn‚Äôt, with no concept of ‚Äúhow many times‚Äù it belongs.\nFormally, if x \\in A (meaning x is an element of set A), then adding another copy of x has no effect on A. This gives us identities like:\n\\{1, 2, 2, 3\\} = \\{1, 2, 3\\} = \\{3, 1, 2\\}\nThis distinguishes sets from other mathematical collections:\n\nLists/Sequences: Order matters and duplicates are allowed\n\n[1, 2, 2, 3] ‚â† [1, 2, 3]\n[1, 2, 3] ‚â† [3, 2, 1]\n\nMultisets: Order doesn‚Äôt matter but duplicates are allowed\n\n{1, 2, 2, 3}‚Çò ‚â† {1, 2, 3}‚Çò\n{1, 2, 2, 3}‚Çò = {3, 2, 1, 2}‚Çò\n\n\nThis unique property of sets - that membership is binary (an element either belongs or doesn‚Äôt) - makes them particularly useful in mathematics for describing collections where we only care about whether something is present, not how many times it appears or in what order.\n\n\n\n\nSet Operations\nBasic set (events) operations (given two sets A and B):\n\nUnion (A \\cup B): Elements in either A OR B (or both)\nIntersection (A \\cap B): Elements in BOTH A AND B\nComplement (A^c or A^{'}): Elements NOT in A\nDifference (A \\setminus B): Elements in A but NOT in B\n\nThese operations follow important laws like:\n(A \\cup B)^c = A^c \\cap B^c (DeMorgan‚Äôs Law)\n(A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C) (Distributive Law)\nSets and the associated operations are easy to visualize in terms of Venn diagrams, as illustrated in the figure below:\n\n\n\nhttp://athenasc.com/probbook.html\n\n\nExamples of Venn diagrams:\n\nThe shaded region is S \\cap T.\nThe shaded region is S \\cup T.\nThe shaded region is S \\cap T^c.\nHere, T \\subset S. The shaded region is the complement of S.\nThe sets S, T, and U are disjoint.\nThe sets S, T, and U form a partition of the universal set \\Omega.\n\nThe Universal Set (often denoted as \\Omega, U, or S):\n\nIn Set Theory:\n\n\nSet containing all elements in a given context\nAll other sets are its subsets\nComplement of set A is A' = \\Omega - A\n\n\nIn Probability:\n\n\nCalled the sample space S or \\Omega\nContains all possible outcomes\nHas probability P(\\Omega) = 1\n\nKey Properties:\n\nA \\subseteq \\Omega\nA \\cup A' = \\Omega\nA \\cap A' = \\emptyset\n\\Omega' = \\emptyset\n\\emptyset' = \\Omega\n\nExamples:\n\nDie roll: \\Omega = \\{1,2,3,4,5,6\\}\nCoin flip: \\Omega = \\{H,T\\}\n\n\n\n\n\n\n\nSet Theory as a Language for Probability\n\n\n\nSet theory provides the mathematical framework for probability theory. Here are the key parallels:\n\n\n\n\n\n\n\n\nSet Theory\nProbability Theory\nDescription\n\n\n\n\n\\Omega (Universal set)\nSample space (S)\nAll possible outcomes\n\n\nx \\in A (Element)\nOutcome\nSingle result\n\n\nA \\subseteq \\Omega (Subset)\nEvent\nCollection of outcomes\n\n\n\\emptyset (Empty set)\nImpossible event\nCannot occur (P(\\emptyset) = 0)\n\n\n\\Omega (Universal set)\nCertain event\nMust occur (P(\\Omega) = 1)\n\n\nA \\cup B (Union)\nEither A OR B\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\n\nA \\cap B (Intersection)\nBoth A AND B\nP(A \\cap B) = P(A)P(B) (if independent)\n\n\nA' (Complement)\nNot A\nP(A') = 1 - P(A)\n\n\nA \\cap B = \\emptyset\nMutually exclusive\nP(A \\cap B) = 0\n\n\n\n\n\n\n\n\n\n\n\nCardinality of Sets\n\n\n\nIn set theory, we denote cardinality (the number of elements in a set) using vertical bars: |A|\nKey points:\n\n|A| means ‚Äúnumber of elements in set A‚Äù\nFor a finite set like A = \\{1, 2, 3\\}, we have |A| = 3\nEmpty set has cardinality zero: |\\emptyset| = 0\nFor two sets A and B:\n\nUnion (no overlap): |A \\cup B| = |A| + |B|\nUnion (with overlap): |A \\cup B| = |A| + |B| - |A \\cap B|\nCartesian product: |A \\times B| = |A| \\times |B|\n\n\nExample:\nIf A = \\{\\spadesuit, \\clubsuit, \\heartsuit, \\diamondsuit\\} and B = \\{K, Q, J\\}, then:\n\n|A| = 4\n|B| = 3\n|A \\times B| = 12 (all possible combinations)\n\nThe cardinality of a set is denoted by |A| or #A. Here are the calculations:\n\n|\\{apple, orange, watermelon\\}| = 3 (Each element is distinct)\n|\\{1, 1, 1, 1, 1\\}| = 1 (In a set, duplicates are counted only once)\n|[0, 1]| = \\aleph_1 (This is an uncountably infinite interval of real numbers)\n|\\{1, 2, 3, \\cdots\\}| = \\aleph_0 (This is countably infinite)\n|\\{\\emptyset, \\{1\\}, \\{2\\}, \\{1, 2\\}\\}| = 4 (Each element is a distinct set)\n|\\{\\emptyset, \\{1\\}, \\{1, 1\\}, \\{1, 1, 1\\}, \\cdots\\}| = 2 (After removing duplicates: \\{\\emptyset, \\{1\\}\\} since \\{1\\} = \\{1, 1\\} = \\{1, 1, 1\\} = \\cdots)\n\n\n\n\n\nUnderstanding Set Relations: Elements vs Subsets (*)\nThe difference between an element belonging to a set and one set being a subset of another.\n\nThe ‚ÄúBelongs To‚Äù Relationship (\\in)\nWhen we say an element belongs to a set (written as x \\in A), we‚Äôre describing membership of a single item in a collection. Think of a classroom: each individual student belongs to (is a member of) the class. They are elements of the set ‚Äúclass.‚Äù\nConsider a deck of cards and let H be the set of all hearts:\nH = \\{2‚ô•, 3‚ô•, 4‚ô•, 5‚ô•, 6‚ô•, 7‚ô•, 8‚ô•, 9‚ô•, 10‚ô•, J‚ô•, Q‚ô•, K‚ô•, A‚ô•\\}\nWe can say:\n\nA‚ô• \\in H (true, because the ace of hearts is one of the hearts)\nK‚ô† \\notin H (false, because the king of spades is not a heart)\n\\{A‚ô•\\} \\notin H (false, this is a set containing the ace of hearts, not the card itself)\n\n\n\nThe ‚ÄúIs Contained In‚Äù Relationship (\\subseteq)\nA subset relationship (written as A \\subseteq B) describes when one set is entirely contained within another set. Every element of the smaller set must appear in the larger set. This is different from set membership (\\in), which describes when a single element belongs to a set.\nTo understand the distinction, let‚Äôs look at some examples:\nConsider the following sets:\n\nA = \\{1, 2\\}\nB = \\{1, 2, 3, 4\\}\nC = \\{1\\}\n\nFor set membership (\\in):\n\n1 \\in A (the number 1 is an element of set A)\n\\{1\\} \\notin A (the set containing 1 is not an element of A)\n2 \\in B (the number 2 is an element of B)\n\nFor subset relationships (\\subseteq):\n\nA \\subseteq B (all elements of A are in B)\nC \\subseteq A (all elements of C are in A)\n\\{1\\} \\subseteq A (the set containing 1 is a subset of A)\n\nA key insight is that while 1 \\in A is true (1 is an element of A), \\{1\\} \\in A is false (the set containing 1 is not an element of A). However, \\{1\\} \\subseteq A is true (the set containing 1 is a subset of A).\nThink of it this way: membership (\\in) asks ‚ÄúIs this single thing in the set?‚Äù while subset (\\subseteq) asks ‚ÄúIs every element of this smaller set found in the larger set?‚Äù\nAnother helpful example is with the empty set \\emptyset:\n\n\\emptyset \\subseteq A for any set A (the empty set is a subset of every set)\nBut \\emptyset \\notin A unless A specifically contains the empty set as an element\n\nExercise (https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf). Using the given sets:\n\nA = \\{1, 3\\}\nB = \\{3, 1\\}\nC = \\{1, 2\\}\nD = \\{\\emptyset, \\{1\\}, \\{2\\}, \\{1, 2\\}, 1, 2\\}\n\nDetermine whether the following are true or false:\n\n1 \\in A : TRUE (1 is an element of A)\n1 \\subseteq A : FALSE (1 is not a set, so subset relation doesn‚Äôt apply)\n\\{1\\} \\subseteq A : TRUE (every element of the set {1} is an element of A)\n\\{1\\} \\in A : FALSE (A doesn‚Äôt contain any sets as elements)\n3 \\notin C : TRUE (3 is not an element of C)\nA \\in B : FALSE (B doesn‚Äôt contain any sets as elements)\nA \\subseteq B : TRUE (A and B contain the same elements)\nC \\in D : TRUE (the set {1,2} appears in D, but not the set C itself)\nC \\subseteq D : TRUE (all elements of C (1 and 2) are also elements of D)\n\\emptyset \\in D : TRUE (empty set is listed as an element of D)\n\\emptyset \\subseteq D : TRUE (True, by definition, the empty set is a subset of any set. This is because if this were not the case, there would have to be an element of \\emptyset which was not in D. But there are no elements in \\emptyset, so the statement is true.)\nA = B : TRUE (they contain the same elements)\n\\emptyset \\subseteq \\emptyset : TRUE (empty set is a subset of itself; the empty set is a subset of any set)\n\\emptyset \\in \\emptyset : FALSE (empty set contains no elements)",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#set-theory-and-power-sets-event-space",
    "href": "probability_en.html#set-theory-and-power-sets-event-space",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "17.2 Set Theory and Power Sets (Event Space)",
    "text": "17.2 Set Theory and Power Sets (Event Space)\nThe power set of a set, denoted as \\mathcal{F}(S) or 2^{|S|}, is the set of all possible subsets of S, including the empty set and S itself.\nThis concept is crucial in probability theory because it helps us understand the relationship between the sample space S (all possible outcomes) and the event space (all possible events (i.e.¬†all possible subsets of S) we might want to consider).\nLet‚Äôs explore this with a simple example. Consider flipping a single coin where: S = \\{H, T\\} (our sample space)\nThe power set would be:\n\\mathcal{F}(S) = \\{\\emptyset, \\{H\\}, \\{T\\}, \\{H,T\\}\\}\nEach element in the power set represents a possible event. For instance:\n\n\\emptyset: The impossible event (e.g., the coin landing neither heads nor tails)\n\\{H\\}: The event of getting heads\n\\{T\\}: The event of getting tails\n\\{H,T\\}: The certain event (the coin must land either heads or tails)\n\nFor a set with n elements, its power set will have 2^n elements. This is because for each element, we have two choices: include it or not include it in a subset.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#counting-rules-in-probability-the-power-of-and-or",
    "href": "probability_en.html#counting-rules-in-probability-the-power-of-and-or",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "17.3 Counting Rules in Probability: The Power of AND & OR",
    "text": "17.3 Counting Rules in Probability: The Power of AND & OR\nA fundamental challenge in probability is counting possible outcomes. Two key rules help us solve these problems:\n\nThe Multiplication Rule for Independent Events (‚ÄúAND‚Äù Situations)\nWhen we need a sequence of independent choices where we must make ALL choices, we multiply the number of possibilities for each choice. This principle applies when we need option A AND option B AND option C, etc.\nFor example, consider creating a password with exactly three characters in this order:\n\nFirst character must be a letter (26 choices)\nSecond character must be a digit (10 choices)\nThird character must be a symbol (@, #, $, or % - so 4 choices)\n\nTotal possible passwords = 26 √ó 10 √ó 4 = 1,040\nThis is like filling three slots where each slot has its own set of valid options. Each new requirement multiplies our total possibilities.\n\n\nThe Addition Rule for Mutually Exclusive Events (‚ÄúOR‚Äù Situations)\nWhen there are multiple valid ways to achieve a goal, and we can use ANY ONE of these ways, we add the number of possibilities. This applies when we accept option A OR option B OR option C, etc.\nFor example, if a password must be EITHER:\n\nA 3-letter word (26¬≥ possibilities) OR\nA 4-digit number (10‚Å¥ possibilities)\n\nTotal possibilities = 26¬≥ + 10‚Å¥ = 17,576 + 10,000 = 27,576\nThink of this as having separate paths to success - we count how many ways each path offers and sum them up.\n\n\nCombining the Rules\nMany real problems require both multiplication and addition.\nExample 1. For instance, if a password must be EITHER:\n\nA letter followed by two digits (26 √ó 10 √ó 10 possibilities) OR\nThree symbols (4 √ó 4 √ó 4 possibilities)\n\nTotal = (26 √ó 10 √ó 10) + (4 √ó 4 √ó 4) = 2,600 + 64 = 2,664\nUnderstanding when to multiply (AND situations) versus when to add (OR situations) is key to solving counting problems correctly.\nExample 2. Calvin wants to reach Milwaukee and has these options:\n\nFirst leg (home ‚Üí Chicago): 3 bus services OR 2 train services\nSecond leg (Chicago ‚Üí Milwaukee): 2 bus services OR 3 train services\n\nTo solve this:\n\nFirst leg options = 3 + 2 = 5 ways\nSecond leg options = 2 + 3 = 5 ways\nTotal routes = 5 √ó 5 = 25 possibilities\n\nWhy multiply at the end? Because for EACH way of reaching Chicago, Calvin can use ANY of the ways to reach Milwaukee. This creates 25 unique combinations like:\n\nBus 1 ‚Üí Bus 1\nBus 1 ‚Üí Train 1\nBus 2 ‚Üí Bus 2 ‚Ä¶and so on.\n\nThe key is recognizing whether you‚Äôre dealing with sequential choices (multiply) or alternative options (add) at each step. Master this distinction, and you‚Äôll solve complex counting problems with ease.\n\n\n\n\n\nflowchart TD\n    Start[Problem: Count Possible Outcomes] --&gt; Q1{\"Are we counting outcomes\\nthat happen in sequence?\"}\n    \n    Q1 --&gt;|Yes| M1[Multiplication Rule:\\nMultiply choices for each step]\n    Q1 --&gt;|No| Q2{\"Are we counting different\\nways to achieve same result?\"}\n    \n    M1 --&gt; ME1[Examples of Sequential Choices]\n    ME1 --&gt; MC1[\"Password: letter then number\\n26 letters √ó 10 numbers\\n= 260 possibilities\"]\n    MC1 --&gt; MC2[\"Travel: bus then train\\n3 bus routes √ó 2 train routes\\n= 6 possible journeys\"]\n    \n    Q2 --&gt;|Yes| Q3{\"Do options overlap?\"}\n    \n    Q3 --&gt;|No| A1[Simple Addition Rule:\\nAdd all possibilities]\n    Q3 --&gt;|Yes| A2[\"Extended Addition Rule:\\nAdd - Overlap\"]\n    \n    A1 --&gt; AE1[Examples of Non-Overlapping Options]\n    AE1 --&gt; AC1[\"Coin toss: H or T\\n1 + 1 = 2 outcomes\"]\n    AC1 --&gt; AC2[\"License type: Car or Motorcycle\\n100 + 50 = 150 types\"]\n    \n    A2 --&gt; AE2[Examples of Overlapping Options]\n    AE2 --&gt; AC3[\"Students in Sports or Music:\\n45 + 35 - 15 in both\\n= 65 students\"]\n    \n    classDef start fill:#2d5a8c,stroke:#333,color:#fff,stroke-width:2px\n    classDef question fill:#d4426e,stroke:#333,color:#fff,stroke-width:2px\n    classDef rule fill:#156b45,stroke:#333,color:#fff,stroke-width:2px\n    classDef example fill:#4a4a4a,stroke:#333,color:#fff\n    \n    class Start start\n    class Q1,Q2,Q3 question\n    class M1,A1,A2 rule\n    class ME1,AE1,AE2,MC1,MC2,AC1,AC2,AC3 example\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n    subgraph Addition_Rules[Addition Rule Examples]\n        direction TB\n        subgraph Exclusive[Mutually Exclusive Example]\n            direction TB\n            A1[\"Coin Flip\"] --&gt; AH((Heads))\n            A1 --&gt; AT((Tails))\n            AT --&gt; AR[\"Total = 1 + 1 = 2\\nNo overlap possible\"]\n            AH --&gt; AR\n        end\n        \n        subgraph Overlapping[Overlapping Sets Example]\n            direction TB\n            O1[\"Students in\\nClubs\"] --&gt; OS[\"Science Club\\n25 students\"] & OA[\"Art Club\\n20 students\"]\n            OS & OA --&gt; OI[\"Both Clubs\\n8 students\"]\n            OI --&gt; OT[\"Total = 25 + 20 - 8\\n= 37 students\"]\n        end\n    end\n    \n    classDef default fill:#f5f5f5,stroke:#333,color:#000\n    classDef set fill:#e6e6e6,stroke:#333,color:#000\n    classDef result fill:#d9d9d9,stroke:#333,color:#000\n    classDef option fill:#ffffff,stroke:#333,color:#000\n    classDef example fill:#f0f0f0,stroke:#333,color:#000\n    \n    class A1,O1 set\n    class AR,OT result\n    class AH,AT option\n    class Exclusive,Overlapping example\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n    subgraph Multiplication_Rule[Multiplication Rule Examples]\n        direction TB\n        M1[\"Choose Breakfast\"] --&gt; MA[\"Drink\\n(3 options)\"] & MB[\"Food\\n(2 options)\"]\n        MA --&gt; MA1((Coffee)) & MA2((Tea)) & MA3((Juice))\n        MB --&gt; MB1((Toast)) & MB2((Cereal))\n        MA1 & MA2 & MA3 --&gt; MR[\"Total Combinations:\\n3 drinks √ó 2 foods\\n= 6 possible breakfasts\"]\n        MB1 & MB2 --&gt; MR\n        \n        subgraph Tree_Example[Tree Diagram]\n            direction TB\n            T1[\"PIN First Digit\\n(0-9)\"] --&gt; T2[\"Second Digit\\n(0-9)\"]\n            T2 --&gt; T3[\"10 √ó 10 = 100\\ntotal combinations\"]\n        end\n    end\n    \n    classDef default fill:#f5f5f5,stroke:#333,color:#000\n    classDef set fill:#e6e6e6,stroke:#333,color:#000\n    classDef result fill:#d9d9d9,stroke:#333,color:#000\n    classDef option fill:#ffffff,stroke:#333,color:#000\n    classDef example fill:#f0f0f0,stroke:#333,color:#000\n    \n    class MA,MB set\n    class MR,T3 result\n    class MA1,MA2,MA3,MB1,MB2 option\n    class Tree_Example example\n\n\n\n\n\n\n\n\n\n\n\n\nThe Inclusion-Exclusion Principle (*)\n\n\n\nThe Inclusion-Exclusion Principle states that for two sets A and B:\n|A \\cup B| = |A| + |B| - |A \\cap B|\nThis means: The size of their union equals the sum of their individual sizes, minus their intersection (to avoid double counting shared elements).\nFor three sets A, B, and C, the principle extends to:\n|A \\cup B \\cup C| = |A| + |B| + |C| - |A \\cap B| - |B \\cap C| - |A \\cap C| + |A \\cap B \\cap C|\nThis pattern continues for more sets, alternating between adding and subtracting intersections of increasing size.\nA simple example:\n\nSet A: Students who play soccer (20 students)\nSet B: Students who play basketball (15 students)\n8 students play both sports\nTotal students in either sport = 20 + 15 - 8 = 27 students\n\nThe principle is essential in probability theory, combinatorics, and set theory. It helps us correctly count elements when sets overlap, avoiding the common error of double-counting shared elements.\n\n\n\nhttps://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#probability-theory-basic-concepts-and-rules",
    "href": "probability_en.html#probability-theory-basic-concepts-and-rules",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "17.4 Probability Theory: Basic Concepts and Rules",
    "text": "17.4 Probability Theory: Basic Concepts and Rules\n\n\n\n\n\n\nCore Concepts\n\n\n\nProbability theory provides a rigorous foundation for quantifying uncertainty and analyzing random phenomena.\n\n\n\nRandom Experiments\nA random experiment is any procedure that has a well-defined set of possible outcomes but whose specific result cannot be predicted with certainty.\nProperties:\n\nRepeatable under identical conditions\nKnown possible outcomes\nUnpredictable specific results\n\n\n\nSample Space (S)\n\nComplete set of all possible outcomes of a random experiment\nDenoted by S (or \\Omega)\nProperties:\n\nMutually exclusive outcomes\nCollectively exhaustive\n\n\nExamples:\n\nCoin flip: S = \\{H, T\\}\nDie roll: S = \\{1, 2, 3, 4, 5, 6\\}\n\n\n\nThe Event Space: What Can Happen in an Experiment\nEvents are subsets of the sample space S. This means we can use standard set operations to work with them in a precise, mathematical way.\nThe event space \\mathcal{F} is a collection of all events (outcomes or sets of outcomes) that we can assign probabilities to in an experiment. It must follow three fundamental rules:\n\nComplete Space Rule\n\nThe entire sample space S must be in \\mathcal{F}\nThis means all possible outcomes together form a valid event\n\nComplement Rule\n\nIf event A is in \\mathcal{F}, then ‚Äúnot A‚Äù (written as A^c) must also be in \\mathcal{F}\nExample: If ‚Äúgetting heads‚Äù is an event, ‚Äúnot getting heads‚Äù must also be an event\n\nUnion Rule\n\nIf we have any sequence of events A_1, A_2, ... in \\mathcal{F}, their union must also be in \\mathcal{F}\nThis means we can combine valid events to form new valid events\n\n\n\n\n\n\n\n\nDiscrete vs.¬†Continuous Probability: Understanding the Two Types of Random Events\n\n\n\nIn probability, we encounter two fundamentally different types of random events: those we can count (discrete) and those we can measure (continuous). This distinction shapes how we calculate and interpret probabilities.\n\nDiscrete Probability\nWhat: Events that can be counted with whole numbers - Like counting marbles, rolling dice, or flipping coins - Has ‚Äúgaps‚Äù between possible values\nKey Examples:\n\nRolling a die\n\nPossible outcomes: 1, 2, 3, 4, 5, or 6\nNothing in between (can‚Äôt roll a 2.5)\nCan say: P(\\text{rolling a 6}) = \\frac{1}{6}\n\nNumber of customers per hour\n\nCould be 0, 1, 2, 3, ‚Ä¶\nCan‚Äôt have 2.7 customers\nCan say: P(\\text{exactly 5 customers}) = 0.1\n\n\n\n\nContinuous Probability\nWhat: Events measured on a continuous scale - Like measuring height, time, or temperature - Values flow smoothly with no gaps\nKey Examples:\n\nPerson‚Äôs height\n\nCould be 170cm, 170.1cm, 170.11cm, ‚Ä¶\nCan measure with increasing precision\nMust use ranges: P(170 \\leq \\text{height} \\leq 171)\n\nTime until next bus arrives\n\nCould be 5 mins, 5.1 mins, 5.01 mins, ‚Ä¶\nInfinitely divisible\nMust use ranges: P(\\text{waiting time} \\leq 10 \\text{ mins})\n\n\n\n\nCritical Differences\n\nIndividual Values\n\nDiscrete: Can have positive probability\n\nP(\\text{rolling a 6}) = \\frac{1}{6} &gt; 0\n\nContinuous: Always have zero probability\n\nP(\\text{height} = 170.000...) = 0\n\n\nHow We Calculate\n\nDiscrete: Can sum individual probabilities\nContinuous: Must use ranges and integrals\n\n\n\n\nReal-World Application\nThink about a pizza delivery:\n\nDiscrete: Number of toppings (1, 2, 3, ‚Ä¶)\nContinuous: Delivery time (15.7 minutes, 15.73 minutes, ‚Ä¶)\n\n\n\nWhy Understanding This Matters\n\nHelps choose appropriate probability tools\nGuides how we collect and analyze data\nDetermines how we express uncertainty\nShapes how we make predictions\n\nThis foundation helps us tackle real-world probability problems with the right approach!\n\n\n\nLet‚Äôs examine a simple coin flip:\n\nSample space: S = \\{H, T\\} (Heads or Tails)\nThe complete event space: \\mathcal{F} = \\{\\emptyset, \\{H\\}, \\{T\\}, \\{H,T\\}\\}\n\n\\emptyset : impossible event (no outcomes)\n\\{H\\} : getting Heads\n\\{T\\} : getting Tails\n\\{H,T\\} : getting either Heads or Tails\n\n\nVerifying the rules:\n\nRule 1: \\{H,T\\} (the sample space) is included\nRule 2: For event \\{H\\}, its complement \\{T\\} is included\nRule 3: The union of any events (like \\{H\\} \\cup \\{T\\} = \\{H,T\\}) is included\n\n\n\n\n\n\n\nUnderstanding Outcomes vs Events\n\n\n\nThere‚Äôs an important distinction between outcomes (also called simple events) and events:\n\nAn outcome or simple event is a single, indivisible result of an experiment. For example, getting heads on a single coin flip is an outcome.\nAn event is a set of outcomes - it can contain one outcome, multiple outcomes, or even no outcomes (the empty set). For example, ‚Äúgetting at least one head when flipping two coins‚Äù is an event containing multiple outcomes.\n\nLet‚Äôs illustrate this with two coin flips where:\nS = \\{HH, HT, TH, TT\\} (our sample space)\nThe power set (all possible events) would contain 2^4 = 16 events:\n\n\\emptyset (impossible event)\nSingle outcomes: \\{HH\\}, \\{HT\\}, \\{TH\\}, \\{TT\\}\nPairs of outcomes: \\{HH,HT\\}, \\{HH,TH\\}, \\{HH,TT\\}, \\{HT,TH\\}, \\{HT,TT\\}, \\{TH,TT\\}\nTriples: \\{HH,HT,TH\\}, \\{HH,HT,TT\\}, \\{HH,TH,TT\\}, \\{HT,TH,TT\\}\nComplete sample space: \\{HH,HT,TH,TT\\}\n\n\n\n\n\n\n\n\n\nEvent Types\n\n\n\n\nSimple Events: Single outcomes\nCompound Events: Multiple outcomes\nSure (or Certain) Event: Sample space S\nImpossible Event: Empty set \\emptyset\n\n\n\n\n\nProbability Measure and Probability Axioms: Assigning Numbers to Events\nA probability measure P is a way to quantify how likely events are to occur. It takes any event from our event space \\mathcal{F} and assigns it a number between 0 and 1.\nThis assignment follows three essential rules/axioms. These axioms, introduced by Andrey Kolmogorov in 1933, serve as the foundation for all probability calculations:\n\nNon-Negativity Rule\n\nFor any event A, its probability must be at least 0: P(A) \\geq 0\nWe can never have a negative probability\nExample: If we roll a die, the probability of getting a 6 is \\frac{1}{6} (it cannot be negative)\n\nTotal Probability Rule\n\nThe probability of all possible outcomes must equal 1: P(S) = 1\nSomething must happen - the probabilities of all possibilities add up to 100%\nExample: For a fair coin, P(\\text{heads}) + P(\\text{tails}) = 0.5 + 0.5 = 1\n\nAddition Rule for Non-Overlapping Events\n\nIf events cannot happen together (they‚Äôre ‚Äúdisjoint‚Äù), the probability of their union equals the sum of their individual probabilities\nWritten formally: P(A_1 \\cup A_2 \\cup ...) = P(A_1) + P(A_2) + ...\nExample: In drawing a card, P(\\text{getting ace}) = P(\\text{ace of hearts}) + P(\\text{ace of diamonds}) + P(\\text{ace of clubs}) + P(\\text{ace of spades})\n\n\nThese rules/axioms ensure that our probability assignments make logical sense and match our intuitive understanding of chance and likelihood.\n\n\n\n\n\n\nImportant Terms in Probability Theory\n\n\n\nLet‚Äôs clarify some closely related but distinct concepts:\n\nProbabilistic Model consists of two fundamental elements:\n\nA sample space \\Omega (or S): the set of all possible outcomes\nA probability law that assigns probabilities to events (subsets of \\Omega)\n\nProbability Measure (P):\n\nThe formal mathematical function that maps events to numbers in [0,1]\nMust satisfy the three axioms (non-negativity, normalization, additivity)\nExample: P(A) gives the probability of event A occurring\n\nProbability Distribution:\n\nThe specific assignment of probabilities to all possible outcomes\nDescribes how probability is distributed across the sample space\nExample: For a fair die, {1: 1/6, 2: 1/6, 3: 1/6, 4: 1/6, 5: 1/6, 6: 1/6}\n\nProbability Law:\n\nOften used as a synonym for probability distribution\nCan also refer to the underlying rule generating the probabilities\nExample: ‚ÄúEach face of a fair die has equal probability‚Äù\n\n\nIn practice, these terms are interrelated: The probability measure implements the probability law, which determines the probability distribution, all within the context of a probabilistic model.\n\n\n\n\n\n\n\n\nVisualizing Sample Spaces\n\n\n\nIn probability theory and statistics, being able to visualize sample spaces is crucial for understanding possible outcomes and their relationships. We‚Äôll explore three main approaches to visualizing sample spaces:\n\nVenn Diagrams\nTree Diagrams\nGrid/Matrix Diagrams\n\n\nVenn Diagrams\nVenn diagrams provide a powerful visual tool for understanding sample spaces.\n\nA Venn diagram is a graphical representation of sets and their relationships using (overlapping or disjoint) circles or other shapes.\nThink of each shape in a Venn diagram as a container that holds items with specific characteristics. Where these shapes overlap, we find items that share characteristics of multiple groups.\n\nIn probability theory, our sample space (usually denoted by Œ© or S) represents all possible outcomes of an experiment. When we draw a Venn diagram, the rectangular frame represents this entire sample space (a universal set), with a probability of 1. Any event is then a subset of this space.\n\n\n\nTree Diagrams\nTree diagrams are particularly useful for visualizing sequential events and their outcomes. Here‚Äôs a tree diagram showing a simple probability experiment: We toss a fair coin twice.\n\n\n\n\n\ngraph LR\n    Start[Start] --&gt; H1[H]\n    Start --&gt; T1[T]\n    H1 --&gt; H2[H]\n    H1 --&gt; T2[T]\n    T1 --&gt; H3[H]\n    T1 --&gt; T3[T]\n    \n    H2 --&gt; HH([HH: 1/4])\n    T2 --&gt; HT([HT: 1/4])\n    H3 --&gt; TH([TH: 1/4])\n    T3 --&gt; TT([TT: 1/4])\n    \n    linkStyle 0,2,3 stroke:#1e88e5,stroke-width:2px\n    linkStyle 4,5 stroke:#ff5252,stroke-width:2px\n    linkStyle 1 stroke:#ff5252,stroke-width:2px\n    \n    style Start fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style H1 fill:#bbdefb,stroke:#1e88e5,stroke-width:2px\n    style T1 fill:#ffcdd2,stroke:#ff5252,stroke-width:2px\n    style H2 fill:#bbdefb,stroke:#1e88e5,stroke-width:2px\n    style T2 fill:#ffcdd2,stroke:#ff5252,stroke-width:2px\n    style H3 fill:#bbdefb,stroke:#1e88e5,stroke-width:2px\n    style T3 fill:#ffcdd2,stroke:#ff5252,stroke-width:2px\n    \n    style HH fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style HT fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style TH fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style TT fill:#f5f5f5,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\n\nGrid/Matrix Diagrams\nGrid diagrams are excellent for showing combinations of events.\nScenario: We have 7 balls in the bag:\n\n4 red balls (R‚ÇÅ, R‚ÇÇ, R‚ÇÉ, R‚ÇÑ)\n3 black balls (B‚ÇÅ, B‚ÇÇ, B‚ÇÉ)\nWe‚Äôll draw 2 balls without replacement\n\nLet‚Äôs visualize the entire sample space using a grid where each cell represents selecting two balls in order (first draw ‚Üí columns, second draw ‚Üì rows):\n\n\n\nFirst Draw ‚Üí\nR‚ÇÅ\nR‚ÇÇ\nR‚ÇÉ\nR‚ÇÑ\nB‚ÇÅ\nB‚ÇÇ\nB‚ÇÉ\n\n\n\n\nSecond Draw ‚Üì\n\n\n\n\n\n\n\n\n\nR‚ÇÅ\nX\n‚ö´\n‚ö´\n‚ö´\n‚ö™\n‚ö™\n‚ö™\n\n\nR‚ÇÇ\n‚ö´\nX\n‚ö´\n‚ö´\n‚ö™\n‚ö™\n‚ö™\n\n\nR‚ÇÉ\n‚ö´\n‚ö´\nX\n‚ö´\n‚ö™\n‚ö™\n‚ö™\n\n\nR‚ÇÑ\n‚ö´\n‚ö´\n‚ö´\nX\n‚ö™\n‚ö™\n‚ö™\n\n\nB‚ÇÅ\n‚ö™\n‚ö™\n‚ö™\n‚ö™\nX\n‚ö´\n‚ö´\n\n\nB‚ÇÇ\n‚ö™\n‚ö™\n‚ö™\n‚ö™\n‚ö´\nX\n‚ö´\n\n\nB‚ÇÉ\n‚ö™\n‚ö™\n‚ö™\n‚ö™\n‚ö´\n‚ö´\nX\n\n\n\nWhere:\n\nX: Impossible (same ball twice)\n‚ö´: Both same color (both red in upper-left, both black in lower-right)\n‚ö™: Different colors (red-black or black-red)\n\nFrom this grid:\n\nBoth red = 12 outcomes (‚ö´ in upper-left quadrant)\nBoth black = 6 outcomes (‚ö´ in lower-right quadrant)\nRed then black = 12 outcomes (‚ö™ in lower-left quadrant)\nBlack then red = 12 outcomes (‚ö™ in upper-right quadrant)\nTotal possible outcomes = 42 (remove seven diagonal X‚Äôs from 7 √ó 7 grid)\n\nTotal count verification: 12 + 6 + 12 + 12 = 42 outcomes\nNote: Each outcome is determined by reading first draw (column) then second draw (row).\n\nGrid/Matrix Diagrams with Unordered Pairs\nFor this modified scenario where order doesn‚Äôt matter, we need to adjust our counting since (R‚ÇÅ,B‚ÇÅ) and (B‚ÇÅ,R‚ÇÅ) would be considered the same outcome.\nScenario: We have 7 balls in the bag:\n\n4 red balls (R‚ÇÅ, R‚ÇÇ, R‚ÇÉ, R‚ÇÑ)\n3 black balls (B‚ÇÅ, B‚ÇÇ, B‚ÇÉ)\nWe‚Äôll draw 2 balls without replacement\nOrder does NOT matter\n\nBecause order doesn‚Äôt matter, we only need to look at half of the grid, excluding the diagonal.\n\n\n\nFirst Draw ‚Üí\nR‚ÇÅ\nR‚ÇÇ\nR‚ÇÉ\nR‚ÇÑ\nB‚ÇÅ\nB‚ÇÇ\nB‚ÇÉ\n\n\n\n\nSecond Draw ‚Üì\n\n\n\n\n\n\n\n\n\nR‚ÇÅ\nX\n‚ö´\n‚ö´\n‚ö´\n‚ö™\n‚ö™\n‚ö™\n\n\nR‚ÇÇ\n‚Äì\nX\n‚ö´\n‚ö´\n‚ö™\n‚ö™\n‚ö™\n\n\nR‚ÇÉ\n‚Äì\n‚Äì\nX\n‚ö´\n‚ö™\n‚ö™\n‚ö™\n\n\nR‚ÇÑ\n‚Äì\n‚Äì\n‚Äì\nX\n‚ö™\n‚ö™\n‚ö™\n\n\nB‚ÇÅ\n‚Äì\n‚Äì\n‚Äì\n‚Äì\nX\n‚ö´\n‚ö´\n\n\nB‚ÇÇ\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\nX\n‚ö´\n\n\nB‚ÇÉ\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\nX\n\n\n\nWhere:\n\nX: Impossible (same ball twice)\n‚ö´: Both same color\n‚ö™: Different colors\n‚Äì: Redundant (already counted in upper half)\n\nFrom this grid:\n\nBoth red = 6 outcomes (‚ö´ in upper-left quadrant)\nBoth black = 3 outcomes (‚ö´ in lower-right quadrant)\nOne red and one black = 12 outcomes (‚ö™ only counted once)\nTotal possible outcomes = 21 (half of the ordered outcomes: 42 √∑ 2)\n\nTotal count verification: 6 + 3 + 12 = 21 unordered outcomes\nNote: Each unordered pair {R‚ÇÅ,B‚ÇÅ} is counted only once, whereas in the ordered scenario we counted both (R‚ÇÅ,B‚ÇÅ) and (B‚ÇÅ,R‚ÇÅ).\n\n\n\n\n\n\nDiscrete Sample Spaces & Probability\nWhen we analyze random events like coin flips or dice rolls, we need a way to list all possible outcomes. This is where discrete sample spaces come in. Let‚Äôs break this down step by step:\n\nDiscrete Sample Spaces (S or \\Omega)\nThink of a sample space as a container holding all possible outcomes of a random event/experiment. In discrete sample spaces, we can count these outcomes one by one, like counting marbles in a bag. We write it as:\n\nFor finite events: S = \\{s_1, s_2, ..., s_n\\} [classical (‚Äònaive‚Äô) probability]\nFor infinite but countable events: S = \\{s_1, s_2, ...\\}\n\n\n\nThree Essential Rules\n\nMust include everything possible (no missing outcomes)\nNo overlap between outcomes (each is unique)\nMust be countable (you can list them out)\n\n\n\nExamples\n1. Equally Likely Outcomes (uniform probability distribution)\nThese are scenarios where each basic outcome has the same probability:\n\nFair die: S = \\{1, 2, 3, 4, 5, 6\\}, each with probability \\frac{1}{6}\nFair deck: S = \\{\\text{52 cards}\\}, each with probability \\frac{1}{52}\nFair coin: S = \\{\\text{H}, \\text{T}\\}, each with probability \\frac{1}{2}\n\n2. Events with Different Probabilities\nSome (discrete) cases:\nBinomial Scenarios (Counting Successes)\n\nExample: Number of successes in n trials, each with probability p\nSample Space: S = \\{0, 1, 2, ..., n\\}\nOutcomes have different probabilities based on:\n\nNumber of ways to get k successes (\\binom{n}{k})\nProbability of each arrangement (p^k(1-p)^{n-k})\n\n\nGeometric Scenarios (Waiting for Success)\n\nExample: Number of trials until first success, probability p per trial\nSample Space: S = \\{1, 2, 3, ...\\}\nProbability decreases with number of trials\nP(\\text{first success on trial }n) = (1-p)^{n-1}p\n\nNote: The term ‚Äúsuccess‚Äù in probability simply means the outcome we‚Äôre tracking - it could be any event of interest. Each trial has two possible outcomes: success (probability p) or failure (probability 1-p).\n\n\nHow to Assign Probabilities\n1. Equal Chances (Classical Probability) When all outcomes are equally likely:\nP(\\text{one outcome}) = \\frac{1}{\\text{total outcomes}}\nExample: Rolling a fair die\n\nProbability of rolling a 4 = \\frac{1}{6}\n\n2. Mathematical Models (Probability Distribution Functions) For more complex situations, we use specific formulas (functions):\n\nMultiple trials (Binomial): P(X=k) = \\binom{n}{k}p^k(1-p)^{n-k}\nFirst success (Geometric): P(X=k) = p(1-p)^{k-1}\n\n3. Using Data (Empirical/Experimental Probability) When we have actual observations:\nP(\\text{outcome}) = \\frac{\\text{times outcome occurred}}{\\text{total observations}}\nExample: If you flip a coin 100 times and get 53 heads: P(\\text{heads}) = \\frac{53}{100} = 0.53\n\n\n\n\n\n\nTip\n\n\n\nClassical (‚ÄúNaive‚Äù) Probability: Why Single Outcomes Have Equal Probabilities\nLet‚Äôs prove this step by step:\n\nStart with n equally likely outcomes: s_1, s_2, ..., s_n\nWe know the total probability must be 1 (rule 2 above)\nCall the probability of each outcome p\nSince outcomes are equally likely, each has the same probability p\nAdding up all outcomes: p + p + ... + p (n times) = 1\nTherefore: np = 1\nSolving for p: p = \\frac{1}{n}\n\nThis gives us the following rule under the assumption of equally likely outcomes:\nP(\\text{single outcome}) = \\frac{1}{\\text{number of possible outcomes}}\n\n\nImportant Considerations\n\nClassical probability (equal likelihood) is a special case\nMany real-world phenomena follow specific probability distributions (a probability distribution is the mathematical function that gives the probabilities of occurrence of possible outcomes for an experiment.)\nThe type of probability assignment depends on the context:\n\nPhysical symmetry ‚Üí Classical probability\nRepeated independent trials ‚Üí Binomial distribution\nRare events ‚Üí Poisson distribution\nWaiting times ‚Üí Geometric distribution\n(‚Ä¶)\n\nAll these cases work within discrete sample spaces\nThe probabilities must always sum to 1 over the entire sample space\n\nThis framework helps us systematically study discrete random variables and their probabilities, whether they follow uniform or non-uniform distributions.\n\n\n\n\n\n\nWarning\n\n\n\nThere is no single, universal formula for calculating probabilities across all probability spaces and situations.\nDifferent Types of Probability Spaces\n\nClassical (Finite, Equally Likely)\n\nOnly here we can use: P(A) = \\frac{\\text{favorable outcomes}}{\\text{total outcomes}}\nLimited to finite, equally likely cases\n\nGeneral Discrete\n\nMust specify individual probabilities\nExample: Loaded die needs experimental/empirical determination\nSum of probabilities must equal 1\n\nContinuous\n\nUses calculus and density functions\nProbabilities found by integration\nExample: P(a \\leq X \\leq b) = \\int_a^b f(x)dx\nNo universal formula for density function\n\nMixed/Hybrid\n\nCombines discrete and continuous elements\nDifferent methods needed for different parts\n\n\nWhy No Universal Formula?\n\nDifferent types of randomness need different mathematical tools\nNature of outcomes (discrete/continuous) affects calculation method\nPrior knowledge or assumptions shape probability calculation\nSome probabilities must be found empirically (frequentist/statistical probability) rather than calculated\n\n\n\n\n\n\n\n\n\nFrequentist/Statistical/Empirical Probability & the Law of Large Numbers\n\n\n\nFrequentist probability defines probability as the long-term relative frequency of an event‚Äôs occurrence in repeated trials under identical conditions:\nP(A) = \\lim_{n \\to \\infty} \\frac{\\text{number of times A occurs}}{n}\nThe Law of Large Numbers states that as we increase the number of trials, the observed frequency converges to the true probability:\n\nIf true probability of heads is 0.5\nIn 10 flips: might get 7 heads (frequency = 0.7)\nIn 1000 flips: might get 495 heads (frequency ‚âà 0.495)\nAs trials ‚Üí ‚àû, frequency ‚Üí 0.5\n\nKey Characteristics:\n\nRequires repeatable experiments under identical conditions\nObjective approach - probability viewed as physical property\nCannot handle one-time events\nFoundation for classical statistical inference\n\nLimitation: We can never perform infinite trials, so we estimate probabilities from large but finite samples.\n\n\n\n\n\n\n\n\nClassical Probability: Equal-Likelihood in Discrete Sample Spaces\n\n\n\nClassical probability, also known as Laplace probability, applies when all outcomes in a finite sample space are equally likely to occur. This framework, developed by Pierre-Simon Laplace, provides a simple yet powerful way to calculate probabilities when symmetry exists.\n\nMathematical Foundation\nFor any event A in sample space S, the classical probability is calculated as:\nP(A) = \\frac{\\text{number of favorable outcomes}}{\\text{number of possible outcomes}} = \\frac{|A|}{|S|}\nThis definition automatically satisfies Kolmogorov‚Äôs probability axioms:\n\nNon-negativity: P(A) \\geq 0 for all A \\subseteq S\nNormalization: P(S) = \\frac{|S|}{|S|} = 1\nAdditivity: For disjoint events A and B, P(A \\cup B) = P(A) + P(B)\n\n\n\nKey Requirements\nTwo essential conditions must be met:\n\nThe sample space S must be finite\nEach elementary outcome s \\in S must be equally likely, with probability P(\\{s\\}) = \\frac{1}{|S|}\n\n\n\nCommon Applications\n\nFair Dice\n\nRolling a six: P(6) = \\frac{1}{6}\nRolling an even number: P(2,4,6) = \\frac{3}{6} = \\frac{1}{2}\nRolling a number greater than 4: P(5,6) = \\frac{2}{6} = \\frac{1}{3}\n\nPlaying Cards\n\nDrawing a heart: P(‚ô•) = \\frac{13}{52} = \\frac{1}{4}\nDrawing a face card: P(J,Q,K) = \\frac{12}{52} = \\frac{3}{13}\nDrawing a red ace: P(\\text{A‚ô•,A‚ô¶}) = \\frac{2}{52} = \\frac{1}{26}\n\nMultiple Coin Flips\n\nTwo fair coins: S = \\{HH, HT, TH, TT\\}\nP(\\text{exactly one head}) = \\frac{|\\{HT,TH\\}|}{|\\{HH,HT,TH,TT\\}|} = \\frac{2}{4} = \\frac{1}{2}\nP(\\text{at least one head}) = \\frac{|\\{HH,HT,TH\\}|}{|\\{HH,HT,TH,TT\\}|} = \\frac{3}{4}\n\n\n\n\nLimitations and Considerations\n\nEqually Likely Assumption\n\nThis framework fails for biased coins, loaded dice, or any scenario where outcomes aren‚Äôt equally likely\nIn such cases, we need empirical probability or other probability measures\n\nFinite Space Requirement\n\nCannot directly apply to infinite sample spaces\nRequires modification for continuous probability spaces\n\nSymmetry Assessment\n\nPhysical symmetry (as in fair dice) often suggests equal likelihood\nBut physical symmetry alone doesn‚Äôt guarantee equal probabilities in practice\n\n\n\n\nConnection to Other Probability Concepts\nClassical probability serves as a foundation for understanding more complex probability concepts:\n\nForms the basis for combinatorial probability\nProvides intuition for uniform distributions\nHelps in understanding probability density functions in continuous spaces\n\nNote: While classical probability is intuitive and mathematically elegant, real-world applications often require more general probability frameworks to handle non-uniform probabilities and infinite sample spaces.\n\n\n\n\n\n\nClassical (or ‚ÄòNaive‚Äô) Probability: The Equal-Likelihood Special Case in Discrete Sample Spaces\nClassical probability applies to finite sample spaces where all outcomes are equally likely to occur. The probability formula is:\nP(\\text{event}) = \\frac{\\text{number of favorable outcomes}}{\\text{total number of possible outcomes}}\nRequirements:\n\nFinite sample space (finite number of outcomes)\nAll outcomes equally likely\nTotal probability sums to 1\n\nExamples:\n\nFair die roll:\n\nP(\\text{rolling a 3}) = \\frac{1}{6}\nP(\\text{rolling an even number}) = \\frac{3}{6} = \\frac{1}{2}\n\nDrawing from a standard deck:\n\nP(\\text{drawing an ace}) = \\frac{4}{52} = \\frac{1}{13}\nP(\\text{drawing a heart}) = \\frac{13}{52} = \\frac{1}{4}\n\n\nKey Limitation:\nClassical probability fails when outcomes are not equally likely (e.g., loaded die) or when the sample space is infinite.\nThis is why it‚Äôs called ‚Äúnaive‚Äù probability - it assumes a simple, idealized situation where simple counting is sufficient.\n\nStarting Assumptions\nWhen developing classical probability theory, we begin with a probability experiment that has two key properties:\n\nThe sample space S is finite, with cardinality |S| = n\nAll elementary outcomes are equally likely\n\nWe can write our sample space explicitly as: S = \\{s_1, s_2, ..., s_n\\}\nThis leads us to classical (or Laplace) probability, which we‚Äôll derive rigorously in the following section. This special case provides a foundation for understanding more complex probability scenarios and helps build crucial probabilistic intuition.\n\n\nCore Axioms of Probability Theory\nTo derive classical probability, we start with the aforementioned key probability axioms. These form the mathematical foundation for all probability theory, including the classical or ‚Äònaive‚Äô probability:\n\nFor any event A, P(A) \\geq 0 (Non-negativity)\nP(S) = 1 (Total probability)\nFor disjoint events A and B, P(A \\cup B) = P(A) + P(B) (Additivity)\n\n\n\nDeriving the Classical (‚ÄòNaive‚Äô) Probability Formula (Equal-Likelihood Case)\n\nStarting Point\nIn a fair game or unbiased experiment where all outcomes are equally likely, we can derive the famous ‚Äúnumber of favorable outcomes divided by total outcomes‚Äù formula.\n\n\nThe Setup\nConsider a finite sample space S with n outcomes: \\{s_1, s_2, ..., s_n\\}\nEqual-Likelihood Assumption:\n\nEach outcome has the same probability p\nMathematically: P(\\{s_1\\}) = P(\\{s_2\\}) = ... = P(\\{s_n\\}) = p\n\n\n\nStep-by-Step Derivation\n\nUse Total Probability Axiom\n\nAll probabilities must sum to 1\nFor our n equally likely outcomes:\n\nP(S) = P(\\{s_1\\}) + P(\\{s_2\\}) + ... + P(\\{s_n\\}) = 1 \\underbrace{p + p + ... + p}_{n \\text{ terms}} = 1 np = 1 p = \\frac{1}{n} = \\frac{1}{|S|}\nCalculate Probability of Any Event A\n\nLet event A contain k outcomes\nBy the addition rule for disjoint events: P(A) = \\underbrace{p + p + ... + p}_{k \\text{ terms}} P(A) = k \\cdot \\frac{1}{|S|} = \\frac{k}{|S|} = \\frac{|A|}{|S|}\n\nThe Classical Probability Formula P(A) = \\frac{|A|}{|S|} = \\frac{\\text{number of favorable outcomes}}{\\text{total number of possible outcomes}}\n\n\n\n\nExamples to Illustrate\n\nFair Die Roll\n\nS = \\{1,2,3,4,5,6\\}, so |S| = 6\nFor getting an even number, A = \\{2,4,6\\}, so |A| = 3\nP(A) = \\frac{3}{6} = \\frac{1}{2}\n\nDrawing a Card\n\n|S| = 52 (total cards)\nFor drawing a king, |A| = 4\nP(\\text{king}) = \\frac{4}{52} = \\frac{1}{13}\n\n\n\n\nImportant Limitations\nThis formula only works when:\n\nSample space is finite (we can count outcomes)\nAll outcomes are equally likely\nEach outcome is distinct and well-defined\n\nIf any of these conditions fail (like with a loaded die), we need different methods to calculate probabilities.\n\n\nUnderstanding the Result\nThis derivation reveals several profound insights about classical probability:\nThe familiar ‚Äúcounting formula‚Äù (‚ÄòNaive‚Äô probability) isn‚Äôt just an intuitive rule - it follows necessarily from our axioms combined with the equal-likelihood assumption. When we say outcomes are equally likely, we‚Äôre forced mathematically to assign each elementary outcome a probability of \\frac{1}{|S|}. This isn‚Äôt a choice but a requirement of the axioms.\nFor any event A, its probability is determined entirely by comparing two cardinalities: the size of the event (|A|) relative to the size of the sample space (|S|).\n\n\n\n\n\n\nFair Coin Toss Probability Space\n\n\n\nA fair coin toss experiment is defined by:\nSample Space:\n\\Omega = \\{H, T\\}\nEvent Space (collection of all possible events):\n\\mathcal{F} = \\{\\{H\\}, \\{T\\}, \\{H, T\\}, \\emptyset\\}\nProbability Measure:\n\nP(\\{H\\}) = P(\\{T\\}) = \\frac{1}{2} (probability of heads or tails)\nP(\\Omega) = P(\\{H, T\\}) = 1 (certainty)\nP(\\emptyset) = 0 (impossible event)\n\nKey Points:\n\nThis is a simple probability space that satisfies the axioms:\n\nP(A) \\geq 0 for all events A\nP(\\Omega) = 1\nP(A \\cup B) = P(A) + P(B) for disjoint events\n\nThe event space \\mathcal{F} includes:\n\nIndividual outcomes: \\{H\\} and \\{T\\}\nThe entire sample space: \\{H, T\\}\nThe empty set: \\emptyset\n\nThe probabilities are equal (\\frac{1}{2}) because it‚Äôs a fair coin\n\nThis example illustrates a complete probability space with its three components: sample space (\\Omega), event space (\\mathcal{F}), and probability measure (P).\n\n\n\n\nExample Application\nLet‚Äôs solidify this understanding by working through a concrete example. Consider rolling a fair six-sided die and finding P(\\text{even number}):\n\nFirst, identify the sample space: S = \\{1, 2, 3, 4, 5, 6\\}, giving us |S| = 6\nThen, identify the event: A = \\{2, 4, 6\\}, giving us |A| = 3\nApply the formula: P(\\text{even number}) = \\frac{|A|}{|S|} = \\frac{3}{6} = \\frac{1}{2}\n\n\n\n\n\n\n\nKey Questions Before Calculating Probabilities\n\n\n\nBefore we can correctly calculate probabilities in any discrete scenario, we must answer two fundamental questions:\n\nDoes Order Matter?\n\nThe importance of order fundamentally changes how we count outcomes. Consider selecting two cards from a deck:\n\nIf we‚Äôre playing poker, order doesn‚Äôt matter - getting an ace and then a king is the same hand as getting a king and then an ace.\nIf we‚Äôre performing a magic trick where we need specific cards in sequence, order matters - getting an ace then a king is different from getting a king then an ace.\n\nWhen order matters, we‚Äôre dealing with permutations. When order doesn‚Äôt matter, we‚Äôre dealing with combinations. This distinction dramatically affects the number of possible outcomes and, consequently, our probability calculations.\n\nIs Sampling With or Without Replacement?\n\nAfter selecting an item, do we put it back before the next selection? This question fundamentally changes the probability structure:\n\nWith replacement: Each selection has the same probability distribution as the first selection. Drawing a red ball and replacing it means the probability of drawing red on the next try remains unchanged.\nWithout replacement: Each selection changes the probability distribution for subsequent selections. Drawing a red ball and not replacing it means there are fewer red balls available for the next draw.\n\nThese sampling schemes lead to different probability models:\n\nWith replacement leads to independent events and often simpler calculations\nWithout replacement leads to dependent events and requires conditional probability\n\n\n\n\n\n\n\n\n\nUnderstanding When to ADD vs MULTIPLY Probabilities\n\n\n\n\nThe Key Principle\n\nADD when events represent different ways (paths) to achieve the same outcome\nMULTIPLY when events must occur in sequence (one after another)\n\nExample 1: Single Die Roll Consider events:\n\nA: ‚Äúrolling an even number‚Äù = {2, 4, 6}\nB: ‚Äúrolling a number &gt; 4‚Äù = {5, 6}\n\nP(A or B) requires ADDITION because we want any outcome satisfying either condition:\n\nP(A or B) = P(A) + P(B) - P(A and B)\n= 3/6 + 2/6 - 1/6 = 4/6\n\nExample 2: Two Coin Flips\nFor P(at least one heads):\n\nADD different successful paths: P(HT or TH or HH)\n= 1/4 + 1/4 + 1/4 = 3/4\n\nFor P(two heads):\n\nMULTIPLY along path: P(H) √ó P(H)\n= 1/2 √ó 1/2 = 1/4\n\n\n\nWhy This Works\n\nAddition combines different ways to succeed\nMultiplication reflects narrowing down possibilities with each sequential requirement\n\n\n\n\n\n\ngraph LR\n    Start[Start] --&gt; H1[H]\n    Start --&gt; T1[T]\n    H1 --&gt; H2[H]\n    H1 --&gt; T2[T]\n    T1 --&gt; H3[H]\n    T1 --&gt; T3[T]\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Classical Probability Through the Urn Example\nIn our urn with 3 green and 2 red balls:\n\nP(\\text{green}) = \\frac{3}{5}\nP(\\text{red}) = \\frac{2}{5}\nP(\\text{green}) + P(\\text{red}) = 1\n\nThe classical definition of probability assumes:\n\nA finite sample space \\Omega with equally likely outcomes (‚Äòfair‚Äô experiment)\nFor an event A, probability is defined as: P(A) = \\frac{\\text{favorable outcomes}}{\\text{total outcomes}}\n\nIn our urn with 3 green and 2 red balls, these assumptions manifest as:\n\nSample space \\Omega = \\{b_1, b_2, b_3, b_4, b_5\\} where each ball is equally likely\nFor green: P(\\text{green}) = \\frac{|\\text{green balls}|}{|\\Omega|} = \\frac{3}{5}\nFor red: P(\\text{red}) = \\frac{|\\text{red balls}|}{|\\Omega|} = \\frac{2}{5}\n\nKey probability axioms are demonstrated:\n\nNon-negativity: P(\\text{green}), P(\\text{red}) \\geq 0\nNormalization: P(\\Omega) = P(\\text{green}) + P(\\text{red}) = 1\nAdditivity: Since green and red are disjoint events, P(\\text{green or red}) = P(\\text{green}) + P(\\text{red})\n\nREMARK: Many probabilistic situations have the property that they involve a number of different possible outcomes, all of which are equally likely. For example, Heads and Tails on a coin are equally likely to be tossed, the numbers 1 through 6 on a die are equally likely to be rolled, and the ten balls in the above box are all equally likely to be picked.\n‚ÄòNaive‚Äô (classical) probability definition assumes uniform probability measure (all outcomes equally likely), and finite uniform sample space.\nWhen considering shapes or elements of the same color in an urn or box, treating them as distinguishable allows you to assume a uniform sample space ‚Äî equally likely outcomes.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#how-to-calculate-basic-probabilities",
    "href": "probability_en.html#how-to-calculate-basic-probabilities",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "17.5 How to Calculate Basic Probabilities",
    "text": "17.5 How to Calculate Basic Probabilities\nLet‚Äôs explore some fundamental probability concepts using a simple example with colored balls in an urn/bag. This example will help us understand:\n\nHow to calculate basic probabilities using the tree diagrams\nHow replacement affects probability\nHow the importance of order affects our calculations\nHow to break down probability problems into steps\n\nTree diagrams are powerful tools for visualizing sequential events. Each branch represents a possible outcome, and probabilities multiply along paths.\n\n\n\n\n\n\nUnderstanding Sampling Methods: Sequences vs Sets\n\n\n\nWhen we count possibilities (sample space size = |S|) in probability problems, we need to think about two important questions:\n\nDoes the order of our selections matter? (Like picking a phone PIN where 1234 is different from 4321)\nCan we reuse items we‚Äôve already selected? (Like picking letters where we can reuse them, versus picking students where we can‚Äôt pick the same person twice)\n\nConsider sampling from elements \\{A, B, C\\}. The key distinction is whether we care about:\n\nSequences (ordered lists): where position matters\nSets: where only membership matters\n\n\n17.6 Sampling Scenarios\n\n\n\n\n\n\n\n\nSampling Method\nWith Replacement\nWithout Replacement\n\n\n\n\nSequences (Order Matters)\nOrdered lists with repetition:(A,A), (A,B), (A,C)(B,A), (B,B), (B,C)(C,A), (C,B), (C,C)\nOrdered lists without repetition:(A,B), (A,C)(B,A), (B,C)(C,A), (C,B)\n\n\nSets (Order Doesn‚Äôt Matter)\nMultisets (sets with repetition):\\{A,A\\}, \\{A,B\\}, \\{A,C\\}\\{B,B\\}, \\{B,C\\}\\{C,C\\}\nSets (no repetition):\\{A,B\\}, \\{A,C\\}\\{B,C\\}\n\n\n\n\n\n17.7 Mathematical Properties\n\nSequences (Order Matters)\n\nElements have positions: a_1, a_2, \\ldots, a_n\nTwo sequences \\mathbf{x}, \\mathbf{y} are equal iff x_i = y_i for all i\nDenoted as ordered tuples: (a_1, a_2, \\ldots, a_n)\n\nSets (Order Doesn‚Äôt Matter)\n\nElements have no position, only membership matters\nTwo sets are equal if they contain the same elements\nDenoted with curly braces: \\{a_1, a_2, \\ldots, a_n\\}\n\n\n\n\n17.8 Factorial Notation\nFor any non-negative integer n, the factorial of n (denoted as n!) is defined as:\nn! = n \\cdot (n-1) \\cdot (n-2) \\cdot ... \\cdot 2 \\cdot 1\nSpecial cases:\n\n0! = 1 (by definition)\n1! = 1\n2! = 2 \\cdot 1 = 2\n3! = 3 \\cdot 2 \\cdot 1 = 6\n4! = 4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\n\nThis can be written recursively as:\n\nn! = n \\cdot (n-1)! for n &gt; 0\n0! = 1\n\nHere‚Äôs why 0! equals 1:\n\nBy definition, for any positive integer n, n! = n √ó (n-1)!\nThis means 1! = 1 √ó 0!\nWe know 1! = 1\nTherefore: 1 = 1 √ó 0!\nSolving for 0!: 0! = 1\n\nThis definition is also consistent with the combinatorial interpretation - there is exactly one way to arrange zero elements.\n\n\n17.9 Number of Outcomes\nFor n distinct elements, selecting k items:\n\nSequences with Replacement\n\nEach position has n choices\nTotal: n^k outcomes\n\nSequences without Replacement\n\nPermutations: P(n,k) = \\frac{n!}{(n-k)!}\nEach next position has one fewer choice\n\nSets with Replacement\n\nCombinations with repetition allowed\nTotal: \\binom{n+k-1}{k} outcomes\n\nSets without Replacement\n\nCombinations: \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\nEach subset of size k counted once\n\n\n\n\n17.10 Key Relationships\n\nFor sequences vs sets without replacement:\nP(n,k) = k! \\cdot \\binom{n}{k}\nFor any sampling scheme:\n\\text{sequences} \\geq \\text{sets} \\text{with replacement} \\geq \\text{without replacement}\n\n\n\n\n\nExample 1: Drawing Two Balls from an Urn/Bag\nConsider drawing two balls from an urn containing 3 green and 2 red balls.\nFind the probabilities of the following random events:\n\nThe first ball is red and the second one is green (order matters, drawing without replacement)\nThe first ball is red and the second one is green (order matters, drawing with replacement)\nThe balls are of different colors (order doesn‚Äôt matter, drawing without replacement)\nThe balls are of different colors (order doesn‚Äôt matter, drawing with replacement)\n\nUnderstanding Event Types in Probability:\n\nSimple events represent a single outcome from a single random action, such as drawing one ball from an urn. The probability of a simple event is calculated directly from the number of favorable outcomes divided by the total possible outcomes.\nCompound events involve multiple outcomes or conditions that must occur together. These can occur simultaneously (like rolling two dice at once) or sequentially (like drawing two balls one after another). The key difference lies in whether the events happen at the same time or in sequence.\n\n\nSequential events are a specific type of compound events where outcomes occur in a particular order over time. Our urn example is particularly instructive here because it demonstrates sequential events through the process of drawing balls one after another. This allows us to explore how the probability of the second draw depends on what happened in the first draw (when sampling without replacement).\n\n\n\n\nSample spaces (S) visualized using the grid diagrams\n\n\nTo better understand how the sample space changes based on our sampling method, let‚Äôs examine two scenarios:\n\nWith Replacement\n\nWhen we sample with replacement, we return the ball to the urn after the first draw. This means:\n\nThe probability remains constant for each draw\nTotal possible outcomes: 25 (5√ó5 grid)\nEach outcome has equal probability\nP(\\text{both green}) = \\frac{3}{5} \\times \\frac{3}{5} = \\frac{9}{25}\nP(\\text{both red}) = \\frac{2}{5} \\times \\frac{2}{5} = \\frac{4}{25}\nP(\\text{mixed}) = \\frac{12}{25}\n\n\nWithout Replacement\n\nWhen we sample without replacement, the first draw affects the probability of the second draw:\n\nTotal possible outcomes: 20 (removing diagonal cells where same ball is drawn twice)\nSecond draw probabilities change based on first draw\nP(\\text{both green}) = \\frac{3}{5} \\times \\frac{2}{4} = \\frac{6}{20}\nP(\\text{both red}) = \\frac{2}{5} \\times \\frac{1}{4} = \\frac{2}{20}\nP(\\text{mixed}) = \\frac{12}{20}\n\nThe grid diagram above visualizes both scenarios, where:\n\nGreen cells represent both balls drawn being green\nRed cells represent both balls drawn being red\nOrange cells represent mixed outcomes (one green, one red)\nCrossed-out cells in the ‚ÄúWithout Replacement‚Äù grid show impossible outcomes\n\nThis visualization helps demonstrate how the sample space and probabilities change between the two sampling methods, while maintaining the fundamental principle that probabilities must sum to 1 in both cases.\n\nDrawing Two Balls Without Replacement\n\nConsider drawing two balls from an urn containing 3 green and 2 red balls. Let‚Äôs analyze all scenarios systematically.\n\n\n\n\n\nflowchart TD\n    A([\"Initial State\\n3G, 2R\"]) --&gt; B[\"First: Green\\n3/5\"]\n    A --&gt; C[\"First: Red\\n2/5\"]\n    B --&gt; D[\"Second: Green\\n2/4\"]\n    B --&gt; E[\"Second: Red\\n2/4\"]\n    C --&gt; F[\"Second: Green\\n3/4\"]\n    C --&gt; G[\"Second: Red\\n1/4\"]\n    \n    D --&gt; H[\"GG: 3/5 √ó 2/4 = 6/20\"]\n    E --&gt; I[\"GR: 3/5 √ó 2/4 = 6/20\"]\n    F --&gt; J[\"RG: 2/5 √ó 3/4 = 6/20\"]\n    G --&gt; K[\"RR: 2/5 √ó 1/4 = 2/20\"]\n\n\n\n\n\n\nLet‚Äôs solve for different scenarios:\n\nFirst red, then green (order matters):\nP(R \\text{ then } G) = \\frac{2}{5} \\cdot \\frac{3}{4} = \\frac{6}{20} = 0.3\nDifferent colors (order doesn‚Äôt matter):\nP(\\text{different colors}) = P(R \\text{ then } G) + P(G \\text{ then } R)\n= \\frac{2}{5} \\cdot \\frac{3}{4} + \\frac{3}{5} \\cdot \\frac{2}{4} = \\frac{6}{20} + \\frac{6}{20} = \\frac{12}{20} = 0.6\n\n\n\nDrawing With Replacement\n\nWhen we replace the first ball before drawing the second, the probabilities for the second draw remain unchanged:\n\n\n\n\n\nflowchart TD\n    A([\"Initial State\\n3G, 2R\"]) --&gt; B[\"First: Green\\n3/5\"]\n    A --&gt; C[\"First: Red\\n2/5\"]\n    B --&gt; D[\"Second: Green\\n3/5\"]\n    B --&gt; E[\"Second: Red\\n2/5\"]\n    C --&gt; F[\"Second: Green\\n3/5\"]\n    C --&gt; G[\"Second: Red\\n2/5\"]\n    \n    D --&gt; H[\"GG: 3/5 √ó 3/5 = 9/25\"]\n    E --&gt; I[\"GR: 3/5 √ó 2/5 = 6/25\"]\n    F --&gt; J[\"RG: 2/5 √ó 3/5 = 6/25\"]\n    G --&gt; K[\"RR: 2/5 √ó 2/5 = 4/25\"]\n\n\n\n\n\n\nNow:\n\nFirst red, then green (order matters):\nP(R \\text{ then } G) = \\frac{2}{5} \\cdot \\frac{3}{5} = \\frac{6}{25} = 0.24\nDifferent colors (order doesn‚Äôt matter):\nP(\\text{different colors}) = \\frac{2}{5} \\cdot \\frac{3}{5} + \\frac{3}{5} \\cdot \\frac{2}{5} = \\frac{12}{25} = 0.48\n\nKey observations:\n\nWithout replacement:\n\nDifferent orders of the same colors have different probabilities\nThe second draw‚Äôs probability depends on the first outcome\n\nWith replacement:\n\nEach draw is independent\nProbabilities multiply directly because sample space remains unchanged\n\n\n\n\nExample 2: The 4 Red and 3 Black Balls Problem\nLet‚Äôs solve a real problem using what we learned. We have:\n\n4 red balls (let‚Äôs call them R‚ÇÅ, R‚ÇÇ, R‚ÇÉ, R‚ÇÑ)\n3 black balls (B‚ÇÅ, B‚ÇÇ, B‚ÇÉ)\nWe‚Äôll draw 2 balls without replacement\nOrder doesn‚Äôt matter (like picking team members)\n\nWe want to find three probabilities:\n\nGetting two red balls\nGetting two black balls\nGetting one of each color\n\n\nMethod 1: Using Counting Rules\nFirst, let‚Äôs count the total possible outcomes:\n\nWe‚Äôre picking 2 balls from 7 total balls, order doesn‚Äôt matter\nTotal outcomes = \\binom{7}{2} = \\frac{7!}{2!(7-2)!} = \\frac{7 \\times 6}{2 \\times 1} = 21\n\nNow let‚Äôs find each probability:\n\nTwo Red Balls\n\n\nWe need to pick 2 red balls from 4 red balls\nThis is like picking 2 team members from 4 people\nNumber of ways = \\binom{4}{2} = \\frac{4 \\times 3}{2 \\times 1} = 6\nProbability = \\frac{6}{21}\n\n\nTwo Black Balls\n\n\nSimilarly, we need to pick 2 black balls from 3 black balls\nNumber of ways = \\binom{3}{2} = \\frac{3 \\times 2}{2 \\times 1} = 3\nProbability = \\frac{3}{21}\n\n\nOne Red and One Black\n\n\nWe need:\n\nOne red ball (we have 4 to choose from)\nOne black ball (we have 3 to choose from)\n\nNumber of ways = 4 \\times 3 = 12\nProbability = \\frac{12}{21}\n\nLet‚Äôs verify our work:\n\nAll probabilities should add to 1\n\\frac{6}{21} + \\frac{3}{21} + \\frac{12}{21} = \\frac{21}{21} = 1 ‚úì\n\nThis matches what we expect - every time we draw two balls, we must get either:\n\nTwo red balls\nTwo black balls\nOne of each color\n\nUnderstanding how to count correctly helps us solve these probability problems systematically and avoid common mistakes like counting the same outcome multiple times.\n\n\nMethod 2: Tree Diagram Approach\nThe tree diagram helps us visualize the sequential nature of the draws:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[First: Red 4/7]\n    A --&gt; C[First: Black 3/7]\n    B --&gt; D[Second: Red 3/6]\n    B --&gt; E[Second: Black 3/6]\n    C --&gt; F[Second: Red 4/6]\n    C --&gt; G[Second: Black 2/6]\n\n\n\n\n\n\nUsing the tree diagram:\n\nP(both red) = \\frac{4}{7} \\cdot \\frac{3}{6} = \\frac{12}{42} = \\frac{6}{21}\nP(red then black) = \\frac{4}{7} \\cdot \\frac{3}{6} = \\frac{12}{42}\nP(multi-colored) = P(red then black) + P(black then red)\n= \\frac{4}{7} \\cdot \\frac{3}{6} + \\frac{3}{7} \\cdot \\frac{4}{6} = \\frac{24}{42}\n\n\n\nMethod 3: Grid Diagram Analysis of Two-Ball Draws\nLet‚Äôs visualize the ordered sample space using a grid where rows represent the second draw and columns represent the first draw:\n\n\n\nFirst Draw ‚Üí\nR‚ÇÅ\nR‚ÇÇ\nR‚ÇÉ\nR‚ÇÑ\nB‚ÇÅ\nB‚ÇÇ\nB‚ÇÉ\n\n\n\n\nSecond Draw ‚Üì\n\n\n\n\n\n\n\n\n\nR‚ÇÅ\nX\n‚ö´\n‚ö´\n‚ö´\n‚ö™\n‚ö™\n‚ö™\n\n\nR‚ÇÇ\n‚ö´\nX\n‚ö´\n‚ö´\n‚ö™\n‚ö™\n‚ö™\n\n\nR‚ÇÉ\n‚ö´\n‚ö´\nX\n‚ö´\n‚ö™\n‚ö™\n‚ö™\n\n\nR‚ÇÑ\n‚ö´\n‚ö´\n‚ö´\nX\n‚ö™\n‚ö™\n‚ö™\n\n\nB‚ÇÅ\n‚ö™\n‚ö™\n‚ö™\n‚ö™\nX\n‚ö´\n‚ö´\n\n\nB‚ÇÇ\n‚ö™\n‚ö™\n‚ö™\n‚ö™\n‚ö´\nX\n‚ö´\n\n\nB‚ÇÉ\n‚ö™\n‚ö™\n‚ö™\n‚ö™\n‚ö´\n‚ö´\nX\n\n\n\nWhere:\n\nX: Impossible (same ball drawn twice)\n‚ö´: Both same color (both red in upper-left, both black in lower-right)\n‚ö™: Different colors (red-black or black-red)\n\nFrom this grid:\n\nBoth red = 12 outcomes (‚ö´ in upper-left quadrant)\nBoth black = 6 outcomes (‚ö´ in lower-right quadrant)\nRed then black = 12 outcomes (‚ö™ in lower-left quadrant)\nBlack then red = 12 outcomes (‚ö™ in upper-right quadrant)\nTotal possible outcomes = 42 (all cells minus 7 diagonal X‚Äôs)\n\n\n\nAnalysis of Two-Ball Draws\nFrom the grid, we can count the following outcomes:\n\nBoth red = 12 outcomes (‚ö´ in upper-left quadrant)\nRed then black = 12 outcomes (‚ö™ in lower-left quadrant)\nBlack then red = 12 outcomes (‚ö™ in upper-right quadrant)\nTotal possible outcomes = 42 (all cells minus 7 diagonal X‚Äôs)\n\nTherefore:\n\nP(both red) = \\frac{12}{42} = \\frac{2}{7}\nP(red then black) = \\frac{12}{42} = \\frac{2}{7}\nP(multi-colored) = \\frac{24}{42} = \\frac{4}{7} (includes both red-then-black and black-then-red)\n\n\n\nComparing the Methods\nEach method highlights different aspects of the problem:\n\nCounting Rules:\n\nMost efficient for calculation\nHelps understand combinations and arrangements\nMay obscure the actual outcomes\n\nTree Diagram:\n\nShows sequential nature of draws\nMakes conditional probability clear\nVisualizes how probabilities combine\nGood for checking intuition\n\nGrid Diagram:\n\nShows entire sample space explicitly\nMakes it clear why diagonal is impossible\nHelps visualize groups of outcomes\nDemonstrates why we divide by total possibilities\nShows symmetry in the problem",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#sampling-scenarios",
    "href": "probability_en.html#sampling-scenarios",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "17.6 Sampling Scenarios",
    "text": "17.6 Sampling Scenarios\n\n\n\n\n\n\n\n\nSampling Method\nWith Replacement\nWithout Replacement\n\n\n\n\nSequences (Order Matters)\nOrdered lists with repetition:(A,A), (A,B), (A,C)(B,A), (B,B), (B,C)(C,A), (C,B), (C,C)\nOrdered lists without repetition:(A,B), (A,C)(B,A), (B,C)(C,A), (C,B)\n\n\nSets (Order Doesn‚Äôt Matter)\nMultisets (sets with repetition):\\{A,A\\}, \\{A,B\\}, \\{A,C\\}\\{B,B\\}, \\{B,C\\}\\{C,C\\}\nSets (no repetition):\\{A,B\\}, \\{A,C\\}\\{B,C\\}",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#mathematical-properties",
    "href": "probability_en.html#mathematical-properties",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "17.7 Mathematical Properties",
    "text": "17.7 Mathematical Properties\n\nSequences (Order Matters)\n\nElements have positions: a_1, a_2, \\ldots, a_n\nTwo sequences \\mathbf{x}, \\mathbf{y} are equal iff x_i = y_i for all i\nDenoted as ordered tuples: (a_1, a_2, \\ldots, a_n)\n\nSets (Order Doesn‚Äôt Matter)\n\nElements have no position, only membership matters\nTwo sets are equal if they contain the same elements\nDenoted with curly braces: \\{a_1, a_2, \\ldots, a_n\\}",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#factorial-notation",
    "href": "probability_en.html#factorial-notation",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "17.8 Factorial Notation",
    "text": "17.8 Factorial Notation\nFor any non-negative integer n, the factorial of n (denoted as n!) is defined as:\nn! = n \\cdot (n-1) \\cdot (n-2) \\cdot ... \\cdot 2 \\cdot 1\nSpecial cases:\n\n0! = 1 (by definition)\n1! = 1\n2! = 2 \\cdot 1 = 2\n3! = 3 \\cdot 2 \\cdot 1 = 6\n4! = 4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\n\nThis can be written recursively as:\n\nn! = n \\cdot (n-1)! for n &gt; 0\n0! = 1\n\nHere‚Äôs why 0! equals 1:\n\nBy definition, for any positive integer n, n! = n √ó (n-1)!\nThis means 1! = 1 √ó 0!\nWe know 1! = 1\nTherefore: 1 = 1 √ó 0!\nSolving for 0!: 0! = 1\n\nThis definition is also consistent with the combinatorial interpretation - there is exactly one way to arrange zero elements.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#number-of-outcomes",
    "href": "probability_en.html#number-of-outcomes",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "17.9 Number of Outcomes",
    "text": "17.9 Number of Outcomes\nFor n distinct elements, selecting k items:\n\nSequences with Replacement\n\nEach position has n choices\nTotal: n^k outcomes\n\nSequences without Replacement\n\nPermutations: P(n,k) = \\frac{n!}{(n-k)!}\nEach next position has one fewer choice\n\nSets with Replacement\n\nCombinations with repetition allowed\nTotal: \\binom{n+k-1}{k} outcomes\n\nSets without Replacement\n\nCombinations: \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\nEach subset of size k counted once",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#key-relationships",
    "href": "probability_en.html#key-relationships",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "17.10 Key Relationships",
    "text": "17.10 Key Relationships\n\nFor sequences vs sets without replacement:\nP(n,k) = k! \\cdot \\binom{n}{k}\nFor any sampling scheme:\n\\text{sequences} \\geq \\text{sets} \\text{with replacement} \\geq \\text{without replacement}",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#problem-solutions-1",
    "href": "probability_en.html#problem-solutions-1",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "17.11 Problem Solutions (1)",
    "text": "17.11 Problem Solutions (1)\n\nProblem 1: Two-Ball Drawing from an Urn\nAn urn contains 3 red, 2 blue, and 1 yellow balls. Two balls are drawn sequentially without replacement. We need to find the probability that the balls drawn are different colors.\n\nInitial Conditions\nLet‚Äôs first state our starting conditions:\n\nTotal number of balls: n = 3 + 2 + 1 = 6\nDistribution of balls:\n\nRed: n_R = 3\nBlue: n_B = 2\nYellow: n_Y = 1\n\n\n\n\nVisual Representation\nLet‚Äôs visualize all possible outcomes using a tree diagram:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[\"R (3/6)\"]\n    A --&gt; C[\"B (2/6)\"]\n    A --&gt; D[\"Y (1/6)\"]\n    \n    B --&gt; E[\"B (2/5)\"]\n    B --&gt; F[\"Y (1/5)\"]\n    B --&gt; G[\"R (2/5)\"]\n    \n    C --&gt; H[\"R (3/5)\"]\n    C --&gt; I[\"Y (1/5)\"]\n    C --&gt; J[\"B (1/5)\"]\n    \n    D --&gt; K[\"R (3/5)\"]\n    D --&gt; L[\"B (2/5)\"]\n    D --&gt; M[\"Y (0/5)\"]\n    \n    E --&gt; N[\"RB (Success)\"]\n    F --&gt; O[\"RY (Success)\"]\n    G --&gt; P[\"RR (Fail)\"]\n    H --&gt; Q[\"BR (Success)\"]\n    I --&gt; R[\"BY (Success)\"]\n    J --&gt; S[\"BB (Fail)\"]\n    K --&gt; T[\"YR (Success)\"]\n    L --&gt; U[\"YB (Success)\"]\n    M --&gt; V[\"YY (Fail)\"]\n\n\n\n\n\n\n\n\nProbability Calculation\nLet‚Äôs calculate the probability of drawing different colors systematically:\n\nStarting with Red (probability \\frac{3}{6}):\n\nRed ‚Üí Blue: P(R,B) = \\frac{3}{6} \\cdot \\frac{2}{5} = \\frac{6}{30}\nRed ‚Üí Yellow: P(R,Y) = \\frac{3}{6} \\cdot \\frac{1}{5} = \\frac{3}{30}\n\nStarting with Blue (probability \\frac{2}{6}):\n\nBlue ‚Üí Red: P(B,R) = \\frac{2}{6} \\cdot \\frac{3}{5} = \\frac{6}{30}\nBlue ‚Üí Yellow: P(B,Y) = \\frac{2}{6} \\cdot \\frac{1}{5} = \\frac{2}{30}\n\nStarting with Yellow (probability \\frac{1}{6}):\n\nYellow ‚Üí Red: P(Y,R) = \\frac{1}{6} \\cdot \\frac{3}{5} = \\frac{3}{30}\nYellow ‚Üí Blue: P(Y,B) = \\frac{1}{6} \\cdot \\frac{2}{5} = \\frac{2}{30}\n\n\n\n\nFinal Solution\nThe total probability of drawing two different colored balls is the sum of all favorable outcomes:\n\n\\begin{align*}\nP(\\text{different colors}) &= P(R,B) + P(R,Y) + P(B,R) + P(B,Y) + P(Y,R) + P(Y,B) \\\\\n&= \\frac{6}{30} + \\frac{3}{30} + \\frac{6}{30} + \\frac{2}{30} + \\frac{3}{30} + \\frac{2}{30} \\\\\n&= \\frac{22}{30} \\\\\n&= \\frac{11}{15} \\\\\n&\\approx 0.733 \\text{ or } 73.3\\%\n\\end{align*}\n\n\n\nVerification\nThis result aligns with our intuition because:\n\nThe sample space contains more ways to draw different colors than same colors\nThe complementary probability (drawing same colors) would be \\frac{4}{15} or about 26.7%\nSince same-color draws are limited to RR, BB, and YY combinations, it makes sense that different-color draws are more likely\n\n\n\n\nProblem 2: Die and Coin Probability Exercise\nLet‚Äôs analyze the probability of getting heads OR tails OR three dots when flipping both a coin and a die. This problem offers an excellent opportunity to explore probability unions and the importance of careful counting.\n\nUnderstanding the Problem Space\nIn our experiment:\n\nWe flip a coin (possible outcomes: heads, tails)\nWe roll a die (possible outcomes: 1, 2, 3, 4, 5, 6 dots)\nThese events occur simultaneously\n\nLet‚Äôs start with a visualization:\n\n\n\n\n\ngraph TD\n    A[Experiment] --&gt; B[Coin]\n    A --&gt; C[Die]\n    B --&gt; D[Heads]\n    B --&gt; E[Tails]\n    C --&gt; F[1 dot]\n    C --&gt; G[2 dots]\n    C --&gt; H[3 dots]\n    C --&gt; I[4 dots]\n    C --&gt; J[5 dots]\n    C --&gt; K[6 dots]\n\n\n\n\n\n\n\n\nCommon Mistakes and Overcounting Analysis\nA common first instinct might be to simply add the individual probabilities:\nP(heads) + P(tails) + P(three dots) = \\frac{1}{2} + \\frac{1}{2} + \\frac{1}{6} = \\frac{7}{6}\nThis incorrect approach reveals several important issues:\n\nThe result exceeds 1, which is impossible for a probability\nWe‚Äôve counted many outcomes multiple times\nWe‚Äôve failed to recognize event overlaps\n\nLet‚Äôs analyze the overcounting:\n\n\n\n\n\ngraph TD\n    A[Overcounting Analysis] --&gt; B[Heads counted: 6/12]\n    A --&gt; C[Tails counted: 6/12]\n    A --&gt; D[Three dots counted: 2/12]\n    B --&gt; E[Including three with heads: 1/12]\n    C --&gt; F[Including three with tails: 1/12]\n    E --&gt; G[Double counted!]\n    F --&gt; G\n\n\n\n\n\n\n\n\nCorrect Solution Using Set Theory\nLet‚Äôs solve this properly using set theory:\n\nSet H: All outcomes with heads\nSet T: All outcomes with tails\nSet 3: All outcomes with three dots\n\nKey insights:\n\nSets H and T are mutually exclusive\nSet 3 is entirely contained within H ‚à™ T\nTherefore, P(H ‚à™ T ‚à™ 3) = P(H ‚à™ T) = 1\n\nWe can write this formally:\nP(H ‚à™ T ‚à™ 3) = P(H) + P(T) - P(H ‚à© T) + P(3) - P(3 ‚à© (H ‚à™ T)) = \\frac{1}{2} + \\frac{1}{2} - 0 + \\frac{1}{6} - \\frac{1}{6} = 1\n\n\nSample Space Analysis\n\n\n\n\n\ngraph TD\n    A[Total Outcomes: 12] --&gt; B[Heads: 6]\n    A --&gt; C[Tails: 6]\n    B --&gt; D[With three: 1]\n    C --&gt; E[With three: 1]\n    D --&gt; F[Already counted in heads]\n    E --&gt; G[Already counted in tails]\n\n\n\n\n\n\nThis visual representation helps us understand why:\n\nThe sample space has 12 total outcomes (2 √ó 6)\nThe three-dot outcomes are already included in heads and tails counts\nAdding P(three dots) would lead to double counting\n\n\n\nKey Learning Points\nThis problem illustrates several fundamental probability concepts:\n\nExhaustive Events: Heads and tails together cover all possible coin outcomes, making additional events redundant unless they introduce new dimensions.\nDouble Counting Protection: The inclusion-exclusion principle helps us avoid counting outcomes multiple times.\nSample Space Structure: Understanding your sample space structure (12 total outcomes) helps verify solution logic.\n\n\n\n\nProblem 3: Laplace‚Äôs Two-Draw Probability Problem\nSuppose there are two urns of coloured marbles:\n\nUrn X contains 3 black marbles, 1 white.\nUrn Y contains 1 black marble, 3 white.\n\nI flip a fair coin to decide which urn to draw from, heads for Urn X and tails for Urn Y. Then I draw marbles at random.\nLaplace asked what happens if we do two draws, with replacement. What‚Äôs the probability both draws will come up black?\nLet‚Äôs solve this fascinating probability problem involving two draws with replacement. This is a particularly interesting case because the replacement aspect affects how we think about sequential probabilities.\n\nUnderstanding the Initial Setup\nFirst, let‚Äôs clarify our starting conditions:\nUrn X (selected with heads):\n\n3 black marbles, 1 white marble\nTotal: 4 marbles\nP(black|X) = \\frac{3}{4}\n\nUrn Y (selected with tails):\n\n1 black marble, 3 white marbles\nTotal: 4 marbles\nP(black|Y) = \\frac{1}{4}\n\nLet‚Äôs visualize this with a tree diagram showing all possible paths:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[Urn X 1/2]\n    A --&gt; C[Urn Y 1/2]\n    \n    B --&gt; D[Draw 1 Black 3/4]\n    B --&gt; E[Draw 1 White 1/4]\n    \n    C --&gt; F[Draw 1 Black 1/4]\n    C --&gt; G[Draw 1 White 3/4]\n    \n    D --&gt; H[Draw 2 Black 3/4]\n    D --&gt; I[Draw 2 White 1/4]\n    \n    E --&gt; J[Draw 2 Black 3/4]\n    E --&gt; K[Draw 2 White 1/4]\n    \n    F --&gt; L[Draw 2 Black 1/4]\n    F --&gt; M[Draw 2 White 3/4]\n    \n    G --&gt; N[Draw 2 Black 1/4]\n    G --&gt; O[Draw 2 White 3/4]\n\n\n\n\n\n\n\n\nStep-by-Step Solution\nLet‚Äôs break this down into manageable steps:\n\nFirst, consider the urn selection:\n\nP(Urn X) = P(heads) = \\frac{1}{2}\nP(Urn Y) = P(tails) = \\frac{1}{2}\n\nFor two black draws from Urn X:\n\nP(black and black|X) = \\frac{3}{4} \\times \\frac{3}{4} = \\frac{9}{16}\nP(X and both black) = \\frac{1}{2} \\times \\frac{9}{16} = \\frac{9}{32}\n\nFor two black draws from Urn Y:\n\nP(black and black|Y) = \\frac{1}{4} \\times \\frac{1}{4} = \\frac{1}{16}\nP(Y and both black) = \\frac{1}{2} \\times \\frac{1}{16} = \\frac{1}{32}\n\nTotal probability (using the law of total probability): P(both black) = P(X and both black) + P(Y and both black) = \\frac{9}{32} + \\frac{1}{32} = \\frac{10}{32} = \\frac{5}{16} ‚âà 0.3125 or about 31.25%\n\n\n\nKey Insights from This Problem\n\nReplacement Matters:\n\nBecause we replace after the first draw, the probabilities remain constant for the second draw\nThis is different from drawing without replacement, where probabilities would change\n\nConditional Independence:\n\nOnce we know which urn we‚Äôre using, the draws are independent\nHowever, the draws are not unconditionally independent\n\nLaw of Total Probability:\n\nWe needed to consider both paths (Urn X and Urn Y) to find the total probability\nEach path‚Äôs contribution is weighted by the probability of selecting that urn",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#core-probability-rules",
    "href": "probability_en.html#core-probability-rules",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "17.12 Core Probability Rules",
    "text": "17.12 Core Probability Rules\n\nThe Complement Rule\nThe complement rule is one of the most fundamental concepts in probability theory. For any event A, there‚Äôs always the possibility that A doesn‚Äôt occur. We call this the complement of A, written as A' or A^c.\nThe complement rule states:\nP(A') = 1 - P(A)\nThis makes intuitive sense because any outcome must either be in A or in A‚Äô (but not both), and something must happen (the total probability must be 1).\nReal-World Example: Consider a weather forecast that predicts a 70% chance of rain tomorrow. Using the complement rule, we can immediately calculate that there‚Äôs a 30% chance it won‚Äôt rain:\nP(\\text{no rain}) = 1 - P(\\text{rain}) = 1 - 0.70 = 0.30\nAnother Example: In a game of roulette, what‚Äôs the probability of not landing on red? There are 18 red numbers, 18 black numbers, and 2 green numbers (0 and 00) on a roulette wheel. Therefore:\nP(\\text{red}) = \\frac{18}{38} P(\\text{not red}) = 1 - \\frac{18}{38} = \\frac{20}{38}\n\n\nThe Addition/Sum Rule\nWhen we want to find the probability of either one event OR another occurring, we use the addition rule. However, we need to be careful about double-counting outcomes that are in both events.\nFor any two events A and B:\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\nThe term P(A \\cap B) represents the probability of both events occurring simultaneously. We subtract it to avoid counting these outcomes twice.\nReal-World Example: In a college class, 65% of students play sports, 45% are in clubs, and 25% do both. What percentage of students are involved in either sports or clubs?\nP(\\text{sports or clubs}) = 65\\% + 45\\% - 25\\% = 85\\%\nFor mutually exclusive events (events that cannot occur simultaneously), P(A \\cap B) = 0, so the formula simplifies to:\nP(A \\cup B) = P(A) + P(B)\nExample: When rolling a die, what‚Äôs the probability of rolling either a 1 or a 6? Since these outcomes can‚Äôt happen simultaneously:\nP(1 \\text{ or } 6) = P(1) + P(6) = \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{3}\n\n\nConditional Probability, the Multiplication Rule, and Bayes‚Äô Theorem\n\nUnderstanding Conditional Probability\nConditional probability represents how the probability of one event changes when we have information about another event. Let‚Äôs start with a simple example:\nImagine you have a deck of 52 playing cards. What‚Äôs the probability of drawing a King given that you‚Äôve drawn a face card (Jack, Queen, or King)?\nTo solve this:\n\nTotal face cards = 12 (4 each of Jack, Queen, King)\nNumber of Kings = 4\nP(\\text{King}|\\text{Face Card}) = \\frac{P(\\text{King} \\cap \\text{Face Card})}{P(\\text{Face Card})} = \\frac{4/52}{12/52} = \\frac{1}{3}\n\nThis illustrates the fundamental formula for conditional probability:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\nIn the context of classical probability, where all outcomes are equally likely, we can express these probabilities in terms of the number of favorable outcomes:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{|A \\cap B|/|S|}{|B|/|S|} = \\frac{|A \\cap B|}{|B|}\nwhere:\n\n|A \\cap B| represents the number of outcomes in both events A and B\n|B| represents the number of outcomes in event B\n|S| represents the total number of outcomes in the sample space\n\nThis is why in our card example: P(\\text{King}|\\text{Face Card}) = \\frac{|\\text{Kings}|}{|\\text{Face Cards}|} = \\frac{4}{12} = \\frac{1}{3}\nThe formula shows that in classical probability, conditional probability is simply the ratio of the number of outcomes favorable to both events to the number of outcomes in the conditioning event.\n\n\nMedical Testing Example\nLet‚Äôs explore a more practical example involving medical testing. Consider a disease that affects 1% of the population and a test T with the following characteristics:\n\nSensitivity (true positive rate): P(T^+|D^+) = 0.95\nSpecificity (true negative rate): P(T^-|D^-) = 0.98\n\nWe can organize this information in a cross table for a population of 10,000:\n\n\n\nTest/Disease\nD^+ Present\nD^- Absent\nTotal\n\n\n\n\nT^+\n95\n198\n293\n\n\nT^-\n5\n9,702\n9,707\n\n\nTotal\n100\n9,900\n10,000\n\n\n\nUsing this, we can calculate important probabilities like:\n\nPositive Predictive Value: P(D^+|T^+) = \\frac{95}{293} \\approx 0.32\nNegative Predictive Value: P(D^-|T^-) = \\frac{9,702}{9,707} \\approx 0.999\n\nNote that:\n\nP(D^+) = 0.01 (disease prevalence)\nP(T^+|D^-) = 0.02 (false positive rate = 1 - specificity)\nP(T^-|D^+) = 0.05 (false negative rate = 1 - sensitivity)\n\nThese probabilities show how a test with seemingly good characteristics (95% sensitivity and 98% specificity) can still lead to many false positives when the condition being tested for is rare in the population.\n\n\n\n\n\n\nUrn Example with Probability Tree\n\n\n\nConsider an urn containing:\n\n3 blue marbles (B)\n\n2 marked with star (S^+)\n1 unmarked (S^-)\n\n2 red marbles (R)\n\n1 marked with star (S^+)\n1 unmarked (S^-)\n\n\nLet‚Äôs calculate the probability of drawing a starred marble given that we drew a blue marble.\nUsing the tree diagram:\n\n\n\n\n\ngraph LR\n    Œ©{Œ©} --&gt; B[B: 3/5]\n    Œ© --&gt; R[R: 2/5]\n    B --&gt; BS+[\"S‚Å∫|B: 2/3\"]\n    B --&gt; BS-[\"S‚Åª|B: 1/3\"]\n    R --&gt; RS+[\"S‚Å∫|R: 1/2\"]\n    R --&gt; RS-[\"S‚Åª|R: 1/2\"]\n    \n    style Œ© fill:#e6e6ff,stroke:#333,stroke-width:2px,color:#000\n    style B fill:#ccf2ff,stroke:#333,stroke-width:2px,color:#000\n    style R fill:#ffe6e6,stroke:#333,stroke-width:2px,color:#000\n    style BS+ fill:#f2f2f2,stroke:#333,stroke-width:1px,color:#000\n    style BS- fill:#f2f2f2,stroke:#333,stroke-width:1px,color:#000\n    style RS+ fill:#f2f2f2,stroke:#333,stroke-width:1px,color:#000\n    style RS- fill:#f2f2f2,stroke:#333,stroke-width:1px,color:#000\n    \n    linkStyle default stroke:#333,stroke-width:1px\n\n\n\n\n\n\nFrom this tree, we can calculate:\n\nP(B) = \\frac{3}{5}\nP(S^+|B) = \\frac{2}{3} (probability of star given blue)\nP(B \\cap S^+) = \\frac{3}{5} \\cdot \\frac{2}{3} = \\frac{2}{5}\n\nWe can verify the conditional probability formula:\nP(S^+|B) = \\frac{P(B \\cap S^+)}{P(B)} = \\frac{2/5}{3/5} = \\frac{2}{3}\nOther probabilities from this scenario:\n\nP(R) = \\frac{2}{5}\nP(S^+|R) = \\frac{1}{2}\nP(S^+) = P(B)P(S^+|B) + P(R)P(S^+|R) = \\frac{3}{5} \\cdot \\frac{2}{3} + \\frac{2}{5} \\cdot \\frac{1}{2} = \\frac{3}{5}\n\nThis example illustrates how:\n\nThe probability tree helps visualize sequential events\nBranch probabilities multiply along paths\nThe conditional probability formula naturally emerges from the tree structure\nThe law of total probability can be visualized as summing across different paths\n\n\n\n\n\n\n\n\n\nSequential Drawing Example\n\n\n\nConsider drawing two balls from an urn containing 3 blue (B) and 2 red (R) balls without replacement. Let‚Äôs find the probability of drawing two blue balls.\n\n\n\n\n\ngraph LR\n    Œ©{Start} --&gt; B1[\"B‚ÇÅ: 3/5\"]\n    Œ© --&gt; R1[\"R‚ÇÅ: 2/5\"]\n    B1 --&gt; B2[\"B‚ÇÇ|B‚ÇÅ: 2/4\"]\n    B1 --&gt; R2[\"R‚ÇÇ|B‚ÇÅ: 2/4\"]\n    R1 --&gt; B3[\"B‚ÇÇ|R‚ÇÅ: 3/4\"]\n    R1 --&gt; R3[\"R‚ÇÇ|R‚ÇÅ: 1/4\"]\n    \n    style Œ© fill:#e6e6ff,stroke:#333,stroke-width:2px,color:#000\n    style B1 fill:#ccf2ff,stroke:#333,stroke-width:2px,color:#000\n    style R1 fill:#ffe6e6,stroke:#333,stroke-width:2px,color:#000\n    style B2 fill:#ccf2ff,stroke:#333,stroke-width:1px,color:#000\n    style R2 fill:#ffe6e6,stroke:#333,stroke-width:1px,color:#000\n    style B3 fill:#ccf2ff,stroke:#333,stroke-width:1px,color:#000\n    style R3 fill:#ffe6e6,stroke:#333,stroke-width:1px,color:#000\n    \n    linkStyle default stroke:#333,stroke-width:1px\n\n\n\n\n\n\nLet‚Äôs calculate various probabilities:\n\nTwo blue balls (B‚ÇÅ and B‚ÇÇ):\n\nP(B_1) = \\frac{3}{5} (first draw)\nP(B_2|B_1) = \\frac{2}{4} (second draw given first was blue)\nP(B_1 \\cap B_2) = \\frac{3}{5} \\cdot \\frac{2}{4} = \\frac{3}{10}\n\nBlue then Red:\n\nP(R_2|B_1) = \\frac{2}{4}\nP(B_1 \\cap R_2) = \\frac{3}{5} \\cdot \\frac{2}{4} = \\frac{3}{10}\n\nRed then Blue:\n\nP(R_1) = \\frac{2}{5}\nP(B_2|R_1) = \\frac{3}{4}\nP(R_1 \\cap B_2) = \\frac{2}{5} \\cdot \\frac{3}{4} = \\frac{3}{10}\n\nTwo red balls:\n\nP(R_2|R_1) = \\frac{1}{4}\nP(R_1 \\cap R_2) = \\frac{2}{5} \\cdot \\frac{1}{4} = \\frac{1}{10}\n\n\nKey observations:\n\nThe probability of second draw depends on first outcome (conditional probability)\nTotal probability = 1: \\frac{3}{10} + \\frac{3}{10} + \\frac{3}{10} + \\frac{1}{10} = 1\nNotice that P(B_1 \\cap R_2) = P(R_1 \\cap B_2) due to symmetry\nThe denominator changes after first draw (4 balls remain)\n\nThis example illustrates how:\n\nProbabilities update based on previous outcomes\nThe multiplication rule applies to sequential events\nSample space reduces after each draw\nOrder can matter in sequential probability calculations\n\n\n\n\n\n\n\n\n\nConditional Probability: A Geometric Perspective\n\n\n\nConditional probability answers the question: ‚ÄúGiven that we know event B has occurred, what is the probability that event A will occur?‚Äù We write this as P(A|B), read as ‚Äúthe probability of A given B.‚Äù\nThe formal definition is:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\nGeometrically, we can visualize this as:\n\nThe original sample space \\Omega represented as a rectangle\nEvent B as a region within \\Omega\nThe intersection A \\cap B as the overlap between regions A and B\nConditional probability as the ratio of the overlap area to the area of B\n\nThis visualization helps understand why we divide by P(B) - we‚Äôre essentially creating a new probability space where B is our universe.\n\n\n\n\n\n\n\n\nWhy P(A|B) ‚â† P(B|A)?\n\n\n\nImagine these two questions:\n\nWhat‚Äôs the probability it‚Äôs raining (A) given there are clouds (B)?\nWhat‚Äôs the probability there are clouds (B) given it‚Äôs raining (A)?\n\nClearly, these are different:\n\nP(A|B): Among all cloudy days, how many are rainy?\nP(B|A): Among all rainy days, how many are cloudy?\n\nP(B|A) would be close to 1 (almost all rainy days have clouds) While P(A|B) might be around 0.3 (not all cloudy days bring rain)\n\nWhen are they equal?\n\nWhen events are independent:\n\nP(A|B) = P(A) and P(B|A) = P(B)\n\nWhen events have symmetric relationship:\n\nDrawing cards: P(\\text{red}|\\text{face}) = P(\\text{face}|\\text{red})\nBoth equal \\frac{6}{26} = \\frac{3}{13}\n\nWhen applying Bayes: if P(A|B) = P(B|A), then P(A) = P(B)\n\nFrom P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\nIf P(A|B) = P(B|A), then P(A) = P(B)\n\n\n\n\n\n\n\nThe Law of Total Probability\n\n\n\n\n\n\nSample Space Partitions\n\n\n\nImagine a sample space \\Omega as a complete population where every individual must be classified into exactly one category. This is what a partition does - it divides our universe of possibilities into distinct, non-overlapping groups that together include all possibilities.\nConsider how we might partition a population:\n\nLet A_1 = ‚ÄúCategory 1‚Äù\nLet A_2 = ‚ÄúCategory 2‚Äù\nLet A_3 = ‚ÄúCategory 3‚Äù\n\nThese classifications form a partition because:\n\nComplete Coverage (\\Omega = A_1 \\cup A_2 \\cup ... \\cup A_n):\n\nEvery outcome in the sample space must belong to exactly one category\nNothing can be left unclassified\nThe categories together capture all possibilities\n\nMutual Exclusivity (A_i \\cap A_j = \\emptyset for i \\neq j):\n\nEach outcome belongs to exactly one category\nCategories cannot overlap\nBeing in one category excludes being in any other\n\n\nThis framework becomes powerful when:\n\nCalculating total probability (sum across all categories)\nUpdating beliefs with new information\nBreaking complex problems into manageable pieces\n\nThink of it like organizing a filing system: each document must go into exactly one folder (mutual exclusivity), and every document must be filed somewhere (complete coverage). When we get new information, we might need to update our filing system, but we always maintain these two key properties.\nThe power of partitioning lies in its ability to help us systematically organize possibilities and update probabilities as new information becomes available. This forms the foundation for understanding more complex concepts like the law of total probability and Bayes‚Äô theorem.\n\n\nThe law of total probability is a fundamental bridge between conditional probabilities and overall probabilities. Given a partition \\{A_1, A_2, ..., A_n\\} of the sample space, for any event B:\nP(B) = \\sum_{i=1}^n P(B|A_i)P(A_i)\nVisually, this represents:\n\nBreaking the sample space into disjoint ‚Äúslices‚Äù (the partition)\nFinding the probability of B within each slice (P(B|A_i))\nWeighting each slice by its probability (P(A_i))\nSumming all contributions\n\nExample: In a tech company:\n\n40% of employees are developers (A_1)\n35% are managers (A_2)\n25% are other roles (A_3)\n\nTo find the probability of an employee working remotely (B):\n\n80% of developers work remotely: P(B|A_1) = 0.80\n60% of managers work remotely: P(B|A_2) = 0.60\n40% of other roles work remotely: P(B|A_3) = 0.40\n\nUsing the law of total probability:\nP(B) = (0.80)(0.40) + (0.60)(0.35) + (0.40)(0.25) = 0.64\nSo 64% of all employees work remotely.\n\n\nThe Multiplication Rule\nThe conditional probability formula can be rearranged to give us the multiplication rule:\nP(A \\cap B) = P(A|B) \\cdot P(B) = P(B|A) \\cdot P(A)\nThis symmetry is crucial because it shows:\n\nWe can compute joint probabilities in two ways\nThe order of conditioning doesn‚Äôt matter\nBoth perspectives must yield the same result\n\n\n\nBayes‚Äô Theorem: From Prior to Posterior Beliefs\nLet‚Äôs derive Bayes‚Äô theorem starting from the fundamental definitions of conditional probability and using the multiplication rule.\n\nStarting Point: Conditional Probability\nThe conditional probability formula for two events A and B is:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\nSimilarly, we can write:\nP(B|A) = \\frac{P(A \\cap B)}{P(A)}\n\n\nMultiplication Rule\nFrom either of these formulas, we can derive the multiplication rule:\nP(A \\cap B) = P(B|A) \\cdot P(A) or equivalently P(A \\cap B) = P(A|B) \\cdot P(B)\n\n\nDeriving Bayes‚Äô Theorem\n\nStart with the conditional probability formula:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\nUse the multiplication rule to express the intersection:\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\nThis gives us Bayes‚Äô theorem in its basic form. The denominator P(B) can be expanded using the law of total probability:\nP(B) = P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)\nLeading to the full form:\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)}\nThis derivation shows how Bayes‚Äô theorem emerges naturally from the basic rules of probability, allowing us to ‚Äúreverse‚Äù conditional probabilities and update prior beliefs with new evidence.\nUsing the law of total probability for the denominator:\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)}\nThe formula has three key components:\n\nPrior probability: P(A) - initial belief about event A\nLikelihood: P(B|A) - probability of evidence B given A is true\nNormalizing constant: P(B) - ensures probabilities sum to 1\n\n\n\nExample 1: Playing Cards\nLet‚Äôs start with a simple example using cards:\nSuppose we draw a card and are told it‚Äôs red. What‚Äôs the probability it‚Äôs a face card?\nGiven:\n\nP(\\text{Face}) = 12/52 = 3/13 (prior)\nP(\\text{Red}|\\text{Face}) = 6/12 = 1/2 (likelihood)\nP(\\text{Red}) = 26/52 = 1/2 (normalizing constant)\n\nUsing Bayes‚Äô theorem: P(\\text{Face}|\\text{Red}) = \\frac{(1/2)(3/13)}{1/2} = \\frac{3}{13}\n\n\nExample 2: Medical Testing\nA more practical application involves medical diagnostics. Let‚Äôs formalize the terminology:\nTesting Framework:\n\nConditions:\n\nD^+: Disease present\nD^-: Disease absent\n\nTest Results:\n\nT^+: Positive test\nT^-: Negative test\n\n\nKey Metrics:\n\nSensitivity: P(T^+|D^+) - True Positive Rate\nSpecificity: P(T^-|D^-) - True Negative Rate\nPPV: P(D^+|T^+) - Positive Predictive Value\nNPV: P(D^-|T^-) - Negative Predictive Value\n\nThese relationships can be visualized in a confusion matrix:\n\n\n\n\nD^+\nD^-\n\n\n\n\nT^+\nTP\nFP\n\n\nT^-\nFN\nTN\n\n\n\nwhere:\n\nTP: True Positives\nTN: True Negatives\nFP: False Positives (Type I error)\nFN: False Negatives (Type II error)\n\nExample Calculation: Consider a test for a rare disease where:\n\nPrevalence: P(D^+) = 0.01 (1%)\nSensitivity: P(T^+|D^+) = 0.95 (95%)\nSpecificity: P(T^-|D^-) = 0.98 (98%)\n\nWhat‚Äôs the probability of having the disease given a positive test?\nUsing Bayes‚Äô theorem:\nP(D^+|T^+) = \\frac{P(T^+|D^+) \\cdot P(D^+)}{P(T^+|D^+) \\cdot P(D^+) + P(T^+|D^-) \\cdot P(D^-)}\nP(D^+|T^+) = \\frac{(0.95)(0.01)}{(0.95)(0.01) + (0.02)(0.99)} \\approx 0.32\nThis counterintuitive result (only 32% chance of disease despite a positive test) illustrates the base rate fallacy - when the condition is rare, even a highly accurate test can have a low positive predictive value.\nThis example shows why Bayes‚Äô theorem is crucial in medical decision-making, as it properly accounts for both the test‚Äôs accuracy and the disease‚Äôs prevalence in the population.\n\n\nSpam Filtering Example\nEmail spam filters are a perfect real-world application of Bayes‚Äô theorem. Let‚Äôs see how it works:\nConsider a single word ‚Äúlottery‚Äù in an email. We want to know: given that an email contains this word, what‚Äôs the probability it‚Äôs spam?\nLet‚Äôs define our events:\n\nS: Email is spam\nW: Email contains the word ‚Äúlottery‚Äù\n\nWe need:\n\nPrior probability: P(S) = 0.30 (30% of all emails are spam)\nLikelihood: P(W|S) = 0.20 (20% of spam emails contain ‚Äúlottery‚Äù)\nFalse positive rate: P(W|S') = 0.001 (0.1% of legitimate emails contain ‚Äúlottery‚Äù)\n\nUsing Bayes‚Äô theorem:\nP(S|W) = \\frac{(0.20)(0.30)}{(0.20)(0.30) + (0.001)(0.70)} \\approx 0.989\nSo if an email contains ‚Äúlottery,‚Äù there‚Äôs a 98.9% chance it‚Äôs spam!\nReal spam filters:\n\nLook at multiple words and features\nUpdate probabilities continuously based on user feedback\nCombine evidence using the multiplication rule for independent events\nUse logarithms to avoid numerical underflow with many multiplications\n\n\n\nWeather Forecasting Example\nAnother practical application is weather forecasting. Suppose we want to know if it will rain tomorrow given certain atmospheric conditions:\nLet‚Äôs define:\n\nR: It rains tomorrow\nC: Current atmospheric conditions (high pressure system)\n\nGiven:\n\nP(R) = 0.25 (25% chance of rain on any day)\nP(C|R) = 0.10 (10% of rainy days have high pressure)\nP(C|R') = 0.70 (70% of non-rainy days have high pressure)\n\nUsing Bayes‚Äô theorem:\nP(R|C) = \\frac{(0.10)(0.25)}{(0.10)(0.25) + (0.70)(0.75)} \\approx 0.045\nSo given high pressure, there‚Äôs only about a 4.5% chance of rain tomorrow.\n\n\n\nKey Interconnections and Applications\nThese concepts form a unified framework with wide-ranging applications:\n\nConditional probability provides the foundation for understanding dependent events\nThe multiplication rule enables complex probability calculations\nTotal probability helps break down complex scenarios into manageable pieces\nBayes‚Äô theorem combines these tools to update probabilities with new evidence\n\nModern Applications:\n\nMachine Learning: Naive Bayes classifiers for text categorization\nMedical Diagnosis: Interpreting test results and screening procedures\nQuality Control: Identifying defective products based on test results\nRisk Assessment: Updating risk probabilities with new information\nNatural Language Processing: Sentiment analysis and language modeling\nForensics: Evaluating evidence in legal cases\nRecommender Systems: Predicting user preferences\n\nUnderstanding these relationships helps in:\n\nChoosing the right probabilistic tool for a given problem\nBreaking complex problems into manageable pieces\nAvoiding common probability misconceptions\nMaking better decisions under uncertainty\nBuilding intuition for machine learning algorithms\n\n\n\n\nIndependent and Disjoint Events\nUnderstanding the difference between independent and disjoint events is crucial for correctly applying probability rules. The key insight is that these concepts are fundamentally different - in fact, disjoint events are always dependent (except in trivial cases).\n\nIndependent Events\nEvents A and B are independent if knowing that one occurred doesn‚Äôt affect the probability of the other occurring. Mathematically, this means any of these equivalent conditions:\n\nP(A|B) = P(A)\nP(B|A) = P(B)\nP(A \\cap B) = P(A) \\cdot P(B)\n\nExample 1: Flipping a fair coin twice\n\nLet A = ‚Äúheads on first flip‚Äù and B = ‚Äúheads on second flip‚Äù\nP(A) = \\frac{1}{2} and P(B) = \\frac{1}{2}\nP(A \\cap B) = \\frac{1}{4} = P(A) \\cdot P(B)\nTherefore, the flips are independent\n\n\n\nDisjoint (Mutually Exclusive) Events\nEvents A and B are disjoint if they cannot occur simultaneously:\nP(A \\cap B) = 0\nExample 2: Rolling a die\n\nLet A = ‚Äúrolling a 6‚Äù and B = ‚Äúrolling an odd number‚Äù\nP(A \\cap B) = 0 (can‚Äôt be both 6 and odd)\nThese events are disjoint\n\n\n\nWhy Disjoint Events are Always Dependent\nLet‚Äôs prove that disjoint events (with non-zero probabilities) must be dependent:\n\nFor disjoint events: P(A \\cap B) = 0\nFor events to be independent, we need: P(A \\cap B) = P(A) \\cdot P(B)\nTherefore, for disjoint events to be independent:\n\n0 = P(A) \\cdot P(B) (from 1 and 2)\nThis equation is only true if either P(A) = 0 or P(B) = 0\nBut if either probability is 0, the event is impossible and trivial\n\nFor any non-trivial disjoint events:\n\nAssume P(A) &gt; 0 and P(B) &gt; 0 (considering non-trivial cases)\nThen P(B|A) = \\frac{P(A \\cap B)}{P(A)} = \\frac{0}{P(A)} = 0\nSince P(B) &gt; 0 (by our assumption of non-trivial cases)\nWe have P(B|A) = 0 \\neq P(B)\nThis inequality proves the events are dependent\n\n\nThe assumption P(B) \\neq 0 is crucial because:\n\nIf we allowed P(B) = 0, then P(B|A) = P(B) would be true (both equal to 0)\nThis would mean the events are technically independent\nBut this is a trivial case where B is an impossible event\n\nExample 3: Rolling a die illustrates dependence\n\nLet A = ‚Äúrolling a 1‚Äù and B = ‚Äúrolling a 2‚Äù\nP(A) = \\frac{1}{6} and P(B) = \\frac{1}{6} (both non-zero)\nP(B|A) = 0 (if we rolled a 1, we definitely didn‚Äôt roll a 2)\nBut P(B) = \\frac{1}{6}\nTherefore P(B|A) \\neq P(B), showing dependence\n\n\n\nKey Insights\n\nIndependence means events don‚Äôt affect each other‚Äôs probabilities\nDisjoint means events can‚Äôt occur together\nThese concepts are almost opposites:\n\nIndependent events can occur together\nDisjoint events must affect each other‚Äôs probabilities\n\nThe only case where events can be both independent and disjoint is when at least one event has probability 0 (impossible event)\n\nThis understanding is crucial for correctly applying probability rules and avoiding common misconceptions in probability calculations.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#problem-solutions-2",
    "href": "probability_en.html#problem-solutions-2",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "17.13 Problem Solutions (2)",
    "text": "17.13 Problem Solutions (2)\n\nProblem 1: Colored Balls - At Least One Red\nQuestion: A bag contains 5 red and 3 blue marbles. Two marbles are drawn simultaneously from the bag. What is the probability that at least one marble is red?\nDetailed Solution:\n\nApproach 1: Using Tree Diagram and Complement Rule\nThe key idea here is that finding the probability of ‚Äúat least one red‚Äù directly can be complex, but finding its complement - ‚Äúno red balls‚Äù (all blue) - is simpler.\nThen we can use P(\\text{at least one red}) = 1 - P(\\text{no red}).\nLet‚Äôs visualize this with a diagram:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[First Draw]\n    B --&gt; C[\"Blue (3/8)\"]\n    B --&gt; D[\"Red (5/8)\"]\n    C --&gt; E[\"Blue (2/7)\"]\n    C --&gt; F[\"Red (5/7)\"]\n    D --&gt; G[\"Blue (3/7)\"]\n    D --&gt; H[\"Red (4/7)\"]\n    \n    E --&gt; I[\"P = 3/8 * 2/7&lt;br/&gt;All Blue\"]\n    F --&gt; J[\"P = 3/8 * 5/7&lt;br/&gt;At least one Red\"]\n    G --&gt; K[\"P = 5/8 * 3/7&lt;br/&gt;At least one Red\"]\n    H --&gt; L[\"P = 5/8 * 4/7&lt;br/&gt;At least one Red\"]\n\n    style I fill:#f9f9f9,stroke:#333\n    style J fill:#f9f9f9,stroke:#333\n    style K fill:#f9f9f9,stroke:#333\n    style L fill:#f9f9f9,stroke:#333\n\n\n\n\n\n\nNow let‚Äôs solve using the complement rule:\nP(\\text{at least one red}) = 1 - P(\\text{no red})\nTo get no red marbles, we need to draw both blue marbles:\n\nTotal marbles: 8\nBlue marbles: 3\nP(\\text{first blue}) = \\frac{3}{8}\nP(\\text{second blue}|\\text{first blue}) = \\frac{2}{7}\n\nP(\\text{no red}) = P(\\text{both blue}) = \\frac{3}{8} \\times \\frac{2}{7} = \\frac{6}{56} = \\frac{3}{28}\nTherefore:\nP(\\text{at least one red}) = 1 - \\frac{3}{28} = \\frac{25}{28} \\approx 0.893 or about 89.3%\n\n\nApproach 2: Using Combinations\nThe combinations approach involves finding all possible ways to select 2 marbles out of 8, then subtracting the unfavorable outcomes (selecting 2 blue marbles).\nLet‚Äôs understand combinations first:\n\nA combination represents the number of ways to select r items from n items where order doesn‚Äôt matter\nNotation: C(n,r) or \\binom{n}{r}\nFormula: C(n,r) = \\frac{n!}{r!(n-r)!}\n\nFor this problem:\n\nTotal possible outcomes = C(8,2) = 28 ways to select 2 marbles from 8\nUnfavorable outcomes = C(3,2) = 3 ways to select 2 blue marbles from 3 blue marbles\nFavorable outcomes = C(8,2) - C(3,2) = 28 - 3 = 25\n\nTherefore:\nP(\\text{at least one red}) = \\frac{25}{28} \\approx 0.893 or about 89.3%\nBoth methods give us the same result! The combinations approach is often more elegant for problems involving simultaneous selection, while the tree diagram approach helps visualize the problem better and is particularly useful when events happen in sequence.\n\n\n\nProblem 2: Probability of Drawing Diamonds or Tens\nFrom a standard deck of 52 cards, find the probability of drawing either a diamond or a ten.\n\nSetup\nLet‚Äôs define our events:\n\nLet D = ‚Äúdrawing a diamond‚Äù\nLet T = ‚Äúdrawing a ten‚Äù\n\nWe need to find P(D \\cup T)\n\n\nSolution Using the Addition Rule\n\nIndividual Probabilities\nFor diamonds:\n\nNumber of diamonds = 13\nP(D) = \\frac{13}{52} = \\frac{1}{4}\n\nFor tens:\n\nNumber of tens = 4\nP(T) = \\frac{4}{52} = \\frac{1}{13}\n\nIntersection\n\nThe ten of diamonds is counted in both events\nP(D \\cap T) = \\frac{1}{52}\n\nAddition Rule\nP(D \\cup T) = P(D) + P(T) - P(D \\cap T)\n= \\frac{13}{52} + \\frac{4}{52} - \\frac{1}{52}\n= \\frac{16}{52} - \\frac{1}{52}\n= \\frac{15}{52}\n\\approx 0.288 or about 28.8%\n\n\n\nVerification\nWe can verify this result is reasonable because:\n\nUpper Bound Check\n\nIf we simply added P(D) and P(T): \\frac{13}{52} + \\frac{4}{52} = \\frac{17}{52}\nOur answer must be less than this due to double counting\n\nLower Bound Check\n\nOur answer must be greater than the larger individual probability (\\frac{13}{52})\n\\frac{15}{52} &gt; \\frac{13}{52} ‚úì\n\n\n\n\nTeaching Notes\nThis problem illustrates several important concepts:\n\nAddition Rule Application\n\nWhy we can‚Äôt simply add probabilities\nThe role of intersection in avoiding double counting\n\nFraction Arithmetic\n\nWorking with common denominators\nSimplifying fractions (if desired)\n\nSet Theory Visualization\n\nThe problem can be illustrated with a Venn diagram\nShows why subtraction of intersection is necessary\n\nReasonableness Checks\n\nUsing bounds to verify answers\nUnderstanding why certain values are impossible\n\n\n\n\n\nProblem 3a: Introduction to Conditional Probability\nA tech company has 100 employees who work on various projects. The company records show that:\n\n60 employees work on Project A\n45 employees work on Project B\n25 employees work on both projects\n\nThe HR manager randomly selects one employee. Given that this employee works on Project A, what is the probability they also work on Project B?\n\nStep-by-Step Solution\n\nDefine Events\n\nLet A = ‚Äúemployee works on Project A‚Äù\nLet B = ‚Äúemployee works on Project B‚Äù\nWe need to find P(B|A)\n\nReview the Conditional Probability Formula\nP(B|A) = \\frac{P(A \\cap B)}{P(A)}\nIdentify Known Values\n\nTotal employees: n = 100\nNumber working on A: n_A = 60\nNumber working on B: n_B = 45\nNumber working on both: n_{A \\cap B} = 25\n\nCalculate Probabilities\n\nP(A) = \\frac{60}{100} = 0.6\nP(A \\cap B) = \\frac{25}{100} = 0.25\n\nApply the Formula\nP(B|A) = \\frac{0.25}{0.6} = \\frac{25}{60} \\approx 0.417\n\n\n\nInterpretation\nThere is about a 41.7% chance that an employee works on Project B, given that they work on Project A. In other words, among the 60 employees who work on Project A, 25 of them (41.7%) also work on Project B.\n\n\nTeaching Notes\nThis problem helps students understand:\n\nBasic Concepts\n\nThe difference between joint probability P(A \\cap B) and conditional probability P(B|A)\nWhy P(B|A) is not the same as P(A \\cap B)\nThe role of the denominator P(A) in ‚Äúrestricting the sample space‚Äù\n\nVisual Representation\n\nThe problem can be illustrated with a Venn diagram:\n\nOne circle for Project A (60)\nOne circle for Project B (45)\nOverlap shows both projects (25)\nTotal space represents all employees (100)\n\n\nCommon Misconceptions\n\nStudents often confuse P(B|A) with P(A \\cap B)\nThey might think P(B|A) = P(B)\nThey might mix up P(B|A) and P(A|B)\n\nExtensions\n\nCalculate P(A|B) for comparison\nFind the probability of working on exactly one project\nConsider what happens if projects were independent\n\n\n\n\n\nProblem 3b: Colored Balls with Replacement and Addition\nQuestion: A box contains 5 red and 3 green balls. One ball is drawn at random, its color is noted, and it is replaced back. Then one more ball of the same color is added. Then a second ball is drawn. What is the probability that both balls drawn are green?\nDetailed Solution:\nThis is a sequential probability problem where the probability of the second event depends on the outcome of the first. Let‚Äôs solve it step by step:\n\nDefine our events:\n\nG‚ÇÅ = first ball is green\nG‚ÇÇ = second ball is green\nWe want P(G‚ÇÅ ‚à© G‚ÇÇ)\n\nCalculate P(G‚ÇÅ):\n\nInitially: 3 green balls out of 8 total\nP(G_1) = \\frac{3}{8}\n\nCalculate P(G‚ÇÇ|G‚ÇÅ):\n\nIf first ball was green:\n\nAfter replacement and adding another green: 4 green balls out of 9 total\n\nP(G_2|G_1) = \\frac{4}{9}\n\nApply the multiplication rule:\nP(G_1 \\cap G_2) = P(G_1) \\cdot P(G_2|G_1) = \\frac{3}{8} \\cdot \\frac{4}{9} = \\frac{12}{72} = \\frac{1}{6} \\approx 0.167 or about 16.7%\n\nUnderstanding the Solution:\n\nThe probability is relatively low because we need two specific events to occur in sequence\nThe addition of a ball of the same color as the first draw creates a dependency between the draws\nIf we had simply replaced the first ball without adding another, the draws would have been independent\n\n\n\nProblem 4a: Bayesian Analysis of Medical Test Results\nA medical test for disease D has the following characteristics:\n\nSensitivity (true positive rate): P(T=1|D=1) = 0.95\nSpecificity (true negative rate): P(T=0|D=0) = 0.95\nPrior probability (disease prevalence): P(D=1) = 0.001 (1/1000)\nTest result for Alicia: Positive (T=1)\n\nWe need to find the posterior probability that Alicia has the disease given a positive test result: P(D=1|T=1)\n\nDerivation Using Bayes‚Äô Theorem\nStarting with the conditional probability formula:\nP(D=1|T=1) = \\frac{P(T=1|D=1)P(D=1)}{P(T=1)}\nThe denominator P(T=1) can be expanded using the law of total probability:\nP(T=1) = P(T=1|D=1)P(D=1) + P(T=1|D=0)P(D=0)\n\n\nComponents Analysis\n\nPrior: P(D=1) = 0.001\n\nComplement: P(D=0) = 0.999\n\nLikelihood:\n\nP(T=1|D=1) = 0.95 (sensitivity)\nP(T=0|D=0) = 0.95 (specificity)\nP(T=1|D=0) = 1 - P(T=0|D=0) = 0.05 (false positive rate)\n\nTotal Probability (denominator): P(T=1) = (0.95)(0.001) + (0.05)(0.999) = 0.00095 + 0.04995 = 0.0509\nPosterior Calculation:\n\nP(D=1|T=1) = \\frac{(0.95)(0.001)}{0.0509} = \\frac{0.00095}{0.0509} \\approx 0.0187\n\n\nInterpretation\nDespite receiving a positive test result, the probability that Alicia has disease D is only about 1.87%. This counterintuitive result is known as the ‚ÄúBayesian flip‚Äù or ‚Äúbase rate fallacy.‚Äù\n\n\nWhy is the Probability So Low?\n\nBase Rate Consideration:\n\nThe very low prevalence (1/1000) means that in a population of 1000 women:\n\n1 woman has the disease\n999 women don‚Äôt have the disease\n\n\nTest Results in Population:\n\nOf the 1 woman with disease:\n\n0.95 will test positive (true positive)\n\nOf the 999 women without disease:\n\nAbout 50 will test positive (false positives)\n\n\nRatio Analysis:\n\nAmong all positive tests (‚âà51), only about 1 is a true positive\nThis explains why P(D=1|T=1) is so low\n\n\n\n\nTeaching Notes\nThis problem illustrates several important concepts:\n\nThe distinction between conditional probabilities:\n\nP(T=1|D=1) (sensitivity)\nP(D=1|T=1) (positive predictive value)\n\nThe crucial role of base rates in Bayesian reasoning\nWhy medical professionals should:\n\nConsider prevalence when interpreting test results\nBe cautious about testing asymptomatic patients\nConsider confirmatory testing for positive results\n\nThe importance of communicating probabilistic information effectively to patients\nThe mathematical relationship between:\n\nPrior probabilities\nTest characteristics (sensitivity/specificity)\nPosterior probabilities\n\n\n\n\n\nProblem 4b: COVID-19 Test Analysis\nQuestion: Given a COVID-19 test with:\n\nSensitivity (P(T=1|D=1)) = 87.5%\nSpecificity (P(T=0|D=0)) = 97.5%\nDisease prevalence (P(D=1)) = 10% Find P(D=1|T=1), the probability that a person with a positive test actually has the disease.\n\nDetailed Solution:\nThis is a perfect application of Bayes‚Äô Theorem. Let‚Äôs break it down:\n\nDefine our variables:\n\nD=1: Person has COVID-19\nD=0: Person doesn‚Äôt have COVID-19\nT=1: Test is positive\nT=0: Test is negative\n\nGiven information:\n\nP(T=1|D=1) = 0.875 (sensitivity)\nP(T=0|D=0) = 0.975 (specificity)\nP(D=1) = 0.1 (prevalence)\n\nCalculate additional probabilities:\n\nP(D=0) = 1 - P(D=1) = 0.9\nP(T=1|D=0) = 1 - P(T=0|D=0) = 0.025 (false positive rate)\n\nApply Bayes‚Äô Theorem: P(D=1|T=1) = \\frac{P(T=1|D=1) \\cdot P(D=1)}{P(T=1)}\nCalculate P(T=1) using the law of total probability: P(T=1) = P(T=1|D=1)P(D=1) + P(T=1|D=0)P(D=0) = (0.875)(0.1) + (0.025)(0.9) = 0.0875 + 0.0225 = 0.11\nNow we can complete Bayes‚Äô Theorem: P(D=1|T=1) = \\frac{(0.875)(0.1)}{0.11} = \\frac{0.0875}{0.11} \\approx 0.795 or about 79.5%\n\nUnderstanding the Result:\nThis result tells us that even with a positive test, there‚Äôs still about a 20.5% chance that the person doesn‚Äôt have COVID-19. This might seem surprising, but it‚Äôs due to the relatively low prevalence of the disease (10%) in the population. This is known as the base rate fallacy - even a test with good sensitivity and specificity can have a significant false positive rate when the condition being tested for is rare.\n\n\nProblem 5: Conditional Probability: Marble Drawing with Coin Flip\nWe have a probability experiment involving two boxes of marbles and a fair coin:\nBox X1:\n\n2 black marbles\n3 red marbles\nTotal: 5 marbles\n\nBox X2:\n\n1 black marble\n1 red marble\nTotal: 2 marbles\n\nA fair coin is flipped to select the box (heads for X1, tails for X2), then one marble is drawn.\nVisual Representation\nLet‚Äôs create a tree diagram to visualize all possible outcomes and their probabilities:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[X1 1/2]\n    A --&gt; C[X2 1/2]\n    \n    B --&gt; D[Black 2/5]\n    B --&gt; E[Red 3/5]\n    \n    C --&gt; F[Black 1/2]\n    C --&gt; G[Red 1/2]\n    \n    D --&gt; H[Black & X1]\n    E --&gt; I[Red & X1]\n    F --&gt; J[Black & X2]\n    G --&gt; K[Red & X2]\n\n\n\n\n\n\n\nSolution\nLet‚Äôs solve each part step by step:\n\n\nP(Black | X1)\nThis is the probability of drawing a black marble given that we selected Box X1.\nP(Black | X1) = \\frac{\\text{Number of black marbles in X1}}{\\text{Total marbles in X1}} = \\frac{2}{5}\nThis is a direct probability from the contents of Box X1. We only consider Box X1‚Äôs marbles since we‚Äôre given that Box X1 was selected.\n\n\nP(Black and X1)\nThis is the probability of both selecting Box X1 and drawing a black marble.\nP(Black and X1) = P(X1) √ó P(Black | X1) = \\frac{1}{2} \\times \\frac{2}{5} = \\frac{1}{5}\nWe multiply these probabilities because both events must occur (intersection).\n\n\nP(Black)\nThis is the total probability of drawing a black marble from either box. We use the law of total probability:\nP(Black) = P(X1) √ó P(Black | X1) + P(X2) √ó P(Black | X2) = \\frac{1}{2} \\times \\frac{2}{5} + \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{5} + \\frac{1}{4} = \\frac{4}{20} + \\frac{5}{20} = \\frac{9}{20}\n\n\nP(X1 | Black)\nThis is the probability that we selected Box X1 given that we drew a black marble. We use Bayes‚Äô Theorem:\nP(X1 | Black) = \\frac{P(Black | X1) \\times P(X1)}{P(Black)} = \\frac{\\frac{2}{5} \\times \\frac{1}{2}}{\\frac{9}{20}} = \\frac{\\frac{1}{5}}{\\frac{9}{20}} = \\frac{4}{9}\n\n\nKey Concepts Demonstrated\n\nConditional Probability: Shown in P(Black | X1), where we consider probability within a subset of outcomes\nMultiplication Rule: Used in finding P(Black and X1), where we multiply probabilities of sequential events\nLaw of Total Probability: Applied in finding P(Black), where we consider all possible ways an event can occur\nBayes‚Äô Theorem: Used to find P(X1 | Black), reversing the direction of conditioning\n\n\n\n\nProblem 6: Probability of Intersecting Events and Independence Analysis\nYou roll a fair die. What is the probability of getting an even number (A) and the number greater or equal to 4 (B)? Are events A and B independent?\nLet‚Äôs explore this problem by first understanding what each event means, then calculating their probabilities both separately and together, and finally examining their independence.\n\nUnderstanding the Events\nLet‚Äôs first identify what numbers satisfy each condition on a standard six-sided die:\nEvent A (Even numbers): {2, 4, 6} Event B (Numbers ‚â• 4): {4, 5, 6}\nWe can visualize this using a Venn diagram:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[\"R (3/6)\"]\n    A --&gt; C[\"B (2/6)\"]\n    A --&gt; D[\"Y (1/6)\"]\n    \n    B --&gt; E[\"B (2/5)\"]\n    B --&gt; F[\"Y (1/5)\"]\n    B --&gt; G[\"R (2/5)\"]\n    \n    C --&gt; H[\"R (3/5)\"]\n    C --&gt; I[\"Y (1/5)\"]\n    C --&gt; J[\"B (1/5)\"]\n    \n    D --&gt; K[\"R (3/5)\"]\n    D --&gt; L[\"B (2/5)\"]\n    D --&gt; M[\"Y (0/5)\"]\n    \n    E --&gt; N[\"RB (Success)\"]\n    F --&gt; O[\"RY (Success)\"]\n    G --&gt; P[\"RR (Fail)\"]\n    H --&gt; Q[\"BR (Success)\"]\n    I --&gt; R[\"BY (Success)\"]\n    J --&gt; S[\"BB (Fail)\"]\n    K --&gt; T[\"YR (Success)\"]\n    L --&gt; U[\"YB (Success)\"]\n    M --&gt; V[\"YY (Fail)\"]\n\n\n\n\n\n\n\n\nCalculating P(A ‚à© B)\nTo find the probability of getting both an even number AND a number greater than or equal to 4:\n\nFirst, let‚Äôs identify the numbers that satisfy both conditions:\n\nMust be even AND ‚â• 4\nNumbers that satisfy both: {4, 6}\n\nTherefore: P(A ‚à© B) = \\frac{\\text{number of favorable outcomes}}{\\text{total number of possible outcomes}} = \\frac{2}{6} = \\frac{1}{3}\n\n\n\nTesting for Independence\nTo determine if events A and B are independent, we need to check if: P(A ‚à© B) = P(A) √ó P(B)\nLet‚Äôs calculate each probability:\n\nP(A) = P(even number) = \\frac{3}{6} = \\frac{1}{2}\n\nFavorable outcomes: {2, 4, 6}\n\nP(B) = P(number ‚â• 4) = \\frac{3}{6} = \\frac{1}{2}\n\nFavorable outcomes: {4, 5, 6}\n\nP(A) √ó P(B) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}\nCompare:\n\nP(A ‚à© B) = \\frac{1}{3}\nP(A) √ó P(B) = \\frac{1}{4}\n\n\nSince \\frac{1}{3} \\neq \\frac{1}{4}, events A and B are NOT independent.\n\n\nUnderstanding the Meaning of Dependence\nThis dependence makes intuitive sense because:\n\nKnowing a number is even affects the probability it‚Äôs ‚â• 4\nIf we know we rolled an even number, there are three possibilities (2, 4, 6)\nWithin these possibilities, the probability of getting ‚â• 4 is \\frac{2}{3}, not \\frac{1}{2}\n\nThis illustrates an important principle: events can be dependent even when they don‚Äôt seem directly related. The overlap in their outcome spaces creates a subtle but measurable dependence.\n\n\nTeaching Extension\nTo deepen understanding, consider this question: How would the independence calculation change if we used ‚Äúnumbers less than 4‚Äù instead of ‚Äúnumbers greater than or equal to 4‚Äù? This variation helps illustrate how the structure of event spaces influences their independence.\n\n\n\nProblem 7: The Monty Hall Problem - Two Solution Approaches\nLet‚Äôs analyze this fascinating probability problem that has puzzled many people, including mathematicians. We‚Äôll solve it using both a tree diagram and conditional probability to build a complete understanding.\n\nProblem Statement\nThe Monty Hall problem:\n\nThere are three doors: behind one is a car, behind the others are goats\nYou pick a door\nMonty Hall (who knows what‚Äôs behind each door) opens another door, always showing a goat\nYou‚Äôre offered the chance to switch to the remaining door\nQuestion: Should you switch? What‚Äôs the probability of winning if you switch vs.¬†if you stay?\n\n\n\nApproach 1: Tree Diagram Solution\nLet‚Äôs visualize all possible scenarios:\n\n\n\n\n\ngraph TD\n    A[Initial Choice] --&gt; B[Car 1/3]\n    A --&gt; C[Goat1 1/3]\n    A --&gt; D[Goat2 1/3]\n    \n    B --&gt; E[Monty Shows Goat2]\n    B --&gt; F[Monty Shows Goat1]\n    \n    C --&gt; G[Monty Must Show Goat2]\n    D --&gt; H[Monty Must Show Goat1]\n    \n    E --&gt; I[Switch loses]\n    F --&gt; J[Switch loses]\n    G --&gt; K[Switch wins]\n    H --&gt; L[Switch wins]\n\n    style I fill:#ffcccc\n    style J fill:#ffcccc\n    style K fill:#ccffcc\n    style L fill:#ccffcc\n\n\n\n\n\n\nAnalyzing the outcomes: 1. If you initially picked the car (1/3 chance): - Monty can show either goat - Switching loses\n\nIf you initially picked a goat (2/3 chance):\n\nMonty must show the other goat\nSwitching wins\n\n\nTherefore:\n\nP(win if stay) = \\frac{1}{3}\nP(win if switch) = \\frac{2}{3}\n\n\n\nApproach 2: Conditional Probability Solution\nLet‚Äôs use Bayes‚Äô Theorem to solve this. Define events:\n\nC‚ÇÅ: Car is behind Door 1 (your initial choice)\nM‚ÇÇ: Monty opens Door 2 showing a goat\n\nP(Car behind Door 3 | Monty opens Door 2) = ?\nWe can write: P(Car in 3 | M‚ÇÇ) = \\frac{P(M‚ÇÇ|Car in 3) \\times P(Car in 3)}{P(M‚ÇÇ)}\nLet‚Äôs calculate each term:\n\nP(Car in 3) = \\frac{1}{3} (prior probability)\nP(M‚ÇÇ|Car in 3) = 1 (Monty must open Door 2)\nP(M‚ÇÇ) = P(M‚ÇÇ|Car in 1) √ó P(Car in 1) + P(M‚ÇÇ|Car in 2) √ó P(Car in 2) + P(M‚ÇÇ|Car in 3) √ó P(Car in 3) = \\frac{1}{2} \\times \\frac{1}{3} + 0 \\times \\frac{1}{3} + 1 \\times \\frac{1}{3} = \\frac{1}{6} + \\frac{1}{3} = \\frac{1}{2}\n\nTherefore:\nP(Car in 3 | M‚ÇÇ) = \\frac{1 \\times \\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}\n\n\nKey Insights\n\nWhy Intuition Fails:\n\nPeople often think it‚Äôs 50-50 after Monty opens a door\nThis ignores the crucial fact that Monty‚Äôs choice is informed, not random\nHis action provides information that should update our probabilities\n\nInformation Value:\n\nMonty‚Äôs choice is constrained (must show a goat)\nThis constraint carries information\nThe probability shifts from the initial \\frac{1}{3} to \\frac{2}{3} for switching\n\nSimulation Verification: We could write a simple program to simulate this game thousands of times, and it would confirm these probabilities. The most convincing evidence is often seeing the results empirically.\n\n\n\n\nProblem 8: The Bertrand Box Paradox - A Teaching Analysis\n\nUnderstanding the Problem Setup\nFirst, let‚Äôs clearly state what we‚Äôre dealing with:\n\nWe have three boxes:\n\nBox 1: Contains two gold coins (GG)\nBox 2: Contains two silver coins (SS)\nBox 3: Contains one gold and one silver coin (GS)\n\nThe process:\n\nWe randomly select a box\nWe randomly draw one coin from the chosen box\nIf we see a gold coin, what‚Äôs the probability it came from the gold-only box?\n\n\nMost people intuitively answer \\frac{1}{2}, but let‚Äôs discover why this isn‚Äôt correct.\n\n\nApproach 1: Tree Diagram Analysis\nLet‚Äôs visualize all possible paths and outcomes:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[Box GG 1/3]\n    A --&gt; C[Box SS 1/3]\n    A --&gt; D[Box GS 1/3]\n    \n    B --&gt; E[Draw G 1]\n    C --&gt; F[Draw S 1]\n    D --&gt; G[Draw G 1/2]\n    D --&gt; H[Draw S 1/2]\n    \n    E --&gt; I[Saw Gold]\n    G --&gt; I[Saw Gold]\n    F --&gt; J[Saw Silver]\n    H --&gt; J[Saw Silver]\n    \n    style I fill:#FFD700\n    style J fill:#C0C0C0\n\n\n\n\n\n\nFollowing the paths where we see gold:\n\nFrom Box GG (probability = \\frac{1}{3} \\times 1 = \\frac{1}{3})\nFrom Box GS (probability = \\frac{1}{3} \\times \\frac{1}{2} = \\frac{1}{6})\n\nTherefore:\n\nTotal probability of seeing gold = \\frac{1}{3} + \\frac{1}{6} = \\frac{1}{2}\nGiven we saw gold, probability it came from Box GG = \\frac{\\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}\n\n\n\nApproach 2: Bayes‚Äô Theorem Solution\nLet‚Äôs solve this formally using Bayes‚Äô Theorem:\nP(Box GG | Gold) = \\frac{P(Gold|Box GG) \\times P(Box GG)}{P(Gold)}\nLet‚Äôs calculate each component:\n\nP(Gold|Box GG) = 1 (certainty of drawing gold)\nP(Box GG) = \\frac{1}{3} (equal box probabilities)\nP(Gold) = \\frac{1}{3} \\times 1 + \\frac{1}{3} \\times 0 + \\frac{1}{3} \\times \\frac{1}{2} = \\frac{1}{2}\n\nPutting it together:\nP(Box GG | Gold) = \\frac{1 \\times \\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}\n\n\nWhy This Is Counterintuitive\nThe reason many people get this wrong reveals interesting aspects of how we think about probability:\n\nThe Setup Trick: People often think, ‚ÄúIf I see gold, it must be from either Box GG or Box GS, so it‚Äôs 50-50.‚Äù This ignores the fact that Box GG has twice the opportunity to show gold.\nPrior vs Posterior: The problem shows how observing evidence (seeing gold) updates our prior probability (\\frac{1}{3}) to a posterior probability (\\frac{2}{3}).\nSample Space Structure: Box GG contributes more gold coins to the total sample space of possible draws than Box GS does.\n\n\n\nA Teaching Analogy\nThink of it this way: Imagine three people named GG, SS, and GS.\n\nGG always raises both hands when asked\nSS never raises hands\nGS raises one hand\n\nIf you see a raised hand randomly, it‚Äôs more likely to belong to GG (who contributes two hands) than GS (who contributes only one).\n\n\nExtension for Deeper Understanding\nTo reinforce this concept, consider: How would the probabilities change if we had:\n\nThree coins in each box?\nDifferent prior probabilities for selecting each box?\nThe ability to see both coins but only after selecting a box?",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#appendix-1.-advanced-counting-in-probability-a-student-guide",
    "href": "probability_en.html#appendix-1.-advanced-counting-in-probability-a-student-guide",
    "title": "17¬† Introduction to (Discrete) Probability",
    "section": "17.14 Appendix 1. Advanced Counting in Probability: A Student Guide (*)",
    "text": "17.14 Appendix 1. Advanced Counting in Probability: A Student Guide (*)\n\nPoker Hands: A Window into Complex Counting\nPoker hands provide some of the most interesting examples for understanding counting in probability. They‚Äôre perfect for learning because they combine multiple counting principles and help us understand common pitfalls. Let‚Äôs explore these concepts step by step.\n\n\nUnderstanding Our Sample Space\nBefore we dive into specific hands, let‚Äôs understand what we‚Äôre working with. A poker hand consists of 5 cards drawn from a standard 52-card deck. Understanding the sample space is crucial because it forms the foundation of all our probability calculations.\nThe total number of possible poker hands represents how many different ways we can select 5 cards from 52 cards, where the order doesn‚Äôt matter (getting ace-king-queen is the same hand as getting king-queen-ace), we can‚Äôt reuse cards (we can‚Äôt have the ace of spades twice in our hand), and we must take exactly 5 cards (not more, not less).\nThis means we‚Äôre dealing with combinations. Let‚Äôs calculate this step by step:\n\\binom{52}{5} = \\frac{52!}{5!(52-5)!} = \\frac{52!}{5!(47)!} = \\frac{52 \\cdot 51 \\cdot 50 \\cdot 49 \\cdot 48}{5 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1} = 2,598,960\nThis number, 2,598,960, will be our denominator for calculating the probability of any specific poker hand.\n\n\nUnderstanding Two Pairs: A Careful Counting Approach\nTwo pairs is one of the most interesting hands for understanding counting principles. To get two pairs, we need:\n\nTwo cards of one rank\nTwo cards of another rank\nOne card of a third rank (the kicker)\n\nLet‚Äôs build this hand step by step, being careful to understand each choice we make:\nFirst, let‚Äôs select our ranks. We might think we should just choose two ranks from 13 for our pairs using \\binom{13}{2}, but this approach hides some important subtleties. Instead, let‚Äôs think about the actual process of constructing the hand:\n\nWe have 13 possible ranks for our first pair\nAfter choosing the first pair‚Äôs rank, we have 12 ranks left for our second pair\nAfter choosing both pair ranks, we have 11 ranks left for our kicker\n\nFor each rank we‚Äôve chosen, we need to select specific cards:\n\nFor our first pair: we choose 2 cards from the 4 available cards of that rank: \\binom{4}{2} = 6 ways\nFor our second pair: again \\binom{4}{2} = 6 ways\nFor our kicker: we choose 1 card from 4: \\binom{4}{1} = 4 ways\n\nNow, here‚Äôs where many students get confused: Does it matter which pair we count ‚Äúfirst‚Äù and which we count ‚Äúsecond‚Äù? The answer reveals a deep truth about counting in probability.\nLet‚Äôs use a concrete example. Suppose we want two pairs with Aces and Kings, and a Two as our kicker. We could:\n\nChoose Aces as our first pair, then Kings as our second pair\nChoose Kings as our first pair, then Aces as our second pair\n\nThese lead to the exact same hand type, but we need to count both paths to this hand because they represent different ways of constructing it. It‚Äôs similar to how we can make a sandwich by putting either cheese slice on first - the order of construction matters for counting all possibilities, even though the final sandwich is the same.\nThis is why our final formula multiplies all these independent choices:\n13 (first pair rank) √ó 12 (second pair rank) √ó 11 (kicker rank) √ó \\binom{4}{2} (first pair cards) √ó \\binom{4}{2} (second pair cards) √ó \\binom{4}{1} (kicker card)\nEach term represents a separate decision we make in constructing the hand. While the order of these decisions doesn‚Äôt affect the final hand we get, we need to account for all possible ways to arrive at each hand to get the correct total.\nLet‚Äôs calculate the total probability:\nP(\\text{two pairs}) = \\frac{13 \\cdot 12 \\cdot 11 \\cdot \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}}{2,598,960} = \\frac{123,552}{2,598,960} \\approx 0.0475\nThis means about 4.75% of all possible poker hands are two pairs.\n\n\nUnderstanding Full House: A Different Counting Challenge\nA full house gives us a perfect contrast to two pairs. While both hands involve multiple cards of the same rank, the counting process reveals important differences in how we approach probability problems.\nIn a full house, we need: - Three cards of one rank (called ‚Äúthree of a kind‚Äù) - Two cards of another rank (a pair)\nLet‚Äôs think about why counting a full house is different from counting two pairs. With two pairs, we had to be careful about the order of selecting our pairs. With a full house, we have a natural order: we must choose our three of a kind first (because it‚Äôs distinct from the pair), then choose our pair.\nLet‚Äôs count step by step:\n\nFor the three of a kind:\n\nChoose the rank: 13 possible ranks\nChoose which three cards of that rank: \\binom{4}{3} = 4 ways\n\nFor the pair:\n\nChoose the rank: 12 remaining ranks\nChoose which two cards of that rank: \\binom{4}{2} = 6 ways\n\n\nMultiplying these together:\n13 (three of a kind rank) √ó \\binom{4}{3} (specific three cards) √ó 12 (pair rank) √ó \\binom{4}{2} (specific pair cards)\n= 13 \\cdot 4 \\cdot 12 \\cdot 6 = 3,744\nTherefore:\nP(\\text{full house}) = \\frac{3,744}{2,598,960} \\approx 0.0014\nAbout 0.14% of all poker hands are full houses, making them significantly rarer than two pairs (4.75%). This makes intuitive sense - it‚Äôs harder to get three of the same rank plus a pair than to get two pairs plus a kicker.\n\n\nThe Birthday Problem: A Beautiful Probability Surprise\nThe birthday problem provides a fascinating connection to our poker probability work, while teaching us something profound about the nature of counting. The classic question is: ‚ÄúHow many people need to be in a room for there to be a 50% chance that at least two share a birthday?‚Äù\nMost people guess around 183 (half of 365), but the actual answer is just 23 people! Let‚Äôs understand why this connects to our previous counting work and why the answer is so surprising.\nFirst, let‚Äôs think about what makes this problem different from our poker calculations:\n\nIn poker, we were looking for specific combinations (like two pairs)\nIn the birthday problem, we‚Äôre looking for any match at all\n\nThis is similar to the difference between asking: - ‚ÄúWhat‚Äôs the probability of drawing the ace of spades and king of hearts specifically?‚Äù - ‚ÄúWhat‚Äôs the probability of drawing any two cards of different ranks?‚Äù\nThe second question has many more ways to succeed.\nLet‚Äôs solve the birthday problem step by step:\n\nFirst, it‚Äôs easier to calculate the probability of no matches\nThen we can subtract from 1 to get the probability of at least one match\n\nFor 23 people, we calculate no matches like this: - First person can have any birthday: \\frac{365}{365} - Second person needs a different birthday: \\frac{364}{365} - Third person needs a different birthday: \\frac{363}{365} And so on until person 23.\nThis gives us:\nP(\\text{no matches}) = \\frac{365}{365} \\cdot \\frac{364}{365} \\cdot \\frac{363}{365} \\cdot ... \\cdot \\frac{343}{365}\n= \\frac{365!}{(365-23)! \\cdot 365^{23}} \\approx 0.492\nTherefore:\nP(\\text{at least one match}) = 1 - 0.492 \\approx 0.508\nThis teaches us something profound about probability: when we‚Äôre looking for any match among many possibilities (like in the birthday problem), we often get much higher probabilities than when we‚Äôre looking for specific matches (like in poker hands).\n\n\nLottery Mathematics\nLet‚Äôs apply everything we‚Äôve learned to understand lottery probabilities. Consider a typical ‚Äú6/49‚Äù lottery where players choose 6 numbers from 1-49. This gives us a perfect opportunity to apply our counting principles in a real-world context.\nThe fundamental question is: What‚Äôs the probability of winning the jackpot (matching all 6 numbers)?\nThis is a combination problem because: - Order doesn‚Äôt matter (matching 1-2-3-4-5-6 is the same as matching 6-5-4-3-2-1) - We can‚Äôt use the same number twice - We need exactly 6 numbers\nTherefore:\nP(\\text{jackpot}) = \\frac{1}{\\binom{49}{6}} = \\frac{1}{13,983,816}\nThis tiny probability (about 0.0000000715) shows why lottery wins are so rare. But modern lotteries have multiple prize tiers, which gives us a chance to explore more interesting probability calculations.\nConsider matching 5 numbers plus a bonus number. For this, we need to: 1. Match 5 of the 6 winning numbers: \\binom{6}{5} ways to choose which 5 2. Match 1 of the remaining 43 numbers with the bonus: \\binom{43}{1} ways\nTherefore:\nP(\\text{5 + bonus}) = \\frac{\\binom{6}{5} \\cdot \\binom{43}{1}}{\\binom{49}{6}} = \\frac{6 \\cdot 43}{13,983,816} \\approx 0.0000184\nThis shows us how breaking down complex probability problems into simpler parts helps us solve them systematically.\n\n\nAppendix 2. Alternative Approaches to Poker Hand Probabilities (*)\nUnderstanding different ways to calculate the same probability deepens our insight into counting principles. Let‚Äôs explore several methods for finding the probabilities of two pairs and full house, seeing how each approach highlights different aspects of the problem.\n\nMultiple Paths to Two Pairs Probability\nLet‚Äôs start with two pairs. We‚Äôve seen one method, but there are several valid approaches:\nMethod 1: Sequential Selection (Our Original Approach) We build the hand step by step: 1. Choose first pair‚Äôs rank: 13 ways 2. Choose second pair‚Äôs rank: 12 ways 3. Choose kicker‚Äôs rank: 11 ways 4. Choose specific cards for first pair: \\binom{4}{2} ways 5. Choose specific cards for second pair: \\binom{4}{2} ways 6. Choose specific card for kicker: \\binom{4}{1} ways\nThis gives us: P(\\text{two pairs}) = \\frac{13 \\cdot 12 \\cdot 11 \\cdot \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}}{2,598,960}\nMethod 2: Complementary Counting We can find two pairs probability by subtracting the probability of all other possible hands from 1. However, this is more complex than direct counting because we need to know the probabilities of all other poker hands. Still, it serves as a good verification:\nP(\\text{two pairs}) = 1 - P(\\text{high card}) - P(\\text{one pair}) - P(\\text{three of a kind}) - P(\\text{straight}) - P(\\text{flush}) - P(\\text{full house}) - P(\\text{four of a kind}) - P(\\text{straight flush})\nMethod 3: Using Permutations with Adjustment We can use permutations and then adjust for overcounting:\n\nChoose an ordered arrangement of two ranks for pairs: P(13,2) = 13 \\cdot 12\nChoose kicker rank: 11 ways\nChoose specific cards for pairs and kicker: \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}\nDivide by 2 to account for the fact that the order of pairs doesn‚Äôt matter\n\nThis gives: P(\\text{two pairs}) = \\frac{P(13,2) \\cdot 11 \\cdot \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}}{2 \\cdot 2,598,960}\nMethod 4: Combination-Based Approach with Multiplication Principle We can separate rank selection from card selection:\n\nFirst, select three ranks: \\binom{13}{3} ways\nFrom these three ranks, designate two for pairs and one for kicker: \\binom{3}{2} ways\nFor each pair rank, select two cards: \\binom{4}{2} \\cdot \\binom{4}{2} ways\nFor the kicker rank, select one card: \\binom{4}{1} ways\n\nThis gives us: P(\\text{two pairs}) = \\frac{\\binom{13}{3} \\cdot \\binom{3}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}}{2,598,960}\n\n\nAlternative Approaches to Full House Probability\nThe full house probability can also be calculated in several ways:\nMethod 1: Direct Sequential Selection (Our Original Approach) 1. Choose rank for three of a kind: 13 ways 2. Choose specific three cards: \\binom{4}{3} ways 3. Choose rank for pair: 12 ways 4. Choose specific two cards: \\binom{4}{2} ways\nLeading to: P(\\text{full house}) = \\frac{13 \\cdot \\binom{4}{3} \\cdot 12 \\cdot \\binom{4}{2}}{2,598,960}\nMethod 2: Using Combinations with Distribution We can think about it as: 1. Choose two ranks from 13: \\binom{13}{2} ways 2. Designate which rank gets three cards: 2 ways (since either rank could be the three of a kind) 3. Choose specific cards: \\binom{4}{3} \\cdot \\binom{4}{2} ways\nThis gives: P(\\text{full house}) = \\frac{\\binom{13}{2} \\cdot 2 \\cdot \\binom{4}{3} \\cdot \\binom{4}{2}}{2,598,960}\nMethod 3: Using the Multiplication Principle with Sets Think about constructing the hand as selecting two sets of cards: 1. First set: three cards of the same rank from 13 ranks - Choose rank: 13 ways - Choose three cards: \\binom{4}{3} ways 2. Second set: two cards of the same rank from 12 remaining ranks - Choose rank: 12 ways - Choose two cards: \\binom{4}{2} ways\nThis yields the same result: P(\\text{full house}) = \\frac{13 \\cdot \\binom{4}{3} \\cdot 12 \\cdot \\binom{4}{2}}{2,598,960}\nEach method illuminates different aspects of the counting process: - Sequential selection helps us understand the step-by-step construction of hands - Combination-based approaches highlight the underlying structure of the selections - Permutation-based methods with adjustment show how overcounting can be handled systematically\nThe fact that all these methods yield the same result serves as a powerful verification tool. When solving complex probability problems, being able to approach the solution in multiple ways not only confirms our answer but also deepens our understanding of the underlying counting principles.\n\n\n\nAppendix 3: Occupancy Problems and Statistical Physics (*)\nUnderstanding how objects can be distributed into containers forms the foundation for both probability theory and statistical mechanics. Let‚Äôs explore this connection, starting with basic counting principles and building up to physical applications.\n\nThe Basic Occupancy Problem\nImagine we have n identical balls and k distinct boxes. How many ways can we distribute the balls? This simple question leads us to three fundamentally different scenarios that mirror important physical systems:\n\nUnrestricted occupancy (Bose-Einstein statistics)\n\nEach box can hold any number of balls\nThe balls are indistinguishable\nLike photons in quantum states\n\nMaximum one per box (Fermi-Dirac statistics)\n\nEach box can hold at most one ball\nThe balls are indistinguishable\nLike electrons in atomic orbitals\n\nAll arrangements count separately (Maxwell-Boltzmann statistics)\n\nEach box can hold any number of balls\nThe balls are distinguishable\nLike classical gas molecules\n\n\n\n\nStars and Bars: Understanding Unrestricted Occupancy\nLet‚Äôs start with the Bose-Einstein case. The ‚Äústars and bars‚Äù method provides a beautiful way to visualize and count these arrangements.\nImagine n=5 balls and k=3 boxes. We can represent any arrangement as a sequence of stars and bars:\n\nStars (*) represent balls\nBars (|) separate different boxes\n\nFor example:\n\n** | ** | * represents 2 balls in first box, 2 in second, 1 in third\n***** | | represents all 5 balls in first box, none in others\n| ***** | represents all 5 balls in middle box\n\nThe key insight is that we need: - n stars (one for each ball) - k-1 bars (to create k sections)\nTherefore, we‚Äôre really just choosing positions for the k-1 bars among n+(k-1) total positions. This gives us:\n\\text{Number of arrangements} = \\binom{n+k-1}{k-1} = \\binom{n+k-1}{n}\n\n\nFrom Counting to Physics\nNow let‚Äôs see how these counting principles reveal deep physical truths:\n\nBose-Einstein Statistics (Unrestricted, Indistinguishable)\n\nThink of photons in a laser\nMany particles can occupy same energy state\nTotal arrangements: \\binom{n+k-1}{k-1}\nExample: Light in a cavity\n\nFermi-Dirac Statistics (Restricted, Indistinguishable)\n\nThink of electrons in atoms\nMaximum one particle per state\nTotal arrangements: \\binom{k}{n} if n \\leq k, 0 otherwise\nExample: Electron configuration in atoms\n\nMaxwell-Boltzmann Statistics (Classical, Distinguishable)\n\nThink of gas molecules\nParticles are distinct\nTotal arrangements: k^n\nExample: Air molecules in a room\n\n\n\n\nAn Intuitive Bridge to Physics\nTo understand why these statistics matter, consider three real scenarios:\n\nPhotons in a Laser (Bose-Einstein) Imagine shining a laser into a mirror cavity. Photons are happy to bunch together in the same quantum state - they‚Äôre ‚Äúsocial particles.‚Äù This is why lasers can produce intense, coherent light.\nElectrons in an Atom (Fermi-Dirac) Electrons are ‚Äúantisocial‚Äù - they refuse to share quantum states (Pauli exclusion principle). This explains atomic structure and why matter is mostly empty space.\nGas Molecules in a Room (Maxwell-Boltzmann) Air molecules bounce around randomly, and we can tell them apart (in principle). This gives us the familiar gas laws and diffusion.\n\n\n\nThe Power of the Star and Bars Method\nThe stars and bars visualization helps us understand more complex problems. For instance, if we have restrictions on box occupancy:\n\nAt least one ball per box:\n\nFirst put one ball in each box\nThen distribute remaining balls freely\nFormula: \\binom{n-k+k-1}{k-1} = \\binom{n-1}{k-1}\n\nMaximum capacity per box:\n\nUse inclusion-exclusion principle\nSubtract arrangements that violate constraints\nMore complex but same underlying principle\n\n\n\n\nConnection to Partition Problems\nThis same framework helps us solve other important problems:\n\nInteger Partitions How many ways can we write n as a sum of positive integers?\n\nLike distributing n balls into unlimited boxes\nEach box represents a different term in the sum\n\nCompositions How many ways can we write n as an ordered sum?\n\nLike distinguishable boxes\nOrder matters here\n\n\nThis connection between simple counting and profound physical phenomena shows the deep unity of mathematics and physics. The same principles that help us count poker hands and lottery combinations govern the behavior of the universe at its most fundamental level.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "rv_pdf_en.html",
    "href": "rv_pdf_en.html",
    "title": "19¬† Random Variables and Probability Distributions",
    "section": "",
    "text": "19.2 Random Variables: Making Outcomes Measurable\nA random variable is a way to assign numbers to the outcomes of a random experiment. Think of it as a function that converts outcomes into numbers, making them measurable and easier to analyze.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "rv_pdf_en.html#models-for-different-data-generating-processes-dgps",
    "href": "rv_pdf_en.html#models-for-different-data-generating-processes-dgps",
    "title": "19¬† Random Variables and Probability Distributions",
    "section": "19.1 Models for Different Data Generating Processes (DGPs)",
    "text": "19.1 Models for Different Data Generating Processes (DGPs)\nDifferent random experiments follow different patterns. We model these using specific probability distributions:\n\nBernoulli: For single yes/no experiments\n\nExample: Single coin flip\nDGP Assumption: Two outcomes with fixed success probability\n\nBinomial: For counting successes in fixed trials\n\nExample: Number of heads in 10 coin flips\nDGP Assumption: Independent trials with same success probability\n\nPoisson: For counting rare events in an interval\n\nExample: Number of customer arrivals per hour\nDGP Assumption: Events occur independently at constant rate\n\n\nüí° Important Note: These are mathematical models based on assumptions about how data is generated. Their usefulness depends on how well these assumptions match reality.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "rv_pdf_en.html#random-variables-making-outcomes-measurable",
    "href": "rv_pdf_en.html#random-variables-making-outcomes-measurable",
    "title": "19¬† Random Variables and Probability Distributions",
    "section": "",
    "text": "Example: Rolling a Die\n\nOutcome space: {‚öÄ, ‚öÅ, ‚öÇ, ‚öÉ, ‚öÑ, ‚öÖ}\nRandom variable X: ‚Äúnumber of dots showing‚Äù\nX converts outcomes to numbers: X(‚öÄ) = 1, X(‚öÅ) = 2, ‚Ä¶, X(‚öÖ) = 6\n\n\n\nExample: Flipping a Coin\n\nOutcome space: {Heads, Tails}\nRandom variable Y: ‚Äú1 if heads, 0 if tails‚Äù\nY converts outcomes to numbers: Y(Heads) = 1, Y(Tails) = 0\n\n\n\nProperties of Discrete Random Variables\n\nEach possible value has a probability\nAll probabilities must be ‚â• 0\nSum of all probabilities must equal 1",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "rv_pdf_en.html#understanding-probability-distributions",
    "href": "rv_pdf_en.html#understanding-probability-distributions",
    "title": "19¬† Random Variables and Probability Distributions",
    "section": "19.3 Understanding Probability Distributions",
    "text": "19.3 Understanding Probability Distributions\nA probability distribution is a mathematical description of the probabilities of different possible outcomes in a random experiment. It tells us:\n\nWhat values can occur (the support of the distribution)\nHow likely each value is to occur (the probability of each outcome)\nHow the probabilities are spread across the possible values\n\nFor discrete random variables, we can represent the distribution as:\n\nA probability mass function (PMF) that gives P(X = x) for each possible value x\nA cumulative distribution function (CDF) that gives P(X ‚â§ x) for each possible value x\n\nKey Properties of Any Probability Distribution:\n\nAll probabilities must be between 0 and 1: 0 \\leq P(X = x) \\leq 1 for all x\nThe sum of all probabilities must equal 1: \\sum P(X = x) = 1\n\nLet‚Äôs explore three fundamental discrete distributions:\n\nUniform Distribution\nThe uniform distribution represents complete randomness - all outcomes are equally likely.\n\nProperties\n\nEach outcome has equal probability\nFor n possible outcomes, P(X = x) = 1/n for each x\nMean: E(X) = \\frac{a + b}{2} where a is minimum and b is maximum\nVariance: Var(X) = \\frac{(b-a+1)^2 - 1}{12} for discrete uniform\n\n\n\nExample: Fair Die\n\n# Visualization of uniform distribution (die roll)\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\ndie_data &lt;- data.frame(\n  outcome = 1:6,\n  probability = rep(1/6, 6)\n)\n\nggplot(die_data, aes(x = factor(outcome), y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  geom_text(aes(label = sprintf(\"1/6\")), vjust = -0.5) +\n  labs(title = \"Probability Distribution of a Fair Die Roll\",\n       x = \"Outcome\",\n       y = \"Probability\") +\n  theme_minimal() +\n  ylim(0, 0.5)\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Distribution\nThe Bernoulli distribution is the simplest probability distribution, modeling a single ‚Äúyes/no‚Äù trial.\n\nProperties\n\nOnly two possible outcomes: 0 (failure) or 1 (success)\nControlled by single parameter p (probability of success)\nMean: E(X) = p\nVariance: Var(X) = p(1-p)\n\n\n\nExample: Biased Coin\n\n# Visualization of Bernoulli distribution (p = 0.7)\nbernoulli_data &lt;- data.frame(\n  outcome = c(\"Failure (0)\", \"Success (1)\"),\n  probability = c(0.3, 0.7)\n)\n\nggplot(bernoulli_data, aes(x = outcome, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"lightgreen\") +\n  geom_text(aes(label = scales::percent(probability)), vjust = -0.5) +\n  labs(title = \"Bernoulli Distribution (p = 0.7)\",\n       x = \"Outcome\",\n       y = \"Probability\") +\n  theme_minimal() +\n  ylim(0, 1)\n\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\nThe binomial distribution models the number of successes in n independent Bernoulli trials.\n\nProperties\n\nParameters: n (number of trials) and p (probability of success)\nPossible values: 0 to n successes\nMean: E(X) = np\nVariance: Var(X) = np(1-p)\nProbability mass function: P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\n\n\nUnderstanding the Formula\nThe binomial probability formula has three parts:\n\n\\binom{n}{k} - Number of ways to get k successes in n trials\np^k - Probability of k successes\n(1-p)^{n-k} - Probability of (n-k) failures\n\n\n\nVisualizing Binomial Distributions\n\nlibrary(ggplot2)\n# Function to calculate binomial probabilities\nbinomial_probs &lt;- function(n, p) {\n  k &lt;- 0:n\n  probs &lt;- dbinom(k, n, p)\n  data.frame(k = k, probability = probs)\n}\n\n# Create plots for different parameters\nggplot(binomial_probs(10, 0.5), aes(x = k, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"salmon\") +\n  labs(title = \"Binomial(n=10, p=0.5)\",\n       x = \"Number of Successes\",\n       y = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(binomial_probs(10, 0.2), aes(x = k, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  labs(title = \"Binomial(n=10, p=0.2)\",\n       x = \"Number of Successes\",\n       y = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nEffect of Parameters\nThe shape of the binomial distribution changes with n and p:\n\nEffect of n (number of trials):\n\nLarger n ‚Üí more possible outcomes\nLarger n ‚Üí distribution becomes more ‚Äúbell-shaped‚Äù\n\nEffect of p (probability of success):\n\np = 0.5 ‚Üí symmetric distribution\np &lt; 0.5 ‚Üí right-skewed distribution\np &gt; 0.5 ‚Üí left-skewed distribution\n\n\n\n\nReal-World Applications\n\nQuality Control\n\nn = number of items inspected\np = probability of defect\nX = number of defective items found\n\nClinical Trials\n\nn = number of patients\np = probability of recovery\nX = number of patients who recover\n\n\n\n\n\nComparing the Distributions\nKey differences between our three distributions:\n\nUniform Distribution\n\nEqual probability for all outcomes\nUsed when all outcomes are equally likely\nExample: rolling a fair die\n\nBernoulli Distribution\n\nSpecial case of binomial with n = 1\nOnly two possible outcomes\nExample: single coin flip\n\nBinomial Distribution\n\nCounts successes in multiple trials\nCombines multiple Bernoulli trials\nExample: number of heads in 10 coin flips\n\n\n\n\nInteractive R Code for Exploration\n\n# Function to compare distributions\ncompare_distributions &lt;- function(n = 10, p = 0.5) {\n  # Binomial probabilities\n  x &lt;- 0:n\n  binom_probs &lt;- dbinom(x, n, p)\n  \n  # Create data frame\n  df &lt;- data.frame(\n    x = x,\n    probability = binom_probs\n  )\n  \n  # Create plot\n  ggplot(df, aes(x = x, y = probability)) +\n    geom_bar(stat = \"identity\", fill = \"steelblue\", alpha = 0.7) +\n    geom_text(aes(label = round(probability, 3)), vjust = -0.5) +\n    labs(title = sprintf(\"Binomial Distribution (n=%d, p=%.2f)\", n, p),\n         x = \"Number of Successes\",\n         y = \"Probability\") +\n    theme_minimal()\n}\n\n# Try different parameters\ncompare_distributions(n = 5, p = 0.5)\n\n\n\n\n\n\n\ncompare_distributions(n = 10, p = 0.3)\n\n\n\n\n\n\n\ncompare_distributions(n = 20, p = 0.7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplying Binomial Distribution: ESP Probability Analysis\n\n\n\nExtrasensory perception (ESP), also known as a sixth sense, is a claimed ability to perceive information through mental means rather than the five physical senses.\nProblem: A man claims to have extrasensory perception (ESP). To test this claim, a fair coin is flipped 10 times and the man is asked to predict each outcome in advance. He gets 7 out of 10 predictions correct. Calculate the probability of obtaining 7 or more correct predictions out of 10 trials by chance alone (assuming no ESP).\nRemark: Under the null hypothesis of no ESP, the probability of each correct prediction is \\frac{1}{2}.\n\nPreliminary Concepts\n\nBernoulli Variable\nA Bernoulli random variable represents a trial with exactly two possible outcomes: success (1) or failure (0). Each trial has probability p of success and 1-p of failure.\nIn our case, each coin flip prediction is a Bernoulli trial where success means a correct prediction.\n\n\nBinomial Distribution\nThe binomial distribution extends the Bernoulli concept to model the sum of n independent Bernoulli trials. It describes the probability of obtaining exactly k successes in these n trials. The probability mass function is:\nP(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\nwhere:\n\nn is the number of trials\nk is the number of successes\np is the probability of success on each trial\n\\binom{n}{k} is the binomial coefficient\n\n\n\nBinomial Coefficient\nThe binomial coefficient \\binom{n}{k} represents the number of ways to choose k items from n items, regardless of order. It is calculated as:\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\nFor example, \\binom{10}{7} = \\frac{10!}{7!(10-7)!} = \\frac{10!}{7!3!} = 120\n\n\n\nProblem Solution\nGiven: - n = 10 coin flips - p = \\frac{1}{2} (probability of correct guess without ESP) - Need P(X \\geq 7) = P(X = 7) + P(X = 8) + P(X = 9) + P(X = 10)\nStep 1: Calculate P(X = 7) P(X = 7) = \\binom{10}{7}(\\frac{1}{2})^7(\\frac{1}{2})^3 = 120 \\cdot (\\frac{1}{128}) \\cdot (\\frac{1}{8}) = 120 \\cdot \\frac{1}{1024} = \\frac{120}{1024} \\approx 0.117\nStep 2: Calculate P(X = 8) P(X = 8) = \\binom{10}{8}(\\frac{1}{2})^8(\\frac{1}{2})^2 = 45 \\cdot (\\frac{1}{256}) \\cdot (\\frac{1}{4}) = 45 \\cdot \\frac{1}{1024} = \\frac{45}{1024} \\approx 0.044\nStep 3: Calculate P(X = 9) P(X = 9) = \\binom{10}{9}(\\frac{1}{2})^9(\\frac{1}{2})^1 = 10 \\cdot (\\frac{1}{512}) \\cdot (\\frac{1}{2}) = 10 \\cdot \\frac{1}{1024} = \\frac{10}{1024} \\approx 0.010\nStep 4: Calculate P(X = 10) P(X = 10) = \\binom{10}{10}(\\frac{1}{2})^{10}(\\frac{1}{2})^0 = 1 \\cdot (\\frac{1}{1024}) \\cdot 1 = \\frac{1}{1024} \\approx 0.001\nStep 5: Sum all probabilities P(X \\geq 7) = \\frac{120 + 45 + 10 + 1}{1024} = \\frac{176}{1024} \\approx 0.172\n\n\nInterpretation\nThere is approximately a 17.2% chance that someone without ESP would correctly guess 7 or more coin flips out of 10 by pure chance. This is a relatively high probability, suggesting that getting 7 out of 10 correct predictions is not strong evidence for ESP. Generally, we would want a much smaller probability (e.g., &lt; 5% or &lt; 1%) before considering the results statistically significant evidence for ESP.\n\n\n\n\n\n\n\n\n\nInductive Derivation of the Binomial Distribution Formula\n\n\n\n\nRandom Variable (X): A function that maps outcomes from a sample space to real numbers. In our case:\n\nX = number of successes in n trials\nX maps ‚ÄúSuccess‚Äù to 1 and ‚ÄúFailure‚Äù to 0 for each trial\nFor n trials, X takes values in {0, 1, 2, ‚Ä¶, n}\n\nProbability Distribution: A function P(X = k) that assigns probabilities to each possible value k of the random variable X, where:\n\nP(X = k) ‚â• 0 for all k\n\\sum_{k=0}^n P(X = k) = 1\n\n\nLet‚Äôs denote success as S and failure as F, with:\n\nProbability of success p = P(\\text{Success}) = P(X = 1) for a single trial\nProbability of failure q = 1-p = P(\\text{Failure}) = P(X = 0) for a single trial\n\n\n1. One Trial\n\n\n\nOutcome\nWays\nProbability\n\n\n\n\nS\n1\np\n\n\nF\n1\nq\n\n\n\nTotal outcomes: p + q = 1\n\n\n2. Two Trials\n\n\n\n# Successes\nWays\nCombinations\nProbability\n\n\n\n\n2\nSS\n1\np^2\n\n\n1\nSF, FS\n2\n2pq\n\n\n0\nFF\n1\nq^2\n\n\n\nNotice: \\binom{2}{k} gives us the number of ways (k = 0,1,2)\nTotal probability: p^2 + 2pq + q^2 = 1\n\n\n3. Three Trials\n\n\n\n# Successes\nWays\nCombinations\nProbability\n\n\n\n\n3\nSSS\n1\np^3\n\n\n2\nSSF, SFS, FSS\n3\n3p^2q\n\n\n1\nSFF, FSF, FFS\n3\n3pq^2\n\n\n0\nFFF\n1\nq^3\n\n\n\nHere, \\binom{3}{k} gives us the combinations (k = 0,1,2,3)\nTotal probability: p^3 + 3p^2q + 3pq^2 + q^3 = 1\n\n\nIllustrating Coordinates for 3 Choose 2\nFor n = 3 and k = 2 successes, \\binom{3}{2} = 3 gives us these success position coordinates:\n\n\n\nSuccess Positions\nSequence\nCoordinate Interpretation\n\n\n\n\n(1,2)\nSSF\nSuccesses in positions 1,2\n\n\n(2,3)\nFSS\nSuccesses in positions 2,3\n\n\n(1,3)\nSFS\nSuccesses in positions 1,3\n\n\n\nThis demonstrates why \\binom{3}{2} = 3 - it counts all possible ways to choose 2 positions from 3 available positions.\n\n\n4. Four Trials\n\n\n\n\n\n\n\n\n\n\n# Successes\nWays\nCombinations\nCoordinate Positions\nProbability\n\n\n\n\n4\nSSSS\n1\n(1,2,3,4)\np^4\n\n\n3\nSSSF, SSFS, SFSS, FSSS\n4\n(1,2,3), (1,2,4), (1,3,4), (2,3,4)\n4p^3q\n\n\n2\nSSFF, SFSF, SFFS, FSSF, FSFS, FFSS\n6\n(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)\n6p^2q^2\n\n\n1\nSFFF, FSFF, FFSF, FFFS\n4\n(1), (2), (3), (4)\n4pq^3\n\n\n0\nFFFF\n1\n()\nq^4\n\n\n\nNote how \\binom{4}{k} gives us the combinations:\n\n\\binom{4}{4} = 1 (one way to choose all positions)\n\\binom{4}{3} = 4 (four ways to choose 3 positions)\n\\binom{4}{2} = 6 (six ways to choose 2 positions)\n\\binom{4}{1} = 4 (four ways to choose 1 position)\n\\binom{4}{0} = 1 (one way to choose no positions)\n\nTotal probability: p^4 + 4p^3q + 6p^2q^2 + 4pq^3 + q^4 = 1\n\n\nPattern Recognition and Generalization\n\nFor n trials:\n\nEach outcome has exactly n positions\nFor k successes, we need k positions filled with S and (n-k) positions with F\n\\binom{n}{k} gives us the number of ways to choose k positions for successes\nEach success contributes p, each failure contributes q\n\nTherefore, for k successes in n trials:\n\nWays = \\binom{n}{k} (binomial coefficient)\nProbability of each way = p^k q^{n-k}\n\n\n\n\nBinomial Distribution Formula\nThe probability of exactly k successes in n trials is:\n\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\nwhere:\n\n\\binom{n}{k} counts the ways to arrange k successes in n positions\np^k accounts for k successes\n(1-p)^{n-k} accounts for (n-k) failures\n\n\n\nCoordinate System Interpretation\nThe binomial coefficient provides a systematic way to count all possible position combinations for successes. For example:\n\nIn the 3-trial case with 2 successes (\\binom{3}{2} = 3):\n\nPosition 1 means success in first trial\nPosition 2 means success in second trial\nPosition 3 means success in third trial\nThe coordinates (1,2), (2,3), (1,3) represent all possible ways to place 2 successes in 3 positions\n\nIn the 4-trial case with 2 successes (\\binom{4}{2} = 6):\n\nWe get six distinct pairs of positions: (1,2), (1,3), (1,4), (2,3), (2,4), (3,4)\nEach pair represents a unique way to distribute 2 successes across 4 trials\nThe remaining positions automatically become failures\n\n\nThis coordinate system demonstrates why the binomial coefficient is the correct counting tool: it systematically accounts for all possible ways to distribute k successes across n positions, where each position must be either a success or failure.\n\n\n\n\nUnderstanding the Binomial Coefficient\nThe binomial coefficient \\binom{n}{k} counts the number of ways to choose k items from n items, where order doesn‚Äôt matter. It relates to the multiplication rule through this key insight:\n\nFirst, count all possible ordered sequences (using multiplication rule)\nThen, divide by number of ways to arrange the k items (to remove order)\n\nFor example, to choose 2 items from 4:\n\nFirst position can be filled in 4 ways\nSecond position can be filled in 3 ways\nTotal ordered sequences = 4 √ó 3 = 12\nNumber of ways to arrange 2 items = 2 √ó 1 = 2\nTherefore \\binom{4}{2} = \\frac{4 √ó 3}{2 √ó 1} = 6",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "inference_en.html",
    "href": "inference_en.html",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "",
    "text": "20.1 Statistical Hypothesis Testing (introduction)\nStatistical inference is how we draw conclusions about a population from a sample. It‚Äôs like being a detective: we never have all the information, but we can make educated guesses based on the evidence we have.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_en.html#statistical-hypothesis-testing-introduction",
    "href": "inference_en.html#statistical-hypothesis-testing-introduction",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "",
    "text": "Key Steps in Statistical Hypothesis Testing (general framework)\n\n\n\n\nInitial Suspicion/Research Question\n\nWe suspect some effect/relationship/difference\nThis guides our research design and analysis\n\nData Collection\n\nWe collect appropriate amount of data\nSample size depends on expected effect and required precision\n\nResult Observation\n\nWe observe and summarize our data\nLook at relevant patterns in the data\n\nHypothesis System\n\nH‚ÇÄ: no effect/no difference (‚Äústatus quo‚Äù)\nH‚ÇÅ: effect exists (one or two-sided)\nChoice of direction based on research question\n\nP-value Approach\n\nConsider: how likely are our results (or more extreme) if H‚ÇÄ is true?\nChoose appropriate probability model based on data type\nCalculate this probability (p-value)\n\nDecision Making\n\nCompare p-value to significance level (typically Œ± = 0.05)\nSmall p-value suggests results unlikely under H‚ÇÄ\n\nConclusion\n\nIf p ‚â§ Œ±, reject H‚ÇÄ\nConclude evidence against null hypothesis\nConsider practical significance\n\n\n\n\n\n\n\n\n\n\nThe Logic of Statistical Hypothesis Testing: A Probabilistic Proof by Contradiction\n\n\n\n\nResearch Context\nA person claims to possess ESP (extrasensory perception) abilities that enable them to predict coin flips. This scenario illustrates the fundamental logic of statistical hypothesis testing.\n\nTask: Predict 100 coin flips before each flip occurs\nObserved Result: 70 correct predictions out of 100 attempts\nKey Consideration: High success could indicate either ESP ability OR a biased coin\n\n\n\nThe Core Logic\nStatistical hypothesis testing follows a logic similar to proof by contradiction in mathematics:\n\nWe start by assuming what we want to disprove (the null hypothesis)\nCalculate the probability of our observed data under this assumption\nIf this probability is extremely small, we reject the initial assumption\n\n\n\nStatistical Framework\n\n1. The Null Hypothesis Mechanism\nThe null hypothesis (H‚ÇÄ) serves as our ‚Äúassumption to be disproven‚Äù and typically represents:\n\nNo effect\nNo difference\nPure chance\nThe status quo\n\nIn our ESP case: Random guessing (p = 0.5)\n\n\n2. The Alternative Hypothesis\nThe alternative hypothesis (H‚ÇÅ) represents what we suspect might be true:\n\nAn effect exists\nA difference exists\nNot due to chance\nA deviation from status quo\n\nIn our ESP case: Better than random guessing (p &gt; 0.5)\n\n\n3. The Decision Rule\nWe establish a conventional cutoff point (Œ±) that defines ‚Äúextremely unlikely‚Äù:\n\nTypically set at Œ± = 0.05 (5%)\nRepresents the threshold for ‚Äúrare enough‚Äù to reject H‚ÇÄ\nA conventional threshold, not a mathematical necessity\n\n\n\n4. The P-value Mechanism\nThe p-value quantifies the logical argument:\n\nAssuming H‚ÇÄ is true\nWhat‚Äôs the probability of our observed result or more extreme?\nSmall p-value means either:\n\nH‚ÇÄ is false (our desired conclusion)\nA rare event occurred under H‚ÇÄ\n\n\n\nP-value (statistical significance): In statistical hypothesis significance testing, the p-value is the probability of obtaining test results (outcomes) at least as extreme as the result actually observed, under the assumption that the null hypothesis (H0) is correct. A very small p-value means that, if the null hypothesis were true, the probability of observing data as extreme as or more extreme than what we actually observed would be very small (the empirical outcome ‚Äúcontradicts‚Äù H0). The smaller the p-value, the stronger the statistical evidence against the null hypothesis, leading us to reject H0 at predetermined significance levels (cut-off or threshold probability) such as 0.05 or 0.01, while recognizing that these thresholds are conventions rather than mathematically derived boundaries.\n\n\n\n\nApplication to ESP Testing\nStatistical Hypotheses: \n\\begin{align*}\nH_0&: p = 0.5 \\text{ (random guessing)} \\\\\nH_1&: p &gt; 0.5 \\text{ (better than guessing)}\n\\end{align*}\n\nProbability Calculation:\nFor 70 successes out of 100 trials:\n\\text{P-value} = P(X \\geq 70) = \\sum_{k=70}^{100} \\binom{100}{k}(0.5)^k(0.5)^{100-k} \\approx 0.0000393\nDecision Framework: \n\\text{Decision Rule} = \\begin{cases}\n\\text{Reject H}_0 & \\text{if p-value} &lt; 0.05 \\\\\n\\text{Fail to reject H}_0 & \\text{if p-value} \\geq 0.05\n\\end{cases}\n\n0.0000393 &lt; 0.05 (significance level)\nThis means that under the null hypothesis (pure guessing):\n\nThe probability of getting 70 or more correct predictions by chance is about 0.00393%\nSuch extreme results would occur by chance only about 4 times in 100,000 trials\nThis is far below our conventional significance level of 0.05 (5%)\n\n\n\nThe General Pattern\n\nAssume the null (like assuming not-A in proof by contradiction)\nCalculate probability of data under null\nIf probability &lt; Œ±:\n\nReject null\nAccept alternative\nConclude evidence supports our suspicion\n\n\n\n\nKey Distinctions from Mathematical Proof\n\nProbabilistic rather than deterministic\nConclusions are ‚Äúsupported‚Äù rather than ‚Äúproven‚Äù\nUses conventional thresholds (Œ±)\nAlways includes uncertainty\n\n\n\n\n\nThe binomial test is a hypothesis test used when you have binary (two-outcome) trials, where each trial is independent and has the same probability of success. It tests whether the observed proportion of successes differs significantly from an expected probability under the null hypothesis. For example: Testing whether a coin is fair by checking if the proportion of heads in 100 flips differs significantly from the expected probability of 0.5 under the null hypothesis.\n\n\n\n\n\n\n\nWhat is a P-value?\n\n\n\nA p-value is a probability that captures how extreme our observed data is relative to a null hypothesis:\n\nThe p-value is the probability of obtaining the observed outcome, or a more extreme one in the direction of the alternative hypothesis, assuming the null hypothesis (H‚ÇÄ) is true.\n\n\nKey Components\n\nObserved outcome: The actual value or statistic computed from our sample data.\nMore extreme outcomes: Additional outcomes that provide even stronger evidence against H‚ÇÄ: \n\\begin{cases}\n\\text{Values } \\leq \\text{ observed} & \\text{for } H_1\\text{: parameter &lt; value} \\\\\n\\text{Values } \\geq \\text{ observed} & \\text{for } H_1\\text{: parameter &gt; value} \\\\\n\\text{Values in both tails} & \\text{for } H_1\\text{: parameter } \\neq \\text{ value}\n\\end{cases}\n\nNull hypothesis assumption: All probabilities are calculated using the parameter value specified in H‚ÇÄ.\n\n\n\nOne-Tailed vs Two-Tailed Tests\nThe choice between one-tailed and two-tailed tests depends on your alternative hypothesis and the context of your research question:\nOne-Tailed Tests:\n\nUsed when H‚ÇÅ specifies a direction (&lt; or &gt;)\nP-value calculated from one tail of the distribution\nHigher power but requires strong directional justification\nExample hypotheses: \n\\begin{align*}\nH_0&: \\mu = \\mu_0 \\\\\nH_1&: \\mu &gt; \\mu_0 \\text{ (right-tailed) or } \\mu &lt; \\mu_0 \\text{ (left-tailed)}\n\\end{align*}\n\n\nTwo-Tailed Tests:\n\nUsed when H‚ÇÅ is non-directional (‚â†)\nP-value includes both tails of the distribution\nMore conservative, standard choice when direction uncertain\nParticularly suitable for symmetric distributions like the normal distribution\nExample hypotheses: \n\\begin{align*}\nH_0&: \\mu = \\mu_0 \\\\\nH_1&: \\mu \\neq \\mu_0\n\\end{align*}\n\n\n\n\nExample: Binomial Test\nTesting if a politician is overestimating 98% support (p = 0.98) when observing 13 supporters in n = 15 people. In this context, a one-tailed test is most appropriate because the research question is inherently directional (overestimating implies p &lt; 0.98).\n\n\\begin{align*}\nH_0&: p = 0.98 \\\\\nH_1&: p &lt; 0.98\n\\end{align*}\n\nP-value calculation: \n\\begin{align*}\n\\text{p-value} &= P(X \\leq 13 \\mid H_0) \\\\\n&= 1 - P(X \\geq 14 \\mid p = 0.98) \\\\\n&= 0.0353\n\\end{align*}\n\n\n\nCommon Misconceptions to Avoid\n\nP-value ‚â† Probability H‚ÇÄ is true\n1 - p-value ‚â† Probability H‚ÇÅ is true\nLarge p-value ‚â† H‚ÇÄ is true\nOne-tailed tests aren‚Äôt automatically ‚Äúbetter‚Äù despite higher power\nThe choice between one-tailed and two-tailed tests should be based on the research context, not just statistical convenience\n\nRemember: The p-value quantifies evidence against H‚ÇÄ but should be considered alongside practical significance and effect size.\n\n\n\n\nThe Method of Proof by Contradiction\n\nIn Mathematics\nProof that \\sqrt{2} is Irrational\nInitial Assumption\nIf \\sqrt{2} is rational, then \\sqrt{2} = \\frac{p}{q} where:\n\np and q are integers\nq \\neq 0\np and q have no common factors\n\nAlgebraic Steps\n\nStarting with \\sqrt{2} = \\frac{p}{q}\nSquare both sides: 2 = \\frac{p^2}{q^2}\nMultiply both sides by q^2: 2q^2 = p^2\n\nProperties of p and q\n\nSince 2q^2 = p^2, p^2 is even\nIf p^2 is even, then p is even\nTherefore p = 2k for some integer k\nSubstituting p = 2k into 2q^2 = p^2: 2q^2 = (2k)^2 = 4k^2\nTherefore q^2 = 2k^2\nThus q^2 is even, so q is even\n\nContradiction\n\nWe proved both p and q are even\nThis contradicts our assumption that p and q have no common factors\nTherefore, \\sqrt{2} cannot be rational\n\nThus, \\sqrt{2} is irrational.\n\n\nIn Statistics\nWe use a similar but probabilistic approach:\n\nMake assumption (null hypothesis)\nSee if data contradicts this assumption\nIf contradiction is strong enough, reject assumption\n\nKey Difference: We deal with probability, not certainty.\n\n\n\nThe Null Hypothesis Framework\n\nStep 1: State the Hypotheses\nFor a coin example:\n\nH‚ÇÄ (Null): Coin is fair (p = 0.5)\nH‚ÇÅ (Alternative): Coin is not fair (p ‚â† 0.5)\n\nThink of H‚ÇÄ as the ‚Äúinnocent until proven guilty‚Äù assumption.\n\n\nStep 2: Collect Evidence\nSuppose we flip coin 100 times:\n\nExpected under H‚ÇÄ: About 50 heads\nActually observe: 70 heads\n\n\n\nStep 3: Assess Evidence\nAsk: ‚ÄúIf coin were truly fair (H‚ÇÄ true), how likely is this result?‚Äù\nThis is like asking in a legal case:\n\n‚ÄúIf defendant were innocent, how do we explain the evidence?‚Äù\nVery improbable evidence suggests innocence might be false\n\n\n\n\n\n\n\nUnderstanding Error Types in Hypothesis Testing\n\n\n\nIn hypothesis testing, we can make two types of errors:\n\nType I Error (False Positive)\n\nDefinition: Rejecting H‚ÇÄ when it is actually true\nProbability: Œ± (significance level)\nExample in Justice System: Convicting an innocent person\nExample in Medicine: Diagnosing healthy patient as sick\n\n\n\nType II Error (False Negative)\n\nDefinition: Failing to reject H‚ÇÄ when it is actually false\nProbability: Œ≤\nPower = 1 - Œ≤ (probability of correctly rejecting false H‚ÇÄ)\nExample in Justice System: Letting guilty person go free\nExample in Medicine: Missing an actual disease\n\n\n\nTrade-off Between Errors\n\nDecreasing Œ± (being more conservative) increases Œ≤\nDecreasing Œ≤ (increasing power) requires either:\n\nLarger sample size\nLarger effect size\nHigher Œ±\n\n\n\n\nPractical Implications\n\n\n\n\n\n\n\n\n\nContext\nType I Concern\nType II Concern\nTypical Œ±\n\n\n\n\nCriminal Justice\nConvict innocent\nFree guilty\n0.001\n\n\nMedical Testing\nUnnecessary treatment\nMiss disease\n0.01\n\n\n\n\n\n\n\n\n\n\n\n\nHistorical Note: Jerzy Neyman (1894-1981)\n\n\n\nThe framework of statistical hypothesis testing as we know it today was largely developed by Jerzy Neyman, a Polish mathematician and statistician, in collaboration with Egon Pearson. Born in Bendery, Imperial Russia (now Moldova), Neyman made fundamental contributions to statistics that transformed both theoretical foundations and practical applications.\nHis most significant contributions include:\n\nDevelopment of the formal hypothesis testing framework, introducing the concepts of Type I and Type II errors\nCreation of confidence intervals as a way to express uncertainty in estimation\nPioneering the potential outcomes framework in causal inference\nAdvancement of sampling theory\n\nThe potential outcomes framework, first introduced by Neyman in his 1923 master‚Äôs thesis on agricultural experiments, revolutionized how we think about causality in statistics. This framework, later rediscovered and expanded by Donald Rubin (hence sometimes called the Neyman-Rubin causal model), introduced the concept of comparing potential outcomes that would occur under different treatments. For each unit, Neyman conceived of multiple potential outcomes, only one of which could be observed - a fundamental concept now known as the ‚Äúfundamental problem of causal inference.‚Äù\nHis approach to statistical inference differed notably from R.A. Fisher‚Äôs significance testing, leading to important debates that helped shape modern statistical theory and practice.\nThe potential outcomes framework he introduced has become particularly influential in modern causal inference, epidemiology, and social sciences research. The impact of his contributions continues to be felt in how we approach statistical inference, experimental design, and causal analysis today.\n\n\n\n\n\nThe Logic of P-values\n\nWhat is a p-value?\np-value = P(seeing this evidence or more extreme | H‚ÇÄ is true)\nLike asking:\n\n‚ÄúHow surprising is this evidence if H‚ÇÄ is true?‚Äù\n‚ÄúCould this easily happen by chance?‚Äù\n\n\n\nExample: Step by Step\nObserve 8 heads in 10 flips:\n\nAssume H‚ÇÄ: p = 0.5 (fair coin)\nCalculate: P(X ‚â• 8) = P(8 heads) + P(9 heads) + P(10 heads) = \\binom{10}{8}(0.5)^8(0.5)^2 + \\binom{10}{9}(0.5)^9(0.5)^1 + \\binom{10}{10}(0.5)^{10} ‚âà 0.055\nInterpret:\n\nAbout 5.5% chance of seeing this under H‚ÇÄ\nModerately unusual, but not extremely so\n\n\n\n\nCommon Misunderstandings\n\n‚Äúp-value is probability H‚ÇÄ is true‚Äù\n\nNo: It‚Äôs probability of data, assuming H‚ÇÄ\nLike P(evidence|innocent), not P(innocent|evidence)\n\n‚ÄúSmall p-value proves H‚ÇÄ false‚Äù\n\nNo: Only suggests H‚ÇÄ unlikely\nLike strong evidence, but not proof\n\n‚ÄúLarge p-value proves H‚ÇÄ true‚Äù\n\nNo: Just fails to provide evidence against H‚ÇÄ\nLike ‚Äúnot guilty‚Äù vs ‚Äúproven innocent‚Äù\n\n\n\n\n\n\n\n\nBeyond Simple Significance Testing\n\n\n\n\nWhy Not Just Use 5%?\nHistorical reasons:\n\nR.A. Fisher suggested as benchmark\nPre-computed tables used 5%\nBecame convention, not logical necessity\n\n\n\nBetter Approach:\nConsider p-value on continuous scale:\n\np = 0.049 vs p = 0.051 are virtually same\nContext matters:\n\nMedical trials might need p &lt; 0.001\nMarket research might accept p &lt; 0.1\n\n\n\n\nPractical Significance\nAlways consider:\n\nEffect size (how big is the difference?)\nPractical importance\nCosts and benefits of decisions\nSample size and power",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_en.html#problem-solutions-part-1-the-binomial-tests-one-tailedsided-tests",
    "href": "inference_en.html#problem-solutions-part-1-the-binomial-tests-one-tailedsided-tests",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "20.2 Problem Solutions ‚Äì part 1: the binomial tests (one-tailed/sided tests)",
    "text": "20.2 Problem Solutions ‚Äì part 1: the binomial tests (one-tailed/sided tests)\n\nThe binomial test is a statistical hypothesis test used when you have binary (two-outcome) trials, where each trial is independent and has the same probability of success. It tests whether the observed proportion of successes differs significantly from an expected probability under the null hypothesis. For example: Testing whether a coin is fair by checking if the proportion of heads in 100 flips differs significantly from the expected probability of 0.5 under the null hypothesis.\n\n\nProblem 1: Binomial Test for Candidate Support\n\nProblem Statement\nAn election candidate believes she has the support of 50% (p = 0.5) of the residents in a particular town. A researcher suspects this might be an underestimation and conducts a survey. The researcher asks 10 people whether they support the candidate or not; 7 people say that they do (70% in a sample).\nCalculate the p-value and decide whether there is enough evidence to reject H0 using data from the sample (assuming the critical probability = 5%).\n\n\nSetup\nHypotheses:\n\nH_0: p = 0.5 (null hypothesis: true proportion (support) is 50%)\nH_1: p &gt; 0.5 (alternative hypothesis: true proportion is greater than 50%)\n\nData:\n\nSample size: n = 10\nObserved successes: x = 7 (70% support)\nHypothesized proportion: p_0 = 0.5\nSignificance level: \\alpha = 0.05 (5%)\n\n\n\nFinding the P-value\nFor a one-sided test, the p-value is the probability of observing 7 or more successes out of 10 trials, assuming H_0 is true. Using the binomial distribution:\nP(X \\geq 7) = P(X = 7) + P(X = 8) + P(X = 9) + P(X = 10)\n\n\n\n\n\n\nWhy One-Tailed Test?\n\n\n\nThis is a one-tailed/sided test because we‚Äôre specifically interested in whether the candidate is under-estimating her support. In statistical terms:\n\nIf she believes support is 50% but it‚Äôs actually 40%, she‚Äôs over-estimating her support\nIf she believes support is 50% but it‚Äôs actually 60%, she‚Äôs under-estimating her support\n\nOur research question only concerns under-estimation, so we only need to consider evidence in that direction (values greater than 50%). This is reflected in our alternative hypothesis H_1: p &gt; 0.5.\n\nWhy Not Just P(X = 7)?\nWe can‚Äôt just calculate P(X = 7) because:\n\nThe p-value represents the probability of observing results as extreme or more extreme than what we saw, assuming H_0 is true\n‚ÄúMore extreme‚Äù means results that provide even stronger evidence against H_0 in the direction of H_1\nSince H_1 suggests higher proportions than 50%, outcomes of 8, 9, or 10 supporters out of 10 would be even stronger evidence against H_0\n\nTherefore, we must sum:\nP(X \\geq 7) = P(X = 7) + P(X = 8) + P(X = 9) + P(X = 10)\nIf we only calculated P(X = 7) = 0.1172, we would ignore these other possible outcomes that also support H_1, leading to an incorrect p-value.\n\n\n\nFor each value k, we use the binomial probability formula:\nP(X = k) = \\binom{n}{k} p_0^k (1-p_0)^{n-k}\nLet‚Äôs calculate each term:\n\nP(X = 7) = \\binom{10}{7} (0.5)^7 (0.5)^3 = 120 \\cdot (0.5)^{10} = 0.1172\nP(X = 8) = \\binom{10}{8} (0.5)^8 (0.5)^2 = 45 \\cdot (0.5)^{10} = 0.0439\nP(X = 9) = \\binom{10}{9} (0.5)^9 (0.5)^1 = 10 \\cdot (0.5)^{10} = 0.0098\nP(X = 10) = \\binom{10}{10} (0.5)^{10} (0.5)^0 = 1 \\cdot (0.5)^{10} = 0.0010\n\nP-value = 0.1172 + 0.0439 + 0.0098 + 0.0010 = 0.1719 (17.19%)\n\n\nDecision\nSince the p-value (0.1719) is greater than the significance level (0.05), we fail to reject the null hypothesis.\n\n\nInterpretation\nThere is not enough evidence to conclude that the candidate is under-estimating her support. While the sample shows 70% support (higher than 50%), this difference could reasonably occur by chance even if the true support was only 50%. The relatively small sample size (n = 10) makes it harder to detect real differences.\n\n\n\nProblem 2: Binomial Test for Candidate Support (2)\n\nProblem Statement\nAn election candidate believes she has the support of 40% (p = 0.4) of the residents in a particular town. A researcher suspects this might be an overestimation and conducts a survey. The researcher asks 20 people whether they support the candidate or not; 3 people say that they do (15% in a sample). Calculate the p-value and decide whether there is enough evidence to reject H0 using data from the sample (assuming the critical probability = 5%).\n\n\nSetup\nHypotheses:\n\nH_0: p = 0.4 (null hypothesis: true proportion is 40%)\nH_1: p &lt; 0.4 (alternative hypothesis: true proportion is less than 40%)\n\nData:\n\nSample size: n = 20\nObserved successes: x = 3 (15% support)\nHypothesized proportion: p_0 = 0.4\nSignificance level: \\alpha = 0.05 (5%)\n\n\n\nWhy One-Tailed Test?\nThis is a one-tailed test because we‚Äôre specifically interested in whether the candidate is over-estimating her support. We only care about evidence suggesting the true proportion is less than 40%, leading to a left-tailed test.\n\n\nFinding the P-value\nFor this left-tailed test, the p-value is the probability of observing 3 or fewer successes out of 20 trials, assuming H_0 is true. Using the binomial distribution:\nP(X \\leq 3) = P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3)\nFor each value k, we use the binomial probability formula: P(X = k) = \\binom{n}{k} p_0^k (1-p_0)^{n-k}\n\nWe want to find P(X ‚â§ 3) when X follows B(20, 0.4) Using the binomial formula:\n\nP(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\n\nCalculate the combinations \\binom{20}{k} for k = 0, 1, 2, 3:\n\n\\binom{20}{0} = 1\n\\binom{20}{1} = 20\n\\binom{20}{2} = \\frac{20 \\times 19}{2 \\times 1} = 190\n\\binom{20}{3} = \\frac{20 \\times 19 \\times 18}{3 \\times 2 \\times 1} = 1,140\n\nCalculate each probability:\n\nFor k = 0: P(X = 0) = \\binom{20}{0}(0.4)^0(0.6)^{20} = 1 \\times 1 \\times 0.6^{20} \\approx 0.0000366\nFor k = 1: P(X = 1) = \\binom{20}{1}(0.4)^1(0.6)^{19} = 20 \\times 0.4 \\times 0.6^{19} \\approx 0.0004875\nFor k = 2: P(X = 2) = \\binom{20}{2}(0.4)^2(0.6)^{18} = 190 \\times 0.16 \\times 0.6^{18} \\approx 0.0030874\nFor k = 3: P(X = 3) = \\binom{20}{3}(0.4)^3(0.6)^{17} = 1,140 \\times 0.064 \\times 0.6^{17} \\approx 0.0123497\n\nSum all probabilities: P(X \\leq 3) = \\sum_{k=0}^3 P(X = k) = 0.0000366 + 0.0004875 + 0.0030874 + 0.0123497 = 0.0159612\nDecision rule:\n\n\nReject H‚ÇÄ if p-value &lt; Œ±\nSince 0.0159612 &lt; 0.05, we reject H‚ÇÄ.\n\n\n\nDecision\nSince the p-value is less than the significance level (0.05), we reject the null hypothesis.\n\n\nInterpretation\nThere is sufficient evidence at the 5% significance level to conclude that the candidate is overestimating her support. The sample shows only 15% support, which is significantly lower than the candidate‚Äôs belief of 40%. The probability of observing such low support (3 or fewer out of 20) would be only about 1.6% if the true support were actually 40%.\n\n\n\nProblem 3: Binomial Test for Candidate Support (3)\n\nProblem Statement\nA political candidate claims that 40% of residents in a town support her campaign (p = 0.4). A researcher suspects this might be an overestimation and conducts a survey. In a random sample of 12 residents, 1 person expresses support for the candidate. Test whether there is sufficient evidence to conclude that the candidate is overestimating her support level, using a significance level of 5%.\n\n\nHypotheses\n\n\\begin{align*}\nH_0&: p = 0.4 \\text{ (The candidate's claim is correct)} \\\\\nH_1&: p &lt; 0.4 \\text{ (The candidate is overestimating support)}\n\\end{align*}\n\n\n\nGiven Information\n\nSample size: n = 12\nNumber of successes: x = 1\nHypothesized proportion: p_0 = 0.4\nSignificance level: \\alpha = 0.05\nObserved proportion: \\hat{p} = \\frac{1}{12} \\approx 0.083\n\n\n\nSolution\n\nFor a left-tailed test, we calculate the probability of observing 1 or fewer successes under H_0.\nUsing the binomial probability formula: P(X \\leq 1) = \\sum_{k=0}^{1} \\binom{12}{k}(0.4)^k(0.6)^{12-k}\nWe find: P(X \\leq 1) = 0.0196\nDecision Rule:\n\nReject H_0 if p-value &lt; \\alpha\nSince 0.0196 &lt; 0.05, we reject H_0\n\n\n\n\nConclusion\nAt a 5% significance level, there is sufficient evidence to conclude that the candidate is overestimating her support. The sample proportion (8.3%) is substantially lower than the claimed 40% support, and this difference is statistically significant (p = 0.0196).\n\n\nStatistical Power Consideration\nWhile the sample size (n = 12) is relatively small, we were still able to detect a significant difference. This is because the observed difference between the claimed proportion (40%) and sample proportion (8.3%) was quite large. However, a larger sample size would provide more reliable results and better estimation of the true support proportion.\n\n\n\nProblem 4: Binomial Test for Candidate Support (4)\nAn election candidate claims that 20% of residents in a town support her campaign. A researcher believes the candidate might be over-estimating her support and wants to test this claim. In a random sample of 12 residents, 4 people express support for the candidate. Test whether there is sufficient evidence to conclude that the candidate is over-estimating her support level, using a significance level of 5%.\nGiven:\n\nClaimed support: p = 0.2\nSample size: n = 12\nNumber of supporters in sample: x = 4\nSignificance level: Œ± = 0.05\n\n\nSolution\nStep 1: State the Hypotheses\nSince we want to test if the candidate is over-estimating (true proportion is less than claimed):\n\n\\begin{align*}\nH_0&: p = 0.2 \\text{ (The candidate's claim is correct)} \\\\\nH_1&: p &lt; 0.2 \\text{ (The candidate is overestimating support)}\n\\end{align*}\n\nStep 2: Choose the Test Statistic\nWe use the number of successes (X) in the sample, where X follows a binomial distribution with n = 12 and p = 0.2 under H‚ÇÄ.\nObserved value: x = 4\nStep 3: Calculate the Test Statistic and P-value\nFor a left-tailed test, we calculate:\nP(X \\leq 4) = \\sum_{k=0}^{4} \\binom{12}{k}(0.2)^k(0.8)^{12-k}\n\n# Calculate p-value\np_value &lt;- pbinom(4, size = 12, prob = 0.2)\nprint(paste(\"P-value =\", round(p_value, 4)))\n\n[1] \"P-value = 0.9274\"\n\n\nThe p-value is 0.9274\nStep 4: Decision Rule\n\nReject H‚ÇÄ if p-value &lt; Œ±\nSince 0.9274 &gt; 0.05, we fail to reject H‚ÇÄ\n\nStep 5: Interpretation\nAt a 5% significance level, there is not enough evidence to conclude that the candidate is over-estimating her support. In fact, the sample data shows 4/12 ‚âà 33.3% support, which is higher than her claimed 20%, going in the opposite direction of our alternative hypothesis.\n\n\nAdditional Notes\n\nSample Proportion: \\hat{p} = \\frac{x}{n} = \\frac{4}{12} = 0.333\nThe high p-value reflects that the sample proportion (33.3%) is actually higher than the hypothesized value (20%), not lower as we were testing for.\nIf we had suspected under-estimation rather than over-estimation, we should have set up the test with H‚ÇÅ: p &gt; 0.2.\nGiven the small sample size (n = 12), the power of this test to detect true differences is limited.\n\n\n\nR Code\nHere‚Äôs the complete R code for this analysis:\n\n# Given values\nn &lt;- 12          # sample size\nx &lt;- 4           # number of successes\np0 &lt;- 0.2        # hypothesized proportion\nalpha &lt;- 0.05    # significance level\n\n# Calculate p-value for left-tailed test\np_value &lt;- pbinom(x, size = n, prob = p0)\n\n# Calculate sample proportion\np_hat &lt;- x/n\n\n# Print results\ncat(\"Sample proportion =\", round(p_hat, 3), \"\\n\")\n\nSample proportion = 0.333 \n\ncat(\"P-value =\", round(p_value, 4), \"\\n\")\n\nP-value = 0.9274 \n\ncat(\"Decision: \", ifelse(p_value &lt; alpha, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\nDecision:  Fail to reject H0 \n\n\n\n\n\nProblem 5: Binomial Test for EU Support\n\nProblem Statement\nA politician believes that support for his country‚Äôs EU membership is about 98% (p = 0.98). A researcher wants to test whether the politician is overestimating this level of support.\nIn a sample of 15 people (n = 15), the researcher observes that 13 people support membership. Let‚Äôs define the random variable X as the number of people in the sample who support EU membership. We observed X = 13 ‚Äúsuccesses‚Äù in 15 Bernoulli trials.\nIs there enough evidence to reject the claim that the support is 98%?\n\n\nSetup\nHypotheses:\n\nH_0: p = 0.98 (null hypothesis: true proportion is 98%)\nH_1: p &lt; 0.98 (alternative hypothesis: true proportion is less than 98%)\n\nData:\n\nSample size: n = 15\nObserved successes: x = 13 (‚âà86.7% support)\nHypothesized proportion: p_0 = 0.98\nSignificance level: \\alpha = 0.05 (5%)\n\n\n\nFinding the P-value\nFor this left-tailed test, we need P(X \\leq 13). Given the high value of p_0, it‚Äôs more efficient to use the complement rule:\nP(X \\leq 13) = 1 - P(X \\geq 14)\n\nThe complement rule in probability states that P(A) = 1 - P(not A), because P(A) + P(not A) = 1. For a left-tailed test, we need P(X ‚â§ 13). Instead of summing P(X = 0) + P(X = 1) + ‚Ä¶ + P(X = 13), it‚Äôs easier to: calculate P(X ‚â§ 13) = 1 - P(X &gt; 13), or P(X ‚â§ 13) = 1 - P(X ‚â• 14).\n\nFor this left-tailed test: P(X \\leq 13) = 1 - P(X &gt; 13) = 1 - P(X = 14) - P(X = 15)\nLet‚Äôs calculate step by step:\n\nFor X = 14:\n\n\nCalculate combination:\n\n\\binom{15}{14} = \\frac{15!}{14!(15-14)!} = \\frac{15}{1} = 15\n\nCalculate probability:\n\nP(X = 14) = \\binom{15}{14}(0.98)^{14}(0.02)^1 = 15 \\times (0.98)^{14} \\times 0.02 = 15 \\times 0.75051... \\times 0.02 \\approx 0.2252\n\nFor X = 15:\n\n\nCalculate combination:\n\n\\binom{15}{15} = 1\n\nCalculate probability:\n\nP(X = 15) = \\binom{15}{15}(0.98)^{15}(0.02)^0 = 1 \\times (0.98)^{15} \\times 1 = (0.98)^{15} \\approx 0.7395\n\nTherefore:\n\nP(X \\leq 13) = 1 - P(X = 14) - P(X = 15) = 1 - 0.2252 - 0.7395 \\approx 0.0353\n\n\nDecision\nSince the p-value (0.0353) is less than the significance level (0.05), we reject the null hypothesis.\n\n\nInterpretation\nThere is sufficient evidence to conclude that the politician is overestimating the support for EU membership. While 86.7% support in the sample is still very high, it‚Äôs significantly lower than the politician‚Äôs claim of 98%. Under the assumption that true support is 98%, the probability of observing 13 or fewer supporters in a sample of 15 people would be only about 3.53%.\nNote on Complement Rule\nThis problem demonstrates the utility of the complement rule in probability calculations. Instead of calculating probabilities for outcomes 0 through 13 (14 calculations), we only needed to calculate probabilities for outcomes 14 and 15 (2 calculations). This is particularly efficient when:\n\nWe have a large number of outcomes in the ‚Äútail‚Äù we‚Äôre interested in\nThe probability is concentrated in the other tail (here, high values due to p_0 = 0.98)",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_en.html#problem-solutions-part-2-the-binomial-tests-two-tailedsided-tests",
    "href": "inference_en.html#problem-solutions-part-2-the-binomial-tests-two-tailedsided-tests",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "20.3 Problem Solutions ‚Äì part 2: the binomial tests (two-tailed/sided tests (*))",
    "text": "20.3 Problem Solutions ‚Äì part 2: the binomial tests (two-tailed/sided tests (*))\n(‚Ä¶)",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_en.html#testing-ols-regression-parameter-significance-using-r",
    "href": "inference_en.html#testing-ols-regression-parameter-significance-using-r",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "20.4 Testing OLS Regression Parameter Significance Using R (*)",
    "text": "20.4 Testing OLS Regression Parameter Significance Using R (*)\n(‚Ä¶)",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_pl.html",
    "href": "inference_pl.html",
    "title": "21¬† Wprowadzenie do Wnioskowania Statystycznego: Logika Testowania Hipotez Statystycznych",
    "section": "",
    "text": "Wnioskowanie statystyczne to spos√≥b, w jaki wyciƒÖgamy wnioski o populacji na podstawie pr√≥by. To jak bycie detektywem: nigdy nie mamy wszystkich informacji, ale mo≈ºemy wyciƒÖgaƒá uzasadnione wnioski na podstawie dostƒôpnych dowod√≥w.\n\nPodstawowa Logika\nWyobra≈∫ sobie, ≈ºe podejrzewasz, i≈º moneta mo≈ºe byƒá nieuczciwa. Jak sprawdziƒá takie przypuszczenie?\n\nZbierz Dowody:\n\nWykonaj wiele rzut√≥w monetƒÖ\nZapisz wyniki\nSprawd≈∫, czy sƒÖ zgodne z tym, czego oczekiwa≈Çby≈õ od uczciwej monety\n\nPodejmij Decyzjƒô:\n\nJe≈õli wyniki wyglƒÖdajƒÖ normalnie ‚Üí kontynuuj za≈Ço≈ºenie, ≈ºe moneta jest uczciwa\nJe≈õli wyniki wyglƒÖdajƒÖ bardzo nietypowo ‚Üí podejrzewaj, ≈ºe moneta jest nieuczciwa\n\n\n\n\n\n\n\n\nKluczowe Kroki Testowania Hipotez Statystycznych (og√≥lny schemat)\n\n\n\n\nWstƒôpne Podejrzenie/Pytanie Badawcze\n\nPodejrzewamy istnienie pewnego efektu/zwiƒÖzku/r√≥≈ºnicy\nTo ukierunkowuje nasze badanie i analizƒô\n\nZbieranie Danych\n\nGromadzimy odpowiedniƒÖ ilo≈õƒá danych\nWielko≈õƒá pr√≥by zale≈ºy od oczekiwanego efektu i wymaganej precyzji\n\nObserwacja Wynik√≥w\n\nObserwujemy i podsumowujemy nasze dane\nSzukamy istotnych wzorc√≥w w danych\n\nSystem Hipotez\n\nH‚ÇÄ: brak efektu/brak r√≥≈ºnicy (‚Äústatus quo‚Äù)\nH‚ÇÅ: efekt istnieje (jedno- lub dwustronny)\nWyb√≥r kierunku zale≈ºy od pytania badawczego\n\nPodej≈õcie Warto≈õci p\n\nRozwa≈ºamy: jak prawdopodobne sƒÖ nasze wyniki (lub bardziej skrajne) je≈õli H‚ÇÄ jest prawdziwa?\nWybieramy odpowiedni model probabilistyczny w zale≈ºno≈õci od typu danych\nObliczamy to prawdopodobie≈Ñstwo (warto≈õƒá p)\n\nPodejmowanie Decyzji\n\nPor√≥wnujemy warto≈õƒá p z poziomem istotno≈õci (zazwyczaj Œ± = 0.05)\nMa≈Ça warto≈õƒá p sugeruje, ≈ºe wyniki sƒÖ ma≈Ço prawdopodobne przy H‚ÇÄ\n\nWniosek\n\nJe≈õli p ‚â§ Œ±, odrzucamy H‚ÇÄ\nWnioskujemy o dowodach przeciwko hipotezie zerowej\nRozwa≈ºamy znaczenie praktyczne\n\n\n\n\n\n\n\n\n\n\nPodstawowa Logika Testowania Hipotez Statystycznych: Analiza Zdolno≈õci ESP\n\n\n\nProblem Badawczy: Testowanie Deklaracji o Posiadaniu Zdolno≈õci Pozazmys≈Çowych\nOsoba twierdzi, ≈ºe posiada zdolno≈õci ESP (percepcjƒô pozazmys≈ÇowƒÖ, tzw. sz√≥sty zmys≈Ç), kt√≥re pozwalajƒÖ jej przewidywaƒá wyniki rzut√≥w monetƒÖ. Aby naukowo przetestowaƒá to twierdzenie, projektujemy eksperyment, w kt√≥rym moneta jest rzucana 100 razy, a osoba musi przewidzieƒá ka≈ºdy wynik przed rzutem.\nOsoba osiƒÖga sukces w 70 na 100 przewidywa≈Ñ. Jednak istnieje subtelne, ale kluczowe zastrze≈ºenie: wysoki wsp√≥≈Çczynnik sukcesu mo≈ºe wskazywaƒá zar√≥wno na zdolno≈õci ESP, JAK I na nieuczciwƒÖ monetƒô.\nDefiniowanie Prawdopodobie≈Ñstwa Bazowego i Oczekiwanej Skuteczno≈õci\nJe≈õli osoba jedynie zgaduje (brak ESP), ka≈ºde przewidywanie jest r√≥wnowa≈ºne losowemu zgadywaniu z prawdopodobie≈Ñstwem sukcesu 0.5. Je≈õli rzeczywi≈õcie posiada zdolno≈õci ESP, spodziewaliby≈õmy siƒô wsp√≥≈Çczynnika sukcesu przekraczajƒÖcego 0.5. To stanowi podstawƒô naszego badania statystycznego.\nUstanawianie Systemu Hipotez Statystycznych\nUstalamy dwie konkurencyjne hipotezy:\n\nHipoteza Zerowa (H‚ÇÄ): Przewidywania opierajƒÖ siƒô na losowym zgadywaniu przy u≈ºyciu uczciwej monety (p = 0.5)\nHipoteza Alternatywna (H‚ÇÅ): Albo osoba posiada zdolno≈õci ESP, ALBO moneta jest nieuczciwa (p &gt; 0.5)\n\nWyb√≥r Miƒôdzy Testami Jedno- i Dwustronnymi\nW testowaniu hipotez musimy zdecydowaƒá, czy testujemy efekt w jednym czy obu kierunkach:\nTest Jednostronny (Nasz Obecny Przypadek):\n\nTestuje efekt tylko w jednym kierunku (tutaj: lepszy ni≈º przypadek)\nWiƒôksza moc wykrywania okre≈õlonego efektu kierunkowego\nOdpowiedni, gdy interesuje nas tylko jeden kierunek\nPrzyk≈Çad: Interesuje nas tylko wynik lepszy ni≈º przypadkowy, nie gorszy\n\nTest Dwustronny (Alternatywne Podej≈õcie):\n\nTestuje efekt w obu kierunkach (zar√≥wno lepszy jak i gorszy ni≈º przypadek)\nMniejsza moc, ale bardziej kompleksowy\nOdpowiedni, gdy ka≈ºde odchylenie od hipotezy zerowej jest interesujƒÖce\nPrzyk≈Çad: Testowanie, czy moneta jest nieuczciwa w kierunku or≈Ça LUB reszki\n\nWyb√≥r Modelu Probabilistycznego: Rozk≈Çad Dwumianowy\nNasz test ESP pasuje do modelu prawdopodobie≈Ñstwa dwumianowego, poniewa≈º:\n\nKa≈ºde przewidywanie jest niezale≈ºne\nKa≈ºde przewidywanie ma dok≈Çadnie dwa mo≈ºliwe wyniki (poprawne/niepoprawne)\nPrawdopodobie≈Ñstwo sukcesu pozostaje sta≈Çe (0.5 przy H‚ÇÄ)\nZliczamy ca≈ÇkowitƒÖ liczbƒô sukces√≥w w ustalonej liczbie pr√≥b\n\nDla naszego przyk≈Çadu obliczamy: P(X \\geq 70) = \\sum_{k=70}^{100} \\binom{100}{k}(0.5)^k(0.5)^{100-k}\nObliczanie i Interpretacja Warto≈õci p\nWarto≈õƒá p pomaga nam oceniƒá, jak zaskakujƒÖce by≈Çyby nasze wyniki, gdyby H‚ÇÄ by≈Ça prawdziwa:\n\nMierzy prawdopodobie≈Ñstwo uzyskania 70 lub wiƒôcej poprawnych przewidywa≈Ñ na 100 pr√≥b przez czysty przypadek\nBardzo ma≈Ça warto≈õƒá p sugeruje, ≈ºe taki sukces by≈Çby rzadki przy losowym zgadywaniu\nKonwencjonalny pr√≥g 0.05 oznacza, ≈ºe wymagamy wynik√≥w, kt√≥re wystƒÖpi≈Çyby przypadkowo rzadziej ni≈º w 5% przypadk√≥w\n\nRegu≈Çy Decyzyjne i Potencjalne B≈Çƒôdy w Testowaniu ESP\n\nB≈ÇƒÖd Typu I (Fa≈Çszywie Pozytywny):\n\nStwierdzenie, ≈ºe kto≈õ ma ESP, gdy mia≈Ç po prostu szczƒô≈õcie\nOgraniczamy to ryzyko do 5% poprzez poziom istotno≈õci\nTo jak b≈Çƒôdne potwierdzenie zdolno≈õci ESP\n\nB≈ÇƒÖd Typu II (Fa≈Çszywie Negatywny):\n\nNiewychwycenie rzeczywistych zdolno≈õci ESP, lub faktu, ≈ºe moneta jest obciƒÖ≈ºona\nBardziej prawdopodobny przy:\n\nWsp√≥≈Çczynniku ‚Äúsukcesu‚Äù (prawdopodobie≈Ñstwo ‚Äúsukcesu‚Äù) niewiele powy≈ºej 0.5\nMa≈Çej liczbie pr√≥b\nRygorystycznych poziomach istotno≈õci\n\n\n\n\n\n\n\n\n\n\n\nObliczenie Warto≈õci p dla Przyk≈Çadu ESP\n\n\n\nSzczeg√≥≈Çy Obliczeniowe\nDla naszego testu ESP z 70 sukcesami na 100 pr√≥b, obliczamy:\nP(X \\geq 70) = \\sum_{k=70}^{100} \\binom{100}{k}(0.5)^k(0.5)^{100-k} \\approx 0.0000393\nOznacza to, ≈ºe przy hipotezie zerowej (czyste zgadywanie):\n\nPrawdopodobie≈Ñstwo uzyskania 70 lub wiƒôcej poprawnych przewidywa≈Ñ przez przypadek wynosi oko≈Ço 0.00393%\nTak skrajne wyniki wystƒÖpi≈Çyby przypadkowo tylko oko≈Ço 4 razy na 100,000 pr√≥b\nJest to znacznie poni≈ºej konwencjonalnego poziomu istotno≈õci 0.05 (5%)\n\nDecyzja Statystyczna\nPoniewa≈º nasza warto≈õƒá p (0.0000393) jest znacznie mniejsza ni≈º Œ± = 0.05:\n\nOdrzucamy hipotezƒô zerowƒÖ\nWnioskujemy, ≈ºe istniejƒÖ silne dowody statystyczne przeciwko ‚Äúlosowemu zgadywaniu‚Äù\nWynik jest uznawany za ‚Äúwysoce istotny statystycznie‚Äù\n\nOstro≈ºna Interpretacja\nMimo ≈ºe nasz wynik jest istotny statystycznie, powinni≈õmy rozwa≈ºyƒá:\n\nIstotno≈õƒá statystyczna nie dowodzi istnienia ESP (moneta mo≈ºe byƒá nieuczciwa?)\nEksperyment powinien byƒá powtarzalny w kontrolowanych warunkach",
    "crumbs": [
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Wprowadzenie do Wnioskowania Statystycznego: Logika Testowania Hipotez Statystycznych</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\n\n\n\nBlair, G., Coppock, A., & Humphreys, M. (2023). Research design in the social sciences: declaration, diagnosis, and redesign. Princeton University Press. https://book.declaredesign.org/\nBryman, A., 2016. Social research methods. Oxford University Press.\nBueno de Mesquita, Ethan and Anthony Fowler. 2021. Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis. Princeton University Press.\nCausality for Machine Learning. https://ff13.fastforwardlabs.com/\nCetinkaya-Rundel, M., Diez, D.M. and Barr, C.D., 2019 (4th ed.). OpenIntro Statistics: an Open-source Textbook: https://www.openintro.org/book/os/\nClaude [Large language model], 2024. https://www.anthropic.com\nConcepts and Computation: An Introduction to Political Methodology. https://pos3713.github.io/notes/\nHannay, K. (2019). Introduction to statistics and data science. http://khannay.com/StatsBook/\nIsmay, C. and Kim, A.Y., 2019. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. https://moderndive.com/index.html\nNavarro, D.J. and Foxcroft, D.R. (2019). Learning statistics with Jamovi: a tutorial for psychology students and other beginners. (Version 0.70). DOI: 10.24384/hgc3-7p15\nRemler, D.K. and Van Ryzin, G.G., 2014. Research methods in practice: Strategies for description and causation. Sage Publications.\nSanchez, G., Marzban, E. (2020) All Models Are Wrong: Concepts of Statistical Learning. https://allmodelsarewrong.github.io\nSchneider, W. J. (2023). Psycheval: A psychological evaluation toolkit. https://github.com/wjschne/psycheval\nTimbers, T., Campbell, T., & Lee, M. (2022). Data science: A first introduction. Chapman and Hall/CRC. https://datasciencebook.ca/",
    "crumbs": [
      "References"
    ]
  }
]