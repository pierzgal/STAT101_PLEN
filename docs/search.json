[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics: An Introduction (PL: Wprowadzenie do Statystyki)",
    "section": "",
    "text": "Preface\n\n\n\n\n\n\nImportant\n\n\n\nThis is a preliminary (unfinished) draft of a Quarto class notes on Statistics. Please do not cite or reproduce its contents, as it may contain errors!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "",
    "text": "1.1 Introduction\nStatistics is a way to learn about the world from data. It teaches how to collect data wisely, spot patterns, estimate population parameters, and make predictions‚Äîstating how wrong we might be.\nImagine you want to understand the average age at first marriage in your country. You cannot possibly ask every single person when they first married (if at all).\nStatistics provides the tools to:\nStatistics and demography are interconnected disciplines that provide powerful tools for understanding populations, their characteristics, and the patterns that emerge from data.\nThe field of statistics can be broadly divided into two complementary branches:",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#introduction",
    "href": "chapter1.html#introduction",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "",
    "text": "Statistics is the science of learning from data under uncertainty.\n\n\n\n\n\n\n\n\nNote\n\n\n\nStatistics is the science of collecting, organizing, analyzing, interpreting, and presenting data. It encompasses both the methods for working with data and the theoretical foundations that justify these methods. But statistics is more than just numbers and formulas‚Äîit‚Äôs a way of thinking about uncertainty and variation in the world around us.\n\n\n\n\n\nSelect a representative group of people to survey\nCalculate meaningful summaries from their responses\nEstimate the likely average for the entire population\nQuantify how confident you can be in your estimate\nTest whether the average age is changing over time\n\n\n\nDemography is the scientific study of human populations, focusing on their size, structure, distribution, and changes over time. It‚Äôs essentially the statistical analysis of people - who they are, where they live, how many there are, and how these characteristics evolve.\n\n\n\n\n\n\n\n\n\nPercent vs Percentage Points (pp)\n\n\n\nWhen news reports say ‚Äúunemployment decreased by 2,‚Äù do they mean 2 percentage points (pp) or 2 percent?\nThese are not the same:\n\n2 pp (absolute change): e.g., 10% ‚Üí 8% (‚àí2 pp).\n2% (relative change): multiply the old rate by 0.98; e.g., 10% ‚Üí 9.8% (‚àí0.2 pp).\n\nAlways ask:\n\nWhat is the baseline (earlier rate)?\nIs the change absolute (pp) or relative (%)?\nCould this be sampling error / random variation?\nHow was unemployment measured (survey vs.¬†administrative), when, and who‚Äôs included?\n\nRule of thumb\n\nUse percentage points (pp) when comparing rates directly (unemployment, turnout).\nUse percent (%) for relative changes (proportional to the starting value).\n\nTiny lookup table\n\n\n\n\n\n\n\n\nStarting rate\n‚ÄúDown 2%‚Äù (relative)\n‚ÄúDown 2 pp‚Äù (absolute)\n\n\n\n\n6%\n6% √ó 0.98 = 5.88% (‚àí0.12 pp)\n4%\n\n\n8%\n8% √ó 0.98 = 7.84% (‚àí0.16 pp)\n6%\n\n\n10%\n10% √ó 0.98 = 9.8% (‚àí0.2 pp)\n8%\n\n\n\nUwaga (PL): 2% ‚â† 2 punkty procentowe (pp).\n\n\n\n\n\n\n\n\n\nRounding and Scientific Notation in Statistics\n\n\n\nMain Rule: Unless otherwise specified, round the decimal parts of decimal numbers to at least 2 significant figures. In statistics, we often work with long decimal parts and very small numbers ‚Äî don‚Äôt round excessively in intermediate steps, round at the end of calculations.\n\nRounding in Statistical Context\nThe decimal part consists of digits after the decimal point. In statistics, it‚Äôs particularly important to maintain appropriate precision:\nDescriptive statistics:\n\nMean: \\bar{x} = 15.847693... \\rightarrow 15.85\nStandard deviation: s = 2.7488... \\rightarrow 2.75\nCorrelation coefficient: r = 0.78432... \\rightarrow 0.78\n\nVery small numbers (p-values, probabilities):\n\np = 0.000347... \\rightarrow 0.00035 or 3.5 \\times 10^{-4}\nP(X &gt; 2) = 0.0000891... \\rightarrow 0.000089 or 8.9 \\times 10^{-5}\n\n\n\nSignificant Figures in Decimal Parts\nIn the decimal part, significant figures are all digits except leading zeros:\n\n.78432 has 5 significant figures ‚Üí round to .78 (2 s.f.)\n.000347 has 3 significant figures ‚Üí round to .00035 (2 s.f.)\n.050600 has 4 significant figures ‚Üí round to .051 (2 s.f.)\n\n\n\nRounding Rules in Statistics\n\nRound only the decimal part to at least 2 significant figures\nThe integer part remains unchanged\nIn long calculations keep 3-4 digits in the decimal part until the final step\nNEVER round to zero - small values have interpretive significance\nFor very small numbers use scientific notation when it improves readability\nP-values often require greater precision ‚Äî keep 2-3 significant figures\n\n‚ö†Ô∏è WARNING: Don‚Äôt round to zero!\n\n\nScientific Notation in Statistics\nIn statistics, we often encounter very small numbers. Use scientific notation when it improves readability:\nP-values and probabilities:\n\np = 0.000347 = 3.47 \\times 10^{-4} (better: 3.5 \\times 10^{-4})\nP(Z &gt; 3.5) = 0.000233 = 2.33 \\times 10^{-4}\n\nVery small standard deviations:\n\n\\sigma = 0.000892 = 8.92 \\times 10^{-4}\n\nLarge numbers (rare in basic statistics):\n\nN = 1\\,234\\,567 = 1.23 \\times 10^6\n\nWhen in doubt: Better to keep an extra digit than to round too aggressively\n\n\n\n\n\n\nDescriptive Statistics\nDescriptive Statistics involves methods for summarizing and presenting data in meaningful ways. This includes:\nMeasures of Central Tendency - Where is the center of your data?\n\nMean: The arithmetic average. If the total household income in a village of 100 households is $5,000,000, the mean income is $50,000. However, if one household is extremely wealthy, this might not represent the typical household well.\nMedian: The middle value when data is ordered. If most households earn between $20,000-$40,000 but one earns $2,000,000, the median better represents the typical household.\nMode: The most frequent value. In studying family size, if most families have 2 children, 2 is the mode, even if the mean is 2.3 children.\n\nMeasures of Variability - How spread out is your data?\n\nRange: The difference between maximum and minimum values. If ages in a community range from 0 to 95 years, the range is 95 years.\nVariance: The average squared deviation from the mean. Measures how far values typically fall from the center.\nStandard Deviation: The square root of variance, in the same units as the original data. If the mean age is 35 years with a standard deviation of 20 years, most people are between 15 and 55 years old.\n\nVisual Representations (examples)\n\nPopulation Pyramids: Show age and sex distribution, revealing demographic history. A wide base indicates high birth rates; a narrow base suggests declining fertility.\nLife Tables: Summarize mortality patterns, showing the probability of surviving to each age.\nTime Series Graphs: Display trends over time, such as changing fertility rates across decades.\n\n\n\n\nhttps://commons.wikimedia.org/wiki/File:%C5%81%C3%B3d%C5%BA_population_pyramid.svg\n\n\n\n\nInferential Statistics\nStatistical inference means using a sample to learn about a population. We estimate effects, describe uncertainty (margin of error, confidence interval), and ask whether results could be explained by chance‚Äîacross experiments (treatment vs.¬†control), regression (how changes in X relate to Y), and surveys (estimating proportions or means for the whole population).\n\n\n\n\n\n\n\nA Soup-Tasting Analogy\n\n\n\n\nConsider a chef preparing soup for 100 people who needs to assess its flavor without consuming the entire batch:\nPopulation: The entire pot of soup (100 servings)\nSample: A single spoonful for tasting\nPopulation Parameter: The true average saltiness of the complete pot (unknown)\nSample Statistic: The saltiness level detected in the spoonful (observable)\nStatistical Inference: Using the spoonful‚Äôs characteristics to draw conclusions about the entire pot\n\nKey Principles\n1. Random sampling is essential: The chef must thoroughly stir the soup before sampling. Consistently sampling from the surface might miss seasoning that has settled, introducing systematic bias.\n2. Sample size affects precision: A larger spoonful provides more reliable information about overall flavor than a small sip, though practical constraints (costs, time) limit sample size.\n3. Uncertainty is inherent: Even with proper sampling technique, the spoonful might not perfectly represent the entire pot‚Äôs characteristics.\n4. Systematic bias undermines inference: If someone secretly adds salt only to the sampling area, conclusions about the whole pot become invalid‚Äîillustrating how sampling bias distorts statistical inference.\n5. Inference has scope limitations: The sample can estimate average saltiness but cannot reveal whether some portions are saltier than others, highlighting the limits of what samples can tell us about population variability.\nThis analogy captures the essence of statistical reasoning: using carefully selected samples to learn about larger populations while explicitly acknowledging and quantifying the inherent uncertainty in this process.\n\n\n\nFor example, if a survey of 1,000 households finds that 23% include three generations living together, inferential statistics helps us:\n\nEstimate that between 20% and 26% of all households in the population likely have this structure (confidence interval)\nTest whether this percentage has increased compared to a decade ago (hypothesis testing)\nExamine whether multigenerational living is more common in certain ethnic groups (comparison of groups)\nPredict future trends based on current patterns (regression and forecasting)\n\n\n\n\n\n\n\n\nNote\n\n\n\nFundamental Principle: Statistics does not eliminate uncertainty‚Äîit helps us measure, manage, and communicate it effectively.\n\n\n\n\nStatistical Thinking: A Quick Start\n\nThe Scenario\nYour university is considering keeping the library open 24/7. The administration needs to know: What proportion of students support this change?\n\n\n\n\n\n\nThe Fundamental Challenge\n\n\n\nIdeal world: Ask all 20,000 students ‚Üí Get the exact answer\nReal world: Survey 100 students ‚Üí Get an estimate with uncertainty\n\n\n\n\nTwo Approaches to the Same Data\nImagine you survey 100 random students and find that 60 support the 24/7 library hours.\n\n\n‚ùå Without Statistical Thinking\n‚Äú60 out of 100 students said yes.‚Äù\nConclusion: ‚ÄúExactly 60% of all students support it.‚Äù\nDecision: ‚ÄúSince it‚Äôs over 50%, we have clear majority support.‚Äù\nProblem: Ignores that a different sample might give 55% or 65%\n\n‚úÖ With Statistical Thinking\n‚Äú60 out of 100 students said yes.‚Äù\nConclusion: ‚ÄúWe estimate 60% support, with a margin of error of ¬±10%‚Äù\nDecision: ‚ÄúTrue support is likely between 50% and 70%‚Äîwe need more data to be certain of majority support.‚Äù\nAdvantage: Acknowledges uncertainty and informs better decisions\n\n\n\n\nWhy This Matters\n\n\n\n\n\n\nThe Key Insight\n\n\n\nStatistical thinking isn‚Äôt about being less certain‚Äîit‚Äôs about being honest about our uncertainty.\nConsider these scenarios with your 60% result (¬±10% margin):\n\nBest case: True support could be 70% (strong majority)\nWorst case: True support could be 50% (no clear majority)\nMost likely: True support is near 60% (moderate majority)\n\nWithout acknowledging this range, you might make costly decisions based on false precision.\n\n\n\n\nThe Power of Sample Size\nWhat if we need more precision? Here‚Äôs how sample size affects our confidence:\n\n\n\n\n\n\n\n\n\n\nSample Size\nYour Result\nMargin of Error\nConfidence Interval\nInterpretation\n\n\n\n\nn = 100\n60%\n¬±10%\n50% to 70%\nUncertain about majority\n\n\nn = 400\n60%\n¬±5%\n55% to 65%\nLikely majority support\n\n\nn = 1,600\n60%\n¬±2.5%\n57.5% to 62.5%\nClear majority support\n\n\n\n\n\n\n\n\n\nRemember\n\n\n\nStatistical thinking transforms ‚Äú60 students said yes‚Äù from a precise-sounding but misleading statement into an honest assessment: ‚ÄúWe‚Äôre reasonably confident that between 50% and 70% of all students support this.‚Äù\nThis humility leads to better decisions.\n\n\n\n\nYour Turn to Think Statistically\nNext time you see a statistic, ask yourself:\n\nWhat‚Äôs the sample size?\nWhat‚Äôs the margin of statistical error?\nCould the true value be meaningfully different from what‚Äôs reported?\nWould my decision change if the true value were at the edge of the uncertainty range?\n\nThese questions are the foundation of statistical thinking.\n\n\n\n\n\n\nHistorical Example: The 1936 Literary Digest Poll\n\n\n\nThe Literary Digest conducted one of the largest polls in history with 2.4 million responses, predicting Alf Landon would defeat Franklin D. Roosevelt in the 1936 presidential election.\nDespite the massive sample size:\nPrediction: Landon 57%, Roosevelt 43%\nActual Result: Roosevelt 62%, Landon 38%\nError: 25 percentage points!\nWhat went wrong? The poll suffered from systematic bias:\nSelection bias in sampling frame:\n\nSources: telephone directories, automobile registrations, club memberships\nProblem: In 1936, these sources overrepresented wealthy Americans who favored Landon\nResult: The sample systematically excluded Roosevelt supporters\n\nNon-response bias:\n\nOnly 24% of those contacted responded (low response rate)\nLikely respondents: those with strong anti-Roosevelt opinions\nNon-respondents: many Roosevelt supporters didn‚Äôt feel compelled to participate\n\nKey Lessons:\n\nA large biased sample is worse than a small representative sample\nStandard errors only measure random error, not bias\n\nSample size cannot fix fundamental sampling problems\nRepresentative sampling matters more than sample size\n\nThis disaster led to major improvements in polling methodology, including the development of probability sampling and response rate tracking.\n\n\n\n\n\nModern Polling\nToday‚Äôs polls, while much smaller than the Literary Digest‚Äôs 2.4 million responses, are far more accurate because they focus on:\nRepresentative sampling: Using probability-based methods to ensure all groups have known chances of selection\nBias detection and correction: Monitoring response rates across demographics and adjusting for known biases\nUncertainty quantification: Reporting margins of statistical error that honestly communicate the limits of what we know\nExample: A modern poll of 1,000 randomly selected voters with a 3% margin of error is far more reliable than the Literary Digest‚Äôs massive but biased survey.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#data-and-distributions",
    "href": "chapter1.html#data-and-distributions",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.2 Data and Distributions",
    "text": "1.2 Data and Distributions\nData: Information collected during research ‚Äì this includes survey responses, experimental results, economic indicators, social media content, or any other measurable observations.\nUnderstanding data types and distributions is fundamental to choosing appropriate analyses and interpreting results correctly.\n\n\n\n\n\n\nUnderstanding Different Types of Data Sets and Their Formats\n\n\n\n\nCross-sectional Data\nObservations for variables (columns in a database) collected at a single point in time across multiple entities/individuals:\n\n\n\nIndividual\nAge\nIncome\nEducation\n\n\n\n\n1\n25\n50000\nBachelor‚Äôs\n\n\n2\n35\n75000\nMaster‚Äôs\n\n\n3\n45\n90000\nPhD\n\n\n\n\n\nTime Series Data\nObservations of a single entity tracked over multiple time points:\n\n\n\nYear\nGDP (in billions)\nUnemployment Rate\n\n\n\n\n2018\n20,580\n3.9%\n\n\n2019\n21,433\n3.7%\n\n\n2020\n20,933\n8.1%\n\n\n\n\n\nPanel Data (Longitudinal Data)\nObservations of multiple entities tracked over time:\n\n\n\nCountry\nYear\nGDP per capita\nLife Expectancy\n\n\n\n\nUSA\n2018\n62,794\n78.7\n\n\nUSA\n2019\n65,118\n78.8\n\n\nCanada\n2018\n46,194\n81.9\n\n\nCanada\n2019\n46,194\n82.0\n\n\n\n\n\nTime-series Cross-sectional (TSCS) Data\nA special case of panel data where:\n\nNumber of time points &gt; Number of entities\nSimilar structure to panel data but with emphasis on temporal depth\nCommon in political science and economics research\n\n\n\n\nData Formats\n\nWide Format\nEach row represents an entity; columns represent variables/time points:\n\n\n\nCountry\nGDP_2018\nGDP_2019\nLE_2018\nLE_2019\n\n\n\n\nUSA\n62,794\n65,118\n78.7\n78.8\n\n\nCanada\n46,194\n46,194\n81.9\n82.0\n\n\n\n\n\nLong Format\nEach row represents a unique entity-time-variable combination:\n\n\n\nCountry\nYear\nVariable\nValue\n\n\n\n\nUSA\n2018\nGDP per capita\n62,794\n\n\nUSA\n2019\nGDP per capita\n65,118\n\n\nUSA\n2018\nLife Expectancy\n78.7\n\n\nUSA\n2019\nLife Expectancy\n78.8\n\n\nCanada\n2018\nGDP per capita\n46,194\n\n\nCanada\n2019\nGDP per capita\n46,194\n\n\nCanada\n2018\nLife Expectancy\n81.9\n\n\nCanada\n2019\nLife Expectancy\n82.0\n\n\n\nNote: Long format is generally preferred for:\n\nData manipulation in R and Python\nStatistical analysis\nData visualization\n\n\n\n\n\n\n\nTypes of Data\nData consists of collected observations or measurements. The type of data determines what mathematical operations (e.g.¬†multiplication) are meaningful and what statistical methods apply.\n\nQuantitative Data\nContinuous Data can take any value within a range:\nExamples with Demographic Relevance:\n\nAge: Can be 25.5 years, 25.51 years, 25.514 years (precision limited only by measurement)\nBody Mass Index: 23.7 kg/m¬≤\nFertility Rate: 1.73 children per woman\nPopulation Density: 4,521.3 people per km¬≤\n\nProperties:\n\nCan perform all arithmetic operations\nCan calculate means, standard deviations\nOften follow known probability distributions (e.g.¬†weight and Normal distribution)\n\nDiscrete Data can only take specific values:\nExamples:\n\nNumber of Children: 0, 1, 2, 3‚Ä¶ (can‚Äôt have 2.5 children)\nNumber of Marriages: 0, 1, 2, 3‚Ä¶\nHousehold Size: 1, 2, 3, 4‚Ä¶ people\nNumber of Doctor Visits: 0, 1, 2, 3‚Ä¶ per year\n\n\n\nQualitative/Categorical Data\nNominal Data represents categories with no inherent order:\nExamples:\n\nCountry of Birth: USA, China, India, Brazil‚Ä¶\nReligion: Christian, Muslim, Hindu, Buddhist, None‚Ä¶\nMarital Status: Single, Married, Divorced, Widowed\nCause of Death: Heart disease, Cancer, Accident, Stroke‚Ä¶\nBlood Type: A, B, AB, O\n\nWhat We Can Do:\n\nCount frequencies\nCalculate proportions\nFind mode\nTest for independence\n\nWhat We Cannot Do:\n\nCalculate mean (average religion makes no sense)\nOrder categories meaningfully\nCompute distances between categories\n\nOrdinal Data represents ordered categories:\nExamples:\n\nEducation Level: None &lt; Primary &lt; Secondary &lt; Tertiary\nSocioeconomic Status: Low &lt; Middle &lt; High\nSelf-Rated Health: Poor &lt; Fair &lt; Good &lt; Excellent\nAgreement Scale: Strongly Disagree &lt; Disagree &lt; Neutral &lt; Agree &lt; Strongly Agree\n\nThe Challenge: Intervals between categories aren‚Äôt necessarily equal. The ‚Äúdistance‚Äù from Poor to Fair health may not equal the distance from Good to Excellent.\n\n\n\nData Distribution\nA data distribution describes how values spread across possible outcomes (what values and how often a variable takes). Distributions tell us what values are common, what values are rare, and what patterns exist in our data.\nUnderstanding distributions is fundamental to statistics because it helps us summarize large datasets, identify patterns, and make informed decisions.\nFor example, knowing that most students score between 60-80 on an exam tells us more than just knowing the average score.\n\nFrequency, Relative Frequency, and Density\nWhen we analyze data, we‚Äôre often interested in how many times each value (or range of values) appears. This leads us to three related concepts:\n(Absolute) Frequency is simply the count of how many times a particular value or category occurs in your data. If 15 students scored between 70-80 points on an exam, the frequency for that range is 15.\nRelative frequency expresses frequency as a proportion or percentage of the total. It answers the question: ‚ÄúWhat fraction of all observations fall into this category?‚Äù Relative frequency is calculated as:\n\\text{Relative Frequency} = \\frac{\\text{Frequency}}{\\text{Total Number of Observations}}\nIf 15 out of 100 students scored 70-80 points, the relative frequency is 15/100 = 0.15 or 15%. Relative frequencies always sum to 1 (or 100%), making them useful for comparing distributions with different sample sizes.\nDensity is similar to relative frequency but accounts for the width of intervals. When we group continuous data (like time or unemployment rate) into bins of different widths, density ensures fair comparison. Density is calculated as:\n\\text{Density} = \\frac{\\text{Relative Frequency}}{\\text{Interval Width}}\nDensity is particularly important for continuous variables because it ensures that the total area under the distribution equals 1, which allows us to interpret areas as probabilities.\n\n\n\n\n\n\nTip\n\n\n\nThe probability of an event is a number between 0 and 1; the larger the probability, the more likely an event is to occur.\n\n\nCumulative frequency tells us how many observations fall at or below a certain value. Instead of asking ‚Äúhow many observations are in this category?‚Äù, cumulative frequency answers ‚Äúhow many observations are in this category or any category below it?‚Äù It‚Äôs calculated by adding up all frequencies from the lowest value up to and including the current value.\nSimilarly, cumulative relative frequency expresses this as a proportion of the total, answering ‚Äúwhat percentage of observations fall at or below this value?‚Äù This is particularly useful for understanding percentiles and quartiles. For example, if the cumulative relative frequency at score 70 is 0.40, this means 40% of students scored 70 or below.\n\n\nDistribution Tables\nA frequency distribution table organizes data by showing how observations are distributed across different values or intervals. Here‚Äôs an example with exam scores:\n\n\n\n\n\n\n\n\n\n\n\nScore Range\nFrequency\nRelative Frequency\nCumulative Frequency\nCumulative Relative Frequency\nDensity\n\n\n\n\n0-50\n10\n0.10\n10\n0.10\n0.002\n\n\n50-70\n30\n0.30\n40\n0.40\n0.015\n\n\n70-90\n45\n0.45\n85\n0.85\n0.0225\n\n\n90-100\n15\n0.15\n100\n1.00\n0.015\n\n\nTotal\n100\n1.00\n-\n-\n-\n\n\n\nThis table immediately reveals that most students scored in the 70-90 range, while very few scored below 50 or above 90. The cumulative columns show us that 40% of students scored below 70, and 85% scored below 90. Such tables are invaluable for getting a quick overview of your data before conducting more complex analyses.\n\n\nVisualizing Distributions: Histograms\nA histogram is a graphical representation of a frequency distribution. It displays data using bars where:\n\nThe x-axis shows the values or intervals (bins)\nThe y-axis can show frequency, relative frequency, or density\nThe height of each bar represents the count, proportion, or density for that interval\nBars touch each other (no gaps) for continuous variables\n\nChoosing bin widths: The number and width of bins significantly affects how your histogram looks. Too few bins hide important patterns, while too many bins create ‚Äúnoise‚Äù and make patterns hard to see.\n\nIn statistics, noise is unwanted random variation that obscures the pattern we‚Äôre trying to find. Think of it like static on a radio‚Äîit makes the music (the ‚Äúsignal‚Äù) harder to hear. In data, noise comes from measurement errors, random fluctuations, or the inherent variability in what we‚Äôre studying. Noise is random variation in data that hides the real patterns we want to see, similar to how background noise makes conversation difficult to hear.\n\nSeveral approaches help determine appropriate bin widths:\n\nSturges‚Äô rule: Use k = 1 + \\log_2(n) bins, where n is the sample size. This works well for roughly symmetric distributions.\nSquare root rule: Use k = \\sqrt{n} bins. A simple, reasonable default for many situations.\n\nIn R, you can specify bins in several ways:\n\n# Specify number of bins\nhist(exam_scores, breaks = 10)\n\n\n\n\n\n\n\n# Specify exact break points\nhist(exam_scores, breaks = seq(0, 100, by = 10))\n\n\n\n\n\n\n\n# Let R choose automatically (uses Sturges' rule by default)\nhist(exam_scores)\n\n\n\n\n\n\n\n\nThe best approach is often to experiment with different bin widths to find what best reveals your data‚Äôs pattern. Start with a default, then try fewer and more bins to see how the story changes.\nDefining bin boundaries: When creating bins for a frequency table, you must decide how to handle values that fall exactly on the boundaries. For example, if you have bins 0-10 and 10-20, which bin does the value 10 belong to?\nThe solution is to use interval notation to specify whether each boundary is included or excluded:\n\nClosed interval [a, b] includes both endpoints: a \\leq x \\leq b\nOpen interval (a, b) excludes both endpoints: a &lt; x &lt; b\nHalf-open interval [a, b) includes the left endpoint but excludes the right: a \\leq x &lt; b\nHalf-open interval (a, b] excludes the left endpoint but includes the right: a &lt; x \\leq b\n\nStandard convention: Most statistical software, including R, uses left-closed, right-open intervals [a, b) for all bins except the last one, which is fully closed [a, b]. This means:\n\nThe value at the lower boundary is included in the bin\nThe value at the upper boundary belongs to the next bin\nThe very last bin includes both boundaries to capture the maximum value\n\nFor example, with bins 0-20, 20-40, 40-60, 60-80, 80-100:\n\n\n\nScore Range\nInterval Notation\nValues Included\n\n\n\n\n0-20\n[0, 20)\n0 ‚â§ score &lt; 20\n\n\n20-40\n[20, 40)\n20 ‚â§ score &lt; 40\n\n\n40-60\n[40, 60)\n40 ‚â§ score &lt; 60\n\n\n60-80\n[60, 80)\n60 ‚â§ score &lt; 80\n\n\n80-100\n[80, 100]\n80 ‚â§ score ‚â§ 100\n\n\n\nThis convention ensures that:\n\nEvery value is counted exactly once (no double-counting)\nNo values fall through the cracks\nThe bins partition the entire range completely\n\nWhen presenting frequency tables in reports, you can simply write ‚Äú0-20, 20-40, ‚Ä¶‚Äù and note that bins are left-closed, right-open, or explicitly show the interval notation if precision is important.\nFrequency histogram shows the raw counts:\n\n# R code example\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     main = \"Distribution of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Frequency\",\n     col = \"lightblue\")\n\n\n\n\n\n\n\n\nRelative frequency histogram shows proportions (useful when comparing groups of different sizes):\n\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     freq = FALSE,  # This creates relative frequency/density\n     main = \"Distribution of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Relative Frequency\",\n     col = \"lightgreen\")\n\n\n\n\n\n\n\n\nDensity histogram adjusts for interval width and is used with density curves:\n\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     freq = FALSE,  # Creates density scale\n     main = \"Distribution of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Density\",\n     col = \"lightcoral\")\n\n\n\n\n\n\n\n\n\n\nDensity Curves\nA density curve is a smooth line that approximates/models the shape of a distribution. Unlike histograms that show actual data in discrete bins, density curves show the overall pattern as a continuous function. The area under the entire curve always equals 1, and the area under any portion of the curve represents the proportion of observations in that range.\n\n# Adding a density curve to a histogram\nhist(exam_scores, \n     freq = FALSE,\n     main = \"Exam Scores with Density Curve\",\n     xlab = \"Score\",\n     ylab = \"Density\",\n     col = \"lightblue\",\n     border = \"white\")\nlines(density(exam_scores), \n      col = \"darkred\", \n      lwd = 2)\n\n\n\n\n\n\n\n\nDensity curves are particularly useful for:\n\nIdentifying the shape of the distribution (symmetric, skewed, bimodal)\nComparing multiple distributions on the same plot\nUnderstanding the theoretical (true) distribution underlying your data\n\n\n\nVisualizing Cumulative Frequency\nCumulative frequency plots, also called ogives (pronounced ‚Äúoh-jive‚Äù), display how frequencies accumulate across values. These plots use lines rather than bars and always increase from left to right, eventually reaching the total number of observations (for cumulative frequency) or 1.0 (for cumulative relative frequency).\nCumulative frequency plots are excellent for:\n\nFinding percentiles and quartiles visually\nDetermining what proportion of data falls below or above a certain value\nComparing distributions of different groups\n\n\n\n\n\n\n\nTip\n\n\n\nIn statistics, a percentile indicates the relative position of a data point within a dataset by showing the percentage of observations that fall at or below that value. For example, if a student scores at the 90th percentile on a test, their score is equal to or higher than 90% of all other scores.\nQuartiles are special percentiles that divide data into four equal parts: the first quartile (Q1, 25th percentile), second quartile (Q2, 50th percentile, also the median), and third quartile (Q3, 75th percentile). If Q1 = 65 points, then 25% of students scored 65 or below.\nMore generally, quantiles are values that divide data into equal-sized groups‚Äîpercentiles divide into 100 parts, quartiles into 4 parts, deciles into 10 parts, and so on.\n\n\n\n# Creating cumulative frequency data\nscore_breaks &lt;- seq(0, 100, by = 10)\nfreq_counts &lt;- hist(exam_scores, breaks = score_breaks, plot = FALSE)$counts\ncumulative_freq &lt;- cumsum(freq_counts)\n\n# Plotting cumulative frequency\nplot(score_breaks[-1], cumulative_freq,\n     type = \"b\",  # both points and lines\n     main = \"Cumulative Frequency of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Cumulative Frequency\",\n     col = \"darkblue\",\n     lwd = 2,\n     pch = 19)\ngrid()\n\n\n\n\n\n\n\n\nFor cumulative relative frequency (which is more commonly used):\n\n# Cumulative relative frequency\ncumulative_rel_freq &lt;- cumulative_freq / length(exam_scores)\n\nplot(score_breaks[-1], cumulative_rel_freq,\n     type = \"b\",\n     main = \"Cumulative Relative Frequency of Exam Scores\",\n     xlab = \"Score\",\n     ylab = \"Cumulative Relative Frequency\",\n     col = \"darkred\",\n     lwd = 2,\n     pch = 19,\n     ylim = c(0, 1))\ngrid()\nabline(h = c(0.25, 0.5, 0.75), lty = 2, col = \"gray\")  # Quartile lines\n\n\n\n\n\n\n\n\nThe cumulative relative frequency curve makes it easy to read percentiles. For example, if you draw a horizontal line at 0.75 and see where it intersects the curve, the corresponding x-value is the 75th percentile‚Äîthe score below which 75% of students fall.\n\n\nDiscrete vs.¬†Continuous Distributions\nThe type of variable you‚Äôre analyzing determines how you visualize its distribution:\nDiscrete distributions apply to variables that can only take specific, countable values. Examples include number of children in a family (0, 1, 2, 3‚Ä¶), number of customer complaints per day, or responses on a 5-point Likert scale.\nFor discrete data, we typically use:\n\nBar charts (with gaps between bars) rather than histograms\nFrequency or relative frequency on the y-axis\nEach distinct value gets its own bar\n\n\n# Example: Number of children per family\nchildren &lt;- c(0, 1, 2, 2, 1, 3, 0, 2, 1, 4, 2, 1, 0, 2, 3)\nbarplot(table(children),\n        main = \"Distribution of Number of Children\",\n        xlab = \"Number of Children\",\n        ylab = \"Frequency\",\n        col = \"skyblue\")\n\n\n\n\n\n\n\n\nContinuous distributions apply to variables that can take any value within a range. Examples include temperature, response time, height, or turnout percentage.\nFor continuous data, we use:\n\nHistograms (with touching bars) that group data into intervals\nDensity curves to show the smooth pattern\nDensity on the y-axis when using density curves\n\n\n# Example: Response time distribution\nhist(response_time, \n     breaks = 15,\n     freq = FALSE,\n     main = \"Distribution of Response Time\",\n     xlab = \"Response Time (seconds)\",\n     ylab = \"Density\",\n     col = \"lightgreen\",\n     border = \"white\")\nlines(density(response_time), \n      col = \"darkgreen\", \n      lwd = 2)\n\n\n\n\n\n\n\n\nThe key difference is that discrete distributions show probability at specific points, while continuous distributions show probability density across ranges. For continuous variables, the probability of any exact value is essentially zero‚Äîinstead, we talk about the probability of falling within an interval.\nUnderstanding whether your variable is discrete or continuous guides your choice of visualization and statistical methods, ensuring your analysis accurately represents the nature of your data.\n\n\nDescribing Distributions\nShape Characteristics:\nSymmetry vs.¬†Skewness:\n\nSymmetric: Mirror image around center (example: heights in homogeneous population)\nRight-skewed (positive skew): Long tail to right (example: income, wealth)\nLeft-skewed (negative skew): Long tail to left (example: age at death in developed countries)\n\nExample of Skewness Impact:\nIncome distribution in the U.S.:\n\nMedian household income: ~$70,000\nMean household income: ~$100,000\nMean &gt; Median indicates right skew\nA few very high incomes pull the mean up\n\n\nModality:\n\nUnimodal: One peak (example: test scores)\nBimodal: Two peaks (example: height when mixing males and females)\nMultimodal: Multiple peaks (example: age distribution in a college town‚Äîpeaks at college age and middle age)\n\n\n\n\n\n\n\n\n\n\nImportant Probability Distributions:\nNormal (Gaussian) Distribution:\n\nBell-shaped, symmetric\nCharacterized by mean (\\mu) and standard deviation (\\sigma)\nAbout 68% of values within \\mu \\pm \\sigma\nAbout 95% within \\mu \\pm 2\\sigma\nAbout 99.7% within \\mu \\pm 3\\sigma\n\nDemographic Applications:\n\nHeights within homogeneous populations\nMeasurement errors\nSampling distributions of means (Central Limit Theorem)\n\nBinomial Distribution:\n\nNumber of successes in n independent trials\nEach trial has probability p of success\nMean = np, Variance = np(1-p)\n\nExample: Number of male births out of 100 births (p \\approx 0.512)\nPoisson Distribution:\n\nCount of events in fixed time/space\nMean = Variance = \\lambda\nGood for rare events\n\nDemographic Applications:\n\nNumber of deaths per day in small town\nNumber of births per hour in hospital\nNumber of accidents at intersection per month\n\n\n\n\nVisualizing Frequency Distributions\nHistogram: For continuous data, shows frequency with bar heights.\n\nX-axis: Value ranges (bins)\nY-axis: Frequency or density\nNo gaps between bars (continuous data)\nBin width affects appearance\n\nBar Chart: For categorical data, shows frequency with separated bars.\n\nX-axis: Categories\nY-axis: Frequency\nGaps between bars (discrete categories)\nOrder may or may not matter\n\nCumulative Distribution Function (CDF): Shows proportion of values ‚â§ each point of data.\n\nAlways increases (or stays flat)\nStarts at 0, ends at 1\nSteep slopes indicate common values\nFlat areas indicate rare values\n\nBox Plot (Box-and-Whisker Plot): A visual summary that displays the distribution‚Äôs key statistics using five key values.\nThe Five-Number Summary:\n\nMinimum: Leftmost whisker end (excluding outliers)\nQ1 (First Quartile): Left edge of the box (25th percentile)\nMedian (Q2): Line inside the box (50th percentile)\n\nQ3 (Third Quartile): Right edge of the box (75th percentile)\nMaximum: Rightmost whisker end (excluding outliers)\n\nWhat It Reveals:\n\nSkewness: If median line is off-center in the box, or whiskers are unequal\nSpread: Wider boxes and longer whiskers indicate more variability\nOutliers: Immediately visible as separate points\nSymmetry: Equal whisker lengths and centered median suggest normal distribution\n\nQuick Interpretation:\n\nNarrow box = consistent data\nLong whiskers = wide range of values\n\nMany outliers = potential data quality issues or interesting extreme cases\nMedian closer to Q1 = right-skewed data (tail extends right)\nMedian closer to Q3 = left-skewed data (tail extends left)\n\nBox plots are especially useful for comparing multiple groups side-by-side!",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#populations-and-samples",
    "href": "chapter1.html#populations-and-samples",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.3 Populations and Samples",
    "text": "1.3 Populations and Samples\nUnderstanding the distinction between populations and samples is crucial for proper statistical analysis. This distinction affects every aspect of our analysis, from planning data collection to interpreting results.\n\nPopulation\nA population is the complete set of individuals, objects, or measurements about which we wish to draw conclusions. The key word here is ‚Äúcomplete‚Äù‚Äîa population includes every single member of the group we‚Äôre studying.\nExamples of Populations in Demography:\n\nAll residents of India as of January 1, 2024: This includes every person living in India on that specific date‚Äîapproximately 1.4 billion people.\nAll births in Sweden during 2023: Every baby born within Swedish borders during that calendar year‚Äîroughly 100,000 births.\nAll households in Tokyo: Every residential unit where people live, cook, and sleep separately from others‚Äîabout 7 million households.\nAll deaths from COVID-19 worldwide in 2020: Every death where COVID-19 was listed as a cause‚Äîseveral million deaths.\n\nPopulations can be:\nFinite: Having a countable number of members (all current U.S. citizens, all Polish municipalities in 2024)\nInfinite: Theoretical or uncountably large (all possible future births)\nFixed: Defined at a specific point in time (all residents on census day)\nDynamic: Changing over time (the population of a city that experiences births, deaths, and migration daily)\n\n\nSample\nA sample is a subset of the population that is actually observed or measured. We study samples because examining entire populations is often impossible, impractical, or unnecessary.\nWhy We Use Samples:\nPractical Impossibility: Imagine testing every person in China for a disease. By the time you finished testing 1.4 billion people, the disease situation would have changed completely, and some people tested early would need retesting.\nCost Considerations: The 2020 U.S. Census cost approximately $16 billion. Conducting such complete enumerations frequently would be prohibitively expensive. A well-designed sample survey can provide accurate estimates at a fraction of the cost.\nTime Constraints: Policy makers often need information quickly. A sample survey of 10,000 people can be completed in weeks, while a census takes years to plan, execute, and process.\nDestructive Measurement: Some measurements destroy what‚Äôs being measured. Testing the lifespan of light bulbs or the breaking point of materials requires using samples.\nGreater Accuracy: Surprisingly, samples can sometimes be more accurate than complete enumerations. With a sample, you can afford better training for interviewers, more careful data collection, and more thorough quality checks.\nExample of Sample vs.¬†Population:\nLet‚Äôs say we want to know the average household size in New York City:\n\nPopulation: All 3.2 million households in NYC\nCensus approach: Attempt to contact every household (expensive, time-consuming, some will be missed)\nSample approach: Randomly select 5,000 households, carefully measure their sizes, and use this to estimate the average for all households\nResult: The sample might find an average of 2.43 people per household with a margin of error of ¬±0.05, meaning we‚Äôre confident the true population average is between 2.38 and 2.48",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#superpopulation-and-data-generating-process-dgp",
    "href": "chapter1.html#superpopulation-and-data-generating-process-dgp",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.4 Superpopulation and Data Generating Process (DGP) (*)",
    "text": "1.4 Superpopulation and Data Generating Process (DGP) (*)\n\n\nSuperpopulation\nA superpopulation is a theoretical infinite population from which your finite population is considered to be one random sample.\nThink of it in three levels:\n\nSuperpopulation: An infinite collection of possible values (theoretical)\nFinite population: The actual population you could theoretically census (e.g., all 50 US states, all 10,000 firms in an industry)\nSample: The subset you actually observe (e.g., 30 states, 500 firms)\n\nWhy do we need this concept?\nConsider the 50 US states. You might measure unemployment rate for all 50 states‚Äîa complete census, no sampling needed. But you still want to:\n\nTest if unemployment is related to education levels\nPredict next year‚Äôs unemployment rates\nDetermine if differences between states are ‚Äústatistically significant‚Äù\n\nWithout the superpopulation concept, you‚Äôre stuck‚Äîyou have all the data, so what‚Äôs left to infer? The answer: treat this year‚Äôs 50 values as one draw from an infinite superpopulation of possible values that could occur under similar conditions.\nMathematical representation:\n\nFinite population value: Y_i (state i‚Äôs unemployment rate)\nSuperpopulation model: Y_i = \\mu + \\epsilon_i where \\epsilon_i \\sim (0, \\sigma^2)\nThe 50 observed values are one realization of this process\n\n\n\n\nData Generating Process: The True Recipe\nThe Data Generating Process (DGP) is the actual mechanism that creates your data‚Äîincluding all factors, relationships, and random elements.\nAn intuitive example: Suppose student test scores are truly generated by:\n\\text{Score}_i = 50 + 2(\\text{StudyHours}_i) + 3(\\text{SleepHours}_i) - 5(\\text{Stress}_i) + 1.5(\\text{Breakfast}_i) + \\epsilon_i\nThis is the TRUE DGP. But you don‚Äôt know this! You might estimate:\n\\text{Score}_i = \\alpha + \\beta(\\text{StudyHours}_i) + u_i\nYour model is simpler than reality. You‚Äôre missing variables (sleep, stress, breakfast), so your estimates might be biased. The u_i term captures everything you missed.\nKey insight: We never know the true DGP. Our statistical models are always approximations, trying to capture the most important parts of the unknown, complex truth.\n\n\n\nTwo Approaches to Statistical Inference\nWhen analyzing data, especially from surveys or samples, we can take two philosophical approaches:\n\n1. Design-Based Inference\n\nPhilosophy: The population values are fixed numbers. Randomness comes ONLY from which units we happened to sample.\nFocus: How we selected the sample (simple random, stratified, cluster sampling, etc.)\nExample: The mean income of California counties is a fixed number. We sample 10 counties. Our uncertainty comes from which 10 we randomly selected.\nNo models needed: We don‚Äôt assume anything about the population values‚Äô distribution\n\n\n\n2. Model-Based Inference\n\nPhilosophy: The population values themselves are realizations from some probability model (superpopulation)\nFocus: The statistical model generating the population values\nExample: Each California county‚Äôs income is drawn from: Y_i = \\mu + \\epsilon_i where \\epsilon_i \\sim N(0, \\sigma^2)\nModels required: We make assumptions about how the data were generated\n\nWhich is better?\n\nLarge populations, good random samples: Design-based works well\nSmall populations (like 50 states): Model-based often necessary\nComplete enumeration: Only model-based allows inference\nModern practice: Often combines both approaches\n\n\n\n\n\nPractical Example: Analyzing State Education Spending\nSuppose you collect education spending per pupil for all 50 US states.\nWithout superpopulation thinking:\n\nYou have all 50 values‚Äîthat‚Äôs it\nThe mean is the mean, no uncertainty\nYou can‚Äôt test hypotheses or make predictions\n\nWith superpopulation thinking:\n\nThis year‚Äôs 50 values are one realization from a superpopulation\nModel: \\text{Spending}_i = \\mu + \\beta(\\text{StateIncome}_i) + \\epsilon_i\nNow you can:\n\nTest if spending relates to state income (\\beta \\neq 0?)\nPredict next year‚Äôs values\nCalculate confidence intervals\n\n\nThe key insight: Even with complete data, the superpopulation framework enables statistical inference by treating observed values as one possible outcome from an underlying stochastic process.\n\n\n\nSummary\n\nSuperpopulation: Treats your finite population as one draw from an infinite possibility space‚Äîessential when your finite population is small or completely observed\nDGP: The true (unknown) process creating your data‚Äîyour models try to approximate it",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#variables-and-measurement-scales",
    "href": "chapter1.html#variables-and-measurement-scales",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.5 Variables and Measurement Scales",
    "text": "1.5 Variables and Measurement Scales\n\nA variable is any characteristic that can take different values across units of observation.\n\n\nMeasurement: Transforming Concepts into Numbers\n\nThe Political World is Full of Data\nPolitical science has evolved from a primarily theoretical discipline to one that increasingly relies on empirical evidence. Whether we‚Äôre studying:\n\nElection outcomes: Why do people vote the way they do?\nPublic opinion: What shapes attitudes toward immigration or climate policy?\nInternational relations: What factors predict conflict between nations?\nPolicy effectiveness: Did a new education policy actually improve outcomes?\n\nWe need systematic ways to analyze data and draw conclusions that go beyond anecdotes and personal impressions.\n\nConsider this question: ‚ÄúDoes democracy lead to economic growth?‚Äù\n\nYour intuition might suggest yes‚Äîdemocratic countries tend to be wealthier. But is this causation or correlation? Are there exceptions? How confident can we be in our conclusions?\nStatistics provides the tools to move from hunches to evidence-based answers, helping us distinguish between what seems true and what actually is true.\n\n\nThe Challenge of Measurement in Social Sciences\nIn social sciences, we often struggle with the fact that key concepts do not translate directly into numbers:\n\nHow do we measure ‚Äúdemocracy‚Äù?\nWhat number captures ‚Äúpolitical ideology‚Äù?\nHow do we quantify ‚Äúinstitutional strength‚Äù?\nHow do we measure ‚Äúpolitical participation‚Äù?\n\n\n\n\n\n\n\n\nüîç Correlation ‚â† Causation: Understanding Spurious Relationships\n\n\n\n\nThe Fundamental Distinction\nCorrelation measures how two variables move together:\n\nPositive: Both increase together (study hours ‚Üë, grades ‚Üë)\nNegative: One increases while other decreases (TV hours ‚Üë, grades ‚Üì)\nMeasured by correlation coefficient: r \\in [-1, 1]\n\nCausation means one variable directly influences another:\n\nX \\rightarrow Y: Changes in X directly cause changes in Y\nRequires: (1) correlation, (2) temporal precedence, (3) no alternative explanations\n\n\n\nThe Danger: Spurious Correlation\nA spurious correlation occurs when two variables appear related but are actually both influenced by a third variable (a confounder).\nClassic Example:\n\nObserved: Ice cream sales correlate with drowning deaths\nSpurious conclusion: Ice cream causes drowning (‚ùå)\nReality: Summer weather (confounder) causes both:\nSummer ‚Üí More ice cream sales\nSummer ‚Üí More swimming ‚Üí More drownings\n\nMathematical representation:\n\nObserved correlation: \\text{Cor}(X,Y) \\neq 0\nBut the true model: X = \\alpha Z + \\epsilon_1 and Y = \\beta Z + \\epsilon_2\nWhere Z is the confounding variable causing both\n\n\n\nConfounding: The Hidden Influence\nA confounding variable (confounder):\n\nAffects both the presumed cause and effect\nCreates an illusion of direct causation 3. Must be controlled for valid causal inference\n\nResearch Example:\n\nObserved: Coffee consumption correlates with heart disease\nPotential confounder: Smoking (coffee drinkers more likely to smoke)\nTrue relationships:\nSmoking ‚Üí Heart disease (causal)\nSmoking ‚Üí Coffee consumption (association)\nCoffee ‚Üí Heart disease (spurious without controlling for smoking)\n\n\n\nHow to Identify Causal Relationships\n\nRandomized Controlled Trials (RCTs): Random assignment breaks confounding\nNatural Experiments: External events create ‚Äúas-if‚Äù random variation\nStatistical Control: Include confounders in regression models\nInstrumental Variables: Find variables affecting X but not Y directly\n\n\n\nKey Takeaway\nFinding correlation is easy. Establishing causation is hard. Always ask: ‚ÄúWhat else could explain this relationship?‚Äù\nRemember: The most dangerous phrase in empirical research is ‚Äúour data shows that X causes Y‚Äù when all you‚Äôve measured is correlation.\n\n\n\n\n\n\n\n\n\n\nüìä Quick Test: Correlation or Causation?\n\n\n\n\n\nFor each scenario, identify whether the relationship is likely causal or spurious:\n\nCities with more churches have more crime\n\nAnswer: Spurious (confounder: population size)\n\nSmoking leads to lung cancer\n\nAnswer: Causal (established through multiple study designs)\n\nStudents with more books at home get better grades\n\nAnswer: Likely spurious (confounders: parental education, income)\n\nCountries with higher chocolate consumption have more Nobel laureates\n\nAnswer: Spurious (confounder: wealth/development level)\n\n\n\n\n\n\n\n\n\nTypes of Variables\nQuantitative Variables represent amounts or quantities and can be:\nContinuous Variables: Can take any value within a range, limited only by measurement precision.\n\nAge (22.5 years, 22.51 years, 22.514 years‚Ä¶)\nIncome ($45,234.67)\nHeight (175.3 cm)\nPopulation density (432.7 people per square kilometer)\n\nDiscrete Variables: Can only take specific values, usually counts.\n\nNumber of children in a family (0, 1, 2, 3‚Ä¶)\nNumber of marriages (0, 1, 2‚Ä¶)\nNumber of rooms in a dwelling (1, 2, 3‚Ä¶)\nNumber of migrants entering a country per year\n\nQualitative Variables represent categories or qualities and can be:\nNominal Variables: Categories with no inherent order.\n\nCountry of birth (USA, Mexico, Canada‚Ä¶)\nReligion (Christian, Muslim, Hindu, Buddhist‚Ä¶)\nBlood type (A, B, AB, O)\nCause of death (heart disease, cancer, accident‚Ä¶)\n\nOrdinal Variables: Categories with a meaningful order but unequal intervals.\n\nEducation level (no schooling, primary, secondary, tertiary)\nSatisfaction with healthcare (very dissatisfied, dissatisfied, neutral, satisfied, very satisfied)\nSocioeconomic status (low, middle, high)\nSelf-rated health (poor, fair, good, excellent)\n\n\n\nMeasurement Scales\nUnderstanding measurement scales is crucial because they determine which statistical methods are appropriate:\nNominal Scale: Categories only‚Äîwe can count frequencies but cannot order or perform arithmetic. Example: We can say 45% of residents were born locally, but we cannot calculate an ‚Äúaverage birthplace.‚Äù\nOrdinal Scale: Order matters but differences between values are not necessarily equal. Example: The difference between ‚Äúpoor‚Äù and ‚Äúfair‚Äù health may not equal the difference between ‚Äúgood‚Äù and ‚Äúexcellent‚Äù health.\nInterval Scale: Equal intervals between values but no true zero point. Example: Temperature in Celsius‚Äîthe difference between 20¬∞C and 30¬∞C equals the difference between 30¬∞C and 40¬∞C, but 0¬∞C doesn‚Äôt mean ‚Äúno temperature.‚Äù\nRatio Scale: Equal intervals with a true zero point, allowing all mathematical operations. Example: Income‚Äî$40,000 is twice as much as $20,000, and $0 means no income.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#parameters-statistics-and-estimation",
    "href": "chapter1.html#parameters-statistics-and-estimation",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.6 Parameters, Statistics, and Estimation",
    "text": "1.6 Parameters, Statistics, and Estimation\nThese concepts form the core of statistical inference‚Äîhow we learn about populations from samples. Understanding the relationships between these terms is essential for proper statistical reasoning.\n\nParameter\nA parameter is a numerical characteristic of a population. Parameters are typically unknown because we cannot measure the entire population. They are fixed values (not random) but unknown to us. We denote parameters with Greek letters.\nCommon Demographic Parameters:\n\n\\mu (mu): Population mean age. For example, the true average age of all Europeans.\n\\sigma^2 (sigma squared): Population variance in income across all households in Brazil.\np: Population proportion. For example, the true proportion of all adults in Japan who are married.\n\\beta (beta): Regression coefficient. The true relationship between education and fertility in a population.\n\\lambda (lambda): Rate parameter. The true rate of migration from rural to urban areas.\n\nExample: The true mean age at first birth for all women in France who gave birth in 2023 is a parameter. Let‚Äôs call it \\mu = 31.2 years. We don‚Äôt know this value without measuring every single birth.\n\n\nStatistic\nA statistic is a numerical characteristic calculated from sample data. Statistics are random variables‚Äîtheir values vary from sample to sample. We use Roman letters for statistics.\nCommon Sample Statistics:\n\n\\bar{x} (x-bar): Sample mean age from a survey of 1,000 people\ns^2: Sample variance in income from 500 surveyed households\n\\hat{p} (p-hat): Sample proportion married from a survey\nr: Sample correlation between education and income\nb: Sample regression coefficient\n\nExample: From a sample of 500 births in France, we calculate a sample mean age at first birth of \\bar{x} = 30.9 years. This is our statistic. A different sample might yield \\bar{x} = 31.4 years.\n\n\nThe Relationship Between Parameters and Statistics\nThink of this relationship like trying to understand the depth of a lake:\n\nParameter: The true average depth of the lake (unknown, fixed)\nStatistic: The average depth from several measurement points (known, varies with different samples)\nEstimation: Using our measurements to guess the true average depth\n\n\n\nEstimator\nAn estimator is a rule or formula for calculating an estimate of a population parameter from sample data. An estimator is a function that maps sample data to parameter estimates. It‚Äôs the recipe, not the cake.\nProperties of Good Estimators:\nUnbiasedness: On average, the estimator equals the true parameter value. If we repeated sampling many times, the average of all our estimates would equal the true parameter.\nExample: The sample mean \\bar{x} is an unbiased estimator of population mean \\mu. If we took 1,000 different samples and calculated 1,000 sample means, their average would be very close to \\mu.\nConsistency: As sample size increases, the estimator converges to the true parameter value.\nExample: With n=10, our estimate of average income might be off by $5,000. With n=1,000, we might be off by only $500. With n=100,000, we might be off by only $50.\nEfficiency: Among unbiased estimators, the one with the smallest variance. The sample mean is more efficient than the sample median for estimating the population mean of a normal distribution.\nCommon Estimators:\n\nSample mean as estimator of population mean: \\bar{x} = \\frac{\\sum x_i}{n}\nSample proportion as estimator of population proportion: \\hat{p} = \\frac{x}{n} (where x is the count of successes)\nSample variance as estimator of population variance: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\n\nNote: We divide by (n-1) not n for sample variance to make it unbiased‚Äîthis is called Bessel‚Äôs correction.\n\n\nEstimand\nThe estimand is the specific population parameter we aim to estimate. It‚Äôs the target of our estimation procedure. Clear specification of the estimand is crucial for proper statistical inference and avoiding misinterpretation.\nExamples of Clearly Defined Estimands:\n\n‚ÄúThe median household income for all households in California as of January 1, 2024‚Äù\n‚ÄúThe difference in life expectancy between males and females born in Sweden in 2023‚Äù\n‚ÄúThe proportion of all adults aged 25-34 in urban areas who completed tertiary education‚Äù\n\nWhy Precise Estimand Definition Matters:\nConsider studying ‚Äúunemployment rate.‚Äù The estimand must specify:\n\nWho counts as unemployed? (Actively seeking work? Discouraged workers?)\nWhat age range? (15+? 16-64?)\nWhat geographic area?\nWhat time period?\n\nDifferent definitions lead to different numbers. The U.S. Bureau of Labor Statistics publishes six different unemployment rates (U-1 through U-6) based on different definitions.\n\n\nEstimate\nAn estimate is the specific numerical value calculated by applying an estimator to observed data. It‚Äôs our best guess at the true parameter value based on available information.\nExample of the Complete Process:\n\nEstimand (target): The proportion of all U.S. adults who approve of the president‚Äôs performance\nParameter (true unknown value): p = 0.42 (42%, but we don‚Äôt know this)\nEstimator (method): Sample proportion \\hat{p} = \\frac{x}{n} where x is approvals and n is sample size\nSample: Survey 1,500 randomly selected adults, 650 approve\nEstimate (calculated value): \\hat{p} = \\frac{650}{1,500} = 0.433 (43.3%)\n\n\n\n\n\n\n\nEstimands: What Exactly Are We Trying to Estimate?\n\n\n\nAn estimand is the specific quantity we aim to estimate‚Äîwhat we‚Äôre targeting with our statistical analysis. While this is often a population parameter, estimands can be more complex.\nExamples of different estimands:\nSimple parameter estimand: The population mean income (\\mu)\nComparative estimand: The difference in mean income between two groups (\\mu_1 - \\mu_2)\nCausal estimand: The average treatment effect of a job training program on earnings\nConditional estimand: Expected voter turnout given specific weather conditions\n\nThe Complete Framework\nUnderstanding statistical inference requires distinguishing between these related but distinct concepts:\n\nPopulation Parameter: The true characteristic of the population (e.g., \\mu)\nEstimand: The specific quantity we want to estimate (often, but not always, a parameter)\nEstimator: The method for computing our estimate (e.g., sample mean)\n\nEstimate: The actual number we calculate from our data\n\nExample in context:\n\nParameter: True mean voter turnout in all elections (\\mu)\nEstimand: Expected turnout difference between rainy vs.¬†sunny election days (\\mu_{\\text{rainy}} - \\mu_{\\text{sunny}})\nEstimator: Difference between sample means from rainy and sunny elections\nEstimate: 3.2 percentage points lower turnout on rainy days\n\nThis framework helps clarify exactly what question we‚Äôre answering and ensures our methods align with our research goals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#statistical-error-and-uncertainty",
    "href": "chapter1.html#statistical-error-and-uncertainty",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.7 Statistical Error and Uncertainty",
    "text": "1.7 Statistical Error and Uncertainty\n\nIntroduction: Why Uncertainty Matters\nNo measurement or estimate is perfect. Understanding different types of error is crucial for interpreting results and improving study design.\n\n\n\n\n\n\nThe Central Challenge\n\n\n\nEvery time we use a sample to learn about a population, we introduce uncertainty. The key is to:\n\nQuantify this uncertainty honestly\nDistinguish between different sources of error\nCommunicate results transparently\n\n\n\n\n\n\nTypes of Error\n\nRandom Error (Sampling Error)\nRandom error arises from natural variability in sampling‚Äîthe unavoidable variation that occurs because we observe a sample rather than the entire population.\n\n\n\n\n\n\nKey Characteristics\n\n\n\n\nUnpredictable Direction: Sometimes too high, sometimes too low\nDecreases with Sample Size: \\propto 1/\\sqrt{n}\nQuantifiable: Calculable using probability theory\nAverages to Zero: Over many samples, errors cancel out\n\n\n\n\nExample: Internet Access Survey\nImagine surveying 100 random households about internet access:\n\n\n\n\n\n\n\n\n\nThe variation around the true value (red line) represents random error. With larger samples, estimates would cluster more tightly.\n\n\n\nSystematic Error (Bias)\nSystematic error represents consistent deviation in a particular direction. Unlike random error, it doesn‚Äôt average out with repeated sampling.\n\nSelection BiasMeasurement BiasResponse BiasNon-response BiasSurvivorship Bias\n\n\nSampling method systematically excludes certain groups.\nExample: Phone surveys during business hours underrepresent employed people.\n\n\nMeasurement instrument consistently over/under-measures.\nExample: Scales that always read 2 pounds heavy.\n\n\nRespondents systematically misreport.\nExample: People underreport alcohol consumption, overreport voting.\n\n\nNon-responders differ systematically from responders.\nExample: Very sick and very healthy people less likely to respond to health surveys.\n\n\nOnly observing ‚Äúsurvivors‚Äù of some process.\nExample: Studying longevity by interviewing 90-year-olds misses those who died younger.\n\n\n\n\n\nThe Bias-Variance Decomposition\nMathematically, total error (Mean Squared Error) decomposes into:\n\\mathrm{MSE}(\\hat\\theta) = \\underbrace{\\mathrm{Var}(\\hat\\theta)}_{\\text{random error}} + \\underbrace{\\big(\\mathrm{Bias}(\\hat\\theta)\\big)^2}_{\\text{systematic error}}\n\n\n\n\n\n\nCritical Insight\n\n\n\nA large biased sample gives a precisely wrong answer.\n\nIncrease n ‚Üí reduces random error\nImprove design ‚Üí reduces systematic error\n\n\n\n\n\n\nDifferent combinations of bias and variance in estimation\n\n\n\n\n\n\nQuantifying Uncertainty\n\nStandard Error\nThe standard error (SE) quantifies how much an estimate varies across different possible samples.\n\n\nFor a Proportion: SE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\nFor a Mean: SE(\\bar{x}) = \\frac{s}{\\sqrt{n}}\n\nFor a Difference: SE(\\bar{x}_1 - \\bar{x}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\n\n\n\nMargin of Error\nThe margin of error (MOE) represents the expected maximum difference between sample estimate and true parameter.\n\\text{MOE} = \\text{Critical Value} \\times \\text{Standard Error}\n\n\n\n\n\n\nUnderstanding the Critical Value\n\n\n\n\n\nFor 95% confidence, we use 1.96 (often simplified to 2). This ensures that ~95% of intervals constructed this way will contain the true parameter.\n\n90% confidence: z = 1.645\n95% confidence: z = 1.96\n99% confidence: z = 2.576\n\n\n\n\n\n\nConfidence Intervals\nA confidence interval provides a range of plausible values:\n\\text{CI} = \\text{Estimate} \\pm (\\text{Critical Value} \\times \\text{Standard Error})\n\n\n\n\nPractical Application: Opinion Polling\n\n\n\n\n\n\nCase Study: Political Polls\n\n\n\nWhen a poll reports ‚ÄúCandidate A: 52%, Candidate B: 48%‚Äù, this is incomplete without uncertainty quantification.\n\n\n\nThe Golden Rule of Polling\nWith ~1,000 randomly selected respondents:\n\nMargin of error: ¬±3 percentage points (95% confidence)\nInterpretation: A reported 52% means true support likely between 49% and 55%\n\n\n\nSample Size and Precision\n\n\n\nSample Size\nMargin of Error (95%)\nUse Case\n\n\n\n\nn = 100\n¬± 10%\nBroad direction only\n\n\nn = 400\n¬± 5%\nGeneral trends\n\n\nn = 1,000\n¬± 3%\nStandard polls\n\n\nn = 2,500\n¬± 2%\nHigh precision\n\n\nn = 10,000\n¬± 1%\nVery high precision\n\n\n\n\n\n\n\n\n\nLaw of Diminishing Returns\n\n\n\nTo halve the margin of error, you need four times the sample size because \\text{MOE} \\propto 1/\\sqrt{n}\n\n\n\n\nWhat Polls Should Report\nQuality polls must disclose:\n\nField dates\nSample definition and size\nTreatment of undecided voters\nMargin of sampling error\nUncertainty in vote margins\n\n\n\n\n\nVisualization: Sampling Variability\nThe following simulation demonstrates how confidence intervals behave across repeated sampling:\n\n\nShow simulation code\nlibrary(ggplot2)\nset.seed(42)\n\n# Parameters\nn_polls      &lt;- 20\nn_people     &lt;- 100\ntrue_support &lt;- 0.50\n\n# Simulate independent polls\nsupport &lt;- rbinom(n_polls, n_people, true_support) / n_people\n\n# Calculate standard errors and margins of error\nse   &lt;- sqrt(support * (1 - support) / n_people)\nmoe  &lt;- 2 * se  # Simplified multiplier for clarity\n\n# Create confidence intervals\nlower &lt;- pmax(0, support - moe)\nupper &lt;- pmin(1, support + moe)\n\n# Check coverage\ncovers &lt;- (lower &lt;= true_support) & (upper &gt;= true_support)\nn_cover &lt;- sum(covers)\n\nresults &lt;- data.frame(\n  poll = seq_len(n_polls),\n  support, se, moe, lower, upper, covers\n)\n\n# Create visualization\nggplot(results, aes(x = poll, y = support, color = covers)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                width = 0.3, alpha = 0.8, size = 1) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = true_support, \n             linetype = \"dashed\", \n             color = \"black\",\n             alpha = 0.7) +\n  scale_color_manual(\n    values = c(\"TRUE\" = \"forestgreen\", \"FALSE\" = \"darkorange\"),\n    labels = c(\"TRUE\" = \"Covers truth\", \"FALSE\" = \"Misses truth\"),\n    name   = NULL\n  ) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0, 1)) +\n  labs(\n    title    = \"Sampling Variability in 20 Independent Polls\",\n    subtitle = paste0(\n      \"Each poll: n = \", n_people, \" | True value = \",\n      scales::percent(true_support),\n      \" | Coverage: \", n_cover, \"/\", n_polls,\n      \" (\", round(100 * n_cover / n_polls), \"%)\"\n    ),\n    x = \"Poll Number\",\n    y = \"Estimated Support\",\n    caption = \"Error bars show approximate 95% confidence intervals\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Observation\n\n\n\nMost intervals capture the true value, but some ‚Äúmiss‚Äù purely due to sampling randomness. This is expected and quantifiable.\n\n\n\n\n\nCommon Misconceptions\n\n\n\n\n\n\nWhat Margin of Error Does NOT Tell Us\n\n\n\n‚ùå Myth: ‚ÄúThe true value is definitely within the margin of error‚Äù ‚úÖ Reality: There‚Äôs still a 5% chance it‚Äôs outside (with 95% confidence)\n‚ùå Myth: ‚ÄúMargin of error covers all types of error‚Äù ‚úÖ Reality: Only covers random sampling error, not systematic bias\n‚ùå Myth: ‚ÄúLarger samples eliminate all error‚Äù ‚úÖ Reality: Reduces random error only; bias remains unchanged\n\n\n\n\n\nKey Takeaways\n\n\n\n\n\n\nEssential Points\n\n\n\n\nTwo types of error: Random (reducible with larger samples) and Systematic (requires better design)\nStandard error measures typical sampling variability\nMargin of error ‚âà 2 √ó SE for 95% confidence\nSample size and precision follow a square root relationship\nConfidence intervals provide ranges, not guarantees\nAlways consider both sampling error AND potential biases\n\n\n\n\n\n\n\n\n\nRemember\n\n\n\nA precisely wrong answer (large biased sample) is worse than an imprecisely right answer (small unbiased sample).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#sampling-and-sampling-methods",
    "href": "chapter1.html#sampling-and-sampling-methods",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.8 Sampling and Sampling Methods",
    "text": "1.8 Sampling and Sampling Methods\nSampling is the process of selecting a subset of individuals from a population to estimate characteristics of the whole population. The way we sample profoundly affects what we can conclude from our data.\n\nThe Sampling Frame\nBefore discussing methods, we must understand the sampling frame‚Äîthe list or device from which we draw our sample. The frame should ideally include every population member exactly once.\nCommon Sampling Frames:\n\nElectoral rolls (for adult citizens)\nTelephone directories (increasingly problematic due to mobile phones and unlisted numbers)\nAddress lists from postal services\nBirth registrations (for newborns)\nSchool enrollment lists (for children)\nTax records (for income earners)\nSatellite imagery (for dwellings in remote areas)\n\nFrame Problems:\n\nUndercoverage: Frame missing population members (homeless individuals not on address lists)\nOvercoverage: Frame includes non-population members (deceased people still on voter rolls)\nDuplication: Same unit appears multiple times (people with multiple phone numbers)\nClustering: Multiple population members per frame unit (multiple families at one address)\n\n\n\nProbability Sampling Methods\nProbability sampling gives every population member a known, non-zero probability of selection. This allows us to make statistical inferences about the population.\n\nSimple Random Sampling (SRS)\nEvery possible sample of size n has equal probability of selection. It‚Äôs the gold standard for statistical theory but often impractical for large populations.\nHow It Works:\n\nNumber every unit in the population from 1 to N\nUse random numbers to select n units\nEach unit has probability n/N of selection\n\nExample: To sample 50 students from a school of 1,000:\n\nAssign each student a number from 1 to 1,000\nGenerate 50 random numbers between 1 and 1,000\nSelect students with those numbers\n\nAdvantages:\n\nStatistically optimal\nEasy to analyze\nNo need for additional information about population\n\nDisadvantages:\n\nRequires complete sampling frame\nCan be expensive (selected units might be far apart)\nMay not represent important subgroups well by chance\n\n\n\nSystematic Sampling\nSelect every kth element from an ordered sampling frame, where k = N/n (the sampling interval).\nHow It Works:\n\nCalculate sampling interval k = N/n\nRandomly select starting point between 1 and k\nSelect every kth unit thereafter\n\nExample: To sample 100 houses from 5,000 on a street listing:\n\nk = 5,000/100 = 50\nRandom start: 23\nSample houses: 23, 73, 123, 173, 223‚Ä¶\n\nAdvantages:\n\nSimple to implement in field\nSpreads sample throughout population\n\nDisadvantages:\n\nCan introduce bias if there‚Äôs periodicity in the frame\n\nHidden Periodicity Example: Sampling every 10th apartment in buildings where corner apartments (numbers ending in 0) are all larger. This would bias our estimate of average apartment size.\n\n\nStratified Sampling\nDivide population into homogeneous subgroups (strata) before sampling. Sample independently within each stratum.\nHow It Works:\n\nDivide population into non-overlapping strata\nSample independently from each stratum\nCombine results with appropriate weights\n\nExample: Studying income in a city with distinct neighborhoods:\n\nStratum 1: High-income neighborhood (10% of population) - sample 100\nStratum 2: Middle-income neighborhood (60% of population) - sample 600\nStratum 3: Low-income neighborhood (30% of population) - sample 300\n\nTypes of Allocation:\nProportional: Sample size in each stratum proportional to stratum size\n\nIf stratum has 20% of population, it gets 20% of sample\n\nOptimal (Neyman): Larger samples from more variable strata\n\nIf income varies more in high-income areas, sample more there\n\nEqual: Same sample size per stratum regardless of population size\n\nUseful when comparing strata is primary goal\n\nAdvantages:\n\nEnsures representation of all subgroups\nCan increase precision substantially\nAllows different sampling methods per stratum\nProvides estimates for each stratum\n\nDisadvantages:\n\nRequires information to create strata\nCan be complex to analyze\n\n\n\nCluster Sampling\nSelect groups (clusters) rather than individuals. Often used when population is naturally grouped or when creating a complete frame is difficult.\nSingle-Stage Cluster Sampling:\n\nDivide population into clusters\nRandomly select some clusters\nInclude all units from selected clusters\n\nTwo-Stage Cluster Sampling:\n\nRandomly select clusters (Primary Sampling Units)\nWithin selected clusters, randomly select individuals (Secondary Sampling Units)\n\nExample: Surveying rural households in a large country:\n\nStage 1: Randomly select 50 villages from 1,000 villages\nStage 2: Within each selected village, randomly select 20 households\nTotal sample: 50 √ó 20 = 1,000 households\n\nMulti-Stage Example: National health survey:\n\nStage 1: Select states\nStage 2: Select counties within selected states\nStage 3: Select census blocks within selected counties\nStage 4: Select households within selected blocks\nStage 5: Select one adult within selected households\n\nAdvantages:\n\nDoesn‚Äôt require complete population list\nReduces travel costs (units clustered geographically)\nCan use different methods at different stages\nNatural for hierarchical populations\n\nDisadvantages:\n\nLess statistically efficient than SRS\nComplex variance estimation\nLarger samples needed for same precision\n\nDesign Effect: Cluster sampling typically requires larger samples than SRS. The design effect (DEFF) quantifies this:\n\\text{DEFF} = \\frac{\\text{Variance(cluster sample)}}{\\text{Variance(SRS)}}\nIf DEFF = 2, you need twice the sample size to achieve the same precision as SRS.\n\n\n\nNon-Probability Sampling Methods\nNon-probability sampling doesn‚Äôt guarantee known selection probabilities. While limiting statistical inference, these methods may be necessary or useful in certain situations.\n\nConvenience Sampling\nSelection based purely on ease of access. No attempt at representation.\nExamples:\n\nSurveying students in your class about study habits\nInterviewing people at a shopping mall about consumer preferences\nOnline polls where anyone can participate\nMedical studies using volunteers who respond to advertisements\n\nWhen It Might Be Acceptable:\n\nPilot studies to test survey instruments\nExploratory research to identify issues\nWhen studying processes believed to be universal\n\nMajor Problems:\n\nNo basis for inference to population\nSevere selection bias likely\nResults may be completely misleading\n\nReal Example: Literary Digest‚Äôs 1936 U.S. presidential poll surveyed 2.4 million people (huge sample!) but used telephone directories and club memberships as frames during the Depression, dramatically overrepresenting wealthy voters and incorrectly predicting Landon would defeat Roosevelt.\n\n\nPurposive (Judgmental) Sampling\nDeliberate selection of specific cases based on researcher judgment about what‚Äôs ‚Äútypical‚Äù or ‚Äúinteresting.‚Äù\nExamples:\n\nSelecting ‚Äútypical‚Äù villages to represent rural areas\nChoosing specific age groups for a developmental study\nSelecting extreme cases to understand range of variation\nPicking information-rich cases for in-depth study\n\nTypes of Purposive Sampling:\nTypical Case: Choose average or normal examples\n\nStudying ‚Äútypical‚Äù American suburbs\n\nExtreme/Deviant Case: Choose unusual examples\n\nStudying villages with unusually low infant mortality to understand success factors\n\nMaximum Variation: Deliberately pick diverse cases\n\nSelecting diverse schools (urban/rural, rich/poor, large/small) for education research\n\nCritical Case: Choose cases that will be definitive\n\n‚ÄúIf it doesn‚Äôt work here, it won‚Äôt work anywhere‚Äù\n\nWhen It‚Äôs Useful:\n\nQualitative research focusing on depth over breadth\nWhen studying rare populations\nResource constraints limit sample size severely\nExploratory phases of research\n\nProblems:\n\nEntirely dependent on researcher judgment\nNo statistical inference possible\nDifferent researchers might select different ‚Äútypical‚Äù cases\n\n\n\nQuota Sampling\nSelection to match population proportions on key characteristics. Like stratified sampling but without random selection within groups.\nHow Quota Sampling Works:\n\nIdentify key characteristics (age, sex, race, education)\nDetermine population proportions for these characteristics\nSet quotas for each combination\nInterviewers fill quotas using convenience methods\n\nDetailed Example: Political poll with quotas:\nPopulation proportions:\n\nMale 18-34: 15%\nMale 35-54: 20%\nMale 55+: 15%\nFemale 18-34: 16%\nFemale 35-54: 19%\nFemale 55+: 15%\n\nFor a sample of 1,000:\n\nInterview 150 males aged 18-34\nInterview 200 males aged 35-54\nAnd so on‚Ä¶\n\nInterviewers might stand on street corners approaching people who appear to fit needed categories until quotas are filled.\nWhy It‚Äôs Popular in Market Research:\n\nFaster than probability sampling\nCheaper (no callbacks for specific individuals)\nEnsures demographic representation\nNo sampling frame needed\n\nWhy It‚Äôs Problematic for Statistical Inference:\nHidden Selection Bias: Interviewers approach people who look approachable, speak the language well, aren‚Äôt in a hurry‚Äîsystematically excluding certain types within each quota cell.\nExample of Bias: An interviewer filling a quota for ‚Äúwomen 18-34‚Äù might approach women at a shopping mall on Tuesday afternoon, systematically missing:\n\nWomen who work during weekdays\nWomen who can‚Äôt afford to shop at malls\nWomen with young children who avoid malls\nWomen who shop online\n\nEven though the final sample has the ‚Äúright‚Äù proportion of young women, they‚Äôre not representative of all young women.\nNo Measure of Sampling Error: Without selection probabilities, we can‚Äôt calculate standard errors or confidence intervals.\nHistorical Cautionary Tale: Quota sampling was standard in polling until the 1948 U.S. presidential election, when polls using quota sampling incorrectly predicted Dewey would defeat Truman. The failure led to adoption of probability sampling in polling.\n\n\nSnowball Sampling\nParticipants recruit additional subjects from their acquaintances. The sample grows like a rolling snowball.\nHow It Works:\n\nIdentify initial participants (seeds)\nAsk them to refer others with required characteristics\nAsk new participants for further referrals\nContinue until sample size reached or referrals exhausted\n\nExample: Studying undocumented immigrants:\n\nStart with 5 immigrants you can identify\nEach refers 3 others they know\nThose 15 each refer 2-3 others\nContinue until you have 100+ participants\n\nWhen It‚Äôs Valuable:\nHidden Populations: Groups without sampling frames\n\nDrug users\nHomeless individuals\nPeople with rare diseases\nMembers of underground movements\n\nSocially Connected Populations: When relationships matter\n\nStudying social network effects\nResearching community transmission of diseases\nUnderstanding information diffusion\n\nTrust-Dependent Research: When referrals increase participation\n\nSensitive topics where trust is essential\nClosed communities suspicious of outsiders\n\nMajor Limitations:\n\nSamples biased toward cooperative, well-connected individuals\nIsolated members of population missed entirely\nStatistical inference generally impossible\nCan reinforce social divisions (chains rarely cross social boundaries)\n\nAdvanced Version - Respondent-Driven Sampling (RDS):\nAttempts to make snowball sampling more rigorous by:\n\nTracking who recruited whom\nLimiting number of referrals per person\nWeighting based on network size\nUsing mathematical models to adjust for bias\n\nStill controversial whether RDS truly allows valid inference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#probability-concepts-for-statistical-analysis",
    "href": "chapter1.html#probability-concepts-for-statistical-analysis",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.9 Probability Concepts for Statistical Analysis",
    "text": "1.9 Probability Concepts for Statistical Analysis\nWhile this is primarily a statistics course, understanding basic probability is essential for statistical inference.\n\nBasic Probability\nProbability quantifies uncertainty on a scale from 0 (impossible) to 1 (certain).\nClassical Probability: P(\\text{event}) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total possible outcomes}}\nExample: Probability a randomly selected person is female \\approx 0.5\nEmpirical Probability: Based on observed frequencies\nExample: In a village, 423 of 1,000 residents are female, so P(\\text{female}) \\approx 0.423\n\n\nConditional Probability\nConditional Probability is the probability of event A given that event B has occurred: P(A|B)\nDemographic Example: Probability of dying within a year given current age:\n\nP(\\text{death within year} | \\text{age 30}) \\approx 0.001\nP(\\text{death within year} | \\text{age 80}) \\approx 0.05\n\nThese conditional probabilities form the basis of life tables.\n\n\nIndependence\nEvents A and B are independent if P(A|B) = P(A).\nTesting Independence in Demographic Data:\nAre education and fertility independent?\n\nP(\\text{3+ children}) = 0.3 overall\nP(\\text{3+ children} | \\text{college degree}) = 0.15\nDifferent probabilities indicate dependence\n\n\n\nLaw of Large Numbers\nAs sample size increases, sample statistics converge to population parameters.\nDemonstration: Estimating sex ratio at birth:\n\n10 births: 7 males (70% - very unstable)\n100 births: 53 males (53% - getting closer to ~51.2%)\n1,000 births: 515 males (51.5% - quite close)\n10,000 births: 5,118 males (51.18% - very close)\n\n\n\nVisualizing the Law of Large Numbers: Coin Flips\nLet‚Äôs see this in action with coin flips. A fair coin has a 50% chance of landing heads, but individual flips are unpredictable.\n\n# Simulate coin flips and show convergence\nset.seed(42)\nn_flips &lt;- 1000\nflips &lt;- rbinom(n_flips, 1, 0.5)  # 1 = heads, 0 = tails\n\n# Calculate cumulative proportion of heads\ncumulative_prop &lt;- cumsum(flips) / seq_along(flips)\n\n# Create data frame for plotting\nlln_data &lt;- data.frame(\n  flip_number = 1:n_flips,\n  cumulative_proportion = cumulative_prop\n)\n\n# Plot the convergence\nggplot(lln_data, aes(x = flip_number, y = cumulative_proportion)) +\n  geom_line(color = \"steelblue\", alpha = 0.7) +\n  geom_hline(yintercept = 0.5, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_hline(yintercept = c(0.45, 0.55), color = \"red\", linetype = \"dotted\", alpha = 0.7) +\n  labs(\n    title = \"Law of Large Numbers: Coin Flip Proportions Converge to 0.5\",\n    x = \"Number of coin flips\",\n    y = \"Cumulative proportion of heads\",\n    caption = \"Red dashed line = true probability (0.5)\\nDotted lines = ¬±5% range\"\n  ) +\n  scale_y_continuous(limits = c(0.3, 0.7), breaks = seq(0.3, 0.7, 0.1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhat this shows:\n\nEarly flips show wild variation (first 10 flips might be 70% or 30% heads)\nAs we add more flips, the proportion stabilizes around 50%\nThe ‚Äúnoise‚Äù of individual outcomes averages out over time\n\n\n\nThe Mathematical Statement\nLet A denote an event of interest (e.g., ‚Äúheads on a coin flip‚Äù, ‚Äúvote for party X‚Äù, ‚Äúsum of dice equals 7‚Äù). If P(A) = p and we observe n independent trials with the same distribution (i.i.d.), then the sample frequency of A:\n\\hat{p}_n = \\frac{\\text{number of occurrences of } A}{n}\nconverges to p as n increases.\n\n\nExamples in Different Contexts\nDice example: The event ‚Äúsum = 7‚Äù with two dice has probability 6/36 ‚âà 16.7\\%, while ‚Äúsum = 4‚Äù has 3/36 ‚âà 8.3\\%. Over many throws, a sum of 7 appears about twice as often as a sum of 4.\nElection polling: If population support for a party equals p, then under random sampling of size n, the observed frequency \\hat{p}_n will approach p as n grows (assuming random sampling and independence).\nQuality control: If 2% of products are defective, then in large batches, approximately 2% will be found defective (assuming independent production).\n\n\nWhy This Matters for Statistics\nBottom line: Randomness underpins statistical inference by turning uncertainty in individual outcomes into predictable distributions for estimates. The Law of Large Numbers guarantees that the ‚Äúnoise‚Äù of individual outcomes averages out, allowing us to:\n\nPredict long-run frequencies\nQuantify uncertainty (margins of error)\n\nDraw reliable inferences from samples\nMake probabilistic statements about populations\n\nThis principle works in surveys, experiments, and even quantum phenomena (in the frequentist interpretation).\n\n\n\n\n\n\nWhat is randomness? (*)\n\n\n\nIn statistics, randomness is an orderly way to describe uncertainty: individual outcomes are unpredictable, yet in long sequences of repetitions stable regularities emerge (e.g., frequencies, means).\nTwo perspectives\n\nSingle realisation ‚Äî we cannot determine how a specific voter will vote at a given moment.\n\nAggregate ‚Äî we can describe the share of voters supporting a party and quantify the associated estimation uncertainty.\n\nEpistemic vs.¬†ontological randomness\n\nEpistemic (due to incomplete knowledge): we treat an outcome as random because not all determinants are observed or conditions are not controlled.\nExamples:\n\nthe decision of an individual respondent in a poll (we do not know the full set of motivations),\nmeasurement error in a survey (limited precision, item nonresponse),\na coin toss modeled as random because minute, unobserved differences in initial conditions determine the outcome.\n\nOntological (intrinsic indeterminacy): even complete knowledge does not remove outcome uncertainty.\nExamples:\n\nthe time to radioactive decay of an atom.\n\n\n\nWhy Randomness Matters\n\nRandom sampling\n\nReduces systematic selection bias so the sample resembles the target population (in expectation).\nMakes uncertainty quantifiable (e.g., margins of error; later we‚Äôll name these ‚Äúconfidence intervals‚Äù), assuming genuinely random selection and good coverage.\n\nRandom assignment (experiments)\n\nBreaks the link between treatment and other factors, making groups comparable on average (both observed and unobserved).\nSupports credible cause-and-effect claims (identifies average treatment effects under standard conditions).\n\n\n\n\nThe Power of Random Sampling\nSuppose we take a random sample of n=1000 voters and observe \\hat p = 0.55 (i.e., 55% support). Then:\n\nOur best single-number estimate of the population share is \\hat p = 0.55.\nA typical ‚Äú95\\% range of plausible values‚Äù around \\hat p can be approximated by \n\\hat p \\;\\pm\\; 2\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\n\\;=\\;\n0.55 \\;\\pm\\; 2\\sqrt{\\frac{0.55\\cdot 0.45}{1000}}\n\\approx\n0.55 \\pm 0.031,\n i.e., roughly 52\\%\\text{‚Äì}58\\% (about \\pm 3.1 percentage points).\nThe width of this range shrinks predictably with sample size: \n\\text{width} \\;\\propto\\; \\frac{1}{\\sqrt{n}}.\n For example, increasing n from 1000 to 4000 cuts the range by about half.\n\n\n\n1.10 Understanding Different Types of Unpredictability\nNot all uncertainty is the same. Understanding different sources of unpredictability helps us choose appropriate statistical methods and interpret results correctly.\n\n\n\n\n\n\n\n\n\nConcept\nWhat is it?\nSource of unpredictability\nExample\n\n\n\n\nRandomness\nIndividual outcomes are uncertain, but the probability distribution is known or modeled.\nFluctuations across realizations; lack of information about a specific outcome.\nDice roll, coin toss, polling sample\n\n\nChaos\nDeterministic dynamics highly sensitive to initial conditions (butterfly effect).\nTiny initial differences grow rapidly ‚Üí large trajectory divergences.\nWeather forecasting, double pendulum, population dynamics\n\n\nEntropy\nA measure of uncertainty/dispersion (information-theoretic or thermodynamic).\nLarger when outcomes are more evenly distributed (less predictive information).\nShannon entropy in data compression\n\n\n‚ÄúHaphazardness‚Äù (colloquial)\nA felt lack of order without an explicit model; a mixture of mechanisms.\nNo structured description or stable rules; overlapping processes.\nTraffic patterns, social media trends\n\n\nQuantum randomness\nA single outcome is not determined; only the distribution is specified (Born rule).\nFundamental (ontological) indeterminacy of individual measurements.\nElectron spin measurement, photon polarization\n\n\n\n\nKey Distinctions for Statistical Practice\nDeterministic chaos ‚â† statistical randomness: A chaotic system is fully deterministic yet practically unpredictable due to extreme sensitivity to initial conditions. Statistical randomness, by contrast, models uncertainty via probability distributions where individual outcomes are genuinely uncertain.\nWhy this matters: In statistics, we typically model phenomena as random processes, assuming we can specify probability distributions even when individual outcomes are unpredictable. This assumption underlies most statistical inference.\n\n\nQuantum Mechanics and Fundamental Randomness\nIn the Copenhagen interpretation, randomness is fundamental (ontological): a single outcome cannot be predicted, but the probability distribution is given by the Born rule.\nThis represents true randomness at the most basic level of nature, not just our ignorance of determining factors.\n\n\n\n\n\n\nCentral Limit Theorem\nThe distribution of sample means approaches normal distribution as sample size increases, regardless of the population distribution.\nWhy This Matters: Even if income is highly skewed, the average income from samples of 100+ people follows approximately normal distribution, allowing us to use normal-based confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#understanding-different-types-of-unpredictability",
    "href": "chapter1.html#understanding-different-types-of-unpredictability",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.10 Understanding Different Types of Unpredictability",
    "text": "1.10 Understanding Different Types of Unpredictability\nNot all uncertainty is the same. Understanding different sources of unpredictability helps us choose appropriate statistical methods and interpret results correctly.\n\n\n\n\n\n\n\n\n\nConcept\nWhat is it?\nSource of unpredictability\nExample\n\n\n\n\nRandomness\nIndividual outcomes are uncertain, but the probability distribution is known or modeled.\nFluctuations across realizations; lack of information about a specific outcome.\nDice roll, coin toss, polling sample\n\n\nChaos\nDeterministic dynamics highly sensitive to initial conditions (butterfly effect).\nTiny initial differences grow rapidly ‚Üí large trajectory divergences.\nWeather forecasting, double pendulum, population dynamics\n\n\nEntropy\nA measure of uncertainty/dispersion (information-theoretic or thermodynamic).\nLarger when outcomes are more evenly distributed (less predictive information).\nShannon entropy in data compression\n\n\n‚ÄúHaphazardness‚Äù (colloquial)\nA felt lack of order without an explicit model; a mixture of mechanisms.\nNo structured description or stable rules; overlapping processes.\nTraffic patterns, social media trends\n\n\nQuantum randomness\nA single outcome is not determined; only the distribution is specified (Born rule).\nFundamental (ontological) indeterminacy of individual measurements.\nElectron spin measurement, photon polarization\n\n\n\n\nKey Distinctions for Statistical Practice\nDeterministic chaos ‚â† statistical randomness: A chaotic system is fully deterministic yet practically unpredictable due to extreme sensitivity to initial conditions. Statistical randomness, by contrast, models uncertainty via probability distributions where individual outcomes are genuinely uncertain.\nWhy this matters: In statistics, we typically model phenomena as random processes, assuming we can specify probability distributions even when individual outcomes are unpredictable. This assumption underlies most statistical inference.\n\n\nQuantum Mechanics and Fundamental Randomness\nIn the Copenhagen interpretation, randomness is fundamental (ontological): a single outcome cannot be predicted, but the probability distribution is given by the Born rule.\nThis represents true randomness at the most basic level of nature, not just our ignorance of determining factors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#statistical-significance-a-quick-start-guide",
    "href": "chapter1.html#statistical-significance-a-quick-start-guide",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.11 Statistical Significance: A Quick Start Guide",
    "text": "1.11 Statistical Significance: A Quick Start Guide\nImagine you flip a coin 10 times and get 8 heads. Is the coin biased, or did you just get lucky? This is the core question statistical significance (statistical inference) helps us answer.\nStatistical significance tells us whether patterns in our data likely reflect something real or could have happened by pure chance.\nStatistical significance is a measure (p-value) of how confident we can be that patterns observed in our sample are not due to chance alone. When a result is statistically significant (typically p-value &lt; 0.05), it means the probability of obtaining such data in the absence of a real effect is very low.\n\nThe Courtroom Analogy\nStatistical hypothesis testing works like a criminal trial:\n\nNull Hypothesis (H_0): The defendant is innocent (no effect exists)\nAlternative Hypothesis (H_1): The defendant is guilty (an effect exists)\nThe Evidence: Your data and test results\nThe Verdict: ‚ÄúGuilty‚Äù (reject H_0) or ‚ÄúNot Guilty‚Äù (fail to reject H_0)\n\nCrucial distinction: ‚ÄúNot guilty‚Äù ‚â† ‚ÄúInnocent‚Äù\n\nA ‚Äúnot guilty‚Äù verdict means insufficient evidence to convict\nSimilarly, ‚Äúnot statistically significant‚Äù means insufficient evidence for an effect, NOT proof of no effect\n\n\n\nStart with Skepticism (Presumption of Innocence)\nIn statistics, we always start by assuming nothing special is happening:\n\nNull Hypothesis (H_0): ‚ÄúThere‚Äôs no effect‚Äù\n\nThe coin is fair\nThe new drug doesn‚Äôt work\nStudy time doesn‚Äôt affect grades\n\nAlternative Hypothesis (H_1): ‚ÄúThere IS an effect‚Äù\n\nThe coin is biased\nThe drug works\nMore study time improves grades\n\n\nKey principle: We maintain the null hypothesis (innocence) unless our data provides strong evidence against it‚Äî‚Äúbeyond a reasonable doubt‚Äù in legal terms, or ‚Äúp &lt; 0.05‚Äù in statistical terms.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-p-value-your-surprise-meter",
    "href": "chapter1.html#the-p-value-your-surprise-meter",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.12 The p-value: Your ‚ÄúSurprise Meter‚Äù",
    "text": "1.12 The p-value: Your ‚ÄúSurprise Meter‚Äù\nThe p-value answers one specific question:\n\n‚ÄúIf nothing special were happening (null hypothesis is true), how surprising would our results be?‚Äù\nA p-value is¬†the probability of observing the study‚Äôs results, or more extreme results, if the null hypothesis (a statement of no effect or no difference) is true.\n\n\nThree Ways to Think About p-values\n\n1. The Surprise Scale\n\np &lt; 0.01: Very surprising! (Strong evidence against H_0)\np &lt; 0.05: Pretty surprising (Moderate evidence against H_0)\np &gt; 0.05: Not that surprising (Insufficient evidence against H_0)\n\n\n\n2. Concrete Example: The Suspicious Coin\nYou flip a coin 10 times and get 8 heads. What‚Äôs the p-value?\nThe calculation: If the coin were fair, the probability of getting 8 or more heads is: p = P(‚â•8 \\text{ heads in 10 flips}) \\approx 0.055 \\approx 5.5\\%\nP(X \\geq 8) = \\sum_{k=8}^{10} \\binom{10}{k} 0,5^{10} = \\frac{56}{1024} \\approx 0,0547\nInterpretation: There‚Äôs a 5.5% chance of getting results this extreme with a fair coin. That‚Äôs somewhat unusual but not shocking.\n\n\n3. The Formal Definition\nA p-value is the probability of getting results at least as extreme as what you observed, assuming the null hypothesis is true.\n\n\n\n\n\n\nWarning\n\n\n\nCommon Mistake: The p-value is NOT the probability that the null hypothesis is true! It assumes the null is true and tells you how unusual your data would be in that world.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-prosecutor-fallacy-a-warning",
    "href": "chapter1.html#the-prosecutor-fallacy-a-warning",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.13 The Prosecutor Fallacy: A Warning",
    "text": "1.13 The Prosecutor Fallacy: A Warning\nI can see why the example might be challenging for beginners! Here‚Äôs a revised version that builds up the intuition more gradually without requiring knowledge of Bayes theorem or significance levels:",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-prosecutor-fallacy-a-warning-1",
    "href": "chapter1.html#the-prosecutor-fallacy-a-warning-1",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.14 The Prosecutor Fallacy: A Warning",
    "text": "1.14 The Prosecutor Fallacy: A Warning\n\nThe Fallacy Explained\nImagine this courtroom scenario:\nProsecutor: ‚ÄúIf the defendant were innocent, there‚Äôs only a 1% chance we‚Äôd find his DNA at the crime scene. We found his DNA. Therefore, there‚Äôs a 99% chance he‚Äôs guilty!‚Äù\nThis is WRONG! The prosecutor confused:\n\nP(Evidence | Innocent) = 0.01 ‚Üê What we know\nP(Innocent | Evidence) = ? ‚Üê What we want to know (but can‚Äôt get from the p-value alone!)\n\n\nWhen we get p = 0.01, it‚Äôs tempting to think:\n‚ùå WRONG: ‚ÄúThere‚Äôs only a 1% chance the null hypothesis is true‚Äù\n‚ùå WRONG: ‚ÄúThere‚Äôs a 99% chance our treatment works‚Äù\n‚úÖ CORRECT: ‚ÄúIf the null hypothesis were true, there‚Äôs only a 1% chance we‚Äôd see data this extreme‚Äù\n\n\nWhy This Matters: A Simple Medical Testing Example\nImagine a rare disease test that‚Äôs 99% accurate:\n\nIf you have the disease, the test is positive 99% of the time\nIf you don‚Äôt have the disease, the test is negative 99% of the time (so 1% false positive rate)\n\nHere‚Äôs the key: Suppose only 1 in 1000 people actually have this disease.\nNow let‚Äôs test 10,000 people:\n\n10 people have the disease ‚Üí 10 test positive (rounded)\n9,990 people don‚Äôt have the disease ‚Üí about 100 test positive by mistake (1% of 9,990)\nTotal positive tests: 110\n\nIf you test positive, what‚Äôs the chance you actually have the disease?\n\nOnly 10 out of 110 positive tests are real\nThat‚Äôs about 9%, not 99%!\n\n\n\nThe Research Analogy\nThe same thing happens in research:\n\nWhen we test many hypotheses (like testing many potential drugs)\nMost don‚Äôt work (like most people don‚Äôt have the rare disease)\nEven with ‚Äúsignificant‚Äù results (like a positive test), most findings might be false positives\n\n\n\n\n\n\n\nImportant\n\n\n\nA p-value tells you how surprising your data would be IF the null hypothesis were true. It doesn‚Äôt tell you the probability that the null hypothesis IS true.\nThink of it like this: The probability of the ground being wet IF it rained is very different from the probability it rained IF the ground is wet‚Äîthe ground could be wet from a sprinkler!\n\nRemember: A p-value tells you P(Data | Null is true), not P(Null is true | Data). These are as different as P(Wet ground | Rain) and P(Rain | Wet ground)‚Äîthe ground could be wet from a sprinkler!",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#introduction-to-regression-analysis-modeling-relationships-between-variables",
    "href": "chapter1.html#introduction-to-regression-analysis-modeling-relationships-between-variables",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.15 Introduction to Regression Analysis: Modeling Relationships Between Variables",
    "text": "1.15 Introduction to Regression Analysis: Modeling Relationships Between Variables\nOne of the most powerful tools in statistical analysis is regression analysis‚Äîa method for understanding and quantifying relationships between variables.\nThe core idea is simple: How does one thing relate to another, and can we use that relationship to make predictions?\n\nThe One-Sentence Summary: Regression helps us understand how things relate to each other in a messy, complicated world where everything affects everything else.\n\n\nWhat is Regression Analysis?\nImagine you‚Äôre curious about the relationship between education and income. You notice that people with more education tend to earn more money, but you want to understand this relationship more precisely:\n\nHow much does each additional year of education increase income, on average?\nHow strong is this relationship?\nAre there other factors we should consider?\nCan we predict someone‚Äôs likely income if we know their education level?\n\nRegression analysis provides systematic answers to these questions. It‚Äôs like finding the ‚Äúbest-fitting story‚Äù that describes how variables relate to each other.\n\n\nVariables and Variation\nA variable is any characteristic that can take different values across units of observation. In political science:\n\nUnits of analysis: Countries, individuals, elections, policies, years\nVariables: GDP, voting preference, democracy score, conflict occurrence\n\n\nüí° In Plain English: A variable is anything that changes. If everyone voted the same way, ‚Äúvoting preference‚Äù wouldn‚Äôt be a variable‚Äîit would be a constant. We study variables because we want to understand why things differ.\n\n\n\n\n\n\n\n\nNote\n\n\n\nConsider a typical pre-election news headline: ‚ÄúCandidate Smith‚Äôs approval rating reaches 68%.‚Äù Your immediate inference likely suggests favorable electoral prospects for Smith‚Äînot guaranteed victory, but a strong position. You naturally understand that higher approval ratings tend to predict better electoral performance, even though the relationship is not perfect.\nThis intuitive assessment exemplifies the core logic of regression analysis. You used one piece of information (approval rating) to make a prediction about another outcome (electoral success). Moreover, you recognized both the relationship between these variables and the uncertainty inherent in your prediction.\nWhile such informal reasoning serves us well in daily life, it has important limitations. How much better are Smith‚Äôs chances at 68% approval compared to 58%? What happens when we need to consider multiple factors simultaneously‚Äîapproval ratings, economic conditions, and incumbency status? How confident should we be in our predictions?\nRegression analysis provides a systematic framework for addressing these questions. It transforms our intuitive understanding of relationships into precise mathematical models that can be tested and refined. Through regression analysis, researchers can:\n\nGenerate precise predictions: Move beyond general assessments to specific numerical estimates‚Äîfor instance, predicting not just that Smith will ‚Äúprobably win,‚Äù but estimating the expected vote share and range of likely outcomes.\nIdentify which factors matter most: Determine the relative importance of different variables‚Äîperhaps discovering that economic conditions influence elections more strongly than approval ratings.\nQuantify uncertainty in predictions: Explicitly measure how confident we should be in our predictions, distinguishing between near-certain outcomes and educated guesses.\nTest theoretical propositions with empirical data: Evaluate whether our beliefs about cause-and-effect relationships hold up when examined systematically across many observations.\n\n\nIn essence, regression analysis systematizes the pattern recognition we perform intuitively, providing tools to make our predictions more accurate, our comparisons more meaningful, and our conclusions more reliable.\n\n\n\nThe Fundamental Model\nA model represents an object, person, or system in an informative way. Models divide into physical representations (such as architectural models) and abstract representations (such as mathematical equations describing atmospheric dynamics).\nThe core of statistical thinking can be expressed as:\nY = f(X) + \\text{error}\nThis equation states that our outcome (Y) equals some function of our predictors (X), plus unpredictable variation.\nComponents:\n\nY = Dependent variable (the phenomenon we seek to explain)\nX = Independent variable(s) (explanatory factors)\nf() = The functional relationship (often assumed linear)\nerror (\\epsilon) = Unexplained variation\n\n\nüí° What This Really Means: Think of it like a recipe. Your grade in a class (Y) depends on study hours (X), but not perfectly. Two students studying 10 hours might get different grades because of test anxiety, prior knowledge, or just luck (the error term). Regression finds the average relationship.\n\nThis model provides the foundation for all statistical analysis‚Äîfrom simple correlations to complex machine learning algorithms.\nRegression helps answer fundamental questions such as:\n\nHow much does education increase political participation?\nWhat factors predict electoral success?\nDo democratic institutions promote economic growth?\n\n\n\n\n\n\nThe Basic Idea: Drawing the Best Line Through Points\n\nSimple Linear Regression\nLet‚Äôs start with the simplest case: the relationship between two variables. Suppose we plot education (years of schooling) on the x-axis and annual income on the y-axis for 100 people. We‚Äôd see a cloud of points, and regression finds the straight line that best represents the pattern in these points.\nWhat makes a line ‚Äúbest‚Äù? The regression line minimizes the total squared vertical distances from all points to the line. Think of it as finding the line that makes the smallest total prediction error.\nThe equation of this line is: Y = a + bX + \\text{error}\nOr in our example: \\text{Income} = a + b \\times \\text{Education} + \\text{error}\nWhere:\n\na (intercept) = predicted income with zero education\nb (slope) = change in income per additional year of education\nerror (e) = difference between actual and predicted income\n\nInterpreting the Results:\nIf our analysis finds: \\text{Income} = 15,000 + 4,000 \\times \\text{Education}\nThis tells us:\n\nSomeone with 0 years of education is predicted to earn $15,000\nEach additional year of education is associated with $4,000 more income\nSomeone with 12 years of education is predicted to earn: $15,000 + (4,000 ) = $63,000\nSomeone with 16 years (bachelor‚Äôs degree) is predicted to earn: $15,000 + (4,000 ) = $79,000\n\n\n\n\nUnderstanding Relationships vs.¬†Proving Causation\nA crucial distinction: regression shows association, not necessarily causation. Our education-income regression shows they‚Äôre related, but doesn‚Äôt prove education causes higher income. Other explanations are possible:\n\nReverse causation: Maybe wealthier families can afford more education for their children\nCommon cause: Perhaps intelligence or motivation affects both education and income\nCoincidence: In small samples, patterns can appear by chance\n\nExample of Spurious Correlation: A regression might show that ice cream sales strongly predict drowning deaths. Does ice cream cause drowning? No! Both increase in summer (the common cause, confounding variable).\n\n\n\nMultiple Regression: Controlling for Other Factors\nReal life is complicated‚Äîmany factors influence outcomes simultaneously. Multiple regression lets us examine one relationship while ‚Äúcontrolling for‚Äù or ‚Äúholding constant‚Äù other variables.\n\nThe Power of Statistical Control\nReturning to education and income, we might wonder: Is the education effect just because educated people tend to be from wealthier families, or live in cities? Multiple regression can separate these effects:\n\\text{Income} = a + b_1 \\times \\text{Education} + b_2 \\times \\text{Age} + b_3 \\times \\text{Urban} + b_4 \\times \\text{Parent Income} + \\text{error}\nNow b_1 represents the education effect after accounting for age, location, and family background. If b_1 = 3,000, it means: ‚ÄúComparing people of the same age, location, and family background, each additional year of education is associated with $3,000 more income.‚Äù\nDemographic Example: Fertility and Women‚Äôs Education\nResearchers studying fertility might find: \\text{Children} = 4.5 - 0.3 \\times \\text{Education}\nThis suggests each year of women‚Äôs education is associated with 0.3 fewer children. But is education the cause, or are educated women different in other ways? Adding controls:\n\\text{Children} = a - 0.15 \\times \\text{Education} - 0.2 \\times \\text{Urban} + 0.1 \\times \\text{Husband Education} - 0.4 \\times \\text{Contraceptive Access}\nNow we see education‚Äôs association is weaker (-0.15 instead of -0.3) after accounting for urban residence and contraceptive access. This suggests part of education‚Äôs apparent effect operates through these other pathways.\n\n\n\nTypes of Variables in Regression\n\nOutcome (Dependent) Variable\nThis is what we‚Äôre trying to understand or predict:\n\nIncome in our first example\nNumber of children in our fertility example\nLife expectancy in health studies\nMigration probability in population studies\n\n\n\nPredictor (Independent) Variables\nThese are factors we think might influence the outcome:\n\nQuantitative: Age, years of education, income, distance\nQualitative (categorical): Gender, race, marital status, region\nBinary (Dummy): Urban/rural, employed/unemployed, married/unmarried\n\nHandling Categorical Variables: We can‚Äôt directly put ‚Äúreligion‚Äù into an equation. Instead, we create binary variables:\n\nChristian = 1 if Christian, 0 otherwise\nMuslim = 1 if Muslim, 0 otherwise\nHindu = 1 if Hindu, 0 otherwise\n(One category becomes the reference group)\n\n\n\n\nDifferent Types of Regression for Different Outcomes\nThe basic regression idea adapts to many situations:\n\nLinear Regression\nFor continuous outcomes (income, height, blood pressure): Y = a + b_1X_1 + b_2X_2 + ‚Ä¶ + \\text{error}\n\n\nLogistic Regression\nFor binary outcomes (died/survived, migrated/stayed, married/unmarried):\nInstead of predicting the outcome directly, we predict the probability: \\log\\left(\\frac{p}{1-p}\\right) = a + b_1X_1 + b_2X_2 + ‚Ä¶\nWhere p is the probability of the event occurring.\nExample: Predicting migration probability based on age, education, and marital status. The model might find young, educated, unmarried people have 40% probability of migrating, while older, less educated, married people have only 5% probability.\n\n\nPoisson Regression\nFor count outcomes (number of children, number of doctor visits): \\log(\\text{expected count}) = a + b_1X_1 + b_2X_2 + ‚Ä¶\nExample: Modeling number of children based on women‚Äôs characteristics. Useful because it ensures predictions are never negative (can‚Äôt have -0.5 children!).\n\n\nSurvival (Cox model)/Hazard Regression\nWhat it‚Äôs for: Predicting when something will happen, not just if it will happen.\nThe challenge: Imagine you‚Äôre studying how long marriages last. You follow 1,000 couples for 10 years, but by the end of your study:\n\n400 couples divorced (you know exactly when)\n600 couples are still married (you don‚Äôt know if/when they‚Äôll divorce)\n\nRegular regression can‚Äôt handle this ‚Äúincomplete story‚Äù problem‚Äîthose 600 ongoing marriages contain valuable information, but we don‚Äôt know their endpoints yet.\nHow Cox models help: Instead of trying to predict the exact timing, they focus on relative risk‚Äîwho‚Äôs more likely to experience the event sooner. Think of it like asking ‚ÄúAt any given moment, who‚Äôs at higher risk?‚Äù rather than ‚ÄúExactly when will this happen?‚Äù\nReal-world applications:\n\nMedical research: Who responds to treatment faster?\nBusiness: Which customers cancel subscriptions sooner?\nSocial science: What factors make life events happen earlier/later?\n\n\n\n\n\nInterpreting Regression Results\n\nCoefficients\nThe coefficient tells us the expected change in outcome for a one-unit increase in the predictor, holding other variables constant.\nExamples of Interpretation:\nLinear regression for income:\n\n‚ÄúEach additional year of education is associated with $3,500 higher annual income, controlling for age and experience‚Äù\n\nLogistic regression for infant mortality:\n\n‚ÄúEach additional prenatal visit is associated with 15% lower odds of infant death, controlling for mother‚Äôs age and education‚Äù\n\nMultiple regression for life expectancy:\n\n‚ÄúEach $1,000 increase in per-capita GDP is associated with 0.4 years longer life expectancy, after controlling for education and healthcare access‚Äù\n\n\n\nStatistical Significance\nThe regression also tests whether relationships could be due to chance:\n\np-value &lt; 0.05: Relationship unlikely due to chance (statistically significant)\np-value &gt; 0.05: Relationship could plausibly be random variation\n\n\nBut remember: Statistical significance ‚â† practical importance. With large samples, tiny effects become ‚Äúsignificant.‚Äù\n\n\n\nConfidence Intervals for Coefficients\nJust as we have confidence intervals for means or proportions, we have them for regression coefficients:\n‚ÄúThe effect of education on income is $3,500 per year, 95% CI: [$2,800, $4,200]‚Äù\nThis means we‚Äôre 95% confident the true effect is between $2,800 and $4,200.\n\n\nR-squared: How Well Does the Model Fit?\nR^2 (R-squared) measures the proportion of variation in the outcome explained by the predictors:\n\nR^2 = 0: Predictors explain nothing\nR^2 = 1: Predictors explain everything\nR^2 = 0.3: Predictors explain 30% of variation\n\nExample: A model of income with only education might have R^2 = 0.15 (education explains 15% of income variation). Adding age, experience, and location might increase R^2 to 0.35 (together they explain 35%).\n\n\n\n\n\n\nAssumptions and Limitations\n\n\n\nRegression makes assumptions that may not hold:\n\nExogeneity (No Hidden Relationships)\nThe most fundamental assumption: predictors must not be correlated with errors. In simple terms, there shouldn‚Äôt be hidden factors that affect both your predictors and outcome.\nExample: If studying education‚Äôs effect on income but omitting ‚Äúability,‚Äù your results are biased - ability affects both education level and income. This assumption is written as: E[\\varepsilon | X] = 0\nWhy it matters: Without it, all your coefficients are wrong, even with millions of observations!\n\n\nLinearity\nAssumes straight-line relationships. But what if education‚Äôs effect on income is stronger at higher levels? We can add polynomial terms: \\text{Income} = a + b_1 \\times \\text{Education} + b_2 \\times \\text{Education}^2\n\n\nIndependence\nAssumes observations are independent. But family members might be similar, repeated measures on the same person are related, and neighbors might influence each other. Special methods handle these dependencies.\n\n\nHomoscedasticity\nAssumes error variance is constant. But prediction errors might be larger for high-income people than low-income people. Diagnostic plots help detect this.\n\n\nNormality\nAssumes errors follow normal distribution. Important for small samples and hypothesis tests, less critical for large samples.\nNote: The first assumption (exogeneity) is about getting the right answer. The others are mostly about precision and statistical inference. Violating exogeneity means your model is fundamentally wrong; violating the others means your confidence intervals and p-values might be off.\n\n\n\n\n\n\n\n\n\nCommon Statistical Pitfalls\n\n\n\n\nEndogeneity (omitted variable bias): Forgetting about hidden factors that affect both X and Y, violating the fundamental exogeneity assumption. Example: Studying education‚Üíincome without accounting for ability.\nSimultaneity/Reverse causality: When X and Y determine each other at the same time. Simple regression assumes one-way causation, but reality is often bidirectional. Example: Price affects demand AND demand affects price simultaneously.\nConfounding: Failing to account for variables that affect both predictor and outcome, leading to spurious relationships. Example: Ice cream sales correlate with drownings (both caused by summer).\nSelection bias: Non-random samples that systematically exclude certain groups, making results ungeneralizable. Example: Surveying only smartphone users about internet usage.\nEcological fallacy: Assuming group-level patterns apply to individuals. Example: Rich countries have lower birth rates ‚â† rich people have fewer children.\nP-hacking (data dredging): Testing multiple hypotheses until finding significance, or tweaking analysis until p &lt; 0.05. With 20 tests, you expect 1 false positive by chance alone!\nOverfitting: Building a model too complex for your data - perfect on training data, useless for prediction. Remember: With enough parameters, you can fit an elephant.\nSurvivorship bias: Analyzing only ‚Äúsurvivors‚Äù while ignoring failures. Example: Studying successful companies while ignoring those that went bankrupt.\nOvergeneralization: Extending findings beyond the studied population, time period, or context. Example: Results from US college students ‚â† universal human behavior.\n\nRemember: The first three are forms of endogeneity - they violate E[\\varepsilon|X]=0 and make your coefficients fundamentally wrong. The others make results misleading or non-representative.\n\n\n\n\n\n\nApplications in Demography\n\nFertility Analysis\nUnderstanding what factors influence fertility decisions: \\text{Children} = f(\\text{Education, Income, Urban, Religion, Contraception, ‚Ä¶})\nHelps identify policy levers for countries concerned about high or low fertility.\nPolicy levers are the tools and methods that governments and organizations use to influence events and achieve specific goals by affecting behavior and outcomes.\n\n\nMortality Modeling\nPredicting life expectancy or mortality risk: \\text{Mortality Risk} = f(\\text{Age, Sex, Smoking, Education, Healthcare Access, ‚Ä¶})\nUsed by insurance companies, public health officials, and researchers.\n\n\nMigration Prediction\nUnderstanding who migrates and why: P(\\text{Migration}) = f(\\text{Age, Education, Employment, Family Ties, Distance, ‚Ä¶})\nHelps predict population flows and plan for demographic change.\n\n\nMarriage and Divorce\nAnalyzing union formation and dissolution: P(\\text{Divorce}) = f(\\text{Age at Marriage, Education Match, Income, Children, Duration, ‚Ä¶})\nInforms social policy and support services.\n\n\n\nCommon Pitfalls and How to Avoid Them\n\nOverfitting\nIncluding too many predictors can make the model fit perfectly in your sample but fail with new data. Like memorizing exam answers instead of understanding concepts.\nSolution: Use simpler models, cross-validation, or reserve some data for testing.\n\n\nMulticollinearity\nWhen predictors are highly correlated (e.g., years of education and degree level), the model can‚Äôt separate their effects.\nSolution: Choose one variable or combine them into an index.\n\n\nOmitted Variable Bias\nLeaving out important variables can make other effects appear stronger or weaker than they really are.\nExample: The relationship between ice cream sales and crime rates disappears when you control for temperature.\n\n\nExtrapolation\nUsing the model outside the range of observed data.\nExample: If your data includes education from 0-20 years, don‚Äôt predict income for someone with 30 years of education.\n\n\n\nMaking Regression Intuitive\nThink of regression as a sophisticated averaging technique:\n\nSimple average: ‚ÄúThe average income is $50,000‚Äù\nConditional average: ‚ÄúThe average income for college graduates is $70,000‚Äù\nRegression: ‚ÄúThe average income for 35-year-old college graduates in urban areas is $78,000‚Äù\n\nEach added variable makes our prediction more specific and (hopefully) more accurate.\n\n\nRegression in Practice: A Complete Example\nResearch Question: What factors influence age at first birth?\nData: Survey of 1,000 women who have had at least one child\nVariables:\n\nOutcome: Age at first birth (years)\nPredictors: Education (years), Urban (0/1), Income (thousands), Religious (0/1)\n\nSimple Regression Result: \\text{Age at First Birth} = 18 + 0.8 \\times \\text{Education}\nInterpretation: Each year of education associated with 0.8 years later first birth.\nMultiple Regression Result: \\text{Age at First Birth} = 16 + 0.5 \\times \\text{Education} + 2 \\times \\text{Urban} + 0.03 \\times \\text{Income} - 1.5 \\times \\text{Religious}\nInterpretation:\n\nEducation effect reduced but still positive (0.5 years per education year)\nUrban women have first births 2 years later\nEach $1,000 income associated with 0.03 years (11 days) later\nReligious women have first births 1.5 years earlier\nR^2 = 0.42 (model explains 42% of variation)\n\nThis richer model helps us understand that education‚Äôs effect partly operates through urban residence and income.\n\n\n\n\n\n\nWarning\n\n\n\nRegression is a gateway to advanced statistical modeling. Once you understand the basic concept‚Äîusing variables to predict outcomes and quantifying relationships‚Äîyou can explore:\n\nInteraction effects: When one variable‚Äôs effect depends on another\nNon-linear relationships: Curves, thresholds, and complex patterns\nMultilevel models: Accounting for grouped data (students in schools, people in neighborhoods)\nTime series regression: Analyzing change over time\nMachine learning extensions: Random forests, neural networks, and more\n\nThe key insight remains: We‚Äôre trying to understand how things relate to each other in a systematic, quantifiable way.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#data-quality-and-sources",
    "href": "chapter1.html#data-quality-and-sources",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.16 Data Quality and Sources",
    "text": "1.16 Data Quality and Sources\nNo analysis is better than the data it‚Äôs based on. Understanding data quality issues is crucial for demographic and social research.\n\nDimensions of Data Quality\nAccuracy: How close are measurements to true values?\nExample: Age reporting often shows ‚Äúheaping‚Äù at round numbers (30, 40, 50) because people round their ages.\nCompleteness: What proportion of the population is covered?\nExample: Birth registration completeness varies widely:\n\nDeveloped countries: &gt;99%\nSome developing countries: &lt;50%\n\nTimeliness: How current is the data?\nExample: Census conducted every 10 years becomes increasingly outdated, especially in rapidly changing areas.\nConsistency: Are definitions and methods stable over time and space?\nExample: Definition of ‚Äúurban‚Äù varies by country, making international comparisons difficult.\nAccessibility: Can researchers and policy makers actually use the data?\n\n\nCommon Data Sources in Demography\nCensus: Complete enumeration of population\nAdvantages:\n\nComplete coverage (in theory)\nSmall area data available\nBaseline for other estimates\n\nDisadvantages:\n\nExpensive and infrequent\nSome populations hard to count\nLimited variables collected\n\nSample Surveys: Detailed data from population subset\nExamples:\n\nDemographic and Health Surveys (DHS)\nAmerican Community Survey (ACS)\nLabour Force Surveys\n\nAdvantages:\n\nCan collect detailed information\nMore frequent than census\nCan focus on specific topics\n\nDisadvantages:\n\nSampling error present\nSmall areas not represented\nResponse burden may reduce quality\n\nAdministrative Records: Data collected for non-statistical purposes\nExamples:\n\nTax records\nSchool enrollment\nHealth insurance claims\nMobile phone data\n\nAdvantages:\n\nAlready collected (no additional burden)\nOften complete for covered population\nContinuously updated\n\nDisadvantages:\n\nCoverage may be selective\nDefinitions may not match research needs\nAccess often restricted\n\n\n\nData Quality Issues Specific to Demography\nAge Heaping: Tendency to report ages ending in 0 or 5\nDetection: Calculate Whipple‚Äôs Index or Myers‚Äô Index\nImpact: Affects age-specific rates and projections\nDigit Preference: Reporting certain final digits more than others\nExample: Birth weights often reported as 3,000g, 3,500g rather than precise values\nRecall Bias: Difficulty remembering past events accurately\nExample: ‚ÄúHow many times did you visit a doctor last year?‚Äù Often underreported for frequent visitors, overreported for rare visitors.\nProxy Reporting: Information provided by someone else\nChallenge: Household head reporting for all members may not know everyone‚Äôs exact age or education",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#ethical-considerations-in-statistical-demographics",
    "href": "chapter1.html#ethical-considerations-in-statistical-demographics",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.17 Ethical Considerations in Statistical Demographics",
    "text": "1.17 Ethical Considerations in Statistical Demographics\nStatistics isn‚Äôt just about numbers‚Äîit involves real people and has real consequences.\n\nInformed Consent\nParticipants should understand:\n\nPurpose of data collection\nHow data will be used\nRisks and benefits\nTheir right to refuse or withdraw\n\nChallenge in Demographics: Census participation is often mandatory, raising ethical questions about consent.\n\n\nConfidentiality and Privacy\nStatistical Disclosure Control: Protecting individual identity in published data\nMethods include:\n\nSuppressing small cells (e.g., ‚Äú&lt;5‚Äù instead of ‚Äú2‚Äù)\nGeographic aggregation\n\nExample: In a table of occupation by age by sex for a small town, there might be only one female doctor aged 60-65, making her identifiable.\n\n\nRepresentation and Fairness\nWho‚Äôs Counted?: Decisions about who to include affect representation\n\nPrisoners: Where are they counted‚Äîprison location or home address?\nHomeless: How to ensure coverage?\nUndocumented immigrants: Include or exclude?\n\nDifferential Privacy: Mathematical framework for privacy protection while maintaining statistical utility\nTrade-off: More privacy protection = less accurate statistics\n\n\nMisuse of Statistics\nCherry-Picking: Selecting only favorable results\nExample: Reporting decline in teen pregnancy from peak year rather than showing full trend\nP-Hacking: Manipulating analysis to achieve statistical significance\nEcological Fallacy: Inferring individual relationships from group data\nExample: Counties with more immigrants have higher average incomes ‚â† immigrants have higher incomes\n\n\nResponsible Reporting\nUncertainty Communication: Always report confidence intervals or margins of error\nContext Provision: Include relevant comparison groups and historical trends\nLimitation Acknowledgment: Clearly state what data can and cannot show",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#common-misconceptions-in-statistics",
    "href": "chapter1.html#common-misconceptions-in-statistics",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.18 Common Misconceptions in Statistics",
    "text": "1.18 Common Misconceptions in Statistics\nUnderstanding what statistics is NOT is as important as understanding what it is.\n\nMisconception 1: ‚ÄúStatistics Can Prove Anything‚Äù\nReality: Statistics can only provide evidence, never absolute proof. And proper statistics, honestly applied, constrains conclusions significantly.\nExample: A study finds correlation between ice cream sales and drowning deaths. Statistics doesn‚Äôt ‚Äúprove‚Äù ice cream causes drowning‚Äîboth are related to summer weather.\n\n\nMisconception 2: ‚ÄúLarger Samples Are Always Better‚Äù\nReality: Beyond a certain point, larger samples add little precision but may add bias.\nExample: Online survey with 1 million responses may be less accurate than probability sample of 1,000 due to self-selection bias.\nDiminishing Returns:\n\nn = 100: Margin of error \\approx 10%\nn = 1,000: Margin of error \\approx 3.2%\nn = 10,000: Margin of error \\approx 1%\nn = 100,000: Margin of error \\approx 0.32%\n\nThe jump from 10,000 to 100,000 barely improves precision but costs 10\\times more.\n\n\nMisconception 3: ‚ÄúStatistical Significance = Practical Importance‚Äù\nReality: With large samples, tiny differences become ‚Äústatistically significant‚Äù even if meaningless.\nExample: Study of 100,000 people finds men are 0.1 cm taller on average (p &lt; 0.001). Statistically significant but practically irrelevant.\n\n\nMisconception 4: ‚ÄúCorrelation Implies Causation‚Äù\nReality: Correlation is necessary but not sufficient for causation.\nClassic Examples:\n\nCities with more churches have more crime (both correlate with population size)\nCountries with more TV sets have longer life expectancy (both correlate with development)\n\n\n\nMisconception 5: ‚ÄúRandom Means Haphazard‚Äù\nReality: Statistical randomness is carefully controlled and systematic.\nExample: Random sampling requires careful procedure, not just grabbing whoever is convenient.\n\n\nMisconception 6: ‚ÄúAverage Represents Everyone‚Äù\nReality: Averages can be misleading when distributions are skewed or multimodal.\nExample: Average income of bar patrons is $50,000. Bill Gates walks in. Now average is $1 million. Nobody‚Äôs actual income changed.\n\n\nMisconception 7: ‚ÄúPast Patterns Guarantee Future Results‚Äù\nReality: Extrapolation assumes conditions remain constant.\nExample: Linear population growth projection from 1950-2000 would badly overestimate 2050 population because it misses fertility decline.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#applications-in-demography-1",
    "href": "chapter1.html#applications-in-demography-1",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.19 Applications in Demography",
    "text": "1.19 Applications in Demography\nThese statistical foundations enable sophisticated demographic analyses. Let‚Äôs explore key applications.\n\nPopulation Estimation and Projection\nIntercensal Estimates: Estimating population between censuses\nComponents Method: P(t+1) = P(t) + B - D + I - E\nWhere:\n\nP(t) = Population at time t\nB = Births\nD = Deaths\nI = Immigration\nE = Emigration\n\nEach component estimated from different sources with different error structures.\nPopulation Projections: Forecasting future population\nCohort Component Method:\n\nProject survival rates by age\nProject fertility rates\nProject migration rates\nApply to base population\nAggregate results\n\nUncertainty increases with projection horizon.\n\n\nDemographic Rate Calculation\nCrude Rates: Events per 1,000 population\n\\text{Crude Birth Rate} = \\frac{\\text{Births}}{\\text{Mid-year Population}} \\times 1,000\nAge-Specific Rates: Control for age structure\n\\text{Age-Specific Fertility Rate} = \\frac{\\text{Births to women aged } x}{\\text{Women aged } x} \\times 1,000\nStandardization: Compare populations with different structures\nDirect Standardization: Apply population‚Äôs rates to standard age structure Indirect Standardization: Apply standard rates to population‚Äôs age structure\n\n\nLife Table Analysis\nLife tables summarize mortality experience of a population.\nKey Columns:\n\nq_x: Probability of dying between age x and x+1\nl_x: Number surviving to age x (from 100,000 births)\nd_x: Deaths between age x and x+1\nL_x: Person-years lived between age x and x+1\ne_x: Life expectancy at age x\n\nExample Interpretation: If q_{65} = 0.015, then 1.5% of 65-year-olds die before reaching 66. If e_{65} = 18.5, then 65-year-olds average 18.5 more years of life.\n\n\nFertility Analysis\nTotal Fertility Rate (TFR): Average children per woman given current age-specific rates\n\\text{TFR} = \\sum (\\text{ASFR} \\times \\text{age interval width})\nExample: If each 5-year age group from 15-49 has ASFR = 20 per 1,000: \\text{TFR} = 7 \\text{ age groups} \\times \\frac{20}{1,000} \\times 5 \\text{ years} = 0.7 \\text{ children per woman}\nThis very low TFR indicates below-replacement fertility.\n\n\nMigration Analysis\nNet Migration Rate: \\text{NMR} = \\frac{\\text{Immigrants} - \\text{Emigrants}}{\\text{Population}} \\times 1,000\nMigration Effectiveness Index: \\text{MEI} = \\frac{|\\text{In} - \\text{Out}|}{\\text{In} + \\text{Out}}\n\nValues near 0: High turnover, little net change\nValues near 1: Mostly one-way flow\n\n\n\nPopulation Health Metrics\nDisability-Adjusted Life Years (DALYs): Years of healthy life lost\nDALY = Years of Life Lost (YLL) + Years Lived with Disability (YLD)\nHealthy Life Expectancy: Expected years in good health\nCombines mortality and morbidity information.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#software-and-tools",
    "href": "chapter1.html#software-and-tools",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.20 Software and Tools",
    "text": "1.20 Software and Tools\nModern demographic and social statistics relies heavily on computational tools.\n\nStatistical Software Packages\nR: Free, open-source, extensive demographic packages\n\nPackages: demography, popReconstruct, bayesPop\nAdvantages: Reproducible research, cutting-edge methods\nDisadvantages: Steep learning curve\n\nStata: Widely used in social sciences\n\nStrengths: Survey data analysis, panel data\nCommon in: Economics, epidemiology\n\nSPSS: User-friendly interface\n\nStrengths: Point-and-click interface\nCommon in: Social sciences, market research\n\nPython: General programming language with statistical libraries\n\nLibraries: pandas, numpy, scipy, statsmodels\nAdvantages: Integration with other applications",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#conclusion",
    "href": "chapter1.html#conclusion",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.21 Conclusion",
    "text": "1.21 Conclusion\n\nKey Terms Summary\nStatistics: The science of collecting, organizing, analyzing, interpreting, and presenting data to understand phenomena and support decision-making\nDescriptive Statistics: Methods for summarizing and presenting data in meaningful ways without extending conclusions beyond the observed data\nInferential Statistics: Techniques for drawing conclusions about populations from samples, including estimation and hypothesis testing\nPopulation: The complete set of individuals, objects, or measurements about which conclusions are to be drawn\nSample: A subset of the population that is actually observed or measured to make inferences about the population\nSuperpopulation: A theoretical infinite population from which observed finite populations are considered to be samples\nParameter: A numerical characteristic of a population (usually unknown and denoted by Greek letters)\nStatistic: A numerical characteristic calculated from sample data (known and denoted by Roman letters)\nEstimator: A rule or formula for calculating estimates of population parameters from sample data\nEstimand: The specific population parameter targeted for estimation\nEstimate: The numerical value produced by applying an estimator to observed data\nRandom Error (Sampling Error): Unpredictable variation arising from the sampling process that decreases with larger samples\nSystematic Error (Bias): Consistent deviation from true values that cannot be reduced by increasing sample size\nSampling: The process of selecting a subset of units from a population for measurement\nSampling Frame: The list or device from which a sample is drawn, ideally containing all population members\nProbability Sampling: Sampling methods where every population member has a known, non-zero probability of selection\nSimple Random Sampling: Every possible sample of size n has equal probability of selection\nSystematic Sampling: Selection of every kth element from an ordered sampling frame\nStratified Sampling: Division of population into homogeneous subgroups before sampling within each\nCluster Sampling: Selection of groups (clusters) rather than individuals\nNon-probability Sampling: Sampling methods without guaranteed known selection probabilities\nConvenience Sampling: Selection based purely on ease of access\nPurposive Sampling: Deliberate selection based on researcher judgment\nQuota Sampling: Selection to match population proportions on key characteristics without random selection\nSnowball Sampling: Participants recruit additional subjects from their acquaintances\nStandard Error: The standard deviation of the sampling distribution of a statistic\nMargin of Error: Maximum expected difference between estimate and parameter at specified confidence\nConfidence Interval: Range of plausible values for a parameter at specified confidence level\nConfidence Level: Probability that the confidence interval method produces intervals containing the parameter\nData: Collected observations or measurements\nQuantitative Data: Numerical measurements (continuous or discrete)\nQualitative Data: Categorical information (nominal or ordinal)\nData Distribution: Description of how values spread across possible outcomes\nFrequency Distribution: Summary showing how often each value occurs in data\nAbsolute Frequency: Count of observations for each value\nRelative Frequency: Proportion of observations in each category\nCumulative Frequency: Running total of frequencies up to each value",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-a-visualizations-for-statistics-demography",
    "href": "chapter1.html#appendix-a-visualizations-for-statistics-demography",
    "title": "1¬† Foundations of Statistics and Demography",
    "section": "1.22 Appendix A: Visualizations for Statistics & Demography",
    "text": "1.22 Appendix A: Visualizations for Statistics & Demography\n\n## ============================================\n## Visualizations for Statistics & Demography\n## Chapter 1: Foundations\n## ============================================\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(gridExtra)\nlibrary(scales)\nlibrary(patchwork)  # for combining plots\n\n# Set theme for all plots\ntheme_set(theme_minimal(base_size = 12))\n\n# Color palette for consistency\ncolors &lt;- c(\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#6A994E\")\n\n\n# ==================================================\n# 1. POPULATION vs SAMPLE VISUALIZATION\n# ==================================================\n\n# Create a population and sample visualization\nset.seed(123)\n\n# Generate population data (e.g., ages of 10,000 people)\npopulation &lt;- data.frame(\n  id = 1:10000,\n  age = round(rnorm(10000, mean = 40, sd = 15))\n)\npopulation$age[population$age &lt; 0] &lt;- 0\npopulation$age[population$age &gt; 100] &lt;- 100\n\n# Take a random sample\nsample_size &lt;- 500\nsample_data &lt;- population[sample(nrow(population), sample_size), ]\n\n# Create visualization\np1 &lt;- ggplot(population, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = colors[1], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = mean(population$age), \n             color = colors[2], linetype = \"dashed\", size = 1.2) +\n  labs(title = \"Population Distribution (N = 10,000)\",\n       subtitle = paste(\"Population mean (Œº) =\", round(mean(population$age), 2), \"years\"),\n       x = \"Age (years)\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(sample_data, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = colors[3], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = mean(sample_data$age), \n             color = colors[4], linetype = \"dashed\", size = 1.2) +\n  labs(title = paste(\"Sample Distribution (n =\", sample_size, \")\"),\n       subtitle = paste(\"Sample mean (xÃÑ) =\", round(mean(sample_data$age), 2), \"years\"),\n       x = \"Age (years)\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Combine plots\npopulation_sample_plot &lt;- p1 / p2\nprint(population_sample_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 2. TYPES OF DATA DISTRIBUTIONS\n# ==================================================\n\n# Generate different distribution types\nset.seed(456)\nn &lt;- 5000\n\n# Normal distribution\nnormal_data &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Right-skewed distribution (income-like)\nright_skewed &lt;- rgamma(n, shape = 2, scale = 15)\n\n# Left-skewed distribution (age at death in developed country)\nleft_skewed &lt;- 90 - rgamma(n, shape = 3, scale = 5)\nleft_skewed[left_skewed &lt; 0] &lt;- 0\n\n# Bimodal distribution (e.g., height of mixed male/female population)\nn2  &lt;- 20000\nnf &lt;- n2 %/% 2; nm &lt;- n2 - nf\nbimodal &lt;- c(rnorm(nf, mean = 164, sd = 5),\n             rnorm(nm, mean = 182, sd = 5))\n\n\n# Create data frame\ndistributions_df &lt;- data.frame(\n  Normal = normal_data,\n  `Right Skewed` = right_skewed,\n  `Left Skewed` = left_skewed,\n  Bimodal = bimodal\n) %&gt;%\n  pivot_longer(everything(), names_to = \"Distribution\", values_to = \"Value\")\n\n# Plot distributions\ndistributions_plot &lt;- ggplot(distributions_df, aes(x = Value, fill = Distribution)) +\n  geom_histogram(bins = 30, alpha = 0.7, color = \"white\") +\n  facet_wrap(~Distribution, scales = \"free\", nrow = 2) +\n  scale_fill_manual(values = colors[1:4]) +\n  labs(title = \"Types of Data Distributions\",\n       subtitle = \"Common patterns in demographic data\",\n       x = \"Value\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"none\")\n\nprint(distributions_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 3. NORMAL DISTRIBUTION WITH 68-95-99.7 RULE\n# ==================================================\n\n# Generate normal distribution data\nset.seed(789)\nmean_val &lt;- 100\nsd_val &lt;- 15\nx &lt;- seq(mean_val - 4*sd_val, mean_val + 4*sd_val, length.out = 1000)\ny &lt;- dnorm(x, mean = mean_val, sd = sd_val)\ndf_norm &lt;- data.frame(x = x, y = y)\n\n# Create the plot\nnormal_plot &lt;- ggplot(df_norm, aes(x = x, y = y)) +\n  # Fill areas under the curve\n  geom_area(data = subset(df_norm, x &gt;= mean_val - sd_val & x &lt;= mean_val + sd_val),\n            aes(x = x, y = y), fill = colors[1], alpha = 0.3) +\n  geom_area(data = subset(df_norm, x &gt;= mean_val - 2*sd_val & x &lt;= mean_val + 2*sd_val),\n            aes(x = x, y = y), fill = colors[2], alpha = 0.2) +\n  geom_area(data = subset(df_norm, x &gt;= mean_val - 3*sd_val & x &lt;= mean_val + 3*sd_val),\n            aes(x = x, y = y), fill = colors[3], alpha = 0.1) +\n  # Add the curve\n  geom_line(size = 1.5, color = \"black\") +\n  # Add vertical lines for standard deviations\n  geom_vline(xintercept = mean_val, linetype = \"solid\", size = 1, color = \"black\") +\n  geom_vline(xintercept = c(mean_val - sd_val, mean_val + sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[1]) +\n  geom_vline(xintercept = c(mean_val - 2*sd_val, mean_val + 2*sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[2]) +\n  geom_vline(xintercept = c(mean_val - 3*sd_val, mean_val + 3*sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[3]) +\n  # Add labels\n  annotate(\"text\", x = mean_val, y = max(y) * 0.5, label = \"68%\", \n           size = 5, fontface = \"bold\", color = colors[1]) +\n  annotate(\"text\", x = mean_val, y = max(y) * 0.3, label = \"95%\", \n           size = 5, fontface = \"bold\", color = colors[2]) +\n  annotate(\"text\", x = mean_val, y = max(y) * 0.1, label = \"99.7%\", \n           size = 5, fontface = \"bold\", color = colors[3]) +\n  # Labels\n  scale_x_continuous(breaks = c(mean_val - 3*sd_val, mean_val - 2*sd_val, \n                                mean_val - sd_val, mean_val, \n                                mean_val + sd_val, mean_val + 2*sd_val, \n                                mean_val + 3*sd_val),\n                     labels = c(\"Œº-3œÉ\", \"Œº-2œÉ\", \"Œº-œÉ\", \"Œº\", \"Œº+œÉ\", \"Œº+2œÉ\", \"Œº+3œÉ\")) +\n  labs(title = \"Normal Distribution: The 68-95-99.7 Rule\",\n       subtitle = \"Proportion of data within standard deviations from the mean\",\n       x = \"Value\", y = \"Probability Density\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\nprint(normal_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 4. SIMPLE LINEAR REGRESSION\n# ==================================================\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(scales)\n\n# Define color palette (this was missing in original code)\ncolors &lt;- c(\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#592E83\")\n\n# Generate data for regression example (Education vs Income)\nset.seed(2024)\nn_reg &lt;- 200\neducation &lt;- round(rnorm(n_reg, mean = 14, sd = 3))\neducation[education &lt; 8] &lt;- 8\neducation[education &gt; 22] &lt;- 22\n\n# Create income with linear relationship plus noise\nincome &lt;- 15000 + 4000 * education + rnorm(n_reg, mean = 0, sd = 8000)\nincome[income &lt; 10000] &lt;- 10000\n\nreg_data &lt;- data.frame(education = education, income = income)\n\n# Fit linear model\nlm_model &lt;- lm(income ~ education, data = reg_data)\n\n# Create subset of data for residual lines\nsubset_indices &lt;- sample(nrow(reg_data), 20)\nsubset_data &lt;- reg_data[subset_indices, ]\nsubset_data$predicted &lt;- predict(lm_model, newdata = subset_data)\n\n# Create regression plot\nregression_plot &lt;- ggplot(reg_data, aes(x = education, y = income)) +\n  # Add points\n  geom_point(alpha = 0.6, size = 2, color = colors[1]) +\n  \n  # Add regression line with confidence interval\n  geom_smooth(method = \"lm\", se = TRUE, color = colors[2], fill = colors[2], alpha = 0.2) +\n  \n  # Add residual lines for a subset of points to show the concept\n  geom_segment(data = subset_data,\n               aes(x = education, xend = education, \n                   y = income, yend = predicted),\n               color = colors[4], alpha = 0.5, linetype = \"dotted\") +\n  \n  # Add equation to plot (adjusted position based on data range)\n  annotate(\"text\", x = min(reg_data$education) + 1, y = max(reg_data$income) * 0.9, \n           label = paste(\"Income = $\", format(round(coef(lm_model)[1]), big.mark = \",\"), \n                        \" + $\", format(round(coef(lm_model)[2]), big.mark = \",\"), \" √ó Education\",\n                        \"\\nR¬≤ = \", round(summary(lm_model)$r.squared, 3), sep = \"\"),\n           hjust = 0, size = 4, fontface = \"italic\") +\n  \n  # Labels and formatting\n  scale_y_continuous(labels = dollar_format()) +\n  labs(title = \"Simple Linear Regression: Education and Income\",\n       subtitle = \"Each year of education associated with higher income\",\n       x = \"Years of Education\", \n       y = \"Annual Income\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\nprint(regression_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 5. SAMPLING ERROR AND SAMPLE SIZE\n# ==================================================\n\n# Show how standard error decreases with sample size\nset.seed(111)\nsample_sizes &lt;- c(10, 25, 50, 100, 250, 500, 1000, 2500, 5000)\nn_simulations &lt;- 1000\n\n# True population parameters\ntrue_mean &lt;- 50\ntrue_sd &lt;- 10\n\n# Run simulations for each sample size\nse_results &lt;- data.frame()\nfor (n in sample_sizes) {\n  sample_means &lt;- replicate(n_simulations, mean(rnorm(n, true_mean, true_sd)))\n  se_results &lt;- rbind(se_results, \n                      data.frame(n = n, \n                                se_empirical = sd(sample_means),\n                                se_theoretical = true_sd / sqrt(n)))\n}\n\n# Create the plot\nse_plot &lt;- ggplot(se_results, aes(x = n)) +\n  geom_line(aes(y = se_empirical, color = \"Empirical SE\"), size = 1.5) +\n  geom_point(aes(y = se_empirical, color = \"Empirical SE\"), size = 3) +\n  geom_line(aes(y = se_theoretical, color = \"Theoretical SE\"), \n            size = 1.5, linetype = \"dashed\") +\n  scale_x_log10(breaks = sample_sizes) +\n  scale_color_manual(values = c(\"Empirical SE\" = colors[1], \n                               \"Theoretical SE\" = colors[2])) +\n  labs(title = \"Standard Error Decreases with Sample Size\",\n       subtitle = \"The precision of estimates improves with larger samples\",\n       x = \"Sample Size (log scale)\", \n       y = \"Standard Error\",\n       color = \"\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"top\")\n\nprint(se_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 6. CONFIDENCE INTERVALS VISUALIZATION\n# ==================================================\n\n# Simulate multiple samples and their confidence intervals\nset.seed(999)\nn_samples &lt;- 20\nsample_size_ci &lt;- 100\ntrue_mean_ci &lt;- 50\ntrue_sd_ci &lt;- 10\n\n# Generate samples and calculate CIs\nci_data &lt;- data.frame()\nfor (i in 1:n_samples) {\n  sample_i &lt;- rnorm(sample_size_ci, true_mean_ci, true_sd_ci)\n  mean_i &lt;- mean(sample_i)\n  se_i &lt;- sd(sample_i) / sqrt(sample_size_ci)\n  ci_lower &lt;- mean_i - 1.96 * se_i\n  ci_upper &lt;- mean_i + 1.96 * se_i\n  contains_true &lt;- (true_mean_ci &gt;= ci_lower) & (true_mean_ci &lt;= ci_upper)\n  \n  ci_data &lt;- rbind(ci_data,\n                   data.frame(sample = i, mean = mean_i, \n                             lower = ci_lower, upper = ci_upper,\n                             contains = contains_true))\n}\n\n# Create CI plot\nci_plot &lt;- ggplot(ci_data, aes(x = sample, y = mean)) +\n  geom_hline(yintercept = true_mean_ci, color = \"red\", \n             linetype = \"dashed\", size = 1) +\n  geom_errorbar(aes(ymin = lower, ymax = upper, color = contains), \n                width = 0.3, size = 0.8) +\n  geom_point(aes(color = contains), size = 2) +\n  scale_color_manual(values = c(\"TRUE\" = colors[1], \"FALSE\" = colors[4]),\n                    labels = c(\"Misses true value\", \"Contains true value\")) +\n  coord_flip() +\n  labs(title = \"95% Confidence Intervals from 20 Different Samples\",\n       subtitle = paste(\"True population mean = \", true_mean_ci, \n                       \" (red dashed line)\", sep = \"\"),\n       x = \"Sample Number\", \n       y = \"Sample Mean with 95% CI\",\n       color = \"\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"bottom\")\n\nprint(ci_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 7. SAMPLING DISTRIBUTIONS (CENTRAL LIMIT THEOREM)\n# ==================================================\n\n# ---- Setup ----\nlibrary(tidyverse)\nlibrary(ggplot2)\ntheme_set(theme_minimal(base_size = 13))\nset.seed(2025)\n\n# Skewed population (Gamma); change if you want another DGP\nNpop &lt;- 100000\npopulation &lt;- rgamma(Npop, shape = 2, scale = 10)  # skewed right\nmu    &lt;- mean(population)\nsigma &lt;- sd(population)\n\n# ---- CLT: sampling distribution of the mean ----\nsample_sizes &lt;- c(1, 5, 10, 30, 100)\nB &lt;- 2000  # resamples per n\n\nclt_df &lt;- purrr::map_dfr(sample_sizes, \\(n) {\n  tibble(n = n,\n         mean = replicate(B, mean(sample(population, n, replace = TRUE))))\n})\n\n# Normal overlays: N(mu, sigma/sqrt(n))\nclt_range &lt;- clt_df |&gt;\n  group_by(n) |&gt;\n  summarise(min_x = min(mean), max_x = max(mean), .groups = \"drop\")\n\nnormal_df &lt;- clt_range |&gt;\n  rowwise() |&gt;\n  mutate(x = list(seq(min_x, max_x, length.out = 200))) |&gt;\n  unnest(x) |&gt;\n  mutate(density = dnorm(x, mean = mu, sd = sigma / sqrt(n)))\n\nclt_plot &lt;- ggplot(clt_df, aes(mean)) +\n  geom_histogram(aes(y = after_stat(density), fill = factor(n)),\n                 bins = 30, alpha = 0.6, color = \"white\") +\n  geom_line(data = normal_df, aes(x, density), linewidth = 0.8) +\n  geom_vline(xintercept = mu, linetype = \"dashed\") +\n  facet_wrap(~ n, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"CLT: Sampling distribution of the mean ‚Üí Normal(Œº, œÉ/‚àön)\",\n    subtitle = sprintf(\"Skewed population: Gamma(shape=2, scale=10).  Œº‚âà%.2f, œÉ‚âà%.2f; B=%d resamples each.\", mu, sigma, B),\n    x = \"Sample mean\", y = \"Density\"\n  ) +\n  guides(fill = \"none\")\n\nclt_plot\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 8. TYPES OF SAMPLING ERROR\n# ==================================================\n\n# Create data to show random vs systematic error\nset.seed(321)\nn_measurements &lt;- 100\ntrue_value &lt;- 50\n\n# Random error only\nrandom_error &lt;- rnorm(n_measurements, mean = true_value, sd = 5)\n\n# Systematic error (bias) only\nsystematic_error &lt;- rep(true_value + 10, n_measurements) + rnorm(n_measurements, 0, 0.5)\n\n# Both errors\nboth_errors &lt;- rnorm(n_measurements, mean = true_value + 10, sd = 5)\n\nerror_data &lt;- data.frame(\n  measurement = 1:n_measurements,\n  `Random Error Only` = random_error,\n  `Systematic Error Only` = systematic_error,\n  `Both Errors` = both_errors\n) %&gt;%\n  pivot_longer(-measurement, names_to = \"Error_Type\", values_to = \"Value\")\n\n# Create error visualization\nerror_plot &lt;- ggplot(error_data, aes(x = measurement, y = Value, color = Error_Type)) +\n  geom_hline(yintercept = true_value, linetype = \"dashed\", size = 1, color = \"black\") +\n  geom_point(alpha = 0.6, size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 1.2) +\n  facet_wrap(~Error_Type, nrow = 1) +\n  scale_color_manual(values = colors[1:3]) +\n  labs(title = \"Random Error vs Systematic Error (Bias)\",\n       subtitle = paste(\"True value = \", true_value, \" (black dashed line)\", sep = \"\"),\n       x = \"Measurement Number\", \n       y = \"Measured Value\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"none\")\n\nprint(error_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 9. DEMOGRAPHIC PYRAMID\n# ==================================================\n\n# Create age pyramid data\nset.seed(777)\nage_groups &lt;- c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \n               \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \n               \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80+\")\n\n# Create data for a developing country pattern\nmale_pop &lt;- c(12, 11.5, 11, 10.5, 10, 9.5, 9, 8.5, 8, 7.5, 7, \n             6, 5, 4, 3, 2, 1.5)\nfemale_pop &lt;- c(11.8, 11.3, 10.8, 10.3, 9.8, 9.3, 8.8, 8.3, 7.8, \n               7.3, 6.8, 5.8, 4.8, 3.8, 2.8, 2.2, 2)\n\npyramid_data &lt;- data.frame(\n  Age = factor(rep(age_groups, 2), levels = rev(age_groups)),\n  Population = c(-male_pop, female_pop),  # Negative for males\n  Sex = c(rep(\"Male\", length(male_pop)), rep(\"Female\", length(female_pop)))\n)\n\n# Create population pyramid\npyramid_plot &lt;- ggplot(pyramid_data, aes(x = Age, y = Population, fill = Sex)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  scale_y_continuous(labels = function(x) paste0(abs(x), \"%\")) +\n  scale_fill_manual(values = c(\"Male\" = colors[1], \"Female\" = colors[3])) +\n  coord_flip() +\n  labs(title = \"Population Pyramid\",\n       subtitle = \"Age and sex distribution (typical developing country pattern)\",\n       x = \"Age Group\", \n       y = \"Percentage of Population\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"top\")\n\nprint(pyramid_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 10. REGRESSION RESIDUALS AND DIAGNOSTICS\n# ==================================================\n\n# Use the previous regression model for diagnostics\nreg_diagnostics &lt;- data.frame(\n  fitted = fitted(lm_model),\n  residuals = residuals(lm_model),\n  standardized_residuals = rstandard(lm_model),\n  education = reg_data$education,\n  income = reg_data$income\n)\n\n# Create diagnostic plots\n# 1. Residuals vs Fitted\np_resid_fitted &lt;- ggplot(reg_diagnostics, aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.5, color = colors[1]) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = colors[2], size = 0.8) +\n  labs(title = \"Residuals vs Fitted Values\",\n       subtitle = \"Check for homoscedasticity\",\n       x = \"Fitted Values\", y = \"Residuals\")\n\n# 2. Q-Q plot\np_qq &lt;- ggplot(reg_diagnostics, aes(sample = standardized_residuals)) +\n  stat_qq(color = colors[1]) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Normal Q-Q Plot\",\n       subtitle = \"Check for normality of residuals\",\n       x = \"Theoretical Quantiles\", y = \"Standardized Residuals\")\n\n# 3. Histogram of residuals\np_hist_resid &lt;- ggplot(reg_diagnostics, aes(x = residuals)) +\n  geom_histogram(bins = 30, fill = colors[3], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Distribution of Residuals\",\n       subtitle = \"Should be approximately normal\",\n       x = \"Residuals\", y = \"Frequency\")\n\n# 4. Residuals vs Predictor\np_resid_x &lt;- ggplot(reg_diagnostics, aes(x = education, y = residuals)) +\n  geom_point(alpha = 0.5, color = colors[4]) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = colors[2], size = 0.8) +\n  labs(title = \"Residuals vs Predictor\",\n       subtitle = \"Check for patterns\",\n       x = \"Education (years)\", y = \"Residuals\")\n\n# Combine diagnostic plots\ndiagnostic_plots &lt;- (p_resid_fitted + p_qq) / (p_hist_resid + p_resid_x)\nprint(diagnostic_plots)\n\n\n\n\n\n\n\n# ==================================================\n# 11. SAVE ALL PLOTS (Optional)\n# ==================================================\n\n# Uncomment to save plots as high-resolution images\n# ggsave(\"population_sample.png\", population_sample_plot, width = 10, height = 8, dpi = 300)\n# ggsave(\"distributions.png\", distributions_plot, width = 12, height = 8, dpi = 300)\n# ggsave(\"normal_distribution.png\", normal_plot, width = 10, height = 6, dpi = 300)\n# ggsave(\"regression.png\", regression_plot, width = 10, height = 7, dpi = 300)\n# ggsave(\"standard_error.png\", se_plot, width = 10, height = 6, dpi = 300)\n# ggsave(\"confidence_intervals.png\", ci_plot, width = 10, height = 8, dpi = 300)\n# ggsave(\"central_limit_theorem.png\", clt_plot, width = 14, height = 5, dpi = 300)\n# ggsave(\"error_types.png\", error_plot, width = 14, height = 5, dpi = 300)\n# ggsave(\"population_pyramid.png\", pyramid_plot, width = 8, height = 8, dpi = 300)\n# ggsave(\"regression_diagnostics.png\", diagnostic_plots, width = 12, height = 10, dpi = 300)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Statistics and Demography</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html",
    "href": "rozdzial1.html",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "",
    "text": "2.1 Wprowadzenie\nStatystyka jest sposobem poznawania ≈õwiata na podstawie danych. Uczy nas, jak mƒÖdrze zbieraƒá dane, dostrzegaƒá wzorce, szacowaƒá parametry (cechy) populacyjne i dokonywaƒá prognoz ‚Äî okre≈õlajƒÖc, jak bardzo mo≈ºemy siƒô myliƒá.\nWyobra≈∫ sobie, ≈ºe chcesz poznaƒá ≈õredni wiek przy pierwszym ma≈Ç≈ºe≈Ñstwie w danym kraju. Nie mo≈ºesz zapytaƒá ka≈ºdej pojedynczej osoby, kiedy po raz pierwszy wstƒÖpi≈Ça w zwiƒÖzek ma≈Ç≈ºe≈Ñski (je≈õli w og√≥le).\nStatystyka dostarcza narzƒôdzi do:\nStatystyka i demografia to powiƒÖzane ze sobƒÖ dyscypliny, kt√≥re dostarczajƒÖ narzƒôdzi do zrozumienia populacji, ich charakterystyk i wzorc√≥w wy≈ÇaniajƒÖcych siƒô z danych.\nDziedzinƒô statystyki mo≈ºna og√≥lnie podzieliƒá na dwie uzupe≈ÇniajƒÖce siƒô ga≈Çƒôzie:",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wprowadzenie",
    "href": "rozdzial1.html#wprowadzenie",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "",
    "text": "Statystyka to nauka o uczeniu siƒô z danych w warunkach niepewno≈õci.\n\n\n\n\n\n\n\n\nNote\n\n\n\nStatystyka to nauka o zbieraniu, organizowaniu, analizowaniu, interpretowaniu i prezentowaniu danych. Obejmuje zar√≥wno metody pracy z danymi, jak i teoretyczne podstawy uzasadniajƒÖce te metody. Ale statystyka to co≈õ wiƒôcej ni≈º tylko liczby i wzory ‚Äî to spos√≥b my≈õlenia o niepewno≈õci i zmienno≈õci w otaczajƒÖcym nas ≈õwiecie.\n\n\n\n\n\nWyboru reprezentatywnej grupy os√≥b do badania\nObliczenia podsumowa≈Ñ opisowych na podstawie ich odpowiedzi\nOszacowania prawdopodobnej ≈õredniej dla ca≈Çej populacji\nOkre≈õlenia, jak pewni mo≈ºemy byƒá naszego oszacowania\nSprawdzenia, czy ≈õredni wiek zmienia siƒô w czasie\n\n\n\nDemografia to nauka zajmujƒÖca siƒô badaniem ludno≈õci, koncentrujƒÖca siƒô na jej wielko≈õci, strukturze, rozmieszczeniu i zmianach zachodzƒÖcych w czasie. To zasadniczo analiza statystyczna populacji - kim sƒÖ ludzie, gdzie mieszkajƒÖ, ilu ich jest i jak te charakterystyki ewoluujƒÖ.\n\n\n\n\n\n\n\n\n\nProcent vs punkty procentowe (pp)\n\n\n\nGdy w mediach s≈Çyszysz, ≈ºe ‚Äûbezrobocie spad≈Ço o 2‚Äù, czy chodzi o 2 punkty procentowe (pp), czy 2 procent?\nTo nie to samo:\n\n2 pp (zmiana absolutna): np. 10% ‚Üí 8% (‚àí2 pp).\n2% (zmiana wzglƒôdna): mno≈ºymy starƒÖ stopƒô przez 0,98; np. 10% ‚Üí 9,8% (‚àí0,2 pp).\n\nZawsze pytaj:\n\nJaka jest warto≈õƒá bazowa (wcze≈õniejsza stopa)?\nCzy to zmiana absolutna (pp), czy wzglƒôdna (%)?\nCzy r√≥≈ºnica mo≈ºe wynikaƒá z b≈Çƒôdu losowego / b≈Çƒôdu pr√≥by?\nJak mierzono bezrobocie (badanie ankietowe vs dane administracyjne), kiedy i kogo uwzglƒôdniono?\n\nProsta zasada\n\nU≈ºywaj punkt√≥w procentowych (pp), gdy por√≥wnujesz stopy/procenty wprost (bezrobocie, frekwencja).\nU≈ºywaj procent√≥w (%) dla zmian wzglƒôdnych (wzglƒôdem warto≈õci wyj≈õciowej).\n\nMa≈Ça ≈õciƒÖga\n\n\n\n\n\n\n\n\nStopa poczƒÖtkowa\n‚ÄûSpadek o 2%‚Äù (wzglƒôdny)\n‚ÄûSpadek o 2 pp‚Äù (absolutny)\n\n\n\n\n6%\n6% √ó 0,98 = 5,88% (‚àí0,12 pp)\n4%\n\n\n8%\n8% √ó 0,98 = 7,84% (‚àí0,16 pp)\n6%\n\n\n10%\n10% √ó 0,98 = 9,8% (‚àí0,2 pp)\n8%\n\n\n\nUwaga: 2% ‚â† 2 punkty procentowe (pp).\n\n\n\n\n\n\n\n\n\nZaokrƒÖglenia i notacja naukowa w statystyce\n\n\n\nZasada g≈Ç√≥wna: O ile nie podano inaczej, czƒô≈õci u≈Çamkowe liczb dziesiƒôtnych zaokrƒÖglaj do co najmniej 2 cyfr znaczƒÖcych. W statystyce czƒôsto pracujemy z d≈Çugimi czƒô≈õciami u≈Çamkowymi i bardzo ma≈Çymi liczbami ‚Äî w obliczeniach, nie zaokrƒÖglaj nadmiernie w krokach po≈õrednich, zaokrƒÖglaj na ko≈Ñcu oblicze≈Ñ.\n\nZaokrƒÖglanie w kontek≈õcie statystycznym\nCzƒô≈õƒá u≈Çamkowa to cyfry po przecinku dziesiƒôtnym. W statystyce szczeg√≥lnie wa≈ºne jest zachowanie odpowiedniej precyzji:\nStatystyki opisowe:\n\n≈örednia: \\bar{x} = 15.847693... \\rightarrow 15.85\nOdchylenie standardowe: s = 2.7488... \\rightarrow 2.75\nWsp√≥≈Çczynnik korelacji: r = 0.78432... \\rightarrow 0.78\n\nBardzo ma≈Çe liczby (p-warto≈õci, prawdopodobie≈Ñstwa):\n\np = 0.000347... \\rightarrow 0.00035 lub 3.5 \\times 10^{-4}\nP(X &gt; 2) = 0.0000891... \\rightarrow 0.000089 lub 8.9 \\times 10^{-5}\n\n\n\nCyfry znaczƒÖce w czƒô≈õci u≈Çamkowej\nW czƒô≈õci u≈Çamkowej cyfry znaczƒÖce to wszystkie cyfry opr√≥cz zer wiodƒÖcych:\n\n.78432 ma 5 cyfr znaczƒÖcych ‚Üí zaokrƒÖglamy do .78 (2 c.z.)\n.000347 ma 3 cyfry znaczƒÖce ‚Üí zaokrƒÖglamy do .00035 (2 c.z.)\n.050600 ma 4 cyfry znaczƒÖce ‚Üí zaokrƒÖglamy do .051 (2 c.z.)\n\n\n\nZasady zaokrƒÖglania w statystyce\n\nZaokrƒÖglaj tylko czƒô≈õƒá u≈ÇamkowƒÖ do co najmniej 2 cyfr znaczƒÖcych\nCzƒô≈õƒá ca≈Çkowita pozostaje niezmieniona\nW d≈Çugich obliczeniach zachowuj 3-4 cyfry w czƒô≈õci u≈Çamkowej do ostatniego kroku\nNIGDY nie zaokrƒÖglaj do zera - ma≈Çe warto≈õci majƒÖ znaczenie interpretacyjne\nDla bardzo ma≈Çych liczb u≈ºywaj notacji naukowej gdy to u≈Çatwia odczyt\nP-warto≈õci czƒôsto wymagajƒÖ wiƒôkszej precyzji ‚Äî zachowaj 2-3 cyfry znaczƒÖce\n\n‚ö†Ô∏è UWAGA: Nie zaokrƒÖglaj do zera!\n\n\nNotacja naukowa w statystyce\nW statystyce czƒôsto spotykamy bardzo ma≈Çe liczby. U≈ºywaj notacji naukowej gdy u≈Çatwia to odczyt:\nP-warto≈õci i prawdopodobie≈Ñstwa:\n\np = 0.000347 = 3.47 \\times 10^{-4} (lepiej: 3.5 \\times 10^{-4})\nP(Z &gt; 3.5) = 0.000233 = 2.33 \\times 10^{-4}\n\nBardzo ma≈Çe odchylenia standardowe:\n\n\\sigma = 0.000892 = 8.92 \\times 10^{-4}\n\nDu≈ºe liczby (rzadko w podstawowej statystyce):\n\nN = 1\\,234\\,567 = 1.23 \\times 10^6\n\nWƒÖtpliwo≈õci: Lepiej zachowaƒá dodatkowƒÖ cyfrƒô ni≈º zaokrƒÖgliƒá zbyt mocno\n\n\n\n\n\n\nStatystyka opisowa (Descriptive Statistics)\nStatystyka opisowa obejmuje metody podsumowywania i prezentowania danych. Obejmuje to:\nMiary tendencji centralnej - Gdzie znajduje siƒô ‚Äúcentrum‚Äù (≈õrednia) zbioru danych?\n\n≈örednia (Mean): ≈örednia arytmetyczna. Je≈õli ca≈Çkowity doch√≥d gospodarstw domowych na wsi liczƒÖcej 100 gospodarstw wynosi 5 000 000 z≈Ç, ≈õredni doch√≥d wynosi 50 000 z≈Ç. Jednak je≈õli jedno gospodarstwo jest niezwykle bogate, mo≈ºe to nie reprezentowaƒá dobrze typowego gospodarstwa.\nMediana (Median): Warto≈õƒá ≈õrodkowa, gdy dane sƒÖ uporzƒÖdkowane. Je≈õli wiƒôkszo≈õƒá gospodarstw zarabia miƒôdzy 20 000-40 000 z≈Ç, ale jedno zarabia 2 000 000 z≈Ç, mediana lepiej reprezentuje typowe gospodarstwo.\nModa/Dominanta (Mode): Najczƒô≈õciej wystƒôpujƒÖca warto≈õƒá. W badaniu wielko≈õci rodziny, je≈õli wiƒôkszo≈õƒá rodzin ma 2 dzieci, 2 jest modƒÖ (dominantƒÖ), nawet je≈õli ≈õrednia wynosi 2,3 dziecka.\n\nMiary zmienno≈õci - Jak rozrzucone/zr√≥≈ºnicowane sƒÖ dane?\n\nRozstƒôp (Range): R√≥≈ºnica miƒôdzy warto≈õciƒÖ maksymalnƒÖ a minimalnƒÖ. Je≈õli wiek w spo≈Çeczno≈õci waha siƒô od 0 do 95 lat, rozstƒôp wynosi 95 lat.\nWariancja (Variance): ≈örednie kwadratowe odchylenie od ≈õredniej. Mierzy, jak daleko warto≈õci przeciƒôtnie odchylajƒÖ siƒô od centrum.\nOdchylenie standardowe (Standard Deviation): Pierwiastek kwadratowy z wariancji, w tych samych jednostkach co oryginalne dane. Je≈õli ≈õredni wiek wynosi 35 lat z odchyleniem standardowym 20 lat, wiƒôkszo≈õƒá ludzi ma miƒôdzy 15 a 55 lat.\n\nReprezentacje wizualne (przyk≈Çady)\n\nPiramidy wieku (Population Pyramids): PokazujƒÖ rozk≈Çad wieku i p≈Çci, ujawniajƒÖc historiƒô demograficznƒÖ. Szeroka podstawa wskazuje na wysokie wska≈∫niki urodze≈Ñ; wƒÖska podstawa sugeruje spadajƒÖcƒÖ p≈Çodno≈õƒá.\nTablice trwania ≈ºycia (Life Tables): PodsumowujƒÖ wzorce ≈õmiertelno≈õci, pokazujƒÖc prawdopodobie≈Ñstwo prze≈ºycia do ka≈ºdego wieku.\nWykresy szereg√≥w czasowych (Time Series Graphs): Wy≈õwietlajƒÖ trendy w czasie, takie jak zmieniajƒÖce siƒô wska≈∫niki p≈Çodno≈õci na przestrzeni dekad.\n\n\n\n\nhttps://commons.wikimedia.org/wiki/File:%C5%81%C3%B3d%C5%BA_population_pyramid.svg\n\n\n\n\nWnioskowanie statystyczne (Inferential Statistics)\nWnioskowanie statystyczne to szacowanie efekt√≥w na podstawie pr√≥by i okre≈õlanie, jak bardzo te wyniki sƒÖ niepewne. Stosujemy je w eksperymentach (por√≥wnanie grup), w modelach regresji (wsp√≥≈Çczynniki opisujƒÖce zwiƒÖzek miƒôdzy zmiennymi) oraz w sonda≈ºach, gdzie z pr√≥by wnioskujemy o populacji (np. odsetki/≈õrednie) i podajemy margines b≈Çƒôdu oraz przedzia≈Ç ufno≈õci.\n\n\n\n\n\n\n\nWnioskowanie z pr√≥by o cechach populacji: Analogia ‚Äûpr√≥bowania zupy‚Äù\n\n\n\n\nRozwa≈ºmy kucharza przygotowujƒÖcego zupƒô dla 100 os√≥b, kt√≥ry musi oceniƒá jej smak bez konsumowania ca≈Çego garnka:\nPopulacja: Ca≈Çy garnek zupy (100 porcji)\nPr√≥ba: Jedna ≈Çy≈ºka do spr√≥bowania\nParametr populacji: Prawdziwy ≈õredni poziom s≈Çono≈õci ca≈Çego garnka (nieznany)\nStatystyka z pr√≥by: Poziom s≈Çono≈õci wykryty w ≈Çy≈ºce (obserwowalny)\nWnioskowanie statystyczne: U≈ºywanie charakterystyk ≈Çy≈ºki do wyciƒÖgania wniosk√≥w o ca≈Çym garnku\n\nWa≈ºne\n1. Losowe pr√≥bkowanie jest niezbƒôdne: Kucharz musi dok≈Çadnie wymieszaƒá zupƒô przed pobraniem pr√≥bki. Konsekwentne pobieranie pr√≥bek z powierzchni mo≈ºe pominƒÖƒá przyprawy, kt√≥re osiad≈Çy, wprowadzajƒÖc systematyczne obciƒÖ≈ºenie.\n2. Wielko≈õƒá pr√≥by wp≈Çywa na precyzjƒô: Wiƒôksza ≈Çy≈ºka dostarcza bardziej wiarygodnych informacji o og√≥lnym smaku ni≈º ma≈Çy ≈Çyk, choƒá np. koszty i czas ograniczajƒÖ mo≈ºliwo≈õƒá dowolnego zwiƒôkszania pr√≥by.\n3. Niepewno≈õƒá jest nieod≈ÇƒÖczna: Nawet przy w≈Ça≈õciwej technice pr√≥bkowania (sampling), ≈Çy≈ºka mo≈ºe nie reprezentowaƒá idealnie charakterystyk ca≈Çego garnka.\n4. Systematyczne obciƒÖ≈ºenie/stronniczo≈õƒá (bias) podwa≈ºa wnioskowanie: Je≈õli kto≈õ potajemnie doda s√≥l tylko do obszaru pr√≥bkowania, wnioski o ca≈Çym garnku stajƒÖ siƒô niewa≈ºne ‚Äî ilustrujƒÖc, jak obciƒÖ≈ºenie pr√≥bkowania zniekszta≈Çca wnioskowanie statystyczne.\n5. Wnioskowanie ma ograniczenia zakresu: Pr√≥ba mo≈ºe oszacowaƒá ≈õredniƒÖ s≈Çono≈õƒá, ale nie mo≈ºe ujawniƒá, czy niekt√≥re czƒô≈õci sƒÖ bardziej s≈Çone ni≈º inne, podkre≈õlajƒÖc granice tego, co pr√≥by mogƒÖ nam powiedzieƒá o zmienno≈õci w populacji.\nTa analogia chwyta istotƒô rozumowania statystycznego: u≈ºywanie starannie wybranych pr√≥b do poznawania wiƒôkszych populacji przy jednoczesnym jawnym uznawaniu i kwantyfikacji nieod≈ÇƒÖcznej niepewno≈õci w tym procesie.\n\n\n\n\nNa przyk≈Çad, je≈õli badanie 1000 gospodarstw domowych stwierdza, ≈ºe 23% obejmuje trzy pokolenia mieszkajƒÖce razem, statystyka inferencyjna pomaga nam:\n\n\nOszacowaƒá, ≈ºe miƒôdzy 20% a 26% wszystkich gospodarstw w populacji prawdopodobnie ma takƒÖ strukturƒô (przedzia≈Ç ufno≈õci)\nSprawdziƒá, czy ten odsetek wzr√≥s≈Ç w por√≥wnaniu do poprzedniej dekady (testowanie hipotez)\nZbadaƒá, czy mieszkanie wielopokoleniowe jest bardziej powszechne w niekt√≥rych grupach etnicznych (por√≥wnanie grup)\nPrzewidzieƒá przysz≈Çe trendy na podstawie obecnych wzorc√≥w (regresja i prognozowanie)\n\n\n\n\n\n\n\n\nNote\n\n\n\nPodstawowa zasada: Statystyka nie eliminuje niepewno≈õci ‚Äî pomaga nam jƒÖ mierzyƒá, zarzƒÖdzaƒá niƒÖ i skutecznie komunikowaƒá.\n\n\n\n\nMy≈õlenie Statystyczne: Wprowadzenie\n\nScenariusz Badawczy\nW≈Çadze uniwersytetu rozwa≈ºajƒÖ udostƒôpnienie biblioteki ca≈Çodobowo. Administracja potrzebuje odpowiedzi na pytanie: Jaka czƒô≈õƒá student√≥w popiera tƒô zmianƒô?\n\n\n\n\n\n\nPodstawowe Wyzwanie\n\n\n\nSytuacja idealna: Zapytanie wszystkich 20 000 student√≥w ‚Üí Uzyskanie dok≈Çadnej odpowiedzi\nSytuacja rzeczywista: Ankietowanie 100 student√≥w ‚Üí Uzyskanie oszacowania z niepewno≈õciƒÖ\n\n\n\n\nDwa Podej≈õcia do Tych Samych Danych\nZa≈Ç√≥≈ºmy, ≈ºe przeprowadzono ankietƒô w≈õr√≥d 100 losowo wybranych student√≥w i stwierdzono, ≈ºe 60 z nich popiera ca≈Çodobowe otwarcie biblioteki.\n\n\n‚ùå Bez My≈õlenia Statystycznego\n‚Äú60 ze 100 student√≥w odpowiedzia≈Ço twierdzƒÖco.‚Äù\nWniosek: ‚ÄúDok≈Çadnie 60% wszystkich student√≥w popiera zmianƒô.‚Äù\nDecyzja: ‚ÄúPoniewa≈º przekracza to 50%, mamy wyra≈∫ne poparcie wiƒôkszo≈õci.‚Äù\nProblem: Ignorowanie faktu, ≈ºe inna pr√≥ba mog≈Çaby daƒá wynik 55% lub 65%\n\n‚úÖ Z Zastosowaniem My≈õlenia Statystycznego\n‚Äú60 ze 100 student√≥w odpowiedzia≈Ço twierdzƒÖco.‚Äù\nWniosek: ‚ÄúSzacujemy poparcie na poziomie 60% z marginesem b≈Çƒôdu ¬±10%‚Äù\nDecyzja: ‚ÄúPrawdziwe poparcie prawdopodobnie mie≈õci siƒô miƒôdzy 50% a 70% ‚Äî potrzebujemy wiƒôkszej pr√≥by dla pewno≈õci wiƒôkszo≈õciowego poparcia.‚Äù\nPrzewaga: Uznanie niepewno≈õci prowadzi do lepszych decyzji\n\n\n\n\nZnaczenie Praktyczne\n\n\n\n\n\n\nWa≈ºna uwaga\n\n\n\nMy≈õlenie statystyczne nie polega na zmniejszeniu pewno≈õci ‚Äî polega na rzetelnym przedstawieniu niepewno≈õci.\nRozwa≈ºmy nastƒôpujƒÖce scenariusze dla wyniku 60% (¬±10% margines b≈Çƒôdu):\n\nScenariusz optymistyczny: Rzeczywiste poparcie mo≈ºe wynosiƒá 70% (silna wiƒôkszo≈õƒá)\nScenariusz pesymistyczny: Rzeczywiste poparcie mo≈ºe wynosiƒá 50% (brak wyra≈∫nej wiƒôkszo≈õci)\nScenariusz najbardziej prawdopodobny: Rzeczywiste poparcie wynosi oko≈Ço 60% (umiarkowana wiƒôkszo≈õƒá)\n\nBez uwzglƒôdnienia tego zakresu mo≈ºemy podejmowaƒá kosztowne decyzje oparte na fa≈Çszywej precyzji.\n\n\n\n\nZnaczenie Wielko≈õci Pr√≥by\nJe≈õli wymagana jest wiƒôksza precyzja, nale≈ºy rozwa≈ºyƒá wp≈Çyw wielko≈õci pr√≥by na poziom ufno≈õci:\n\n\n\n\n\n\n\n\n\n\nWielko≈õƒá Pr√≥by\nWynik Badania\nMargines B≈Çƒôdu\nPrzedzia≈Ç Ufno≈õci\nInterpretacja\n\n\n\n\nn = 100\n60%\n¬±10%\n50% do 70%\nNiepewno≈õƒá co do wiƒôkszo≈õci\n\n\nn = 400\n60%\n¬±5%\n55% do 65%\nPrawdopodobne poparcie wiƒôkszo≈õci\n\n\nn = 1 600\n60%\n¬±2,5%\n57,5% do 62,5%\nWyra≈∫ne poparcie wiƒôkszo≈õci\n\n\n\n\n\n\n\n\n\nNale≈ºy Pamiƒôtaƒá\n\n\n\nMy≈õlenie statystyczne przekszta≈Çca stwierdzenie ‚Äú60 student√≥w odpowiedzia≈Ço twierdzƒÖco‚Äù z pozornie precyzyjnej, lecz wprowadzajƒÖcej w b≈ÇƒÖd informacji w rzetelnƒÖ ocenƒô: ‚ÄúZ du≈ºym prawdopodobie≈Ñstwem mo≈ºna stwierdziƒá, ≈ºe miƒôdzy 50% a 70% wszystkich student√≥w popiera tƒô propozycjƒô.‚Äù\nTa ostro≈ºno≈õƒá w formu≈Çowaniu wniosk√≥w prowadzi do lepszych decyzji.\n\n\n\n\nZastosowanie My≈õlenia Statystycznego w Praktyce\nAnalizujƒÖc dane statystyczne, nale≈ºy zawsze rozwa≈ºyƒá nastƒôpujƒÖce pytania:\n\nJaka jest wielko≈õƒá pr√≥by badawczej?\nJaki jest margines b≈Çƒôdu statystycznego?\nCzy prawdziwa warto≈õƒá mo≈ºe znaczƒÖco r√≥≈ºniƒá siƒô od warto≈õci raportowanej?\nCzy decyzja uleg≈Çaby zmianie, gdyby prawdziwa warto≈õƒá znajdowa≈Ça siƒô na granicy przedzia≈Çu niepewno≈õci/ufno≈õci?\n\nOdpowiedzi na te pytania stanowiƒÖ fundament my≈õlenia statystycznego.\n\n\n\n\n\n\nPrzyk≈Çad historyczny: Sonda≈º Literary Digest z 1936 roku\n\n\n\nLiterary Digest przeprowadzi≈Ç jeden z najwiƒôkszych sonda≈ºy w historii z 2,4 miliona odpowiedzi, przewidujƒÖc, ≈ºe Alf Landon pokona Franklina D. Roosevelta w wyborach prezydenckich w 1936 roku.\nPomimo ogromnej wielko≈õci pr√≥by:\nPrzewidywanie: Landon 57%, Roosevelt 43% Rzeczywisty wynik: Roosevelt 62%, Landon 38% B≈ÇƒÖd: 25 punkt√≥w procentowych!\nCo posz≈Ço nie tak? Sonda≈º by≈Ç wadliwy z powodu systematycznego b≈Çƒôdu/obciƒÖ≈ºenia (bias):\nObciƒÖ≈ºenie selekcyjne (selection bias) w ramce/operacie losowania:\n\n≈πr√≥d≈Ça: ksiƒÖ≈ºki telefoniczne, rejestracje samochod√≥w, cz≈Çonkostwo w klubach\nProblem: W 1936 roku ≈∫r√≥d≈Ça te nadreprezentowa≈Çy bogatych Amerykan√≥w, kt√≥rzy faworyzowali Landona\nWynik: Pr√≥ba systematycznie wyklucza≈Ça zwolennik√≥w Roosevelta\n\nObciƒÖ≈ºenie braku odpowiedzi:\n\nTylko 24% skontaktowanych os√≥b odpowiedzia≈Ço (problem z ‚Äúresponse rate‚Äù)\nPrawdopodobni respondenci: osoby z silnymi opiniami anty-Roosevelt\nStruktura odm√≥w: wielu zwolennik√≥w Roosevelta nie czu≈Ço potrzeby uczestniczenia\n\nKluczowe wnioski:\n\nDu≈ºa obciƒÖ≈ºona (biased) pr√≥ba jest gorsza ni≈º ma≈Ça reprezentatywna pr√≥ba\nB≈Çƒôdy standardowe mierzƒÖ tylko b≈ÇƒÖd losowy, nie obciƒÖ≈ºenie\nWielko≈õƒá pr√≥by nie mo≈ºe naprawiƒá fundamentalnych problem√≥w dotyczƒÖcych nielosowego doboru pr√≥by\nReprezentatywne losowanie ma wiƒôksze znaczenie ni≈º wielko≈õƒá pr√≥by\n\nTa katastrofa doprowadzi≈Ça do znacznych ulepsze≈Ñ w metodologii sonda≈ºowej, w tym rozwoju losowania probabilistycznego i ≈õledzenia wska≈∫nik√≥w odpowiedzi.\n\n\n\n\n\nWsp√≥≈Çczesne sonda≈ºe\nDzisiejsze sonda≈ºe, choƒá znacznie mniejsze ni≈º 2,4 miliona odpowiedzi Literary Digest, sƒÖ zwykle znacznie dok≈Çadniejsze, poniewa≈º koncentrujƒÖ siƒô na:\nReprezentatywnym losowaniu: U≈ºywanie metod opartych na prawdopodobie≈Ñstwie, aby zapewniƒá wszystkim grupom znane szanse selekcji\nWykrywanie i korekcja obciƒÖ≈ºenia: Monitorowanie wska≈∫nik√≥w odpowiedzi w r√≥≈ºnych grupach demograficznych i korygowanie znanych obciƒÖ≈ºe≈Ñ (bias)\nKwantyfikacja niepewno≈õci: Raportowanie margines√≥w b≈Çƒôdu (statystyczny b≈ÇƒÖd losowy), kt√≥re uczciwie komunikujƒÖ granice tego, co wiemy\nPrzyk≈Çad: Wsp√≥≈Çczesny sonda≈º 1000 losowo wybranych wyborc√≥w z 3% marginesem b≈Çƒôdu jest zwykle znacznie bardziej wiarygodny ni≈º masywne, ale obciƒÖ≈ºone badanie Literary Digest.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#dane-i-rozk≈Çady",
    "href": "rozdzial1.html#dane-i-rozk≈Çady",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.2 Dane i rozk≈Çady",
    "text": "2.2 Dane i rozk≈Çady\nDane: Informacje zebrane podczas badania ‚Äì obejmujƒÖ odpowiedzi z ankiet, wyniki eksperyment√≥w, wska≈∫niki ekonomiczne, tre≈õci z medi√≥w spo≈Çeczno≈õciowych lub wszelkie inne mierzalne obserwacje.\nZrozumienie typ√≥w danych i rozk≈Çad√≥w jest fundamentalne dla wyboru odpowiednich analiz i poprawnej interpretacji wynik√≥w.\n\n\n\n\n\n\nRodzaje i Formaty Zbior√≥w Danych\n\n\n\n\nDane Przekrojowe\nObserwacje na zmiennych (kolumny w bazie danych) zebrane w jednym punkcie czasowym dla wielu podmiot√≥w:\n\n\n\nOsoba\nWiek\nDoch√≥d\nWykszta≈Çcenie\n\n\n\n\n1\n25\n5000\nLicencjat\n\n\n2\n35\n7500\nMagister\n\n\n3\n45\n9000\nDoktorat\n\n\n\n\n\nSzeregi Czasowe\nObserwacje jednego podmiotu w kolejnych punktach czasowych:\n\n\n\nRok\nPKB (w mld)\nStopa Bezrobocia\n\n\n\n\n2018\n20.580\n3,9%\n\n\n2019\n21.433\n3,7%\n\n\n2020\n20.933\n8,1%\n\n\n\n\n\nDane Panelowe (Longitudinalne)\nObserwacje wielu podmiot√≥w w czasie:\n\n\n\nKraj\nRok\nPKB per capita\nD≈Çugo≈õƒá ≈ºycia\n\n\n\n\nPolska\n2018\n32.794\n76,7\n\n\nPolska\n2019\n35.118\n76,8\n\n\nNiemcy\n2018\n46.194\n81,9\n\n\nNiemcy\n2019\n46.194\n82,0\n\n\n\n\n\nDane Przekrojowo-Czasowe (TSCS)\nSzczeg√≥lny przypadek danych panelowych gdzie:\n\nLiczba punkt√≥w czasowych &gt; liczba podmiot√≥w\nStruktura podobna do danych panelowych\nCzƒôsto stosowane w ekonomii i politologii\n\n\n\n\nFormaty Danych\n\nFormat Szeroki\nKa≈ºdy wiersz to podmiot; kolumny to zmienne/punkty czasowe:\n\n\n\nKraj\nPKB_2018\nPKB_2019\nD≈ª_2018\nD≈ª_2019\n\n\n\n\nPolska\n32.794\n35.118\n76,7\n76,8\n\n\nNiemcy\n46.194\n46.194\n81,9\n82,0\n\n\n\n\n\nFormat D≈Çugi\nKa≈ºdy wiersz to unikalna kombinacja podmiot-czas-zmienna:\n\n\n\nKraj\nRok\nZmienna\nWarto≈õƒá\n\n\n\n\nPolska\n2018\nPKB per capita\n32.794\n\n\nPolska\n2019\nPKB per capita\n35.118\n\n\nPolska\n2018\nD≈Çugo≈õƒá ≈ºycia\n76,7\n\n\nPolska\n2019\nD≈Çugo≈õƒá ≈ºycia\n76,8\n\n\nNiemcy\n2018\nPKB per capita\n46.194\n\n\nNiemcy\n2019\nPKB per capita\n46.194\n\n\nNiemcy\n2018\nD≈Çugo≈õƒá ≈ºycia\n81,9\n\n\nNiemcy\n2019\nD≈Çugo≈õƒá ≈ºycia\n82,0\n\n\n\nUwaga: Format d≈Çugi jest zazwyczaj preferowany do:\n\nManipulacji danymi w R i Pythonie\nAnaliz statystycznych\nWizualizacji danych\n\n\n\n\n\n\n\nTypy danych\nDane sk≈ÇadajƒÖ siƒô z zebranych obserwacji lub pomiar√≥w. Typ danych okre≈õla, jakie operacje matematyczne sƒÖ wykonalne i jakie metody statystyczne majƒÖ zastosowanie.\n\nDane ilo≈õciowe\nDane ciƒÖg≈Çe mogƒÖ przyjmowaƒá dowolnƒÖ warto≈õƒá w przedziale:\nPrzyk≈Çady o znaczeniu demograficznym:\n\nWiek: Mo≈ºe wynosiƒá 25,5 lat, 25,51 lat, 25,514 lat (precyzja ograniczona tylko dok≈Çadno≈õciƒÖ narzƒôdzia pomiarowego)\nWska≈∫nik masy cia≈Ça: 23,7 kg/m¬≤\nWsp√≥≈Çczynnik dzietno≈õci: 1,73 dzieci na kobietƒô\nGƒôsto≈õƒá zaludnienia: 4521,3 osoby na km¬≤\n\nW≈Ça≈õciwo≈õci:\n\nMo≈ºna wykonywaƒá wszystkie operacje arytmetyczne\nMo≈ºna obliczaƒá ≈õrednie, odchylenia standardowe\nCzƒôsto ‚ÄúpodƒÖ≈ºajƒÖ‚Äù za znanymi rozk≈Çadami prawdopodobie≈Ñstwa (np. waga za rozk≈Çadem normalnym)\n\nDane dyskretne mogƒÖ przyjmowaƒá tylko okre≈õlone warto≈õci:\nPrzyk≈Çady:\n\nLiczba dzieci: 0, 1, 2, 3‚Ä¶ (nie mo≈ºna mieƒá 2,5 dziecka)\nLiczba ma≈Ç≈ºe≈Ñstw: 0, 1, 2, 3‚Ä¶\nWielko≈õƒá gospodarstwa domowego: 1, 2, 3, 4‚Ä¶ os√≥b\nLiczba wizyt u lekarza: 0, 1, 2, 3‚Ä¶ rocznie\n\n\n\nDane jako≈õciowe/kategorialne\nDane nominalne reprezentujƒÖ kategorie bez naturalnego porzƒÖdku:\nPrzyk≈Çady:\n\nKraj urodzenia: USA, Chiny, Indie, Brazylia‚Ä¶\nReligia: Chrze≈õcija≈Ñstwo, Islam, Hinduizm, Buddyzm, Brak‚Ä¶\nStan cywilny: Kawaler/Panna, ≈ªonaty/Mƒô≈ºatka, Rozwiedziony/a, Wdowiec/Wdowa\nPrzyczyna ≈õmierci: Choroby serca, Rak, Wypadek, Udar‚Ä¶\nGrupa krwi: A, B, AB, 0\n\nCo mo≈ºemy zrobiƒá:\n\nLiczyƒá czƒôsto≈õci\nObliczaƒá proporcje\nZnale≈∫ƒá modƒô\nTestowaƒá niezale≈ºno≈õƒá\n\nCzego nie mo≈ºemy zrobiƒá:\n\nObliczaƒá ≈õredniej (≈õrednia religia nie ma sensu)\nPorzƒÖdkowaƒá kategorii znaczƒÖco\nObliczaƒá odleg≈Ço≈õci miƒôdzy kategoriami\n\nDane porzƒÖdkowe reprezentujƒÖ uporzƒÖdkowane kategorie:\nPrzyk≈Çady:\n\nPoziom wykszta≈Çcenia: Brak &lt; Podstawowe &lt; ≈örednie &lt; Wy≈ºsze\nStatus spo≈Çeczno-ekonomiczny: Niski &lt; ≈öredni &lt; Wysoki\nSamoocena zdrowia: Z≈Çy &lt; Przeciƒôtny &lt; Dobry &lt; Doskona≈Çy\nSkala zgody: Zdecydowanie siƒô nie zgadzam &lt; Nie zgadzam siƒô &lt; Neutralny &lt; Zgadzam siƒô &lt; Zdecydowanie siƒô zgadzam\n\nUwaga: Interwa≈Çy miƒôdzy kategoriami niekoniecznie sƒÖ r√≥wne. ‚ÄûOdleg≈Ço≈õƒá‚Äù od Z≈Çego do Przeciƒôtnego zdrowia mo≈ºe nie r√≥wnaƒá siƒô odleg≈Ço≈õci od Dobrego do Doskona≈Çego.\n\n\n\nRozk≈Çad Danych\nRozk≈Çad danych (data distribution) opisuje, jak warto≈õci rozk≈ÇadajƒÖ siƒô miƒôdzy mo≈ºliwymi wynikami (jakie warto≈õci przyjmuje zmienna i jak czƒôsto). Rozk≈Çady m√≥wiƒÖ nam, kt√≥re warto≈õci sƒÖ powszechne, kt√≥re sƒÖ rzadkie i jakie wzorce istniejƒÖ w naszych danych.\nZrozumienie rozk≈Çad√≥w jest fundamentalne dla statystyki, poniewa≈º pomaga nam podsumowywaƒá du≈ºe zbiory danych, identyfikowaƒá wzorce i podejmowaƒá ≈õwiadome decyzje.\nNa przyk≈Çad, wiedza o tym, ≈ºe wiƒôkszo≈õƒá student√≥w uzyskuje wyniki miƒôdzy 60-80 punkt√≥w na egzaminie, m√≥wi nam wiƒôcej ni≈º sama ≈õrednia.\n\nCzƒôsto≈õƒá, Czƒôsto≈õƒá Wzglƒôdna i Gƒôsto≈õƒá\nAnalizujƒÖc dane, czƒôsto interesuje nas, ile razy pojawia siƒô ka≈ºda warto≈õƒá (lub przedzia≈Ç warto≈õci). Prowadzi nas to do trzech powiƒÖzanych pojƒôƒá:\nCzƒôsto≈õƒá (bezwzglƒôdna) (frequency) to po prostu liczba wystƒÖpie≈Ñ danej warto≈õci lub kategorii w naszych danych. Je≈õli 15 student√≥w uzyska≈Ço wyniki miƒôdzy 70-80 punkt√≥w na egzaminie, czƒôsto≈õƒá dla tego przedzia≈Çu wynosi 15.\nCzƒôsto≈õƒá wzglƒôdna (relative frequency) wyra≈ºa czƒôsto≈õƒá jako proporcjƒô lub procent ca≈Ço≈õci. Odpowiada na pytanie: ‚ÄúJaka czƒô≈õƒá wszystkich obserwacji nale≈ºy do tej kategorii?‚Äù Czƒôsto≈õƒá wzglƒôdna obliczana jest jako:\n\\text{Czƒôsto≈õƒá wzglƒôdna} = \\frac{\\text{Czƒôsto≈õƒá}}{\\text{Ca≈Çkowita liczba obserwacji}}\nJe≈õli 15 ze 100 student√≥w uzyska≈Ço 70-80 punkt√≥w, czƒôsto≈õƒá wzglƒôdna wynosi 15/100 = 0,15 lub 15%. Czƒôsto≈õci wzglƒôdne zawsze sumujƒÖ siƒô do 1 (lub 100%), co czyni je u≈ºytecznymi do por√≥wnywania rozk≈Çad√≥w o r√≥≈ºnych liczebno≈õciach pr√≥by.\nGƒôsto≈õƒá (density) jest podobna do czƒôsto≈õci wzglƒôdnej, ale uwzglƒôdnia szeroko≈õƒá przedzia≈Ç√≥w. Gdy grupujemy dane ciƒÖg≈Çe (takie jak czas czy stopa bezrobocia) w przedzia≈Çy o r√≥≈ºnych szeroko≈õciach, gƒôsto≈õƒá zapewnia uczciwe por√≥wnanie. Gƒôsto≈õƒá obliczana jest jako:\n\\text{Gƒôsto≈õƒá} = \\frac{\\text{Czƒôsto≈õƒá wzglƒôdna}}{\\text{Szeroko≈õƒá przedzia≈Çu}}\nGƒôsto≈õƒá jest szczeg√≥lnie wa≈ºna dla zmiennych ciƒÖg≈Çych, poniewa≈º zapewnia, ≈ºe ca≈Çkowite pole pod rozk≈Çadem r√≥wna siƒô 1, co pozwala nam interpretowaƒá pola jako prawdopodobie≈Ñstwa.\n\n\n\n\n\n\nTip\n\n\n\nPrawdopodobie≈Ñstwo zdarzenia to liczba z przedzia≈Çu od 0 do 1; im wiƒôksze prawdopodobie≈Ñstwo, tym bardziej prawdopodobne jest wystƒÖpienie zdarzenia.\n\n\nCzƒôsto≈õƒá skumulowana (cumulative frequency) m√≥wi nam, ile obserwacji znajduje siƒô na danym poziomie lub poni≈ºej niego. Zamiast pytaƒá ‚Äúile obserwacji jest w tej kategorii?‚Äù, czƒôsto≈õƒá skumulowana odpowiada na pytanie ‚Äúile obserwacji jest w tej kategorii lub w kategoriach poni≈ºej?‚Äù. Obliczana jest przez sumowanie wszystkich czƒôsto≈õci od najni≈ºszej warto≈õci do bie≈ºƒÖcej warto≈õci w≈ÇƒÖcznie.\nPodobnie, czƒôsto≈õƒá wzglƒôdna skumulowana (cumulative relative frequency) wyra≈ºa to jako proporcjƒô ca≈Ço≈õci, odpowiadajƒÖc na pytanie ‚Äújaki procent obserwacji znajduje siƒô na tym poziomie lub poni≈ºej?‚Äù. Jest to szczeg√≥lnie u≈ºyteczne do zrozumienia percentyli i kwartyli. Na przyk≈Çad, je≈õli czƒôsto≈õƒá wzglƒôdna skumulowana dla wyniku 70 wynosi 0,40, oznacza to, ≈ºe 40% student√≥w uzyska≈Ço wynik 70 lub ni≈ºszy.\n\n\nTablice Rozk≈Çadu (szereg rozdzielczy danych)\nTablica rozk≈Çadu czƒôsto≈õci (frequency distribution table) organizuje dane, pokazujƒÖc jak obserwacje rozk≈ÇadajƒÖ siƒô miƒôdzy r√≥≈ºnymi warto≈õciami lub przedzia≈Çami. Oto przyk≈Çad z wynikami egzamin√≥w:\n\n\n\n\n\n\n\n\n\n\n\nPrzedzia≈Ç wynik√≥w\nCzƒôsto≈õƒá\nCzƒôsto≈õƒá wzglƒôdna\nCzƒôsto≈õƒá skumulowana\nCzƒôsto≈õƒá wzglƒôdna skumulowana\nGƒôsto≈õƒá\n\n\n\n\n0-50\n10\n0,10\n10\n0,10\n0,002\n\n\n50-70\n30\n0,30\n40\n0,40\n0,015\n\n\n70-90\n45\n0,45\n85\n0,85\n0,0225\n\n\n90-100\n15\n0,15\n100\n1,00\n0,015\n\n\nSuma\n100\n1,00\n-\n-\n-\n\n\n\nTa tablica pokazuje, ≈ºe wiƒôkszo≈õƒá student√≥w uzyska≈Ça wyniki w przedziale 70-90, podczas gdy bardzo niewielu uzyska≈Ço wyniki poni≈ºej 50 lub powy≈ºej 90. Kolumny skumulowane pokazujƒÖ nam, ≈ºe 40% student√≥w uzyska≈Ço wyniki poni≈ºej 70, a 85% poni≈ºej 90.\nTakie tablice sƒÖ u≈ºyteczne dla szybkiego przeglƒÖdu danych przed przeprowadzeniem bardziej z≈Ço≈ºonych analiz.\n\n\nWizualizacja Rozk≈Çad√≥w: Histogramy\nHistogram to graficzna reprezentacja rozk≈Çadu czƒôsto≈õci. Wy≈õwietla dane u≈ºywajƒÖc s≈Çupk√≥w, gdzie:\n\nO≈õ x pokazuje warto≈õci lub przedzia≈Çy (klasy, bins)\nO≈õ y mo≈ºe pokazywaƒá czƒôsto≈õƒá, czƒôsto≈õƒá wzglƒôdnƒÖ lub gƒôsto≈õƒá\nWysoko≈õƒá ka≈ºdego s≈Çupka reprezentuje liczbƒô, proporcjƒô lub gƒôsto≈õƒá dla danego przedzia≈Çu\nS≈Çupki stykajƒÖ siƒô ze sobƒÖ (brak przerw) dla zmiennych ciƒÖg≈Çych\n\nWyb√≥r szeroko≈õci klas: Liczba i szeroko≈õƒá klas znaczƒÖco wp≈Çywa na wyglƒÖd histogramu. Zbyt ma≈Ço klas ukrywa wa≈ºne wzorce, podczas gdy zbyt wiele klas tworzy ‚Äúszum‚Äù i utrudnia dostrze≈ºenie wzorc√≥w.\n\nW statystyce szum (noise) to niepo≈ºƒÖdana losowa zmienno≈õƒá, kt√≥ra przes≈Çania wzorzec, kt√≥ry staramy siƒô znale≈∫ƒá. Mo≈ºna to por√≥wnaƒá do trzask√≥w w radiu ‚Äî utrudniajƒÖ one s≈Çyszenie muzyki (‚Äúsygna≈Çu‚Äù). W danych szum pochodzi z b≈Çƒôd√≥w pomiarowych, losowych fluktuacji lub naturalnej zmienno≈õci badanego zjawiska. Szum to losowa zmienno≈õƒá w danych, kt√≥ra ukrywa prawdziwe wzorce, kt√≥re chcemy dostrzec, podobnie jak ha≈Ças w tle utrudnia us≈Çyszenie rozmowy.\n\nKilka metod pomaga okre≈õliƒá odpowiednie szeroko≈õci klas:\n\nRegu≈Ça Sturgesa (Sturges‚Äô rule): U≈ºyj k = 1 + \\log_2(n) klas, gdzie n to liczebno≈õƒá pr√≥by. Dzia≈Ça dobrze dla w przybli≈ºeniu symetrycznych rozk≈Çad√≥w.\nRegu≈Ça pierwiastka kwadratowego (square root rule): U≈ºyj k = \\sqrt{n} klas. Proste, domy≈õlne ustawienie dzia≈ÇajƒÖce w wielu przypadkach wystarczajƒÖco dobrze.\n\nW R mo≈ºesz okre≈õliƒá klasy na kilka sposob√≥w:\n\n# Okre≈õlenie liczby klas\nhist(exam_scores, breaks = 10)\n\n\n\n\n\n\n\n# Okre≈õlenie dok≈Çadnych punkt√≥w podzia≈Çu\nhist(exam_scores, breaks = seq(0, 100, by = 10))\n\n\n\n\n\n\n\n# Pozw√≥l R wybraƒá automatycznie (domy≈õlnie u≈ºywa regu≈Çy Sturgesa)\nhist(exam_scores)\n\n\n\n\n\n\n\n\nNajlepszym podej≈õciem jest czƒôsto eksperymentowanie z r√≥≈ºnymi szeroko≈õciami klas, aby znale≈∫ƒá to, co najlepiej ujawnia wzorzec w danych. Zacznij od ustawienia domy≈õlnego, nastƒôpnie spr√≥buj mniej i wiƒôcej klas, aby zobaczyƒá, jak zmienia siƒô obraz.\nDefiniowanie granic klas: TworzƒÖc klasy dla tablicy czƒôsto≈õci, musisz zdecydowaƒá, jak obs≈Çugiwaƒá warto≈õci, kt√≥re dok≈Çadnie przypadajƒÖ na granice przedzia≈Ç√≥w klasowych. Na przyk≈Çad, je≈õli masz klasy 0-10 i 10-20, do kt√≥rej klasy nale≈ºy warto≈õƒá 10?\nRozwiƒÖzaniem jest u≈ºycie notacji przedzia≈Çowej (interval notation), aby okre≈õliƒá, czy ka≈ºda granica jest w≈ÇƒÖczona czy wy≈ÇƒÖczona:\n\nPrzedzia≈Ç domkniƒôty (closed interval) [a, b] zawiera oba ko≈Ñce: a \\leq x \\leq b\nPrzedzia≈Ç otwarty (open interval) (a, b) wyklucza oba ko≈Ñce: a &lt; x &lt; b\nPrzedzia≈Ç lewostronnie domkniƒôty (half-open interval) [a, b) zawiera lewy koniec, ale wyklucza prawy: a \\leq x &lt; b\nPrzedzia≈Ç prawostronnie domkniƒôty (half-open interval) (a, b] wyklucza lewy koniec, ale zawiera prawy: a &lt; x \\leq b\n\nStandardowa konwencja: Wiƒôkszo≈õƒá oprogramowania statystycznego, w≈ÇƒÖczajƒÖc R, u≈ºywa przedzia≈Ç√≥w lewostronnie domkniƒôtych [a, b) dla wszystkich klas opr√≥cz ostatniej, kt√≥ra jest w pe≈Çni domkniƒôta [a, b]. Oznacza to:\n\nWarto≈õƒá na dolnej granicy jest w≈ÇƒÖczona do klasy\nWarto≈õƒá na g√≥rnej granicy nale≈ºy do nastƒôpnej klasy\nSama ostatnia klasa zawiera obie granice, aby uchwyciƒá warto≈õƒá maksymalnƒÖ\n\nNa przyk≈Çad, dla klas 0-20, 20-40, 40-60, 60-80, 80-100:\n\n\n\nPrzedzia≈Ç wynik√≥w\nNotacja przedzia≈Çowa\nZawarte warto≈õci\n\n\n\n\n0-20\n[0, 20)\n0 ‚â§ wynik &lt; 20\n\n\n20-40\n[20, 40)\n20 ‚â§ wynik &lt; 40\n\n\n40-60\n[40, 60)\n40 ‚â§ wynik &lt; 60\n\n\n60-80\n[60, 80)\n60 ‚â§ wynik &lt; 80\n\n\n80-100\n[80, 100]\n80 ‚â§ wynik ‚â§ 100\n\n\n\nTa konwencja zapewnia, ≈ºe:\n\nKa≈ºda warto≈õƒá jest liczona dok≈Çadnie raz (bez podw√≥jnego liczenia)\n≈ªadne warto≈õci nie przepadajƒÖ\nKlasy w pe≈Çni pokrywajƒÖ ca≈Çy zakres\n\nPrzedstawiajƒÖc tablice czƒôsto≈õci w raportach, mo≈ºesz po prostu napisaƒá ‚Äú0-20, 20-40, ‚Ä¶‚Äù i zaznaczyƒá, ≈ºe klasy sƒÖ lewostronnie domkniƒôte, lub jawnie pokazaƒá notacjƒô przedzia≈ÇowƒÖ, je≈õli precyzja jest wa≈ºna.\nHistogram czƒôsto≈õci pokazuje surowe liczebno≈õci:\n\n# Przyk≈Çad kodu R\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     main = \"Rozk≈Çad wynik√≥w egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Czƒôsto≈õƒá\",\n     col = \"lightblue\")\n\n\n\n\n\n\n\n\nHistogram czƒôsto≈õci wzglƒôdnej pokazuje proporcje (u≈ºyteczne przy por√≥wnywaniu grup o r√≥≈ºnych liczebno≈õciach):\n\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     freq = FALSE,  # Tworzy histogram czƒôsto≈õci wzglƒôdnej/gƒôsto≈õci\n     main = \"Rozk≈Çad wynik√≥w egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Czƒôsto≈õƒá wzglƒôdna\",\n     col = \"lightgreen\")\n\n\n\n\n\n\n\n\nHistogram gƒôsto≈õci dostosowuje siƒô do szeroko≈õci przedzia≈Ç√≥w i jest u≈ºywany z krzywymi gƒôsto≈õci:\n\nhist(exam_scores, \n     breaks = seq(0, 100, by = 10),\n     freq = FALSE,  # Tworzy skalƒô gƒôsto≈õci\n     main = \"Rozk≈Çad wynik√≥w egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Gƒôsto≈õƒá\",\n     col = \"lightcoral\")\n\n\n\n\n\n\n\n\n\n\nKrzywe Gƒôsto≈õci\nKrzywa gƒôsto≈õci (density curve) to wyg≈Çadzona linia, kt√≥ra przybli≈ºa/modeluje kszta≈Çt rozk≈Çadu. W przeciwie≈Ñstwie do histogram√≥w, kt√≥re pokazujƒÖ rzeczywiste dane w dyskretnych klasach, krzywe gƒôsto≈õci pokazujƒÖ og√≥lny wzorzec jako funkcjƒô ciƒÖg≈ÇƒÖ. Pole pod ca≈ÇƒÖ krzywƒÖ zawsze r√≥wna siƒô 1, a pole pod dowolnƒÖ czƒô≈õciƒÖ krzywej reprezentuje proporcjƒô obserwacji w tym zakresie.\n\n# Dodawanie krzywej gƒôsto≈õci do histogramu\nhist(exam_scores, \n     freq = FALSE,\n     main = \"Wyniki egzaminacyjne z krzywƒÖ gƒôsto≈õci\",\n     xlab = \"Wynik\",\n     ylab = \"Gƒôsto≈õƒá\",\n     col = \"lightblue\",\n     border = \"white\")\nlines(density(exam_scores), \n      col = \"darkred\", \n      lwd = 2)\n\n\n\n\n\n\n\n\nKrzywe gƒôsto≈õci sƒÖ szczeg√≥lnie u≈ºyteczne do:\n\nIdentyfikacji kszta≈Çtu rozk≈Çadu (symetryczny, sko≈õny, bimodalny)\nPor√≥wnywania wielu rozk≈Çad√≥w na tym samym wykresie\nZrozumienia teoretycznego (‚Äúprawdziwego‚Äù) rozk≈Çadu le≈ºƒÖcego u podstaw danych\n\n\n\nWizualizacja Czƒôsto≈õci Skumulowanej\nWykresy czƒôsto≈õci skumulowanej, zwane tak≈ºe ogiwami (ogives, wymawiane ‚Äúoh-d≈ºajw‚Äù), pokazujƒÖ jak czƒôsto≈õci kumulujƒÖ siƒô w zakresie warto≈õci. Te wykresy u≈ºywajƒÖ linii zamiast s≈Çupk√≥w i zawsze rosnƒÖ od lewej do prawej, ostatecznie osiƒÖgajƒÖc ca≈ÇkowitƒÖ liczbƒô obserwacji (dla czƒôsto≈õci skumulowanej) lub 1,0 (dla czƒôsto≈õci wzglƒôdnej skumulowanej).\nWykresy czƒôsto≈õci skumulowanej sƒÖ wykorzytywane do:\n\nWizualnego odnajdywania percentyli i kwartyli\nOkre≈õlania, jaka proporcja danych znajduje siƒô poni≈ºej lub powy≈ºej okre≈õlonej warto≈õci\nPor√≥wnywania rozk≈Çad√≥w r√≥≈ºnych grup\n\n\n\n\n\n\n\nTip\n\n\n\nW statystyce percentyl (percentile) wskazuje wzglƒôdnƒÖ pozycjƒô punktu danych w zbiorze, pokazujƒÖc procent obserwacji, kt√≥re znajdujƒÖ siƒô na tym poziomie lub poni≈ºej. Na przyk≈Çad, je≈õli student uzyska≈Ç wynik na 90. percentylu w te≈õcie, jego wynik jest r√≥wny lub wy≈ºszy ni≈º 90% wszystkich innych wynik√≥w.\nKwartyle (quartiles) to specjalne percentyle, kt√≥re dzielƒÖ dane na cztery r√≥wne czƒô≈õci: pierwszy kwartyl (Q1, 25. percentyl), drugi kwartyl (Q2, 50. percentyl, czyli mediana), i trzeci kwartyl (Q3, 75. percentyl). Je≈õli Q1 = 65 punkt√≥w, oznacza to, ≈ºe 25% student√≥w uzyska≈Ço 65 punkt√≥w lub mniej.\nBardziej og√≥lnie, kwantyle (quantiles) to warto≈õci, kt√≥re dzielƒÖ dane na grupy o r√≥wnej liczebno≈õci ‚Äî percentyle dzielƒÖ na 100 czƒô≈õci, kwartyle na 4 czƒô≈õci, decyle (deciles) na 10 czƒô≈õci, itp.\n\n\n\n# Tworzenie danych czƒôsto≈õci skumulowanej\nscore_breaks &lt;- seq(0, 100, by = 10)\nfreq_counts &lt;- hist(exam_scores, breaks = score_breaks, plot = FALSE)$counts\ncumulative_freq &lt;- cumsum(freq_counts)\n\n# Wykres czƒôsto≈õci skumulowanej\nplot(score_breaks[-1], cumulative_freq,\n     type = \"b\",  # zar√≥wno punkty, jak i linie\n     main = \"Czƒôsto≈õƒá skumulowana wynik√≥w egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Czƒôsto≈õƒá skumulowana\",\n     col = \"darkblue\",\n     lwd = 2,\n     pch = 19)\ngrid()\n\n\n\n\n\n\n\n\nDla czƒôsto≈õci wzglƒôdnej skumulowanej (kt√≥ra jest czƒô≈õciej u≈ºywana):\n\n# Czƒôsto≈õƒá wzglƒôdna skumulowana\ncumulative_rel_freq &lt;- cumulative_freq / length(exam_scores)\n\nplot(score_breaks[-1], cumulative_rel_freq,\n     type = \"b\",\n     main = \"Czƒôsto≈õƒá wzglƒôdna skumulowana wynik√≥w egzaminacyjnych\",\n     xlab = \"Wynik\",\n     ylab = \"Czƒôsto≈õƒá wzglƒôdna skumulowana\",\n     col = \"darkred\",\n     lwd = 2,\n     pch = 19,\n     ylim = c(0, 1))\ngrid()\nabline(h = c(0.25, 0.5, 0.75), lty = 2, col = \"gray\")  # Linie kwartyli\n\n\n\n\n\n\n\n\nKrzywa czƒôsto≈õci wzglƒôdnej skumulowanej u≈Çatwia odczytywanie percentyli. Na przyk≈Çad, je≈õli narysujesz liniƒô poziomƒÖ na 0,75 i zobaczysz, gdzie przecina krzywƒÖ, odpowiadajƒÖca warto≈õƒá x to 75. percentyl ‚Äî wynik, poni≈ºej kt√≥rego znajduje siƒô 75% student√≥w.\n\n\nRozk≈Çady Dyskretne a CiƒÖg≈Çe\nTyp zmiennej, kt√≥rƒÖ analizujesz, okre≈õla spos√≥b wizualizacji jej rozk≈Çadu:\nRozk≈Çady dyskretne (discrete distributions) stosujƒÖ siƒô do zmiennych, kt√≥re mogƒÖ przyjmowaƒá tylko okre≈õlone, policzalne warto≈õci. Przyk≈Çady obejmujƒÖ liczbƒô dzieci w rodzinie (0, 1, 2, 3‚Ä¶), liczbƒô skarg klient√≥w dziennie lub odpowiedzi na 5-stopniowej skali Likerta.\nDla danych dyskretnych zazwyczaj u≈ºywamy:\n\nWykres√≥w s≈Çupkowych (z przerwami miƒôdzy s≈Çupkami) zamiast histogram√≥w\nCzƒôsto≈õci lub czƒôsto≈õci wzglƒôdnej na osi y\nKa≈ºda odrƒôbna warto≈õƒá otrzymuje w≈Çasny s≈Çupek\n\n\n# Przyk≈Çad: Liczba dzieci w rodzinie\nchildren &lt;- c(0, 1, 2, 2, 1, 3, 0, 2, 1, 4, 2, 1, 0, 2, 3)\nbarplot(table(children),\n        main = \"Rozk≈Çad liczby dzieci\",\n        xlab = \"Liczba dzieci\",\n        ylab = \"Czƒôsto≈õƒá\",\n        col = \"skyblue\")\n\n\n\n\n\n\n\n\nRozk≈Çady ciƒÖg≈Çe (continuous distributions) stosujƒÖ siƒô do zmiennych, kt√≥re mogƒÖ przyjmowaƒá dowolnƒÖ warto≈õƒá w zakresie. Przyk≈Çady obejmujƒÖ temperaturƒô, czas reakcji, wzrost lub procent frekwencji.\nDla danych ciƒÖg≈Çych u≈ºywamy:\n\nHistogram√≥w (ze stykajƒÖcymi siƒô s≈Çupkami), kt√≥re grupujƒÖ dane w przedzia≈Çy\nKrzywych gƒôsto≈õci, aby pokazaƒá wyg≈Çadzony wzorzec\nGƒôsto≈õci na osi y przy u≈ºywaniu krzywych gƒôsto≈õci\n\n\n# Przyk≈Çad: Rozk≈Çad czasu reakcji\nhist(response_time, \n     breaks = 15,\n     freq = FALSE,\n     main = \"Rozk≈Çad czasu reakcji\",\n     xlab = \"Czas reakcji (sekundy)\",\n     ylab = \"Gƒôsto≈õƒá\",\n     col = \"lightgreen\",\n     border = \"white\")\nlines(density(response_time), \n      col = \"darkgreen\", \n      lwd = 2)\n\n\n\n\n\n\n\n\nKluczowa r√≥≈ºnica polega na tym, ≈ºe rozk≈Çady dyskretne pokazujƒÖ prawdopodobie≈Ñstwo w konkretnych punktach, podczas gdy rozk≈Çady ciƒÖg≈Çe pokazujƒÖ gƒôsto≈õƒá prawdopodobie≈Ñstwa w zakresach. Dla zmiennych ciƒÖg≈Çych prawdopodobie≈Ñstwo jakiejkolwiek dok≈Çadnej warto≈õci jest w zasadzie r√≥wne zeru ‚Äî zamiast tego m√≥wimy o prawdopodobie≈Ñstwie znalezienia siƒô w przedziale.\nZrozumienie, czy twoja zmienna jest dyskretna czy ciƒÖg≈Ça, kieruje wyborem wizualizacji i metod statystycznych, zapewniajƒÖc, ≈ºe twoja analiza dok≈Çadnie reprezentuje naturƒô twoich danych.\n\n\nOpisywanie rozk≈Çad√≥w\nCharakterystyki kszta≈Çtu:\nSymetria vs.¬†Sko≈õno≈õƒá:\n\nSymetryczny: Lustrzane odbicie wok√≥≈Ç ≈õrodka (przyk≈Çad: wzrost w jednorodnej populacji)\nPrawostronnie sko≈õny (sko≈õno≈õƒá dodatnia): D≈Çugi ogon po prawej stronie (przyk≈Çad: doch√≥d, bogactwo)\nLewostronnie sko≈õny (sko≈õno≈õƒá ujemna): D≈Çugi ogon po lewej stronie (przyk≈Çad: liczba lat ≈ºycia w krajach rozwiniƒôtych)\n\nPrzyk≈Çad wp≈Çywu sko≈õno≈õci:\nRozk≈Çad dochodu w USA:\n\nMediana dochodu gospodarstwa domowego: ~70 000 USD\n≈öredni doch√≥d gospodarstwa domowego: ~100 000 USD\n≈örednia &gt; Mediana wskazuje na sko≈õno≈õƒá prawostronnƒÖ\nKilka bardzo wysokich dochod√≥w podnosi ≈õredniƒÖ\n\n\nModalno≈õƒá:\n\nJednomodalny: Jeden szczyt (przyk≈Çad: wyniki test√≥w)\nDwumodalny: Dwa szczyty (przyk≈Çad: wzrost przy mieszaniu mƒô≈ºczyzn i kobiet)\nWielomodalny: Wiele szczyt√≥w (przyk≈Çad: rozk≈Çad wieku w mie≈õcie uniwersyteckim ‚Äî szczyty w wieku studenckim i ≈õrednim wieku)\n\n\n\n\n\n\n\n\n\n\nWa≈ºne rozk≈Çady prawdopodobie≈Ñstwa:\nRozk≈Çad normalny (Gaussa):\n\nKszta≈Çt dzwonu, symetryczny\nCharakteryzowany przez ≈õredniƒÖ (\\mu) i odchylenie standardowe (\\sigma)\nOko≈Ço 68% warto≈õci w granicach \\mu \\pm \\sigma\nOko≈Ço 95% w granicach \\mu \\pm 2\\sigma\nOko≈Ço 99,7% w granicach \\mu \\pm 3\\sigma\n\nZastosowania demograficzne:\n\nWzrost w jednorodnych populacjach\nB≈Çƒôdy pomiarowe\nRozk≈Çady pr√≥bkowania ≈õrednich (Centralne Twierdzenie Graniczne)\n\nRozk≈Çad dwumianowy:\n\nLiczba sukces√≥w w n niezale≈ºnych pr√≥bach\nKa≈ºda pr√≥ba ma prawdopodobie≈Ñstwo p sukcesu\n≈örednia = np, Wariancja = np(1-p)\n\nPrzyk≈Çad: Liczba urodze≈Ñ ch≈Çopc√≥w na 100 urodze≈Ñ (p \\approx 0,512)\nRozk≈Çad Poissona:\n\nLiczba zdarze≈Ñ w sta≈Çym czasie/przestrzeni\n≈örednia = Wariancja = \\lambda\nDobry dla rzadkich zdarze≈Ñ\n\nZastosowania demograficzne:\n\nLiczba zgon√≥w dziennie w ma≈Çym mie≈õcie\nLiczba urodze≈Ñ na godzinƒô w szpitalu\nLiczba wypadk√≥w na skrzy≈ºowaniu miesiƒôcznie\n\n\n\n\nWizualizacja rozk≈Çad√≥w czƒôsto≈õci\nHistogram: Dla danych ciƒÖg≈Çych, pokazuje czƒôsto≈õƒá wysoko≈õciami s≈Çupk√≥w.\n\nO≈õ X: Zakresy warto≈õci (przedzia≈Çy)\nO≈õ Y: Czƒôsto≈õƒá lub gƒôsto≈õƒá\nBrak przerw miƒôdzy s≈Çupkami (dane ciƒÖg≈Çe)\nSzeroko≈õƒá przedzia≈Çu wp≈Çywa na wyglƒÖd\n\nWykres s≈Çupkowy: Dla danych kategorycznych, pokazuje czƒôsto≈õƒá z oddzielonymi s≈Çupkami.\n\nO≈õ X: Kategorie\nO≈õ Y: Czƒôsto≈õƒá\nPrzerwy miƒôdzy s≈Çupkami (dyskretne kategorie)\nKolejno≈õƒá mo≈ºe mieƒá znaczenie lub nie\n\nDystrybuanta (Funkcja Rozk≈Çadu Skumulowanego): Pokazuje proporcjƒô warto≈õci ‚â§ ka≈ºdego punktu danych. - Zawsze ro≈õnie (lub pozostaje p≈Çaska) - Zaczyna siƒô od 0, ko≈Ñczy na 1 - Strome nachylenia wskazujƒÖ na czƒôste warto≈õci - P≈Çaskie obszary wskazujƒÖ na rzadkie warto≈õci\nWykres Pude≈Çkowy (Wykres Skrzynkowy): Wizualne podsumowanie, kt√≥re przedstawia kluczowe statystyki rozk≈Çadu przy u≈ºyciu piƒôciu kluczowych warto≈õci.\nPodsumowanie Piƒôciu Liczb:\n\nMinimum: Koniec lewego wƒÖsa (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych)\nQ1 (Pierwszy Kwartyl): Lewa krawƒôd≈∫ pude≈Çka (25. percentyl)\nMediana (Q2): Linia wewnƒÖtrz pude≈Çka (50. percentyl)\n\nQ3 (Trzeci Kwartyl): Prawa krawƒôd≈∫ pude≈Çka (75. percentyl)\nMaksimum: Koniec prawego wƒÖsa (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych)\n\nCo Pokazuje:\n\nSko≈õno≈õƒá: Je≈õli linia mediany jest przesuniƒôta w pude≈Çku lub wƒÖsy sƒÖ nier√≥wne\nRozrzut: Szersze pude≈Çka i d≈Çu≈ºsze wƒÖsy wskazujƒÖ na wiƒôkszƒÖ zmienno≈õƒá\nWarto≈õci odstajƒÖce: Natychmiast widoczne jako oddzielne punkty\nSymetria: R√≥wne d≈Çugo≈õci wƒÖs√≥w i wy≈õrodkowana mediana sugerujƒÖ rozk≈Çad normalny\n\nSzybka Interpretacja:\n\nWƒÖskie pude≈Çko = sp√≥jne dane\nD≈Çugie wƒÖsy = szeroki zakres warto≈õci\n\nWiele warto≈õci odstajƒÖcych = potencjalne problemy z jako≈õciƒÖ danych lub interesujƒÖce przypadki skrajne\nMediana bli≈ºej Q1 = dane sko≈õne prawostronnie (ogon rozciƒÖga siƒô w prawo)\nMediana bli≈ºej Q3 = dane sko≈õne lewostronnie (ogon rozciƒÖga siƒô w lewo)\n\nWykresy pude≈Çkowe sƒÖ szczeg√≥lnie u≈ºyteczne do por√≥wnywania wielu grup obok siebie!",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#populacje-i-pr√≥by",
    "href": "rozdzial1.html#populacje-i-pr√≥by",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.3 Populacje i pr√≥by",
    "text": "2.3 Populacje i pr√≥by\nZrozumienie rozr√≥≈ºnienia miƒôdzy populacjami a pr√≥bami jest kluczowe dla w≈Ça≈õciwej analizy statystycznej. To rozr√≥≈ºnienie wp≈Çywa na ka≈ºdy aspekt naszej analizy, od zbierania danych po interpretacjƒô wynik√≥w.\n\nPopulacja (Population)\nPopulacja to kompletny zbi√≥r jednostek, obiekt√≥w lub pomiar√≥w, o kt√≥rych chcemy wyciƒÖgnƒÖƒá wnioski. Kluczowe s≈Çowo to ‚Äûkompletny‚Äù ‚Äî populacja obejmuje ka≈ºdego pojedynczego cz≈Çonka grupy, kt√≥rƒÖ badamy.\nPrzyk≈Çady populacji w demografii:\n\nWszyscy mieszka≈Ñcy Polski na dzie≈Ñ 1 stycznia 2024: Obejmuje ka≈ºdƒÖ osobƒô mieszkajƒÖcƒÖ w Polsce w tym konkretnym dniu ‚Äî oko≈Ço 38 milion√≥w os√≥b.\nWszystkie urodzenia w Szwecji w 2023 roku: Ka≈ºde dziecko urodzone w granicach Szwecji w tym roku kalendarzowym ‚Äî oko≈Ço 100 000 urodze≈Ñ.\nWszystkie gospodarstwa domowe w Tokio: Ka≈ºda jednostka mieszkalna, gdzie ludzie mieszkajƒÖ, gotujƒÖ i ≈õpiƒÖ ‚Äî oko≈Ço 7 milion√≥w gospodarstw.\nWszystkie zgony z powodu COVID-19 na ≈õwiecie w 2020 roku: Ka≈ºdy zgon, gdzie COVID-19 zosta≈Ç wymieniony jako przyczyna ‚Äî kilka milion√≥w zgon√≥w.\n\nPopulacje mogƒÖ byƒá:\nSko≈Ñczone (Finite): MajƒÖce policzalnƒÖ liczbƒô cz≈Çonk√≥w (wszyscy obecni obywatele Polski, wszytkie gminy w Polsce w 2024 r.)\nNiesko≈Ñczone (Infinite): Teoretyczne lub niepoliczalnie du≈ºe (wszystkie mo≈ºliwe przysz≈Çe urodzenia)\nSta≈Çe (Fixed): Zdefiniowane w okre≈õlonym punkcie czasu (wszyscy mieszka≈Ñcy w dniu spisu)\nDynamiczne (Dynamic): ZmieniajƒÖce siƒô w czasie (populacja miasta do≈õwiadczajƒÖca urodze≈Ñ, zgon√≥w i migracji codziennie)\n\n\nPr√≥ba (Sample)\nPr√≥ba to podzbi√≥r populacji, kt√≥ry jest faktycznie obserwowany lub mierzony. Badamy pr√≥by, poniewa≈º badanie ca≈Çych populacji jest czƒôsto niemo≈ºliwe, niepraktyczne lub niepotrzebne.\nDlaczego u≈ºywamy pr√≥b:\nPraktyczna niemo≈ºliwo≈õƒá: Wyobra≈∫ sobie testowanie ka≈ºdej osoby w Chinach na obecno≈õƒá pewnej choroby. Zanim sko≈Ñczy≈Çby≈õ testowaƒá 1,4 miliarda ludzi, sytuacja chorobowa ca≈Çkowicie by siƒô zmieni≈Ça, a niekt√≥rzy ludzie testowani wcze≈õnie wymagaliby ponownego testowania.\nWzglƒôdy kosztowe: Ameryka≈Ñski spis powszechny z 2020 roku kosztowa≈Ç oko≈Ço 16 miliard√≥w dolar√≥w. Przeprowadzanie tak kompletnych wylicze≈Ñ czƒôsto by≈Çoby zbyt kosztowne.\nOgraniczenia czasowe: Decydenci czƒôsto potrzebujƒÖ informacji szybko. Badanie ankietowe 10 000 os√≥b mo≈ºna uko≈Ñczyƒá w ciƒÖgu tygodni, podczas gdy spis wymaga lat planowania, wykonania i przetwarzania.\nPomiar destrukcyjny: Niekt√≥re pomiary niszczƒÖ to, co jest mierzone. Testowanie ≈ºywotno≈õci ≈ºar√≥wek wymaga u≈ºycia pr√≥b.\nWiƒôksza dok≈Çadno≈õƒá: Co zaskakujƒÖce, pr√≥by mogƒÖ czasem byƒá dok≈Çadniejsze ni≈º badania pe≈Çne. Z pr√≥bƒÖ mo≈ºna pozwoliƒá sobie na lepsze szkolenie ankieter√≥w, bardziej staranne zbieranie danych i dok≈Çadniejsze kontrole jako≈õci.\nPrzyk≈Çad pr√≥by vs.¬†populacja:\nPowiedzmy, ≈ºe chcemy poznaƒá ≈õredniƒÖ wielko≈õƒá gospodarstwa domowego w Warszawie:\n\nPopulacja: Wszystkie 800 000 gospodarstw domowych w Warszawie\nPodej≈õcie spisowe: Pr√≥ba skontaktowania siƒô z ka≈ºdym gospodarstwem (drogie, czasoch≈Çonne, niekt√≥re zostanƒÖ pominiƒôte)\nPodej≈õcie pr√≥bkowe: Losowo wybraƒá 5000 gospodarstw, dok≈Çadnie zmierzyƒá ich wielko≈õci i u≈ºyƒá tego do oszacowania ≈õredniej dla wszystkich gospodarstw\nWynik: Pr√≥ba mo≈ºe znale≈∫ƒá ≈õredniƒÖ 2,43 os√≥b na gospodarstwo z marginesem b≈Çƒôdu ¬±0,05, co oznacza, ≈ºe jeste≈õmy pewni, ≈ºe prawdziwa ≈õrednia populacji mie≈õci siƒô miƒôdzy 2,38 a 2,48\n\n\n\n\n\n\n\n\nSuperpopulacja i Proces Generowania Danych (DGP) (*)\n\n\n\n\n\nSuperpopulacja (Superpopulation)\nSuperpopulacja to teoretyczna niesko≈Ñczona populacja, z kt√≥rej twoja sko≈Ñczona populacja jest traktowana jako jedna losowa pr√≥ba.\nPomy≈õl o tym w trzech poziomach:\n\nSuperpopulacja: Niesko≈Ñczony zbi√≥r mo≈ºliwych warto≈õci (teoretyczny)\nPopulacja sko≈Ñczona (finite population): Rzeczywista populacja, kt√≥rƒÖ teoretycznie mo≈ºesz spisaƒá (np. wszystkie 50 stan√≥w USA, wszystkie 10 000 firm w bran≈ºy)\nPr√≥ba (sample): Podzbi√≥r, kt√≥ry faktycznie obserwujesz (np. 30 stan√≥w, 500 firm)\n\nDlaczego potrzebujemy tego pojƒôcia?\nRozwa≈ºmy 50 stan√≥w USA. Mo≈ºesz zmierzyƒá stopƒô bezrobocia dla wszystkich 50 stan√≥w ‚Äî pe≈Çny spis, bez pr√≥bkowania. Ale nadal chcesz:\n\nSprawdziƒá, czy bezrobocie jest powiƒÖzane z poziomem wykszta≈Çcenia\nPrzewidzieƒá przysz≈Çoroczne stopy bezrobocia\nOkre≈õliƒá, czy r√≥≈ºnice miƒôdzy stanami sƒÖ ‚Äûistotne statystycznie‚Äù\n\nBez koncepcji superpopulacji utkniesz ‚Äî masz wszystkie dane, wiƒôc co pozostaje do wnioskowania? Odpowied≈∫: traktuj tegoroczne 50 warto≈õci jako jedno losowanie z niesko≈Ñczonej superpopulacji mo≈ºliwych warto≈õci, kt√≥re mog≈Çyby wystƒÖpiƒá w podobnych warunkach.\nReprezentacja matematyczna:\n\nWarto≈õƒá populacji sko≈Ñczonej: Y_i (stopa bezrobocia stanu i)\nModel superpopulacji: Y_i = \\mu + \\epsilon_i gdzie \\epsilon_i \\sim (0, \\sigma^2)\n50 zaobserwowanych warto≈õci to jedna realizacja tego procesu\n\n\n\n\nProces Generowania Danych (Data Generating Process): Prawdziwa Recepta\nProces Generowania Danych (DGP) to rzeczywisty mechanizm, kt√≥ry tworzy twoje dane ‚Äî w≈ÇƒÖczajƒÖc wszystkie czynniki, relacje i elementy losowe.\nIntuicyjny przyk≈Çad: Za≈Ç√≥≈ºmy, ≈ºe wyniki test√≥w uczni√≥w sƒÖ naprawdƒô generowane przez:\n\\text{Wynik}_i = 50 + 2(\\text{GodzinyNauki}_i) + 3(\\text{GodzinySnu}_i) - 5(\\text{Stres}_i) + 1.5(\\text{≈öniadanie}_i) + \\epsilon_i\nTo jest PRAWDZIWY DGP. Ale ty tego nie wiesz! Mo≈ºesz estymowaƒá:\n\\text{Wynik}_i = \\alpha + \\beta(\\text{GodzinyNauki}_i) + u_i\nTw√≥j model jest prostszy ni≈º rzeczywisto≈õƒá. Brakuje ci zmiennych (sen, stres, ≈õniadanie), wiƒôc twoje oszacowania mogƒÖ byƒá obciƒÖ≈ºone (biased). Sk≈Çadnik u_i zawiera wszystko, co pominƒÖ≈Çe≈õ.\nKluczowa intuicja: Nigdy nie znamy prawdziwego DGP. Nasze modele statystyczne sƒÖ zawsze przybli≈ºeniami, pr√≥bujƒÖcymi uchwyciƒá najwa≈ºniejsze czƒô≈õci nieznanej, z≈Ço≈ºonej prawdy.\n\n\n\nDwa Podej≈õcia do Wnioskowania Statystycznego\nAnalizujƒÖc dane, szczeg√≥lnie z bada≈Ñ czy pr√≥b, mo≈ºemy przyjƒÖƒá dwa filozoficzne podej≈õcia:\n\n1. Wnioskowanie Oparte na Schemacie (Design-Based Inference)\n\nFilozofia: Warto≈õci populacji sƒÖ sta≈Çymi liczbami. Losowo≈õƒá pochodzi TYLKO z tego, kt√≥re jednostki wylosowali≈õmy.\nSkupienie: Jak wybrali≈õmy pr√≥bƒô (losowanie proste, warstwowe, gniazdowe itp.)\nPrzyk≈Çad: ≈öredni doch√≥d hrabstw Kalifornii jest sta≈ÇƒÖ liczbƒÖ. Losujemy 10 hrabstw. Nasza niepewno≈õƒá wynika z tego, kt√≥re 10 losowo wybrali≈õmy.\nBez modeli: Nie zak≈Çadamy nic o rozk≈Çadzie warto≈õci populacji\n\n\n\n2. Wnioskowanie Oparte na Modelu (Model-Based Inference)\n\nFilozofia: Same warto≈õci populacji sƒÖ realizacjami z pewnego modelu probabilistycznego (superpopulacji)\nSkupienie: Model statystyczny generujƒÖcy warto≈õci populacji\nPrzyk≈Çad: Doch√≥d ka≈ºdego hrabstwa Kalifornii jest losowany z: Y_i = \\mu + \\epsilon_i gdzie \\epsilon_i \\sim N(0, \\sigma^2)\nWymagane modele: Przyjmujemy za≈Ço≈ºenia o tym, jak dane zosta≈Çy wygenerowane\n\nKt√≥re jest lepsze?\n\nDu≈ºe populacje, dobre pr√≥by losowe: Podej≈õcie oparte na schemacie dzia≈Ça dobrze\nMa≈Çe populacje (jak 50 stan√≥w): Czƒôsto konieczne podej≈õcie modelowe\nPe≈Çne spisanie: Tylko podej≈õcie modelowe umo≈ºliwia wnioskowanie\nWsp√≥≈Çczesna praktyka: Czƒôsto ≈ÇƒÖczy oba podej≈õcia\n\n\n\n\n\nPraktyczny Przyk≈Çad: Analiza Wydatk√≥w Stanowych na Edukacjƒô\nZa≈Ç√≥≈ºmy, ≈ºe zbierasz wydatki na edukacjƒô per ucze≈Ñ dla wszystkich 50 stan√≥w USA.\nBez my≈õlenia superpopulacyjnego:\n\nMasz wszystkie 50 warto≈õci ‚Äî to wszystko\n≈örednia to ≈õrednia, bez niepewno≈õci\nNie mo≈ºesz testowaƒá hipotez ani tworzyƒá prognoz\n\nZ my≈õleniem superpopulacyjnym:\n\nTegoroczne 50 warto≈õci to jedna realizacja z superpopulacji\nModel: \\text{Wydatki}_i = \\mu + \\beta(\\text{Doch√≥dStanu}_i) + \\epsilon_i\nTeraz mo≈ºesz:\n\nTestowaƒá, czy wydatki sƒÖ powiƒÖzane z dochodem stanu (\\beta \\neq 0?)\nPrzewidywaƒá przysz≈Çoroczne warto≈õci\nObliczaƒá przedzia≈Çy ufno≈õci\n\n\nKluczowa intuicja: Nawet z kompletnymi danymi, ramy superpopulacji umo≈ºliwiajƒÖ wnioskowanie statystyczne poprzez traktowanie obserwowanych warto≈õci jako jednego mo≈ºliwego wyniku z podstawowego procesu stochastycznego.\n\n\n\nPodsumowanie\n\nSuperpopulacja: Traktuje twojƒÖ populacjƒô sko≈ÑczonƒÖ jako jedno losowanie z niesko≈Ñczonej przestrzeni mo≈ºliwo≈õci ‚Äî niezbƒôdne, gdy twoja populacja sko≈Ñczona jest ma≈Ça lub ca≈Çkowicie obserwowana\nDGP: Prawdziwy (nieznany) proces tworzƒÖcy twoje dane ‚Äî twoje modele pr√≥bujƒÖ go przybli≈ºyƒá",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zmienne-i-skale-pomiarowe",
    "href": "rozdzial1.html#zmienne-i-skale-pomiarowe",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.4 Zmienne i skale pomiarowe",
    "text": "2.4 Zmienne i skale pomiarowe\n\nZmienna to ka≈ºda charakterystyka, kt√≥ra mo≈ºe przyjmowaƒá r√≥≈ºne warto≈õci dla r√≥≈ºnych jednostek obserwacji.\n\n\nPomiar: przekszta≈Çcanie pojƒôƒá w liczby\n\n≈öwiat polityki jest pe≈Çen danych\nPolitologia ewoluowa≈Ça z dyscypliny g≈Ç√≥wnie teoretycznej do takiej, kt√≥ra coraz bardziej opiera siƒô na dowodach empirycznych. Niezale≈ºnie od tego, czy badamy:\n\nWyniki wybor√≥w: Dlaczego ludzie g≈ÇosujƒÖ tak, jak g≈ÇosujƒÖ?\nOpiniƒô publicznƒÖ: Co kszta≈Çtuje postawy wobec imigracji lub polityki klimatycznej?\nStosunki miƒôdzynarodowe: Jakie czynniki przewidujƒÖ konflikt miƒôdzy narodami/pa≈Ñstwami?\nSkuteczno≈õƒá polityk: Czy nowa polityka edukacyjna rzeczywi≈õcie poprawi≈Ça wyniki uczni√≥w?\n\nPotrzebujemy systematycznych sposob√≥w analizowania danych i wyciƒÖgania wniosk√≥w, kt√≥re wykraczajƒÖ poza anegdoty i osobiste wra≈ºenia.\n\nRozwa≈º to pytanie: ‚ÄúCzy demokracja prowadzi do wzrostu gospodarczego?‚Äù\n\nTwoja intuicja mo≈ºe sugerowaƒá, ≈ºe tak - kraje demokratyczne sƒÖ zazwyczaj bogatsze. Ale czy to przyczynowo≈õƒá, czy korelacja? Czy sƒÖ wyjƒÖtki? Jak pewni mo≈ºemy byƒá naszych wniosk√≥w?\nStatystyka dostarcza narzƒôdzi do przej≈õcia od przeczuƒá do odpowiedzi opartych na dowodach, pomagajƒÖc nam rozr√≥≈ºniƒá miƒôdzy tym, co wydaje siƒô prawdziwe, a tym, co rzeczywi≈õcie jest prawdziwe.\n\n\nPomiar w naukach spo≈Çecznych\nW naukach spo≈Çecznych czƒôsto zmagamy siƒô z tym, ≈ºe kluczowe pojƒôcia nie przek≈ÇadajƒÖ siƒô wprost na liczby:\n\nJak zmierzyƒá ‚Äûdemokracjƒô‚Äù?\nJaka liczba oddaje ‚Äûideologiƒô politycznƒÖ‚Äù?\nJak ilo≈õciowo ujƒÖƒá ‚Äûsi≈Çƒô instytucji‚Äù?\nJak zmierzyƒá ‚Äûpartycypacjƒô politycznƒÖ‚Äù?\n\n\n\n\n\n\n\n\nüîç Korelacja ‚â† Przyczynowo≈õƒá: Zrozumienie ZwiƒÖzk√≥w Pozornych (spurious correlation)\n\n\n\n\nFundamentalne Rozr√≥≈ºnienie\nKorelacja (correlation) mierzy, jak dwie zmienne poruszajƒÖ siƒô razem:\n\nDodatnia: Obie rosnƒÖ razem (godziny nauki ‚Üë, oceny ‚Üë)\nUjemna: Jedna ro≈õnie, gdy druga maleje (godziny TV ‚Üë, oceny ‚Üì)\nMierzona wsp√≥≈Çczynnikiem korelacji: r \\in [-1, 1]\n\nPrzyczynowo≈õƒá (causation) oznacza, ≈ºe jedna zmienna bezpo≈õrednio wp≈Çywa na drugƒÖ:\n\nX \\rightarrow Y: Zmiany w X bezpo≈õrednio powodujƒÖ zmiany w Y\nWymaga: (1) korelacji, (2) poprzedzania czasowego, (3) braku alternatywnych wyja≈õnie≈Ñ\n\n\n\nZagro≈ºenie: Korelacja Pozorna\nKorelacja pozorna (spurious correlation) wystƒôpuje, gdy dwie zmienne wydajƒÖ siƒô powiƒÖzane, ale w rzeczywisto≈õci obie sƒÖ pod wp≈Çywem trzeciej zmiennej (czynnika zak≈Ç√≥cajƒÖcego/confoundera).\nKlasyczny przyk≈Çad:\n\nObserwacja: Sprzeda≈º lod√≥w koreluje z liczbƒÖ utoniƒôƒá\nPozorny wniosek: Lody powodujƒÖ utoniƒôcia (‚ùå)\nRzeczywisto≈õƒá: Letnia pogoda (czynnik zak≈Ç√≥cajƒÖcy) powoduje oba zjawiska:\nLato ‚Üí Wiƒôcej sprzedanych lod√≥w\nLato ‚Üí Wiƒôcej p≈Çywania ‚Üí Wiƒôcej utoniƒôƒá\n\nReprezentacja matematyczna:\n\nObserwowana korelacja: \\text{Cor}(X,Y) \\neq 0\nAle prawdziwy model: X = \\alpha Z + \\epsilon_1 oraz Y = \\beta Z + \\epsilon_2\nGdzie Z to zmienna zak≈Ç√≥cajƒÖca powodujƒÖca oba zjawiska\n\n\n\nCzynniki Zak≈Ç√≥cajƒÖce (Confounding): Ukryty Wp≈Çyw\nZmienna zak≈Ç√≥cajƒÖca (confounding variable/confounder):\n\nWp≈Çywa zar√≥wno na domniemanƒÖ przyczynƒô, jak i skutek\nTworzy iluzjƒô bezpo≈õredniej przyczynowo≈õci\nMusi byƒá kontrolowana dla wa≈ºnego wnioskowania przyczynowego\n\nPrzyk≈Çad badawczy:\n\nObserwacja: Spo≈ºycie kawy koreluje z chorobami serca\nPotencjalny czynnik zak≈Ç√≥cajƒÖcy: Palenie (osoby pijƒÖce kawƒô czƒô≈õciej palƒÖ)\nPrawdziwe relacje:\nPalenie ‚Üí Choroby serca (przyczynowa)\nPalenie ‚Üí Spo≈ºycie kawy (zwiƒÖzek)\nKawa ‚Üí Choroby serca (pozorna bez kontroli palenia)\n\n\n\nJak Identyfikowaƒá ZwiƒÖzki Przyczynowe\n\nRandomizowane badania kontrolowane (RCTs): Losowy przydzia≈Ç przerywa wp≈Çyw czynnik√≥w zak≈Ç√≥cajƒÖcych\nEksperymenty naturalne (natural experiments): Zdarzenia zewnƒôtrzne tworzƒÖ ‚Äûjakby‚Äù losowƒÖ zmienno≈õƒá\nKontrola statystyczna: W≈ÇƒÖczenie czynnik√≥w zak≈Ç√≥cajƒÖcych do modeli regresji\nZmienne instrumentalne (instrumental variables): Znalezienie zmiennych wp≈ÇywajƒÖcych na X, ale nie bezpo≈õrednio na Y\n\n\n\nKluczowy Wniosek\nZnalezienie korelacji jest ≈Çatwe. Ustalenie przyczynowo≈õci jest trudne. Zawsze pytaj: ‚ÄûCo jeszcze mog≈Çoby wyja≈õniaƒá ten zwiƒÖzek?‚Äù\nPamiƒôtaj: Najbardziej niebezpieczne zdanie w badaniach empirycznych to ‚Äûnasze dane pokazujƒÖ, ≈ºe X powoduje Y‚Äù, gdy tak naprawdƒô zmierzy≈Çe≈õ tylko korelacjƒô.\n\n\n\n\n\n\n\n\n\n\nüìä Szybki Test: Korelacja czy Przyczynowo≈õƒá?\n\n\n\n\n\nDla ka≈ºdego scenariusza okre≈õl, czy zwiƒÖzek jest prawdopodobnie przyczynowy czy pozorny:\n\nMiasta z wiƒôkszƒÖ liczbƒÖ ko≈õcio≈Ç√≥w majƒÖ wiƒôcej przestƒôpstw\n\nOdpowied≈∫: Pozorny (czynnik zak≈Ç√≥cajƒÖcy: wielko≈õƒá populacji)\n\nPalenie prowadzi do raka p≈Çuc\n\nOdpowied≈∫: Przyczynowy (ustalony poprzez wiele projekt√≥w badawczych)\n\nUczniowie z wiƒôkszƒÖ liczbƒÖ ksiƒÖ≈ºek w domu majƒÖ lepsze oceny\n\nOdpowied≈∫: Prawdopodobnie pozorny (czynniki zak≈Ç√≥cajƒÖce: wykszta≈Çcenie rodzic√≥w, doch√≥d)\n\nKraje z wy≈ºszym spo≈ºyciem czekolady majƒÖ wiƒôcej laureat√≥w Nobla\n\nOdpowied≈∫: Pozorny (czynnik zak≈Ç√≥cajƒÖcy: poziom zamo≈ºno≈õci/rozwoju)\n\n\n\n\n\n\n\n\n\nTypy zmiennych\nZmienne ilo≈õciowe (Quantitative Variables) reprezentujƒÖ ilo≈õci lub wielko≈õci i mogƒÖ byƒá:\nZmienne ciƒÖg≈Çe (Continuous Variables): MogƒÖ przyjmowaƒá dowolnƒÖ warto≈õƒá w przedziale, ograniczonƒÖ tylko precyzjƒÖ pomiaru.\n\nWiek (22,5 lat, 22,51 lat, 22,514 lat‚Ä¶)\nDoch√≥d (45 234,67 z≈Ç)\nWzrost (175,3 cm)\nGƒôsto≈õƒá zaludnienia (432,7 os√≥b na kilometr kwadratowy)\n\nZmienne dyskretne (Discrete Variables): MogƒÖ przyjmowaƒá tylko okre≈õlone warto≈õci, zazwyczaj liczenia.\n\nLiczba dzieci w rodzinie (0, 1, 2, 3‚Ä¶)\nLiczba ma≈Ç≈ºe≈Ñstw (0, 1, 2‚Ä¶)\nLiczba pokoi w mieszkaniu (1, 2, 3‚Ä¶)\nLiczba migrant√≥w wje≈ºd≈ºajƒÖcych do kraju rocznie\n\nZmienne jako≈õciowe (Qualitative Variables) reprezentujƒÖ kategorie lub cechy i mogƒÖ byƒá:\nZmienne nominalne (Nominal Variables): Kategorie bez naturalnego porzƒÖdku.\n\nKraj urodzenia (Polska, Meksyk, Kanada‚Ä¶)\nReligia (Chrze≈õcija≈Ñstwo, Islam, Hinduizm, Buddyzm‚Ä¶)\nGrupa krwi (A, B, AB, 0)\nPrzyczyna ≈õmierci (choroby serca, nowotwory, wypadek‚Ä¶)\n\nZmienne porzƒÖdkowe (Ordinal Variables): Kategorie ze znaczƒÖcym porzƒÖdkiem, ale nier√≥wnymi interwa≈Çami.\n\nPoziom wykszta≈Çcenia (brak wykszta≈Çcenia, podstawowe, ≈õrednie, wy≈ºsze)\nZadowolenie z opieki zdrowotnej (bardzo niezadowolony, niezadowolony, neutralny, zadowolony, bardzo zadowolony)\nStatus spo≈Çeczno-ekonomiczny (niski, ≈õredni, wysoki)\nSamoocena stanu zdrowia (z≈Çy, przeciƒôtny, dobry, doskona≈Çy)\n\n\n\nSkale pomiarowe\nZrozumienie skal pomiarowych jest kluczowe, poniewa≈º determinujƒÖ, kt√≥re metody statystyczne sƒÖ odpowiednie:\nSkala nominalna (Nominal Scale): Tylko kategorie ‚Äî mo≈ºemy liczyƒá czƒôsto≈õci, ale nie mo≈ºemy porzƒÖdkowaƒá ani wykonywaƒá operacji arytmetycznych. Przyk≈Çad: Mo≈ºemy powiedzieƒá, ≈ºe 45% mieszka≈Ñc√≥w urodzi≈Ço siƒô lokalnie, ale nie mo≈ºemy obliczyƒá ‚Äû≈õredniego miejsca urodzenia‚Äù.\nSkala porzƒÖdkowa (Ordinal Scale): Kolejno≈õƒá ma znaczenie, ale r√≥≈ºnice miƒôdzy warto≈õciami niekoniecznie sƒÖ r√≥wne. Przyk≈Çad: R√≥≈ºnica miƒôdzy ‚Äûz≈Çym‚Äù a ‚Äûprzeciƒôtnym‚Äù zdrowiem mo≈ºe nie r√≥wnaƒá siƒô r√≥≈ºnicy miƒôdzy ‚Äûdobrym‚Äù a ‚Äûdoskona≈Çym‚Äù zdrowiem.\nSkala interwa≈Çowa (Interval Scale): R√≥wne interwa≈Çy miƒôdzy warto≈õciami, ale brak prawdziwego punktu zerowego. Przyk≈Çad: Temperatura w stopniach Celsjusza ‚Äî r√≥≈ºnica miƒôdzy 20¬∞C a 30¬∞C r√≥wna siƒô r√≥≈ºnicy miƒôdzy 30¬∞C a 40¬∞C, ale 0¬∞C nie oznacza ‚Äûbraku temperatury‚Äù.\nSkala ilorazowa (Ratio Scale): R√≥wne interwa≈Çy z prawdziwym punktem zerowym, umo≈ºliwiajƒÖce wszystkie operacje matematyczne. Przyk≈Çad: Doch√≥d ‚Äî 40 000 z≈Ç to dwa razy wiƒôcej ni≈º 20 000 z≈Ç, a 0 z≈Ç oznacza brak dochodu.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#parametry-statystyki-i-estymacja",
    "href": "rozdzial1.html#parametry-statystyki-i-estymacja",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.5 Parametry, statystyki i estymacja",
    "text": "2.5 Parametry, statystyki i estymacja\nTe pojƒôcia stanowiƒÖ rdze≈Ñ wnioskowania statystycznego ‚Äî jak uczymy siƒô o populacjach z pr√≥b. Zrozumienie relacji miƒôdzy tymi terminami jest niezbƒôdne dla w≈Ça≈õciwego rozumowania statystycznego.\n\nParametr\nParametr to liczbowa charakterystyka populacji. Parametry sƒÖ zazwyczaj nieznane, poniewa≈º nie mo≈ºemy zmierzyƒá ca≈Çej populacji. SƒÖ to warto≈õci sta≈Çe (nie losowe), ale nieznane nam. Oznaczamy parametry literami greckimi.\nPowszechne parametry demograficzne:\n\n\\mu (mi): ≈öredni wiek populacji. Na przyk≈Çad, prawdziwy ≈õredni wiek wszystkich Europejczyk√≥w.\n\\sigma^2 (sigma kwadrat): Wariancja populacji w dochodzie we wszystkich gospodarstwach domowych w Brazylii.\np: Proporcja populacji. Na przyk≈Çad, prawdziwa proporcja wszystkich doros≈Çych w Japonii, kt√≥rzy sƒÖ ma≈Ç≈ºonkami.\n\\beta (beta): Wsp√≥≈Çczynnik regresji. Prawdziwa relacja miƒôdzy wykszta≈Çceniem a p≈Çodno≈õciƒÖ w populacji.\n\\lambda (lambda): Parametr stopy. Prawdziwa stopa migracji z obszar√≥w wiejskich do miejskich.\n\nPrzyk≈Çad: Prawdziwy ≈õredni wiek przy pierwszym porodzie dla wszystkich kobiet we Francji, kt√≥re urodzi≈Çy dziecko w 2023 roku, jest parametrem. Nazwijmy go \\mu = 31,2 lat. Nie znamy tej warto≈õci bez zmierzenia ka≈ºdego pojedynczego porodu.\n\n\nStatystyka\nStatystyka to liczbowa charakterystyka obliczona z danych z pr√≥by. Statystyki sƒÖ zmiennymi losowymi ‚Äî ich warto≈õci r√≥≈ºniƒÖ siƒô od pr√≥by do pr√≥by. U≈ºywamy ≈Çaci≈Ñskich liter dla statystyk.\nPowszechne statystyki z pr√≥by:\n\n\\bar{x} (x z kreskƒÖ): ≈örednia wieku z pr√≥by z badania 1000 os√≥b\ns^2: Wariancja dochodu z pr√≥by z 500 ankietowanych gospodarstw\n\\hat{p} (p z daszkiem): Proporcja ma≈Ç≈ºonk√≥w z pr√≥by z badania\nr: Korelacja z pr√≥by miƒôdzy wykszta≈Çceniem a dochodem\nb: Wsp√≥≈Çczynnik regresji z pr√≥by\n\nPrzyk≈Çad: Z pr√≥by 500 urodze≈Ñ we Francji obliczamy ≈õredni wiek przy pierwszym porodzie z pr√≥by \\bar{x} = 30,9 lat. To jest nasza statystyka. Inna pr√≥ba mo≈ºe daƒá \\bar{x} = 31,4 lat.\n\n\nRelacja miƒôdzy parametrami a statystykami\nPomy≈õl o tej relacji jak o pr√≥bie zrozumienia g≈Çƒôboko≈õci jeziora:\n\nParametr: Prawdziwa ≈õrednia g≈Çƒôboko≈õƒá jeziora (nieznana, sta≈Ça)\nStatystyka: ≈örednia g≈Çƒôboko≈õƒá z kilku punkt√≥w pomiarowych (znana, zmienia siƒô z r√≥≈ºnymi pr√≥bami)\nEstymacja: U≈ºywanie naszych pomiar√≥w do zgadywania prawdziwej ≈õredniej g≈Çƒôboko≈õci\n\n\n\nEstymator\nEstymator to regu≈Ça lub formu≈Ça do obliczania oszacowania parametru populacji z danych z pr√≥by. Estymator to funkcja, kt√≥ra odwzorowuje dane z pr√≥by na oszacowania parametr√≥w.\nW≈Ça≈õciwo≈õci dobrych estymator√≥w:\nNieobciƒÖ≈ºono≈õƒá (Unbiasedness): ≈örednio estymator r√≥wna siƒô prawdziwej warto≈õci parametru. Gdyby≈õmy powt√≥rzyli pr√≥bkowanie wiele razy, ≈õrednia wszystkich naszych oszacowa≈Ñ r√≥wna≈Çaby siƒô prawdziwemu parametrowi.\nPrzyk≈Çad: ≈örednia z pr√≥by \\bar{x} jest nieobciƒÖ≈ºonym estymatorem ≈õredniej populacji \\mu. Gdyby≈õmy wziƒôli 1000 r√≥≈ºnych pr√≥b i obliczyli 1000 ≈õrednich z pr√≥b, ich ≈õrednia by≈Çaby bardzo bliska \\mu.\nZgodno≈õƒá (Consistency): Gdy wielko≈õƒá pr√≥by wzrasta, estymator zbiega siƒô do prawdziwej warto≈õci parametru.\nPrzyk≈Çad: Z n=10, nasze oszacowanie ≈õredniego dochodu mo≈ºe byƒá oddalone o 5000 z≈Ç. Z n=1000, mo≈ºemy byƒá oddaleni tylko o 500 z≈Ç. Z n=100 000, mo≈ºemy byƒá oddaleni tylko o 50 z≈Ç.\nEfektywno≈õƒá (Efficiency): W≈õr√≥d nieobciƒÖ≈ºonych estymator√≥w, ten z najmniejszƒÖ wariancjƒÖ. ≈örednia z pr√≥by jest bardziej efektywna ni≈º mediana z pr√≥by do estymowania ≈õredniej populacji rozk≈Çadu normalnego.\nPowszechne estymatory:\n\n≈örednia z pr√≥by jako estymator ≈õredniej populacji: \\bar{x} = \\frac{\\sum x_i}{n}\nProporcja z pr√≥by jako estymator proporcji populacji: \\hat{p} = \\frac{x}{n} (gdzie x to liczba sukces√≥w)\nWariancja z pr√≥by jako estymator wariancji populacji: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\n\nUwaga: Dzielimy przez (n-1), a nie n dla wariancji z pr√≥by, aby uczyniƒá jƒÖ nieobciƒÖ≈ºonƒÖ ‚Äî to siƒô nazywa korekta Bessela.\n\n\nEstimand\nEstimand to konkretny parametr populacji, kt√≥ry chcemy oszacowaƒá. To cel naszej procedury estymacji. Jasna specyfikacja estimandu jest kluczowa dla w≈Ça≈õciwego wnioskowania statystycznego i unikania b≈Çƒôdnej interpretacji.\nPrzyk≈Çady jasno zdefiniowanych estimand√≥w:\n\n‚ÄûMediana dochodu gospodarstwa domowego dla wszystkich gospodarstw w Kalifornii na dzie≈Ñ 1 stycznia 2024‚Äù\n‚ÄûR√≥≈ºnica w oczekiwanej d≈Çugo≈õci ≈ºycia miƒôdzy mƒô≈ºczyznami a kobietami urodzonymi w Szwecji w 2023 roku‚Äù\n‚ÄûProporcja wszystkich doros≈Çych w wieku 25-34 lat w obszarach miejskich, kt√≥rzy uko≈Ñczyli edukacjƒô wy≈ºszƒÖ‚Äù\n\nDlaczego precyzyjna definicja estimandu ma znaczenie:\nRozwa≈º badanie ‚Äûstopy bezrobocia‚Äù. Estimand musi okre≈õliƒá:\n\nKto liczy siƒô jako bezrobotny? (Aktywnie poszukujƒÖcy pracy? Zniechƒôceni pracownicy?)\nJaki zakres wieku? (15+? 16-64?)\nJaki obszar geograficzny?\nJaki okres czasu?\n\nR√≥≈ºne definicje prowadzƒÖ do r√≥≈ºnych liczb. Ameryka≈Ñskie Biuro Statystyki Pracy publikuje sze≈õƒá r√≥≈ºnych st√≥p bezrobocia (U-1 do U-6) na podstawie r√≥≈ºnych definicji.\n\n\nOszacowanie (Estimate)\nOszacowanie to konkretna warto≈õƒá numeryczna obliczona przez zastosowanie estymatora do obserwowanych danych. To nasze najlepsze przypuszczenie o prawdziwej warto≈õci parametru na podstawie dostƒôpnych informacji.\nPrzyk≈Çad kompletnego procesu:\n\nEstimand (cel): Proporcja wszystkich doros≈Çych Amerykan√≥w, kt√≥rzy popierajƒÖ dzia≈Çalno≈õƒá prezydenta\nParametr (prawdziwa nieznana warto≈õƒá): p = 0,42 (42%, ale tego nie wiemy)\nEstymator (metoda): Proporcja z pr√≥by \\hat{p} = \\frac{x}{n} gdzie x to liczba popierajƒÖcych, a n to wielko≈õƒá pr√≥by\nPr√≥ba: Ankietujemy 1500 losowo wybranych doros≈Çych, 650 popiera\nOszacowanie (obliczona warto≈õƒá): \\hat{p} = \\frac{650}{1500} = 0,433 (43,3%)\n\n\n\n\n\n\n\nEstimandy: Co dok≈Çadnie pr√≥bujemy oszacowaƒá?\n\n\n\nEstimand to konkretna wielko≈õƒá, kt√≥rƒÖ chcemy oszacowaƒá ‚Äî na co celujemy naszƒÖ analizƒÖ statystycznƒÖ. Choƒá czƒôsto jest to parametr populacji, estimandy mogƒÖ byƒá bardziej z≈Ço≈ºone.\nPrzyk≈Çady r√≥≈ºnych estimand√≥w:\nProsty estimand parametru: ≈öredni doch√≥d populacji (\\mu) Por√≥wnawczy estimand: R√≥≈ºnica w ≈õrednim dochodzie miƒôdzy dwiema grupami (\\mu_1 - \\mu_2) Przyczynowy estimand: ≈öredni efekt leczenia programu szkoleniowego na zarobki Warunkowy estimand: Oczekiwana frekwencja wyborcza przy konkretnych warunkach pogodowych\n\nKompletna struktura\nZrozumienie wnioskowania statystycznego wymaga rozr√≥≈ºnienia miƒôdzy tymi powiƒÖzanymi, ale odrƒôbnymi pojƒôciami:\n\nParametr populacji: Prawdziwa charakterystyka populacji (np. \\mu)\nEstimand: Konkretna wielko≈õƒá, kt√≥rƒÖ chcemy oszacowaƒá (czƒôsto, ale nie zawsze, parametr)\nEstymator: Metoda obliczania naszego oszacowania (np. ≈õrednia z pr√≥by)\nOszacowanie: Rzeczywista liczba, kt√≥rƒÖ obliczamy z naszych danych\n\nPrzyk≈Çad w kontek≈õcie:\n\nParametr: Prawdziwa ≈õrednia frekwencja wyborcza we wszystkich wyborach (\\mu)\nEstimand: Oczekiwana r√≥≈ºnica frekwencji miƒôdzy deszczowymi a s≈Çonecznymi dniami wybor√≥w (\\mu_{\\text{deszczowy}} - \\mu_{\\text{s≈Çoneczny}})\nEstymator: R√≥≈ºnica miƒôdzy ≈õrednimi z pr√≥b z deszczowych i s≈Çonecznych wybor√≥w\nOszacowanie: 3,2 punkty procentowe ni≈ºsza frekwencja w deszczowe dni\n\nTa struktura pomaga wyja≈õniƒá dok≈Çadnie, na jakie pytanie odpowiadamy i zapewnia, ≈ºe nasze metody sƒÖ zgodne z naszymi celami badawczymi.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#b≈ÇƒÖd-statystyczny-i-niepewno≈õƒá",
    "href": "rozdzial1.html#b≈ÇƒÖd-statystyczny-i-niepewno≈õƒá",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.6 B≈ÇƒÖd Statystyczny i Niepewno≈õƒá",
    "text": "2.6 B≈ÇƒÖd Statystyczny i Niepewno≈õƒá\n\nWprowadzenie\n≈ªaden pomiar ani oszacowanie nie jest doskona≈Çe. Zrozumienie r√≥≈ºnych typ√≥w b≈Çƒôd√≥w jest kluczowe dla interpretacji wynik√≥w i poprawy projektu badania.\n\n\n\n\n\n\nG≈Ç√≥wne Wyzwanie\n\n\n\nZa ka≈ºdym razem, gdy u≈ºywamy pr√≥by do poznania populacji, wprowadzamy niepewno≈õƒá. Kluczem jest:\n\nUczciwe kwantyfikowanie tej niepewno≈õci\nRozr√≥≈ºnianie miƒôdzy r√≥≈ºnymi ≈∫r√≥d≈Çami b≈Çƒôdu\nTransparentne komunikowanie wynik√≥w\n\n\n\n\n\n\nTypy B≈Çƒôd√≥w\n\nB≈ÇƒÖd Losowy (B≈ÇƒÖd Pr√≥bkowania)\nB≈ÇƒÖd losowy wynika z naturalnej zmienno≈õci w pr√≥bkowaniu ‚Äî nieuniknionej wariacji, kt√≥ra wystƒôpuje, poniewa≈º obserwujemy pr√≥bƒô, a nie ca≈ÇƒÖ populacjƒô.\n\n\n\n\n\n\nKluczowe Cechy\n\n\n\n\nNieprzewidywalny Kierunek: Czasem za wysoko, czasem za nisko\nMaleje z Wielko≈õciƒÖ Pr√≥by: \\propto 1/\\sqrt{n}\nKwantyfikowalny: Mo≈ºliwy do obliczenia przy u≈ºyciu teorii prawdopodobie≈Ñstwa\nU≈õrednia siƒô do Zera: W wielu pr√≥bach b≈Çƒôdy siƒô znoszƒÖ\n\n\n\n\nPrzyk≈Çad: Badanie Dostƒôpu do Internetu\nWyobra≈∫ sobie badanie 100 losowych gospodarstw domowych o dostƒôpie do internetu:\n\n\n\n\n\n\n\n\n\nZmienno≈õƒá wok√≥≈Ç prawdziwej warto≈õci (czerwona linia) reprezentuje b≈ÇƒÖd losowy. Przy wiƒôkszych pr√≥bach oszacowania by≈Çyby bardziej skupione.\n\n\n\nB≈ÇƒÖd Systematyczny (ObciƒÖ≈ºenie)\nB≈ÇƒÖd systematyczny reprezentuje sta≈Çe odchylenie w okre≈õlonym kierunku. W przeciwie≈Ñstwie do b≈Çƒôdu losowego, nie u≈õrednia siƒô przy powtarzanym pr√≥bkowaniu.\n\nObciƒÖ≈ºenie SelekcjiObciƒÖ≈ºenie PomiaruObciƒÖ≈ºenie OdpowiedziObciƒÖ≈ºenie Braku OdpowiedziObciƒÖ≈ºenie Przetrwania\n\n\nMetoda pr√≥bkowania systematycznie wyklucza pewne grupy.\nPrzyk≈Çad: Ankiety telefoniczne w godzinach pracy niedostatecznie reprezentujƒÖ osoby pracujƒÖce.\n\n\nInstrument pomiarowy konsekwentnie zawy≈ºa/zani≈ºa pomiary.\nPrzyk≈Çad: Waga, kt√≥ra zawsze pokazuje 1 kg za du≈ºo.\n\n\nRespondenci systematycznie b≈Çƒôdnie raportujƒÖ.\nPrzyk≈Çad: Ludzie zani≈ºajƒÖ spo≈ºycie alkoholu, zawy≈ºajƒÖ uczestnictwo w g≈Çosowaniu.\n\n\nOsoby nieudzielajƒÖce odpowiedzi r√≥≈ºniƒÖ siƒô systematycznie od respondent√≥w.\nPrzyk≈Çad: Bardzo chorzy i bardzo zdrowi ludzie rzadziej odpowiadajƒÖ na ankiety zdrowotne.\n\n\nObserwowanie tylko ‚Äúocala≈Çych‚Äù z jakiego≈õ procesu.\nPrzyk≈Çad: Badanie d≈Çugowieczno≈õci poprzez wywiady z 90-latkami pomija tych, kt√≥rzy zmarli m≈Çodsi.\n\n\n\n\n\nDekompozycja ObciƒÖ≈ºenia-Wariancji\nMatematycznie, ca≈Çkowity b≈ÇƒÖd (B≈ÇƒÖd ≈öredniokwadratowy) rozk≈Çada siƒô na:\n\\mathrm{MSE}(\\hat\\theta) = \\underbrace{\\mathrm{Var}(\\hat\\theta)}_{\\text{b≈ÇƒÖd losowy}} + \\underbrace{\\big(\\mathrm{Bias}(\\hat\\theta)\\big)^2}_{\\text{b≈ÇƒÖd systematyczny}}\n\n\n\n\n\n\nKluczowa Uwaga\n\n\n\nDu≈ºa obciƒÖ≈ºona pr√≥ba daje precyzyjnie b≈ÇƒôdnƒÖ odpowied≈∫.\n\nZwiƒôksz n ‚Üí redukuje b≈ÇƒÖd losowy\nPopraw projekt ‚Üí redukuje b≈ÇƒÖd systematyczny\n\n\n\n\n\n\nR√≥≈ºne kombinacje obciƒÖ≈ºenia i wariancji w estymacji\n\n\n\n\n\n\nKwantyfikowanie Niepewno≈õci\n\nB≈ÇƒÖd Standardowy\nB≈ÇƒÖd standardowy (SE) kwantyfikuje, jak bardzo oszacowanie r√≥≈ºni siƒô w r√≥≈ºnych mo≈ºliwych pr√≥bach.\n\n\nDla Proporcji: SE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\nDla ≈öredniej: SE(\\bar{x}) = \\frac{s}{\\sqrt{n}}\n\nDla R√≥≈ºnicy: SE(\\bar{x}_1 - \\bar{x}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\n\n\n\nMargines B≈Çƒôdu\nMargines b≈Çƒôdu (MOE) reprezentuje oczekiwanƒÖ maksymalnƒÖ r√≥≈ºnicƒô miƒôdzy oszacowaniem z pr√≥by a prawdziwym parametrem.\n\\text{MOE} = \\text{Warto≈õƒá Krytyczna} \\times \\text{B≈ÇƒÖd Standardowy}\n\n\n\n\n\n\nZrozumienie Warto≈õci Krytycznej\n\n\n\n\n\nDla 95% ufno≈õci u≈ºywamy 1,96 (czƒôsto upraszczane do 2). To zapewnia, ≈ºe ~95% przedzia≈Ç√≥w skonstruowanych w ten spos√≥b bƒôdzie zawieraƒá prawdziwy parametr.\n\n90% ufno≈õci: z = 1,645\n95% ufno≈õci: z = 1,96\n99% ufno≈õci: z = 2,576\n\n\n\n\n\n\nPrzedzia≈Çy Ufno≈õci\nPrzedzia≈Ç ufno≈õci dostarcza zakres prawdopodobnych warto≈õci:\n\\text{CI} = \\text{Oszacowanie} \\pm (\\text{Warto≈õƒá Krytyczna} \\times \\text{B≈ÇƒÖd Standardowy})\n\n\n\n\nPraktyczne Zastosowanie: Sonda≈ºe Opinii\n\n\n\n\n\n\nStudium Przypadku: Sonda≈ºe Polityczne\n\n\n\nGdy sonda≈º podaje ‚ÄúKandydat A: 52%, Kandydat B: 48%‚Äù, jest to niepe≈Çne bez kwantyfikacji niepewno≈õci.\n\n\n\nZ≈Çota Zasada Sonda≈ºy\nPrzy ~1000 losowo wybranych respondentach:\n\nMargines b≈Çƒôdu: ¬±3 punkty procentowe (95% ufno≈õci)\nInterpretacja: Zg≈Çoszone 52% oznacza prawdziwe poparcie prawdopodobnie miƒôdzy 49% a 55%\n\n\n\nWielko≈õƒá Pr√≥by i Precyzja\n\n\n\nWielko≈õƒá Pr√≥by\nMargines B≈Çƒôdu (95%)\nZastosowanie\n\n\n\n\nn = 100\n¬± 10%\nTylko og√≥lny kierunek\n\n\nn = 400\n¬± 5%\nOg√≥lne trendy\n\n\nn = 1000\n¬± 3%\nStandardowe sonda≈ºe\n\n\nn = 2500\n¬± 2%\nWysoka precyzja\n\n\nn = 10000\n¬± 1%\nBardzo wysoka precyzja\n\n\n\n\n\n\n\n\n\nPrawo MalejƒÖcych Przychod√≥w\n\n\n\nAby zmniejszyƒá margines b≈Çƒôdu o po≈Çowƒô, potrzebujesz cztery razy wiƒôkszej pr√≥by, poniewa≈º \\text{MOE} \\propto 1/\\sqrt{n}\n\n\n\n\nCo Powinny Raportowaƒá Sonda≈ºe\nWysokiej jako≈õci sonda≈ºe muszƒÖ ujawniaƒá:\n\nDaty przeprowadzenia badania\nDefinicjƒô i wielko≈õƒá pr√≥by\nSpos√≥b traktowania niezdecydowanych wyborc√≥w\nMargines b≈Çƒôdu pr√≥bkowania\nNiepewno≈õƒá w marginesach g≈Çosowania\n\n\n\n\n\nWizualizacja: Zmienno≈õƒá Pr√≥bkowania\nPoni≈ºsza symulacja pokazuje, jak zachowujƒÖ siƒô przedzia≈Çy ufno≈õci przy powtarzanym pr√≥bkowaniu:\n\n\nPoka≈º kod symulacji\nlibrary(ggplot2)\nset.seed(42)\n\n# Parametry\nn_polls      &lt;- 20\nn_people     &lt;- 100\ntrue_support &lt;- 0.50\n\n# Symulacja niezale≈ºnych sonda≈ºy\nsupport &lt;- rbinom(n_polls, n_people, true_support) / n_people\n\n# Obliczanie b≈Çƒôd√≥w standardowych i margines√≥w b≈Çƒôdu\nse   &lt;- sqrt(support * (1 - support) / n_people)\nmoe  &lt;- 2 * se  # Uproszczony mno≈ºnik dla jasno≈õci\n\n# Tworzenie przedzia≈Ç√≥w ufno≈õci\nlower &lt;- pmax(0, support - moe)\nupper &lt;- pmin(1, support + moe)\n\n# Sprawdzanie pokrycia\ncovers &lt;- (lower &lt;= true_support) & (upper &gt;= true_support)\nn_cover &lt;- sum(covers)\n\nresults &lt;- data.frame(\n  poll = seq_len(n_polls),\n  support, se, moe, lower, upper, covers\n)\n\n# Tworzenie wizualizacji\nggplot(results, aes(x = poll, y = support, color = covers)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                width = 0.3, alpha = 0.8, size = 1) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = true_support, \n             linetype = \"dashed\", \n             color = \"black\",\n             alpha = 0.7) +\n  scale_color_manual(\n    values = c(\"TRUE\" = \"forestgreen\", \"FALSE\" = \"darkorange\"),\n    labels = c(\"TRUE\" = \"Pokrywa prawdƒô\", \"FALSE\" = \"Mija siƒô z prawdƒÖ\"),\n    name   = NULL\n  ) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0, 1)) +\n  labs(\n    title    = \"Zmienno≈õƒá Pr√≥bkowania w 20 Niezale≈ºnych Sonda≈ºach\",\n    subtitle = paste0(\n      \"Ka≈ºdy sonda≈º: n = \", n_people, \" | Prawdziwa warto≈õƒá = \",\n      scales::percent(true_support),\n      \" | Pokrycie: \", n_cover, \"/\", n_polls,\n      \" (\", round(100 * n_cover / n_polls), \"%)\"\n    ),\n    x = \"Numer Sonda≈ºu\",\n    y = \"Oszacowane Poparcie\",\n    caption = \"S≈Çupki b≈Çƒôdu pokazujƒÖ przybli≈ºone 95% przedzia≈Çy ufno≈õci\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKluczowa Obserwacja\n\n\n\nWiƒôkszo≈õƒá przedzia≈Ç√≥w obejmuje prawdziwƒÖ warto≈õƒá, ale niekt√≥re ‚ÄúmijajƒÖ siƒô‚Äù wy≈ÇƒÖcznie z powodu losowo≈õci pr√≥bkowania. Jest to oczekiwane i kwantyfikowalne.\n\n\n\n\n\nCzƒôste B≈Çƒôdne Przekonania\n\n\n\n\n\n\nCzego NIE M√≥wi Nam Margines B≈Çƒôdu\n\n\n\n‚ùå Mit: ‚ÄúPrawdziwa warto≈õƒá na pewno mie≈õci siƒô w marginesie b≈Çƒôdu‚Äù ‚úÖ Rzeczywisto≈õƒá: Nadal istnieje 5% szans, ≈ºe jest poza (przy 95% ufno≈õci)\n‚ùå Mit: ‚ÄúMargines b≈Çƒôdu obejmuje wszystkie typy b≈Çƒôd√≥w‚Äù ‚úÖ Rzeczywisto≈õƒá: Obejmuje tylko losowy b≈ÇƒÖd pr√≥bkowania, nie systematyczne obciƒÖ≈ºenie\n‚ùå Mit: ‚ÄúWiƒôksze pr√≥by eliminujƒÖ wszystkie b≈Çƒôdy‚Äù ‚úÖ Rzeczywisto≈õƒá: RedukujƒÖ tylko b≈ÇƒÖd losowy; obciƒÖ≈ºenie pozostaje niezmienione\n\n\n\n\n\nKluczowe Wnioski\n\n\n\n\n\n\nNajwa≈ºniejsze Punkty\n\n\n\n\nDwa typy b≈Çƒôd√≥w: Losowy (redukowany wiƒôkszymi pr√≥bami) i Systematyczny (wymaga lepszego projektu)\nB≈ÇƒÖd standardowy mierzy typowƒÖ zmienno≈õƒá pr√≥bkowania\nMargines b≈Çƒôdu ‚âà 2 √ó SE dla 95% ufno≈õci\nWielko≈õƒá pr√≥by i precyzja sƒÖ w relacji pierwiastkowej\nPrzedzia≈Çy ufno≈õci dostarczajƒÖ zakres√≥w, nie gwarancji\nZawsze rozwa≈º zar√≥wno b≈ÇƒÖd pr√≥bkowania JAK I potencjalne obciƒÖ≈ºenia\n\n\n\n\n\n\n\n\n\nZapamiƒôtaj\n\n\n\nPrecyzyjna b≈Çƒôdna odpowied≈∫ (du≈ºa obciƒÖ≈ºona pr√≥ba) jest gorsza ni≈º nieprecyzyjna prawid≈Çowa odpowied≈∫ (ma≈Ça nieobciƒÖ≈ºona pr√≥ba).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#pr√≥bkowanie-i-metody-pr√≥bkowania",
    "href": "rozdzial1.html#pr√≥bkowanie-i-metody-pr√≥bkowania",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.7 Pr√≥bkowanie i metody pr√≥bkowania",
    "text": "2.7 Pr√≥bkowanie i metody pr√≥bkowania\nPr√≥bkowanie to proces wyboru podzbioru jednostek z populacji w celu oszacowania charakterystyk ca≈Çej populacji. Spos√≥b, w jaki pr√≥bkujemy, g≈Çƒôboko wp≈Çywa na to, co mo≈ºemy wywnioskowaƒá z naszych danych.\n\nOperat losowania (Sampling Frame)\nZanim om√≥wimy metody, musimy zrozumieƒá operant losowania ‚Äî listƒô lub urzƒÖdzenie, z kt√≥rego pobieramy naszƒÖ pr√≥bƒô. Operant powinien idealnie obejmowaƒá ka≈ºdego cz≈Çonka populacji dok≈Çadnie raz.\nPowszechne operanty losowania:\n\nListy wyborcze (dla doros≈Çych obywateli)\nKsiƒÖ≈ºki telefoniczne (coraz bardziej problematyczne z powodu telefon√≥w kom√≥rkowych i numer√≥w nienotowanych)\nListy adresowe z poczty\nRejestracje urodze≈Ñ (dla noworodk√≥w)\nListy zapis√≥w do szk√≥≈Ç (dla dzieci)\nRejestry podatkowe (dla os√≥b zarabiajƒÖcych)\nZdjƒôcia satelitarne (dla mieszka≈Ñ w odleg≈Çych obszarach)\n\nProblemy z operatami losowania:\n\nNiepe≈Çne pokrycie (Undercoverage): Operat pomija cz≈Çonk√≥w populacji (bezdomni nieobecni na listach adresowych)\nNadmierne pokrycie (Overcoverage): Operat obejmuje osoby spoza populacji (zmarli nadal na listach wyborc√≥w)\nDuplikacja: Ta sama jednostka pojawia siƒô wielokrotnie (osoby z wieloma numerami telefon√≥w)\nGrupowanie (Clustering): Wielu cz≈Çonk√≥w populacji na jednostkƒô operatu (wiele rodzin pod jednym adresem)\n\n\n\nMetody pr√≥bkowania probabilistycznego\nPr√≥bkowanie probabilistyczne daje ka≈ºdemu cz≈Çonkowi populacji znane, niezerowe prawdopodobie≈Ñstwo selekcji. To pozwala nam dokonywaƒá wnioskowa≈Ñ statystycznych o populacji.\n\nProste losowanie (Simple Random Sampling - SRS)\nKa≈ºda mo≈ºliwa pr√≥ba o wielko≈õci n ma r√≥wne prawdopodobie≈Ñstwo selekcji. To z≈Çoty standard teorii statystycznej, ale czƒôsto niepraktyczny dla du≈ºych populacji.\nJak to dzia≈Ça:\n\nPonumeruj ka≈ºdƒÖ jednostkƒô w populacji od 1 do N\nU≈ºyj liczb losowych do wybrania n jednostek\nKa≈ºda jednostka ma prawdopodobie≈Ñstwo n/N selekcji\n\nPrzyk≈Çad: Aby wybraƒá pr√≥bƒô 50 uczni√≥w ze szko≈Çy liczƒÖcej 1000:\n\nPrzypisz ka≈ºdemu uczniowi numer od 1 do 1000\nWygeneruj 50 losowych liczb miƒôdzy 1 a 1000\nWybierz uczni√≥w z tymi numerami\n\nZalety:\n\nStatystycznie optymalny\n≈Åatwy do analizy\nNie wymaga dodatkowych informacji o populacji\n\nWady:\n\nWymaga kompletnego operatu losowania\nMo≈ºe byƒá kosztowny (wybrane jednostki mogƒÖ byƒá daleko od siebie)\nMo≈ºe nie reprezentowaƒá dobrze wa≈ºnych podgrup przez przypadek\n\n\n\nLosowanie systematyczne (Systematic Sampling)\nWybierz co k-ty element z uporzƒÖdkowanego operatu losowania, gdzie k = N/n (interwa≈Ç pr√≥bkowania).\nJak to dzia≈Ça:\n\nOblicz interwa≈Ç pr√≥bkowania k = N/n\nLosowo wybierz punkt poczƒÖtkowy miƒôdzy 1 a k\nWybierz co k-tƒÖ jednostkƒô nastƒôpnie\n\nPrzyk≈Çad: Aby wybraƒá pr√≥bƒô 100 dom√≥w z 5000 na li≈õcie ulic:\n\nk = 5000/100 = 50\nLosowy start: 23\nPr√≥ba gospodarstw domowych: 23, 73, 123, 173, 223‚Ä¶\n\nZalety:\n\nProste do wdro≈ºenia w terenie\nRozprzestrzenia pr√≥bƒô w ca≈Çej populacji\n\nWady:\n\nMo≈ºe wprowadziƒá obciƒÖ≈ºenie, je≈õli jest okresowo≈õƒá w operacie\n\nPrzyk≈Çad ukrytej okresowo≈õci: Pr√≥bkowanie co 10. mieszkania w budynkach, gdzie mieszkania naro≈ºne (numery ko≈ÑczƒÖce siƒô na 0) sƒÖ wszystkie wiƒôksze. To zawy≈ºy≈Çoby nasze oszacowanie ≈õredniej wielko≈õci mieszkania.\n\n\nLosowanie warstwowe (Stratified Sampling)\nPodziel populacjƒô na jednorodne podgrupy (warstwy) przed pr√≥bkowaniem. Pr√≥bkuj niezale≈ºnie w ka≈ºdej warstwie.\nJak to dzia≈Ça:\n\nPodziel populacjƒô na nienachodzƒÖce warstwy\nPr√≥bkuj niezale≈ºnie z ka≈ºdej warstwy\nPo≈ÇƒÖcz wyniki z odpowiednimi wagami\n\nPrzyk≈Çad: Badanie dochodu w mie≈õcie z odrƒôbnymi dzielnicami:\n\nWarstwa 1: Dzielnica wysokich dochod√≥w (10% populacji) - pr√≥ba 100\nWarstwa 2: Dzielnica ≈õrednich dochod√≥w (60% populacji) - pr√≥ba 600\nWarstwa 3: Dzielnica niskich dochod√≥w (30% populacji) - pr√≥ba 300\n\nTypy alokacji:\nProporcjonalna: Wielko≈õƒá pr√≥by w ka≈ºdej warstwie proporcjonalna do wielko≈õci warstwy\n\nJe≈õli warstwa ma 20% populacji, dostaje 20% pr√≥by\n\nOptymalna (Neymana): Wiƒôksze pr√≥by z bardziej zmiennych warstw\n\nJe≈õli doch√≥d bardziej siƒô r√≥≈ºni w obszarach wysokich dochod√≥w, pr√≥bkuj tam wiƒôcej\n\nR√≥wna: Ta sama wielko≈õƒá pr√≥by na warstwƒô niezale≈ºnie od wielko≈õci populacji\n\nPrzydatna, gdy por√≥wnywanie warstw jest g≈Ç√≥wnym celem\n\nZalety:\n\nZapewnia reprezentacjƒô wszystkich podgrup\nMo≈ºe znacznie zwiƒôkszyƒá precyzjƒô\nPozwala na r√≥≈ºne metody pr√≥bkowania w warstwie\nDostarcza oszacowania dla ka≈ºdej warstwy\n\nWady:\n\nWymaga informacji do utworzenia warstw\nMo≈ºe byƒá trudna do badania\n\n\n\nLosowanie grupowe (Cluster Sampling)\nWybierz grupy (klastry) zamiast jednostek. Czƒôsto u≈ºywane, gdy populacja jest naturalnie pogrupowana lub gdy utworzenie kompletnego operanta jest trudne.\nJednostopniowe losowanie grupowe:\n\nPodziel populacjƒô na klastry\nLosowo wybierz niekt√≥re klastry\nUwzglƒôdnij wszystkie jednostki z wybranych klastr√≥w\n\nDwustopniowe losowanie grupowe:\n\nLosowo wybierz klastry (Pierwotne Jednostki Losowania)\nW wybranych klastrach losowo wybierz jednostki (Wt√≥rne Jednostki Losowania)\n\nPrzyk≈Çad: Badanie gospodarstw wiejskich w du≈ºym kraju:\n\nEtap 1: Losowo wybierz 50 wsi z 1000 wsi\nEtap 2: W ka≈ºdej wybranej wsi losowo wybierz 20 gospodarstw\nCa≈Çkowita pr√≥ba: 50 √ó 20 = 1000 gospodarstw\n\nPrzyk≈Çad wielostopniowy: Krajowe badanie zdrowotne:\n\nEtap 1: Wybierz wojew√≥dztwa\nEtap 2: Wybierz powiaty w wybranych wojew√≥dztwach\nEtap 3: Wybierz obwody spisowe w wybranych powiatach\nEtap 4: Wybierz gospodarstwa w wybranych obwodach\nEtap 5: Wybierz jednego doros≈Çego w wybranych gospodarstwach\n\nZalety:\n\nNie wymaga kompletnej listy populacji\nRedukuje koszty podr√≥≈ºy (jednostki zgrupowane geograficznie)\nMo≈ºe u≈ºywaƒá r√≥≈ºnych metod na r√≥≈ºnych etapach\nNaturalne dla populacji hierarchicznych\n\nWady:\n\nMniej statystycznie efektywne ni≈º SRS\nZ≈Ço≈ºona estymacja wariancji\nWiƒôksze pr√≥by potrzebne dla tej samej precyzji\n\nEfekt projektu (Design Effect): Losowanie grupowe zazwyczaj wymaga wiƒôkszych pr√≥b ni≈º SRS. Efekt projektu (DEFF) kwantyfikuje to:\n\\text{DEFF} = \\frac{\\text{Wariancja(pr√≥ba grupowa)}}{\\text{Wariancja(SRS)}}\nJe≈õli DEFF = 2, potrzebujesz dwukrotnie wiƒôkszej pr√≥by, aby osiƒÖgnƒÖƒá takƒÖ samƒÖ precyzjƒô jak SRS.\n\n\n\nMetody pr√≥bkowania nieprobabilistycznego\nPr√≥bkowanie nieprobabilistyczne nie gwarantuje znanych prawdopodobie≈Ñstw selekcji. Choƒá ogranicza wnioskowanie statystyczne, te metody mogƒÖ byƒá konieczne lub przydatne w pewnych sytuacjach.\n\nPr√≥bkowanie wygodne (Convenience Sampling)\nSelekcja oparta wy≈ÇƒÖcznie na ≈Çatwo≈õci dostƒôpu. Brak pr√≥by reprezentacji.\nPrzyk≈Çady:\n\nAnkietowanie student√≥w w twojej klasie o nawykach nauki\nWywiadowanie ludzi w centrum handlowym o preferencjach konsumenckich\nAnkiety online, w kt√≥rych ka≈ºdy mo≈ºe uczestniczyƒá\nBadania medyczne u≈ºywajƒÖce wolontariuszy, kt√≥rzy odpowiadajƒÖ na og≈Çoszenia\n\nKiedy mo≈ºe byƒá akceptowalne:\n\nBadania pilota≈ºowe do testowania instrument√≥w ankietowych\nBadania eksploracyjne do identyfikacji problem√≥w\nGdy badane procesy uwa≈ºa siƒô za uniwersalne\n\nG≈Ç√≥wne problemy:\n\nBrak podstaw do wnioskowania o populacji\nPrawdopodobne powa≈ºne obciƒÖ≈ºenie selekcyjne\nWyniki mogƒÖ byƒá ca≈Çkowicie mylƒÖce\n\nPrawdziwy przyk≈Çad: Sonda≈º prezydencki Literary Digest z 1936 roku ankietowa≈Ç 2,4 miliona os√≥b (ogromna pr√≥ba!), ale u≈ºywa≈Ç ksiƒÖ≈ºek telefonicznych i cz≈Çonkostwa w klubach jako operant√≥w podczas Wielkiego Kryzysu, dramatycznie nadreprezentujƒÖc bogatych wyborc√≥w i niepoprawnie przewidujƒÖc, ≈ºe Landon pokona Roosevelta.\n\n\nPr√≥bkowanie celowe (Purposive/Judgmental Sampling)\nCelowy wyb√≥r konkretnych przypadk√≥w oparty na osƒÖdzie badacza o tym, co jest ‚Äûtypowe‚Äù lub ‚ÄûinteresujƒÖce‚Äù.\nPrzyk≈Çady:\n\nWyb√≥r ‚Äûtypowych‚Äù wsi do reprezentowania obszar√≥w wiejskich\nWyb√≥r konkretnych grup wiekowych do badania rozwojowego\nWyb√≥r skrajnych przypadk√≥w do zrozumienia zakresu zmienno≈õci\nWyb√≥r przypadk√≥w bogatych w informacje do dog≈Çƒôbnego badania\n\nTypy pr√≥bkowania celowego:\nTypowy przypadek: Wybierz przeciƒôtne lub normalne przyk≈Çady\n\nBadanie ‚Äûtypowych‚Äù polskich przedmie≈õƒá\n\nSkrajny/dewiacyjny przypadek: Wybierz niezwyk≈Çe przyk≈Çady\n\nBadanie wsi z niezwykle niskƒÖ ≈õmiertelno≈õciƒÖ niemowlƒÖt, aby zrozumieƒá czynniki sukcesu\n\nMaksymalna zmienno≈õƒá: Celowo wybierz r√≥≈ºnorodne przypadki\n\nWyb√≥r r√≥≈ºnych szk√≥≈Ç (miejskich/wiejskich, bogatych/biednych, du≈ºych/ma≈Çych) do bada≈Ñ edukacyjnych\n\nPrzypadek krytyczny: Wybierz przypadki, kt√≥re bƒôdƒÖ definitywne\n\n‚ÄûJe≈õli to nie dzia≈Ça tutaj, nie zadzia≈Ça nigdzie‚Äù\n\nKiedy jest przydatne:\n\nBadania jako≈õciowe skupiajƒÖce siƒô na g≈Çƒôbi nad szeroko≈õciƒÖ\nGdy badane sƒÖ rzadkie populacje\nOgraniczenia zasob√≥w powa≈ºnie limitujƒÖ wielko≈õƒá pr√≥by\nFazy eksploracyjne bada≈Ñ\n\nProblemy:\n\nCa≈Çkowicie zale≈ºne od osƒÖdu badacza\nNiemo≈ºliwe wnioskowanie statystyczne\nR√≥≈ºni badacze mogƒÖ wybraƒá r√≥≈ºne ‚Äûtypowe‚Äù przypadki\n\n\n\nPr√≥bkowanie kwotowe (Quota Sampling)\nSelekcja w celu dopasowania proporcji populacji w kluczowych charakterystykach. Jak losowanie warstwowe, ale bez losowej selekcji w grupach.\nJak dzia≈Ça pr√≥bkowanie kwotowe:\n\nZidentyfikuj kluczowe charakterystyki (wiek, p≈Çeƒá, rasa, wykszta≈Çcenie)\nOkre≈õl proporcje populacji dla tych charakterystyk\nUstaw kwoty dla ka≈ºdej kombinacji\nAnkieterzy wype≈ÇniajƒÖ kwoty u≈ºywajƒÖc metod wygodnych\n\nSzczeg√≥≈Çowy przyk≈Çad: Sonda≈º polityczny z kwotami:\nProporcje populacji:\n\nMƒô≈ºczyzna 18-34: 15%\nMƒô≈ºczyzna 35-54: 20%\nMƒô≈ºczyzna 55+: 15%\nKobieta 18-34: 16%\nKobieta 35-54: 19%\nKobieta 55+: 15%\n\nDla pr√≥by 1000:\n\nWywiad z 150 mƒô≈ºczyznami w wieku 18-34\nWywiad z 200 mƒô≈ºczyznami w wieku 35-54\nI tak dalej‚Ä¶\n\nAnkieterzy mogƒÖ staƒá na rogach ulic, podchodzƒÖc do os√≥b, kt√≥re wydajƒÖ siƒô pasowaƒá do potrzebnych kategorii, a≈º kwoty zostanƒÖ wype≈Çnione.\nDlaczego jest popularne w badaniach rynkowych:\n\nSzybsze ni≈º pr√≥bkowanie probabilistyczne\nTa≈Ñsze (brak ponownych kontakt√≥w dla konkretnych os√≥b)\nZapewnia reprezentacjƒô demograficznƒÖ\nNie wymaga operatu losowania\n\nDlaczego jest problematyczne dla wnioskowania statystycznego:\nUkryte obciƒÖ≈ºenie selekcyjne: Ankieterzy podchodzƒÖ do os√≥b, kt√≥re wyglƒÖdajƒÖ na przystƒôpne, dobrze m√≥wiƒÖ jƒôzykiem, nie spieszƒÖ siƒô ‚Äî systematycznie wykluczajƒÖc pewne typy w ka≈ºdej kom√≥rce kwotowej.\nPrzyk≈Çad obciƒÖ≈ºenia: Ankieter wype≈ÇniajƒÖcy kwotƒô dla ‚Äûkobiet 18-34‚Äù mo≈ºe podchodziƒá do kobiet w centrum handlowym we wtorek po po≈Çudniu, systematycznie pomijajƒÖc:\n\nKobiety pracujƒÖce w dni powszednie\nKobiety, kt√≥rych nie staƒá na zakupy w centrach handlowych\nKobiety z ma≈Çymi dzieƒámi, kt√≥re unikajƒÖ centr√≥w handlowych\nKobiety robiƒÖce zakupy online\n\nMimo ≈ºe ko≈Ñcowa pr√≥ba ma ‚Äûw≈Ça≈õciwƒÖ‚Äù proporcjƒô m≈Çodych kobiet, nie sƒÖ one reprezentatywne dla wszystkich m≈Çodych kobiet.\nBrak miary b≈Çƒôdu pr√≥bkowania: Bez prawdopodobie≈Ñstw selekcji nie mo≈ºemy obliczyƒá b≈Çƒôd√≥w standardowych ani przedzia≈Ç√≥w ufno≈õci.\nHistoryczna przestroga: Pr√≥bkowanie kwotowe by≈Ço standardem w sonda≈ºach do wybor√≥w prezydenckich w USA w 1948 roku, gdy sonda≈ºe u≈ºywajƒÖce pr√≥bkowania kwotowego niepoprawnie przewidzia≈Çy, ≈ºe Dewey pokona Trumana. Niepowodzenie doprowadzi≈Ço do przyjƒôcia pr√≥bkowania probabilistycznego w sonda≈ºach.\n\n\nPr√≥bkowanie kuli ≈õnie≈ºnej (Snowball Sampling)\nUczestnicy rekrutujƒÖ dodatkowych uczestnik√≥w ze swoich znajomych. Pr√≥ba ro≈õnie jak toczƒÖca siƒô kula ≈õnie≈ºna.\nJak to dzia≈Ça:\n\nZidentyfikuj poczƒÖtkowych uczestnik√≥w (nasiona)\nPopro≈õ ich o polecenie innych z wymaganymi charakterystykami\nPopro≈õ nowych uczestnik√≥w o dalsze polecenia\nKontynuuj, a≈º osiƒÖgniƒôta zostanie wielko≈õƒá pr√≥by lub wyczerpiƒÖ siƒô polecenia\n\nPrzyk≈Çad: Badanie nieudokumentowanych imigrant√≥w:\n\nZacznij od 5 imigrant√≥w, kt√≥rych mo≈ºesz zidentyfikowaƒá\nKa≈ºdy poleca 3 innych, kt√≥rych zna\nTych 15 ka≈ºdy poleca 2-3 innych\nKontynuuj, a≈º masz 100+ uczestnik√≥w\n\nKiedy jest warto≈õciowe:\nUkryte populacje: Grupy bez operant√≥w losowania\n\nU≈ºytkownicy narkotyk√≥w\nOsoby bezdomne\nOsoby z rzadkimi chorobami\nCz≈Çonkowie ruch√≥w podziemnych\n\nPopulacje po≈ÇƒÖczone spo≈Çecznie: Gdy relacje majƒÖ znaczenie\n\nBadanie efekt√≥w sieci spo≈Çecznych\nBadanie transmisji chor√≥b w spo≈Çeczno≈õci\nZrozumienie dyfuzji informacji\n\nBadania zale≈ºne od zaufania: Gdy polecenia zwiƒôkszajƒÖ uczestnictwo\n\nWra≈ºliwe tematy, gdzie zaufanie jest niezbƒôdne\nZamkniƒôte spo≈Çeczno≈õci podejrzliwe wobec obcych\n\nG≈Ç√≥wne ograniczenia:\n\nPr√≥by obciƒÖ≈ºone w kierunku os√≥b wsp√≥≈ÇpracujƒÖcych, dobrze po≈ÇƒÖczonych\nOdizolowani cz≈Çonkowie populacji ca≈Çkowicie pominiƒôci\nWnioskowanie statystyczne generalnie niemo≈ºliwe\nMo≈ºe wzmacniaƒá podzia≈Çy spo≈Çeczne (≈Ça≈Ñcuchy rzadko przekraczajƒÖ granice spo≈Çeczne)\n\nZaawansowana wersja ‚Äî Pr√≥bkowanie sterowane przez respondent√≥w (Respondent-Driven Sampling - RDS):\nPr√≥buje uczyniƒá pr√≥bkowanie kuli ≈õnie≈ºnej bardziej rygorystycznym poprzez:\n\n≈öledzenie, kto zrekrutowa≈Ç kogo\nOgraniczanie liczby polece≈Ñ na osobƒô\nWa≈ºenie na podstawie wielko≈õci sieci\nU≈ºywanie modeli matematycznych do korekty obciƒÖ≈ºenia\n\nNadal kontrowersyjne, czy RDS naprawdƒô pozwala na wa≈ºne wnioskowanie.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#pojƒôcia-prawdopodobie≈Ñstwa-w-analizie-statystycznej",
    "href": "rozdzial1.html#pojƒôcia-prawdopodobie≈Ñstwa-w-analizie-statystycznej",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.8 Pojƒôcia prawdopodobie≈Ñstwa w analizie statystycznej",
    "text": "2.8 Pojƒôcia prawdopodobie≈Ñstwa w analizie statystycznej\nChoƒá to przede wszystkim kurs statystyki, zrozumienie podstawowego prawdopodobie≈Ñstwa jest niezbƒôdne dla wnioskowania statystycznego.\n\nPodstawowe prawdopodobie≈Ñstwo\nPrawdopodobie≈Ñstwo kwantyfikuje niepewno≈õƒá na skali od 0 (niemo≈ºliwe) do 1 (pewne).\nPrawdopodobie≈Ñstwo klasyczne: P(\\text{zdarzenie}) = \\frac{\\text{Liczba korzystnych wynik√≥w}}{\\text{Ca≈Çkowita liczba mo≈ºliwych wynik√≥w}}\nPrzyk≈Çad: Prawdopodobie≈Ñstwo, ≈ºe losowo wybrana osoba jest kobietƒÖ \\approx 0,5\nPrawdopodobie≈Ñstwo empiryczne: Oparte na obserwowanych czƒôsto≈õciach\nPrzyk≈Çad: W wiosce 423 z 1000 mieszka≈Ñc√≥w to kobiety, wiƒôc P(\\text{kobieta}) \\approx 0,423\n\n\nPrawdopodobie≈Ñstwo warunkowe\nPrawdopodobie≈Ñstwo warunkowe to prawdopodobie≈Ñstwo zdarzenia A, przy za≈Ço≈ºeniu ≈ºe zdarzenie B wystƒÖpi≈Ço: P(A|B)\nPrzyk≈Çad demograficzny: Prawdopodobie≈Ñstwo ≈õmierci w ciƒÖgu roku przy danym wieku:\n\nP(\\text{≈õmierƒá w ciƒÖgu roku} | \\text{wiek 30}) \\approx 0,001\nP(\\text{≈õmierƒá w ciƒÖgu roku} | \\text{wiek 80}) \\approx 0,05\n\nTe prawdopodobie≈Ñstwa warunkowe stanowiƒÖ podstawƒô tablic trwania ≈ºycia.\n\n\nNiezale≈ºno≈õƒá\nZdarzenia A i B sƒÖ niezale≈ºne, je≈õli P(A|B) = P(A).\nTestowanie niezale≈ºno≈õci w danych demograficznych:\nCzy wykszta≈Çcenie i p≈Çodno≈õƒá sƒÖ niezale≈ºne?\n\nP(\\text{3+ dzieci}) = 0,3 og√≥lnie\nP(\\text{3+ dzieci} | \\text{wykszta≈Çcenie wy≈ºsze}) = 0,15\nR√≥≈ºne prawdopodobie≈Ñstwa wskazujƒÖ na zale≈ºno≈õƒá\n\n\n\nPrawo wielkich liczb\nGdy wielko≈õƒá pr√≥by wzrasta, statystyki z pr√≥by zbiegajƒÖ siƒô do parametr√≥w populacji.\nDemonstracja: Szacowanie proporcji p≈Çci przy urodzeniu:\n\n10 urodze≈Ñ: 7 ch≈Çopc√≥w (70% - bardzo niestabilne)\n100 urodze≈Ñ: 53 ch≈Çopc√≥w (53% - zbli≈ºamy siƒô do ~51,2%)\n1000 urodze≈Ñ: 515 ch≈Çopc√≥w (51,5% - ca≈Çkiem blisko)\n10 000 urodze≈Ñ: 5118 ch≈Çopc√≥w (51,18% - bardzo blisko)\n\n\n\nWizualizacja Prawa wielkich liczb: rzuty monetƒÖ\nZobaczmy to w dzia≈Çaniu na przyk≈Çadzie rzut√≥w monetƒÖ. Uczciwa moneta ma 50% szansy na wypadniƒôcie or≈Ça, ale poszczeg√≥lne rzuty sƒÖ nieprzewidywalne.\n\n# Symulacja rzut√≥w monetƒÖ i pokazanie zbie≈ºno≈õci\nset.seed(42)\nn_flips &lt;- 1000\nflips &lt;- rbinom(n_flips, 1, 0.5)  # 1 = orze≈Ç, 0 = reszka\n\n# Obliczanie skumulowanej proporcji or≈Ç√≥w\ncumulative_prop &lt;- cumsum(flips) / seq_along(flips)\n\n# Utworzenie ramki danych do wizualizacji\nlln_data &lt;- data.frame(\n  flip_number = 1:n_flips,\n  cumulative_proportion = cumulative_prop\n)\n\n# Wykres zbie≈ºno≈õci\nggplot(lln_data, aes(x = flip_number, y = cumulative_proportion)) +\n  geom_line(color = \"steelblue\", alpha = 0.7) +\n  geom_hline(yintercept = 0.5, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_hline(yintercept = c(0.45, 0.55), color = \"red\", linetype = \"dotted\", alpha = 0.7) +\n  labs(\n    title = \"Prawo wielkich liczb: Proporcje rzut√≥w monetƒÖ zbiegajƒÖ do 0,5\",\n    x = \"Liczba rzut√≥w monetƒÖ\",\n    y = \"Skumulowana proporcja or≈Ç√≥w\",\n    caption = \"Czerwona linia przerywana = prawdziwe prawdopodobie≈Ñstwo (0,5)\\nLinie kropkowane = zakres ¬±5%\"\n  ) +\n  scale_y_continuous(limits = c(0.3, 0.7), breaks = seq(0.3, 0.7, 0.1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCo to pokazuje:\n\nPoczƒÖtkowe rzuty wykazujƒÖ du≈ºe wahania (pierwsze 10 rzut√≥w mo≈ºe daƒá 70% lub 30% or≈Ç√≥w)\nW miarƒô dodawania kolejnych rzut√≥w, proporcja stabilizuje siƒô wok√≥≈Ç 50%\n‚ÄûSzum‚Äù poszczeg√≥lnych wynik√≥w siƒô u≈õrednia w czasie\n\n\n\nSformu≈Çowanie matematyczne\nNiech A oznacza zdarzenie nas interesujƒÖce (np. ‚Äûorze≈Ç w rzucie monetƒÖ‚Äù, ‚Äûg≈Ços na partiƒô X‚Äù, ‚Äûsuma kostek r√≥wna 7‚Äù). Je≈õli P(A) = p i obserwujemy n niezale≈ºnych pr√≥b z tym samym rozk≈Çadem (i.i.d.), to czƒôsto≈õƒá pr√≥bkowa zdarzenia A:\n\\hat{p}_n = \\frac{\\text{liczba wystƒÖpie≈Ñ zdarzenia } A}{n}\nzbiega do p gdy n ro≈õnie.\n\n\nPrzyk≈Çady w r√≥≈ºnych kontekstach\nPrzyk≈Çad z kostkami: Zdarzenie ‚Äûsuma = 7‚Äù przy dw√≥ch kostkach ma prawdopodobie≈Ñstwo 6/36 ‚âà 16,7\\%, podczas gdy ‚Äûsuma = 4‚Äù ma 3/36 ‚âà 8,3\\%. Przy wielu rzutach suma 7 pojawia siƒô oko≈Ço dwa razy czƒô≈õciej ni≈º suma 4.\nSonda≈ºe wyborcze: Je≈õli poparcie populacyjne dla partii wynosi p, to przy losowym doborze pr√≥by o wielko≈õci n obserwowana czƒôsto≈õƒá \\hat{p}_n bƒôdzie zbli≈ºaƒá siƒô do p w miarƒô wzrostu n (zak≈ÇadajƒÖc losowy dob√≥r i niezale≈ºno≈õƒá pr√≥b).\nKontrola jako≈õci: Je≈õli 2% produkt√≥w jest wadliwych, to w du≈ºych partiach oko≈Ço 2% zostanie uznanych za wadliwe (zak≈ÇadajƒÖc niezale≈ºnƒÖ produkcjƒô).\n\n\nDlaczego to ma znaczenie dla statystyki\nWniosek: Losowo≈õƒá stanowi podstawƒô wnioskowania statystycznego, przekszta≈ÇcajƒÖc niepewno≈õƒá poszczeg√≥lnych wynik√≥w w przewidywalne rozk≈Çady dla estymator√≥w. Prawo wielkich liczb gwarantuje, ≈ºe ‚Äûszum‚Äù poszczeg√≥lnych wynik√≥w siƒô u≈õrednia, pozwalajƒÖc nam:\n\nPrzewidywaƒá d≈Çugookresowe czƒôsto≈õci\nKwantyfikowaƒá niepewno≈õƒá (marginesy b≈Çƒôdu)\nWyciƒÖgaƒá rzetelne wnioski z pr√≥b\nFormu≈Çowaƒá probabilistyczne stwierdzenia o populacjach\n\nTa zasada dzia≈Ça w sonda≈ºach, eksperymentach, a nawet w zjawiskach kwantowych (w interpretacji czƒôsto≈õciowej).\n\n\n\n\n\n\nCzym jest losowo≈õƒá? (*)\n\n\n\nW statystyce losowo≈õƒá to uporzƒÖdkowany spos√≥b opisu niepewno≈õci: pojedyncze wyniki sƒÖ nieprzewidywalne, natomiast w d≈Çugiej serii powt√≥rze≈Ñ ujawniajƒÖ siƒô stabilne prawid≈Çowo≈õci (np. czƒôsto≈õci, ≈õrednie).\nDwie perspektywy\n\nPojedyncza realizacja ‚Äî nie potrafimy przesƒÖdziƒá, jak zag≈Çosuje konkretny wyborca w danym momencie.\n\nZbiorowo≈õƒá ‚Äî mo≈ºemy opisaƒá odsetek wyborc√≥w g≈ÇosujƒÖcych na danƒÖ partiƒô oraz zwiƒÖzanƒÖ z nim niepewno≈õƒá estymacji.\n\nLosowo≈õƒá epistemiczna a ontologiczna\n\nEpistemiczna (zwiƒÖzana z niewiedzƒÖ): wynik traktujemy jako losowy, poniewa≈º nie obserwujemy wszystkich determinant lub nie kontrolujemy warunk√≥w.\nPrzyk≈Çady:\n\ndecyzja pojedynczego wyborcy w sonda≈ºu (nie znamy pe≈Çnych motywacji),\nb≈ÇƒÖd pomiaru w ankiecie (ograniczona precyzja, brak odpowiedzi),\nrzut monetƒÖ modelowany jako losowy, poniewa≈º drobne, nieobserwowalne r√≥≈ºnice warunk√≥w poczƒÖtkowych determinujƒÖ wynik.\n\nOntologiczna (wrodzona przypadkowo≈õƒá zjawiska): nawet pe≈Çna wiedza nie usuwa niepewno≈õci wyniku.\nPrzyk≈Çady:\n\nczas rozpadu promieniotw√≥rczego atomu.\n\n\n\nZnaczenie losowo≈õci\n\nLosowe pr√≥bkowanie\n\nOgranicza systematyczny b≈ÇƒÖd doboru, dziƒôki czemu pr√≥ba przypomina populacjƒô docelowƒÖ (w ≈õrednim ujƒôciu / w warto≈õci oczekiwanej).\nUmo≈ºliwia ilo≈õciowe ujƒôcie niepewno≈õci (np. margines b≈Çƒôdu; pojƒôcie ‚Äûprzedzia≈Çu ufno≈õci‚Äù om√≥wimy p√≥≈∫niej), pod warunkiem rzeczywi≈õcie losowego doboru i dobrego pokrycia populacji.\n\nLosowy przydzia≈Ç (eksperymenty)\n\nPrzerywa zwiƒÖzek miƒôdzy przydzia≈Çem a innymi czynnikami, czyniƒÖc grupy por√≥wnywalnymi ≈õrednio (zar√≥wno pod wzglƒôdem cech obserwowalnych, jak i nieobserwowalnych).\nUmo≈ºliwia wiarygodne wnioskowanie przyczynowe (identyfikacjƒô ≈õrednich efekt√≥w przy standardowych za≈Ço≈ºeniach).\n\n\n\n\nSi≈Ça losowego pr√≥bkowania\nZa≈Ç√≥≈ºmy, ≈ºe losujemy pr√≥bƒô prostƒÖ o liczebno≈õci n=1000 wyborc√≥w i obserwujemy \\hat p = 0{,}55 (tj. 55% poparcia). W√≥wczas:\n\nNaszƒÖ najlepszƒÖ, jednowarto≈õciowƒÖ ocenƒÖ odsetka w populacji p jest \\hat p = 0{,}55.\nOrientacyjny ‚Äû95\\% zakres warto≈õci plauzybilnych‚Äù wok√≥≈Ç \\hat p mo≈ºna przybli≈ºyƒá wzorem \n\\hat p \\,\\pm\\, 2\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\n\\;=\\;\n0{,}55 \\,\\pm\\, 2\\sqrt{\\frac{0{,}55\\cdot 0{,}45}{1000}}\n\\approx\n0{,}55 \\pm 0{,}031,\n czyli w przybli≈ºeniu 52\\%\\text{‚Äì}58\\% (oko≈Ço \\pm 3{,}1 punktu procentowego, pp).\nSzeroko≈õƒá tego zakresu maleje wraz z liczebno≈õciƒÖ pr√≥by: \n\\text{szeroko≈õƒá} \\;\\propto\\; \\frac{1}{\\sqrt{n}}.\n Przyk≈Çadowo, zwiƒôkszenie n z 1000 do 4000 mniej wiƒôcej zmniejsza b≈ÇƒÖd o po≈Çowƒô.\n\n\n\n2.9 Rozumienie r√≥≈ºnych typ√≥w nieprzewidywalno≈õci\nNie wszystkie rodzaje niepewno≈õci sƒÖ takie same. Zrozumienie r√≥≈ºnych ≈∫r√≥de≈Ç nieprzewidywalno≈õci pomaga w wyborze odpowiednich metod statystycznych i prawid≈Çowej interpretacji wynik√≥w.\n\n\n\n\n\n\n\n\n\nPojƒôcie\nCzym jest?\n≈πr√≥d≈Ço nieprzewidywalno≈õci\nPrzyk≈Çad\n\n\n\n\nLosowo≈õƒá (randomness)\nPoszczeg√≥lne wyniki sƒÖ niepewne, ale rozk≈Çad prawdopodobie≈Ñstwa jest znany lub modelowany.\nFluktuacje miƒôdzy realizacjami; brak informacji o konkretnym wyniku.\nRzut kostkƒÖ, rzut monetƒÖ, pr√≥ba sonda≈ºowa\n\n\nChaos\nDynamika deterministyczna bardzo wra≈ºliwa na warunki poczƒÖtkowe (efekt motyla).\nNiewielkie r√≥≈ºnice poczƒÖtkowe szybko narastajƒÖ ‚Üí du≈ºe rozbie≈ºno≈õci trajektorii.\nPrognoza pogody, podw√≥jne wahad≈Ço, dynamika populacyjna\n\n\nEntropia\nMiara niepewno≈õci/rozproszenia (teorioinformacyjna lub termodynamiczna).\nWiƒôksza gdy wyniki sƒÖ bardziej r√≥wnomiernie roz≈Ço≈ºone (mniej informacji predykcyjnej).\nEntropia Shannona w kompresji danych\n\n\n‚ÄûPrzypadkowo≈õƒá‚Äù (potoczne)\nOdczuwany brak porzƒÖdku bez wyra≈∫nego modelu; mieszanka mechanizm√≥w.\nBrak uporzƒÖdkowanego opisu lub stabilnych regu≈Ç; nak≈ÇadajƒÖce siƒô procesy.\nWzorce ruchu, trendy w mediach spo≈Çeczno≈õciowych\n\n\nLosowo≈õƒá kwantowa (quantum randomness)\nPojedynczy wynik nie jest zdeterminowany; tylko rozk≈Çad jest okre≈õlony (regu≈Ça Borna).\nFundamentalna (ontologiczna) nieokre≈õlono≈õƒá poszczeg√≥lnych pomiar√≥w.\nPomiar spinu elektronu, polaryzacja fotonu\n\n\n\n\nKluczowe rozr√≥≈ºnienia dla praktyki statystycznej\nChaos deterministyczny ‚â† losowo≈õƒá statystyczna: System chaotyczny jest w pe≈Çni deterministyczny, ale praktycznie nieprzewidywalny z powodu ekstremalnej wra≈ºliwo≈õci na warunki poczƒÖtkowe. Losowo≈õƒá statystyczna modeluje natomiast niepewno≈õƒá poprzez rozk≈Çady prawdopodobie≈Ñstwa, gdzie poszczeg√≥lne wyniki sƒÖ rzeczywi≈õcie niepewne.\nDlaczego to wa≈ºne: W statystyce zazwyczaj modelujemy zjawiska jako procesy losowe, zak≈ÇadajƒÖc, ≈ºe mo≈ºemy okre≈õliƒá rozk≈Çady prawdopodobie≈Ñstwa, nawet gdy poszczeg√≥lne wyniki sƒÖ nieprzewidywalne. To za≈Ço≈ºenie stanowi podstawƒô wiƒôkszo≈õci wnioskowa≈Ñ statystycznych.\n\n\nMechanika kwantowa i fundamentalna losowo≈õƒá\nW interpretacji kopenhaskiej losowo≈õƒá jest fundamentalna (ontologiczna): pojedynczy wynik nie mo≈ºe byƒá przewidziany, ale rozk≈Çad prawdopodobie≈Ñstwa jest dany przez regu≈Çƒô Borna.\nTo reprezentuje prawdziwƒÖ losowo≈õƒá na najbardziej podstawowym poziomie natury, nie tylko naszƒÖ ignorancjƒô czynnik√≥w determinujƒÖcych.\n\n\n\n\n\n\nCentralne Twierdzenie Graniczne\nRozk≈Çad ≈õrednich z pr√≥b zbli≈ºa siƒô do rozk≈Çadu normalnego, gdy wielko≈õƒá pr√≥by wzrasta, niezale≈ºnie od rozk≈Çadu populacji.\nDlaczego to ma znaczenie: Nawet je≈õli doch√≥d jest bardzo sko≈õny, ≈õredni doch√≥d z pr√≥b 100+ os√≥b podƒÖ≈ºa za przybli≈ºonym rozk≈Çadem normalnym, pozwalajƒÖc nam u≈ºywaƒá przedzia≈Ç√≥w ufno≈õci opartych na rozk≈Çadzie normalnym.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#rozumienie-r√≥≈ºnych-typ√≥w-nieprzewidywalno≈õci",
    "href": "rozdzial1.html#rozumienie-r√≥≈ºnych-typ√≥w-nieprzewidywalno≈õci",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.9 Rozumienie r√≥≈ºnych typ√≥w nieprzewidywalno≈õci",
    "text": "2.9 Rozumienie r√≥≈ºnych typ√≥w nieprzewidywalno≈õci\nNie wszystkie rodzaje niepewno≈õci sƒÖ takie same. Zrozumienie r√≥≈ºnych ≈∫r√≥de≈Ç nieprzewidywalno≈õci pomaga w wyborze odpowiednich metod statystycznych i prawid≈Çowej interpretacji wynik√≥w.\n\n\n\n\n\n\n\n\n\nPojƒôcie\nCzym jest?\n≈πr√≥d≈Ço nieprzewidywalno≈õci\nPrzyk≈Çad\n\n\n\n\nLosowo≈õƒá (randomness)\nPoszczeg√≥lne wyniki sƒÖ niepewne, ale rozk≈Çad prawdopodobie≈Ñstwa jest znany lub modelowany.\nFluktuacje miƒôdzy realizacjami; brak informacji o konkretnym wyniku.\nRzut kostkƒÖ, rzut monetƒÖ, pr√≥ba sonda≈ºowa\n\n\nChaos\nDynamika deterministyczna bardzo wra≈ºliwa na warunki poczƒÖtkowe (efekt motyla).\nNiewielkie r√≥≈ºnice poczƒÖtkowe szybko narastajƒÖ ‚Üí du≈ºe rozbie≈ºno≈õci trajektorii.\nPrognoza pogody, podw√≥jne wahad≈Ço, dynamika populacyjna\n\n\nEntropia\nMiara niepewno≈õci/rozproszenia (teorioinformacyjna lub termodynamiczna).\nWiƒôksza gdy wyniki sƒÖ bardziej r√≥wnomiernie roz≈Ço≈ºone (mniej informacji predykcyjnej).\nEntropia Shannona w kompresji danych\n\n\n‚ÄûPrzypadkowo≈õƒá‚Äù (potoczne)\nOdczuwany brak porzƒÖdku bez wyra≈∫nego modelu; mieszanka mechanizm√≥w.\nBrak uporzƒÖdkowanego opisu lub stabilnych regu≈Ç; nak≈ÇadajƒÖce siƒô procesy.\nWzorce ruchu, trendy w mediach spo≈Çeczno≈õciowych\n\n\nLosowo≈õƒá kwantowa (quantum randomness)\nPojedynczy wynik nie jest zdeterminowany; tylko rozk≈Çad jest okre≈õlony (regu≈Ça Borna).\nFundamentalna (ontologiczna) nieokre≈õlono≈õƒá poszczeg√≥lnych pomiar√≥w.\nPomiar spinu elektronu, polaryzacja fotonu\n\n\n\n\nKluczowe rozr√≥≈ºnienia dla praktyki statystycznej\nChaos deterministyczny ‚â† losowo≈õƒá statystyczna: System chaotyczny jest w pe≈Çni deterministyczny, ale praktycznie nieprzewidywalny z powodu ekstremalnej wra≈ºliwo≈õci na warunki poczƒÖtkowe. Losowo≈õƒá statystyczna modeluje natomiast niepewno≈õƒá poprzez rozk≈Çady prawdopodobie≈Ñstwa, gdzie poszczeg√≥lne wyniki sƒÖ rzeczywi≈õcie niepewne.\nDlaczego to wa≈ºne: W statystyce zazwyczaj modelujemy zjawiska jako procesy losowe, zak≈ÇadajƒÖc, ≈ºe mo≈ºemy okre≈õliƒá rozk≈Çady prawdopodobie≈Ñstwa, nawet gdy poszczeg√≥lne wyniki sƒÖ nieprzewidywalne. To za≈Ço≈ºenie stanowi podstawƒô wiƒôkszo≈õci wnioskowa≈Ñ statystycznych.\n\n\nMechanika kwantowa i fundamentalna losowo≈õƒá\nW interpretacji kopenhaskiej losowo≈õƒá jest fundamentalna (ontologiczna): pojedynczy wynik nie mo≈ºe byƒá przewidziany, ale rozk≈Çad prawdopodobie≈Ñstwa jest dany przez regu≈Çƒô Borna.\nTo reprezentuje prawdziwƒÖ losowo≈õƒá na najbardziej podstawowym poziomie natury, nie tylko naszƒÖ ignorancjƒô czynnik√≥w determinujƒÖcych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#istotno≈õƒá-statystyczna-wprowadzenie",
    "href": "rozdzial1.html#istotno≈õƒá-statystyczna-wprowadzenie",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.10 Istotno≈õƒá Statystyczna: Wprowadzenie",
    "text": "2.10 Istotno≈õƒá Statystyczna: Wprowadzenie\nWyobra≈∫ sobie, ≈ºe rzucasz monetƒÖ 10 razy i wypad≈Ço 8 or≈Ç√≥w. Czy moneta jest fa≈Çszywa, czy po prostu mia≈Çe≈õ szczƒô≈õcie? To jest kluczowe pytanie, na kt√≥re pomaga odpowiedzieƒá istotno≈õƒá statystyczna (wnioskowanie statystyczne).\nIstotno≈õƒá statystyczna to miara (p-value) tego, na ile mo≈ºemy byƒá pewni, ≈ºe wzorce obserwowane w naszej pr√≥bie nie sƒÖ dzie≈Çem przypadku. Gdy wynik jest statystycznie istotny (zwykle przyjmujemy p-value &lt; 0.05), oznacza to, ≈ºe prawdopodobie≈Ñstwo uzyskania takich danych przy braku rzeczywistego efektu jest bardzo niskie.\nIstotno≈õƒá statystyczna pomaga nam rozr√≥≈ºniƒá miƒôdzy rzeczywistymi zjawiskami a przypadkowymi fluktuacjami w danych. Gdy m√≥wimy, ≈ºe wynik jest statystycznie istotny, znaczy to, ≈ºe prawdopodobnie nie powsta≈Ç przez zwyk≈Çy zbieg okoliczno≈õci.\n\nAnalogia do Sali SƒÖdowej\nTestowanie hipotez statystycznych dzia≈Ça jak proces karny:\n\nHipoteza Zerowa (H_0): Oskar≈ºony jest niewinny (nie ma efektu)\nHipoteza Alternatywna (H_1): Oskar≈ºony jest winny (efekt istnieje)\nDowody: Twoje dane i wyniki test√≥w\nWerdykt: ‚ÄúWinny‚Äù (odrzuƒá H_0) lub ‚ÄúNiewinny‚Äù (nie odrzucaj H_0)\n\nKluczowe rozr√≥≈ºnienie: ‚ÄúNiewinny‚Äù ‚â† ‚ÄúNiewinny‚Äù\n\nWerdykt ‚Äúniewinny‚Äù oznacza niewystarczajƒÖce dowody do skazania\nPodobnie, ‚Äúbrak istotno≈õci statystycznej‚Äù oznacza niewystarczajƒÖce dowody na istnienie efektu, NIE dow√≥d braku efektu\n\n\n\nBrak efektu (‚ÄúDomniemanie niewinno≈õci‚Äù)\nW statystyce zawsze zaczynamy od za≈Ço≈ºenia, ≈ºe nic specjalnego siƒô nie dzieje:\n\nHipoteza Zerowa (H_0): ‚ÄúNie ma efektu‚Äù\n\nMoneta jest uczciwa\nNowy lek nie dzia≈Ça\nCzas nauki nie wp≈Çywa na wyniki w nauce\n\nHipoteza Alternatywna (H_1): ‚ÄúEfekt ISTNIEJE‚Äù\n\nMoneta jest fa≈Çszywa\nLek dzia≈Ça\nWiƒôcej nauki poprawia oceny\n\n\nKluczowa zasada: Podtrzymujemy hipotezƒô zerowƒÖ (niewinno≈õƒá), chyba ≈ºe dane dostarczƒÖ mocnych dowod√≥w przeciwko niej ‚Äî ‚Äúponad wszelkƒÖ wƒÖtpliwo≈õƒá‚Äù w terminologii prawnej, lub ‚Äúp &lt; 0,05‚Äù w terminologii statystycznej.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#warto≈õƒá-p-p-value-tw√≥j-miernik-zaskoczenia",
    "href": "rozdzial1.html#warto≈õƒá-p-p-value-tw√≥j-miernik-zaskoczenia",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.11 Warto≈õƒá p (p-value): Tw√≥j ‚ÄúMiernik Zaskoczenia‚Äù",
    "text": "2.11 Warto≈õƒá p (p-value): Tw√≥j ‚ÄúMiernik Zaskoczenia‚Äù\nWarto≈õƒá p odpowiada na jedno konkretne pytanie:\n\n‚ÄúGdyby nic specjalnego siƒô nie dzia≈Ço (hipoteza zerowa jest prawdziwa), jak zaskakujƒÖce by≈Çyby nasze wyniki?‚Äù\n\n\nWarto≈õƒá p, p-warto≈õƒá, prawdopodobie≈Ñstwo testowe (ang. p-value, probability value) ‚Äì prawdopodobie≈Ñstwo uzyskania wynik√≥w testu co najmniej tak samo skrajnych, jak te zaobserwowane w rzeczywisto≈õci (w pr√≥bie badawczej), obliczone przy za≈Ço≈ºeniu, ≈ºe hipoteza zerowa (brak efektu, r√≥≈ºnicy, itp.) jest prawdziwa.\n\n\nTrzy Sposoby My≈õlenia o Warto≈õciach p\n\n1. Skala Zaskoczenia\n\np &lt; 0,01: Bardzo zaskakujƒÖce! (Mocne dowody przeciwko H_0)\np &lt; 0,05: Do≈õƒá zaskakujƒÖce (Umiarkowane dowody przeciwko H_0)\np &gt; 0,05: Niezbyt zaskakujƒÖce (NiewystarczajƒÖce dowody przeciwko H_0)\n\n\n\n2. Konkretny Przyk≈Çad: Podejrzana Moneta\nRzucasz monetƒÖ 10 razy i wypad≈Ço 8 or≈Ç√≥w. Jaka jest warto≈õƒá p?\nObliczenie: Je≈õli moneta by≈Çaby uczciwa, prawdopodobie≈Ñstwo uzyskania 8 lub wiƒôcej or≈Ç√≥w wynosi:\np = P(‚â•8 \\text{ or≈Ç√≥w w 10 rzutach}) \\approx 0.055 \\approx 5.5\\%\nP(X \\geq 8) = \\sum_{k=8}^{10} \\binom{10}{k} 0,5^{10} = \\frac{56}{1024} \\approx 0,0547\nInterpretacja: Jest 5,5% szans na uzyskanie tak ekstremalnych wynik√≥w z uczciwƒÖ monetƒÖ. To trochƒô nietypowe, ale nie jest to skrajnie nieprawdopodobny wynik.\n\n\n3. Formalna Definicja\nWarto≈õƒá p to prawdopodobie≈Ñstwo uzyskania wynik√≥w co najmniej tak ekstremalnych jak zaobserwowane, zak≈ÇadajƒÖc ≈ºe hipoteza zerowa jest prawdziwa.\n\n\n\n\n\n\nWarning\n\n\n\nCzƒôsty B≈ÇƒÖd: Warto≈õƒá p NIE jest prawdopodobie≈Ñstwem, ≈ºe hipoteza zerowa jest prawdziwa! Zak≈Çada ona, ≈ºe hipoteza zerowa jest prawdziwa i m√≥wi, jak nietypowe by≈Çyby twoje dane w tym ≈õwiecie (w kt√≥rym H_0 jest prawdziwa).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#b≈ÇƒÖd-rozumowania-prokuratorskiego-ostrze≈ºenie",
    "href": "rozdzial1.html#b≈ÇƒÖd-rozumowania-prokuratorskiego-ostrze≈ºenie",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.12 B≈ÇƒÖd Rozumowania Prokuratorskiego: Ostrze≈ºenie",
    "text": "2.12 B≈ÇƒÖd Rozumowania Prokuratorskiego: Ostrze≈ºenie\n\nWyja≈õnienie b≈Çƒôdu\nWyobra≈∫ sobie takƒÖ scenƒô w sƒÖdzie:\nProkurator: ‚ÄúJe≈õli oskar≈ºony by≈Çby niewinny, istnieje tylko 1% szans, ≈ºe znale≈∫liby≈õmy jego DNA na miejscu zbrodni. Znale≈∫li≈õmy jego DNA. Zatem istnieje 99% pewno≈õci, ≈ºe jest winny!‚Äù\nTo B≈ÅƒÑD! Prokurator pomyli≈Ç:\n\nP(Dow√≥d | Niewinny) = 0,01 ‚Üê To, co wiemy\nP(Niewinny | Dow√≥d) = ? ‚Üê To, co chcemy wiedzieƒá (ale nie mo≈ºemy tego wywnioskowaƒá z samej warto≈õci p!)\n\n\nGdy otrzymujemy p = 0,01, kuszƒÖce jest my≈õlenie:\n‚ùå ≈πLE: ‚ÄúJest tylko 1% szans, ≈ºe hipoteza zerowa jest prawdziwa‚Äù\n‚ùå ≈πLE: ‚ÄúJest 99% szans, ≈ºe nasze leczenie dzia≈Ça‚Äù\n‚úÖ DOBRZE: ‚ÄúJe≈õli hipoteza zerowa by≈Çaby prawdziwa, istnieje tylko 1% szans, ≈ºe zobaczyliby≈õmy tak ekstremalne dane‚Äù\n\n\nDlaczego to wa≈ºne: Prosty przyk≈Çad testu medycznego\nWyobra≈∫ sobie test na rzadkƒÖ chorobƒô, kt√≥ry jest dok≈Çadny w 99%:\n\nJe≈õli masz chorobƒô, test jest pozytywny w 99% przypadk√≥w\nJe≈õli nie masz choroby, test jest negatywny w 99% przypadk√≥w (czyli 1% wynik√≥w fa≈Çszywie pozytywnych)\n\nOto klucz: Za≈Ç√≥≈ºmy, ≈ºe tylko 1 na 1000 os√≥b faktycznie ma tƒô chorobƒô.\nPrzetestujmy 10 000 os√≥b:\n\n10 os√≥b ma chorobƒô ‚Üí 10 ma pozytywny wynik testu (w zaokrƒÖgleniu)\n9 990 os√≥b nie ma choroby ‚Üí oko≈Ço 100 ma pozytywny wynik przez pomy≈Çkƒô (1% z 9 990)\n≈ÅƒÖcznie pozytywnych test√≥w: 110\n\nJe≈õli tw√≥j test jest pozytywny, jakie jest prawdopodobie≈Ñstwo, ≈ºe rzeczywi≈õcie masz chorobƒô?\n\nTylko 10 ze 110 pozytywnych test√≥w to prawdziwe przypadki\nTo oko≈Ço 9%, nie 99%!\n\n\n\nAnalogia do bada≈Ñ naukowych\nTo samo dzieje siƒô w badaniach:\n\nGdy testujemy wiele hipotez (jak testowanie wielu potencjalnych lek√≥w)\nWiƒôkszo≈õƒá nie dzia≈Ça (jak wiƒôkszo≈õƒá ludzi nie ma rzadkiej choroby)\nNawet przy ‚Äúistotnych‚Äù wynikach (jak pozytywny test), wiƒôkszo≈õƒá odkryƒá mo≈ºe byƒá fa≈Çszywie pozytywna\n\n\n\n\n\n\n\nImportant\n\n\n\nWarto≈õƒá p m√≥wi ci, jak zaskakujƒÖce by≈Çyby twoje dane, GDYBY hipoteza zerowa by≈Ça prawdziwa. Nie m√≥wi ci o prawdopodobie≈Ñstwie, ≈ºe hipoteza zerowa JEST prawdziwa.\nPomy≈õl o tym tak: Prawdopodobie≈Ñstwo, ≈ºe ziemia bƒôdzie mokra, JE≈öLI pada≈Ço, jest zupe≈Çnie inne ni≈º prawdopodobie≈Ñstwo, ≈ºe pada≈Ço, JE≈öLI ziemia jest mokra ‚Äî ziemia mog≈Ça byƒá mokra od zraszacza!\nPamiƒôtaj: Warto≈õƒá p m√≥wi ci P(Dane | Hipoteza zerowa jest prawdziwa), nie P(Hipoteza zerowa jest prawdziwa | Dane). To tak r√≥≈ºne jak P(Mokra ziemia | Deszcz) i P(Deszcz | Mokra ziemia) ‚Äî ziemia mo≈ºe byƒá mokra od zraszacza!",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wprowadzenie-do-analizy-regresji-modelowanie-relacji-miƒôdzy-zmiennymi",
    "href": "rozdzial1.html#wprowadzenie-do-analizy-regresji-modelowanie-relacji-miƒôdzy-zmiennymi",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.13 Wprowadzenie do analizy regresji: Modelowanie relacji miƒôdzy zmiennymi",
    "text": "2.13 Wprowadzenie do analizy regresji: Modelowanie relacji miƒôdzy zmiennymi\nJednym z najwa≈ºniejszych narzƒôdzi w analizie statystycznej jest analiza regresji ‚Äî metoda zrozumienia i kwantyfikacji relacji miƒôdzy zmiennymi.\nPodstawowa idea jest prosta: Jak jedna rzecz odnosi siƒô do drugiej i czy mo≈ºemy u≈ºyƒá tej relacji do dokonywania przewidywa≈Ñ (np. jak liczba lat nauki wp≈Çywa na dochody?)?\n\nW jednym zdaniu: Regresja pomaga nam zrozumieƒá, jak r√≥≈ºne zjawiska sƒÖ ze sobƒÖ powiƒÖzane w skomplikowanym ≈õwiecie, gdzie wszystko wp≈Çywa na wszystko inne.\n\n\nCzym jest analiza regresji?\nWyobra≈∫ sobie, ≈ºe jeste≈õ ciekawy relacji miƒôdzy wykszta≈Çceniem a dochodem. Zauwa≈ºasz, ≈ºe ludzie z wiƒôkszym wykszta≈Çceniem zwykle zarabiajƒÖ wiƒôcej pieniƒôdzy, ale chcesz zrozumieƒá tƒô relacjƒô bardziej precyzyjnie:\n\nO ile ≈õrednio ka≈ºdy dodatkowy rok edukacji zwiƒôksza doch√≥d?\nJak silna jest ta relacja?\nCzy sƒÖ inne czynniki, kt√≥re powinni≈õmy rozwa≈ºyƒá?\nCzy mo≈ºemy przewidzieƒá prawdopodobny doch√≥d kogo≈õ, je≈õli znamy jego poziom wykszta≈Çcenia?\n\nAnaliza regresji w spos√≥b systematyczny odpowiada na te pytania ‚Äî szuka najlepiej dopasowanego opisu relacji miƒôdzy zmiennymi.\n\n\nZmienne i Zmienno≈õƒá\nZmienna to ka≈ºda charakterystyka, kt√≥ra mo≈ºe przyjmowaƒá r√≥≈ºne warto≈õci dla r√≥≈ºnych jednostek obserwacji. W naukach politycznych:\n\nJednostki analizy: Kraje, osoby, wybory, polityki, lata\nZmienne: PKB, preferencje wyborcze, wska≈∫nik demokracji, wystƒÖpienie konfliktu\n\n\nüí° M√≥wiƒÖc Prosto: Zmienna to wszystko, co siƒô zmienia. Gdyby wszyscy g≈Çosowali tak samo, ‚Äúpreferencje wyborcze‚Äù nie by≈Çyby zmiennƒÖ - by≈Çyby sta≈ÇƒÖ. Badamy zmienne, poniewa≈º chcemy zrozumieƒá, dlaczego rzeczy siƒô r√≥≈ºniƒÖ.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRozwa≈ºmy typowy nag≈Ç√≥wek prasowy przed wyborami: ‚ÄûPoparcie dla kandydata Kowalskiego siƒôga 68%.‚Äù Najprawdopodobniej wyciƒÖgniesz wniosek, ≈ºe Kowalski ma dobre perspektywy wyborcze‚Äînie gwarantowane zwyciƒôstwo, ale silnƒÖ pozycjƒô. Intuicyjnie rozumiesz, ≈ºe wy≈ºsze poparcie zwykle przek≈Çada siƒô na lepsze wyniki wyborcze, nawet je≈õli zwiƒÖzek ten nie jest doskona≈Çy.\nTa intuicyjna ocena ilustruje istotƒô analizy regresji. Wykorzysta≈Çe≈õ jednƒÖ informacjƒô (wska≈∫nik poparcia), aby przewidzieƒá inny wynik (sukces wyborczy). Co wiƒôcej, rozpozna≈Çe≈õ zar√≥wno zwiƒÖzek miƒôdzy tymi zmiennymi, jak i niepewno≈õƒá zwiƒÖzanƒÖ z twojƒÖ prognozƒÖ.\nChocia≈º takie nieformalne rozumowanie dobrze nam s≈Çu≈ºy w ≈ºyciu codziennym, ma istotne ograniczenia. O ile lepsze sƒÖ szanse Kowalskiego przy 68% poparciu w por√≥wnaniu do 58%? Co siƒô dzieje, gdy musimy jednocze≈õnie uwzglƒôdniƒá wiele czynnik√≥w‚Äîpoparcie, sytuacjƒô gospodarczƒÖ i status urzƒôdujƒÖcego kandydata? Jak pewni powinni≈õmy byƒá naszych prognoz?\nAnaliza regresji dostarcza systematycznych ram do odpowiedzi na te pytania. Przekszta≈Çca nasze intuicyjne rozumienie zwiƒÖzk√≥w w precyzyjne modele matematyczne, kt√≥re mo≈ºna testowaƒá i udoskonalaƒá. Dziƒôki analizie regresji badacze mogƒÖ:\n\nGenerowaƒá precyzyjne prognozy: Wyj≈õƒá poza og√≥lne oceny ku konkretnym liczbowym szacunkom‚Äîna przyk≈Çad przewidywaƒá nie tylko, ≈ºe Kowalski ‚Äûprawdopodobnie wygra‚Äù, ale oszacowaƒá oczekiwany procent g≈Ços√≥w i zakres prawdopodobnych wynik√≥w.\nOkre≈õliƒá, kt√≥re czynniki sƒÖ najwa≈ºniejsze: Ustaliƒá wzglƒôdne znaczenie r√≥≈ºnych zmiennych‚Äîbyƒá mo≈ºe odkrywajƒÖc, ≈ºe warunki gospodarcze wp≈ÇywajƒÖ na wybory silniej ni≈º wska≈∫niki poparcia.\nOkre≈õliƒá ilo≈õciowo niepewno≈õƒá prognoz: Dok≈Çadnie zmierzyƒá, jak pewni powinni≈õmy byƒá naszych przewidywa≈Ñ, rozr√≥≈ºniajƒÖc miƒôdzy niemal pewnymi wynikami a edukowanymi przypuszczeniami.\nTestowaƒá propozycje teoretyczne danymi empirycznymi: Oceniƒá, czy nasze przekonania o zwiƒÖzkach przyczynowo-skutkowych sprawdzajƒÖ siƒô, gdy testujemy je systematycznie na wielu obserwacjach.\n\n\nW istocie analiza regresji systematyzuje rozpoznawanie wzorc√≥w, kt√≥re wykonujemy intuicyjnie, dostarczajƒÖc narzƒôdzi do tego, aby nasze prognozy by≈Çy dok≈Çadniejsze, nasze por√≥wnania bardziej znaczƒÖce, a nasze wnioski bardziej wiarygodne.\n\n\n\nModel Podstawowy\nModel reprezentuje obiekt, osobƒô lub system w spos√≥b informatywny. Modele dzielƒÖ siƒô na reprezentacje fizyczne (takie jak modele architektoniczne) i abstrakcyjne (takie jak r√≥wnania matematyczne opisujƒÖce dynamikƒô atmosfery).\nRdze≈Ñ my≈õlenia statystycznego mo≈ºna wyraziƒá jako:\nY = f(X) + \\text{b≈ÇƒÖd}\nTo r√≥wnanie stwierdza, ≈ºe nasz wynik (Y) r√≥wna siƒô jakiej≈õ funkcji naszych predyktor√≥w (X), plus nieprzewidywalna zmienno≈õƒá.\nSk≈Çadniki:\n\nY = Zmienna zale≈ºna (zjawisko, kt√≥re chcemy wyja≈õniƒá)\nX = Zmienna(e) niezale≈ºna(e) (czynniki wyja≈õniajƒÖce)\nf() = ZwiƒÖzek funkcyjny (czƒôsto zak≈Çadamy liniowy)\nb≈ÇƒÖd (\\epsilon) = Niewyja≈õniona zmienno≈õƒá\n\n\nüí° Co To Naprawdƒô Oznacza: Mo≈ºna to por√≥wnaƒá do przepisu kulinarnego. Ocena z przedmiotu (Y) zale≈ºy od godzin nauki (X), ale nie doskonale. Dw√≥ch student√≥w uczƒÖcych siƒô 10 godzin mo≈ºe otrzymaƒá r√≥≈ºne oceny z powodu stresu przed egzaminem, wcze≈õniejszej wiedzy czy po prostu szczƒô≈õcia (sk≈Çadnik b≈Çƒôdu). Regresja znajduje ≈õredni zwiƒÖzek.\n\nTen model stanowi podstawƒô ca≈Çej analizy statystycznej - od prostych korelacji po z≈Ço≈ºone algorytmy uczenia maszynowego.\nRegresja pomaga odpowiedzieƒá na fundamentalne pytania takie jak:\n\nO ile edukacja zwiƒôksza uczestnictwo polityczne?\nJakie czynniki przewidujƒÖ sukces wyborczy?\nCzy instytucje demokratyczne promujƒÖ wzrost gospodarczy?\n\n\n\n\n\n\n\nPodstawowa idea: Rysowanie najlepszej linii przez punkty\n\nProsta regresja liniowa (Simple Linear Regression)\nZacznijmy od najprostszego przypadku: relacji miƒôdzy dwiema zmiennymi. Za≈Ç√≥≈ºmy, ≈ºe rysujemy wykszta≈Çcenie (lata nauki) na osi x i roczny doch√≥d na osi y dla 100 os√≥b. Zobaczyliby≈õmy chmurƒô punkt√≥w, a regresja znajduje prostƒÖ liniƒô, kt√≥ra najlepiej reprezentuje wzorzec w tych punktach.\nCo czyni liniƒô ‚ÄûnajlepszƒÖ‚Äù? Linia regresji minimalizuje ca≈ÇkowitƒÖ sumƒô kwadrat√≥w pionowych odleg≈Ço≈õci od wszystkich punkt√≥w do linii. Pomy≈õl o tym jako o znalezieniu linii, kt√≥ra tworzy najmniejszy ca≈Çkowity b≈ÇƒÖd predykcji.\nR√≥wnanie tej linii to: Y = a + bX + \\text{b≈ÇƒÖd}\nLub w naszym przyk≈Çadzie: \\text{Doch√≥d} = a + b \\times \\text{Wykszta≈Çcenie} + \\text{b≈ÇƒÖd}\nGdzie:\n\na (wyraz wolny/intercept) = przewidywany doch√≥d przy zerowym wykszta≈Çceniu\nb (nachylenie/slope) = zmiana dochodu na ka≈ºdy dodatkowy rok wykszta≈Çcenia\nb≈ÇƒÖd (e) = r√≥≈ºnica miƒôdzy rzeczywistym a przewidywanym dochodem\n\nInterpretacja wynik√≥w:\nJe≈õli nasza analiza znajduje: \\text{Doch√≥d} = 15000 + 4000 \\times \\text{Wykszta≈Çcenie}\nTo m√≥wi nam:\n\nKto≈õ z 0 latami wykszta≈Çcenia przewidywany jest na zarobki 15 000 z≈Ç\nKa≈ºdy dodatkowy rok wykszta≈Çcenia jest zwiƒÖzany z 4000 z≈Ç wiƒôkszym dochodem\nKto≈õ z 12 latami wykszta≈Çcenia przewidywany jest na zarobki: 15 000 + (4000 √ó 12) = 63 000 z≈Ç\nKto≈õ z 16 latami (licencjat) przewidywany jest na zarobki: 15 000 + (4000 √ó 16) = 79 000 z≈Ç\n\n\n\n\nZrozumienie relacji vs.¬†dowodzenie przyczynowo≈õci\nKluczowe rozr√≥≈ºnienie: regresja pokazuje zwiƒÖzek (association), niekoniecznie przyczynowo≈õƒá (causation). Nasza regresja wykszta≈Çcenie-doch√≥d pokazuje, ≈ºe sƒÖ powiƒÖzane, ale nie dowodzi, ≈ºe wykszta≈Çcenie powoduje wy≈ºszy doch√≥d. Inne wyja≈õnienia sƒÖ mo≈ºliwe:\n\nOdwrotna przyczynowo≈õƒá: Mo≈ºe bogatsze rodziny mogƒÖ sobie pozwoliƒá na wiƒôcej edukacji dla swoich dzieci\nWsp√≥lna przyczyna: Byƒá mo≈ºe inteligencja lub motywacja wp≈Çywa zar√≥wno na wykszta≈Çcenie, jak i doch√≥d\nZbieg okoliczno≈õci: W ma≈Çych pr√≥bach wzorce mogƒÖ pojawiƒá siƒô przez przypadek\n\nPrzyk≈Çad pozornej korelacji: Regresja mo≈ºe pokazaƒá, ≈ºe sprzeda≈º lod√≥w silnie przewiduje utopienia. Czy lody powodujƒÖ utopienia? Nie! Oba wzrastajƒÖ latem (wsp√≥lna przyczyna, confounding variable).\n\n\n\nRegresja wieloraka (Multiple Regression): Kontrolowanie innych czynnik√≥w\nRzeczywisto≈õƒá jest skomplikowana ‚Äî wiele czynnik√≥w wp≈Çywa na wyniki jednocze≈õnie. Regresja wieloraka pozwala nam badaƒá jednƒÖ relacjƒô, jednocze≈õnie ‚ÄûkontrolujƒÖc‚Äù lub ‚ÄûutrzymujƒÖc na sta≈Çym poziomie‚Äù inne zmienne.\n\nMoc kontroli statystycznej\nWracajƒÖc do wykszta≈Çcenia i dochodu, mo≈ºemy siƒô zastanawiaƒá: Czy efekt wykszta≈Çcenia wynika tylko z tego, ≈ºe wykszta≈Çceni ludzie sƒÖ zwykle z bogatszych rodzin lub mieszkajƒÖ w miastach? Regresja wieloraka mo≈ºe oddzieliƒá te efekty:\n\\text{Doch√≥d} = a + b_1 \\times \\text{Wykszta≈Çcenie} + b_2 \\times \\text{Wiek} + b_3 \\times \\text{Miasto} + b_4 \\times \\text{Doch√≥d rodzic√≥w} + \\text{b≈ÇƒÖd}\nTeraz b_1 reprezentuje efekt wykszta≈Çcenia po uwzglƒôdnieniu wieku, lokalizacji i pochodzenia rodzinnego. Je≈õli b_1 = 3000, oznacza to: ‚ÄûPor√≥wnujƒÖc osoby w tym samym wieku, lokalizacji i pochodzeniu rodzinnym, ka≈ºdy dodatkowy rok wykszta≈Çcenia jest zwiƒÖzany z 3000 z≈Ç wiƒôkszym dochodem.‚Äù\nPrzyk≈Çad demograficzny: P≈Çodno≈õƒá i wykszta≈Çcenie kobiet\nBadacze badajƒÖcy p≈Çodno≈õƒá mogƒÖ znale≈∫ƒá: \\text{Dzieci} = 4,5 - 0,3 \\times \\text{Wykszta≈Çcenie}\nTo sugeruje, ≈ºe ka≈ºdy rok wykszta≈Çcenia kobiet jest zwiƒÖzany z 0,3 mniej dzieci. Ale czy wykszta≈Çcenie jest przyczynƒÖ, czy wykszta≈Çcone kobiety r√≥≈ºniƒÖ siƒô w innych aspektach? DodajƒÖc kontrole:\n\\text{Dzieci} = a - 0,15 \\times \\text{Wykszta≈Çcenie} - 0,2 \\times \\text{Miasto} + 0,1 \\times \\text{Wykszta≈Çcenie mƒô≈ºa} - 0,4 \\times \\text{Dostƒôp do antykoncepcji}\nTeraz widzimy, ≈ºe zwiƒÖzek wykszta≈Çcenia jest s≈Çabszy (-0,15 zamiast -0,3) po uwzglƒôdnieniu zamieszkania w mie≈õcie i dostƒôpu do antykoncepcji. To sugeruje, ≈ºe czƒô≈õƒá pozornego efektu wykszta≈Çcenia dzia≈Ça przez te inne ≈õcie≈ºki.\n\n\n\nTypy zmiennych w regresji\n\nZmienna wynikowa (zale≈ºna)\nTo jest to, co pr√≥bujemy zrozumieƒá lub przewidzieƒá:\n\nDoch√≥d w naszym pierwszym przyk≈Çadzie\nLiczba dzieci w naszym przyk≈Çadzie p≈Çodno≈õci\nOczekiwana d≈Çugo≈õƒá ≈ºycia w badaniach zdrowotnych\nPrawdopodobie≈Ñstwo migracji w badaniach populacyjnych\n\n\n\nZmienne predykcyjne (niezale≈ºne)\nTo sƒÖ czynniki, kt√≥re wed≈Çug nas mogƒÖ wp≈Çywaƒá na wynik:\n\nIlo≈õciowe: Wiek, lata wykszta≈Çcenia, doch√≥d, odleg≈Ço≈õƒá\nJako≈õciowe (kategorialne): P≈Çeƒá, rasa, stan cywilny, region\nBinarne (dummy): Miasto/wie≈õ, zatrudniony/bezrobotny, ≈ºonaty/nie≈ºonaty\n\nObs≈Çuga zmiennych kategorialnych: Nie mo≈ºemy bezpo≈õrednio wstawiƒá ‚Äûreligii‚Äù do r√≥wnania. Zamiast tego tworzymy zmienne binarne:\n\nChrze≈õcijanin = 1 je≈õli chrze≈õcijanin, 0 w przeciwnym razie\nMuzu≈Çmanin = 1 je≈õli muzu≈Çmanin, 0 w przeciwnym razie\nBuddysta = 1 je≈õli buddysta, 0 w przeciwnym razie\n(Jedna kategoria staje siƒô grupƒÖ referencyjnƒÖ)\n\n\n\n\nR√≥≈ºne typy regresji dla r√≥≈ºnych wynik√≥w\nPodstawowa idea regresji dostosowuje siƒô do wielu sytuacji:\n\nRegresja liniowa\nDla wynik√≥w ilo≈õciowych (doch√≥d, wzrost, ci≈õnienie krwi): Y = a + b_1X_1 + b_2X_2 + ‚Ä¶ + \\text{b≈ÇƒÖd}\n\n\nRegresja logistyczna\nDla wynik√≥w binarnych (zmar≈Ç/prze≈ºy≈Ç, wyemigrowa≈Ç/zosta≈Ç, ≈ºonaty/nie≈ºonaty):\nZamiast przewidywaƒá wynik bezpo≈õrednio, przewidujemy prawdopodobie≈Ñstwo: \\log\\left(\\frac{p}{1-p}\\right) = a + b_1X_1 + b_2X_2 + ‚Ä¶\nGdzie p to prawdopodobie≈Ñstwo wystƒÖpienia zdarzenia.\nPrzyk≈Çad: Przewidywanie prawdopodobie≈Ñstwa migracji na podstawie wieku, wykszta≈Çcenia i stanu cywilnego. Model mo≈ºe stwierdziƒá, ≈ºe m≈Çodzi, wykszta≈Çceni, nie≈ºonaci ludzie majƒÖ 40% prawdopodobie≈Ñstwo migracji, podczas gdy starsi, mniej wykszta≈Çceni, ≈ºonaci ludzie majƒÖ tylko 5% prawdopodobie≈Ñstwo.\n\n\nRegresja Poissona\nDla wynik√≥w ‚Äúzliczeniowych‚Äù/count data (liczba dzieci, liczba wizyt u lekarza): \\log(\\text{oczekiwana liczba}) = a + b_1X_1 + b_2X_2 + ‚Ä¶\nPrzyk≈Çad: Modelowanie liczby dzieci na podstawie charakterystyk kobiet. Przydatne, poniewa≈º zapewnia, ≈ºe przewidywania nigdy nie sƒÖ ujemne (nie mo≈ºna mieƒá -0,5 dziecka!).\n\n\nAnaliza prze≈ºycia (model Coxa)/Regresja hazardu\nDo czego s≈Çu≈ºy: Przewidywanie kiedy co≈õ siƒô stanie, nie tylko czy siƒô stanie.\nProblem: Wyobra≈∫ sobie, ≈ºe badasz jak d≈Çugo trwajƒÖ ma≈Ç≈ºe≈Ñstwa. Obserwujesz 1000 par przez 10 lat, ale na koniec badania: - 400 par siƒô rozwiod≈Ço (wiesz dok≈Çadnie kiedy) - 600 par jest nadal w ma≈Ç≈ºe≈Ñstwie (nie wiesz czy/kiedy siƒô rozwiodƒÖ)\nZwyk≈Ça regresja nie radzi sobie z tym problemem ‚Äúniekompletnej historii‚Äù ‚Äî te 600 trwajƒÖcych ma≈Ç≈ºe≈Ñstw zawiera cenne informacje, ale nie znamy jeszcze ich zako≈Ñczenia.\nJak pomagajƒÖ modele Coxa: Zamiast pr√≥bowaƒá przewidzieƒá dok≈Çadny moment, skupiajƒÖ siƒô na ryzyku wzglƒôdnym ‚Äî kto ma wiƒôkszƒÖ szansƒô na wcze≈õniejsze do≈õwiadczenie zdarzenia. To jak pytanie ‚ÄúW dowolnym momencie, kto jest bardziej nara≈ºony?‚Äù zamiast ‚ÄúDok≈Çadnie kiedy to siƒô stanie?‚Äù\nZastosowania praktyczne: - Badania medyczne: Kto szybciej reaguje na leczenie? - Biznes: Kt√≥rzy klienci wcze≈õniej rezygnujƒÖ z subskrypcji? - Nauki spo≈Çeczne: Jakie czynniki powodujƒÖ, ≈ºe wydarzenia ≈ºyciowe nastƒôpujƒÖ wcze≈õniej/p√≥≈∫niej?\n\n\n\n\nInterpretacja wynik√≥w regresji\n\nWsp√≥≈Çczynniki\nWsp√≥≈Çczynnik m√≥wi nam o oczekiwanej zmianie wyniku przy wzro≈õcie predyktora o jednƒÖ jednostkƒô, przy zachowaniu sta≈Ço≈õci innych zmiennych.\nPrzyk≈Çady interpretacji:\nRegresja liniowa dla dochodu:\n\n‚ÄûKa≈ºdy dodatkowy rok wykszta≈Çcenia jest zwiƒÖzany z 3500 z≈Ç wy≈ºszym rocznym dochodem, kontrolujƒÖc wiek i do≈õwiadczenie‚Äù\n\nRegresja logistyczna dla ≈õmiertelno≈õci niemowlƒÖt:\n\n‚ÄûKa≈ºda dodatkowa wizyta prenatalna jest zwiƒÖzana z 15% ni≈ºszymi szansami ≈õmierci niemowlƒôcia, kontrolujƒÖc wiek i wykszta≈Çcenie matki‚Äù\n\nRegresja wieloraka dla oczekiwanej d≈Çugo≈õci ≈ºycia:\n\n‚ÄûKa≈ºde 1000 USD wzrostu PKB per capita jest zwiƒÖzane z 0,4 roku d≈Çu≈ºszƒÖ oczekiwanƒÖ d≈Çugo≈õciƒÖ ≈ºycia, po kontroli wykszta≈Çcenia i dostƒôpu do opieki zdrowotnej‚Äù\n\n\n\nIstotno≈õƒá statystyczna\nRegresja testuje r√≥wnie≈º, czy relacje mogƒÖ wynikaƒá z przypadku:\n\nwarto≈õƒá p &lt; 0,05: Relacja nieprawdopodobna z powodu przypadku (statystycznie istotna)\nwarto≈õƒá p &gt; 0,05: Relacja mo≈ºe byƒá prawdopodobnie losowƒÖ zmienno≈õciƒÖ\n\n\nAle pamiƒôtaj: Istotno≈õƒá statystyczna ‚â† praktyczne znaczenie (‚Äúpraktyczna istotno≈õƒá‚Äù). Przy du≈ºych pr√≥bach malutkie efekty stajƒÖ siƒô ‚Äûistotne‚Äù.\n\n\n\nPrzedzia≈Çy ufno≈õci dla wsp√≥≈Çczynnik√≥w\nTak jak mamy przedzia≈Çy ufno≈õci dla ≈õrednich lub propocji, mamy je dla wsp√≥≈Çczynnik√≥w regresji:\n‚ÄûEfekt wykszta≈Çcenia na doch√≥d wynosi 3500 z≈Ç rocznie, 95% CI: [2800 z≈Ç, 4200 z≈Ç]‚Äù\nTo oznacza, ≈ºe jeste≈õmy 95% pewni, ≈ºe prawdziwy efekt mie≈õci siƒô miƒôdzy 2800 z≈Ç a 4200 z≈Ç.\n\n\nR-kwadrat: Jak dobrze model pasuje do danych?\nR^2 (R-kwadrat) mierzy proporcjƒô zmienno≈õci wyniku wyja≈õnionƒÖ przez predyktory:\n\nR^2 = 0: Predyktory nic nie wyja≈õniajƒÖ\nR^2 = 1: Predyktory wyja≈õniajƒÖ wszystko\nR^2 = 0,3: Predyktory wyja≈õniajƒÖ 30% zmienno≈õci\n\nPrzyk≈Çad: Model dochodu z tylko wykszta≈Çceniem mo≈ºe mieƒá R^2 = 0,15 (wykszta≈Çcenie wyja≈õnia 15% zmienno≈õci dochodu). Dodanie wieku, do≈õwiadczenia i lokalizacji mo≈ºe zwiƒôkszyƒá R^2 do 0,35 (razem wyja≈õniajƒÖ 35%).\n\n\n\n\n\n\nZa≈Ço≈ºenia i ograniczenia\n\n\n\nRegresja opiera siƒô na za≈Ço≈ºeniach, kt√≥re mogƒÖ nie byƒá spe≈Çnione:\n\nEgzogeniczno≈õƒá (brak ukrytych zale≈ºno≈õci)\nNajwa≈ºniejsze za≈Ço≈ºenie: predyktory nie mogƒÖ byƒá skorelowane z b≈Çƒôdami. Pro≈õciej m√≥wiƒÖc, nie powinny istnieƒá ukryte czynniki wp≈ÇywajƒÖce jednocze≈õnie na zmienne obja≈õniajƒÖce i wynik.\nPrzyk≈Çad: BadajƒÖc wp≈Çyw edukacji na doch√≥d, ale pomijajƒÖc ‚Äúzdolno≈õci‚Äù, otrzymasz obciƒÖ≈ºone wyniki - zdolno≈õci wp≈ÇywajƒÖ zar√≥wno na poziom wykszta≈Çcenia, jak i doch√≥d. To za≈Ço≈ºenie zapisujemy jako: E[\\varepsilon | X] = 0\nDlaczego to kluczowe: Bez tego wszystkie twoje wsp√≥≈Çczynniki sƒÖ b≈Çƒôdne, nawet przy milionach obserwacji!\n\n\nLiniowo≈õƒá\nZak≈Çada zwiƒÖzki prostoliniowe. A co je≈õli wp≈Çyw edukacji na doch√≥d jest silniejszy na wy≈ºszych poziomach? Mo≈ºemy dodaƒá cz≈Çony wielomianowe: \\text{Doch√≥d} = a + b_1 \\times \\text{Edukacja} + b_2 \\times \\text{Edukacja}^2\n\n\nNiezale≈ºno≈õƒá\nZak≈Çada, ≈ºe obserwacje sƒÖ niezale≈ºne. Ale cz≈Çonkowie rodziny mogƒÖ byƒá podobni, powtarzane pomiary tej samej osoby sƒÖ powiƒÖzane, a sƒÖsiedzi mogƒÖ na siebie wp≈Çywaƒá. Specjalne metody radzƒÖ sobie z tymi zale≈ºno≈õciami.\n\n\nHomoskedastyczno≈õƒá\nZak≈Çada sta≈ÇƒÖ wariancjƒô b≈Çƒôd√≥w. Ale b≈Çƒôdy predykcji mogƒÖ byƒá wiƒôksze dla os√≥b o wysokich dochodach ni≈º niskich. Wykresy diagnostyczne pomagajƒÖ to wykryƒá.\n\n\nNormalno≈õƒá\nZak≈Çada, ≈ºe b≈Çƒôdy majƒÖ rozk≈Çad normalny. Wa≈ºne dla ma≈Çych pr√≥b i test√≥w hipotez, mniej krytyczne dla du≈ºych pr√≥b.\nUwaga: Pierwsze za≈Ço≈ºenie (egzogeniczno≈õƒá) dotyczy otrzymania poprawnej odpowiedzi. Pozosta≈Çe dotyczƒÖ g≈Ç√≥wnie precyzji i wnioskowania statystycznego. Naruszenie egzogeniczno≈õci oznacza, ≈ºe model jest fundamentalnie b≈Çƒôdny; naruszenie pozosta≈Çych oznacza, ≈ºe przedzia≈Çy ufno≈õci i p-warto≈õci mogƒÖ byƒá niedok≈Çadne.\n\n\n\n\n\n\n\n\n\nCzƒôste pu≈Çapki statystyczne\n\n\n\n\nEndogeniczno≈õƒá (obciƒÖ≈ºenie pominiƒôtƒÖ zmiennƒÖ): Zapominanie o ukrytych czynnikach wp≈ÇywajƒÖcych zar√≥wno na X jak i Y, co narusza fundamentalne za≈Ço≈ºenie egzogeniczno≈õci. Przyk≈Çad: Badanie edukacja‚Üídoch√≥d bez uwzglƒôdnienia zdolno≈õci.\nSymultaniczno≈õƒá/Odwrotna przyczynowo≈õƒá: Gdy X i Y okre≈õlajƒÖ siƒô wzajemnie w tym samym czasie. Prosta regresja zak≈Çada jednokierunkowƒÖ przyczynowo≈õƒá, ale rzeczywisto≈õƒá czƒôsto jest dwukierunkowa. Przyk≈Çad: Cena wp≈Çywa na popyt ORAZ popyt wp≈Çywa na cenƒô jednocze≈õnie.\nZmienne zak≈Ç√≥cajƒÖce (confounding): Nieuwzglƒôdnienie zmiennych wp≈ÇywajƒÖcych zar√≥wno na predyktor jak i wynik, co prowadzi do pozornych zale≈ºno≈õci. Przyk≈Çad: Sprzeda≈º lod√≥w koreluje z utoniƒôciami (oba powodowane przez lato).\nB≈ÇƒÖd selekcji: Nielosowe pr√≥by systematycznie wykluczajƒÖce pewne grupy, uniemo≈ºliwiajƒÖce generalizacjƒô. Przyk≈Çad: Badanie u≈ºycia internetu tylko w≈õr√≥d posiadaczy smartfon√≥w.\nB≈ÇƒÖd ekologiczny: Zak≈Çadanie, ≈ºe wzorce grupowe dotyczƒÖ jednostek. Przyk≈Çad: Bogate kraje majƒÖ ni≈ºszƒÖ dzietno≈õƒá ‚â† bogaci ludzie majƒÖ mniej dzieci.\nP-hacking (drƒÖ≈ºenie danych): Testowanie wielu hipotez a≈º do znalezienia istotno≈õci, lub modyfikowanie analizy a≈º p &lt; 0,05. Przy 20 testach spodziewasz siƒô 1 fa≈Çszywego wyniku przez przypadek!\nPrzeuczenie (overfitting): Budowanie modelu zbyt z≈Ço≈ºonego dla twoich danych - idealny na danych treningowych, bezu≈ºyteczny do predykcji. Pamiƒôtaj: Z wystarczajƒÖcƒÖ liczbƒÖ parametr√≥w mo≈ºesz dopasowaƒá s≈Çonia.\nB≈ÇƒÖd przetrwania: Analizowanie tylko ‚Äúocala≈Çych‚Äù ignorujƒÖc pora≈ºki. Przyk≈Çad: Badanie firm sukcesu pomijajƒÖc te, kt√≥re zbankrutowa≈Çy.\nNadmierna generalizacja: Rozszerzanie wniosk√≥w poza badanƒÖ populacjƒô, okres czasu lub kontekst. Przyk≈Çad: Wyniki z ameryka≈Ñskich student√≥w ‚â† uniwersalne zachowanie ludzkie.\n\nPamiƒôtaj: Pierwsze trzy to formy endogeniczno≈õci - naruszajƒÖ E[\\varepsilon|X]=0 i sprawiajƒÖ, ≈ºe wsp√≥≈Çczynniki sƒÖ fundamentalnie b≈Çƒôdne. Pozosta≈Çe czyniƒÖ wyniki mylƒÖcymi lub niereprezentatywnymi.\n\n\n\n\n\n\nZastosowania w demografii\n\nAnaliza p≈Çodno≈õci\nZrozumienie, jakie czynniki wp≈ÇywajƒÖ na decyzje o p≈Çodno≈õci: \\text{Dzieci} = f(\\text{Wykszta≈Çcenie, Doch√≥d, Miasto, Religia, Antykoncepcja, ‚Ä¶})\nPomaga zidentyfikowaƒá d≈∫wignie polityczne dla kraj√≥w zaniepokojonych wysokƒÖ lub niskƒÖ p≈Çodno≈õciƒÖ.\n\n\nModelowanie ≈õmiertelno≈õci\nPrzewidywanie oczekiwanej d≈Çugo≈õci ≈ºycia lub ryzyka ≈õmiertelno≈õci: \\text{Ryzyko ≈õmiertelno≈õci} = f(\\text{Wiek, P≈Çeƒá, Palenie, Wykszta≈Çcenie, Dostƒôp do opieki zdrowotnej, ‚Ä¶})\nU≈ºywane przez firmy ubezpieczeniowe, urzƒôdnik√≥w zdrowia publicznego i badaczy.\n\n\nPrzewidywanie migracji\nZrozumienie, kto migruje i dlaczego: P(\\text{Migracja}) = f(\\text{Wiek, Wykszta≈Çcenie, Zatrudnienie, Wiƒôzi rodzinne, Odleg≈Ço≈õƒá, ‚Ä¶})\nPomaga przewidywaƒá przep≈Çywy populacji i planowaƒá zmiany demograficzne.\n\n\nMa≈Ç≈ºe≈Ñstwo i rozw√≥d\nAnalizowanie formowania i rozpadu zwiƒÖzk√≥w: P(\\text{Rozw√≥d}) = f(\\text{Wiek przy ma≈Ç≈ºe≈Ñstwie, Dopasowanie wykszta≈Çcenia, Doch√≥d, Dzieci, Czas trwania, ‚Ä¶})\nInformuje politykƒô spo≈ÇecznƒÖ i us≈Çugi wsparcia.\n\n\n\nPowszechne pu≈Çapki i jak ich unikaƒá\n\nPrzeuczenie (Overfitting)\nW≈ÇƒÖczenie zbyt wielu predyktor√≥w mo≈ºe sprawiƒá, ≈ºe model idealnie pasuje do twojej pr√≥by, ale zawiedzie z nowymi danymi. Jak zapamiƒôtywanie odpowiedzi na egzamin zamiast zrozumienia pojƒôƒá.\nRozwiƒÖzanie: U≈ºyj prostszych modeli, walidacji krzy≈ºowej lub zarezerwuj niekt√≥re dane do testowania.\n\n\nWsp√≥≈Çliniowo≈õƒá (Multicollinearity)\nGdy predyktory sƒÖ silnie skorelowane (np. lata wykszta≈Çcenia i poziom stopnia), model nie mo≈ºe oddzieliƒá ich efekt√≥w.\nRozwiƒÖzanie: Wybierz jednƒÖ zmiennƒÖ lub po≈ÇƒÖcz je w indeks.\n\n\nObciƒÖ≈ºenie pominiƒôtej zmiennej (Omitted Variable Bias)\nPominiƒôcie wa≈ºnych zmiennych mo≈ºe sprawiƒá, ≈ºe inne efekty wydajƒÖ siƒô silniejsze lub s≈Çabsze ni≈º naprawdƒô sƒÖ.\nPrzyk≈Çad: Relacja miƒôdzy sprzeda≈ºƒÖ lod√≥w a wska≈∫nikami przestƒôpczo≈õci znika, gdy kontrolujesz temperaturƒô.\n\n\nEkstrapolacja\nU≈ºywanie modelu poza zakresem obserwowanych danych.\nPrzyk≈Çad: Je≈õli twoje dane obejmujƒÖ wykszta≈Çcenie od 0-20 lat, nie przewiduj dochodu dla kogo≈õ z 30 latami wykszta≈Çcenia.\n\n\n\nIntuicje\nPomy≈õl o regresji jako o wyrafinowanej technice u≈õredniania:\n\nProsta ≈õrednia: ‚Äû≈öredni doch√≥d wynosi 50 000 z≈Ç‚Äù\n≈örednia warunkowa: ‚Äû≈öredni doch√≥d dla absolwent√≥w uczelni wynosi 70 000 z≈Ç‚Äù\nRegresja: ‚Äû≈öredni doch√≥d dla 35-letnich absolwent√≥w uczelni w obszarach miejskich wynosi 78 000 z≈Ç‚Äù\n\nKa≈ºda dodana zmienna czyni nasze przewidywanie bardziej konkretnym i (miejmy nadziejƒô) dok≈Çadniejszym.\n\n\nRegresja w praktyce: Kompletny przyk≈Çad\nPytanie badawcze: Jakie czynniki wp≈ÇywajƒÖ na wiek przy pierwszym porodzie?\nDane: Badanie 1000 kobiet, kt√≥re mia≈Çy co najmniej jedno dziecko\nZmienne:\n\nWynik: Wiek przy pierwszym porodzie (lata)\nPredyktory: Wykszta≈Çcenie (lata), Miasto (0/1), Doch√≥d (tysiƒÖce), Religijno≈õƒá (0/1)\n\nWynik prostej regresji: \\text{Wiek przy pierwszym porodzie} = 18 + 0,8 \\times \\text{Wykszta≈Çcenie}\nInterpretacja: Ka≈ºdy rok wykszta≈Çcenia zwiƒÖzany z 0,8 roku p√≥≈∫niejszym pierwszym porodem.\nWynik regresji wielorakiej: \\text{Wiek przy pierwszym porodzie} = 16 + 0,5 \\times \\text{Wykszta≈Çcenie} + 2 \\times \\text{Miasto} + 0,03 \\times \\text{Doch√≥d} - 1,5 \\times \\text{Religijno≈õƒá}\nInterpretacja:\n\nEfekt wykszta≈Çcenia zredukowany, ale nadal dodatni (0,5 roku na rok wykszta≈Çcenia)\nKobiety miejskie majƒÖ pierwsze porody 2 lata p√≥≈∫niej\nKa≈ºde 1000 z≈Ç dochodu zwiƒÖzane z 0,03 roku (11 dni) p√≥≈∫niej\nReligijne kobiety majƒÖ pierwsze porody 1,5 roku wcze≈õniej\nR^2 = 0,42 (model wyja≈õnia 42% zmienno≈õci)\n\nTen bogatszy model pomaga nam zrozumieƒá, ≈ºe efekt wykszta≈Çcenia czƒô≈õciowo dzia≈Ça przez zamieszkanie w mie≈õcie i doch√≥d.\n\n\n\n\n\n\nWarning\n\n\n\nRegresja jest bramƒÖ do zaawansowanego modelowania statystycznego. Gdy zrozumiesz podstawowƒÖ koncepcjƒô ‚Äî u≈ºywanie zmiennych do przewidywania wynik√≥w i kwantyfikowania relacji ‚Äî mo≈ºesz eksplorowaƒá:\n\nEfekty interakcji: Gdy efekt jednej zmiennej zale≈ºy od innej\nRelacje nieliniowe: Krzywe, progi i z≈Ço≈ºone wzorce\nModele wielopoziomowe: Uwzglƒôdnianie zgrupowanych danych (uczniowie w szko≈Çach, ludzie w dzielnicach)\nRegresja szereg√≥w czasowych: Analizowanie zmian w czasie\nRozszerzenia uczenia maszynowego: Lasy losowe, sieci neuronowe i wiƒôcej\n\nKluczowy wglƒÖd pozostaje: Pr√≥bujemy zrozumieƒá, jak rzeczy odnoszƒÖ siƒô do siebie w systematyczny, kwantyfikowalny spos√≥b.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#jako≈õƒá-i-≈∫r√≥d≈Ça-danych",
    "href": "rozdzial1.html#jako≈õƒá-i-≈∫r√≥d≈Ça-danych",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.14 Jako≈õƒá i ≈∫r√≥d≈Ça danych",
    "text": "2.14 Jako≈õƒá i ≈∫r√≥d≈Ça danych\n≈ªadna analiza nie jest lepsza ni≈º dane, na kt√≥rych siƒô opiera. Zrozumienie problem√≥w jako≈õci danych jest kluczowe dla bada≈Ñ demograficznych i spo≈Çecznych.\n\nWymiary jako≈õci danych\nDok≈Çadno≈õƒá (Accuracy): Jak blisko pomiar√≥w sƒÖ prawdziwe warto≈õci?\nPrzyk≈Çad: Raportowanie wieku czƒôsto pokazuje ‚Äûskupianie‚Äù na okrƒÖg≈Çych liczbach (30, 40, 50), poniewa≈º ludzie zaokrƒÖglajƒÖ sw√≥j wiek.\nKompletno≈õƒá (Completeness): Jaka proporcja populacji jest objƒôta?\nPrzyk≈Çad: Kompletno≈õƒá rejestracji urodze≈Ñ r√≥≈ºni siƒô znacznie:\n\nKraje rozwiniƒôte: &gt;99%\nNiekt√≥re kraje rozwijajƒÖce siƒô: &lt;50%\n\nAktualno≈õƒá (Timeliness): Jak aktualne sƒÖ dane?\nPrzyk≈Çad: Spis przeprowadzany co 10 lat staje siƒô coraz bardziej nieaktualny, szczeg√≥lnie w szybko zmieniajƒÖcych siƒô obszarach.\nSp√≥jno≈õƒá (Consistency): Czy definicje i metody sƒÖ stabilne w czasie i przestrzeni?\nPrzyk≈Çad: Definicja ‚Äûmiasta‚Äù r√≥≈ºni siƒô miƒôdzy krajami, utrudniajƒÖc miƒôdzynarodowe por√≥wnania.\nDostƒôpno≈õƒá (Accessibility): Czy badacze i decydenci mogƒÖ faktycznie u≈ºywaƒá danych?\n\n\nPowszechne ≈∫r√≥d≈Ça danych w demografii\nSpis powszechny (Census): Kompletne wyliczenie populacji\nZalety:\n\nKompletne pokrycie (w teorii)\nDane dla ma≈Çych obszar√≥w dostƒôpne\nPunkt odniesienia dla innych oszacowa≈Ñ\n\nWady:\n\nDrogie i rzadkie\nNiekt√≥re populacje trudne do policzenia\nOgraniczone zbierane zmienne\n\nRejestry urzƒôdu stanu cywilnego (Vital Registration): CiƒÖg≈Çe rejestrowanie urodze≈Ñ, zgon√≥w, ma≈Ç≈ºe≈Ñstw\nZalety:\n\nCiƒÖg≈Çe i aktualne\nWym√≥g prawny zapewnia zgodno≈õƒá\nInformacje o medycznej przyczynie ≈õmierci\n\nWady:\n\nPokrycie r√≥≈ºni siƒô wed≈Çug poziomu rozwoju\nJako≈õƒá kodowania przyczyny ≈õmierci siƒô r√≥≈ºni\nOp√≥≈∫niona rejestracja powszechna w niekt√≥rych obszarach\n\nBadania pr√≥bkowe (Sample Surveys): Szczeg√≥≈Çowe dane z podzbioru populacji\nPrzyk≈Çady:\n\nBadania demograficzne i zdrowotne (DHS)\nAmeryka≈Ñskie Badanie Spo≈Çeczno≈õci (ACS)\nBadania Si≈Çy Roboczej (np. BAEL GUS)\n\nZalety:\n\nMo≈ºna zbieraƒá szczeg√≥≈Çowe informacje\nCzƒôstsze ni≈º spis\nMo≈ºna skupiƒá siƒô na konkretnych tematach\n\nWady:\n\nObecny b≈ÇƒÖd pr√≥bkowania\nMa≈Çe obszary niereprezentowane\nObciƒÖ≈ºenie odpowiedzi mo≈ºe zmniejszyƒá jako≈õƒá\n\nRejestry administracyjne (Administrative Records): Dane zbierane do cel√≥w niestatystycznych\nPrzyk≈Çady:\n\nRejestry podatkowe\nZapisy szkolne\nRoszczenia ubezpieczenia zdrowotnego\nDane telefonii kom√≥rkowej\n\nZalety:\n\nJu≈º zebrane (bez dodatkowego obciƒÖ≈ºenia)\nCzƒôsto kompletne dla objƒôtej populacji\nCiƒÖgle aktualizowane\n\nWady:\n\nPokrycie mo≈ºe byƒá selektywne\nDefinicje mogƒÖ nie odpowiadaƒá potrzebom badawczym\nDostƒôp czƒôsto ograniczony\n\n\n\nProblemy jako≈õci danych specyficzne dla demografii\nSkupianie wieku (Age Heaping): Tendencja do raportowania wieku ko≈ÑczƒÖcego siƒô na 0 lub 5\nWykrywanie: Oblicz Indeks Whipple‚Äôa lub Indeks Myersa\nWp≈Çyw: Wp≈Çywa na wska≈∫niki specyficzne dla wieku i projekcje\nPreferencja cyfr (Digit Preference): Raportowanie niekt√≥rych ko≈Ñcowych cyfr czƒô≈õciej ni≈º innych\nPrzyk≈Çad: Wagi urodzeniowe czƒôsto raportowane jako 3000g, 3500g zamiast dok≈Çadnych warto≈õci\nObciƒÖ≈ºenie przypominania (Recall Bias): Trudno≈õƒá dok≈Çadnego przypominania przesz≈Çych wydarze≈Ñ\nPrzyk≈Çad: ‚ÄûIle razy odwiedzi≈Çe≈õ lekarza w zesz≈Çym roku?‚Äù Czƒôsto niedoszacowane dla czƒôstych odwiedzajƒÖcych, przeszacowane dla rzadkich odwiedzajƒÖcych.\nRaportowanie przez pe≈Çnomocnika (Proxy Reporting): Informacje dostarczane przez kogo≈õ innego\nWyzwanie: G≈Çowa gospodarstwa domowego raportujƒÖca za wszystkich cz≈Çonk√≥w mo≈ºe nie znaƒá dok≈Çadnego wieku lub wykszta≈Çcenia ka≈ºdego",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wzglƒôdy-etyczne-w-demografii-statystycznej",
    "href": "rozdzial1.html#wzglƒôdy-etyczne-w-demografii-statystycznej",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.15 Wzglƒôdy etyczne w demografii statystycznej",
    "text": "2.15 Wzglƒôdy etyczne w demografii statystycznej\nStatystyka to nie tylko liczby ‚Äî dotyczy prawdziwych ludzi i ma prawdziwe konsekwencje.\n\n≈öwiadoma zgoda\nUczestnicy powinni zrozumieƒá:\n\nCel zbierania danych\nJak dane bƒôdƒÖ u≈ºywane\nRyzyka i korzy≈õci\nIch prawo do odmowy lub wycofania siƒô\n\nWyzwanie w demografii: Uczestnictwo w spisie jest czƒôsto obowiƒÖzkowe, co rodzi pytania etyczne o zgodƒô.\n\n\nPoufno≈õƒá i prywatno≈õƒá\nStatystyczna kontrola ujawniania: Ochrona to≈ºsamo≈õci jednostek w opublikowanych danych\nMetody obejmujƒÖ:\n\nT≈Çumienie ma≈Çych kom√≥rek (np. ‚Äû&lt;5‚Äù zamiast ‚Äû2‚Äù)\nAgregacja geograficzna\n\nPrzyk≈Çad: W tabeli zawodu wed≈Çug wieku wed≈Çug p≈Çci dla ma≈Çego miasta mo≈ºe byƒá tylko jedna lekarka w wieku 60-65 lat, co czyni jƒÖ identyfikowalnƒÖ.\n\n\nReprezentacja i uczciwo≈õƒá\nKto jest liczony?: Decyzje o tym, kogo uwzglƒôdniƒá, wp≈ÇywajƒÖ na reprezentacjƒô\n\nWiƒô≈∫niowie: Gdzie sƒÖ liczeni ‚Äî lokalizacja wiƒôzienia czy adres domowy?\nBezdomni: Jak zapewniƒá pokrycie?\nNieudokumentowani imigranci: Uwzglƒôdniƒá czy wykluczyƒá?\n\nPrywatno≈õƒá r√≥≈ºnicowa (Differential Privacy): Matematyczna struktura ochrony prywatno≈õci przy zachowaniu u≈ºyteczno≈õci statystycznej\nKompromis: Wiƒôksza ochrona prywatno≈õci = mniej dok≈Çadne statystyki\n\n\nNiew≈Ça≈õciwe u≈ºycie statystyk\nWybieranie wisienek (Cherry-Picking): Wybieranie tylko korzystnych wynik√≥w\nPrzyk≈Çad: Raportowanie spadku ciƒÖ≈º nastolatek od roku szczytowego zamiast pokazywania pe≈Çnego trendu\nP-Hacking: Manipulowanie analizƒÖ w celu osiƒÖgniƒôcia istotno≈õci statystycznej\nB≈ÇƒÖd ekologiczny: Wnioskowanie relacji indywidualnych z danych grupowych\nPrzyk≈Çad: Powiaty z wiƒôkszƒÖ liczbƒÖ imigrant√≥w majƒÖ wy≈ºsze ≈õrednie dochody ‚â† imigranci majƒÖ wy≈ºsze dochody\n\n\nOdpowiedzialne raportowanie\nKomunikacja niepewno≈õci: Zawsze raportuj przedzia≈Çy ufno≈õci lub marginesy b≈Çƒôdu\nDostarczanie kontekstu: Uwzglƒôdnij odpowiednie grupy por√≥wnawcze i trendy historyczne\nUznanie ogranicze≈Ñ: Jasno okre≈õl, co dane mogƒÖ i nie mogƒÖ pokazaƒá",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#powszechne-nieporozumienia-w-statystyce",
    "href": "rozdzial1.html#powszechne-nieporozumienia-w-statystyce",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.16 Powszechne nieporozumienia w statystyce",
    "text": "2.16 Powszechne nieporozumienia w statystyce\nZrozumienie, czym statystyka NIE jest, jest r√≥wnie wa≈ºne jak zrozumienie, czym jest.\n\nNieporozumienie 1: ‚ÄûStatystyki mogƒÖ udowodniƒá wszystko‚Äù\nRzeczywisto≈õƒá: Statystyki mogƒÖ dostarczyƒá tylko dowod√≥w, nigdy absolutnego dowodu. A w≈Ça≈õciwa statystyka, uczciwie zastosowana, znacznie ogranicza wnioski.\nPrzyk≈Çad: Badanie znajduje korelacjƒô miƒôdzy sprzeda≈ºƒÖ lod√≥w a utopieniami. Statystyka nie ‚Äûdowodzi‚Äù, ≈ºe lody powodujƒÖ utopienia ‚Äî oba sƒÖ zwiƒÖzane z letniƒÖ pogodƒÖ.\n\n\nNieporozumienie 2: ‚ÄûWiƒôksze pr√≥by sƒÖ zawsze lepsze‚Äù\nRzeczywisto≈õƒá: Poza pewnym punktem wiƒôksze pr√≥by dodajƒÖ niewiele precyzji, ale mogƒÖ dodaƒá obciƒÖ≈ºenie.\nPrzyk≈Çad: Ankieta online z 1 milionem odpowiedzi mo≈ºe byƒá mniej dok≈Çadna ni≈º pr√≥ba probabilistyczna 1000 os√≥b z powodu obciƒÖ≈ºenia samoselekcji.\nMalejƒÖce zyski:\n\nn = 100: Margines b≈Çƒôdu \\approx 10%\nn = 1000: Margines b≈Çƒôdu \\approx 3,2%\nn = 10 000: Margines b≈Çƒôdu \\approx 1%\nn = 100 000: Margines b≈Çƒôdu \\approx 0,32%\n\nSkok z 10 000 do 100 000 ledwo poprawia precyzjƒô, ale kosztuje 10\\times wiƒôcej.\n\n\nNieporozumienie 3: ‚ÄúIstotno≈õƒá statystyczna = Praktyczne znaczenie‚Äù\nRzeczywisto≈õƒá: Przy du≈ºych pr√≥bach malutkie r√≥≈ºnice stajƒÖ siƒô ‚Äûstatystycznie istotne‚Äù, nawet je≈õli sƒÖ bez znaczenia.\nPrzyk≈Çad: Badanie 100 000 os√≥b stwierdza, ≈ºe mƒô≈ºczy≈∫ni sƒÖ ≈õrednio o 0,1 cm wy≈ºsi (p &lt; 0,001). Statystycznie istotne, ale praktycznie nieistotne.\n\n\nNieporozumienie 4: ‚ÄúKorelacja implikuje przyczynowo≈õƒá‚Äù\nRzeczywisto≈õƒá: Korelacja jest konieczna, ale niewystarczajƒÖca dla przyczynowo≈õci.\nKlasyczne przyk≈Çady:\n\nMiasta z wiƒôkszƒÖ liczbƒÖ ko≈õcio≈Ç√≥w majƒÖ wiƒôcej przestƒôpstw (oba korelujƒÖ z wielko≈õciƒÖ populacji)\nKraje z wiƒôkszƒÖ liczbƒÖ telewizor√≥w majƒÖ d≈Çu≈ºszƒÖ oczekiwanƒÖ d≈Çugo≈õƒá ≈ºycia (oba korelujƒÖ z rozwojem)\n\n\n\nNieporozumienie 5: ‚ÄúLosowy oznacza przypadkowy‚Äù\nRzeczywisto≈õƒá: Statystyczna losowo≈õƒá jest starannie kontrolowana i systematyczna.\nPrzyk≈Çad: Losowe pr√≥bkowanie wymaga starannej procedury, a nie tylko chwytania kogokolwiek wygodnego.\n\n\nNieporozumienie 6: ‚Äú≈örednia reprezentuje wszystkich‚Äù\nRzeczywisto≈õƒá: ≈örednie mogƒÖ byƒá mylƒÖce, gdy rozk≈Çady sƒÖ sko≈õne lub wielomodalne.\nPrzyk≈Çad: ≈öredni doch√≥d bywalc√≥w baru wynosi 50 000 z≈Ç. Bill Gates wchodzi. Teraz ≈õrednia wynosi 1 milion z≈Ç. Rzeczywisty doch√≥d nikogo siƒô nie zmieni≈Ç.\n\n\nNieporozumienie 7: ‚ÄúPrzesz≈Çe wzorce gwarantujƒÖ przysz≈Çe wyniki‚Äù\nRzeczywisto≈õƒá: Ekstrapolacja zak≈Çada, ≈ºe warunki pozostajƒÖ sta≈Çe.\nPrzyk≈Çad: Liniowa projekcja wzrostu populacji z lat 1950-2000 ≈∫le przeszacowa≈Çaby populacjƒô 2050 roku, poniewa≈º pomija spadek p≈Çodno≈õci.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zastosowania-w-demografii-1",
    "href": "rozdzial1.html#zastosowania-w-demografii-1",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.17 Zastosowania w demografii",
    "text": "2.17 Zastosowania w demografii\n\nSzacowanie i projekcja populacji\nOszacowania miƒôdzyspisowe: Szacowanie populacji miƒôdzy spisami\nMetoda komponent√≥w: P(t+1) = P(t) + B - D + I - E\nGdzie:\n\nP(t) = Populacja w czasie t\nB = Urodzenia\nD = Zgony\nI = Imigracja\nE = Emigracja\n\nKa≈ºdy komponent szacowany z r√≥≈ºnych ≈∫r√≥de≈Ç z r√≥≈ºnymi strukturami b≈Çƒôd√≥w.\nProjekcje populacji: Prognozowanie przysz≈Çej populacji\nMetoda komponent√≥w kohortowych:\n\nPrognozuj wska≈∫niki prze≈ºycia wed≈Çug wieku\nPrognozuj wska≈∫niki p≈Çodno≈õci\nPrognozuj wska≈∫niki migracji\nZastosuj do populacji bazowej\nZagreguj wyniki\n\nNiepewno≈õƒá wzrasta z horyzontem projekcji.\n\n\nObliczanie wska≈∫nik√≥w demograficznych\nWska≈∫niki surowe (Crude Rates): Zdarzenia na 1000 populacji\n\\text{Surowy wsp√≥≈Çczynnik urodze≈Ñ} = \\frac{\\text{Urodzenia}}{\\text{Populacja w po≈Çowie roku}} \\times 1000\nWska≈∫niki specyficzne dla wieku (Age-Specific Fertility Rate): Kontrola struktury wieku\n\\text{Wsp√≥≈Çczynnik p≈Çodno≈õci specyficzny dla wieku} = \\frac{\\text{Urodzenia kobietom w wieku } x}{\\text{Kobiety w wieku } x} \\times 1000\nStandaryzacja: Por√≥wnywanie populacji z r√≥≈ºnymi strukturami\nStandaryzacja bezpo≈õrednia: Zastosuj wska≈∫niki populacji do standardowej struktury wieku Standaryzacja po≈õrednia: Zastosuj standardowe wska≈∫niki do struktury wieku populacji\n\n\nAnaliza tablic trwania ≈ºycia\nTablice ≈ºycia podsumowujƒÖ do≈õwiadczenie ≈õmiertelno≈õci populacji.\nKluczowe kolumny:\n\nq_x: Prawdopodobie≈Ñstwo ≈õmierci miƒôdzy wiekiem x a x+1\nl_x: Liczba prze≈ºywajƒÖcych do wieku x (ze 100 000 urodze≈Ñ)\nd_x: Zgony miƒôdzy wiekiem x a x+1\nL_x: Osobo-lata prze≈ºyte miƒôdzy wiekiem x a x+1\ne_x: Oczekiwana d≈Çugo≈õƒá ≈ºycia w wieku x\n\nPrzyk≈Çad interpretacji: Je≈õli q_{65} = 0,015, to 1,5% 65-latk√≥w umrze przed osiƒÖgniƒôciem 66 lat. Je≈õli e_{65} = 18,5, to 65-latkowie ≈õrednio ≈ºyjƒÖ jeszcze 18,5 roku.\n\n\nAnaliza p≈Çodno≈õci\nWsp√≥≈Çczynnik dzietno≈õci ca≈Çkowitej (TFR - Total Fertility Rate): ≈örednia liczba dzieci na kobietƒô przy obecnych wska≈∫nikach p≈Çodno≈õci specyficznych dla wieku (ASFR - Age-Specific Fertility Rate)\n\\text{TFR} = \\sum (\\text{ASFR} \\times \\text{szeroko≈õƒá przedzia≈Çu wieku})\nPrzyk≈Çad: Je≈õli ka≈ºda 5-letnia grupa wiekowa od 15-49 ma ASFR = 20 na 1000: \\text{TFR} = 7 \\text{ grup wiekowych} \\times \\frac{20}{1000} \\times 5 \\text{ lat} = 0,7 \\text{ dzieci na kobietƒô}\nTen bardzo niski TFR wskazuje na p≈Çodno≈õƒá poni≈ºej poziomu zastƒôpowalno≈õci.\n\n\nAnaliza migracji\nWsp√≥≈Çczynnik migracji netto: \\text{NMR} = \\frac{\\text{Imigranci} - \\text{Emigranci}}{\\text{Populacja}} \\times 1000\nWska≈∫nik efektywno≈õci migracji: \\text{MEI} = \\frac{|\\text{Nap≈Çyw} - \\text{Odp≈Çyw}|}{\\text{Nap≈Çyw} + \\text{Odp≈Çyw}}\n\nWarto≈õci blisko 0: Wysoka rotacja, ma≈Ça zmiana netto\nWarto≈õci blisko 1: G≈Ç√≥wnie przep≈Çyw jednokierunkowy\n\n\n\nMetryki zdrowia populacji\nLata ≈ºycia skorygowane o niepe≈Çnosprawno≈õƒá (DALYs): Utracone lata zdrowego ≈ºycia\nDALY = Utracone lata ≈ºycia (YLL) + Lata prze≈ºyte z niepe≈Çnosprawno≈õciƒÖ (YLD)\nOczekiwana d≈Çugo≈õƒá ≈ºycia w zdrowiu: Oczekiwane lata w dobrym zdrowiu\n≈ÅƒÖczy informacje o ≈õmiertelno≈õci i chorobowo≈õci.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#oprogramowanie-i-narzƒôdzia",
    "href": "rozdzial1.html#oprogramowanie-i-narzƒôdzia",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.18 Oprogramowanie i narzƒôdzia",
    "text": "2.18 Oprogramowanie i narzƒôdzia\nWsp√≥≈Çczesna statystyka demograficzna opiera siƒô w du≈ºej mierze na narzƒôdziach obliczeniowych.\n\nPakiety oprogramowania statystycznego\nR: Darmowy, otwarty, rozbudowane pakiety demograficzne\n\nPakiety: demography, popReconstruct, bayesPop\nZalety: Powtarzalne badania, najnowocze≈õniejsze metody\nWady: Stroma krzywa uczenia\n\nStata: Szeroko u≈ºywany w naukach spo≈Çecznych\n\nMocne strony: Analiza danych z bada≈Ñ, dane panelowe\nPowszechny w: Ekonomii, epidemiologii\n\nSPSS: Przyjazny interfejs u≈ºytkownika\n\nMocne strony: Interfejs wska≈º-i-kliknij\nPowszechny w: Naukach spo≈Çecznych, badaniach rynkowych\n\nPython: Jƒôzyk programowania og√≥lnego przeznaczenia z bibliotekami statystycznymi\n\nBiblioteki: pandas, numpy, scipy, statsmodels\nZalety: Integracja z innymi aplikacjami",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zako≈Ñczenie",
    "href": "rozdzial1.html#zako≈Ñczenie",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.19 Zako≈Ñczenie",
    "text": "2.19 Zako≈Ñczenie\n\nPodsumowanie kluczowych termin√≥w\nStatystyka: Nauka o zbieraniu, organizowaniu, analizowaniu, interpretowaniu i prezentowaniu danych w celu zrozumienia zjawisk i wsparcia podejmowania decyzji\nStatystyka opisowa: Metody podsumowywania i prezentowania danych w znaczƒÖcy spos√≥b bez rozszerzania wniosk√≥w poza obserwowane dane\nStatystyka wnioskowania: Techniki wyciƒÖgania wniosk√≥w o populacjach z pr√≥b, w tym estymacja i testowanie hipotez\nPopulacja: Kompletny zbi√≥r jednostek, obiekt√≥w lub pomiar√≥w, o kt√≥rych chcemy wyciƒÖgnƒÖƒá wnioski\nPr√≥ba: Podzbi√≥r populacji, kt√≥ry jest faktycznie obserwowany lub mierzony w celu dokonania wniosk√≥w o populacji\nSuperpopulacja: Teoretyczna niesko≈Ñczona populacja, z kt√≥rej obserwowane sko≈Ñczone populacje sƒÖ uwa≈ºane za pr√≥by\nParametr: Liczbowa charakterystyka populacji (zazwyczaj nieznana i oznaczana literami greckimi)\nStatystyka: Liczbowa charakterystyka obliczona z danych z pr√≥by (znana i oznaczana literami ≈Çaci≈Ñskimi)\nEstymator: Regu≈Ça lub formu≈Ça do obliczania oszacowa≈Ñ parametr√≥w populacji z danych z pr√≥by\nEstimand: Konkretny parametr populacji bƒôdƒÖcy celem estymacji\nOszacowanie: Warto≈õƒá liczbowa uzyskana przez zastosowanie estymatora do obserwowanych danych\nB≈ÇƒÖd losowy: Nieprzewidywalna zmienno≈õƒá wynikajƒÖca z procesu pr√≥bkowania, kt√≥ra maleje z wiƒôkszymi pr√≥bami\nB≈ÇƒÖd systematyczny (ObciƒÖ≈ºenie): Konsekwentne odchylenie od prawdziwych warto≈õci, kt√≥rego nie mo≈ºna zmniejszyƒá przez zwiƒôkszenie wielko≈õci pr√≥by\nPr√≥bkowanie: Proces wyboru podzbioru jednostek z populacji do pomiaru\nOperat losowania: Lista lub urzƒÖdzenie, z kt√≥rego pobierana jest pr√≥ba, idealnie zawierajƒÖce wszystkich cz≈Çonk√≥w populacji\nPr√≥bkowanie probabilistyczne: Metody pr√≥bkowania, w kt√≥rych ka≈ºdy cz≈Çonek populacji ma znane, niezerowe prawdopodobie≈Ñstwo selekcji\nProste losowanie: Ka≈ºda mo≈ºliwa pr√≥ba wielko≈õci n ma r√≥wne prawdopodobie≈Ñstwo selekcji\nLosowanie systematyczne: Wyb√≥r co k-tego elementu z uporzƒÖdkowanego operanta losowania\nLosowanie warstwowe: Podzia≈Ç populacji na jednorodne podgrupy przed pr√≥bkowaniem w ka≈ºdej\nLosowanie grupowe: Wyb√≥r grup (klastr√≥w) zamiast jednostek\nPr√≥bkowanie nieprobabilistyczne: Metody pr√≥bkowania bez gwarantowanych znanych prawdopodobie≈Ñstw selekcji\nPr√≥bkowanie wygodne: Wyb√≥r oparty wy≈ÇƒÖcznie na ≈Çatwo≈õci dostƒôpu\nPr√≥bkowanie celowe: Celowy wyb√≥r oparty na osƒÖdzie badacza\nPr√≥bkowanie kwotowe: Wyb√≥r w celu dopasowania proporcji populacji w kluczowych charakterystykach bez losowej selekcji\nPr√≥bkowanie kuli ≈õnie≈ºnej: Uczestnicy rekrutujƒÖ dodatkowych uczestnik√≥w ze swoich znajomych\nB≈ÇƒÖd standardowy: Odchylenie standardowe rozk≈Çadu pr√≥bkowania statystyki\nMargines b≈Çƒôdu: Maksymalna oczekiwana r√≥≈ºnica miƒôdzy oszacowaniem a parametrem przy okre≈õlonym poziomie ufno≈õci\nPrzedzia≈Ç ufno≈õci: Zakres prawdopodobnych warto≈õci dla parametru przy okre≈õlonym poziomie ufno≈õci\nPoziom ufno≈õci: Prawdopodobie≈Ñstwo, ≈ºe metoda przedzia≈Çu ufno≈õci wytworzy przedzia≈Çy zawierajƒÖce parametr\nDane: Zebrane obserwacje lub pomiary\nDane ilo≈õciowe: Pomiary liczbowe (ciƒÖg≈Çe lub dyskretne)\nDane jako≈õciowe: Informacje kategoryczne (nominalne lub porzƒÖdkowe)\nRozk≈Çad danych: Opis tego, jak warto≈õci rozk≈ÇadajƒÖ siƒô na mo≈ºliwe wyniki\nRozk≈Çad czƒôsto≈õci: Podsumowanie pokazujƒÖce, jak czƒôsto ka≈ºda warto≈õƒá wystƒôpuje w danych\nCzƒôsto≈õƒá bezwzglƒôdna: Liczba obserwacji dla ka≈ºdej warto≈õci\nCzƒôsto≈õƒá wzglƒôdna: Proporcja obserwacji w ka≈ºdej kategorii\nCzƒôsto≈õƒá skumulowana: Suma bie≈ºƒÖca czƒôsto≈õci do ka≈ºdej warto≈õci",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#za≈ÇƒÖcznik-a-visualizations-for-statistics-demography",
    "href": "rozdzial1.html#za≈ÇƒÖcznik-a-visualizations-for-statistics-demography",
    "title": "2¬† Podstawy Statystyki i Demografii",
    "section": "2.20 Za≈ÇƒÖcznik A: Visualizations for Statistics & Demography",
    "text": "2.20 Za≈ÇƒÖcznik A: Visualizations for Statistics & Demography\n\n## ============================================\n## Visualizations for Statistics & Demography\n## Chapter 1: Foundations\n## ============================================\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(gridExtra)\nlibrary(scales)\nlibrary(patchwork)  # for combining plots\n\n# Set theme for all plots\ntheme_set(theme_minimal(base_size = 12))\n\n# Color palette for consistency\ncolors &lt;- c(\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#6A994E\")\n\n\n# ==================================================\n# 1. POPULATION vs SAMPLE VISUALIZATION\n# ==================================================\n\n# Create a population and sample visualization\nset.seed(123)\n\n# Generate population data (e.g., ages of 10,000 people)\npopulation &lt;- data.frame(\n  id = 1:10000,\n  age = round(rnorm(10000, mean = 40, sd = 15))\n)\npopulation$age[population$age &lt; 0] &lt;- 0\npopulation$age[population$age &gt; 100] &lt;- 100\n\n# Take a random sample\nsample_size &lt;- 500\nsample_data &lt;- population[sample(nrow(population), sample_size), ]\n\n# Create visualization\np1 &lt;- ggplot(population, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = colors[1], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = mean(population$age), \n             color = colors[2], linetype = \"dashed\", size = 1.2) +\n  labs(title = \"Population Distribution (N = 10,000)\",\n       subtitle = paste(\"Population mean (Œº) =\", round(mean(population$age), 2), \"years\"),\n       x = \"Age (years)\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\np2 &lt;- ggplot(sample_data, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = colors[3], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = mean(sample_data$age), \n             color = colors[4], linetype = \"dashed\", size = 1.2) +\n  labs(title = paste(\"Sample Distribution (n =\", sample_size, \")\"),\n       subtitle = paste(\"Sample mean (xÃÑ) =\", round(mean(sample_data$age), 2), \"years\"),\n       x = \"Age (years)\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\n# Combine plots\npopulation_sample_plot &lt;- p1 / p2\nprint(population_sample_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 2. TYPES OF DATA DISTRIBUTIONS\n# ==================================================\n\n# Generate different distribution types\nset.seed(456)\nn &lt;- 5000\n\n# Normal distribution\nnormal_data &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Right-skewed distribution (income-like)\nright_skewed &lt;- rgamma(n, shape = 2, scale = 15)\n\n# Left-skewed distribution (age at death in developed country)\nleft_skewed &lt;- 90 - rgamma(n, shape = 3, scale = 5)\nleft_skewed[left_skewed &lt; 0] &lt;- 0\n\n# Bimodal distribution (e.g., height of mixed male/female population)\nn2  &lt;- 20000\nnf &lt;- n2 %/% 2; nm &lt;- n2 - nf\nbimodal &lt;- c(rnorm(nf, mean = 164, sd = 5),\n             rnorm(nm, mean = 182, sd = 5))\n\n\n# Create data frame\ndistributions_df &lt;- data.frame(\n  Normal = normal_data,\n  `Right Skewed` = right_skewed,\n  `Left Skewed` = left_skewed,\n  Bimodal = bimodal\n) %&gt;%\n  pivot_longer(everything(), names_to = \"Distribution\", values_to = \"Value\")\n\n# Plot distributions\ndistributions_plot &lt;- ggplot(distributions_df, aes(x = Value, fill = Distribution)) +\n  geom_histogram(bins = 30, alpha = 0.7, color = \"white\") +\n  facet_wrap(~Distribution, scales = \"free\", nrow = 2) +\n  scale_fill_manual(values = colors[1:4]) +\n  labs(title = \"Types of Data Distributions\",\n       subtitle = \"Common patterns in demographic data\",\n       x = \"Value\", y = \"Frequency\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"none\")\n\nprint(distributions_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 3. NORMAL DISTRIBUTION WITH 68-95-99.7 RULE\n# ==================================================\n\n# Generate normal distribution data\nset.seed(789)\nmean_val &lt;- 100\nsd_val &lt;- 15\nx &lt;- seq(mean_val - 4*sd_val, mean_val + 4*sd_val, length.out = 1000)\ny &lt;- dnorm(x, mean = mean_val, sd = sd_val)\ndf_norm &lt;- data.frame(x = x, y = y)\n\n# Create the plot\nnormal_plot &lt;- ggplot(df_norm, aes(x = x, y = y)) +\n  # Fill areas under the curve\n  geom_area(data = subset(df_norm, x &gt;= mean_val - sd_val & x &lt;= mean_val + sd_val),\n            aes(x = x, y = y), fill = colors[1], alpha = 0.3) +\n  geom_area(data = subset(df_norm, x &gt;= mean_val - 2*sd_val & x &lt;= mean_val + 2*sd_val),\n            aes(x = x, y = y), fill = colors[2], alpha = 0.2) +\n  geom_area(data = subset(df_norm, x &gt;= mean_val - 3*sd_val & x &lt;= mean_val + 3*sd_val),\n            aes(x = x, y = y), fill = colors[3], alpha = 0.1) +\n  # Add the curve\n  geom_line(size = 1.5, color = \"black\") +\n  # Add vertical lines for standard deviations\n  geom_vline(xintercept = mean_val, linetype = \"solid\", size = 1, color = \"black\") +\n  geom_vline(xintercept = c(mean_val - sd_val, mean_val + sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[1]) +\n  geom_vline(xintercept = c(mean_val - 2*sd_val, mean_val + 2*sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[2]) +\n  geom_vline(xintercept = c(mean_val - 3*sd_val, mean_val + 3*sd_val), \n             linetype = \"dashed\", size = 0.8, color = colors[3]) +\n  # Add labels\n  annotate(\"text\", x = mean_val, y = max(y) * 0.5, label = \"68%\", \n           size = 5, fontface = \"bold\", color = colors[1]) +\n  annotate(\"text\", x = mean_val, y = max(y) * 0.3, label = \"95%\", \n           size = 5, fontface = \"bold\", color = colors[2]) +\n  annotate(\"text\", x = mean_val, y = max(y) * 0.1, label = \"99.7%\", \n           size = 5, fontface = \"bold\", color = colors[3]) +\n  # Labels\n  scale_x_continuous(breaks = c(mean_val - 3*sd_val, mean_val - 2*sd_val, \n                                mean_val - sd_val, mean_val, \n                                mean_val + sd_val, mean_val + 2*sd_val, \n                                mean_val + 3*sd_val),\n                     labels = c(\"Œº-3œÉ\", \"Œº-2œÉ\", \"Œº-œÉ\", \"Œº\", \"Œº+œÉ\", \"Œº+2œÉ\", \"Œº+3œÉ\")) +\n  labs(title = \"Normal Distribution: The 68-95-99.7 Rule\",\n       subtitle = \"Proportion of data within standard deviations from the mean\",\n       x = \"Value\", y = \"Probability Density\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\nprint(normal_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 4. SIMPLE LINEAR REGRESSION\n# ==================================================\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(scales)\n\n# Define color palette (this was missing in original code)\ncolors &lt;- c(\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#592E83\")\n\n# Generate data for regression example (Education vs Income)\nset.seed(2024)\nn_reg &lt;- 200\neducation &lt;- round(rnorm(n_reg, mean = 14, sd = 3))\neducation[education &lt; 8] &lt;- 8\neducation[education &gt; 22] &lt;- 22\n\n# Create income with linear relationship plus noise\nincome &lt;- 15000 + 4000 * education + rnorm(n_reg, mean = 0, sd = 8000)\nincome[income &lt; 10000] &lt;- 10000\n\nreg_data &lt;- data.frame(education = education, income = income)\n\n# Fit linear model\nlm_model &lt;- lm(income ~ education, data = reg_data)\n\n# Create subset of data for residual lines\nsubset_indices &lt;- sample(nrow(reg_data), 20)\nsubset_data &lt;- reg_data[subset_indices, ]\nsubset_data$predicted &lt;- predict(lm_model, newdata = subset_data)\n\n# Create regression plot\nregression_plot &lt;- ggplot(reg_data, aes(x = education, y = income)) +\n  # Add points\n  geom_point(alpha = 0.6, size = 2, color = colors[1]) +\n  \n  # Add regression line with confidence interval\n  geom_smooth(method = \"lm\", se = TRUE, color = colors[2], fill = colors[2], alpha = 0.2) +\n  \n  # Add residual lines for a subset of points to show the concept\n  geom_segment(data = subset_data,\n               aes(x = education, xend = education, \n                   y = income, yend = predicted),\n               color = colors[4], alpha = 0.5, linetype = \"dotted\") +\n  \n  # Add equation to plot (adjusted position based on data range)\n  annotate(\"text\", x = min(reg_data$education) + 1, y = max(reg_data$income) * 0.9, \n           label = paste(\"Income = $\", format(round(coef(lm_model)[1]), big.mark = \",\"), \n                        \" + $\", format(round(coef(lm_model)[2]), big.mark = \",\"), \" √ó Education\",\n                        \"\\nR¬≤ = \", round(summary(lm_model)$r.squared, 3), sep = \"\"),\n           hjust = 0, size = 4, fontface = \"italic\") +\n  \n  # Labels and formatting\n  scale_y_continuous(labels = dollar_format()) +\n  labs(title = \"Simple Linear Regression: Education and Income\",\n       subtitle = \"Each year of education associated with higher income\",\n       x = \"Years of Education\", \n       y = \"Annual Income\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n\nprint(regression_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 5. SAMPLING ERROR AND SAMPLE SIZE\n# ==================================================\n\n# Show how standard error decreases with sample size\nset.seed(111)\nsample_sizes &lt;- c(10, 25, 50, 100, 250, 500, 1000, 2500, 5000)\nn_simulations &lt;- 1000\n\n# True population parameters\ntrue_mean &lt;- 50\ntrue_sd &lt;- 10\n\n# Run simulations for each sample size\nse_results &lt;- data.frame()\nfor (n in sample_sizes) {\n  sample_means &lt;- replicate(n_simulations, mean(rnorm(n, true_mean, true_sd)))\n  se_results &lt;- rbind(se_results, \n                      data.frame(n = n, \n                                se_empirical = sd(sample_means),\n                                se_theoretical = true_sd / sqrt(n)))\n}\n\n# Create the plot\nse_plot &lt;- ggplot(se_results, aes(x = n)) +\n  geom_line(aes(y = se_empirical, color = \"Empirical SE\"), size = 1.5) +\n  geom_point(aes(y = se_empirical, color = \"Empirical SE\"), size = 3) +\n  geom_line(aes(y = se_theoretical, color = \"Theoretical SE\"), \n            size = 1.5, linetype = \"dashed\") +\n  scale_x_log10(breaks = sample_sizes) +\n  scale_color_manual(values = c(\"Empirical SE\" = colors[1], \n                               \"Theoretical SE\" = colors[2])) +\n  labs(title = \"Standard Error Decreases with Sample Size\",\n       subtitle = \"The precision of estimates improves with larger samples\",\n       x = \"Sample Size (log scale)\", \n       y = \"Standard Error\",\n       color = \"\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"top\")\n\nprint(se_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 6. CONFIDENCE INTERVALS VISUALIZATION\n# ==================================================\n\n# Simulate multiple samples and their confidence intervals\nset.seed(999)\nn_samples &lt;- 20\nsample_size_ci &lt;- 100\ntrue_mean_ci &lt;- 50\ntrue_sd_ci &lt;- 10\n\n# Generate samples and calculate CIs\nci_data &lt;- data.frame()\nfor (i in 1:n_samples) {\n  sample_i &lt;- rnorm(sample_size_ci, true_mean_ci, true_sd_ci)\n  mean_i &lt;- mean(sample_i)\n  se_i &lt;- sd(sample_i) / sqrt(sample_size_ci)\n  ci_lower &lt;- mean_i - 1.96 * se_i\n  ci_upper &lt;- mean_i + 1.96 * se_i\n  contains_true &lt;- (true_mean_ci &gt;= ci_lower) & (true_mean_ci &lt;= ci_upper)\n  \n  ci_data &lt;- rbind(ci_data,\n                   data.frame(sample = i, mean = mean_i, \n                             lower = ci_lower, upper = ci_upper,\n                             contains = contains_true))\n}\n\n# Create CI plot\nci_plot &lt;- ggplot(ci_data, aes(x = sample, y = mean)) +\n  geom_hline(yintercept = true_mean_ci, color = \"red\", \n             linetype = \"dashed\", size = 1) +\n  geom_errorbar(aes(ymin = lower, ymax = upper, color = contains), \n                width = 0.3, size = 0.8) +\n  geom_point(aes(color = contains), size = 2) +\n  scale_color_manual(values = c(\"TRUE\" = colors[1], \"FALSE\" = colors[4]),\n                    labels = c(\"Misses true value\", \"Contains true value\")) +\n  coord_flip() +\n  labs(title = \"95% Confidence Intervals from 20 Different Samples\",\n       subtitle = paste(\"True population mean = \", true_mean_ci, \n                       \" (red dashed line)\", sep = \"\"),\n       x = \"Sample Number\", \n       y = \"Sample Mean with 95% CI\",\n       color = \"\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"bottom\")\n\nprint(ci_plot)\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 7. SAMPLING DISTRIBUTIONS (CENTRAL LIMIT THEOREM)\n# ==================================================\n\n# ---- Setup ----\nlibrary(tidyverse)\nlibrary(ggplot2)\ntheme_set(theme_minimal(base_size = 13))\nset.seed(2025)\n\n# Skewed population (Gamma); change if you want another DGP\nNpop &lt;- 100000\npopulation &lt;- rgamma(Npop, shape = 2, scale = 10)  # skewed right\nmu    &lt;- mean(population)\nsigma &lt;- sd(population)\n\n# ---- CLT: sampling distribution of the mean ----\nsample_sizes &lt;- c(1, 5, 10, 30, 100)\nB &lt;- 2000  # resamples per n\n\nclt_df &lt;- purrr::map_dfr(sample_sizes, \\(n) {\n  tibble(n = n,\n         mean = replicate(B, mean(sample(population, n, replace = TRUE))))\n})\n\n# Normal overlays: N(mu, sigma/sqrt(n))\nclt_range &lt;- clt_df |&gt;\n  group_by(n) |&gt;\n  summarise(min_x = min(mean), max_x = max(mean), .groups = \"drop\")\n\nnormal_df &lt;- clt_range |&gt;\n  rowwise() |&gt;\n  mutate(x = list(seq(min_x, max_x, length.out = 200))) |&gt;\n  unnest(x) |&gt;\n  mutate(density = dnorm(x, mean = mu, sd = sigma / sqrt(n)))\n\nclt_plot &lt;- ggplot(clt_df, aes(mean)) +\n  geom_histogram(aes(y = after_stat(density), fill = factor(n)),\n                 bins = 30, alpha = 0.6, color = \"white\") +\n  geom_line(data = normal_df, aes(x, density), linewidth = 0.8) +\n  geom_vline(xintercept = mu, linetype = \"dashed\") +\n  facet_wrap(~ n, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"CLT: Sampling distribution of the mean ‚Üí Normal(Œº, œÉ/‚àön)\",\n    subtitle = sprintf(\"Skewed population: Gamma(shape=2, scale=10).  Œº‚âà%.2f, œÉ‚âà%.2f; B=%d resamples each.\", mu, sigma, B),\n    x = \"Sample mean\", y = \"Density\"\n  ) +\n  guides(fill = \"none\")\n\nclt_plot\n\n\n\n\n\n\n\n\n\n# ==================================================\n# 8. TYPES OF SAMPLING ERROR\n# ==================================================\n\n# Create data to show random vs systematic error\nset.seed(321)\nn_measurements &lt;- 100\ntrue_value &lt;- 50\n\n# Random error only\nrandom_error &lt;- rnorm(n_measurements, mean = true_value, sd = 5)\n\n# Systematic error (bias) only\nsystematic_error &lt;- rep(true_value + 10, n_measurements) + rnorm(n_measurements, 0, 0.5)\n\n# Both errors\nboth_errors &lt;- rnorm(n_measurements, mean = true_value + 10, sd = 5)\n\nerror_data &lt;- data.frame(\n  measurement = 1:n_measurements,\n  `Random Error Only` = random_error,\n  `Systematic Error Only` = systematic_error,\n  `Both Errors` = both_errors\n) %&gt;%\n  pivot_longer(-measurement, names_to = \"Error_Type\", values_to = \"Value\")\n\n# Create error visualization\nerror_plot &lt;- ggplot(error_data, aes(x = measurement, y = Value, color = Error_Type)) +\n  geom_hline(yintercept = true_value, linetype = \"dashed\", size = 1, color = \"black\") +\n  geom_point(alpha = 0.6, size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 1.2) +\n  facet_wrap(~Error_Type, nrow = 1) +\n  scale_color_manual(values = colors[1:3]) +\n  labs(title = \"Random Error vs Systematic Error (Bias)\",\n       subtitle = paste(\"True value = \", true_value, \" (black dashed line)\", sep = \"\"),\n       x = \"Measurement Number\", \n       y = \"Measured Value\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"none\")\n\nprint(error_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 9. DEMOGRAPHIC PYRAMID\n# ==================================================\n\n# Create age pyramid data\nset.seed(777)\nage_groups &lt;- c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \n               \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \n               \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80+\")\n\n# Create data for a developing country pattern\nmale_pop &lt;- c(12, 11.5, 11, 10.5, 10, 9.5, 9, 8.5, 8, 7.5, 7, \n             6, 5, 4, 3, 2, 1.5)\nfemale_pop &lt;- c(11.8, 11.3, 10.8, 10.3, 9.8, 9.3, 8.8, 8.3, 7.8, \n               7.3, 6.8, 5.8, 4.8, 3.8, 2.8, 2.2, 2)\n\npyramid_data &lt;- data.frame(\n  Age = factor(rep(age_groups, 2), levels = rev(age_groups)),\n  Population = c(-male_pop, female_pop),  # Negative for males\n  Sex = c(rep(\"Male\", length(male_pop)), rep(\"Female\", length(female_pop)))\n)\n\n# Create population pyramid\npyramid_plot &lt;- ggplot(pyramid_data, aes(x = Age, y = Population, fill = Sex)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  scale_y_continuous(labels = function(x) paste0(abs(x), \"%\")) +\n  scale_fill_manual(values = c(\"Male\" = colors[1], \"Female\" = colors[3])) +\n  coord_flip() +\n  labs(title = \"Population Pyramid\",\n       subtitle = \"Age and sex distribution (typical developing country pattern)\",\n       x = \"Age Group\", \n       y = \"Percentage of Population\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        legend.position = \"top\")\n\nprint(pyramid_plot)\n\n\n\n\n\n\n\n# ==================================================\n# 10. REGRESSION RESIDUALS AND DIAGNOSTICS\n# ==================================================\n\n# Use the previous regression model for diagnostics\nreg_diagnostics &lt;- data.frame(\n  fitted = fitted(lm_model),\n  residuals = residuals(lm_model),\n  standardized_residuals = rstandard(lm_model),\n  education = reg_data$education,\n  income = reg_data$income\n)\n\n# Create diagnostic plots\n# 1. Residuals vs Fitted\np_resid_fitted &lt;- ggplot(reg_diagnostics, aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.5, color = colors[1]) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = colors[2], size = 0.8) +\n  labs(title = \"Residuals vs Fitted Values\",\n       subtitle = \"Check for homoscedasticity\",\n       x = \"Fitted Values\", y = \"Residuals\")\n\n# 2. Q-Q plot\np_qq &lt;- ggplot(reg_diagnostics, aes(sample = standardized_residuals)) +\n  stat_qq(color = colors[1]) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Normal Q-Q Plot\",\n       subtitle = \"Check for normality of residuals\",\n       x = \"Theoretical Quantiles\", y = \"Standardized Residuals\")\n\n# 3. Histogram of residuals\np_hist_resid &lt;- ggplot(reg_diagnostics, aes(x = residuals)) +\n  geom_histogram(bins = 30, fill = colors[3], alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Distribution of Residuals\",\n       subtitle = \"Should be approximately normal\",\n       x = \"Residuals\", y = \"Frequency\")\n\n# 4. Residuals vs Predictor\np_resid_x &lt;- ggplot(reg_diagnostics, aes(x = education, y = residuals)) +\n  geom_point(alpha = 0.5, color = colors[4]) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = colors[2], size = 0.8) +\n  labs(title = \"Residuals vs Predictor\",\n       subtitle = \"Check for patterns\",\n       x = \"Education (years)\", y = \"Residuals\")\n\n# Combine diagnostic plots\ndiagnostic_plots &lt;- (p_resid_fitted + p_qq) / (p_hist_resid + p_resid_x)\nprint(diagnostic_plots)\n\n\n\n\n\n\n\n# ==================================================\n# 11. SAVE ALL PLOTS (Optional)\n# ==================================================\n\n# Uncomment to save plots as high-resolution images\n# ggsave(\"population_sample.png\", population_sample_plot, width = 10, height = 8, dpi = 300)\n# ggsave(\"distributions.png\", distributions_plot, width = 12, height = 8, dpi = 300)\n# ggsave(\"normal_distribution.png\", normal_plot, width = 10, height = 6, dpi = 300)\n# ggsave(\"regression.png\", regression_plot, width = 10, height = 7, dpi = 300)\n# ggsave(\"standard_error.png\", se_plot, width = 10, height = 6, dpi = 300)\n# ggsave(\"confidence_intervals.png\", ci_plot, width = 10, height = 8, dpi = 300)\n# ggsave(\"central_limit_theorem.png\", clt_plot, width = 14, height = 5, dpi = 300)\n# ggsave(\"error_types.png\", error_plot, width = 14, height = 5, dpi = 300)\n# ggsave(\"population_pyramid.png\", pyramid_plot, width = 8, height = 8, dpi = 300)\n# ggsave(\"regression_diagnostics.png\", diagnostic_plots, width = 12, height = 10, dpi = 300)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Podstawy Statystyki i Demografii</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "3¬† Understanding Data Types in Social Sciences",
    "section": "",
    "text": "3.1 Foundations in Number Sets\nIn social science research, understanding the nature of our data is crucial for selecting appropriate analysis methods and drawing valid conclusions.\nBefore diving into data types, it‚Äôs essential to understand the basic number sets that form the foundation of our understanding of data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#foundations-in-number-sets",
    "href": "chapter2.html#foundations-in-number-sets",
    "title": "3¬† Understanding Data Types in Social Sciences",
    "section": "",
    "text": "Basic Number Sets\n\nNatural Numbers (‚Ñï): The counting numbers {0, 1, 2, 3, ‚Ä¶}\nIntegers (‚Ñ§): Includes natural numbers, their negatives, and zero {‚Ä¶, -2, -1, 0, 1, 2, ‚Ä¶}\nRational Numbers (‚Ñö): Numbers that can be expressed as a fraction of two integers\nReal Numbers (‚Ñù): All numbers on the number line, including rationals and irrationals\n\n\n\nProperties of Sets\n\nCountable Sets: Sets whose elements can be put in a one-to-one correspondence with the natural numbers. For example, the set of integers is countable.\nUncountable Sets: Sets that are not countable. The set of real numbers is uncountable.\nDiscrete Sets: Sets where each element is separated from other elements by a finite gap. The integers form a discrete set.\nDense Sets: Sets where between any two elements, there is always another element of the set. The rational numbers and real numbers are dense sets.\n\n\n\n\n\n\n\nNote\n\n\n\nUnderstanding these set properties is crucial for grasping the nature of different data types in social sciences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#discrete-vs.-continuous-data",
    "href": "chapter2.html#discrete-vs.-continuous-data",
    "title": "3¬† Understanding Data Types in Social Sciences",
    "section": "3.2 Discrete vs.¬†Continuous Data",
    "text": "3.2 Discrete vs.¬†Continuous Data\nIn data science and statistics, we often categorize variables as either discrete or continuous. However, the distinction is not always clear-cut, and some variables exhibit characteristics of both types. This section explores the concepts of discrete and continuous data, their differences, and the interesting cases of variables that can be treated as both or challenge our intuitive understanding.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDiscrete Data\nDiscrete data can only take on specific, countable values. These values are often (but not always) integers.\n\nCharacteristics of Discrete Data:\n\nCountable\nOften represented by integers\nCan be finite or infinite\nNo values between two adjacent data points\n\n\n\nExamples:\n\nNumber of students in a class\nNumber of cars sold by a dealership\nShoe sizes\n\n\n\n\nContinuous Data\nContinuous data can take on any value within a given range, including fractional and decimal values. It‚Äôs important to note that continuity is not solely determined by uncountability, but also by density.\n\nCharacteristics of Continuous Data:\n\nCan be uncountable (like real numbers) or dense (like rational numbers)\nCan be measured to any level of precision (theoretically)\nRepresented by real numbers or dense subsets of real numbers\nThere are always values between any two data points\n\n\n\nExamples:\n\nHeight\nWeight\nTemperature\nPercentages (explained further below)\n\n\n\n\nThe Discrete-Continuous Spectrum\nIn practice, some variables that are mathematically discrete are often treated as if they are continuous. This dual nature provides flexibility in how these variables can be analyzed and interpreted.\n\nReasons for Treating Discrete Data as Continuous:\n\nDense Granularity\n\nWhen a discrete variable has a large number of possible values within a range, it can approximate continuity.\nExample: Income measured in individual cents. While technically discrete, the large number of possible values makes it behave similarly to a continuous variable.\n\nAnalytical Convenience\n\nContinuous methods often yield reasonable and useful results even for dense discrete variables.\nIt‚Äôs often easier to use existing statistical tools if continuity is assumed, as this allows the use of calculus-based methods.\n\nApproximation of Underlying Phenomena\n\nIn some cases, a discrete measurement might be an approximation of an underlying continuous process.\nExample: While we measure time in discrete units (seconds, minutes, hours), time itself is continuous.\n\n\n\n\nExamples of Variables with Dual Discrete-Continuous Nature:\n\nAge\n\nDiscrete: Typically measured in whole years\nContinuous: Can be considered as a continuous variable in many analyses, especially when dealing with large populations\n\nPrice and Income\n\nDiscrete: Prices and incomes are actually measured in discrete units (e.g., cents or smallest currency unit)\nContinuous: In economic models and many analyses, prices and incomes are treated as continuous variables due to their dense nature and analytical convenience\n\nTest Scores\n\nDiscrete: Often given as whole numbers\nContinuous: In statistical analyses, test scores might be treated as continuous, especially when the range of possible scores is large\n\n\n\n\n\nSpecial Case: Percentages and Rational Numbers\nPercentages present an interesting case in the discrete-continuous spectrum:\n\nRational Nature: Percentages are essentially fractions (m/100), making them rational numbers.\nDense but Countable: The set of rational numbers is dense (between any two rationals, there‚Äôs another rational) but also countable.\nPractical Continuity: In most practical applications, percentages are treated as continuous due to their dense nature.\nFinite Precision: In reality, percentages are often reported to a limited number of decimal places, creating a finite set of possible values.\n\n\n\n\n\n\n\nPercentages: Bridging Discrete and Continuous\n\n\n\nVariables measured in percentages, such as unemployment rates or voter turnout, challenge our intuitive understanding of discreteness and continuity:\n\nThey are rational numbers (fractions with denominator 100), which are technically countable.\nThey form a dense set within their range (0% to 100%), allowing for values between any two percentages.\nIn practice, they are often treated as continuous variables due to their dense nature and analytical convenience.\nThe precision of measurement (e.g., reporting to one or two decimal places) can impose a discrete structure on what is conceptually a dense set.\n\nThis duality allows for flexible analytical approaches, depending on the specific research context and required precision.\n\n\n\n\nImplications for Data Analysis\nUnderstanding the nuanced nature of variables as discrete, continuous, or somewhere in between has important implications for data analysis:\n\nFlexibility in Modeling: It allows for the use of a wider range of statistical techniques.\nSimplified Calculations: Treating dense discrete data as continuous can simplify calculations and make certain analyses more tractable.\nImproved Interpretability: In some cases, treating discrete data as continuous can lead to more intuitive or useful interpretations of results.\nPotential for Error: It‚Äôs important to be aware of when approximations are appropriate and when they might lead to misleading results.\nTheoretical vs.¬†Practical Considerations: While the mathematical nature of the data is important, practical considerations in measurement and analysis often guide how we treat variables.\n\n\n\nConclusion\nThe distinction between discrete and continuous data is not always rigid in social sciences. Many variables, including those involving money, percentages, or dense measurements, can be viewed through both discrete and continuous lenses. The choice of treatment should be guided by the nature of the data, the goals of the analysis, and the potential implications of the choice. This flexibility, when used thoughtfully, provides powerful tools for social science researchers to gain insights from their data.\n\n\n\n\n\n\nDiscrete vs.¬†Continuous Numerical Data: A Language-Based Analogies\n\n\n\n\nThe Language Connection\nThink about how you naturally ask questions about quantities:\n\n‚ÄúHow many cookies are in the jar?‚Äù (counting)\n‚ÄúHow much water is in the glass?‚Äù (measuring)\n\nThis natural language distinction reflects the two fundamental types of numerical data:\n\n\nDiscrete Data = ‚ÄúHow Many?‚Äù Questions\n\nLike counting whole objects (countable nouns)\nTakes specific values with gaps between them\nExamples:\n\nNumber of pets: 0, 1, 2, 3‚Ä¶ (can‚Äôt have 2.5 pets)\nDice rolls: 1, 2, 3, 4, 5, 6\nStudents in a class: 20, 21, 22‚Ä¶\n\n\nü§î Self-Check: Can you find a value between 2 and 3 students? Why not?\n\n\nContinuous Data = ‚ÄúHow Much?‚Äù Questions\n\nLike measuring quantities (uncountable nouns)\nCan take any value within a range\nExamples:\n\nHeight: 1.7231‚Ä¶ meters\nTemperature: 36.8325‚Ä¶ ¬∞C\nTime: 3.5792‚Ä¶ hours\n\n\nü§î Self-Check: Write down three different values between 1.72 and 1.73 meters\n\n\nQuick Recognition Guide\n\nIf you naturally ask ‚ÄúHow many?‚Äù ‚Üí Discrete\nIf you naturally ask ‚ÄúHow much?‚Äù ‚Üí Continuous\nIf you can measure it more precisely ‚Üí Continuous\nIf you can only use whole numbers ‚Üí Discrete\n\n‚úçÔ∏è Practice: Classify these quantities as discrete or continuous\n\nYour age in years: _____\nYour height: _____\nNumber of songs in a playlist: _____\nVolume of water: _____",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#introduction-to-stevens-data-typology",
    "href": "chapter2.html#introduction-to-stevens-data-typology",
    "title": "3¬† Understanding Data Types in Social Sciences",
    "section": "3.3 Introduction to Stevens‚Äô Data Typology",
    "text": "3.3 Introduction to Stevens‚Äô Data Typology\nStanley S. Stevens, an American psychologist, introduced a classification system for scales of measurement in his 1946 paper ‚ÄúOn the Theory of Scales of Measurement.‚Äù This system, known as Stevens‚Äô data typology or levels of measurement, has become fundamental in understanding how different types of data should be analyzed and interpreted.\nStevens proposed four levels of measurement:\n\nNominal\nOrdinal\nInterval\nRatio\n\nEach level has specific properties and allows for different types of statistical operations and analyses.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\nNominal Scale\n\nDefinition\nThe nominal scale is the most basic level of measurement. It uses labels or categories to classify data without any quantitative value or order.\n\n\nProperties\n\nCategories are mutually exclusive\nNo inherent order among categories\nNo meaningful arithmetic operations can be performed\n\n\n\nExamples\n\nNationality (Polish, English, ‚Ä¶)\nBlood types (A, B, AB, O)\nEye color (Blue, Brown, Green, Hazel)\nBinary variables (‚ÄúSuccess‚Äù versus ‚ÄúFailure‚Äù)\n\n\n\n\nOrdinal Scale\n\nDefinition\nThe ordinal scale categorizes data into ordered categories, but the intervals between categories are not necessarily equal or meaningful.\n\n\nProperties\n\nCategories have a defined order\nDifferences between categories are not quantifiable\nArithmetic operations on the numbers are not meaningful\n\n\n\nExamples\n\nEducation levels (High School, Bachelor‚Äôs, Master‚Äôs, PhD)\nLikert scales (Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree)\nSocioeconomic status (Low, Medium, High)\n\n\n\n\nInterval Scale\n\nDefinition\nThe interval scale has ordered categories with equal intervals between adjacent categories. However, it lacks a true zero point.\n\n\nProperties\n\nEqual intervals between adjacent categories\nNo true zero point (zero is arbitrary)\nRatios between values are not meaningful\n\n\n\nExamples\n\nTemperature in Celsius or Fahrenheit\nCalendar years\npH scale (the difference between pH 4 and 5 represents the same change in hydrogen ion concentration as between pH 6 and 7)\nElevation above sea level\n\n\n\n\nRatio Scale\n\nDefinition\nThe ratio scale is the highest level of measurement. It has all the properties of the interval scale plus a true zero point, making ratios between values meaningful.\n\n\nProperties\n\nAll properties of interval scales\nTrue zero point\nRatios between values are meaningful\n\n\n\nExamples\n\nHeight\nWeight\nAge\nIncome\n\n\n\n\n\n\n\nWhy Some Statistics Work (and Others Don‚Äôt) for Interval Scales\n\n\n\n\nKey Idea\nAn interval scale is one where the distances between values are meaningful, but the zero point is arbitrary. For interval scales (e.g., temperature):\n\nAllowed: Addition/subtraction of values and multiplication/division by constants.\nNot allowed: Multiplication/division of values from the scale by each other, as this leads to results without physical interpretation.\n\n\n\nProperties of Interval Scales\n\nEqual intervals represent the same differences:\n\nThe difference between 20¬∞C and 25¬∞C (5¬∞C) represents the same change as between 30¬∞C and 35¬∞C.\nProportions of differences are preserved: 10¬∞C is twice the change of 5¬∞C.\n\nThe zero point is arbitrary:\n\n0¬∞C is the freezing point of water, not the absence of temperature.\nThe same physical state has different values in different scales: 0¬∞C = 32¬∞F.\n\nLinear transformation:\n\nGeneral formula: y = ax + b, where a \\neq 0.\nFor temperature: F = C \\times \\frac{9}{5} + 32.\n\n\n\n\nTheoretical Conclusions\n\nAllowed operations:\n\nAddition/subtraction (preserves differences).\nMultiplication/division by constants (scaling).\nArithmetic means.\nComparing temperature differences.\n\nNot allowed operations:\n\nMultiplying temperatures by each other.\nDividing temperatures by each other.\nGeometric means.\nCoefficient of variation.\n\nPractical implications:\n\nVariance and standard deviation require careful interpretation.\nBetter to use measures based on differences (e.g., MAD - mean absolute deviation).\nWhen comparing variability, it is advisable to standardize the data.\n\n\n\n\nPractical Rule\nIf your calculations involve multiplying values from an interval scale by each other, be particularly cautious in interpreting the results!\n\n\n\n\n\n\nImportance in Research and Analysis\nUnderstanding Stevens‚Äô data typology is crucial for several reasons:\n\nChoosing appropriate statistical tests: The level of measurement determines which statistical analyses are appropriate for a given dataset.\nInterpreting results: The meaning of statistical results depends on the level of measurement of the variables involved.\nDesigning measurement instruments: When creating surveys or other measurement tools, researchers must consider the level of measurement they want to achieve.\nData transformation: Sometimes, data can be transformed from one level to another, but this must be done carefully to avoid misinterpretation.\n\n\n\nControversies and Limitations\nWhile Stevens‚Äô typology is widely used, it has faced some criticisms:\n\nRigidity: Some argue that the typology is too rigid and that many real-world measurements fall between these categories.\nTreatment of ordinal data: There‚Äôs ongoing debate about when it‚Äôs appropriate to treat ordinal data as interval for certain analyses.\nPsychological scaling: Some psychological constructs (like intelligence) are difficult to categorize definitively within this system.\n\n\n\nConclusion\nStevens‚Äô data typology provides a fundamental framework for understanding different types of data and their properties. By recognizing the level of measurement of their variables, researchers can make informed decisions about data collection, analysis, and interpretation. However, it‚Äôs important to remember that while this typology is a useful guide, real-world data often requires nuanced consideration and may not always fit neatly into these categories.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not ‚Äútwice as acidic‚Äù as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.\n\n\n\n\nReality: IQ is Fundamentally an Ordinal Scale\nHow IQ scores are created ‚Äì step by step:\n\nCollecting raw scores: People take a test and receive a number of correct answers (e.g., 45 out of 60 questions)\nOrdering: All raw scores are arranged from worst to best\nAssigning ranks: Each score is assigned a position in the ranking\nTransformation to IQ scale: Ranks are mathematically transformed so that the mean equals 100 and standard deviation equals 15\n\nKey problem: This process forces a normal distribution onto data that may not have been normal in its original form. This means that equal differences in IQ points (e.g., difference between IQ 100 and 115 vs.¬†difference between IQ 115 and 130) may not correspond to equal differences in actual cognitive abilities.\n\n\n\n\n\n\nKey Point\n\n\n\nIQ 130 does not mean ‚Äútwice the intelligence‚Äù of IQ 65. IQ points only show a person‚Äôs position relative to other people in the sample, not the actual amount of intelligence. This is similar to places in a competition ‚Äì the winner might win by a hair or by miles, but will still be in first place.\n\n\nIn research practice: why do we sometimes treat IQ as an interval scale?\nThis is a methodological compromise that allows for the use of more precise statistical tools:\n‚úÖ Treating IQ as an interval scale is acceptable when: - Using standard statistical tests (correlations, regressions, t-tests) - Comparing groups within the same test and population - Being aware of the limitations of this approach - Our conclusions don‚Äôt depend on differences being exactly equal\n‚ö†Ô∏è Remember the limitations: - This is a simplification of reality - The assumption works better for scores near the mean (IQ 85-115) than at the extremes - Results must be interpreted carefully\n‚ùå Never: - Say that IQ differences mean equal differences in intelligence - Use statements like ‚Äútwice as intelligent‚Äù - Forget that the normal distribution was imposed, not discovered in the data\n\n\nPractical Guidelines for Researchers\n\nBe transparent:\n\nClearly state: ‚ÄúWe treat IQ as an interval scale for statistical purposes, remembering that it is fundamentally an ordinal scale‚Äù\n\nConsider alternatives:\n\nUse non-parametric tests when sample size allows\nCompare results from different analytical methods\n\nInterpret cautiously:\n\nFocus on statements about order (‚Äúgroup A achieved higher scores than group B‚Äù)\nAvoid precise statements about the magnitude of differences\nRemember: a 15-point IQ difference means ‚Äúone standard deviation in the sample,‚Äù not ‚Äúa specific amount of additional intelligence‚Äù\n\n\n\n\n\n\n\n\nTip\n\n\n\nIQ is an ordinal scale that has been transformed to look like an interval scale. It can be used in statistical analyses requiring an interval scale, but one must always remember its true nature when interpreting results. The key is understanding that IQ points tell us about position in a group, not about the absolute amount of intelligence.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#common-ordinal-scales-in-behavioural-research",
    "href": "chapter2.html#common-ordinal-scales-in-behavioural-research",
    "title": "3¬† Understanding Data Types in Social Sciences",
    "section": "3.4 Common Ordinal Scales in Behavioural Research",
    "text": "3.4 Common Ordinal Scales in Behavioural Research\n\nLikert Scales\nLikert scales are widely used in psychology and social sciences to measure attitudes, opinions, and perceptions. Named after psychologist Rensis Likert, these scales typically consist of a series of statements or questions that respondents rate on a scale, often from ‚ÄúStrongly Disagree‚Äù to ‚ÄúStrongly Agree.‚Äù\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nWhy Likert Scales are Ordinal Variables\nLikert scales are considered ordinal variables for several reasons:\n\nOrder without equal intervals: While the responses have a clear order (e.g., ‚ÄúStrongly Disagree‚Äù &lt; ‚ÄúDisagree‚Äù &lt; ‚ÄúNeutral‚Äù &lt; ‚ÄúAgree‚Äù &lt; ‚ÄúStrongly Agree‚Äù), the intervals between these categories are not necessarily equal.\nSubjective interpretation: The difference between ‚ÄúStrongly Disagree‚Äù and ‚ÄúDisagree‚Äù may not be the same as the difference between ‚ÄúAgree‚Äù and ‚ÄúStrongly Agree‚Äù for all respondents.\nLack of true zero point: Likert scales typically don‚Äôt have a true zero point, which is a characteristic of interval or ratio scales.\n\n\n\n\nIQ and Other Psychological Variables as Ordinal Measures\nMany psychological measures, including IQ, are often treated as interval scales but are, in fact, ordinal. Here‚Äôs why:\n\nIQ Scores:\n\nWhile IQ scores are presented as numbers, the difference between an IQ of 100 and 110 may not represent the same cognitive difference as between 130 and 140.\nThe scale is normalized and adjusted over time, making it difficult to claim true interval properties.\n\nOther Psychological Measures:\n\nDepression scales (e.g., Beck Depression Inventory)\nAnxiety measures (e.g., State-Trait Anxiety Inventory)\nPersonality assessments (e.g., Big Five Inventory)\n\n\nThese measures often use summed Likert-type items or other scoring methods that don‚Äôt guarantee equal intervals between scores.\n\n\nImplications for Analysis\nRecognizing these measures as ordinal has important implications for data analysis:\n\nAppropriate statistical tests: Use non-parametric tests (e.g., Mann-Whitney U, Kruskal-Wallis) instead of parametric ones.\nCorrelation analysis: Use Spearman‚Äôs rank correlation instead of Pearson‚Äôs correlation.\nCentral tendency: Report median and mode rather than mean.\nData visualization: Use methods appropriate for ordinal data, such as bar plots or stacked bar charts.\n\n\n\nConclusion\nWhile Likert scales and many behavioural measures are often treated as interval data for practical reasons, it‚Äôs crucial to remember their ordinal nature.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n\n\n\n\nExercise: Identifying Measurement Scales\n\n\n\nFor each of the following variables, determine the most appropriate scale of measurement (Nominal, Ordinal, Interval, or Ratio). Also evaluate whether the variable is discrete or continuous.\n\nGender: nominal level of measurement, and discrete;\nCustomer satisfaction: Poor, Fair, Good, Excellent\nHeight (questionnaire): ‚ÄúI am: very short, short, average, tall, very tall‚Äù\nHeight (inches)\nReaction time (milliseconds)\nPostal codes: e.g., 61548, 61761, 62461, 47424, 65233\nAge (years)\nNationality\nStreet addresses\nMilitary ranks\nLeft-Right political scale placement\nFamily size: 1 child, 2 children, 3 children, ‚Ä¶\nIQ score\nShirt size (S, M, L, ‚Ä¶)\nMovie ratings (1 star, 2 stars, 3 stars)\nTemperature (Celsius)\nTemperature (Kelvin)\nBlood types: A, B, AB, O\nIncome categories: low, medium, high\nVoter turnout\nPolitical party affiliation\nElectoral district magnitude\n\nRemember to justify your choices for each variable.\nFor instance: In Stevens‚Äô typology of measurement scales, street addresses are nominal data. This is because:\nThey serve purely as labels/identifiers. They have no inherent ordering (123 Main St isn‚Äôt ‚Äúmore than‚Äù 23 Oak St). You can‚Äôt perform meaningful mathematical operations on them.The only valid operation is testing for equality/inequality (is this the same address or different?)",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#appendix-a",
    "href": "chapter2.html#appendix-a",
    "title": "3¬† Understanding Data Types in Social Sciences",
    "section": "3.5 Appendix A",
    "text": "3.5 Appendix A\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nMedian\nMediana\n‚àí\n‚úì\n‚úì\n‚úì\n\n\nArithmetic Mean\n≈örednia arytmetyczna\n‚àí\n‚àí\n‚úì\n‚úì\n\n\nGeometric Mean\n≈örednia geometryczna\n‚àí\n‚àí\n‚àí\n‚úì\n\n\nHarmonic Mean\n≈örednia harmoniczna\n‚àí\n‚àí\n‚àí\n‚úì\n\n\nTrimmed Mean\n≈örednia ucinana\n‚àí\n‚àí\n‚úì\n‚úì\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nFrequency Distribution\nRozk≈Çad czƒôsto≈õci\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nRange\nRozstƒôp\n‚àí\n‚úì\n‚úì\n‚úì\n\n\nInterquartile Range (IQR)\nRozstƒôp miƒôdzykwartylowy\n‚àí\n‚úì\n‚úì\n‚úì\n\n\nQuartile Deviation\nOdchylenie ƒáwiartkowe\n‚àí\n‚úì\n‚úì\n‚úì\n\n\nMean Absolute Deviation (MAD)\n≈örednie odchylenie bezwzglƒôdne\n‚àí\n‚àí\n‚úì\n‚úì\n\n\nVariance\nWariancja\n‚àí\n‚àí\n‚úì*\n‚úì\n\n\nStandard Deviation\nOdchylenie standardowe\n‚àí\n‚àí\n‚úì*\n‚úì\n\n\nCoefficient of Variation (CV)\nWsp√≥≈Çczynnik zmienno≈õci\n‚àí\n‚àí\n‚àí\n‚úì\n\n\nAssociation / Wsp√≥≈Çzale≈ºno≈õƒá:\n\n\n\n\n\n\n\nChi-square (œá¬≤)\nChi-kwadrat\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nCram√©r‚Äôs V\nV Cram√©ra\n‚úì\n‚úì\n‚àí\n‚àí\n\n\nSpearman‚Äôs rho (œÅ)\nKorelacja Spearmana\n‚àí\n‚úì\n‚úì\n‚úì\n\n\nKendall‚Äôs tau (œÑ)\nTau Kendalla\n‚àí\n‚úì\n‚úì\n‚úì\n\n\nPearson‚Äôs r\nKorelacja Pearsona\n‚àí\n‚àí\n‚úì*\n‚úì\n\n\nCovariance\nKowariancja\n‚àí\n‚àí\n‚úì*\n‚úì\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\nNotes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\nNominal: Categories without order (e.g., gender, color) / Kategorie bez uporzƒÖdkowania (np. p≈Çeƒá, kolor)\nOrdinal: Ordered categories (e.g., education level, satisfaction) / Kategorie uporzƒÖdkowane (np. poziom wykszta≈Çcenia, satysfakcja)\nInterval: Equal intervals, arbitrary zero (e.g., temperature in ¬∞C, IQ) / R√≥wne interwa≈Çy, umowne zero (np. temperatura w ¬∞C, IQ)\nRatio: Equal intervals, absolute zero (e.g., height, income) / R√≥wne interwa≈Çy, absolutne zero (np. wzrost, doch√≥d)\n\nPractical Considerations / Aspekty praktyczne:\n\nMeasures marked with * are commonly applied to interval data despite theoretical concerns about the arbitrary zero point / Miary oznaczone * sƒÖ powszechnie stosowane dla danych przedzia≈Çowych pomimo zastrze≈ºe≈Ñ teoretycznych dotyczƒÖcych umownego zera\nHigher-level scales inherit all measures from lower levels (ratio &gt; interval &gt; ordinal &gt; nominal) / Skale wy≈ºszego poziomu dziedziczƒÖ wszystkie miary z poziom√≥w ni≈ºszych\nFor skewed distributions, robust measures (median, IQR) may be preferred over mean and SD / Dla rozk≈Çad√≥w sko≈õnych, miary odporne (mediana, IQR) mogƒÖ byƒá preferowane nad ≈õredniƒÖ i SD\n\n\n\n\n\nKey improvements made: 1. Added Trimmed Mean, Frequency Distribution, Quartile Deviation, and Cram√©r‚Äôs V for completeness 2. Changed hyphens to proper minus signs (‚àí) for better typography 3. Added examples to scale definitions for clarity 4. Expanded the note about the asterisk to be more specific 5. Added a practical tip about robust measures 6. Minor formatting improvements for consistency",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html",
    "href": "rozdzial2.html",
    "title": "4¬† Typy Danych w Naukach Spo≈Çecznych",
    "section": "",
    "text": "4.1 Podstawy Zbior√≥w Liczbowych\nW badaniach z obszaru nauk spo≈Çecznych zrozumienie natury danych jest kluczowe dla wyboru odpowiednich metod analizy i wyciƒÖgania prawid≈Çowych wniosk√≥w.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Typy Danych w Naukach Spo≈Çecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#podstawy-zbior√≥w-liczbowych",
    "href": "rozdzial2.html#podstawy-zbior√≥w-liczbowych",
    "title": "4¬† Typy Danych w Naukach Spo≈Çecznych",
    "section": "",
    "text": "Note\n\n\n\nZrozumienie w≈Ça≈õciwo≈õci zbior√≥w liczbowych jest kluczowe dla uchwycenia natury r√≥≈ºnych typ√≥w danych w naukach spo≈Çecznych.\n\n\n\nPodstawowe Zbiory Liczbowe\n\nLiczby Naturalne (‚Ñï): Liczby u≈ºywane do liczenia obiekt√≥w {0, 1, 2, 3, ‚Ä¶}\nLiczby Ca≈Çkowite (‚Ñ§): ObejmujƒÖ liczby naturalne, ich przeciwno≈õci i zero {‚Ä¶, -2, -1, 0, 1, 2, ‚Ä¶}\nLiczby Wymierne (‚Ñö): Liczby, kt√≥re mo≈ºna wyraziƒá jako u≈Çamek dw√≥ch liczb ca≈Çkowitych\nLiczby Rzeczywiste (‚Ñù): Wszystkie liczby na osi liczbowej, w tym wymierne i niewymierne\n\n\n\nW≈Ça≈õciwo≈õci Zbior√≥w\n\nZbiory Przeliczalne: Zbiory, kt√≥rych elementy mo≈ºna ustawiƒá w relacji jeden do jednego z liczbami naturalnymi. Na przyk≈Çad, zbi√≥r liczb ca≈Çkowitych jest przeliczalny.\nZbiory Nieprzeliczalne: Zbiory, kt√≥re nie sƒÖ przeliczalne. Zbi√≥r liczb rzeczywistych jest nieprzeliczalny.\nZbiory Dyskretne: Zbiory, w kt√≥rych ka≈ºdy element jest oddzielony od innych element√≥w sko≈ÑczonƒÖ przerwƒÖ. Liczby ca≈Çkowite tworzƒÖ zbi√≥r dyskretny.\nZbiory Gƒôste: Zbiory, w kt√≥rych miƒôdzy dowolnymi dwoma elementami zawsze znajduje siƒô inny element zbioru. Liczby wymierne i rzeczywiste sƒÖ zbiorami gƒôstymi.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Typy Danych w Naukach Spo≈Çecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#dane-dyskretne-vs.-ciƒÖg≈Çe",
    "href": "rozdzial2.html#dane-dyskretne-vs.-ciƒÖg≈Çe",
    "title": "4¬† Typy Danych w Naukach Spo≈Çecznych",
    "section": "4.2 Dane Dyskretne vs.¬†CiƒÖg≈Çe",
    "text": "4.2 Dane Dyskretne vs.¬†CiƒÖg≈Çe\nW nauce o danych i statystyce czƒôsto kategoryzujemy zmienne jako dyskretne lub ciƒÖg≈Çe. Jednak rozr√≥≈ºnienie to nie zawsze jest jednoznaczne, a niekt√≥re zmienne wykazujƒÖ cechy obu typ√≥w. Ta sekcja bada koncepcje danych dyskretnych i ciƒÖg≈Çych, ich r√≥≈ºnice oraz interesujƒÖce przypadki zmiennych, kt√≥re mo≈ºna traktowaƒá jako oba typy lub kt√≥re kwestionujƒÖ nasze intuicyjne rozumienie.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDane Dyskretne\nDane dyskretne mogƒÖ przyjmowaƒá tylko okre≈õlone, przeliczalne warto≈õci. Te warto≈õci czƒôsto (ale nie zawsze) sƒÖ liczbami ca≈Çkowitymi.\n\nCechy Danych Dyskretnych:\n\nPrzeliczalne\nCzƒôsto reprezentowane przez liczby ca≈Çkowite\nMogƒÖ byƒá sko≈Ñczone lub niesko≈Ñczone\nBrak warto≈õci miƒôdzy dwoma sƒÖsiednimi punktami danych\n\n\n\nPrzyk≈Çady:\n\nLiczba student√≥w w klasie\nLiczba samochod√≥w sprzedanych przez dealera\nRozmiary but√≥w\n\n\n\n\nDane CiƒÖg≈Çe\nDane ciƒÖg≈Çe mogƒÖ przyjmowaƒá dowolnƒÖ warto≈õƒá w danym zakresie, w tym warto≈õci u≈Çamkowe i dziesiƒôtne. Wa≈ºne jest, aby zauwa≈ºyƒá, ≈ºe ciƒÖg≈Ço≈õƒá nie jest okre≈õlona wy≈ÇƒÖcznie przez nieprzeliczalno≈õƒá, ale r√≥wnie≈º przez gƒôsto≈õƒá.\n\nCechy Danych CiƒÖg≈Çych:\n\nMogƒÖ byƒá nieprzeliczalne (jak liczby rzeczywiste) lub gƒôste (jak liczby wymierne)\nMogƒÖ byƒá mierzone z dowolnƒÖ precyzjƒÖ (teoretycznie)\nReprezentowane przez liczby rzeczywiste lub gƒôste podzbiory liczb rzeczywistych\nZawsze istniejƒÖ warto≈õci miƒôdzy dowolnymi dwoma punktami danych\n\n\n\nPrzyk≈Çady:\n\nWzrost\nWaga\nTemperatura\nProcenty (wyja≈õnione dalej poni≈ºej)\n\n\n\n\nSpektrum Dyskretno-CiƒÖg≈Çe\nW praktyce niekt√≥re zmienne, kt√≥re matematycznie sƒÖ dyskretne, czƒôsto sƒÖ traktowane tak, jakby by≈Çy ciƒÖg≈Çe. Ta dwoista natura zapewnia elastyczno≈õƒá w analizie i interpretacji tych zmiennych.\n\nPowody Traktowania Danych Dyskretnych jako CiƒÖg≈Çych:\n\nGƒôsta Granularno≈õƒá\n\nGdy zmienna dyskretna ma du≈ºƒÖ liczbƒô mo≈ºliwych warto≈õci w danym zakresie, mo≈ºe przybli≈ºaƒá ciƒÖg≈Ço≈õƒá.\nPrzyk≈Çad: Doch√≥d mierzony w pojedynczych groszach. Choƒá technicznie dyskretny, du≈ºa liczba mo≈ºliwych warto≈õci sprawia, ≈ºe zachowuje siƒô podobnie do zmiennej ciƒÖg≈Çej.\n\nWygoda Analityczna\n\nMetody ciƒÖg≈Çe czƒôsto dajƒÖ rozsƒÖdne i u≈ºyteczne wyniki nawet dla gƒôstych zmiennych dyskretnych.\nCzƒôsto ≈Çatwiej jest u≈ºywaƒá istniejƒÖcych narzƒôdzi statystycznych, je≈õli za≈Ço≈ºymy ciƒÖg≈Ço≈õƒá, poniewa≈º pozwala to na stosowanie metod opartych na rachunku r√≥≈ºniczkowym.\n\nPrzybli≈ºenie Zjawisk Bazowych\n\nW niekt√≥rych przypadkach dyskretny pomiar mo≈ºe byƒá przybli≈ºeniem bazowego procesu ciƒÖg≈Çego.\nPrzyk≈Çad: Chocia≈º mierzymy czas w dyskretnych jednostkach (sekundy, minuty, godziny), sam czas jest ciƒÖg≈Çy.\n\n\n\n\nPrzyk≈Çady Zmiennych o Dwoistej Naturze Dyskretno-CiƒÖg≈Çej:\n\nWiek\n\nDyskretny: Typowo mierzony w pe≈Çnych latach\nCiƒÖg≈Çy: Mo≈ºe byƒá uznany za zmiennƒÖ ciƒÖg≈ÇƒÖ w wielu analizach, szczeg√≥lnie przy du≈ºych populacjach\n\nCena i Doch√≥d\n\nDyskretne: Ceny i dochody sƒÖ w rzeczywisto≈õci mierzone w dyskretnych jednostkach (np. grosze lub najmniejsza jednostka waluty)\nCiƒÖg≈Çe: W modelach ekonomicznych i wielu analizach ceny i dochody sƒÖ traktowane jako zmienne ciƒÖg≈Çe ze wzglƒôdu na ich gƒôstƒÖ naturƒô i wygodƒô analitycznƒÖ\n\nWyniki Test√≥w\n\nDyskretne: Czƒôsto podawane jako liczby ca≈Çkowite\nCiƒÖg≈Çe: W analizach statystycznych wyniki test√≥w mogƒÖ byƒá traktowane jako ciƒÖg≈Çe, szczeg√≥lnie gdy zakres mo≈ºliwych wynik√≥w jest du≈ºy\n\n\n\n\n\nPrzypadek Szczeg√≥lny: Procenty i Liczby Wymierne\nProcenty przedstawiajƒÖ interesujƒÖcy przypadek w spektrum dyskretno-ciƒÖg≈Çym:\n\nNatura Wymierna: Procenty sƒÖ zasadniczo u≈Çamkami (m/100), co czyni je liczbami wymiernymi.\nGƒôste, ale Przeliczalne: Zbi√≥r liczb wymiernych jest gƒôsty (miƒôdzy dowolnymi dwoma wymiernymi jest inny wymierny), ale tak≈ºe przeliczalny.\nPraktyczna CiƒÖg≈Ço≈õƒá: W wiƒôkszo≈õci praktycznych zastosowa≈Ñ procenty sƒÖ traktowane jako ciƒÖg≈Çe ze wzglƒôdu na ich gƒôstƒÖ naturƒô.\nSko≈Ñczona Precyzja: W rzeczywisto≈õci procenty sƒÖ czƒôsto podawane z ograniczonƒÖ liczbƒÖ miejsc po przecinku, tworzƒÖc sko≈Ñczony zbi√≥r mo≈ºliwych warto≈õci.\n\n\n\n\n\n\n\nProcenty: ≈ÅƒÖczenie Dyskretnego i CiƒÖg≈Çego\n\n\n\nZmienne mierzone w procentach, takie jak stopy bezrobocia czy frekwencja wyborcza, kwestionujƒÖ nasze intuicyjne rozumienie dyskretno≈õci i ciƒÖg≈Ço≈õci:\n\nSƒÖ liczbami wymiernymi (u≈Çamki z mianownikiem 100), kt√≥re technicznie sƒÖ przeliczalne.\nTworzƒÖ zbi√≥r gƒôsty w swoim zakresie (od 0% do 100%), pozwalajƒÖc na warto≈õci miƒôdzy dowolnymi dwoma procentami.\nW praktyce sƒÖ czƒôsto traktowane jako zmienne ciƒÖg≈Çe ze wzglƒôdu na ich gƒôstƒÖ naturƒô i wygodƒô analitycznƒÖ.\nPrecyzja pomiaru (np. podawanie do jednego lub dw√≥ch miejsc po przecinku) mo≈ºe narzuciƒá dyskretnƒÖ strukturƒô na to, co koncepcyjnie jest zbiorem gƒôstym.\n\nTa dwoisto≈õƒá pozwala na elastyczne podej≈õcia analityczne, w zale≈ºno≈õci od konkretnego kontekstu badawczego i wymaganej precyzji.\n\n\n\n\nImplikacje dla Analizy Danych\nZrozumienie zniuansowanej natury zmiennych jako dyskretnych, ciƒÖg≈Çych lub gdzie≈õ pomiƒôdzy ma wa≈ºne implikacje dla analizy danych:\n\nElastyczno≈õƒá w Modelowaniu: Pozwala na wykorzystanie szerszego zakresu technik statystycznych.\nUproszczone Obliczenia: Traktowanie gƒôstych danych dyskretnych jako ciƒÖg≈Çych mo≈ºe upro≈õciƒá obliczenia i uczyniƒá niekt√≥re analizy bardziej wykonalnymi.\nLepsza Interpretowalno≈õƒá: W niekt√≥rych przypadkach traktowanie danych dyskretnych jako ciƒÖg≈Çych mo≈ºe prowadziƒá do bardziej intuicyjnych lub u≈ºytecznych interpretacji wynik√≥w.\nPotencja≈Ç B≈Çƒôdu: Wa≈ºne jest, aby byƒá ≈õwiadomym, kiedy przybli≈ºenia sƒÖ odpowiednie, a kiedy mogƒÖ prowadziƒá do mylƒÖcych wynik√≥w.\nRozwa≈ºania Teoretyczne vs.¬†Praktyczne: Choƒá matematyczna natura danych jest wa≈ºna, praktyczne wzglƒôdy w pomiarze i analizie czƒôsto kierujƒÖ tym, jak traktujemy zmienne.\n\n\n\nWnioski\nRozr√≥≈ºnienie miƒôdzy danymi dyskretnymi a ciƒÖg≈Çymi nie zawsze jest sztywne w naukach spo≈Çecznych. Wiele zmiennych, w tym te dotyczƒÖce pieniƒôdzy, procent√≥w czy gƒôstych pomiar√≥w, mo≈ºna oglƒÖdaƒá przez pryzmat zar√≥wno dyskretny, jak i ciƒÖg≈Çy. Wyb√≥r sposobu traktowania powinien byƒá kierowany naturƒÖ danych, celami analizy i potencjalnymi implikacjami tego wyboru. Ta elastyczno≈õƒá, gdy jest u≈ºywana rozwa≈ºnie, zapewnia potƒô≈ºne narzƒôdzia dla badaczy nauk spo≈Çecznych do uzyskiwania wglƒÖdu w ich dane.\n\n\n\n\n\n\nDane Dyskretne vs.¬†CiƒÖg≈Çe: Analogia Jƒôzykowa\n\n\n\n\nKluczowe Rozr√≥≈ºnienie Jƒôzykowe\nW jƒôzyku polskim mamy precyzyjne rozr√≥≈ºnienie:\n\n‚ÄúLiczba‚Äù ‚Üí u≈ºywamy dla rzeczy policzalnych\n‚ÄúIlo≈õƒá‚Äù ‚Üí u≈ºywamy dla rzeczy niepoliczalnych\n\nTo rozr√≥≈ºnienie doskonale odzwierciedla dwa podstawowe typy danych liczbowych:\n\n\nDane Dyskretne = ‚ÄúLiczba czego≈õ‚Äù\n\nU≈ºywamy s≈Çowa ‚Äúliczba‚Äù (tak jak m√≥wimy ‚Äúliczba student√≥w‚Äù)\nWarto≈õci sƒÖ rozdzielone jak pojedyncze elementy\nPrzyk≈Çady:\n\nLiczba ksiƒÖ≈ºek: 0, 1, 2, 3‚Ä¶\nLiczba punkt√≥w w te≈õcie: 0, 1, 2‚Ä¶\nLiczba mieszka≈Ñc√≥w: 100, 101, 102‚Ä¶\n\n\nü§î Czy poprawne jest powiedzenie ‚Äúilo≈õƒá student√≥w‚Äù czy ‚Äúliczba student√≥w‚Äù? (Poprawna forma pomo≈ºe Ci rozpoznaƒá typ danych)\n\n\nDane CiƒÖg≈Çe = ‚ÄúIlo≈õƒá czego≈õ‚Äù\n\nU≈ºywamy s≈Çowa ‚Äúilo≈õƒá‚Äù (tak jak m√≥wimy ‚Äúilo≈õƒá wody‚Äù)\nWarto≈õci p≈Çynnie przechodzƒÖ jedna w drugƒÖ\nPrzyk≈Çady:\n\nIlo≈õƒá cieczy: 1,5231‚Ä¶ litra\nIlo≈õƒá czasu: 2,3891‚Ä¶ godziny\nIlo≈õƒá energii: 5,7123‚Ä¶ kWh\n\n\nü§î Czy m√≥wimy ‚Äúilo≈õƒá wody‚Äù czy ‚Äúliczba wody‚Äù? (Poprawna forma wskazuje na typ danych)\n\n\nSpos√≥b Rozpoznawania\n\nCzy u≈ºy≈Çby≈õ s≈Çowa ‚Äúliczba‚Äù? ‚Üí Dane dyskretne\nCzy u≈ºy≈Çby≈õ s≈Çowa ‚Äúilo≈õƒá‚Äù? ‚Üí Dane ciƒÖg≈Çe\n\n‚úçÔ∏è ƒÜwiczenie: Uzupe≈Çnij poprawnym s≈Çowem i okre≈õl typ danych\n\n_____ uczni√≥w w klasie (liczba/ilo≈õƒá): typ _____\n_____ deszczu (liczba/ilo≈õƒá): typ _____\n_____ piosenek (liczba/ilo≈õƒá): typ _____\n_____ temperatury (liczba/ilo≈õƒá): typ _____",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Typy Danych w Naukach Spo≈Çecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "href": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "title": "4¬† Typy Danych w Naukach Spo≈Çecznych",
    "section": "4.3 Wprowadzenie do Typologii Danych Stevensa",
    "text": "4.3 Wprowadzenie do Typologii Danych Stevensa\nStanley S. Stevens, ameryka≈Ñski psycholog, wprowadzi≈Ç system klasyfikacji skal pomiarowych w swoim artykule z 1946 roku ‚ÄúOn the Theory of Scales of Measurement‚Äù. Ten system, znany jako typologia danych Stevensa lub poziomy pomiaru, sta≈Ç siƒô fundamentalny dla zrozumienia, jak r√≥≈ºne typy danych powinny byƒá analizowane i interpretowane.\nStevens zaproponowa≈Ç cztery poziomy pomiaru:\n\nNominalny\nPorzƒÖdkowy\nInterwa≈Çowy\nIlorazowy\n\nKa≈ºdy poziom ma specyficzne w≈Ça≈õciwo≈õci i pozwala na r√≥≈ºne rodzaje operacji statystycznych i analiz.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\nSkala Nominalna\n\nDefinicja\nSkala nominalna jest najbardziej podstawowym poziomem pomiaru. U≈ºywa etykiet lub kategorii do klasyfikacji danych bez ≈ºadnej warto≈õci ilo≈õciowej ani porzƒÖdku.\n\n\nW≈Ça≈õciwo≈õci\n\nKategorie sƒÖ wzajemnie wykluczajƒÖce siƒô\nBrak inherentnego porzƒÖdku miƒôdzy kategoriami\nNie mo≈ºna wykonywaƒá znaczƒÖcych operacji arytmetycznych\n\n\n\nPrzyk≈Çady\n\nNarodowo≈õƒá (Polak, Niemiec, ‚Ä¶)\nGrupy krwi (A, B, AB, O)\nKolor oczu (Niebieskie, BrƒÖzowe, Zielone, Piwne)\nZmienne binarne (‚ÄúSukces‚Äù versus ‚ÄúNiepowodzenie‚Äù)\n\n\n\n\nSkala PorzƒÖdkowa\n\nDefinicja\nSkala porzƒÖdkowa kategoryzuje dane w uporzƒÖdkowane kategorie, ale odstƒôpy miƒôdzy kategoriami niekoniecznie sƒÖ r√≥wne lub znaczƒÖce.\n\n\nW≈Ça≈õciwo≈õci\n\nKategorie majƒÖ zdefiniowany porzƒÖdek\nR√≥≈ºnice miƒôdzy kategoriami nie sƒÖ kwantyfikowalne\nOperacje arytmetyczne na liczbach nie sƒÖ znaczƒÖce\n\n\n\nPrzyk≈Çady\n\nPoziomy wykszta≈Çcenia (Szko≈Ça ≈örednia, Licencjat, Magister, Doktorat)\nSkale Likerta (Zdecydowanie siƒô nie zgadzam, Nie zgadzam siƒô, Neutralnie, Zgadzam siƒô, Zdecydowanie siƒô zgadzam)\nStatus spo≈Çeczno-ekonomiczny (Niski, ≈öredni, Wysoki)\n\n\n\n\nSkala Interwa≈Çowa\n\nDefinicja\nSkala interwa≈Çowa ma uporzƒÖdkowane kategorie z r√≥wnymi odstƒôpami miƒôdzy sƒÖsiednimi kategoriami. Jednak brakuje jej prawdziwego punktu zerowego.\n\n\nW≈Ça≈õciwo≈õci\n\nR√≥wne odstƒôpy miƒôdzy sƒÖsiednimi kategoriami\nBrak prawdziwego punktu zerowego (zero jest umowne)\nStosunki miƒôdzy warto≈õciami nie sƒÖ znaczƒÖce\n\n\n\nPrzyk≈Çady\n\nTemperatura w stopniach Celsjusza lub Fahrenheita\nLata kalendarzowe\nSkala pH\nWysoko≈õƒá nad poziomem morza\n\n\n\n\nSkala Ilorazowa\n\nDefinicja\nSkala ilorazowa jest najwy≈ºszym poziomem pomiaru. Ma wszystkie w≈Ça≈õciwo≈õci skali interwa≈Çowej plus prawdziwy punkt zerowy, co sprawia, ≈ºe stosunki miƒôdzy warto≈õciami sƒÖ znaczƒÖce.\n\n\nW≈Ça≈õciwo≈õci\n\nWszystkie w≈Ça≈õciwo≈õci skal interwa≈Çowych\nPrawdziwy punkt zerowy\nStosunki miƒôdzy warto≈õciami sƒÖ znaczƒÖce\n\n\n\nPrzyk≈Çady\n\nWzrost\nWaga\nWiek\nDoch√≥d\n\n\n\n\nZnaczenie w Badaniach i Analizie\nZrozumienie typologii danych Stevensa jest kluczowe z kilku powod√≥w:\n\nWyb√≥r odpowiednich test√≥w statystycznych: Poziom pomiaru determinuje, kt√≥re analizy statystyczne sƒÖ odpowiednie dla danego zbioru danych.\nInterpretacja wynik√≥w: Znaczenie wynik√≥w statystycznych zale≈ºy od poziomu pomiaru zaanga≈ºowanych zmiennych.\nProjektowanie narzƒôdzi pomiarowych: Przy tworzeniu ankiet lub innych narzƒôdzi pomiarowych badacze muszƒÖ wziƒÖƒá pod uwagƒô poziom pomiaru, kt√≥ry chcƒÖ osiƒÖgnƒÖƒá.\nTransformacja danych: Czasami dane mogƒÖ byƒá przekszta≈Çcane z jednego poziomu na drugi, ale musi to byƒá robione ostro≈ºnie, aby uniknƒÖƒá b≈Çƒôdnej interpretacji.\n\n\n\nKontrowersje i Ograniczenia\nChocia≈º typologia Stevensa jest szeroko stosowana, spotka≈Ça siƒô z pewnymi krytykami:\n\nSztywno≈õƒá: Niekt√≥rzy twierdzƒÖ, ≈ºe typologia jest zbyt sztywna i ≈ºe wiele rzeczywistych pomiar√≥w mie≈õci siƒô pomiƒôdzy tymi kategoriami.\nTraktowanie danych porzƒÖdkowych: Trwa debata na temat tego, kiedy w≈Ça≈õciwe jest traktowanie danych porzƒÖdkowych jako interwa≈Çowych dla pewnych analiz.\nSkalowanie psychologiczne: Niekt√≥re konstrukty psychologiczne (jak inteligencja) sƒÖ trudne do jednoznacznego skategoryzowania w ramach tego systemu.\n\n\n\nPodsumowanie\nTypologia danych Stevensa dostarcza fundamentalnych ram dla zrozumienia r√≥≈ºnych rodzaj√≥w danych i ich w≈Ça≈õciwo≈õci. RozpoznajƒÖc poziom pomiaru swoich zmiennych, badacze mogƒÖ podejmowaƒá ≈õwiadome decyzje dotyczƒÖce gromadzenia danych, analizy i interpretacji. Jednak wa≈ºne jest, aby pamiƒôtaƒá, ≈ºe chocia≈º ta typologia jest u≈ºytecznym przewodnikiem, rzeczywiste dane czƒôsto wymagajƒÖ niuansowego podej≈õcia i nie zawsze pasujƒÖ idealnie do tych kategorii.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not ‚Äútwice as acidic‚Äù as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Typy Danych w Naukach Spo≈Çecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#popularne-skale-porzƒÖdkowe-w-badaniach-behawioralnych",
    "href": "rozdzial2.html#popularne-skale-porzƒÖdkowe-w-badaniach-behawioralnych",
    "title": "4¬† Typy Danych w Naukach Spo≈Çecznych",
    "section": "4.4 Popularne Skale PorzƒÖdkowe w Badaniach Behawioralnych",
    "text": "4.4 Popularne Skale PorzƒÖdkowe w Badaniach Behawioralnych\n\nSkale Likerta\nSkale Likerta sƒÖ szeroko stosowane w psychologii i naukach spo≈Çecznych do pomiaru postaw, opinii i percepcji. Nazwane na cze≈õƒá psychologa Rensisa Likerta, skale te zazwyczaj sk≈ÇadajƒÖ siƒô z serii stwierdze≈Ñ lub pyta≈Ñ, kt√≥re respondenci oceniajƒÖ na skali, czƒôsto od ‚ÄúZdecydowanie siƒô nie zgadzam‚Äù do ‚ÄúZdecydowanie siƒô zgadzam‚Äù.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\nDlaczego Skale Likerta sƒÖ Zmiennymi PorzƒÖdkowymi\nSkale Likerta sƒÖ uwa≈ºane za zmienne porzƒÖdkowe z kilku powod√≥w:\n\nPorzƒÖdek bez r√≥wnych odstƒôp√≥w: Chocia≈º odpowiedzi majƒÖ wyra≈∫nƒÖ kolejno≈õƒá (np. ‚ÄúZdecydowanie siƒô nie zgadzam‚Äù &lt; ‚ÄúNie zgadzam siƒô‚Äù &lt; ‚ÄúNeutralnie‚Äù &lt; ‚ÄúZgadzam siƒô‚Äù &lt; ‚ÄúZdecydowanie siƒô zgadzam‚Äù), odstƒôpy miƒôdzy tymi kategoriami niekoniecznie sƒÖ r√≥wne.\nSubiektywna interpretacja: R√≥≈ºnica miƒôdzy ‚ÄúZdecydowanie siƒô nie zgadzam‚Äù a ‚ÄúNie zgadzam siƒô‚Äù mo≈ºe nie byƒá taka sama jak r√≥≈ºnica miƒôdzy ‚ÄúZgadzam siƒô‚Äù a ‚ÄúZdecydowanie siƒô zgadzam‚Äù dla wszystkich respondent√≥w.\nBrak prawdziwego punktu zerowego: Skale Likerta zazwyczaj nie majƒÖ prawdziwego punktu zerowego, co jest cechƒÖ charakterystycznƒÖ skal interwa≈Çowych lub ilorazowych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Typy Danych w Naukach Spo≈Çecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#skale-pomiarowe-wed≈Çug-typologii-stevensa-zmienne-ilo≈õciowe-vs.-porzƒÖdkowe",
    "href": "rozdzial2.html#skale-pomiarowe-wed≈Çug-typologii-stevensa-zmienne-ilo≈õciowe-vs.-porzƒÖdkowe",
    "title": "4¬† Typy Danych w Naukach Spo≈Çecznych",
    "section": "4.5 Skale pomiarowe wed≈Çug typologii Stevensa: zmienne ilo≈õciowe vs.¬†porzƒÖdkowe",
    "text": "4.5 Skale pomiarowe wed≈Çug typologii Stevensa: zmienne ilo≈õciowe vs.¬†porzƒÖdkowe\n\nCzym sƒÖ zmienne ilo≈õciowe (numeryczne)?\nW typologii Stevensa wyr√≥≈ºniamy cztery skale pomiarowe. Zmienne ilo≈õciowe to te, kt√≥re mierzone sƒÖ na skalach interwa≈Çowych lub ilorazowych:\n\nSkala interwa≈Çowa: Posiada r√≥wne odstƒôpy miƒôdzy jednostkami, ale brak naturalnego punktu zerowego\nSkala ilorazowa: Posiada r√≥wne odstƒôpy miƒôdzy jednostkami oraz naturalny punkt zerowy\n\nZmienne ilo≈õciowe charakteryzujƒÖ siƒô nastƒôpujƒÖcymi w≈Ça≈õciwo≈õciami:\n\nR√≥wne interwa≈Çy: R√≥≈ºnica miƒôdzy 5 a 6 reprezentuje takƒÖ samƒÖ wielko≈õƒá jak r√≥≈ºnica miƒôdzy 95 a 96\nSp√≥jne jednostki: Ka≈ºdy przyrost reprezentuje takƒÖ samƒÖ ilo≈õƒá mierzonej cechy\nObiektywny pomiar: MierzƒÖ rzeczywiste ilo≈õci, a nie tylko wzglƒôdne pozycje\nDopuszczalne operacje matematyczne: Mo≈ºna wykonywaƒá operacje arytmetyczne\n\nPrzyk≈Çady zmiennych mierzonych na skali ilorazowej:\n\nWzrost: 170 cm jest dok≈Çadnie o 10 cm wy≈ºsze ni≈º 160 cm, a 170 cm jest dok≈Çadnie dwa razy wy≈ºsze ni≈º 85 cm\nWaga: R√≥≈ºnica miƒôdzy 50 kg a 60 kg to taka sama ilo≈õƒá wagi jak miƒôdzy 80 kg a 90 kg\nCzas: 4 godziny to dwa razy d≈Çu≈ºej ni≈º 2 godziny, a r√≥≈ºnica miƒôdzy 3 a 4 godzinami jest taka sama jak miƒôdzy 9 a 10 godzinami\nTemperatura w Kelwinach: 200K jest dwa razy cieplejsza ni≈º 100K (poniewa≈º 0K to zero absolutne)\n\nPrzyk≈Çad skali interwa≈Çowej:\n\nTemperatura w stopniach Celsjusza: R√≥≈ºnica miƒôdzy 20¬∞C a 30¬∞C jest taka sama jak miƒôdzy 70¬∞C a 80¬∞C, ale 40¬∞C nie jest ‚Äúdwa razy cieplejsze‚Äù ni≈º 20¬∞C (brak naturalnego zera)\nRok kalendarzowy: R√≥≈ºnica miƒôdzy 2020 a 2021 jest taka sama jak miƒôdzy 1950 a 1951, ale rok 2000 nie jest ‚Äúdwa razy starszy‚Äù ni≈º rok 1000\n\n\n\nRzeczywisto≈õƒá: IQ to fundamentalnie skala porzƒÖdkowa\nJak powstajƒÖ wyniki IQ ‚Äì krok po kroku:\n\nZbieranie surowych wynik√≥w: Ludzie rozwiƒÖzujƒÖ test i otrzymujƒÖ liczbƒô poprawnych odpowiedzi (np. 45 z 60 pyta≈Ñ)\nUporzƒÖdkowanie: Wszystkie surowe wyniki sƒÖ uszeregowane od najgorszego do najlepszego\nPrzypisanie rang: Ka≈ºdemu wynikowi przypisuje siƒô pozycjƒô w rankingu\nPrzekszta≈Çcenie na skalƒô IQ: Rangi sƒÖ przekszta≈Çcane matematycznie tak, aby ≈õrednia wynosi≈Ça 100, a odchylenie standardowe 15\n\nKluczowy problem: Proces ten wymusza rozk≈Çad normalny na dane, kt√≥re mo≈ºe wcale nie by≈Çy normalne w pierwotnej postaci. To znaczy, ≈ºe r√≥wne r√≥≈ºnice w punktach IQ (np. r√≥≈ºnica miƒôdzy IQ 100 a 115 vs.¬†r√≥≈ºnica miƒôdzy IQ 115 a 130) nie muszƒÖ odpowiadaƒá r√≥wnym r√≥≈ºnicom w rzeczywistych zdolno≈õciach poznawczych.\n\n\n\n\n\n\nImportant\n\n\n\nIQ 130 nie oznacza ‚Äûdwukrotnie wiƒôkszej inteligencji‚Äù ni≈º IQ 65. Punkty IQ pokazujƒÖ tylko pozycjƒô danej osoby wzglƒôdem innych ludzi w pr√≥bie, nie rzeczywistƒÖ ilo≈õƒá inteligencji. To podobnie jak miejsca w konkursie ‚Äì zwyciƒôzca mo≈ºe wygraƒá o w≈Ços lub o kilometry, ale nadal bƒôdzie pierwszym miejscem.\n\n\nW praktyce badawczej: dlaczego czasem traktujemy IQ jako skalƒô interwa≈ÇowƒÖ?\nJest to metodologiczny kompromis, kt√≥ry pozwala na u≈ºycie bardziej precyzyjnych narzƒôdzi statystycznych:\n‚úÖ Traktowanie IQ jako skali interwa≈Çowej jest dopuszczalne gdy: - U≈ºywamy standardowych test√≥w statystycznych (korelacje, regresje, testy t) - Por√≥wnujemy grupy w ramach tego samego testu i populacji - Jeste≈õmy ≈õwiadomi ogranicze≈Ñ tego podej≈õcia - Nasze wnioski nie zale≈ºƒÖ od tego, czy r√≥≈ºnice sƒÖ dok≈Çadnie r√≥wne\n‚ö†Ô∏è Pamiƒôtaj o ograniczeniach: - To uproszczenie rzeczywisto≈õci - Za≈Ço≈ºenie dzia≈Ça lepiej dla wynik√≥w bliskich ≈õredniej (IQ 85-115) ni≈º na kra≈Ñcach - Wyniki trzeba interpretowaƒá ostro≈ºnie\n‚ùå Nigdy nie wolno: - M√≥wiƒá, ≈ºe r√≥≈ºnice IQ oznaczajƒÖ r√≥wne r√≥≈ºnice w inteligencji - U≈ºywaƒá stwierdze≈Ñ typu ‚Äúdwa razy bardziej inteligentny‚Äù - Zapomnieƒá, ≈ºe normalny rozk≈Çad zosta≈Ç wymuszony, a nie odkryty w danych\n\n\nPraktyczne wskaz√≥wki dla badaczy\n\nBƒÖd≈∫ transparentny:\n\nWyra≈∫nie wspominaj: ‚ÄúTraktujemy IQ jako skalƒô interwa≈ÇowƒÖ do cel√≥w statystycznych, pamiƒôtajƒÖc ≈ºe fundamentalnie jest to skala porzƒÖdkowa‚Äù\n\nRozwa≈ºaj alternatywy:\n\nU≈ºywaj test√≥w nieparametrycznych, gdy wielko≈õƒá pr√≥by na to pozwala\nPor√≥wnaj wyniki r√≥≈ºnych metod analitycznych\n\nInterpretuj ostro≈ºnie:\n\nSkupiaj siƒô na stwierdzeniach o kolejno≈õci (‚Äûgrupa A osiƒÖgnƒô≈Ça wy≈ºsze wyniki ni≈º grupa B‚Äù)\nUnikaj precyzyjnych stwierdze≈Ñ o wielko≈õci r√≥≈ºnic\nPamiƒôtaj: r√≥≈ºnica 15 punkt√≥w IQ oznacza ‚Äûjedno odchylenie standardowe w pr√≥bie‚Äù, nie ‚ÄûkonkretnƒÖ ilo≈õƒá dodatkowej inteligencji‚Äù\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIQ to skala porzƒÖdkowa, kt√≥ra zosta≈Ça przekszta≈Çcona tak, aby wyglƒÖda≈Ça jak skala interwa≈Çowa. Mo≈ºna u≈ºywaƒá jej w analizach statystycznych wymagajƒÖcych skali interwa≈Çowej, ale zawsze nale≈ºy pamiƒôtaƒá o jej rzeczywistej naturze przy interpretacji wynik√≥w. Kluczowe jest zrozumienie, ≈ºe punkty IQ m√≥wiƒÖ nam o pozycji w grupie, nie o bezwzglƒôdnej ilo≈õci inteligencji.\n\n\nMimo liczbowego zapisu, wyniki IQ sƒÖ w istocie zmiennƒÖ porzƒÖdkowƒÖ (w typologii Stevensa), a nie zmiennƒÖ interwa≈ÇowƒÖ, poniewa≈º:\n\nBrak jednolitej jednostki miary: Nie istnieje naturalna jednostka mierzƒÖca ‚Äúinteligencjƒô‚Äù\nKonstrukcja oparta na rangach: Skala powstaje przez uszeregowanie ludzi wzglƒôdem siebie, a nastƒôpnie przekszta≈Çcenie tych rang w wyniki liczbowe\nNier√≥wne interwa≈Çy: R√≥≈ºnica miƒôdzy IQ 100 a 110 niekoniecznie reprezentuje takƒÖ samƒÖ r√≥≈ºnicƒô poznawczƒÖ jak miƒôdzy 130 a 140\nBrak absolutnego zera: Nie istnieje znaczƒÖce pojƒôcie ‚Äúzerowej inteligencji‚Äù\nZale≈ºno≈õƒá od testu: R√≥≈ºne testy IQ mogƒÖ daƒá r√≥≈ºne wyniki dla tej samej osoby\n\nPrzyk≈Çad: Rozwa≈ºmy trzy osoby z wynikami IQ 85, 100 i 115. Choƒá mogliby≈õmy chcieƒá powiedzieƒá, ≈ºe r√≥≈ºnica miƒôdzy pierwszƒÖ a drugƒÖ osobƒÖ r√≥wna siƒô r√≥≈ºnicy miƒôdzy drugƒÖ a trzeciƒÖ, nie jest to faktycznie znaczƒÖce‚Äîzdolno≈õci poznawcze reprezentowane przez te wyniki nie wzrastajƒÖ w r√≥wnych krokach, mimo ≈ºe cyfry sugerujƒÖ r√≥wne odstƒôpy. Wyniki te informujƒÖ nas g≈Ç√≥wnie o pozycji osoby wzglƒôdem innych, co jest cechƒÖ skali porzƒÖdkowej.\nInny przyk≈Çad: Osoba z IQ 140 nie jest ‚Äúdwa razy inteligentniejsza‚Äù ni≈º osoba z IQ 70, mimo ≈ºe stosunek liczb wynosi 2:1. Takie por√≥wnanie nie ma sensu na skali porzƒÖdkowej.\n\n\nPunkty egzaminacyjne - pomiƒôdzy skalƒÖ porzƒÖdkowƒÖ a interwa≈ÇowƒÖ\nChoƒá traktujemy punkty egzaminacyjne jak zmienne ilo≈õciowe (interwa≈Çowe), czƒôsto majƒÖ one cechy zmiennych porzƒÖdkowych:\n\nNier√≥wna trudno≈õƒá pyta≈Ñ: Pytanie za 10 punkt√≥w z fizyki kwantowej nie mierzy tej samej ilo≈õci wiedzy co pytanie za 10 punkt√≥w z podstawowej arytmetyki\nR√≥≈ºne rodzaje kompetencji: R√≥≈ºne pytania testujƒÖ r√≥≈ºne umiejƒôtno≈õci (zapamiƒôtywanie, rozumienie, zastosowanie, analiza)\nSubiektywne przydzielanie punkt√≥w: Punktacja zale≈ºy od oceny egzaminatora, a nie obiektywnych jednostek miary\nBrak addytywno≈õci: Student zdobywajƒÖcy 90 punkt√≥w niekoniecznie jest ‚Äúdwa razy bardziej wykszta≈Çcony‚Äù ni≈º student zdobywajƒÖcy 45 punkt√≥w\n\nPrzyk≈Çad: Wyobra≈∫my sobie dw√≥ch student√≥w:\n\nStudent A: Odpowiada prawid≈Çowo na wszystkie ≈Çatwe i ≈õrednie pytania (zdobywa 75 punkt√≥w)\nStudent B: Odpowiada prawid≈Çowo na wszystkie trudne pytania, ale ≈ºadne ≈Çatwe (zdobywa 75 punkt√≥w)\n\nMimo r√≥wnej punktacji, ich wiedza jest jako≈õciowo r√≥≈ºna. Punkty sugerujƒÖ r√≥wno≈õƒá, ale w rzeczywisto≈õci sƒÖ to r√≥≈ºne profile kompetencji ‚Äî typowy problem ze zmiennymi, kt√≥re nie sƒÖ w pe≈Çni ilo≈õciowe.\n\n\nTraktowanie zmiennych porzƒÖdkowych jako ilo≈õciowych w praktyce\nZe wzglƒôd√≥w praktycznych czƒôsto traktujemy zmienne porzƒÖdkowe jak zmienne ilo≈õciowe, poniewa≈º:\n\nUmo≈ºliwia to stosowanie znanych operacji matematycznych (≈õrednie, odchylenia)\nUpraszcza komunikacjƒô i interpretacjƒô wynik√≥w (‚Äú≈õredni wynik 78%‚Äù)\nPozwala na stosowanie bardziej zaawansowanych metod statystycznych\n\nPrzyk≈Çad: ≈örednia ocen\n\nObliczamy ≈õredniƒÖ, przypisujƒÖc warto≈õci liczbowe ocenom (5, 4, 3, 2, 1)\nTraktujemy te warto≈õci jak zmienne ilo≈õciowe, obliczajƒÖc np. ≈õredniƒÖ 4,5\nAle czy r√≥≈ºnica miƒôdzy ocenƒÖ 5 a 4 (5-4=1) reprezentuje takƒÖ samƒÖ r√≥≈ºnicƒô wiedzy jak miƒôdzy ocenƒÖ 2 a 1 (2-1=1)?\nI czy ≈õrednia 5.0 jest naprawdƒô ‚Äúdwa razy lepsza‚Äù ni≈º ≈õrednia 2.5?\n\nInny przyk≈Çad: Skale Likerta\n\nW ankietach czƒôsto stosujemy skale typu: 1 = ‚Äúzdecydowanie nie zgadzam siƒô‚Äù, 5 = ‚Äúzdecydowanie zgadzam siƒô‚Äù\nObliczamy ≈õrednie odpowiedzi, zak≈ÇadajƒÖc r√≥wne odstƒôpy miƒôdzy kategoriami\nAle czy odleg≈Ço≈õƒá miƒôdzy ‚Äúzdecydowanie nie zgadzam siƒô‚Äù a ‚Äúraczej nie zgadzam siƒô‚Äù jest naprawdƒô taka sama jak miƒôdzy ‚Äúraczej zgadzam siƒô‚Äù a ‚Äúzdecydowanie zgadzam siƒô‚Äù?\n\n\n\nZnaczenie rozr√≥≈ºnienia skal pomiarowych\nRozumienie typologii skal pomiarowych Stevensa ma istotne konsekwencje praktyczne:\n\nZmienne jako≈õciowe porzƒÖdkowe: PozwalajƒÖ na stwierdzenie, ≈ºe co≈õ jest ‚Äúwiƒôksze/lepsze‚Äù lub ‚Äúmniejsze/gorsze‚Äù, ale nie okre≈õlajƒÖ ‚Äúo ile‚Äù (dopuszczalne por√≥wnania typu &gt;, &lt;, =)\nZmienne ilo≈õciowe: PozwalajƒÖ na okre≈õlenie dok≈Çadnych r√≥≈ºnic i proporcji (dopuszczalne operacje +, -, √ó, √∑)\n\n≈öwiadomo≈õƒá ogranicze≈Ñ skali pomiarowej pomaga w poprawnej interpretacji danych i doborze odpowiednich metod analizy.\nPrzyk≈Çad: Je≈õli Anna uzyska≈Ça 75 punkt√≥w na te≈õcie z historii i 85 punkt√≥w na te≈õcie z matematyki, nie mo≈ºemy jednoznacznie stwierdziƒá, ≈ºe jest ‚Äúlepsza z matematyki o 10 jednostek umiejƒôtno≈õci‚Äù. Punkty z r√≥≈ºnych test√≥w nie sƒÖ bezpo≈õrednio por√≥wnywalne, a interwa≈Çy mogƒÖ nie byƒá r√≥wnowa≈ºne.\nPoprawniejsze podej≈õcie: Lepiej por√≥wnaƒá jej wyniki z rozk≈Çadem wynik√≥w innych uczni√≥w. Je≈õli w historii 75 punkt√≥w plasuje jƒÖ w 50. percentylu, a 85 punkt√≥w z matematyki w 90. percentylu, to mo≈ºemy powiedzieƒá, ≈ºe wzglƒôdnie rzecz biorƒÖc, radzi sobie lepiej z matematykƒÖ ni≈º historiƒÖ - co jest wnioskiem opartym na skali porzƒÖdkowej.\n\n\nPodsumowanie\nChocia≈º skale Likerta i wiele miar psychologicznych jest czƒôsto traktowanych jako dane interwa≈Çowe ze wzglƒôd√≥w praktycznych, wa≈ºne jest, aby pamiƒôtaƒá o ich porzƒÖdkowym charakterze.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n\n\n\n\nƒÜwiczenie: Identyfikacja Skal Pomiarowych\n\n\n\nDla ka≈ºdej z poni≈ºszych zmiennych okre≈õl najbardziej odpowiedniƒÖ skalƒô pomiaru (Nominalna, PorzƒÖdkowa, Przedzia≈Çowa lub Stosunkowa). Czy zmienna jest dyskretna, czy ciƒÖg≈Ça?\n\nP≈Çeƒá: skala nominalna; zmienna dyskretna;\nSatysfakcja klienta: Niska, ≈örednia, Dobra, Doskona≈Ça\nWzrost (ankieta): ‚ÄúJestem: bardzo niski, niski, przeciƒôtnego wzrostu, wysoki, bardzo wysoki‚Äù\nWzrost mierzony w centymetrach\nCzas reakcji (w milisekundach)\nKody pocztowe: np. 00-001, 00-950, 80-452, 31-072\nWiek (w latach)\nMarki samochod√≥w\nNarodowo≈õƒá\nLiczba dzieci w rodzinie: 1 dziecko, 2 dzieci, 3 dzieci, ‚Ä¶\nWynik testu IQ\nTemperatura (skala Celsjusza)\nTemperatura (skala Kelvina)\nFrekwencja wyborcza\nPrzynale≈ºno≈õƒá partyjna\nWielko≈õƒá okrƒôgu wyborczego\nWsp√≥≈Çrzƒôdne w uk≈Çadzie kartezja≈Ñskim\nData (wzglƒôdem okre≈õlonej epoki, np. n.e.)\nWysoko≈õƒá nad poziomem morza\nGrupy krwi: A, B, AB, 0\nKategorie dochod√≥w: niskie, ≈õrednie, wysokie\nStopnie wojskowe\n\nPamiƒôtaj, aby uzasadniƒá sw√≥j wyb√≥r skali dla ka≈ºdej zmiennej.\nDla przyk≈Çadu: W typologii skal pomiarowych Stevensa, adresy uliczne sƒÖ danymi nominalnymi. Dlaczego?\nPe≈ÇniƒÖ wy≈ÇƒÖcznie funkcjƒô etykiet/identyfikator√≥w Nie majƒÖ naturalnego uporzƒÖdkowania (ul. Mickiewicza 5 nie jest ‚Äúwiƒôksza‚Äù ni≈º ul. S≈Çowackiego 10) Nie mo≈ºna wykonywaƒá na nich sensownych operacji matematycznych Jedyna dozwolona operacja to sprawdzanie r√≥wno≈õci/nier√≥wno≈õci (czy to ten sam adres czy inny?)\nMimo ≈ºe numery dom√≥w sƒÖ liczbami, w systemie adresowym funkcjonujƒÖ jako etykiety, a nie warto≈õci ilo≈õciowe. Liczba 100 w adresie ‚Äúul. Kili≈Ñskiego 100‚Äù nie jest u≈ºywana matematycznie - r√≥wnie dobrze mog≈Çaby to byƒá ‚Äúul. Jab≈Çkowa‚Äù czy ‚Äúul. Zeusa‚Äù, je≈õli chodzi o jej funkcjƒô w adresie.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Typy Danych w Naukach Spo≈Çecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#appendix-a",
    "href": "rozdzial2.html#appendix-a",
    "title": "4¬† Typy Danych w Naukach Spo≈Çecznych",
    "section": "4.6 Appendix A",
    "text": "4.6 Appendix A\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nMedian\nMediana\n‚àí\n‚úì\n‚úì\n‚úì\n\n\nArithmetic Mean\n≈örednia arytmetyczna\n‚àí\n‚àí\n‚úì\n‚úì\n\n\nGeometric Mean\n≈örednia geometryczna\n‚àí\n‚àí\n‚àí\n‚úì\n\n\nHarmonic Mean\n≈örednia harmoniczna\n‚àí\n‚àí\n‚àí\n‚úì\n\n\nTrimmed Mean\n≈örednia ucinana\n‚àí\n‚àí\n‚úì\n‚úì\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nFrequency Distribution\nRozk≈Çad czƒôsto≈õci\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nRange\nRozstƒôp\n‚àí\n‚úì\n‚úì\n‚úì\n\n\nInterquartile Range (IQR)\nRozstƒôp miƒôdzykwartylowy\n‚àí\n‚úì\n‚úì\n‚úì\n\n\nQuartile Deviation\nOdchylenie ƒáwiartkowe\n‚àí\n‚úì\n‚úì\n‚úì\n\n\nMean Absolute Deviation (MAD)\n≈örednie odchylenie bezwzglƒôdne\n‚àí\n‚àí\n‚úì\n‚úì\n\n\nVariance\nWariancja\n‚àí\n‚àí\n‚úì*\n‚úì\n\n\nStandard Deviation\nOdchylenie standardowe\n‚àí\n‚àí\n‚úì*\n‚úì\n\n\nCoefficient of Variation (CV)\nWsp√≥≈Çczynnik zmienno≈õci\n‚àí\n‚àí\n‚àí\n‚úì\n\n\nAssociation / Wsp√≥≈Çzale≈ºno≈õƒá:\n\n\n\n\n\n\n\nChi-square (œá¬≤)\nChi-kwadrat\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nCram√©r‚Äôs V\nV Cram√©ra\n‚úì\n‚úì\n‚àí\n‚àí\n\n\nSpearman‚Äôs rho (œÅ)\nKorelacja Spearmana\n‚àí\n‚úì\n‚úì\n‚úì\n\n\nKendall‚Äôs tau (œÑ)\nTau Kendalla\n‚àí\n‚úì\n‚úì\n‚úì\n\n\nPearson‚Äôs r\nKorelacja Pearsona\n‚àí\n‚àí\n‚úì*\n‚úì\n\n\nCovariance\nKowariancja\n‚àí\n‚àí\n‚úì*\n‚úì\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\nNotes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\nNominal: Categories without order (e.g., gender, color) / Kategorie bez uporzƒÖdkowania (np. p≈Çeƒá, kolor)\nOrdinal: Ordered categories (e.g., education level, satisfaction) / Kategorie uporzƒÖdkowane (np. poziom wykszta≈Çcenia, satysfakcja)\nInterval: Equal intervals, arbitrary zero (e.g., temperature in ¬∞C, IQ) / R√≥wne interwa≈Çy, umowne zero (np. temperatura w ¬∞C, IQ)\nRatio: Equal intervals, absolute zero (e.g., height, income) / R√≥wne interwa≈Çy, absolutne zero (np. wzrost, doch√≥d)\n\nPractical Considerations / Aspekty praktyczne:\n\nMeasures marked with * are commonly applied to interval data despite theoretical concerns about the arbitrary zero point / Miary oznaczone * sƒÖ powszechnie stosowane dla danych przedzia≈Çowych pomimo zastrze≈ºe≈Ñ teoretycznych dotyczƒÖcych umownego zera\nHigher-level scales inherit all measures from lower levels (ratio &gt; interval &gt; ordinal &gt; nominal) / Skale wy≈ºszego poziomu dziedziczƒÖ wszystkie miary z poziom√≥w ni≈ºszych\nFor skewed distributions, robust measures (median, IQR) may be preferred over mean and SD / Dla rozk≈Çad√≥w sko≈õnych, miary odporne (mediana, IQR) mogƒÖ byƒá preferowane nad ≈õredniƒÖ i SD\n\n\n\n\n\nKey improvements made: 1. Added Trimmed Mean, Frequency Distribution, Quartile Deviation, and Cram√©r‚Äôs V for completeness 2. Changed hyphens to proper minus signs (‚àí) for better typography 3. Added examples to scale definitions for clarity 4. Expanded the note about the asterisk to be more specific 5. Added a practical tip about robust measures 6. Minor formatting improvements for consistency",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Typy Danych w Naukach Spo≈Çecznych</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "5.1 Introduction to Sigma Notation (Œ£)\nDescriptive statistics are fundamental tools in social science research, providing a concise summary of data characteristics. They serve several crucial functions:",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction-to-sigma-notation-œÉ",
    "href": "chapter5.html#introduction-to-sigma-notation-œÉ",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "What is Sigma summation notation? Sigma (Œ£) is a mathematical operator that instructs us to sum (add) a sequence of terms - it functions as a directive to perform addition of all elements within a specified range.\nPurpose: Provides a concise way to write sums of many similar terms using a single symbol, avoiding lengthy addition expressions.\n\n\nBasic Formula\n\nThe general form of sigma notation is: \\sum_{i=a}^{b} f(i)\nSummation index: i\nLower bound: a\nUpper bound: b\nFunction: f(i)\n\n\n\nExamples of Sigma Notation Applications\n\nSimple Example: Sum of Natural Numbers\n\nSuppose you want to add the first five positive integers: \\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15\nThe above notation adds the first five positive integers.\n\n\n\nSum of Squares\n\nSuppose you want to sum the squares of the first four positive integers: \\sum_{i=1}^{4} i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 1 + 4 + 9 + 16 = 30\nThis is the sum of squares of the first four positive integers.\n\n\n\nSum of a Constant Value\n\nSumming a constant value c for n terms: \\sum_{i=1}^{n} c = c + c + c + ... + c \\text{ (n times)} = n \\cdot c\nExample: Sum of five fives: \\sum_{i=1}^{5} 5 = 5 + 5 + 5 + 5 + 5 = 5 \\cdot 5 = 25\n\n\n\n\nSimple Examples in Statistical Context\n\\sum_{i=1}^{n} x_i - Summation index: i (typically denotes a specific observation in a dataset) - Lower bound: 1 (we usually start from the first observation) - Upper bound: n (total number of observations in our dataset) - Expression: x_i (value of the ith observation)\n\nSumming Observation Values\n\nWe have a dataset: 5, 8, 12, 15, 20\nSum of all values: \\sum_{i=1}^{5} x_i = x_1 + x_2 + x_3 + x_4 + x_5 = 5 + 8 + 12 + 15 + 20 = 60\nThis sum is a key element when calculating the arithmetic mean.\n\n\n\nSum of Deviations from the Mean\n\nFor the same dataset (5, 8, 12, 15, 20), the mean is \\bar{x} = 60/5 = 12\nSum of deviations from the mean: \\sum_{i=1}^{5} (x_i - \\bar{x}) = (5-12) + (8-12) + (12-12) + (15-12) + (20-12) = -7 + (-4) + 0 + 3 + 8 = 0\nImportant observation: The sum of deviations from the mean always equals 0, which is a fundamental property of the arithmetic mean.\n\n\n\n\nSummary\n\nSigma Notation (Œ£) allows for concise expression of key statistical formulas\nThe most important applications include calculating:\n\nArithmetic mean\nVariance and standard deviation\nVarious sums of squares used in regression analysis\n\n\n\n\n\n\n\n\nSummation (Œ£) and Product (Œ†) Operators\n\n\n\n\nSigma (Œ£) Operator\n\\sum is a summation operator that instructs us to add terms:\n\\sum_{i=1}^{n} x_i = x_1 + x_2 + ... + x_n\nwhere: - i is the index variable - The lower value under Œ£ (here i=1) is the starting point - The upper value (here n) is the ending point\n\n\nPi (Œ†) Operator\n\\prod is a product operator that instructs us to multiply terms:\n\\prod_{i=1}^{n} x_i = x_1 \\times x_2 \\times ... \\times x_n\nwhere: - i is the index variable - The lower value under Œ† (here i=1) is the starting point - The upper value (here n) is the ending point\n\n\n\n\n\n\n\n\n\nExample of Œ£\n\n\n\n\\sum_{i=1}^{4} i = 1 + 2 + 3 + 4 = 10\n\n\n\n\n\n\n\n\nExample of Œ†\n\n\n\n\\prod_{i=1}^{4} i = 1 \\times 2 \\times 3 \\times 4 = 24\n\n\n\n\n\n\n\n\nKey Differences\n\n\n\n\nŒ£ represents repeated addition\nŒ† represents repeated multiplication",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#types-of-data-distributions",
    "href": "chapter5.html#types-of-data-distributions",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.2 Types of Data Distributions",
    "text": "5.2 Types of Data Distributions\n\n\n\n\n\n\nImportant\n\n\n\nData distribution informs what values a variable takes and how often.\n\n\nUnderstanding data distributions is crucial for data analysis and visualization. In this document, we‚Äôll explore various types of distributions and how to visualize them using ggplot2 in R.\n\nNormal Distribution\nThe normal distribution, also known as the Gaussian distribution, is symmetric and bell-shaped.\n\n# Generate normal distribution data\nnormal_data &lt;- data.frame(x = rnorm(1000))\n\n# Plot\nggplot(normal_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Normal Distribution\", x = \"Value\", y = \"Density\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\nUniform Distribution\nIn a uniform distribution, all values have an equal probability of occurrence.\n\n# Generate uniform distribution data\nuniform_data &lt;- data.frame(x = runif(1000))\n\n# Plot\nggplot(uniform_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Uniform Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\nSkewed Distributions\nSkewed distributions are asymmetric, with one tail longer than the other.\n\n# Generate right-skewed data\nright_skewed &lt;- data.frame(x = rlnorm(1000))\n\n# Plot\nggplot(right_skewed, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightyellow\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Right-Skewed Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\nBimodal Distribution\nA bimodal distribution has two peaks, indicating two distinct subgroups in the data.\n\n# Generate bimodal data\nbimodal_data &lt;- data.frame(x = c(rnorm(500, mean = -2), rnorm(500, mean = 2)))\n\n# Plot\nggplot(bimodal_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightpink\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Bimodal Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nKey Properties\nExamples\n\n\n\n\nSymmetric (Normal)\nSymmetric, bell-shaped, most values close to the mean\nAdult height in population, IQ test scores, measurement errors, standardized exam results\n\n\nUniform\nEqual probability across the entire range\nLast digit of phone numbers, random day of the week selection, position of pointer after spinning a wheel of fortune\n\n\nBimodal\nTwo distinct peaks, suggests presence of subgroups\nAge structure in university towns (students and permanent residents), opinions on strongly polarizing topics, traffic intensity hours (morning and afternoon peak)\n\n\nRight-skewed (Positively skewed)\nExtended ‚Äútail‚Äù on the right side, most values less than the mean\nQueue waiting time, commute time to work, age at first marriage\n\n\nHeavy-tailed skewed (Log-normal)\nStrong right asymmetry, values cannot be negative, long ‚Äúfat tail‚Äù\nPersonal income, housing prices, household size\n\n\nExtreme-tailed skewed (Power law)\nExtreme asymmetry, ‚Äúrich get richer‚Äù effect, no characteristic scale\nWealth of the richest individuals, city populations, number of followers on social media, number of citations of scientific publications",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#visualizing-real-world-data-distributions",
    "href": "chapter5.html#visualizing-real-world-data-distributions",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.3 Visualizing Real-World Data Distributions",
    "text": "5.3 Visualizing Real-World Data Distributions\nLet‚Äôs use the palmerpenguins dataset to explore data distributions.\n\nHistogram and Density Plot\n\n\n\n\n\n\nUnderstanding Histograms and Density\n\n\n\n‚≠ê A histogram is a special graph for numerical data where:\n\nData is grouped into ranges (called ‚Äúbins‚Äù)\nBars touch each other (unlike bar charts!) because the data is continuous\nEach bar‚Äôs height shows how many values fall into that range\n\nThink of density as showing how common or concentrated certain values are in your data:\n\nA higher point on a density curve (or taller bar in a histogram) means those values appear more frequently in your data\nA lower point means those values are less common\n\nJust like a crowded area has more people per space (higher density), a taller part of the graph shows values that appear more often in your dataset!\n\n\n\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Distribution of Penguin Flipper Lengths\", \n       x = \"Flipper Length (mm)\", \n       y = \"Density\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\nBox Plot\nBox plots are useful for comparing distributions across categories.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Distribution of Penguin Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nViolin Plot\nViolin plots combine box plot and density plot features.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Distribution of Penguin Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nRidgeline Plot\nRidgeline plots are useful for comparing multiple distributions.\n\nlibrary(ggridges)\n\nggplot(penguins, aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.6) +\n  labs(title = \"Distribution of Flipper Length by Penguin Species\",\n       x = \"Flipper Length (mm)\",\n       y = \"Species\")\n\nPicking joint bandwidth of 2.38\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nUnderstanding and visualizing data distributions is crucial in data analysis. ggplot2 provides a flexible and powerful toolkit for creating various types of distribution plots. By exploring different visualization techniques, we can gain insights into the underlying patterns and characteristics of our data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-outliers",
    "href": "chapter5.html#understanding-outliers",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.4 Understanding Outliers",
    "text": "5.4 Understanding Outliers\nBefore diving into specific measures, it‚Äôs crucial to understand the concept of outliers, as they can significantly impact many descriptive statistics.\nOutliers are data points that differ significantly from other observations in the dataset. They can occur due to:\n\nMeasurement or recording errors\nGenuine extreme values in the population\n\nOutliers can have a substantial effect on many statistical measures, especially those based on means or sums of squared deviations. Therefore, it‚Äôs essential to:\n\nIdentify outliers through both statistical methods and domain knowledge\nInvestigate the cause of outliers\nMake informed decisions about whether to include or exclude them in analyses\n\nThroughout this guide, we‚Äôll discuss how different descriptive measures are affected by outliers.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#statistical-symbols-and-notations---summary",
    "href": "chapter5.html#statistical-symbols-and-notations---summary",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.5 Statistical Symbols and Notations - Summary",
    "text": "5.5 Statistical Symbols and Notations - Summary\n\n\n\n\n\n\n\n\n\n\nMeasure\nPopulation Parameter\nSample Statistic\nAlternative Notations\nUsage Notes\n\n\n\n\nSize\nN\nn\n-\nTotal count of observations\n\n\nMean\n\\mu\n\\bar{x}, m\nM, E(X)\nE(X) used in probability theory\n\n\nVariance\n\\sigma^2\ns^2\n\\text{Var}(X), V(X)\nSquared deviations from mean\n\n\nStandard Deviation\n\\sigma\ns\n\\text{SD}, \\text{std}\nSquare root of variance\n\n\nProportion\n\\pi, P\n\\hat{p}\n\\text{prop}\nRelative frequencies\n\n\nCorrelation\n\\rho\nr\n\\text{corr}(x,y)\nRanges from -1 to +1\n\n\nStandard Error\n\\sigma_{\\bar{x}}\ns_{\\bar{x}}\n\\text{SE}\nStandard error of mean\n\n\nSum\n\\sum\n\\sum\n\\sum_{i=1}^n\nWith indexing\n\n\nIndividual Value\nX_i\nx_i\n-\nith observation\n\n\nCovariance\n\\sigma_{xy}\ns_{xy}\n\\text{Cov}(X,Y)\nJoint variation\n\n\nMedian\n\\eta\n\\text{Med}\nM\nCentral value\n\n\nRange\nR\nr\n\\text{max}(X) - \\text{min}(X)\nSpread measure\n\n\nMode\n\\text{Mo}\n\\text{mo}\n\\text{mod}\nMost frequent value\n\n\nSkewness\n\\gamma_1\ng_1\n\\text{SK}\nDistribution asymmetry\n\n\nKurtosis\n\\gamma_2\ng_2\n\\text{KU}\nDistribution peakedness\n\n\n\nAdditional useful notations:\n\nSample moments: m_k = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^k\nPopulation moments: \\mu_k = E[(X - \\mu)^k]",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-central-tendency",
    "href": "chapter5.html#measures-of-central-tendency",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.6 Measures of Central Tendency",
    "text": "5.6 Measures of Central Tendency\nMeasures of central tendency aim to identify the ‚Äútypical‚Äù or ‚Äúcentral‚Äù value in a dataset. The three primary measures are mean, median, and mode.\n\nArithmetic Mean\nThe arithmetic mean is the sum of all values divided by the number of values.\nFormula: \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\nImportant Property: The mean is a balancing point in the data. The sum of deviations from the mean is always zero:\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0\nThis property makes the mean useful in many statistical calculations.\n\n\n\n\n\n\nUnderstanding Mean as a Balance Point üéØ\n\n\n\nLet‚Äôs consider a dataset X = \\{1, 2, 6, 7, 9\\} on a number line, imagining it as a seesaw:\n\n\n\nhttps://www.gastonsanchez.com/matrix4sl/mean-as-a-balancing-point.html\n\n\nThe mean (\\mu) acts as the perfect balance point of this seesaw. For our data:\n\\mu = \\frac{1 + 2 + 6 + 7 + 9}{5} = 5\n\nWhat happens at different support points? ü§î\n\nSupport point at 6 (too high):\n\nLeft side: Values (1, 2) are below\nRight side: Values (7, 9) are above\n\\sum distances from left = (6-1) + (6-2) = 9\n\\sum distances from right = (7-6) + (9-6) = 4\nThe seesaw tilts left! ‚¨ÖÔ∏è because 9 &gt; 4\n\nSupport point at 4 (too low):\n\nLeft side: Values (1, 2) are below\nRight side: Values (6, 7, 9) are above\n\\sum distances from left = (4-1) + (4-2) = 5\n\\sum distances from right = (6-4) + (7-4) + (9-4) = 10\nThe seesaw tilts right! ‚û°Ô∏è because 5 &lt; 10\n\nSupport point at mean (5) (perfect balance):\n\n\\sum distances below = \\sum distances above\n((5-1) + (5-2)) = ((6-5) + (7-5) + (9-5))\n7 = 7 ‚ú® Perfect balance!\n\n\nThis shows why the mean is the unique balance point, where:\n\\sum_{i=1}^n (x_i - \\mu) = 0\nThe seesaw will always tilt unless the support point is placed exactly at the mean! üé™\n\n\n\n\n\n\n\n\n\nMean as a Balance Point\n\n\n\nThis visualization shows how the arithmetic mean (5) acts as a balance point between clustered points on the left and dispersed points on the right:\nLeft side of the mean: - Points with values 2 and 3 - Close together (difference of 1 unit) - Distances from mean: 3 and 2 units - Sum of ‚Äúpull‚Äù = 5 units\nRight side of the mean: - Points with values 6 and 9 - More spread out (difference of 3 units) - Distances from mean: 1 and 4 units - Sum of ‚Äúpull‚Äù = 5 units\nKey observations:\n\nThe mean (5) is a balance point, even though:\n\nPoints on the left are clustered (2,3)\nPoints on the right are dispersed (6,9)\nGreen arrows show distances from the mean\n\nBalance is maintained because:\n\nSum of distances balances out: (5-2) + (5-3) = (6-5) + (9-5)\nTotal sum of distances = 5 units on each side\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual Calculation Example:\nLet‚Äôs calculate the mean for the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nSum all values\n2 + 4 + 4 + 5 + 5 + 7 + 9 = 36\n\n\n2\nCount the number of values\nn = 7\n\n\n3\nDivide the sum by n\n36 / 7 = 5.14\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(data)\n\n[1] 5.142857\n\n\nPros:\n\nEasy to calculate and understand\nUses all data points\nUseful for further statistical calculations\n\nCons:\n\nSensitive to outliers\nNot ideal for skewed distributions\n\nExample with outlier:\n\ndata_with_outlier &lt;- c(2, 4, 4, 5, 5, 7, 100)\nmean(data_with_outlier)\n\n[1] 18.14286\n\n\nAs we can see, the outlier (100) drastically affects the mean.\n\n\nMedian\nThe median is the middle value when the data is ordered.\nManual Calculation Example:\nUsing the same dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nResult\n\n\n\n\n1\nOrder the data\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nFind the middle value\n5\n\n\n\nFor even number of values, take the average of the two middle values.\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmedian(data)\n\n[1] 5\n\nmedian(data_with_outlier)\n\n[1] 5\n\n\nPros:\n\nNot affected by extreme outliers\nBetter for skewed distributions\n\nCons:\n\nDoesn‚Äôt use all data points\nLess useful for further statistical calculations\n\n\n\n\n\n\n\nWarning\n\n\n\nTo find the position of the median in a dataset:\n\nFirst sort the data in ascending order\nIf n is odd:\n\nMedian position = \\frac{n + 1}{2}\n\nIf n is even:\n\nFirst median position = \\frac{n}{2}\nSecond median position = \\frac{n}{2} + 1\nMedian = \\frac{\\text{value at }\\frac{n}{2} + \\text{value at }(\\frac{n}{2}+1)}{2}\n\n\nFor example:\n\nOdd n=7: position = \\frac{7+1}{2} = 4th value\nEven n=8: positions = \\frac{8}{2} = 4th and 4+1 = 5th value\n\n\n\n\n\nMode\nThe mode is the most frequently occurring value.\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nValue\nFrequency\n\n\n\n\n2\n1\n\n\n4\n2\n\n\n5\n2\n\n\n7\n1\n\n\n9\n1\n\n\n\nThe mode is 4 and 5 (bimodal).\nR calculation:\n\nlibrary(modeest)\nmfv(data)  # Most frequent value\n\n[1] 4 5\n\n\nPros:\n\nOnly measure of central tendency for nominal data\nCan identify multiple peaks in the data\n\nCons:\n\nNot always uniquely defined\nNot useful for continuous data\n\n\n\nWeighted (arithmetic) Mean (*)\nThe weighted mean is used when some data points are more important than others. There are two types of weighted means: with not normalized weights and with normalized weights.\n\nWeighted Mean with Not Normalized Weights\nThis is the standard form of the weighted mean, where weights can be any positive numbers representing the importance of each data point.\nFormula: \\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\nManual Calculation Example:\nLet‚Äôs calculate the weighted mean for the dataset: 2, 4, 5, 7 with weights 1, 2, 3, 1\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nMultiply each value by its weight\n(2 * 1) + (4 * 2) + (5 * 3) + (7 * 1) = 2 + 8 + 15 + 7 = 32\n\n\n2\nSum the weights\n1 + 2 + 3 + 1 = 7\n\n\n3\nDivide the result from step 1 by the result from step 2\n32 / 7 = 4.57\n\n\n\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\nWeighted Mean with Normalized Weights (Fractions)\nIn this case, the weights are fractions that sum to 1, representing the proportion of importance for each data point.\nFormula: \\bar{x}_w = \\sum_{i=1}^n w_i x_i, where \\sum_{i=1}^n w_i = 1\nManual Calculation Example:\nLet‚Äôs calculate the weighted mean for the dataset: 2, 4, 5, 7 with normalized weights 0.1, 0.3, 0.4, 0.2\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nMultiply each value by its weight\n(2 * 0.1) + (4 * 0.3) + (5 * 0.4) + (7 * 0.2)\n\n\n2\nSum the results\n0.2 + 1.2 + 2.0 + 1.4 = 4.8\n\n\n\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Note: these sum to 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nPros of Weighted Means:\n\nAccount for varying importance of data points\nUseful in survey analysis with different sample sizes or importance levels\nCan adjust for unequal probabilities in sampling designs\n\nCons of Weighted Means:\n\nRequire justification for weights\nCan be misused to manipulate results\nMay be less intuitive to interpret than simple arithmetic mean",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-variability",
    "href": "chapter5.html#measures-of-variability",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.7 Measures of Variability",
    "text": "5.7 Measures of Variability\nThese measures describe how spread out the data is. They are crucial for understanding the dispersion of data points around the central tendency.\n\n\n\n\n\n\nUnderstanding Variance\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†5.1: Three dot plots showing increasing variance with constant mean\n\n\n\n\n\nThe three dot plots above demonstrate how variance measures the spread of data around a central value:\n\nAll distributions have the same mean (Œº = 10), shown by the dashed line\nLow Variance (œÉ¬≤ = 1): Points cluster tightly around the mean\nMedium Variance (œÉ¬≤ = 4): Points show moderate spread\nHigh Variance (œÉ¬≤ = 9): Points spread widely around the mean\n\n\n\n\n\n\n\n\n\nUnderstanding Different Levels of Variability\n\n\n\n\n\n\n\n\n\n\n\n\nThis visualization shows three normal distributions with the same mean (Œº = 10) but different levels of variability:\n\nLow Variability (œÉ = 0.5)\n\nData points cluster tightly around the mean\nThe density curve is tall and narrow\nMost observations fall within ¬±0.5 units of the mean\n\nMedium Variability (œÉ = 2.0)\n\nData points spread out more from the mean\nThe density curve is lower and wider\nMost observations fall within ¬±2 units of the mean\n\nHigh Variability (œÉ = 4.0)\n\nData points spread widely from the mean\nThe density curve is much flatter and wider\nMost observations fall within ¬±4 units of the mean\n\n\n\n\n\nRange\nThe range is the difference between the maximum and minimum values.\nFormula: R = x_{max} - x_{min}\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nFind the maximum value\n9\n\n\n2\nFind the minimum value\n2\n\n\n3\nSubtract minimum from maximum\n9 - 2 = 7\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nrange(data)\n\n[1] 2 9\n\nmax(data) - min(data)\n\n[1] 7\n\n\nPros:\n\nSimple to calculate and understand\nGives an immediate sense of data spread\n\nCons:\n\nExtremely sensitive to outliers\nDoesn‚Äôt provide information about the distribution between extremes\n\n\n\nInterquartile Range (IQR)\nThe IQR is the difference between the 75th and 25th percentiles.\nFormula: IQR = Q_3 - Q_1\nTo find quartiles manually:\n\nFor odd number of values:\n\nQ2 (median) is the middle value\nQ1 is the median of the lower half (excluding the median of all observations)\nQ3 is the median of the upper half (excluding the median of all observations)\n\nFor even number of values:\n\nQ2 is the average of the two middle values\nQ1 is the median of the lower half (excluding the median of all observations)\nQ3 is the median of the upper half (excluding the median of all observations)\n\n\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nOrder the data\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nFind Q2 (median)\n5\n\n\n3\nFind Q1 (median of lower half)\n4\n\n\n4\nFind Q3 (median of upper half)\n7\n\n\n5\nCalculate IQR\nQ3 - Q1 = 7 - 4 = 3\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nprint(data)\n\n[1] 2 4 4 5 5 7 9\n\nquantile(data, type = 1)\n\n  0%  25%  50%  75% 100% \n   2    4    5    7    9 \n\nIQR(data, type = 1)\n\n[1] 3\n\n\nPros:\n\nRobust to outliers\nProvides information about the spread of the middle 50% of the data\n\nCons:\n\nIgnores the tails of the distribution\nLess efficient than standard deviation for normal distributions\n\n\n\nVariance\nVariance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n\n\n\n\n\n\nVariance: Understanding Average Squared Deviations\n\n\n\nWhat is Variance? Variance measures how ‚Äúspread out‚Äù numbers are from their mean - it‚Äôs the average of squared deviations from the mean.\nFormula: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nSimple Example: Consider numbers: 2, 4, 6, 8, 10 Mean (\\bar{x}) = 6\n\n\n\n\n\n\n\n\n\nCalculating Deviations:\n\n\n\n\n\n\n\n\n\n\n\n\nValue\nDeviation from mean\nSquare of deviation\n\n\n\n\n2\n-4\n16\n\n\n4\n-2\n4\n\n\n6\n0\n0\n\n\n8\n+2\n4\n\n\n10\n+4\n16\n\n\n\nVariance = \\frac{16 + 4 + 0 + 4 + 16}{4} = 10\nKey Points:\n\nMean acts as a reference line (blue dashed line)\nDeviations show distance from mean (red dotted lines)\nSquaring makes all deviations positive (blue bars)\nLarger deviations contribute more to variance\n\n\n\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the mean\n\\bar{x} = 5.14\n\n\n2\nSubtract the mean from each value and square the result\n(2 - 5.14)^2 = 9.86\n\n\n\n\n(4 - 5.14)^2 = 1.30\n\n\n\n\n(4 - 5.14)^2 = 1.30\n\n\n\n\n(5 - 5.14)^2 = 0.02\n\n\n\n\n(5 - 5.14)^2 = 0.02\n\n\n\n\n(7 - 5.14)^2 = 3.46\n\n\n\n\n(9 - 5.14)^2 = 14.90\n\n\n3\nSum the squared differences\n30.86\n\n\n4\nDivide by (n-1), i.e.¬†by the number of observations - 1\n30.86 / 6 = 5.14\n\n\n\nR calculation:\n\nvar(data)\n\n[1] 5.142857\n\n\nPros:\n\nUses all data points\nFoundation for many statistical tests\n\nCons:\n\nUnits are squared, making interpretation less intuitive\nSensitive to outliers\n\n\n\n\n\n\n\nBessel‚Äôs Correction: Why We Divide by (n-1) And Not by n\n\n\n\nThe Key Insight:\nWhen we calculate deviations from the mean, they must sum to zero. This is a mathematical fact: \\sum(x_i - \\bar{x}) = 0\nThink of it Like This:\nIf you have 5 numbers and their mean:\n\nOnce you calculate 4 deviations from the mean\nThe 5th deviation MUST be whatever makes the sum zero\nYou don‚Äôt really have 5 independent deviations\nYou only have 4 truly ‚Äúfree‚Äù deviations\n\nSimple Example:\nNumbers: 2, 4, 6, 8, 10\n\nMean = 6\nDeviations: -4, -2, 0, +2, +4\nNotice they sum to zero\nIf you know any 4 deviations, the 5th is predetermined!\n\nThis is Why:\n\nWhen calculating variance: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nWe divide by (n-1) not n\nBecause only (n-1) deviations are truly independent\nThe last one is determined by the others\n\nDegrees of Freedom:\n\nn = number of observations\n1 = constraint (deviations must sum to zero)\nn-1 = degrees of freedom = number of truly independent deviations\n\nWhen to Use It:\n\nWhen calculating sample variance\nWhen calculating sample standard deviation\n\nWhen NOT to Use It:\n\nPopulation calculations (when you have all data)\n\nRemember:\n\nIt‚Äôs not just a statistical trick\nDeviations from the mean must sum to zero\nThis constraint costs us one degree of freedom\n\n\n\n\n\nStandard Deviation\nThe standard deviation is the square root of the variance and measures the average dispersion of the data about their arithmetic mean. In contrast to the variance, it has the advantage of being expressed in the same units as the original measurements, making its interpretation more intuitive.\nFormula: s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the variance\ns^2 = 5.14 (from previous calculation)\n\n\n2\nTake the square root\ns = \\sqrt{5.14} = 2.27\n\n\n\nR calculation:\n\nsd(data)\n\n[1] 2.267787\n\n\nPros:\n\nIn same units as original data\nWidely used and understood\n\nCons:\n\nStill sensitive to outliers\nAssumes data is roughly ‚Äúnormally‚Äù distributed\n\n\n\nCoefficient of Variation (*)\nThe coefficient of variation is the standard deviation divided by the mean, often expressed as a percentage.\nFormula: CV = \\frac{s}{\\bar{x}} \\times 100\\%\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the mean\n\\bar{x} = 5.14\n\n\n2\nCalculate the standard deviation\ns = 2.27\n\n\n3\nDivide s by the mean and multiply by 100\n(2.27 / 5.14) * 100 = 44.16\\%\n\n\n\nR calculation:\n\n(sd(data) / mean(data)) * 100\n\n[1] 44.09586\n\n\nPros:\n\nAllows comparison of variability between datasets with different units or means\nUseful in fields like finance for risk assessment\n\nCons:\n\nNot meaningful for data with both positive and negative values\nCan be misleading when mean is close to zero\n\n\n\n\n\n\n\nLimitations of Coefficient of Variation (CV)\n\n\n\nThe coefficient of variation, calculated as (œÉ/Œº) √ó 100\\%, has two important limitations:\n\nNot meaningful for data with both positive and negative values\n\nThe mean could be close to zero due to positive and negative values cancelling out\nExample: Dataset {-5, -3, 2, 6} has mean = 0\n\nCV = (std dev / 0) √ó 100%\nThis leads to division by zero\nEven if mean isn‚Äôt exactly zero, the CV doesn‚Äôt represent true relative variability when data cross zero\n\nThe CV assumes a natural zero point and meaningful ratios between values\n\n\n\nMisleading when mean is close to zero\n\nSince CV = (œÉ/Œº) √ó 100\\%, as Œº approaches zero:\n\nThe denominator becomes very small\nResults in extremely large CV values\nThese large values don‚Äôt meaningfully represent relative variability\n\nExample:\n\nDataset A: {0.001, 0.002, 0.003} has mean = 0.002\nEven small standard deviations will produce very large CVs\nThe resulting large CV might suggest extreme variability when the data are actually quite close together\n\n\n\n\nBest Use Cases\nCV is most useful for:\n\nStrictly positive data\nData measured on a ratio scale\nData with means well above zero\nComparing variability between datasets with different units or scales",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-relative-position-standing",
    "href": "chapter5.html#measures-of-relative-position-standing",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.8 Measures of Relative Position (Standing)",
    "text": "5.8 Measures of Relative Position (Standing)\nUnderstanding where values sit within a dataset is crucial for data analysis. Let‚Äôs explore these concepts step by step.\n\nQuartiles (Q): The Basics\nThink of quartiles as special numbers that split your ordered data into four equal parts.\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\n\nWhat Are Quartiles?\nFirst Quartile (Q1):\n\nSeparates the lowest 25% of data from the rest\nAlso called the 25th percentile\nExample: If Q1 = 50 in a test score dataset, 25% of students scored below 50\n\nSecond Quartile (Q2):\n\nThe median - splits data in half\nAlso called the 50th percentile\nExample: If Q2 = 70, half the students scored below 70\n\nThird Quartile (Q3):\n\nSeparates the highest 25% of data from the rest\nAlso called the 75th percentile\nExample: If Q3 = 85, 75% of students scored below 85\n\n\n\nHow to Calculate Quartiles (Step by Step) - Two Methods\nLet‚Äôs examine student test scores using both common quartile calculation methods:\nExample 1: Odd Number Case (11 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 88, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 11 values (odd)\nMedian position = (n + 1)/2 = 6\nQ2 = 78\n\nStep 2: Find Q1\n\nTukey‚Äôs Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (n + 1)/4 = (11 + 1)/4 = 3\nQ1 = 70 (3rd value)\n\n\nStep 3: Find Q3\n\nTukey‚Äôs Method:\n\nLook at upper half: 80, 82, 85, 88, 90\nQ3 = median of upper half = 85\n\nInterpolation Method:\n\nPosition = 3(n + 1)/4 = 3(12)/4 = 9\nQ3 = 85 (9th value)\n\n\nExample 2: Even Number Case (10 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 10 values (even)\nMedian positions = 5 and 6\nQ2 = (75 + 78)/2 = 76.5\n\nStep 2: Find Q1\n\nTukey‚Äôs Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (10 + 1)/4 = 2.75\nQ1 = 65 + 0.75(70 - 65) = 68.75\n\n\nStep 3: Find Q3\n\nTukey‚Äôs Method:\n\nLook at upper half: 78, 80, 82, 85, 90\nQ3 = median of upper half = 82\n\nInterpolation Method:\n\nPosition = 3(10 + 1)/4 = 8.25\nQ3 = 82 + 0.25(85 - 82) = 82.75\n\n\nImportant Notes:\n\nTukey‚Äôs Method:\n\nFirst find the median (Q2)\nSplit the data into lower and upper halves\nFind Q1 as the median of the lower half\nFind Q3 as the median of the upper half\nWhen n is odd, the median is not included in either half\n\nInterpolation Method:\n\nUses positions (n+1)/4 for Q1 and 3(n+1)/4 for Q3\nWhen position falls between values, uses linear interpolation\nDoesn‚Äôt require splitting data into halves\n\n\nBoth methods give the same results for simple positions (Example 1) but can differ when interpolation is needed (Example 2).\n\n\n\n\n\n\nManual Construction of Tukey Boxplot\n\n\n\nStep 1: Calculate Key Components\n\nFind quartiles: Q_1, Q_2 (median), Q_3\nCalculate Interquartile Range: IQR = Q_3 - Q_1\n\nStep 2: Determine Whisker Boundaries\n\nLower fence: Q_1 - 1.5 \\times IQR\nUpper fence: Q_3 + 1.5 \\times IQR\n\nStep 3: Identify Outliers Data points are outliers if they are:\n\nBelow lower fence: x &lt; Q_1 - 1.5 \\times IQR\nAbove upper fence: x &gt; Q_3 + 1.5 \\times IQR\n\nExample: Given data: 2, 4, 6, 8, 9, 10, 11, 12, 14, 16, 50\n\nFind quartiles:\n\nQ_1 = 6\nQ_2 = 10\nQ_3 = 14\n\nCalculate IQR:\n\nIQR = 14 - 6 = 8\n\nCalculate fences:\n\nLower: 6 - (1.5 \\times 8) = -6\nUpper: 14 + (1.5 \\times 8) = 26\n\nIdentify outliers:\n\n50 &gt; 26, therefore 50 is an outlier\n\n\nGraphical Elements:\n\nBox: Draw from Q_1 to Q_3\nLine inside box: Draw at Q_2\nWhiskers: Extend to most extreme non-outlier points\nPoints: Plot outliers individually beyond whiskers\n\n\n\n\n\n\nPercentiles: A More Precise Measure of Relative Standing (*)\n\nWhat Are Percentiles?\nPercentiles give us a more detailed view by dividing data into 100 equal parts.\nKey Points:\n\nThe 25th percentile equals Q1\nThe 50th percentile equals Q2 (median)\nThe 75th percentile equals Q3\n\n\n\nCalculating Percentiles\nThe Formula: P_k = \\frac{k(n+1)}{100}\nWhere:\n\nP_k is the position for the kth percentile\nk is the percentile we want (1-100)\nn is the number of observations\n\nExample 3: Finding the 60th Percentile Let‚Äôs use student homework scores: 72, 75, 78, 80, 82, 85, 88, 90, 92, 95\nStep 1: Calculate position\n\nn = 10 scores\nFor 60th percentile: P_{60} = \\frac{60(10+1)}{100} = 6.6\n\nStep 2: Find surrounding values\n\nPosition 6: score of 85\nPosition 7: score of 88\n\nStep 3: Interpolate (important: percentiles use linear interpolation)\n\nWe need to go 0.6 of the way between 85 and 88 P_{60} = 85 + 0.6(88-85) P_{60} = 85 + 0.6(3) P_{60} = 85 + 1.8 = 86.8\n\nWhat this means: 60% of students scored 86.8 or below.\n\n\n\nPercentile Ranks (PR) (*)\n\nWhat is a Percentile Rank?\nWhile percentiles tell us the value at a certain position, percentile rank tells us what percentage of values fall below a specific score. Think of it as answering the question ‚ÄúWhat percentage of the class did I score higher than?‚Äù\nPR = \\frac{\\text{number of values below } + 0.5 \\times \\text{number of equal values}}{\\text{total number of values}} \\times 100\nExample 4: Finding a Percentile Rank Consider these exam scores:\n65, 70, 70, 75, 75, 75, 80, 85, 85, 90\nLet‚Äôs find the PR for a score of 75.\nStep 1: Count carefully\n\nValues below 75: 65, 70, 70 (3 values)\nValues equal to 75: 75, 75, 75 (3 values)\nTotal values: 10\n\nStep 2: Apply the formula\nPR = \\frac{3 + 0.5(3)}{10} \\times 100 PR = \\frac{3 + 1.5}{10} \\times 100 PR = \\frac{4.5}{10} \\times 100 = 45\\%\nInterpretation: A score of 75 is higher than 45% of the class scores.\nRemark:\nQ1: ‚ÄúWhy do we use 0.5 for equal values in PR?‚Äù\nA1: This is because we‚Äôre assuming people with the same score are evenly spread across that position. It‚Äôs like saying they share the position equally.\n\n\n\nUnderstanding and Interpreting Box Plots\nBox plots (also known as box-and-whisker plots) are powerful visualization tools for understanding data distributions. In this section, we‚Äôll explore how to construct and interpret box plots using height measurements from two groups.\n\nConstruction of the Tukey Box Plot\nThe box plot was introduced by John Tukey as part of his exploratory data analysis toolkit. It provides a standardized way of displaying the distribution of data based on a five-number summary.\n\nThe Five-Number Summary\nA box plot represents five key statistical values:\n\nMinimum: The smallest value in the dataset (excluding outliers)\nFirst Quartile (Q1): The 25th percentile, below which 25% of observations fall\nMedian (Q2): The 50th percentile, which divides the dataset into two equal halves\nThird Quartile (Q3): The 75th percentile, below which 75% of observations fall\nMaximum: The largest value in the dataset (excluding outliers)\n\n\n\nBox Plot Components\n\n\n\n\n\n\n\n\nFigure¬†5.2: Boxplot diagram showing its key components.\n\n\n\n\n\nThe components of a box plot include:\n\nThe Box:\n\nRepresents the interquartile range (IQR), containing the middle 50% of the data\nLower edge represents Q1\nUpper edge represents Q3\nLine inside the box represents the median (Q2)\n\nThe Whiskers:\n\nExtend from the box to show the range of non-outlier data\nIn a Tukey box plot, whiskers extend up to 1.5 √ó IQR from the box edges:\n\nLower whisker: extends to the minimum value ‚â• (Q1 - 1.5 √ó IQR)\nUpper whisker: extends to the maximum value ‚â§ (Q3 + 1.5 √ó IQR)\n\n\nOutliers:\n\nPoints that fall beyond the whiskers\nIndividually plotted as dots or symbols\nValues that are &lt; (Q1 - 1.5 √ó IQR) or &gt; (Q3 + 1.5 √ó IQR)\n\n\n\n\nKey Features to Observe\nWhen interpreting box plots, look for these characteristics:\n\nCentral Tendency: Location of the median line within the box\nDispersion: Width of the box (IQR) and length of the whiskers\nSkewness:\n\nSymmetrical data: median is approximately in the middle of the box, whiskers are roughly equal in length\nRight (positive) skew: median is closer to the bottom of the box, upper whisker is longer\nLeft (negative) skew: median is closer to the top of the box, lower whisker is longer\n\nOutliers: Presence of individual points beyond the whiskers\n\n\n\n\nCase Study: Comparing Heights Between Groups\nLet‚Äôs apply our understanding of box plots to a real dataset. We have height measurements (in centimeters) from two groups of 25 students each.\n\n# Create the dataset\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Transform dataset from wide to long format\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Display the first few rows\nhead(data_height_l)\n\n  Group_number height\n1      group_1    150\n2      group_1    160\n3      group_1    165\n4      group_1    168\n5      group_1    172\n6      group_1    173\n\n\nLet‚Äôs calculate some summary statistics for each group:\n\n# Calculate summary statistics for each group\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Calculate IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Create a comparison table\nstats_table &lt;- rbind(\n  group1_stats,\n  group2_stats\n)\nrownames(stats_table) &lt;- c(\"Group 1\", \"Group 2\")\n\n# Display the table\nstats_table\n\n        Min. 1st Qu. Median Mean 3rd Qu. Max.\nGroup 1  150     175    180  179     183  200\nGroup 2  138     165    175  172     182  210\n\n# Display IQR values\ncat(\"IQR for Group 1:\", group1_iqr, \"\\n\")\n\nIQR for Group 1: 8 \n\ncat(\"IQR for Group 2:\", group2_iqr, \"\\n\")\n\nIQR for Group 2: 17 \n\n\n\n\nVisualizing the Height Data\nNow, let‚Äôs visualize the data using box plots and density plots:\n\n# Create horizontal boxplots\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Height Distribution by Group\",\n       x = \"Group\",\n       y = \"Height (cm)\")\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` argument instead.\n‚Ñπ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` argument instead.\n‚Ñπ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\n\n\n\n\n\n\nFigure¬†5.3: Box plots comparing height distributions between groups.\n\n\n\n\n\nTo complement our box plots, let‚Äôs also look at the density distributions:\n\n# Create density plots\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Height Density by Group\",\n       x = \"Height (cm)\",\n       y = \"Density\")\n\n\n\n\n\n\n\nFigure¬†5.4: Density plots showing the height distributions for each group.\n\n\n\n\n\n\n\nBox Plot Interpretation Exercise\nBased on the box plots and density plots above, determine whether each of the following statements is True or False. For each statement, provide a brief explanation based on evidence from the visualizations.\n\n\n\n\n\n\nExercise Questions\n\n\n\n\nStudents from group 2 (G2) in the studied sample are, on average, taller than those from group 1 (G1).\nGroup 1 (G1) height measurements are more dispersed/spread out than group 2 (G2).\nThe lowest person is in group 2 (G2).\nBoth data sets are negatively (left) skewed.\nHalf of the students in group 2 (G2) measure at least 175 cm.\n\n\n\n\nHints for Interpretation\nWhen answering these questions, consider:\n\nThe position of the median line within each box\nThe relative sizes of the boxes (IQR)\nThe positions of the minimum and maximum values\nThe symmetry of the distributions (balanced or skewed)\nThe lengths of the whiskers\n\nFor each statement, determine whether it is True or False and provide your explanation:\n\n\n\n\n\n\nAnswer Template\n\n\n\n\n\n\nStudents from G2 are, on average, taller than G1: [True/False]\n\nExplanation:\n\nG1 height is more dispersed/spread out: [True/False]\n\nExplanation:\n\nThe lowest person is in G2: [True/False]\n\nExplanation:\n\nBoth data sets are negatively (left) skewed: [True/False]\n\nExplanation:\n\nHalf of G2 measure at least 175 cm: [True/False]\n\nExplanation:\n\n\n\n\n\nLet‚Äôs review the answers to our box plot interpretation questions:\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nStudents from G2 are, on average, taller than G1: False\n\nExplanation: The median height (middle line in the boxplot) for G1 is higher than G2.\n\nG1 height is more dispersed/spread out: False\n\nExplanation: G2 shows greater dispersion. This is visible in the boxplot where G2 has a larger interquartile range (IQR) of 17.5 cm compared to G1‚Äôs 9.5 cm. G2 also has a wider range from minimum to maximum values.\n\nThe lowest person is in G2: True\n\nExplanation: The minimum value in G2 is 138 cm, which is lower than the minimum value in G1 (150 cm).\n\nBoth data sets are negatively (left) skewed: True\n\nExplanation: In both groups, the median line is positioned toward the upper part of the box, and the lower whisker is longer than the upper whisker. This indicates that there‚Äôs a longer tail on the left side of the distribution, which means negative skewness.\n\nHalf of G2 measure at least 175 cm: True\n\nExplanation: The median (middle line in the boxplot) for G2 is 175 cm, which means that 50% of the values are greater than or equal to 175 cm.\n\n\n\n\n\n\n\n\nR Code Reference\nHere‚Äôs the complete R code used in this section:\n\n# Load required packages\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n# Set display options\noptions(scipen = 999, digits = 3)\n\n# Create the dataset\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Transform dataset from wide to long format\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Display the first few rows\nhead(data_height_l)\n\n# Calculate summary statistics for each group\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Calculate IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Create horizontal boxplots\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Height Distribution by Group\",\n       x = \"Group\",\n       y = \"Height (cm)\")\n\n# Create density plots\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Height Density by Group\",\n       x = \"Height (cm)\",\n       y = \"Density\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#shape-measures",
    "href": "chapter5.html#shape-measures",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.9 Shape Measures",
    "text": "5.9 Shape Measures\n\nSkewness\n\nDefinition\nSkewness quantifies the asymmetry of a data distribution. It indicates whether data tends to cluster more on one side of the mean than the other.\n\n\nMathematical Expression\nSK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3 where: - n is the sample size - x_i is the i-th observation - \\bar{x} is the sample mean - s is the sample standard deviation\n\n\nSimplified Numerical Example\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Three example datasets with different types of skewness\n# 1. Positive skewness (right tail)\npositive_skew_data &lt;- c(2, 3, 4, 4, 5, 5, 5, 6, 6, 7, 8, 12, 15, 20)\n# 2. Negative skewness (left tail)\nnegative_skew_data &lt;- c(1, 5, 10, 13, 14, 15, 16, 16, 17, 17, 18, 18, 19, 20)\n# 3. Near-zero skewness (symmetry)\nsymmetric_data &lt;- c(1, 3, 5, 7, 9, 10, 11, 12, 13, 15, 17, 19, 21)\n\n# Calculating skewness\npositive_skewness &lt;- skewness(positive_skew_data)\nnegative_skewness &lt;- skewness(negative_skew_data)\nsymmetric_skewness &lt;- skewness(symmetric_data)\n\n# Summary of results\nskewness_data &lt;- data.frame(\n  \"Distribution Type\" = c(\"Positive skewness\", \"Negative skewness\", \"Symmetric distribution\"),\n  \"Skewness value\" = round(c(positive_skewness, negative_skewness, symmetric_skewness), 3),\n  \"Interpretation\" = c(\n    \"Longer right tail (majority of data on the left side)\",\n    \"Longer left tail (majority of data on the right side)\",\n    \"Data distributed symmetrically\"\n  )\n)\n\n# Display table\nskewness_data\n\n       Distribution.Type Skewness.value\n1      Positive skewness           1.42\n2      Negative skewness          -1.33\n3 Symmetric distribution           0.00\n                                         Interpretation\n1 Longer right tail (majority of data on the left side)\n2 Longer left tail (majority of data on the right side)\n3                        Data distributed symmetrically\n\n\n\n\nVisualizations of Skewness Types\n\n# Create a data frame for all sets\ndf_skewness &lt;- rbind(\n  data.frame(value = positive_skew_data, type = \"Positive skewness\", \n             skewness = round(positive_skewness, 2)),\n  data.frame(value = negative_skew_data, type = \"Negative skewness\", \n             skewness = round(negative_skewness, 2)),\n  data.frame(value = symmetric_data, type = \"Symmetric distribution\", \n             skewness = round(symmetric_skewness, 2))\n)\n\n# Histograms for three types of skewness\np1 &lt;- ggplot(df_skewness, aes(x = value)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~type, scales = \"free_x\") +\n  geom_vline(data = df_skewness %&gt;% group_by(type) %&gt;% summarise(mean = mean(value)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_skewness %&gt;% group_by(type) %&gt;% summarise(median = median(value)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = unique(df_skewness[, c(\"type\", \"skewness\")]),\n           aes(x = Inf, y = Inf, label = paste(\"SK =\", skewness)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histograms showing different types of skewness\",\n    subtitle = \"Red line: mean, Green line: median\",\n    x = \"Value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n# Box plots\np2 &lt;- ggplot(df_skewness, aes(x = type, y = value, fill = type)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"skyblue\", \"lightgreen\", \"lightsalmon\")) +\n  labs(\n    title = \"Box plots for different types of skewness\",\n    x = \"Distribution type\",\n    y = \"Value\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Display plots\ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nExample: Voter Turnout Analysis\n\n# Generate three datasets reflecting different types of skewness\nset.seed(123)\n\n# 1. Positive skewness - typical for turnout in regions with low engagement\npositive_turnout &lt;- c(\n  runif(50, min = 20, max = 30),  # Small group with low turnout\n  rbeta(200, shape1 = 2, shape2 = 5) * 50 + 30  # Majority of results shifted to the left\n)\n\n# 2. Negative skewness - typical for regions with high political engagement\nnegative_turnout &lt;- c(\n  rbeta(200, shape1 = 5, shape2 = 2) * 30 + 50,  # Majority of results shifted to the right\n  runif(50, min = 40, max = 50)  # Small group with lower turnout\n)\n\n# 3. Symmetric distribution - typical for regions with uniform engagement\nsymmetric_turnout &lt;- rnorm(250, mean = 65, sd = 8)\n\n# Create data frame\ndf_turnout &lt;- rbind(\n  data.frame(turnout = positive_turnout, region = \"Region A: Positive skewness\"),\n  data.frame(turnout = negative_turnout, region = \"Region B: Negative skewness\"),\n  data.frame(turnout = symmetric_turnout, region = \"Region C: Symmetric distribution\")\n)\n\n# Calculate skewness for each region\nregion_skewness &lt;- df_turnout %&gt;%\n  group_by(region) %&gt;%\n  summarise(skewness = round(skewness(turnout), 2))\n\n# Histogram of turnout by region\np3 &lt;- ggplot(df_turnout, aes(x = turnout)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~region, ncol = 1) +\n  geom_vline(data = df_turnout %&gt;% group_by(region) %&gt;% summarise(mean = mean(turnout)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_turnout %&gt;% group_by(region) %&gt;% summarise(median = median(turnout)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = region_skewness,\n           aes(x = 25, y = 20, label = paste(\"SK =\", skewness)),\n           size = 3.5) +\n  labs(\n    title = \"Voter turnout in different regions\",\n    subtitle = \"Showing three types of skewness\",\n    x = \"Voter turnout (%)\",\n    y = \"Number of districts\"\n  ) +\n  theme_minimal()\n\n# Box plot\np4 &lt;- ggplot(df_turnout, aes(x = region, y = turnout, fill = region)) +\n  geom_boxplot() +\n  labs(\n    title = \"Comparison of turnout distributions across regions\",\n    x = \"Region\",\n    y = \"Voter turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Display plots\ngrid.arrange(p3, p4, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nInterpretation Guide\n\nPositive Skewness (&gt; 0): Distribution has a longer right tail - most values are concentrated on the left side\nNegative Skewness (&lt; 0): Distribution has a longer left tail - most values are concentrated on the right side\nZero Skewness: Distribution is approximately symmetric - values are evenly distributed around the mean\n\n\n\n\nKurtosis\n\nDefinition\nKurtosis measures the ‚Äútailedness‚Äù of a distribution, indicating the presence of extreme values compared to a normal distribution.\n\n\nMathematical Expression\nK = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\n\n\nSimplified Numerical Example\n\n# Three example datasets with different levels of kurtosis\n# 1. Leptokurtic distribution (high kurtosis, \"heavy tails\")\nleptokurtic_data &lt;- c(\n  rnorm(80, mean = 50, sd = 5),  # Most data clustered around the mean\n  c(20, 25, 30, 70, 75, 80)      # A few extreme values\n)\n\n# 2. Platykurtic distribution (low kurtosis, \"flat\")\nplatykurtic_data &lt;- c(\n  runif(50, min = 30, max = 70)  # Uniform distribution of values\n)\n\n# 3. Mesokurtic distribution (normal kurtosis)\nmesokurtic_data &lt;- rnorm(50, mean = 50, sd = 10)\n\n# Calculate kurtosis\nkurtosis_lepto &lt;- kurtosis(leptokurtic_data)\nkurtosis_platy &lt;- kurtosis(platykurtic_data)\nkurtosis_meso &lt;- kurtosis(mesokurtic_data)\n\n# Summary of results\nkurtosis_data &lt;- data.frame(\n  \"Distribution Type\" = c(\"Leptokurtic\", \"Platykurtic\", \"Mesokurtic\"),\n  \"Kurtosis value\" = round(c(kurtosis_lepto, kurtosis_platy, kurtosis_meso), 3),\n  \"Interpretation\" = c(\n    \"Many values near the mean, but also more extreme values\",\n    \"Values more uniformly distributed - flat distribution\",\n    \"Similar to normal distribution\"\n  )\n)\n\n# Display table\nkurtosis_data\n\n  Distribution.Type Kurtosis.value\n1       Leptokurtic           7.39\n2       Platykurtic           1.85\n3        Mesokurtic           2.25\n                                           Interpretation\n1 Many values near the mean, but also more extreme values\n2   Values more uniformly distributed - flat distribution\n3                          Similar to normal distribution\n\n\n\n\nVisualizations of Kurtosis Levels\n\n# Create a data frame for all sets\ndf_kurtosis &lt;- rbind(\n  data.frame(value = leptokurtic_data, type = \"Leptokurtic (K &gt; 3)\", \n             kurtosis = round(kurtosis_lepto, 2)),\n  data.frame(value = platykurtic_data, type = \"Platykurtic (K &lt; 3)\", \n             kurtosis = round(kurtosis_platy, 2)),\n  data.frame(value = mesokurtic_data, type = \"Mesokurtic (K ‚âà 3)\", \n             kurtosis = round(kurtosis_meso, 2))\n)\n\n# Histograms for three types of kurtosis\np5 &lt;- ggplot(df_kurtosis, aes(x = value)) +\n  geom_histogram(bins = 15, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~type, scales = \"free_y\") +\n  geom_text(data = unique(df_kurtosis[, c(\"type\", \"kurtosis\")]),\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtosis)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histograms showing different levels of kurtosis\",\n    x = \"Value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n# Box plots\np6 &lt;- ggplot(df_kurtosis, aes(x = type, y = value, fill = type)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"lightgreen\", \"lightsalmon\", \"skyblue\")) +\n  labs(\n    title = \"Box plots for different levels of kurtosis\",\n    x = \"Distribution type\",\n    y = \"Value\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Display plots\ngrid.arrange(p5, p6, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nExample: Parliamentary Voting Analysis\n\n# Generate three datasets reflecting different levels of kurtosis\nset.seed(456)\n\n# 1. Leptokurtic distribution - typical for votes with strong party discipline\nlepto_voting &lt;- c(\n  rnorm(150, mean = 75, sd = 3),  # Most votes with high agreement\n  c(20, 25, 30, 35, 40, 95, 96, 97, 98, 99)  # A few outlier votes\n)\n\n# 2. Platykurtic distribution - typical for controversial votes\nplaty_voting &lt;- c(\n  runif(80, min = 40, max = 60),  # Votes with moderate agreement\n  runif(80, min = 60, max = 80)   # Votes with higher agreement\n)\n\n# 3. Mesokurtic distribution - typical for normal votes\nmeso_voting &lt;- rnorm(160, mean = 65, sd = 10)\n\n# Create data frame\ndf_voting &lt;- rbind(\n  data.frame(agreement = lepto_voting, bill_type = \"Bills A: Leptokurtic\"),\n  data.frame(agreement = platy_voting, bill_type = \"Bills B: Platykurtic\"),\n  data.frame(agreement = meso_voting, bill_type = \"Bills C: Mesokurtic\")\n)\n\n# Calculate kurtosis for each bill type\nbill_kurtosis &lt;- df_voting %&gt;%\n  group_by(bill_type) %&gt;%\n  summarise(kurtosis = round(kurtosis(agreement), 2))\n\n# Histogram of voting agreement\np7 &lt;- ggplot(df_voting, aes(x = agreement)) +\n  geom_histogram(bins = 20, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~bill_type, ncol = 1) +\n  geom_text(data = bill_kurtosis,\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtosis)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Voting agreement for different types of bills\",\n    subtitle = \"Showing three levels of kurtosis\",\n    x = \"Voting agreement index (%)\",\n    y = \"Number of votes\"\n  ) +\n  theme_minimal()\n\n# Box plot\np8 &lt;- ggplot(df_voting, aes(x = bill_type, y = agreement, fill = bill_type)) +\n  geom_boxplot() +\n  labs(\n    title = \"Comparison of voting agreement distributions\",\n    x = \"Bill type\",\n    y = \"Voting agreement index (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Display plots\ngrid.arrange(p7, p8, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nInterpretation Guide\n\nLeptokurtic (K &gt; 3): ‚ÄúSlender‚Äù distribution with heavy tails - more extreme values than in a normal distribution\nPlatykurtic (K &lt; 3): ‚ÄúFlat‚Äù distribution - fewer extreme values than in a normal distribution\nMesokurtic (K ‚âà 3): Distribution similar to normal in terms of extreme values",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "href": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.10 Exercise 1. Center and dispersion of data",
    "text": "5.10 Exercise 1. Center and dispersion of data\n\nData\nWe have salary data (in thousands of euros) from two small European companies:\n\n\n\nIndex\nCompany X\nCompany Y\n\n\n\n\n1\n2\n3\n\n\n2\n2\n3\n\n\n3\n2\n4\n\n\n4\n3\n4\n\n\n5\n3\n4\n\n\n6\n3\n4\n\n\n7\n3\n4\n\n\n8\n3\n4\n\n\n9\n3\n5\n\n\n10\n4\n5\n\n\n11\n4\n5\n\n\n12\n4\n5\n\n\n13\n4\n5\n\n\n14\n4\n5\n\n\n15\n5\n6\n\n\n16\n5\n6\n\n\n17\n5\n6\n\n\n18\n5\n7\n\n\n19\n20\n7\n\n\n20\n35\n8\n\n\n\nThis table presents the data for both Company X and Company Y side by side, with an index column for easy reference.\n\n\nMeasures of Central Tendency\n\nMean\nThe mean is the average of all values in a dataset.\nFormula: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\nMo≈ºna te≈º zapisaƒá ten wz√≥r w postaci:\n\\bar{x} = \\frac{\\sum_{i=1}^{k} x_i f_i}{n}\ngdzie f_i to czƒôsto≈õƒá bezwzglƒôdna (liczba wystƒÖpie≈Ñ, waga bezwzglƒôdna) i-tej warto≈õci, a k to liczba r√≥≈ºnych warto≈õci cechy (liczba warto≈õci wyr√≥≈ºnionych).\nZ u≈ºyciem czƒôsto≈õci wzglƒôdnych:\n\\bar{x} = \\sum_{i=1}^{k} x_i p_i\ngdzie p_i to czƒôsto≈õƒá wzglƒôdna (frakcja, waga znormalizowana) i-tej warto≈õci, a k to liczba r√≥≈ºnych warto≈õci cechy (liczba warto≈õci wyr√≥≈ºnionych).\n\nManual Calculation for Company X\n\n\n\nValue (x_i)\nFrequency (f_i)\nx_i \\cdot f_i\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nTotal\nn = 20\nSum = 119\n\n\n\n\\bar{x} = \\frac{119}{20} = 5.95\n\n\nManual Calculation for Company Y\n\n\n\nValue (x_i)\nFrequency (f_i)\nx_i \\cdot f_i\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nTotal\nn = 20\nSum = 100\n\n\n\n\\bar{y} = \\frac{100}{20} = 5\n\n\nR Verification\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\nMedian\nThe median is the middle value when the data is ordered.\n\nManual Calculation for Company X\nOrdered data: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\frac{4 + 4}{2} = 4\n\n\nManual Calculation for Company Y\nOrdered data: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\frac{5 + 5}{2} = 5\n\n\nR Verification\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\nMode\nThe mode is the most frequent value in the dataset.\nFor Company X, the mode is 3 (appears 6 times). For Company Y, there are two modes: 4 and 5 (both appear 6 times).\n\n# Function to calculate mode\nget_mode &lt;- function(x) {\n  unique_x &lt;- unique(x)\n  unique_x[which.max(tabulate(match(x, unique_x)))]\n}\n\nget_mode(X)\n\n[1] 3\n\nget_mode(Y)\n\n[1] 4\n\n\n\n\n\nMeasures of Dispersion\n\nVariance\nThe variance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\nPoprawka Bessela jest stosowana przy obliczaniu wariancji z pr√≥by, aby uzyskaƒá nieobciƒÖ≈ºony estymator wariancji populacji. W standardowym wzorze na wariancjƒô z pr√≥by dzielimy przez (n-1) zamiast przez n.\nModyfikacje wzoru dla danych pogrupowanych (szereg czƒôsto≈õci):\nMo≈ºna te≈º zapisaƒá ten wz√≥r w postaci:\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{k} f_i(x_i - \\bar{x})^2\ngdzie f_i to czƒôsto≈õƒá bezwzglƒôdna (liczba wystƒÖpie≈Ñ) i-tej warto≈õci.\nGdy w obliczeniach stosujemy czƒôsto≈õci wzglƒôdne p = f_i/n, gdzie:\n\nf_i to czƒôsto≈õƒá (liczba wystƒÖpie≈Ñ)\nn to ca≈Çkowita liczebno≈õƒá pr√≥by\n\nWz√≥r na wariancjƒô z uwzglƒôdnieniem poprawki Bessela przyjmuje postaƒá:\ns^2 = \\frac{n}{n-1} \\sum_{i=1}^{k} p_i(x_i - \\bar{x})^2\ngdzie:\n\ns^2 to wariancja z pr√≥by\nn to liczebno≈õƒá pr√≥by\np_i to czƒôsto≈õƒá wzglƒôdna i-tej warto≈õci\nx_i to i-ta warto≈õƒá cechy\n\\bar{x} to ≈õrednia arytmetyczna\nk to liczba r√≥≈ºnych warto≈õci cechy\n\nKluczowe jest to, ≈ºe przy stosowaniu czƒôsto≈õci wzglƒôdnych mno≈ºymy ca≈Çe wyra≈ºenie przez czynnik \\frac{n}{n-1}, kt√≥ry wprowadza poprawkƒô Bessela.\n\nManual Calculation for Company X\n\n\n\n\n\n\n\n\n\n\nx_i\nf_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\nf_i(x_i - \\bar{x})^2\n\n\n\n\n2\n3\n-3.95\n15.6025\n46.8075\n\n\n3\n6\n-2.95\n8.7025\n52.215\n\n\n4\n5\n-1.95\n3.8025\n19.0125\n\n\n5\n4\n-0.95\n0.9025\n3.61\n\n\n20\n1\n14.05\n197.4025\n197.4025\n\n\n35\n1\n29.05\n843.9025\n843.9025\n\n\nTotal\n20\n\n\n1162.95\n\n\n\ns^2 = \\frac{1162.95}{19} = 61.21\n\n\nManual Calculation for Company Y\n\n\n\n\n\n\n\n\n\n\ny_i\nf_i\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\nf_i(y_i - \\bar{y})^2\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nTotal\n20\n\n\n34\n\n\n\ns^2 = \\frac{34}{19} = 1.79\n\n\nR Verification\n\nvar(X)\n\n[1] 61.2\n\nvar(Y)\n\n[1] 1.79\n\n\n\n\n\nStandard Deviation\nThe standard deviation is the square root of the variance.\nFormula: s = \\sqrt{s^2}\n\nFor Company X: s = \\sqrt{61.21} = 7.82\nFor Company Y: s = \\sqrt{1.79} = 1.34\n\n\nR Verification\n\nsd(X)\n\n[1] 7.82\n\nsd(Y)\n\n[1] 1.34\n\n\n\n\n\n\nQuartiles\nQuartiles divide the dataset into four equal parts.\n\nManual Calculation for Company X\nOrdered data: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\n\nQ1 (25th percentile): median of first 10 numbers = 3\nQ2 (50th percentile, median): 4\nQ3 (75th percentile): median of last 10 numbers = 5\n\n\n\nManual Calculation for Company Y\nOrdered data: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\n\nQ1 (25th percentile): median of first 10 numbers = 4\nQ2 (50th percentile, median): 5\nQ3 (75th percentile): median of last 10 numbers = 6\n\n\n\nR Verification\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\nIQR\n\nIQR_x = 5 - 3 = 2\nIQR_y = 6 - 4 = 2\n\n\n\n\nTukey Box Plot\nA Tukey box plot visually represents the distribution of data based on quartiles. We‚Äôll use ggplot2 to create the plot.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Prepare the data\ndata &lt;- data.frame(\n  Company = rep(c(\"X\", \"Y\"), each = 20),\n  Salary = c(X, Y)\n)\n\n# Create the box plot\nggplot(data, aes(x = Company, y = Salary, fill = Company)) +\n  geom_boxplot() +\n  labs(title = \"Salary Distribution in Companies X and Y\",\n       x = \"Company\",\n       y = \"Salary (thousands of euros)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n# Create the box plot\nggplot(data, aes(x = Company, y = Salary, fill = Company)) +\n  geom_boxplot(outliers = F) +\n  labs(title = \"Salary Distribution in Companies X and Y\",\n       x = \"Company\",\n       y = \"Salary (thousands of euros)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\nInterpreting the Box Plot\n\nThe box represents the interquartile range (IQR) from Q1 to Q3.\nThe line inside the box is the median (Q2).\nWhiskers extend to the smallest and largest values within 1.5 * IQR.\nPoints beyond the whiskers are considered outliers.\n\n\n\n\nComparison of Results\n\n\n\nMeasure\nCompany X\nCompany Y\n\n\n\n\nMean\n5.95\n5.00\n\n\nMedian\n4\n5\n\n\nMode\n3\n4 and 5\n\n\nVariance\n61.21\n1.79\n\n\nStandard Deviation\n7.82\n1.34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\nKey Observations:\n\nCentral Tendency: Company X has a higher mean but lower median than Company Y, indicating a right-skewed distribution for Company X.\nDispersion: Company X shows much higher variance and standard deviation, suggesting greater salary disparities.\nDistribution Shape: Company Y‚Äôs salaries are more tightly clustered, while Company X has extreme values (potential outliers) that significantly affect its mean and variance.\nQuartiles: Company Y‚Äôs interquartile range (Q3 - Q1) is slightly larger, but its overall range is much smaller than Company X‚Äôs.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-2.-comparing-electoral-district-size-variation-between-countries",
    "href": "chapter5.html#exercise-2.-comparing-electoral-district-size-variation-between-countries",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.11 Exercise 2. Comparing Electoral District Size Variation Between Countries",
    "text": "5.11 Exercise 2. Comparing Electoral District Size Variation Between Countries\n\nData\nWe have electoral district size data from two countries:\n\nx &lt;- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)  # Country high variance\ny &lt;- c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13)  # Country low variance\n\nkable(data.frame(\n  \"Country X (High var.)\" = x,\n  \"Country Y (Low var.)\" = y\n))\n\n\n\n\nCountry.X..High.var..\nCountry.Y..Low.var..\n\n\n\n\n1\n8\n\n\n3\n9\n\n\n5\n9\n\n\n7\n10\n\n\n9\n10\n\n\n11\n11\n\n\n13\n11\n\n\n15\n12\n\n\n17\n12\n\n\n19\n13\n\n\n\n\n\n\n\nMeasures of Central Tendency\n\nArithmetic Mean\nFormula: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\nCalculations for Country X\n\n\n\nElement\nValue\n\n\n\n\n1\n1\n\n\n2\n3\n\n\n3\n5\n\n\n4\n7\n\n\n5\n9\n\n\n6\n11\n\n\n7\n13\n\n\n8\n15\n\n\n9\n17\n\n\n10\n19\n\n\nSum\n100\n\n\n\n\\bar{x} = \\frac{100}{10} = 10\n\nmean_x &lt;- mean(x)\nc(\"Manual\" = 10, \"R\" = mean_x)\n\nManual      R \n    10     10 \n\n\n\n\nCalculations for Country Y\n\n\n\nElement\nValue\n\n\n\n\n1\n8\n\n\n2\n9\n\n\n3\n9\n\n\n4\n10\n\n\n5\n10\n\n\n6\n11\n\n\n7\n11\n\n\n8\n12\n\n\n9\n12\n\n\n10\n13\n\n\nSum\n105\n\n\n\n\\bar{y} = \\frac{105}{10} = 10.5\n\nmean_y &lt;- mean(y)\nc(\"Manual\" = 10.5, \"R\" = mean_y)\n\nManual      R \n  10.5   10.5 \n\n\n\n\n\nMedian\nThe median is the middle value in an ordered dataset.\n\nCalculations for Country X\nOrdered data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nFor n = 10 (even number of observations): Middle positions: 5 and 6 Middle values: 9 and 11\nMedian = \\frac{9 + 11}{2} = 10\n\nmedian_x &lt;- median(x)\nc(\"Manual\" = 10, \"R\" = median_x)\n\nManual      R \n    10     10 \n\n\n\n\nCalculations for Country Y\nOrdered data: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nFor n = 10 (even number of observations): Middle positions: 5 and 6 Middle values: 10 and 11\nMedian = \\frac{10 + 11}{2} = 10.5\n\nmedian_y &lt;- median(y)\nc(\"Manual\" = 10.5, \"R\" = median_y)\n\nManual      R \n  10.5   10.5 \n\n\n\n\n\nMode\n\nCalculations for Country X\n\n\n\nValue\nFrequency\n\n\n\n\n1\n1\n\n\n3\n1\n\n\n5\n1\n\n\n7\n1\n\n\n9\n1\n\n\n11\n1\n\n\n13\n1\n\n\n15\n1\n\n\n17\n1\n\n\n19\n1\n\n\n\nConclusion: No mode (all values occur once)\n\n\nCalculations for Country Y\n\n\n\nValue\nFrequency\n\n\n\n\n8\n1\n\n\n9\n2\n\n\n10\n2\n\n\n11\n2\n\n\n12\n2\n\n\n13\n1\n\n\n\nConclusion: Four modes: 9, 10, 11, 12 (each occurs twice)\n\n# Frequency tables\ntable_x &lt;- table(x)\ntable_y &lt;- table(y)\n\nlist(\n  \"Country X\" = table_x,\n  \"Country Y\" = table_y\n)\n\n$`Country X`\nx\n 1  3  5  7  9 11 13 15 17 19 \n 1  1  1  1  1  1  1  1  1  1 \n\n$`Country Y`\ny\n 8  9 10 11 12 13 \n 1  2  2  2  2  1 \n\n\n\n\n\nVariance\nVariance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\nCalculations for Country X\n\n\n\nx_i\n(x_i - \\bar{x})\n(x_i - \\bar{x})^2\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSum\n\n330\n\n\n\ns^2_X = \\frac{330}{9} = 36.67\n\nvar_x &lt;- var(x)\nc(\"Manual\" = 36.67, \"R\" = var_x)\n\nManual      R \n 36.67  36.67 \n\n\n\n\nCalculations for Country Y\n\n\n\nx_i\n(y_i - \\bar{y})\n(y_i - \\bar{y})^2\n\n\n\n\n8\n-2.5\n6.25\n\n\n9\n-1.5\n2.25\n\n\n9\n-1.5\n2.25\n\n\n10\n-0.5\n0.25\n\n\n10\n-0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n12\n1.5\n2.25\n\n\n12\n1.5\n2.25\n\n\n13\n2.5\n6.25\n\n\nSum\n\n22.5\n\n\n\ns^2_Y = \\frac{22.5}{9} = 2.5\n\nvar_y &lt;- var(y)\nc(\"Manual\" = 2.5, \"R\" = var_y)\n\nManual      R \n   2.5    2.5 \n\n\n\n\n\nStandard Deviation\nStandard deviation is the square root of variance. It measures variability in the same units as the data.\nFormula: s = \\sqrt{s^2}\n\nCalculations for Country X\nUsing previously calculated variance: s^2_X = 36.67\nCalculate square root: s_X = \\sqrt{36.67} \\approx 6.06\n\n\n\nStep\nCalculation\nResult\n\n\n\n\n1. Variance\ns^2_X\n36.67\n\n\n2. Square root\n\\sqrt{36.67}\n6.06\n\n\n\n\nsd_x &lt;- sd(x)\nc(\"Manual\" = 6.06, \"R\" = sd_x)\n\nManual      R \n 6.060  6.055 \n\n\n\n\nCalculations for Country Y\nUsing previously calculated variance: s^2_Y = 2.5\nCalculate square root: s_Y = \\sqrt{2.5} \\approx 1.58\n\n\n\nStep\nCalculation\nResult\n\n\n\n\n1. Variance\ns^2_Y\n2.5\n\n\n2. Square root\n\\sqrt{2.5}\n1.58\n\n\n\n\nsd_y &lt;- sd(y)\nc(\"Manual\" = 1.58, \"R\" = sd_y)\n\nManual      R \n 1.580  1.581 \n\n\nInterpretation:\n\nCountry X: Average deviation from the mean is about 6 seats\nCountry Y: Average deviation from the mean is about 1.6 seats\n\n\n\n\n\nCoefficient of Variation (CV)\nThe coefficient of variation is the ratio of standard deviation to mean, expressed as a percentage.\nFormula: CV = \\frac{s}{\\bar{x}} \\times 100\\%\n\nCalculations for Country X\nCV_X = \\frac{6.06}{10} \\times 100\\% = 60.6\\%\n\n\n\nComponent\nValue\n\n\n\n\nStandard deviation (s)\n6.06\n\n\nMean (\\bar{x})\n10\n\n\nCV\n60.6%\n\n\n\n\ncv_x &lt;- sd(x) / mean(x) * 100\nc(\"Manual\" = 60.6, \"R\" = cv_x)\n\nManual      R \n 60.60  60.55 \n\n\n\n\nCalculations for Country Y\nCV_Y = \\frac{1.58}{10.5} \\times 100\\% = 15.0\\%\n\n\n\nComponent\nValue\n\n\n\n\nStandard deviation (s)\n1.58\n\n\nMean (\\bar{x})\n10.5\n\n\nCV\n15.0%\n\n\n\n\ncv_y &lt;- sd(y) / mean(y) * 100\nc(\"Manual\" = 15.0, \"R\" = cv_y)\n\nManual      R \n 15.00  15.06 \n\n\n\n\n\nQuartiles and Interquartile Range (IQR)\n\nMethods for Calculating Quartiles\nThere are different methods for calculating quartiles. In our manual calculations, we‚Äôll use the median-excluding method:\n\nSplit the series at the median\nMedian is not included in quartile calculations\nCalculate median of each part - these will be Q1 and Q3 respectively\n\n\n\nCalculations for Country X\nOrdered data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMedian = 10 (not included in quartile calculations)\nLower half: 1, 3, 5, 7, 9 Q1 = median of lower half = 5\nUpper half: 11, 13, 15, 17, 19 Q3 = median of upper half = 15\nIQR = Q3 - Q1 = 15 - 5 = 10\n\n\nCalculations for Country Y\nOrdered data: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMedian = 10.5 (not included in quartile calculations)\nLower half: 8, 9, 9, 10, 10 Q1 = median of lower half = 9\nUpper half: 11, 11, 12, 12, 13 Q3 = median of upper half = 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n# Comparison of different quartile calculation methods in R\nmethods_comparison &lt;- data.frame(\n  Method = c(\"Manual (excl. median)\", \n             \"R type=1\", \"R type=2\", \"R type=7 (default)\"),\n  \"Q1 Country X\" = c(5, \n                    quantile(x, 0.25, type=1),\n                    quantile(x, 0.25, type=2),\n                    quantile(x, 0.25, type=7)),\n  \"Q3 Country X\" = c(15,\n                    quantile(x, 0.75, type=1),\n                    quantile(x, 0.75, type=2),\n                    quantile(x, 0.75, type=7)),\n  \"Q1 Country Y\" = c(9,\n                    quantile(y, 0.25, type=1),\n                    quantile(y, 0.25, type=2),\n                    quantile(y, 0.25, type=7)),\n  \"Q3 Country Y\" = c(12,\n                    quantile(y, 0.75, type=1),\n                    quantile(y, 0.75, type=2),\n                    quantile(y, 0.75, type=7))\n)\n\nkable(methods_comparison, digits = 2,\n      caption = \"Comparison of different quartile calculation methods\")\n\n\nComparison of different quartile calculation methods\n\n\n\n\n\n\n\n\n\nMethod\nQ1.Country.X\nQ3.Country.X\nQ1.Country.Y\nQ3.Country.Y\n\n\n\n\nManual (excl. median)\n5.0\n15.0\n9.00\n12.00\n\n\nR type=1\n5.0\n15.0\n9.00\n12.00\n\n\nR type=2\n5.0\n15.0\n9.00\n12.00\n\n\nR type=7 (default)\n5.5\n14.5\n9.25\n11.75\n\n\n\n\n\n\n\nExplanation of Different Quartile Calculation Methods\n\nManual method (excluding median):\n\nSplits data into two parts\nExcludes median\nFinds median of each part\n\nR type=1:\n\nFirst method in R\nUses whole positions\nNo interpolation\n\nR type=2:\n\nSecond method in R\nUses whole positions\nInterpolates when position is not whole\n\nR type=7 (default):\n\nDefault method in R\nUses quantile()[5] from SAS\nInterpolates according to Hyndman and Fan method\n\n\n\n\n\nResults Comparison\n\nsummary_df &lt;- data.frame(\n  Measure = c(\"Mean\", \"Median\", \"Mode\", \"Range\", \"Variance\", \n              \"Std. Dev.\", \"Q1\", \"Q3\", \"IQR\", \"CV (%)\"),\n  \"Country X\" = c(10, 10, \"none\", 18, 36.67, 6.06, 5, 15, 10, 60.6),\n  \"Country Y\" = c(10.5, 10.5, \"9,10,11,12\", 5, 2.5, 1.58, 9, 12, 3, 15.0)\n)\n\nkable(summary_df, \n      caption = \"Summary of all statistical measures\",\n      align = c('l', 'r', 'r'))\n\n\nSummary of all statistical measures\n\n\nMeasure\nCountry.X\nCountry.Y\n\n\n\n\nMean\n10\n10.5\n\n\nMedian\n10\n10.5\n\n\nMode\nnone\n9,10,11,12\n\n\nRange\n18\n5\n\n\nVariance\n36.67\n2.5\n\n\nStd. Dev.\n6.06\n1.58\n\n\nQ1\n5\n9\n\n\nQ3\n15\n12\n\n\nIQR\n10\n3\n\n\nCV (%)\n60.6\n15\n\n\n\n\n\n\n\nComparison using Box Plot\n\ndf_long &lt;- data.frame(\n  country = rep(c(\"X\", \"Y\"), each = 10),\n  size = c(x, y)\n)\n\n# Basic plot\np &lt;- ggplot(df_long, aes(x = country, y = size, fill = country)) +\n  geom_boxplot(outlier.shape = NA) +  # Disable default outlier points\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Add points with transparency\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\")) +\n  labs(\n    title = \"Comparison of Electoral District Size Variation\",\n    subtitle = paste(\"CV: Country X =\", round(cv_x, 1), \"%, Country Y =\", round(cv_y, 1), \"%\"),\n    x = \"Country\",\n    y = \"District Size\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Add quartile annotations\np + annotate(\n  \"text\", \n  x = c(1, 1, 1, 2, 2, 2), \n  y = c(max(x)+1, mean(x), min(x)-1, max(y)+1, mean(y), min(y)-1),\n  label = c(\n    paste(\"Q3 =\", quantile(x, 0.75, type=1)),\n    paste(\"M =\", median(x)),\n    paste(\"Q1 =\", quantile(x, 0.25, type=1)),\n    paste(\"Q3 =\", quantile(y, 0.75, type=1)),\n    paste(\"M =\", median(y)),\n    paste(\"Q1 =\", quantile(y, 0.25, type=1))\n  ),\n  size = 3\n)\n\n\n\n\n\n\n\n\n\n\nMethodological Notes\n\nQuartile Calculations:\n\nThe median-excluding method used may give different results than R‚Äôs default functions\nDifferences in calculation methods don‚Äôt affect overall conclusions\nAlways important to specify the method used in reports\n\nVisualization:\n\nBox plot effectively shows differences in distributions\nAdditional points show actual values\nAnnotations facilitate interpretation\n\n\n\n\nApplication Notes\n\nUsing the Analysis:\n\nAll calculations can be reproduced using the provided R code\nCode chunks are self-contained and documented\nData format requirements are clearly specified\n\nCustomization:\n\nAnalysis can be adapted for different district size datasets\nVisualization parameters can be adjusted for different presentation needs\nStatistical methods can be modified based on specific requirements\n\n\n\n\nConclusion\n\nSummary Statistics Comparison\n\n\n\nMeasure\nCountry X\nCountry Y\nRelative Difference\n\n\n\n\nMean\n10.0\n10.5\nSimilar\n\n\nMedian\n10.0\n10.5\nSimilar\n\n\nMode\nNone\nMultiple (9,10,11,12)\n-\n\n\nRange\n18\n5\n3.6√ó larger in X\n\n\nVariance\n36.67\n2.5\n14.7√ó larger in X\n\n\nIQR\n10\n3\n3.3√ó larger in X\n\n\nCV\n60.6%\n15.0%\n4.0√ó larger in X\n\n\n\n\n\nDistribution Characteristics\nCountry X:\n\nUniform distribution pattern\nNo dominant district size (no mode)\nWide range: 1 to 19 seats\nHigh variability (CV = 60.6%) - Even spread of values across range\n\nCountry Y:\n\nClustered distribution pattern\nMultiple common sizes (four modes)\nNarrow range: 8 to 13 seats\nLow variability (CV = 15.0%) - Values concentrated around mean\n\n\n\nBox Plot Interpretation\nThe box plot visualization reveals:\nStructure Elements:\n\nBox: Shows interquartile range (IQR)\nLower edge: First quartile (Q1)\nUpper edge: Third quartile (Q3)\nInternal line: Median (Q2)\nWhiskers: Extend to ¬±1.5 IQR - Points: Individual district sizes\n\nKey Visual Findings:\n\nBox Size:\n\n\nCountry X: Large box indicates wide spread of middle 50%\nCountry Y: Small box shows tight clustering of middle values\n\n\nWhisker Length:\n\nCountry X: Long whiskers indicate broad overall distribution\nCountry Y: Short whiskers show limited total spread\n\nPoint Distribution:\n\nCountry X: Points widely dispersed\nCountry Y: Points densely clustered\n\n\n\n\nKey Observations\n\nCentral Tendency:\n\nSimilar average district sizes\nDifferent distribution patterns\nDistinct approaches to standardization\n\nVariability Measures:\n\nAll metrics show Country X with 3-15 times more variation\nConsistent pattern across different statistical measures\nSystematic difference in district design\n\nSystem Design:\n\nCountry X: Flexible, varied approach\nCountry Y: Standardized, uniform approach\nDifferent philosophical approaches to representation\n\nRepresentative Implications:\n\nCountry X: Variable voter-to-representative ratios\nCountry Y: More consistent representation levels\nDifferent approaches to democratic representation\n\n\nThis analysis demonstrates fundamental differences in electoral system design between the two countries, with Country X adopting a more varied approach and Country Y maintaining greater uniformity in district sizes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-3.-understanding-boxplots-through-life-expectancy-data",
    "href": "chapter5.html#exercise-3.-understanding-boxplots-through-life-expectancy-data",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.12 Exercise 3. Understanding Boxplots Through Life Expectancy Data",
    "text": "5.12 Exercise 3. Understanding Boxplots Through Life Expectancy Data\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# Prepare data\ndata_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007)",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction-to-boxplots",
    "href": "chapter5.html#introduction-to-boxplots",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.13 Introduction to Boxplots",
    "text": "5.13 Introduction to Boxplots\nA boxplot (also known as a box-and-whisker plot) reveals key statistics about your data:\n\nMedian: The middle line in the box (50th percentile)\nFirst quartile (Q1): Bottom of the box (25th percentile)\nThird quartile (Q3): Top of the box (75th percentile)\nInterquartile Range (IQR): The height of the box (Q3 - Q1)\nWhiskers: Extend to the most extreme non-outlier values (Tukey‚Äôs method: 1.5 √ó IQR)\nOutliers: Individual points beyond the whiskers\n\n\nVisualizing Life Expectancy\n\nggplot(data_2007, aes(x = reorder(continent, lifeExp, FUN = median), y = lifeExp)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 24, \n               outlier.fill = \"red\", outlier.alpha = 0.6, outlier.size = 4) +\n  geom_jitter(width = 0.2, alpha = 0.4, color = \"darkblue\") +\n  labs(title = \"Life Expectancy by Continent (2007)\",\n       subtitle = \"Individual points show raw data; red points indicate outliers\",\n       x = \"Continent\",\n       y = \"Life Expectancy (years)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 14)\n  ) +\n  scale_y_continuous(breaks = seq(40, 85, by = 5))",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-the-data",
    "href": "chapter5.html#understanding-the-data",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.14 Understanding the Data",
    "text": "5.14 Understanding the Data\n\nMedian and Distribution\nAnswer True or False:\n\n50% of African countries have life expectancy below 54 years\nThe median life expectancy in Europe is approximately 78 years\nMore than 75% of countries in Oceania have life expectancy above 74 years\n25% of Asian countries have life expectancy below 65 years\nThe middle 50% of life expectancies in Europe fall between 74 and 80 years\n\n\n\nSpread and Variation\nAnswer True or False:\n\nAsia shows the largest spread (IQR) in life expectancy\nEurope has the smallest IQR among all continents\nThe variation in Africa‚Äôs life expectancy is greater than in the Americas\nOceania shows the least variation in life expectancy\nThe range (excluding outliers) in Asia is approximately 20 years\n\n\n\nOutliers and Extremes\nAnswer True or False:\n\nAfrica has two countries with unusually low life expectancy\nThere are no outliers in Oceania‚Äôs distribution\nAsia has both high and low outliers",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#changes-over-time",
    "href": "chapter5.html#changes-over-time",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.15 Changes Over Time",
    "text": "5.15 Changes Over Time\n\ntime_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  mutate(year = factor(year))\n\nggplot(time_comparison, aes(x = continent, y = lifeExp, fill = year)) +\n  geom_boxplot(alpha = 0.7, position = \"dodge\", outlier.shape = 21,\n               outlier.alpha = 0.6) +\n  labs(title = \"Life Expectancy: 1957 vs 2007\",\n       subtitle = \"Comparing distribution changes over 50 years\",\n       x = \"Continent\",\n       y = \"Life Expectancy (years)\",\n       fill = \"Year\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(breaks = seq(30, 85, by = 5))\n\n\n\n\n\n\n\n\n\nTime Comparison Questions\nAnswer True or False:\n\nThe median life expectancy increased in all continents between 1957 and 2007\nThe variation in life expectancy (IQR) decreased in most continents over time\nAfrica showed the smallest improvement in median life expectancy\nThe spread of life expectancies in Asia decreased substantially from 1957 to 2007\nOceania maintained the highest median life expectancy in both time periods\n\n\n\nStatistical Summary\n\n# Calculate summary statistics\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    median = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    n_outliers = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Summary Statistics by Continent and Year\")\n\n\nSummary Statistics by Continent and Year\n\n\ncontinent\nyear\nmedian\nq1\nq3\niqr\nn_outliers\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n0",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#key-learning-points",
    "href": "chapter5.html#key-learning-points",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.16 Key Learning Points",
    "text": "5.16 Key Learning Points\n\nDistribution Center:\n\nMedian shows the typical life expectancy\nChanges in median reflect overall improvements\n\nSpread and Variation:\n\nIQR (box height) indicates data dispersion\nWider boxes suggest more inequality in life expectancy\n\nOutliers and Extremes:\n\nOutliers often represent countries with unique circumstances\n\nTime Comparison:\n\nShows both absolute improvements and changes in variation\nHighlights persistent regional disparities\nReveals different rates of progress across continents",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "href": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "title": "5¬† Fundamentals of Univariate Descriptive Statistics",
    "section": "5.17 Appendix: Summary Tables for Data Types and Applicable Statistical Measures",
    "text": "5.17 Appendix: Summary Tables for Data Types and Applicable Statistical Measures\n\nTable 1: Pros and Cons of Various Statistical Measures\n\nMeasures of Center\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nMean\n- Uses all data points- Allows for further statistical calculations- Ideal for normally distributed data\n- Sensitive to outliers- Not ideal for skewed distributions- Not meaningful for nominal data\nInterval, Ratio, some Discrete, Continuous\n\n\nMedian\n- Not affected by outliers- Good for skewed distributions- Can be used with ordinal data\n- Ignores the actual values of most data points- Less useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nMode\n- Can be used with any data type- Good for finding most common category\n- May not be unique (multimodal)- Not useful for many types of analyses- Ignores magnitude of differences between values\nAll types\n\n\n\n\n\nMeasures of Variability\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nRange\n- Simple to calculate and understand- Gives quick idea of data spread\n- Very sensitive to outliers- Ignores all data between extremes- Not useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nInterquartile Range (IQR)\n- Not affected by outliers- Good for skewed distributions\n- Ignores 50% of the data- Less intuitive than range\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nVariance\n- Uses all data points- Basis for many statistical procedures\n- Sensitive to outliers- Units are squared (less intuitive)\nInterval, Ratio, some Discrete, Continuous\n\n\nStandard Deviation\n- Uses all data points- Same units as original data- Widely used and understood\n- Sensitive to outliers- Assumes roughly normal distribution for interpretation\nInterval, Ratio, some Discrete, Continuous\n\n\nCoefficient of Variation\n- Allows comparison between datasets with different units or means\n- Can be misleading when means are close to zero- Not meaningful for data with negative values\nRatio, some Interval\n\n\n\n\n\nMeasures of Correlation/Association\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nPearson‚Äôs r\n- Measures linear relationship- Widely used and understood\n- Assumes normal distribution- Sensitive to outliers- Only captures linear relationships\nInterval, Ratio, Continuous\n\n\nSpearman‚Äôs rho\n- Can be used with ordinal data- Captures monotonic relationships- Less sensitive to outliers\n- Loses information by converting to ranks- May miss some types of relationships\nOrdinal, Interval, Ratio\n\n\nKendall‚Äôs tau\n- Can be used with ordinal data- More robust than Spearman‚Äôs for small samples- Has nice interpretation (probability of concordance)\n- Loses information by only considering order- Computationally more intensive\nOrdinal, Interval, Ratio\n\n\nChi-square\n- Can be used with nominal data- Tests independence of categorical variables\n- Requires large sample sizes- Sensitive to sample size- Doesn‚Äôt measure strength of association\nNominal, Ordinal\n\n\nCram√©r‚Äôs V\n- Can be used with nominal data- Provides measure of strength of association- Normalized to [0,1] range\n- Interpretation can be subjective- May overestimate association in small samples\nNominal, Ordinal\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nMedian\nMediana\n-\n‚úì\n‚úì\n‚úì\n\n\nArithmetic Mean\n≈örednia arytmetyczna\n-\n-\n‚úì*\n‚úì\n\n\nGeometric Mean\n≈örednia geometryczna\n-\n-\n-\n‚úì\n\n\nHarmonic Mean\n≈örednia harmoniczna\n-\n-\n-\n‚úì\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstƒôp\n-\n‚úì\n‚úì\n‚úì\n\n\nInterquartile Range\nRozstƒôp miƒôdzykwartylowy\n-\n‚úì\n‚úì\n‚úì\n\n\nMean Absolute Deviation\n≈örednie odchylenie bezwzglƒôdne\n-\n-\n‚úì\n‚úì\n\n\nVariance\nWariancja\n-\n-\n‚úì*\n‚úì\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n‚úì*\n‚úì\n\n\nCoefficient of Variation\nWsp√≥≈Çczynnik zmienno≈õci\n-\n-\n-\n‚úì\n\n\nAssociation / Wsp√≥≈Çzale≈ºno≈õƒá:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n‚úì\n‚úì\n‚úì\n\n\nKendall‚Äôs Tau\nTau Kendalla\n-\n‚úì\n‚úì\n‚úì\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n‚úì*\n‚úì\n\n\nCovariance\nKowariancja\n-\n-\n‚úì*\n‚úì\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\nNotes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporzƒÖdkowania\nOrdinal: Ordered categories / Kategorie uporzƒÖdkowane\nInterval: Equal intervals, arbitrary zero / R√≥wne interwa≈Çy, umowne zero\nRatio: Equal intervals, absolute zero / R√≥wne interwa≈Çy, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ‚úì* are commonly used for interval data despite theoretical issues / Niekt√≥re miary oznaczone ‚úì* sƒÖ powszechnie stosowane dla danych przedzia≈Çowych pomimo problem√≥w teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wyb√≥r miary powinien uwzglƒôdniaƒá zar√≥wno poprawno≈õƒá teoretycznƒÖ jak i u≈ºyteczno≈õƒá praktycznƒÖ\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalajƒÖ na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html",
    "href": "rozdzial5.html",
    "title": "6¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "6.1 Wprowadzenie do Notacji Sigma (Œ£)\nStatystyki opisowe sƒÖ fundamentalnymi narzƒôdziami w badaniach nauk spo≈Çecznych, zapewniajƒÖcymi zwiƒôz≈Çe podsumowanie charakterystyk danych. Pe≈ÇniƒÖ kilka kluczowych funkcji:",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wprowadzenie-do-notacji-sigma-œÉ",
    "href": "rozdzial5.html#wprowadzenie-do-notacji-sigma-œÉ",
    "title": "6¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "Co to jest notacja sumacyjna Sigma? Sigma (Œ£) to operator matematyczny, kt√≥ry nakazuje nam zsumowaƒá (dodaƒá) sekwencjƒô wyraz√≥w - dzia≈Ça jak instrukcja wykonania dodawania wszystkich element√≥w w okre≈õlonym zakresie.\nCel: Zapewnia zwiƒôz≈Çy spos√≥b zapisu sum wielu podobnych wyraz√≥w za pomocƒÖ jednego symbolu, unikajƒÖc d≈Çugich wyra≈ºe≈Ñ dodawania.\n\n\nPodstawowa formu≈Ça\n\nOg√≥lna forma notacji sigma to:\n\n\\sum_{i=a}^{b} f(i)\n\nIndeks sumowania: i\nDolna granica: a\nG√≥rna granica: b\nFunkcja: f(i)\n\n\n\nPrzyk≈Çady zastosowania notacji Sigma\n\nProsty przyk≈Çad: Suma liczb naturalnych\n\nZa≈Ç√≥≈ºmy, ≈ºe chcesz dodaƒá pierwsze piƒôƒá dodatnich liczb ca≈Çkowitych:\n\n\\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15\n\nPowy≈ºszy zapis dodaje pierwsze piƒôƒá dodatnich liczb ca≈Çkowitych.\n\n\n\nSuma kwadrat√≥w\n\nZa≈Ç√≥≈ºmy, ≈ºe chcesz zsumowaƒá kwadraty pierwszych czterech dodatnich liczb ca≈Çkowitych:\n\n\\sum_{i=1}^{4} i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 1 + 4 + 9 + 16 = 30\n\nJest to suma kwadrat√≥w pierwszych czterech dodatnich liczb ca≈Çkowitych.\n\n\n\nSuma warto≈õci sta≈Çej\n\nSumowanie sta≈Çej warto≈õci c dla n wyraz√≥w:\n\n\\sum_{i=1}^{n} c = c + c + c + ... + c \\text{ (n razy)} = n \\cdot c\n\nPrzyk≈Çad: Suma piƒôciu piƒÖtek:\n\n\\sum_{i=1}^{5} 5 = 5 + 5 + 5 + 5 + 5 = 5 \\cdot 5 = 25\n\n\n\nProste przyk≈Çady w kontek≈õcie statystyki\n\\sum_{i=1}^{n} x_i\n\nIndeks sumowania: i (zazwyczaj oznacza konkretnƒÖ obserwacjƒô w zbiorze danych)\nDolna granica: 1 (zwykle zaczynamy od pierwszej obserwacji)\nG√≥rna granica: n (ca≈Çkowita liczba obserwacji w naszym zbiorze danych)\nWyra≈ºenie: x_i (warto≈õƒá i-tej obserwacji)\n\n\nSumowanie warto≈õci obserwacji\n\nMamy zbi√≥r danych: 5, 8, 12, 15, 20\nSuma wszystkich warto≈õci:\n\n\\sum_{i=1}^{5} x_i = x_1 + x_2 + x_3 + x_4 + x_5 = 5 + 8 + 12 + 15 + 20 = 60\n\nTa suma jest kluczowym elementem przy obliczaniu ≈õredniej arytmetycznej.\n\n\n\nSuma odchyle≈Ñ od ≈õredniej\n\nDla tego samego zbioru danych (5, 8, 12, 15, 20), ≈õrednia wynosi \\bar{x} = 60/5 = 12\nSuma odchyle≈Ñ od ≈õredniej:\n\n\\sum_{i=1}^{5} (x_i - \\bar{x}) = (5-12) + (8-12) + (12-12) + (15-12) + (20-12) = -7 + (-4) + 0 + 3 + 8 = 0\n\nWa≈ºna obserwacja: Suma odchyle≈Ñ od ≈õredniej zawsze wynosi 0, co jest podstawowƒÖ w≈Ça≈õciwo≈õciƒÖ ≈õredniej arytmetycznej.\n\n\n\n\nPodsumowanie\n\nNotacja Sigma (Œ£) pozwala na zwiƒôz≈Çy zapis kluczowych wzor√≥w statystycznych\nNajwa≈ºniejsze zastosowania obejmujƒÖ obliczanie:\n\n≈öredniej arytmetycznej\nWariancji i odchylenia standardowego\nR√≥≈ºnych sum kwadrat√≥w u≈ºywanych w analizie regresji\n\n\n\n\n\n\n\n\nOperatory Sumy (Œ£) i Iloczynu (Œ†)\n\n\n\n\nOperator Sigma (Œ£)\n\\sum to operator sumowania, kt√≥ry nakazuje nam dodaƒá wyrazy:\n\\sum_{i=1}^{n} x_i = x_1 + x_2 + ... + x_n\ngdzie: - i to zmienna indeksowa - Dolna warto≈õƒá pod Œ£ (tutaj i=1) to punkt poczƒÖtkowy - G√≥rna warto≈õƒá (tutaj n) to punkt ko≈Ñcowy\n\n\nOperator Pi (Œ†)\n\\prod to operator iloczynu, kt√≥ry nakazuje nam pomno≈ºyƒá wyrazy:\n\\prod_{i=1}^{n} x_i = x_1 \\times x_2 \\times ... \\times x_n\ngdzie: - i to zmienna indeksowa - Dolna warto≈õƒá pod Œ† (tutaj i=1) to punkt poczƒÖtkowy - G√≥rna warto≈õƒá (tutaj n) to punkt ko≈Ñcowy\n\n\n\n\n\n\n\n\n\nPrzyk≈Çad Œ£\n\n\n\n\\sum_{i=1}^{4} i = 1 + 2 + 3 + 4 = 10\n\n\n\n\n\n\n\n\nPrzyk≈Çad Œ†\n\n\n\n\\prod_{i=1}^{4} i = 1 \\times 2 \\times 3 \\times 4 = 24\n\n\n\n\n\n\n\n\nKluczowe R√≥≈ºnice\n\n\n\n\nŒ£ oznacza wielokrotne dodawanie\nŒ† oznacza wielokrotne mno≈ºenie",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#typy-rozk≈Çad√≥w-danych",
    "href": "rozdzial5.html#typy-rozk≈Çad√≥w-danych",
    "title": "6¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.2 Typy rozk≈Çad√≥w danych",
    "text": "6.2 Typy rozk≈Çad√≥w danych\n\n\n\n\n\n\nImportant\n\n\n\nRozk≈Çad danych informuje o tym, jakie warto≈õci przyjmuje zmienna i jak czƒôsto.\n\n\nZrozumienie rozk≈Çad√≥w danych jest kluczowe dla analizy i wizualizacji danych. W tym dokumencie przyjrzymy siƒô r√≥≈ºnym typom rozk≈Çad√≥w i sposobom ich wizualizacji przy u≈ºyciu ggplot2 w R.\n\nRozk≈Çad normalny\nRozk≈Çad normalny, znany r√≥wnie≈º jako rozk≈Çad Gaussa, jest symetryczny i ma kszta≈Çt dzwonu.\n\n# Generowanie danych o rozk≈Çadzie normalnym\ndane_normalne &lt;- data.frame(x = rnorm(1000))\n\n# Wykres\nggplot(dane_normalne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozk≈Çad normalny\", x = \"Warto≈õƒá\", y = \"Gƒôsto≈õƒá\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\nRozk≈Çad jednostajny\nW rozk≈Çadzie jednostajnym wszystkie warto≈õci majƒÖ r√≥wne prawdopodobie≈Ñstwo wystƒÖpienia.\n\n# Generowanie danych o rozk≈Çadzie jednostajnym\ndane_jednostajne &lt;- data.frame(x = runif(1000))\n\n# Wykres\nggplot(dane_jednostajne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozk≈Çad jednostajny\", x = \"Warto≈õƒá\", y = \"Gƒôsto≈õƒá\")\n\n\n\n\n\n\n\n\n\n\nRozk≈Çady sko≈õne\nRozk≈Çady sko≈õne sƒÖ asymetryczne, z jednym ogonem d≈Çu≈ºszym ni≈º drugi.\n\n# Generowanie danych o rozk≈Çadzie prawosko≈õnym\ndane_prawoskosne &lt;- data.frame(x = rlnorm(1000))\n\n# Wykres\nggplot(dane_prawoskosne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightyellow\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozk≈Çad prawosko≈õny\", x = \"Warto≈õƒá\", y = \"Gƒôsto≈õƒá\")\n\n\n\n\n\n\n\n\n\n\nRozk≈Çad bimodalny\nRozk≈Çad bimodalny ma dwa szczyty (dwie dominanty), wskazujƒÖce na dwie odrƒôbne podgrupy w danych.\n\n# Generowanie danych bimodalnych\ndane_bimodalne &lt;- data.frame(x = c(rnorm(500, mean = -2), rnorm(500, mean = 2)))\n\n# Wykres\nggplot(dane_bimodalne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightpink\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozk≈Çad bimodalny\", x = \"Warto≈õƒá\", y = \"Gƒôsto≈õƒá\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRozk≈Çad\nKluczowe w≈Ça≈õciwo≈õci\nPrzyk≈Çady\n\n\n\n\nSymetryczny (Normalny)\nSymetryczny, kszta≈Çt dzwonu, wiƒôkszo≈õƒá warto≈õci blisko ≈õredniej\nWzrost doros≈Çych w populacji, wyniki test√≥w IQ, b≈Çƒôdy pomiarowe, wyniki egzamin√≥w standaryzowanych\n\n\nR√≥wnomierny (Jednostajny)\nJednakowe prawdopodobie≈Ñstwo w ca≈Çym zakresie\nOstatnia cyfra numeru telefonu, wyb√≥r losowego dnia tygodnia, pozycja wskaz√≥wki po zakrƒôceniu ko≈Çem fortuny\n\n\nDwumodalny (Bimodalny)\nDwa wyra≈∫ne szczyty, sugeruje istnienie podgrup\nStruktura wieku w miastach uniwersyteckich (studenci i stali mieszka≈Ñcy), opinie na tematy silnie polaryzujƒÖce spo≈Çecze≈Ñstwo, godziny natƒô≈ºenia ruchu drogowego (poranny i popo≈Çudniowy szczyt)\n\n\nSko≈õny w prawo (Prawostronnie asymetryczny)\nWyd≈Çu≈ºony ‚Äúogon‚Äù po prawej stronie, wiƒôkszo≈õƒá warto≈õci mniejsza od ≈õredniej\nCzas oczekiwania w kolejce, czas dojazdu do pracy, wiek zawarcia pierwszego ma≈Ç≈ºe≈Ñstwa\n\n\nSko≈õny z grubym ogonem (Log-normalny)\nSilna asymetria w prawo, warto≈õci nie mogƒÖ byƒá ujemne, d≈Çugi ‚Äúgruby ogon‚Äù\nDochody osobiste, ceny mieszka≈Ñ, wielko≈õƒá gospodarstw domowych\n\n\nSko≈õny o ekstremalnym ogonie (Potƒôgowy)\nEkstremalna asymetria, efekt ‚Äúbogaty staje siƒô bogatszym‚Äù, brak charakterystycznej skali\nMajƒÖtek najbogatszych os√≥b, populacja miast, liczba obserwujƒÖcych w mediach spo≈Çeczno≈õciowych, liczba cytowa≈Ñ publikacji naukowych",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wizualizacja-rozk≈Çad√≥w-danych-rzeczywistych",
    "href": "rozdzial5.html#wizualizacja-rozk≈Çad√≥w-danych-rzeczywistych",
    "title": "6¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.3 Wizualizacja rozk≈Çad√≥w danych rzeczywistych",
    "text": "6.3 Wizualizacja rozk≈Çad√≥w danych rzeczywistych\nU≈ºyjemy zbioru danych palmerpenguins do wizualizacji rozk≈Çad√≥w danych.\n\nHistogram i wykres gƒôsto≈õci\n\n\n\n\n\n\nUnderstanding Histograms and Density\n\n\n\n‚≠ê A histogram is a special graph for numerical data where:\n\nData is grouped into ranges (called ‚Äúbins‚Äù)\nBars touch each other (unlike bar charts!) because the data is continuous\nEach bar‚Äôs height shows how many values fall into that range\n\nThink of density as showing how common or concentrated certain values are in your data:\n\nA higher point on a density curve (or taller bar in a histogram) means those values appear more frequently in your data\nA lower point means those values are less common\n\nJust like a crowded area has more people per space (higher density), a taller part of the graph shows values that appear more often in your dataset!\n\n\n\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozk≈Çad d≈Çugo≈õci p≈Çetw pingwin√≥w\", \n       x = \"D≈Çugo≈õƒá p≈Çetwy (mm)\", \n       y = \"Gƒôsto≈õƒá\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\nWykres pude≈Çkowy\nWykresy pude≈Çkowe sƒÖ przydatne do por√≥wnywania rozk≈Çad√≥w miƒôdzy kategoriami.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Rozk≈Çad masy cia≈Ça pingwin√≥w wed≈Çug gatunku\", \n       x = \"Gatunek\", \n       y = \"Masa cia≈Ça (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nWykres skrzypcowy\nWykresy skrzypcowe ≈ÇƒÖczƒÖ cechy wykresu pude≈Çkowego i wykresu gƒôsto≈õci.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Rozk≈Çad masy cia≈Ça pingwin√≥w wed≈Çug gatunku\", \n       x = \"Gatunek\", \n       y = \"Masa cia≈Ça (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nWykres grzbietowy\nWykresy grzbietowe sƒÖ przydatne do por√≥wnywania wielu rozk≈Çad√≥w.\n\nlibrary(ggridges)\n\nggplot(penguins, aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.6) +\n  labs(title = \"Rozk≈Çad d≈Çugo≈õci p≈Çetw wed≈Çug gatunku pingwina\",\n       x = \"D≈Çugo≈õƒá p≈Çetwy (mm)\",\n       y = \"Gatunek\")\n\nPicking joint bandwidth of 2.38\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n\n\nPodsumowanie\nZrozumienie i wizualizacja rozk≈Çad√≥w danych sƒÖ kluczowe w analizie danych. ggplot2 zapewnia elastyczny i potƒô≈ºny zestaw narzƒôdzi do tworzenia r√≥≈ºnych typ√≥w wykres√≥w rozk≈Çad√≥w. BadajƒÖc r√≥≈ºne techniki wizualizacji, mo≈ºemy uzyskaƒá wglƒÖd w podstawowe wzorce i charakterystyki naszych danych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#warto≈õci-odstajƒÖce-outliers",
    "href": "rozdzial5.html#warto≈õci-odstajƒÖce-outliers",
    "title": "6¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.4 Warto≈õci OdstajƒÖce (Outliers)",
    "text": "6.4 Warto≈õci OdstajƒÖce (Outliers)\nPrzed zag≈Çƒôbieniem siƒô w konkretne miary, kluczowe jest zrozumienie pojƒôcia warto≈õci odstajƒÖcych, poniewa≈º mogƒÖ one znaczƒÖco wp≈Çywaƒá na wiele statystyk opisowych.\nWarto≈õci odstajƒÖce to punkty danych, kt√≥re znacznie r√≥≈ºniƒÖ siƒô od innych obserwacji w zbiorze danych. MogƒÖ wystƒÖpiƒá z powodu:\n\nB≈Çƒôd√≥w pomiaru lub zapisu\nPrawdziwych ekstremalnych warto≈õci w populacji\n\nWarto≈õci odstajƒÖce mogƒÖ mieƒá istotny wp≈Çyw na wiele miar statystycznych, szczeg√≥lnie tych opartych na ≈õrednich lub sumach kwadrat√≥w odchyle≈Ñ. Dlatego wa≈ºne jest, aby:\n\nIdentyfikowaƒá warto≈õci odstajƒÖce zar√≥wno poprzez metody statystyczne, jak i wiedzƒô dziedzinowƒÖ\nBadaƒá przyczyny warto≈õci odstajƒÖcych\nPodejmowaƒá ≈õwiadome decyzje o tym, czy w≈ÇƒÖczaƒá je do analiz, czy nie",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#symbole-stosowane-w-statystyce---podsumowanie",
    "href": "rozdzial5.html#symbole-stosowane-w-statystyce---podsumowanie",
    "title": "6¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.5 Symbole Stosowane w Statystyce - podsumowanie",
    "text": "6.5 Symbole Stosowane w Statystyce - podsumowanie\n\n\n\n\n\n\n\n\n\n\nMiara\nParametr Populacji\nStatystyka z Pr√≥by\nAlternatywne Oznaczenia\nUwagi\n\n\n\n\nLiczebno≈õƒá\nN\nn\n-\nCa≈Çkowita liczba obserwacji\n\n\n≈örednia\n\\mu\n\\bar{x}\nE(X), M\nE(X) stosowane w rachunku prawdopodobie≈Ñstwa\n\n\nWariancja\n\\sigma^2\ns^2\n\\text{Var}(X), V(X)\nKwadrat odchyle≈Ñ od ≈õredniej\n\n\nOdchylenie standardowe\n\\sigma\ns\n\\text{OS}, \\text{std}\nPierwiastek z wariancji\n\n\nFrakcja/Proporcja\n\\pi, P\n\\hat{p}\n\\text{fr}\nCzƒôsto≈õci wzglƒôdne\n\n\nWsp√≥≈Çczynnik korelacji\n\\rho\nr\n\\text{kor}(x,y)\nWarto≈õci od -1 do +1\n\n\nB≈ÇƒÖd standardowy\n\\sigma_{\\bar{x}}\ns_{\\bar{x}}\n\\text{BS}\nB≈ÇƒÖd standardowy ≈õredniej\n\n\nSuma\n\\sum\n\\sum\n\\sum_{i=1}^n\nZ indeksowaniem\n\n\nPojedyncza obserwacja\nX_i\nx_i\n-\ni-ta obserwacja\n\n\nKowariancja\n\\sigma_{xy}\ns_{xy}\n\\text{Cov}(X,Y)\nWsp√≥lna zmienno≈õƒá\n\n\nMediana\n\\eta\n\\text{Me}\nM\nWarto≈õƒá ≈õrodkowa\n\n\nRozstƒôp\nR\nr\n\\text{max}(X) - \\text{min}(X)\nMiara rozproszenia\n\n\nDominanta\n\\text{Mo}\n\\text{mo}\n\\text{mod}\nWarto≈õƒá najczƒôstsza\n\n\nSko≈õno≈õƒá\n\\gamma_1\ng_1\n\\text{SK}\nAsymetria rozk≈Çadu\n\n\nKurtoza\n\\gamma_2\ng_2\n\\text{KU}\nSp≈Çaszczenie rozk≈Çadu\n\n\n\nDodatkowe wa≈ºne wzory:\n\nMomenty z pr√≥by: m_k = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^k\nMomenty populacji: \\mu_k = E[(X - \\mu)^k]",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-tendencji-centralnej",
    "href": "rozdzial5.html#miary-tendencji-centralnej",
    "title": "6¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.6 Miary Tendencji Centralnej",
    "text": "6.6 Miary Tendencji Centralnej\nMiary tendencji centralnej majƒÖ na celu identyfikacjƒô ‚Äútypowej‚Äù lub ‚Äúcentralnej‚Äù warto≈õci w zbiorze danych. Trzy podstawowe miary to ≈õrednia, mediana i moda.\n\n≈örednia Arytmetyczna\n≈örednia arytmetyczna to suma wszystkich warto≈õci podzielona przez liczbƒô warto≈õci.\nWz√≥r: \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\nWa≈ºna W≈Ça≈õciwo≈õƒá: ≈örednia jest punktem r√≥wnowagi w danych. Suma odchyle≈Ñ od ≈õredniej zawsze wynosi zero:\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0\nTa w≈Ça≈õciwo≈õƒá sprawia, ≈ºe ≈õrednia jest u≈ºyteczna w wielu obliczeniach statystycznych.\n\n\n\n\n\n\nZrozumienie ≈õredniej jako punktu r√≥wnowagi üéØ\n\n\n\nRozwa≈ºmy zbi√≥r danych X = \\{1, 2, 6, 7, 9\\} na osi liczbowej, wyobra≈ºajƒÖc go sobie jako hu≈õtawkƒô:\n\n\n\nhttps://www.gastonsanchez.com/matrix4sl/mean-as-a-balancing-point.html\n\n\n≈örednia (\\mu) dzia≈Ça jak idealny punkt r√≥wnowagi tej hu≈õtawki. Dla naszych danych:\n\\mu = \\frac{1 + 2 + 6 + 7 + 9}{5} = 5\n\nCo siƒô dzieje przy r√≥≈ºnych punktach podparcia? ü§î\n\nPunkt podparcia w 6 (za wysoko):\n\nLewa strona: Warto≈õci (1, 2) sƒÖ poni≈ºej\nPrawa strona: Warto≈õci (7, 9) sƒÖ powy≈ºej\n\\sum odleg≈Ço≈õci z lewej = (6-1) + (6-2) = 9\n\\sum odleg≈Ço≈õci z prawej = (7-6) + (9-6) = 4\nHu≈õtawka przechyla siƒô w lewo! ‚¨ÖÔ∏è bo 9 &gt; 4\n\nPunkt podparcia w 4 (za nisko):\n\nLewa strona: Warto≈õci (1, 2) sƒÖ poni≈ºej\nPrawa strona: Warto≈õci (6, 7, 9) sƒÖ powy≈ºej\n\\sum odleg≈Ço≈õci z lewej = (4-1) + (4-2) = 5\n\\sum odleg≈Ço≈õci z prawej = (6-4) + (7-4) + (9-4) = 10\nHu≈õtawka przechyla siƒô w prawo! ‚û°Ô∏è bo 5 &lt; 10\n\nPunkt podparcia w ≈õredniej (5) (idealna r√≥wnowaga):\n\n\\sum odleg≈Ço≈õci poni≈ºej = \\sum odleg≈Ço≈õci powy≈ºej\n((5-1) + (5-2)) = ((6-5) + (7-5) + (9-5))\n7 = 7 ‚ú® Idealna r√≥wnowaga!\n\n\nTo pokazuje, dlaczego ≈õrednia jest unikalnym punktem r√≥wnowagi, gdzie:\n\\sum_{i=1}^n (x_i - \\mu) = 0\nHu≈õtawka zawsze bƒôdzie siƒô przechylaƒá, chyba ≈ºe punkt podparcia zostanie umieszczony dok≈Çadnie w ≈õredniej! üé™\n\n\n\n\n\n\n\n\n\n≈örednia jako punkt r√≥wnowagi\n\n\n\nTa wizualizacja pokazuje, jak ≈õrednia arytmetyczna (5) dzia≈Ça jako punkt r√≥wnowagi pomiƒôdzy skupionymi punktami z lewej strony a rozproszonymi punktami z prawej strony:\nLewa strona ≈õredniej:\n\nPunkty o warto≈õciach 2 i 3\nBlisko siebie (r√≥≈ºnica 1 jednostka)\nOdleg≈Ço≈õci od ≈õredniej: 3 i 2 jednostki\nSuma ‚ÄúciƒÖ≈ºenia‚Äù = 5 jednostek\n\nPrawa strona ≈õredniej:\n\nPunkty o warto≈õciach 6 i 9\nBardziej oddalone (r√≥≈ºnica 3 jednostki)\nOdleg≈Ço≈õci od ≈õredniej: 1 i 4 jednostki\nSuma ‚ÄúciƒÖ≈ºenia‚Äù = 5 jednostek\n\nKluczowe obserwacje:\n\n≈örednia (5) jest punktem r√≥wnowagi, mimo ≈ºe:\n\nPunkty po lewej sƒÖ skupione (2,3)\nPunkty po prawej sƒÖ rozproszone (6,9)\nZielone strza≈Çki pokazujƒÖ odleg≈Ço≈õci od ≈õredniej\n\nR√≥wnowaga jest zachowana poniewa≈º:\n\nSuma odleg≈Ço≈õci siƒô r√≥wnowa≈ºy: (5-2) + (5-3) = (6-5) + (9-5)\nCa≈Çkowita suma odleg≈Ço≈õci = 5 jednostek po ka≈ºdej stronie\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrzyk≈Çad Rƒôcznego Obliczenia:\nObliczmy ≈õredniƒÖ dla zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nSumuj wszystkie warto≈õci\n2 + 4 + 4 + 5 + 5 + 7 + 9 = 36\n\n\n2\nPolicz liczbƒô warto≈õci\nn = 7\n\n\n3\nPodziel sumƒô przez n\n36 / 7 = 5,14\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(dane)\n\n[1] 5.142857\n\n\nZalety:\n\n≈Åatwa do obliczenia i zrozumienia\nWykorzystuje wszystkie punkty danych\n\nWady:\n\nWra≈ºliwa na warto≈õci odstajƒÖce\nMo≈ºe nie byƒá dobrƒÖ miarƒÖ dla silnie asymetrycznych rozk≈Çad√≥w danych\n\n\n\nMediana\nMediana to ≈õrodkowa warto≈õƒá, gdy dane sƒÖ uporzƒÖdkowane.\nPrzyk≈Çad Rƒôcznego Obliczenia:\nU≈ºywajƒÖc tego samego zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nWynik\n\n\n\n\n1\nUporzƒÖdkuj dane\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nZnajd≈∫ ≈õrodkowƒÖ warto≈õƒá\n5\n\n\n\nDla parzystej liczby warto≈õci, we≈∫ ≈õredniƒÖ z dw√≥ch ≈õrodkowych warto≈õci.\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmedian(dane)\n\n[1] 5\n\n\nZalety:\n\nNie jest zniekszta≈Çcona przez skrajne warto≈õci odstajƒÖce (outliers)\nLepsza dla rozk≈Çad√≥w sko≈õnych\n\nWady:\n\nNie wykorzystuje wszystkich punkt√≥w danych\n\n\n\n\n\n\n\nWarning\n\n\n\nJak znale≈∫ƒá pozycjƒô mediany w zbiorze danych:\n\nNajpierw posortuj dane rosnƒÖco\nGdy n jest nieparzyste:\n\nPozycja mediany = \\frac{n + 1}{2}\n\nGdy n jest parzyste:\n\nPierwsza pozycja mediany = \\frac{n}{2}\nDruga pozycja mediany = \\frac{n}{2} + 1\nMediana = \\frac{\\text{warto≈õƒá na pozycji }\\frac{n}{2} + \\text{warto≈õƒá na pozycji }(\\frac{n}{2}+1)}{2}\n\n\nPrzyk≈Çady:\n\nNieparzyste n=7: pozycja = \\frac{7+1}{2} = 4-ta warto≈õƒá\nParzyste n=8: pozycje = \\frac{8}{2} = 4-ta i 4+1 = 5-ta warto≈õƒá\n\n\n\n\n\nModa (Dominanta)\nModa to najczƒô≈õciej wystƒôpujƒÖca warto≈õƒá.\nPrzyk≈Çad Rƒôcznego Obliczenia:\nU≈ºywajƒÖc zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nWarto≈õƒá\nCzƒôsto≈õƒá\n\n\n\n\n2\n1\n\n\n4\n2\n\n\n5\n2\n\n\n7\n1\n\n\n9\n1\n\n\n\nModa to 4 i 5 (rozk≈Çad bimodalny).\nObliczenie w R:\n\nlibrary(modeest)\nmfv(dane)  # Najczƒô≈õciej wystƒôpujƒÖca warto≈õƒá\n\n[1] 4 5\n\n\nZalety:\n\nJedyna miara tendencji centralnej dla danych nominalnych\nMo≈ºe identyfikowaƒá wiele punkt√≥w szczytowych (dominujƒÖcych) w danych\n\nWady:\n\nNie zawsze jednoznacznie zdefiniowana\nNie jest odpowiednia dla danych ciƒÖg≈Çych\n\n\n\n≈örednia (arytmetyczna) Wa≈ºona (*)\n≈örednia wa≈ºona jest u≈ºywana, gdy niekt√≥re punkty danych sƒÖ wa≈ºniejsze ni≈º inne. WystƒôpujƒÖ dwa typy ≈õrednich wa≈ºonych: z wagami nienormalizowanymi i z wagami znormalizowanymi.\n\n≈örednia Wa≈ºona z Wagami Nienormalizowanymi\nJest to standardowa forma ≈õredniej wa≈ºonej, gdzie wagi mogƒÖ byƒá dowolnymi liczbami dodatnimi reprezentujƒÖcymi wa≈ºno≈õƒá ka≈ºdego punktu danych.\nWz√≥r: \\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\nPrzyk≈Çad Oblicze≈Ñ Rƒôcznych: Obliczmy ≈õredniƒÖ wa≈ºonƒÖ dla zbioru danych: 2, 4, 5, 7 z wagami 1, 2, 3, 1\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPomn√≥≈º ka≈ºdƒÖ warto≈õƒá przez jej wagƒô\n(2 * 1) + (4 * 2) + (5 * 3) + (7 * 1) = 2 + 8 + 15 + 7 = 32\n\n\n2\nZsumuj wagi\n1 + 2 + 3 + 1 = 7\n\n\n3\nPodziel wynik z kroku 1 przez wynik z kroku 2\n32 / 7 = 4.57\n\n\n\nObliczenia w R:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\n≈örednia Wa≈ºona z Wagami Znormalizowanymi (U≈Çamki)\nW tym przypadku wagi sƒÖ u≈Çamkami sumujƒÖcymi siƒô do 1, reprezentujƒÖcymi proporcjƒô wa≈ºno≈õci dla ka≈ºdego punktu danych.\nWz√≥r: \\bar{x}_w = \\sum_{i=1}^n w_i x_i, gdzie \\sum_{i=1}^n w_i = 1\nPrzyk≈Çad Oblicze≈Ñ Rƒôcznych:\nObliczmy ≈õredniƒÖ wa≈ºonƒÖ dla zbioru danych: 2, 4, 5, 7 z wagami znormalizowanymi 0.1, 0.3, 0.4, 0.2\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPomn√≥≈º ka≈ºdƒÖ warto≈õƒá przez jej wagƒô\n(2 * 0.1) + (4 * 0.3) + (5 * 0.4) + (7 * 0.2)\n\n\n2\nZsumuj wyniki\n0.2 + 1.2 + 2.0 + 1.4 = 4.8\n\n\n\nObliczenia w R:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Uwaga: sumujƒÖ siƒô do 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nZalety ≈örednich Wa≈ºonych:\n\nUwzglƒôdniajƒÖ r√≥≈ºnƒÖ wa≈ºno≈õƒá punkt√≥w danych\n\nWady ≈örednich Wa≈ºonych:\n\nWymagajƒÖ uzasadnienia dla wag\nMogƒÖ byƒá niew≈Ça≈õciwie wykorzystane w celu manipulacji wynikami\nMogƒÖ byƒá mniej intuicyjne w interpretacji ni≈º prosta ≈õrednia arytmetyczna",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-zmienno≈õci-rozproszenia",
    "href": "rozdzial5.html#miary-zmienno≈õci-rozproszenia",
    "title": "6¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.7 Miary Zmienno≈õci (Rozproszenia)",
    "text": "6.7 Miary Zmienno≈õci (Rozproszenia)\nTe miary opisujƒÖ, jak bardzo rozproszone sƒÖ dane.\n\n\n\n\n\n\nZrozumienie Wariancji\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†6.1: Trzy wykresy punktowe pokazujƒÖce rosnƒÖcƒÖ wariancjƒô przy sta≈Çej ≈õredniej\n\n\n\n\n\nPowy≈ºsze trzy wykresy punktowe pokazujƒÖ, w jaki spos√≥b wariancja mierzy rozproszenie danych wok√≥≈Ç warto≈õci centralnej:\n\nWszystkie rozk≈Çady majƒÖ tƒô samƒÖ ≈õredniƒÖ (Œº = 10), oznaczonƒÖ liniƒÖ przerywanƒÖ\nMa≈Ça Wariancja (œÉ¬≤ = 1): Punkty sƒÖ skupione blisko ≈õredniej\n≈örednia Wariancja (œÉ¬≤ = 4): Punkty wykazujƒÖ umiarkowane rozproszenie\nDu≈ºa Wariancja (œÉ¬≤ = 9): Punkty sƒÖ szeroko rozproszone wok√≥≈Ç ≈õredniej\n\n\n\n\n\n\n\n\n\nR√≥≈ºne Poziomy Zmienno≈õci\n\n\n\n\n\n\n\n\n\n\n\n\nTa wizualizacja przedstawia trzy rozk≈Çady normalne o tej samej ≈õredniej (Œº = 10), ale r√≥≈ºnych poziomach zmienno≈õci:\n\nMa≈Ça zmienno≈õƒá (œÉ = 0.5)\n\nPunkty danych grupujƒÖ siƒô ≈õci≈õle wok√≥≈Ç ≈õredniej\nKrzywa gƒôsto≈õci jest wysoka i wƒÖska\nWiƒôkszo≈õƒá obserwacji mie≈õci siƒô w przedziale ¬±0.5 jednostki (odchylenia stand.) od ≈õredniej\n\n≈örednia zmienno≈õƒá (œÉ = 2.0)\n\nPunkty danych sƒÖ bardziej rozproszone wok√≥≈Ç ≈õredniej\nKrzywa gƒôsto≈õci jest ni≈ºsza i szersza\nWiƒôkszo≈õƒá obserwacji mie≈õci siƒô w przedziale ¬±2 jednostki od ≈õredniej\n\nDu≈ºa zmienno≈õƒá (œÉ = 4.0)\n\nPunkty danych sƒÖ szeroko rozproszone wok√≥≈Ç ≈õredniej\nKrzywa gƒôsto≈õci jest znacznie bardziej p≈Çaska i szeroka\nWiƒôkszo≈õƒá obserwacji mie≈õci siƒô w przedziale ¬±4 jednostki od ≈õredniej\n\n\nZwr√≥ƒá uwagƒô, jak odchylenie standardowe (œÉ) bezpo≈õrednio powiƒÖzane jest z rozproszeniem rozk≈Çadu - wiƒôksze warto≈õci œÉ wskazujƒÖ na wiƒôkszƒÖ zmienno≈õƒá danych, podczas gdy mniejsze warto≈õci oznaczajƒÖ, ≈ºe punkty danych majƒÖ tendencjƒô do grupowania siƒô bli≈ºej ≈õredniej.\n\n\n\nRozstƒôp\nRozstƒôp to r√≥≈ºnica miƒôdzy warto≈õciƒÖ maksymalnƒÖ a minimalnƒÖ.\nWz√≥r: R = x_{max} - x_{min}\nPrzyk≈Çad Rƒôcznego Obliczenia:\nU≈ºywajƒÖc zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nZnajd≈∫ warto≈õƒá maksymalnƒÖ\n9\n\n\n2\nZnajd≈∫ warto≈õƒá minimalnƒÖ\n2\n\n\n3\nOdejmij minimum od maksimum\n9 - 2 = 7\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nrange(dane)\n\n[1] 2 9\n\nmax(dane) - min(dane)\n\n[1] 7\n\n\nZalety:\n\nProsty do obliczenia i zrozumienia\nSzybka informacja o og√≥lnym rozproszeniu danych\n\nWady:\n\nBardzo wra≈ºliwy na warto≈õci odstajƒÖce\nNie dostarcza informacji o rozk≈Çadzie miƒôdzy skrajno≈õciami\n\n\n\nRozstƒôp Miƒôdzykwartylowy (IQR)\nIQR to r√≥≈ºnica miƒôdzy 75. a 25. percentylem (3. a 1. kwartylem).\nWz√≥r: IQR = Q_3 - Q_1\nAby znale≈∫ƒá kwartyle rƒôcznie:\n\nDla nieparzystej liczby warto≈õci:\n\nQ2 (mediana) to ≈õrodkowa warto≈õƒá\nQ1 to mediana dolnej po≈Çowy (wy≈ÇƒÖczajƒÖc medianƒô dla wszystkich obserwacji)\nQ3 to mediana g√≥rnej po≈Çowy (wy≈ÇƒÖczajƒÖc medianƒô dla wszystkich obserwacji)\n\nDla parzystej liczby warto≈õci:\n\nQ2 to ≈õrednia z dw√≥ch ≈õrodkowych warto≈õci\nQ1 to mediana dolnej po≈Çowy (wy≈ÇƒÖczajƒÖc medianƒô dla wszystkich obserwacji)\nQ3 to mediana g√≥rnej po≈Çowy (wy≈ÇƒÖczajƒÖc medianƒô dla wszystkich obserwacji)\n\n\nPrzyk≈Çad Rƒôcznego Obliczenia:\nU≈ºywajƒÖc zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nUporzƒÖdkuj dane\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nZnajd≈∫ Q2 (medianƒô)\n5\n\n\n3\nZnajd≈∫ Q1 (medianƒô dolnej po≈Çowy)\n4\n\n\n4\nZnajd≈∫ Q3 (medianƒô g√≥rnej po≈Çowy)\n7\n\n\n5\nOblicz IQR\nQ3 - Q1 = 7 - 4 = 3\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nprint(dane)\n\n[1] 2 4 4 5 5 7 9\n\nquantile(dane, type = 1)\n\n  0%  25%  50%  75% 100% \n   2    4    5    7    9 \n\nIQR(dane, type = 1)\n\n[1] 3\n\n\nZalety:\n\nOdporny na warto≈õci odstajƒÖce\nDostarcza informacji o rozproszeniu ≈õrodkowych 50% danych\n\nWady:\n\nIgnoruje ogony rozk≈Çadu\nMniej efektywny ni≈º odchylenie standardowe dla rozk≈Çad√≥w normalnych\n\n\n\nWariancja\nWariancja mierzy ≈õrednie kwadratowe odchylenie od ≈õredniej.\nWz√≥r: s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n\n\n\n\n\n\nWariancja: Zrozumienie ≈öredniego Odchylenia Kwadratowego\n\n\n\nCzym jest Wariancja? Wariancja mierzy, jak bardzo punkty danych sƒÖ ‚Äúrozrzucone‚Äù wok√≥≈Ç ≈õredniej - jest ≈õredniƒÖ kwadrat√≥w odchyle≈Ñ od ≈õredniej.\nWz√≥r: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nProsty Przyk≈Çad: Rozwa≈ºmy liczby: 2, 4, 6, 8, 10 ≈örednia (\\bar{x}) = 6\n\n\n\n\n\n\n\n\n\nObliczanie Odchyle≈Ñ:\n\n\n\n\n\n\n\n\n\n\n\n\nWarto≈õƒá\nOdchylenie od ≈õredniej\nKwadrat odchylenia\n\n\n\n\n2\n-4\n16\n\n\n4\n-2\n4\n\n\n6\n0\n0\n\n\n8\n+2\n4\n\n\n10\n+4\n16\n\n\n\nWariancja = \\frac{16 + 4 + 0 + 4 + 16}{4} = 10\nKluczowe Punkty:\n\n≈örednia s≈Çu≈ºy jako punkt odniesienia (niebieska przerywana linia)\nOdchylenia pokazujƒÖ odleg≈Ço≈õƒá od ≈õredniej (czerwone kropkowane linie)\nPodniesienie do kwadratu sprawia, ≈ºe wszystkie odchylenia sƒÖ dodatnie (niebieskie s≈Çupki)\nWiƒôksze odchylenia majƒÖ wiƒôkszy wp≈Çyw na wariancjƒô\n\n\n\nPrzyk≈Çad Rƒôcznego Obliczenia:\nU≈ºywajƒÖc zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz ≈õredniƒÖ\n\\bar{x} = 5,14\n\n\n2\nOdejmij ≈õredniƒÖ od ka≈ºdej obserwacji i podnie≈õ wynik do kwadratu\n(2 - 5,14)^2 = 9,86\n\n\n\n\n(4 - 5,14)^2 = 1,30\n\n\n\n\n(4 - 5,14)^2 = 1,30\n\n\n\n\n(5 - 5,14)^2 = 0,02\n\n\n\n\n(5 - 5,14)^2 = 0,02\n\n\n\n\n(7 - 5,14)^2 = 3,46\n\n\n\n\n(9 - 5,14)^2 = 14,90\n\n\n3\nSumuj kwadraty r√≥≈ºnic\n30,86\n\n\n4\nPodziel przez (n-1), czyli przez liczbƒô obserwacji - 1\n30,86 / 6 = 5,14\n\n\n\nObliczenie w R:\n\nvar(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nWykorzystuje wszystkie punkty danych\nPodstawa dla wielu test√≥w statystycznych*\n\nWady:\n\nJednostki sƒÖ podniesione do kwadratu, co utrudnia interpretacjƒô\nWra≈ºliwa na warto≈õci odstajƒÖce\n\n\n\n\n\n\n\nPoprawka Bessela: Dlaczego Dzielimy przez (n-1), a nie po prostu przez n\n\n\n\nGdy obliczamy odchylenia od ≈õredniej, ich suma musi wynosiƒá zero. To matematyczny fakt: \\sum(x_i - \\bar{x}) = 0\nPomy≈õl o tym Tak:\nJe≈õli masz 5 liczb i ich ≈õredniƒÖ:\n\nPo obliczeniu 4 odchyle≈Ñ od ≈õredniej\n5-te odchylenie MUSI byƒá takie, ≈ºeby suma by≈Ça zero\nNie masz tak naprawdƒô 5 niezale≈ºnych odchyle≈Ñ\nMasz tylko 4 prawdziwie ‚Äúswobodne‚Äù odchylenia\n\nProsty Przyk≈Çad:\nLiczby: 2, 4, 6, 8, 10\n\n≈örednia = 6\nOdchylenia: -4, -2, 0, +2, +4\nZauwa≈º, ≈ºe sumujƒÖ siƒô do zera\nJe≈õli znasz dowolne 4 odchylenia, 5-te jest z g√≥ry okre≈õlone!\n\nDlatego W≈Ça≈õnie:\n\nPrzy obliczaniu wariancji: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nDzielimy przez (n-1), a nie n\nPoniewa≈º tylko (n-1) odchyle≈Ñ jest naprawdƒô niezale≈ºnych\nOstatnie jest okre≈õlone przez pozosta≈Çe\n\nStopnie Swobody:\n\nn = liczba obserwacji\n1 = ograniczenie (odchylenia muszƒÖ sumowaƒá siƒô do zera)\nn-1 = stopnie swobody = liczba prawdziwie niezale≈ºnych odchyle≈Ñ\n\nKiedy Stosowaƒá:\n\nPrzy obliczaniu wariancji z pr√≥by\nPrzy obliczaniu odchylenia standardowego z pr√≥by\n\nKiedy NIE Stosowaƒá:\n\nW obliczeniach dla ca≈Çej populacji (gdy mamy wszystkie dane)\nPrzy obliczaniu odchylenia od ustalonej, znanej warto≈õci parametru populacji statystycznej\n\nPamiƒôtaj:\n\nTo nie jest tylko statystyczny trik\nOdchylenia od ≈õredniej muszƒÖ sumowaƒá siƒô do zera\nTo ograniczenie kosztuje nas jeden stopie≈Ñ swobody\n\n\n\n\n\nOdchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji i mierzy przeciƒôtne rozproszenie danych wzglƒôdem ich ≈õredniej arytmetycznej. W przeciwie≈Ñstwie do wariancji, jest to miara mianowana i interpretowana w jednostkach bdanej zmiennej.\nWz√≥r: s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\nPrzyk≈Çad Rƒôcznego Obliczenia:\nU≈ºywajƒÖc zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz wariancjƒô\ns^2 = 5,14 (z poprzedniego obliczenia)\n\n\n2\nWyciƒÖgnij pierwiastek kwadratowy\ns = \\sqrt{5,14} = 2,27\n\n\n\nObliczenie w R:\n\nsd(dane)\n\n[1] 2.267787\n\n\nZalety:\n\nW tych samych jednostkach co oryginalne dane\nSzeroko stosowane i zrozumia≈Çe\n\nWady:\n\nNadal wra≈ºliwe na warto≈õci odstajƒÖce\nZak≈Çada, ≈ºe dane sƒÖ w przybli≈ºeniu ‚Äúnormalnie‚Äù roz≈Ço≈ºone\n\n\n\nWsp√≥≈Çczynnik zmienno≈õci (*)\nWsp√≥≈Çczynnik zmienno≈õci to odchylenie standardowe podzielone przez ≈õredniƒÖ arytmetycznƒÖ, czƒôsto wyra≈ºany jako warto≈õƒá procentowa.\nWz√≥r: CV = \\frac{s}{\\bar{x}} \\times 100\\%\nPrzyk≈Çad oblicze≈Ñ rƒôcznych:\nDla zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nOblicz ≈õredniƒÖ arytmetycznƒÖ\n\\bar{x} = 5,14\n\n\n2\nOblicz odchylenie standardowe\ns = 2,27\n\n\n3\nPodziel s przez ≈õredniƒÖ i pomn√≥≈º przez 100\n(2,27 / 5,14) * 100 = 44,16\\%\n\n\n\nObliczenia w R:\n\n(sd(dane) / mean(dane)) * 100\n\n[1] 44.09586\n\n\nZalety:\n- Umo≈ºliwia por√≥wnanie zmienno≈õci miƒôdzy zbiorami danych o r√≥≈ºnych jednostkach lub ≈õrednich\n- Przydatny w dziedzinach takich jak finanse do oceny ryzyka\nWady:\n- Nie ma znaczenia dla danych zawierajƒÖcych zar√≥wno warto≈õci dodatnie, jak i ujemne\n- Mo≈ºe byƒá mylƒÖcy, gdy ≈õrednia jest bliska zeru\n\n\n\n\n\n\nOgraniczenia Wsp√≥≈Çczynnika Zmienno≈õci (CV)\n\n\n\nWsp√≥≈Çczynnik zmienno≈õci, obliczany jako (œÉ/Œº) √ó 100\\%, ma dwa istotne ograniczenia:\n\nNie ma interpretacji dla danych zawierajƒÖcych warto≈õci dodatnie i ujemne\n\n≈örednia mo≈ºe byƒá bliska zeru ze wzglƒôdu na wzajemne znoszenie siƒô warto≈õci dodatnich i ujemnych\nPrzyk≈Çad: Zbi√≥r danych {-5, -3, 2, 6} ma ≈õredniƒÖ = 0\n\nCV = (odch. std. / 0) √ó 100%\nProwadzi to do dzielenia przez zero\nNawet gdy ≈õrednia nie jest dok≈Çadnie zero, CV nie reprezentuje prawdziwej wzglƒôdnej zmienno≈õci, gdy dane przechodzƒÖ przez zero\n\nCV zak≈Çada naturalny punkt zerowy i sensowne proporcje miƒôdzy warto≈õciami\n\n\n\nMylƒÖcy gdy ≈õrednia jest bliska zeru\n\nPoniewa≈º CV = (œÉ/Œº) √ó 100\\%, gdy Œº zbli≈ºa siƒô do zera:\n\nMianownik staje siƒô bardzo ma≈Çy\nSkutkuje to ekstremalnie du≈ºymi warto≈õciami CV\nTe du≈ºe warto≈õci nie reprezentujƒÖ sensownie wzglƒôdnej zmienno≈õci\n\nPrzyk≈Çad:\n\nZbi√≥r danych A: {0.001, 0.002, 0.003} ma ≈õredniƒÖ = 0.002\nNawet ma≈Çe odchylenia standardowe dadzƒÖ bardzo du≈ºe CV\nWynikajƒÖcy z tego du≈ºy CV mo≈ºe sugerowaƒá ekstremalne zr√≥≈ºnicowanie, gdy w rzeczywisto≈õci dane sƒÖ do≈õƒá skoncentrowane\n\n\n\n\nNajlepsze zastosowania\nCV jest najbardziej u≈ºyteczny dla:\n\nDanych ≈õci≈õle dodatnich\nDanych mierzonych na skali ilorazowej\nDanych ze ≈õredniƒÖ znacznie powy≈ºej zera\nPor√≥wnywania zmienno≈õci miƒôdzy zbiorami danych o r√≥≈ºnych jednostkach lub skalach",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-po≈Ço≈ºenia-wzglƒôdnego-wzglƒôdnej-pozycji",
    "href": "rozdzial5.html#miary-po≈Ço≈ºenia-wzglƒôdnego-wzglƒôdnej-pozycji",
    "title": "6¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.8 Miary Po≈Ço≈ºenia Wzglƒôdnego (Wzglƒôdnej Pozycji)",
    "text": "6.8 Miary Po≈Ço≈ºenia Wzglƒôdnego (Wzglƒôdnej Pozycji)\nZrozumienie relatywnej (wzglƒôdnej) pozycji warto≈õci w zbiorze danych.\n\nKwartyle (Q): Podstawy\nKwartyle to specjalne liczby, kt√≥re dzielƒÖ uporzƒÖdkowane dane na cztery r√≥wne czƒô≈õci.\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\n\nCzym sƒÖ Kwartyle?\nPierwszy Kwartyl (Q1):\n\nOddziela najni≈ºsze 25% danych od reszty\nNazywany r√≥wnie≈º 25-tym percentylem\nPrzyk≈Çad: Je≈õli Q1 = 50 w zbiorze wynik√≥w testu, 25% uczni√≥w uzyska≈Ço wynik poni≈ºej 50\n\nDrugi Kwartyl (Q2):\n\nMediana - dzieli dane na p√≥≈Ç\nNazywany r√≥wnie≈º 50-tym percentylem\nPrzyk≈Çad: Je≈õli Q2 = 70, po≈Çowa uczni√≥w uzyska≈Ça wynik poni≈ºej 70\n\nTrzeci Kwartyl (Q3):\n\nOddziela najwy≈ºsze 25% danych od reszty\nNazywany r√≥wnie≈º 75-tym percentylem\nPrzyk≈Çad: Je≈õli Q3 = 85, 75% uczni√≥w uzyska≈Ço wynik poni≈ºej 85\n\nZadanie 1: Kwartyle\nDane: 10, 12, 15, 15, 18, 20, 22, 25, 25 Znajd≈∫: Q1, Q2, Q3\nRozwiƒÖzanie:\n\nQ2 (n = 9, nieparzyste)\n\nPozycja = (9 + 1)/2 = 5\nQ2 = 18\n\nQ1\n\nPozycja = (9 + 1)/4 = 2.5\nMiƒôdzy 12 a 15\nQ1 = (12 + 15)/2 = 13.5\n\nQ3\n\nPozycja = 3(9 + 1)/4 = 7.5\nMiƒôdzy 22 a 25\nQ3 = (22 + 25)/2 = 23.5\n\n\n\n\nJak Obliczaƒá Kwartyle (Krok po Kroku) - Dwie Metody\nPrzeanalizujmy wyniki test√≥w uczni√≥w u≈ºywajƒÖc obu popularnych metod wyznaczania kwartyli:\nPrzyk≈Çad 1: Przypadek Nieparzystej Liczby Wynik√≥w (11 wynik√≥w)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 88, 90\nKrok 1: Znajd≈∫ Q2 (medianƒô) - Tak samo dla obu metod\n\nPrzy n = 11 warto≈õciach (nieparzyste)\nPozycja mediany = 2(n + 1)/4 = (n + 1)/2 = 6\nQ2 = 78\n\nKrok 2: Znajd≈∫ Q1\n\nMetoda Tukeya:\n\nSp√≥jrz na dolnƒÖ po≈Çowƒô: 60, 65, 70, 72, 75\nQ1 = mediana dolnej po≈Çowy = 70\n\nMetoda Interpolacji:\n\nPozycja = (n + 1)/4 = (11 + 1)/4 = 3\nQ1 = 70 (3-cia warto≈õƒá)\n\n\nKrok 3: Znajd≈∫ Q3\n\nMetoda Tukeya:\n\nSp√≥jrz na g√≥rnƒÖ po≈Çowƒô: 80, 82, 85, 88, 90\nQ3 = mediana g√≥rnej po≈Çowy = 85\n\nMetoda Interpolacji:\n\nPozycja = 3(n + 1)/4 = 3(12)/4 = 9\nQ3 = 85 (9-ta warto≈õƒá)\n\n\nPrzyk≈Çad 2: Przypadek Parzystej Liczby (10 wynik√≥w)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 90\nKrok 1: Znajd≈∫ Q2 (medianƒô) - Tak samo dla obu metod\n\nPrzy n = 10 warto≈õciach (parzyste)\nPozycje mediany = 5 i 6\nQ2 = (75 + 78)/2 = 76.5\n\nKrok 2: Znajd≈∫ Q1\n\nMetoda Tukeya:\n\nSp√≥jrz na dolnƒÖ po≈Çowƒô: 60, 65, 70, 72, 75\nQ1 = mediana dolnej po≈Çowy = 70\n\nMetoda Interpolacji:\n\nPozycja = (10 + 1)/4 = 2.75\nQ1 = 65 + 0.75(70 - 65) = 68.75\n\n\nKrok 3: Znajd≈∫ Q3\n\nMetoda Tukeya:\n\nSp√≥jrz na g√≥rnƒÖ po≈Çowƒô: 78, 80, 82, 85, 90\nQ3 = mediana g√≥rnej po≈Çowy = 82\n\nMetoda Interpolacji:\n\nPozycja = 3(10 + 1)/4 = 8.25\nQ3 = 82 + 0.25(85 - 82) = 82.75\n\n\nWa≈ºne Uwagi:\n\nMetoda Tukeya:\n\nNajpierw znajd≈∫ medianƒô (Q2)\nPodziel dane na dolnƒÖ i g√≥rnƒÖ po≈Çowƒô\nZnajd≈∫ Q1 jako medianƒô dolnej po≈Çowy\nZnajd≈∫ Q3 jako medianƒô g√≥rnej po≈Çowy\nGdy n jest nieparzyste, mediana nie jest uwzglƒôdniana w ≈ºadnej po≈Çowie\n\nMetoda Interpolacji:\n\nU≈ºywa pozycji (n+1)/4 dla Q1 i 3(n+1)/4 dla Q3\nGdy pozycja wypada miƒôdzy warto≈õciami, stosuje interpolacjƒô liniowƒÖ\nNie wymaga podzia≈Çu danych na po≈Çowy\n\n\n\n\n\n\n\n\nManual Construction of Tukey Boxplot\n\n\n\nStep 1: Calculate Key Components\n\nFind quartiles: Q_1, Q_2 (median), Q_3\nCalculate Interquartile Range: IQR = Q_3 - Q_1\n\nStep 2: Determine Whisker Boundaries\n\nLower fence: Q_1 - 1.5 \\times IQR\nUpper fence: Q_3 + 1.5 \\times IQR\n\nStep 3: Identify Outliers Data points are outliers if they are:\n\nBelow lower fence: x &lt; Q_1 - 1.5 \\times IQR\nAbove upper fence: x &gt; Q_3 + 1.5 \\times IQR\n\nExample: Given data: 2, 4, 6, 8, 9, 10, 11, 12, 14, 16, 50\n\nFind quartiles:\n\nQ_1 = 6\nQ_2 = 10\nQ_3 = 14\n\nCalculate IQR:\n\nIQR = 14 - 6 = 8\n\nCalculate fences:\n\nLower: 6 - (1.5 \\times 8) = -6\nUpper: 14 + (1.5 \\times 8) = 26\n\nIdentify outliers:\n\n50 &gt; 26, therefore 50 is an outlier\n\n\nGraphical Elements:\n\nBox: Draw from Q_1 to Q_3\nLine inside box: Draw at Q_2\nWhiskers: Extend to most extreme non-outlier points\nPoints: Plot outliers individually beyond whiskers\n\n\n\n\n\n\nPercentyle: Bardziej Precyzyjna Miara Wzglƒôdnej Pozycji (*)\n\nCzym sƒÖ Percentyle?\nPercentyle dajƒÖ nam bardziej szczeg√≥≈Çowy obraz, dzielƒÖc dane na 100 r√≥wnych czƒô≈õci.\nKluczowe Punkty:\n\n25-ty percentyl r√≥wna siƒô Q1\n50-ty percentyl r√≥wna siƒô Q2 (mediana)\n75-ty percentyl r√≥wna siƒô Q3\n\n\n\nObliczanie Percentyli\nWz√≥r: P_k = \\frac{k(n+1)}{100}\nGdzie:\n\nP_k to pozycja dla k-tego percentyla\nk to percentyl, kt√≥ry chcemy znale≈∫ƒá (1-100)\nn to liczba obserwacji\n\nPrzyk≈Çad 3: Znajdowanie 60-tego Percentyla U≈ºyjmy wynik√≥w zada≈Ñ domowych uczni√≥w: 72, 75, 78, 80, 82, 85, 88, 90, 92, 95\nKrok 1: Oblicz pozycjƒô\n\nn = 10 wynik√≥w\nDla 60-tego percentyla: P_{60} = \\frac{60(10+1)}{100} = 6.6\n\nKrok 2: Znajd≈∫ otaczajƒÖce warto≈õci\n\nPozycja 6: wynik 85\nPozycja 7: wynik 88\n\nKrok 3: Interpoluj (wa≈ºne: percentyle u≈ºywajƒÖ interpolacji liniowej)\n\nMusimy przej≈õƒá 0.6 drogi miƒôdzy 85 a 88 P_{60} = 85 + 0.6(88-85) P_{60} = 85 + 0.6(3) P_{60} = 85 + 1.8 = 86.8\n\nCo to oznacza: 60% uczni√≥w uzyska≈Ço wynik 86.8 lub ni≈ºszy.\n\n\n\nRangi Percentylowe (PR) (*)\n\nCzym jest Ranga Percentylowa?\nPodczas gdy percentyle m√≥wiƒÖ nam o warto≈õci na okre≈õlonej pozycji, ranga percentylowa m√≥wi nam, jaki procent warto≈õci znajduje siƒô poni≈ºej okre≈õlonego wyniku. Mo≈ºna to traktowaƒá jako odpowied≈∫ na pytanie ‚ÄúJaki procent klasy uzyska≈Ç wynik ni≈ºszy ni≈º ja?‚Äù\nPR = \\frac{\\text{liczba warto≈õci poni≈ºej } + 0.5 \\times \\text{liczba r√≥wnych warto≈õci}}{\\text{ca≈Çkowita liczba warto≈õci}} \\times 100\nPrzyk≈Çad 4: Znajdowanie Rangi Percentylowej Rozwa≈ºmy te wyniki egzaminu:\n65, 70, 70, 75, 75, 75, 80, 85, 85, 90\nZnajd≈∫my PR dla wyniku 75.\nKrok 1: Dok≈Çadnie policz\n\nWarto≈õci poni≈ºej 75: 65, 70, 70 (3 warto≈õci)\nWarto≈õci r√≥wne 75: 75, 75, 75 (3 warto≈õci)\nCa≈Çkowita liczba warto≈õci: 10\n\nKrok 2: Zastosuj wz√≥r\nPR = \\frac{3 + 0.5(3)}{10} \\times 100 PR = \\frac{3 + 1.5}{10} \\times 100 PR = \\frac{4.5}{10} \\times 100 = 45\\%\nInterpretacja: Wynik 75 jest wy≈ºszy ni≈º 45% wynik√≥w w klasie.\nUwaga:\nP1: ‚ÄúDlaczego u≈ºywamy 0.5 dla r√≥wnych warto≈õci w PR?‚Äù\nO1: Jest tak, poniewa≈º zak≈Çadamy, ≈ºe osoby z tym samym wynikiem sƒÖ r√≥wnomiernie roz≈Ço≈ºone na tej pozycji. To jak powiedzenie, ≈ºe dzielƒÖ pozycjƒô po r√≥wno.\n\n\n\n\n\n\nPodw√≥jna Rola Mediany\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†6.2: Wizualizacja podw√≥jnej roli mediany\n\n\n\n\n\nMediana pe≈Çni dwie odrƒôbne, ale powiƒÖzane ze sobƒÖ role:\nA. Jako Miara Centrum:\n\nReprezentuje ≈õrodkowy punkt danych\nR√≥wnowa≈ºy liczbƒô obserwacji po obu stronach\nJest odporna na warto≈õci odstajƒÖce (w przeciwie≈Ñstwie do ≈õredniej arytmetycznej)\n\nB. Jako Miara Pozycji Wzglƒôdnej:\n\nWyznacza 50-ty percentyl\nDzieli dane na dwie r√≥wne czƒô≈õci\nKa≈ºdƒÖ warto≈õƒá mo≈ºna do niej odnie≈õƒá:\n\nPoni≈ºej mediany: dolne 50%\nPowy≈ºej mediany: g√≥rne 50%\n\n\nTa podw√≥jna natura sprawia, ≈ºe mediana jest szczeg√≥lnie przydatna do:\n\nOpisywania warto≈õci typowych (tendencja centralna)\nZrozumienia pozycji w rozk≈Çadzie (pozycja wzglƒôdna)\nDokonywania por√≥wna≈Ñ miƒôdzy r√≥≈ºnymi zbiorami danych\n\n\n\n\n\n\nWykres pude≈Çkowy\nWykresy pude≈Çkowe (znane r√≥wnie≈º jako wykresy skrzynkowe lub box-and-whisker plots) sƒÖ u≈ºytecznymi narzƒôdziami wizualizacji rozk≈Çad√≥w danych.\n\nKonstrukcja wykresu pude≈Çkowego Tukeya\nWykres pude≈Çkowy zosta≈Ç wprowadzony przez Johna Tukeya jako czƒô≈õƒá jego zestawu narzƒôdzi eksploracyjnej analizy danych. Wykres wizualizuje rozk≈Çad danych na podstawie piƒôciu podstawowych statystyk.\n\nPodsumowanie piƒôciu liczb\nWykres pude≈Çkowy reprezentuje piƒôƒá kluczowych warto≈õci statystycznych:\n\nMinimum: Najmniejsza warto≈õƒá w zbiorze danych (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych)\nPierwszy kwartyl (Q1): 25. percentyl, poni≈ºej kt√≥rego znajduje siƒô 25% obserwacji\nMediana (Q2): 50. percentyl, kt√≥ry dzieli zbi√≥r danych na dwie r√≥wne po≈Çowy\nTrzeci kwartyl (Q3): 75. percentyl, poni≈ºej kt√≥rego znajduje siƒô 75% obserwacji\nMaksimum: Najwiƒôksza warto≈õƒá w zbiorze danych (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych)\n\n\n\nKomponenty wykresu pude≈Çkowego\n\n\n\n\n\n\n\n\nFigure¬†6.3: Diagram wykresu pude≈Çkowego pokazujƒÖcy jego kluczowe komponenty.\n\n\n\n\n\nKomponenty wykresu pude≈Çkowego obejmujƒÖ:\n\nPude≈Çko:\n\nReprezentuje rozstƒôp miƒôdzykwartylowy (IQR), zawierajƒÖcy ≈õrodkowe 50% danych\nDolna krawƒôd≈∫ reprezentuje Q1\nG√≥rna krawƒôd≈∫ reprezentuje Q3\nLinia wewnƒÖtrz pude≈Çka reprezentuje medianƒô (Q2)\n\nWƒÖsy:\n\nRozciƒÖgajƒÖ siƒô od pude≈Çka, aby pokazaƒá zakres danych niebƒôdƒÖcych warto≈õciami odstajƒÖcymi\nW wykresie pude≈Çkowym Tukeya wƒÖsy rozciƒÖgajƒÖ siƒô do 1,5 √ó IQR od krawƒôdzi pude≈Çka:\n\nDolny wƒÖs: rozciƒÖga siƒô do minimalnej warto≈õci ‚â• (Q1 - 1,5 √ó IQR)\nG√≥rny wƒÖs: rozciƒÖga siƒô do maksymalnej warto≈õci ‚â§ (Q3 + 1,5 √ó IQR)\n\n\nWarto≈õci odstajƒÖce:\n\nPunkty, kt√≥re wykraczajƒÖ poza wƒÖsy\nIndywidualnie zaznaczone jako kropki lub inne symbole\nWarto≈õci, kt√≥re sƒÖ &lt; (Q1 - 1,5 √ó IQR) lub &gt; (Q3 + 1,5 √ó IQR)\n\n\n\n\nKluczowe cechy do obserwacji\nInterpretujƒÖc wykresy pude≈Çkowe, zwr√≥ƒá uwagƒô na nastƒôpujƒÖce cechy:\n\nTendencja centralna: Po≈Ço≈ºenie linii mediany wewnƒÖtrz pude≈Çka\nRozproszenie: Szeroko≈õƒá pude≈Çka (IQR) i d≈Çugo≈õƒá wƒÖs√≥w\nSko≈õno≈õƒá:\n\nDane symetryczne: mediana znajduje siƒô w przybli≈ºeniu na ≈õrodku pude≈Çka, wƒÖsy majƒÖ podobnƒÖ d≈Çugo≈õƒá\nSko≈õno≈õƒá prawostronna (dodatnia): mediana jest bli≈ºej dolnej czƒô≈õci pude≈Çka, g√≥rny wƒÖs jest d≈Çu≈ºszy\nSko≈õno≈õƒá lewostronna (ujemna): mediana jest bli≈ºej g√≥rnej czƒô≈õci pude≈Çka, dolny wƒÖs jest d≈Çu≈ºszy\n\nWarto≈õci odstajƒÖce: Obecno≈õƒá pojedynczych punkt√≥w poza wƒÖsami\n\n\n\n\nStudium przypadku: Por√≥wnanie wzrostu miƒôdzy grupami\nZastosujmy nasze zrozumienie wykres√≥w pude≈Çkowych do rzeczywistego zbioru danych. Mamy pomiary wzrostu (w centymetrach) z dw√≥ch grup, ka≈ºda po 25 student√≥w.\n\n# Utworzenie zbioru danych\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Przekszta≈Çcenie zbioru danych z formatu szerokiego na d≈Çugi\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Wy≈õwietlenie pierwszych kilku wierszy\nhead(data_height_l)\n\n  Group_number height\n1      group_1    150\n2      group_1    160\n3      group_1    165\n4      group_1    168\n5      group_1    172\n6      group_1    173\n\n\nObliczmy kilka statystyk podsumowujƒÖcych dla ka≈ºdej grupy:\n\n# Obliczenie statystyk podsumowujƒÖcych dla ka≈ºdej grupy\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Obliczenie IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Utworzenie tabeli por√≥wnawczej\nstats_table &lt;- rbind(\n  group1_stats,\n  group2_stats\n)\nrownames(stats_table) &lt;- c(\"Grupa 1\", \"Grupa 2\")\n\n# Wy≈õwietlenie tabeli\nstats_table\n\n        Min. 1st Qu. Median Mean 3rd Qu. Max.\nGrupa 1  150     175    180  179     183  200\nGrupa 2  138     165    175  172     182  210\n\n# Wy≈õwietlenie warto≈õci IQR\ncat(\"IQR dla Grupy 1:\", group1_iqr, \"\\n\")\n\nIQR dla Grupy 1: 8 \n\ncat(\"IQR dla Grupy 2:\", group2_iqr, \"\\n\")\n\nIQR dla Grupy 2: 17 \n\n\n\n\nWizualizacja danych dotyczƒÖcych wzrostu\nTeraz zwizualizujmy dane za pomocƒÖ wykres√≥w pude≈Çkowych i wykres√≥w gƒôsto≈õci:\n\n# Tworzenie poziomych wykres√≥w pude≈Çkowych\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Rozk≈Çad wzrostu wed≈Çug grupy\",\n       x = \"Grupa\",\n       y = \"Wzrost (cm)\")\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` argument instead.\n‚Ñπ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` argument instead.\n‚Ñπ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\n\n\n\n\n\n\nFigure¬†6.4: Wykresy pude≈Çkowe por√≥wnujƒÖce rozk≈Çady wzrostu miƒôdzy grupami.\n\n\n\n\n\nAby uzupe≈Çniƒá nasze wykresy pude≈Çkowe, przyjrzyjmy siƒô r√≥wnie≈º rozk≈Çadom gƒôsto≈õci:\n\n# Tworzenie wykres√≥w gƒôsto≈õci\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Gƒôsto≈õƒá wzrostu wed≈Çug grupy\",\n       x = \"Wzrost (cm)\",\n       y = \"Gƒôsto≈õƒá\")\n\n\n\n\n\n\n\nFigure¬†6.5: Wykresy gƒôsto≈õci pokazujƒÖce rozk≈Çady wzrostu dla ka≈ºdej grupy.\n\n\n\n\n\n\n\nƒÜwiczenie z interpretacji wykres√≥w pude≈Çkowych\nNa podstawie powy≈ºszych wykres√≥w pude≈Çkowych i wykres√≥w gƒôsto≈õci okre≈õl, czy ka≈ºde z poni≈ºszych stwierdze≈Ñ jest Prawdziwe czy Fa≈Çszywe. Dla ka≈ºdego stwierdzenia podaj kr√≥tkie wyja≈õnienie oparte na dowodach z wizualizacji.\n\n\n\n\n\n\nPytania ƒáwiczeniowe\n\n\n\n\nStudenci z grupy 2 (G2) w badanej pr√≥bie sƒÖ, ≈õrednio, wy≈ºsi ni≈º ci z grupy 1 (G1).\nWzrost w grupie 1 (G1) jest bardziej rozproszony/roz≈Ço≈ºony ni≈º w grupie 2 (G2).\nNajni≈ºsza osoba jest w grupie 2 (G2).\nOba zbiory danych majƒÖ sko≈õno≈õƒá ujemnƒÖ (lewostronnƒÖ).\nPo≈Çowa student√≥w w grupie 2 (G2) ma wzrost co najmniej 175 cm.\n\n\n\n\nWskaz√≥wki do interpretacji\nOdpowiadajƒÖc na te pytania, we≈∫ pod uwagƒô:\n\nPozycjƒô linii mediany w ka≈ºdym pude≈Çku\nWzglƒôdne rozmiary pude≈Çek (IQR)\nPozycje warto≈õci minimalnych i maksymalnych\nSymetriƒô rozk≈Çad√≥w (zr√≥wnowa≈ºone czy z sko≈õno≈õciƒÖ)\nD≈Çugo≈õci wƒÖs√≥w\n\nDla ka≈ºdego stwierdzenia ustal, czy jest Prawdziwe czy Fa≈Çszywe i podaj swoje wyja≈õnienie:\n\n\n\n\n\n\nSzablon odpowiedzi\n\n\n\n\n\n\nStudenci z G2 sƒÖ, ≈õrednio, wy≈ºsi ni≈º z G1: [Prawda/Fa≈Çsz]\n\nWyja≈õnienie:\n\nWzrost G1 jest bardziej rozproszony/roz≈Ço≈ºony: [Prawda/Fa≈Çsz]\n\nWyja≈õnienie:\n\nNajni≈ºsza osoba jest w G2: [Prawda/Fa≈Çsz]\n\nWyja≈õnienie:\n\nOba zbiory danych majƒÖ sko≈õno≈õƒá ujemnƒÖ (lewostronnƒÖ): [Prawda/Fa≈Çsz]\n\nWyja≈õnienie:\n\nPo≈Çowa G2 ma wzrost co najmniej 175 cm: [Prawda/Fa≈Çsz]\n\nWyja≈õnienie:\n\n\n\n\n\nPrzeanalizujmy odpowiedzi na nasze pytania dotyczƒÖce interpretacji wykres√≥w pude≈Çkowych:\n\n\n\n\n\n\nRozwiƒÖzania\n\n\n\n\n\n\nStudenci z G2 sƒÖ, ≈õrednio, wy≈ºsi ni≈º z G1: Fa≈Çsz\n\nWyja≈õnienie: Mediana wzrostu (≈õrodkowa linia w wykresie pude≈Çkowym) dla G1 jest wy≈ºsza ni≈º dla G2.\n\nWzrost G1 jest bardziej rozproszony/roz≈Ço≈ºony: Fa≈Çsz\n\nWyja≈õnienie: G2 wykazuje wiƒôksze rozproszenie. Jest to widoczne na wykresie pude≈Çkowym, gdzie G2 ma wiƒôkszy rozstƒôp miƒôdzykwartylowy (IQR) wynoszƒÖcy 17,5 cm w por√≥wnaniu z 9,5 cm dla G1. G2 ma r√≥wnie≈º szerszy zakres od warto≈õci minimalnej do maksymalnej.\n\nNajni≈ºsza osoba jest w G2: Prawda\n\nWyja≈õnienie: Warto≈õƒá minimalna w G2 wynosi 138 cm, co jest ni≈ºsze ni≈º warto≈õƒá minimalna w G1 (150 cm).\n\nOba zbiory danych majƒÖ sko≈õno≈õƒá ujemnƒÖ (lewostronnƒÖ): Prawda\n\nWyja≈õnienie: W obu grupach linia mediany jest umieszczona w kierunku g√≥rnej czƒô≈õci pude≈Çka, a dolny wƒÖs jest d≈Çu≈ºszy ni≈º g√≥rny. Wskazuje to na d≈Çu≈ºszy ogon po lewej stronie rozk≈Çadu, co oznacza sko≈õno≈õƒá ujemnƒÖ.\n\nPo≈Çowa G2 ma wzrost co najmniej 175 cm: Prawda\n\nWyja≈õnienie: Mediana (≈õrodkowa linia w wykresie pude≈Çkowym) dla G2 wynosi 175 cm, co oznacza, ≈ºe 50% warto≈õci jest wiƒôkszych lub r√≥wnych 175 cm.\n\n\n\n\n\n\n\n\nKod R\n\n# Wczytanie wymaganych pakiet√≥w\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n# Ustawienie opcji wy≈õwietlania\noptions(scipen = 999, digits = 3)\n\n# Utworzenie zbioru danych\ndata_height &lt;- data.frame(\n  group_1 = c(150, 160, 165, 168, 172, 173, 175, 176, 177, 178, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 186, 188, 190, 191, 200),\n  group_2 = c(138, 140, 148, 152, 164, 164, 165, 165, 166, 166, 170, 175, 175, 175, 182, 182, 182, 182, 182, 182, 183, 183, 183, 188, 210)\n)\n\n# Przekszta≈Çcenie zbioru danych z formatu szerokiego na d≈Çugi\ndata_height_l &lt;- gather(data = data_height, key = \"Group_number\", value = \"height\", group_1:group_2)\n\n# Wy≈õwietlenie pierwszych kilku wierszy\nhead(data_height_l)\n\n# Obliczenie statystyk podsumowujƒÖcych dla ka≈ºdej grupy\ngroup1_stats &lt;- summary(data_height$group_1)\ngroup2_stats &lt;- summary(data_height$group_2)\n\n# Obliczenie IQR\ngroup1_iqr &lt;- IQR(data_height$group_1)\ngroup2_iqr &lt;- IQR(data_height$group_2)\n\n# Tworzenie poziomych wykres√≥w pude≈Çkowych\nggplot(data = data_height_l) + \n  geom_boxplot(aes(x = Group_number, y = height, colour = Group_number), notch = FALSE) + \n  coord_flip() + \n  scale_y_continuous(breaks = seq(130, 210, 5)) + \n  theme_pubr() + \n  grids(linetype = \"dashed\") +\n  labs(title = \"Rozk≈Çad wzrostu wed≈Çug grupy\",\n       x = \"Grupa\",\n       y = \"Wzrost (cm)\")\n\n# Tworzenie wykres√≥w gƒôsto≈õci\nggplot(data = data_height_l) + \n  geom_density(aes(x = height, fill = Group_number), alpha = 0.5) + \n  facet_grid(~ Group_number) + \n  scale_x_continuous(breaks = seq(130, 210, 10)) +\n  labs(title = \"Gƒôsto≈õƒá wzrostu wed≈Çug grupy\",\n       x = \"Wzrost (cm)\",\n       y = \"Gƒôsto≈õƒá\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-kszta≈Çtu",
    "href": "rozdzial5.html#miary-kszta≈Çtu",
    "title": "6¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.9 Miary Kszta≈Çtu",
    "text": "6.9 Miary Kszta≈Çtu\n\nSko≈õno≈õƒá\n\nDefinicja\nSko≈õno≈õƒá kwantyfikuje asymetriƒô rozk≈Çadu danych. Wskazuje, czy dane grupujƒÖ siƒô bardziej po jednej stronie ≈õredniej ni≈º po drugiej.\n\n\nWyra≈ºenie Matematyczne\nSK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3 gdzie: - n to wielko≈õƒá pr√≥by - x_i to i-ta obserwacja - \\bar{x} to ≈õrednia z pr√≥by - s to odchylenie standardowe z pr√≥by\n\n\nUproszczony Przyk≈Çad Numeryczny\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Trzy przyk≈Çadowe zestawy danych z r√≥≈ºnymi typami sko≈õno≈õci\n# 1. Sko≈õno≈õƒá dodatnia (prawy ogon)\ndane_skosnosc_dodatnia &lt;- c(2, 3, 4, 4, 5, 5, 5, 6, 6, 7, 8, 12, 15, 20)\n# 2. Sko≈õno≈õƒá ujemna (lewy ogon)\ndane_skosnosc_ujemna &lt;- c(1, 5, 10, 13, 14, 15, 16, 16, 17, 17, 18, 18, 19, 20)\n# 3. Sko≈õno≈õƒá bliska zeru (symetria)\ndane_skosnosc_symetryczna &lt;- c(1, 3, 5, 7, 9, 10, 11, 12, 13, 15, 17, 19, 21)\n\n# Obliczenie sko≈õno≈õci\nskosnosc_dodatnia &lt;- skewness(dane_skosnosc_dodatnia)\nskosnosc_ujemna &lt;- skewness(dane_skosnosc_ujemna)\nskosnosc_symetryczna &lt;- skewness(dane_skosnosc_symetryczna)\n\n# Zestawienie wynik√≥w\ndane_skosnosci &lt;- data.frame(\n  \"Typ rozk≈Çadu\" = c(\"Sko≈õno≈õƒá dodatnia\", \"Sko≈õno≈õƒá ujemna\", \"Rozk≈Çad symetryczny\"),\n  \"Warto≈õƒá sko≈õno≈õci\" = round(c(skosnosc_dodatnia, skosnosc_ujemna, skosnosc_symetryczna), 3),\n  \"Interpretacja\" = c(\n    \"D≈Çu≈ºszy prawy ogon (wiƒôkszo≈õƒá danych po lewej stronie)\",\n    \"D≈Çu≈ºszy lewy ogon (wiƒôkszo≈õƒá danych po prawej stronie)\",\n    \"Dane roz≈Ço≈ºone symetrycznie\"\n  )\n)\n\n# Wy≈õwietlenie tabeli\ndane_skosnosci\n\n         Typ.rozk≈Çadu Warto≈õƒá.sko≈õno≈õci\n1   Sko≈õno≈õƒá dodatnia              1.42\n2     Sko≈õno≈õƒá ujemna             -1.33\n3 Rozk≈Çad symetryczny              0.00\n                                           Interpretacja\n1 D≈Çu≈ºszy prawy ogon (wiƒôkszo≈õƒá danych po lewej stronie)\n2 D≈Çu≈ºszy lewy ogon (wiƒôkszo≈õƒá danych po prawej stronie)\n3                            Dane roz≈Ço≈ºone symetrycznie\n\n\n\n\nWizualizacje Typ√≥w Sko≈õno≈õci\n\n# Tworzymy ramkƒô danych dla wszystkich zestaw√≥w\ndf_skosnosc &lt;- rbind(\n  data.frame(wartosc = dane_skosnosc_dodatnia, typ = \"Sko≈õno≈õƒá dodatnia\", \n             skosnosc = round(skosnosc_dodatnia, 2)),\n  data.frame(wartosc = dane_skosnosc_ujemna, typ = \"Sko≈õno≈õƒá ujemna\", \n             skosnosc = round(skosnosc_ujemna, 2)),\n  data.frame(wartosc = dane_skosnosc_symetryczna, typ = \"Rozk≈Çad symetryczny\", \n             skosnosc = round(skosnosc_symetryczna, 2))\n)\n\n# Histogramy dla trzech typ√≥w sko≈õno≈õci\np1 &lt;- ggplot(df_skosnosc, aes(x = wartosc)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~typ, scales = \"free_x\") +\n  geom_vline(data = df_skosnosc %&gt;% group_by(typ) %&gt;% summarise(mean = mean(wartosc)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_skosnosc %&gt;% group_by(typ) %&gt;% summarise(median = median(wartosc)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = unique(df_skosnosc[, c(\"typ\", \"skosnosc\")]),\n           aes(x = Inf, y = Inf, label = paste(\"SK =\", skosnosc)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histogramy pokazujƒÖce r√≥≈ºne typy sko≈õno≈õci\",\n    subtitle = \"Czerwona linia: ≈õrednia, Zielona linia: mediana\",\n    x = \"Warto≈õƒá\",\n    y = \"Czƒôsto≈õƒá\"\n  ) +\n  theme_minimal()\n\n# Wykresy pude≈Çkowe\np2 &lt;- ggplot(df_skosnosc, aes(x = typ, y = wartosc, fill = typ)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"skyblue\", \"lightgreen\", \"lightsalmon\")) +\n  labs(\n    title = \"Wykresy pude≈Çkowe dla r√≥≈ºnych typ√≥w sko≈õno≈õci\",\n    x = \"Typ rozk≈Çadu\",\n    y = \"Warto≈õƒá\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Wy≈õwietlenie wykres√≥w\ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nPrzyk≈Çad: Analiza Frekwencji Wyborczej\n\n# Generujemy trzy zestawy danych odzwierciedlajƒÖce r√≥≈ºne typy sko≈õno≈õci\nset.seed(123)\n\n# 1. Sko≈õno≈õƒá dodatnia - typowa dla frekwencji w regionach o niskim zaanga≈ºowaniu\nfrekwencja_dodatnia &lt;- c(\n  runif(50, min = 20, max = 30),  # Ma≈Ça grupa z niskƒÖ frekwencjƒÖ\n  rbeta(200, shape1 = 2, shape2 = 5) * 50 + 30  # Wiƒôkszo≈õƒá wynik√≥w przesuniƒôtych w lewo\n)\n\n# 2. Sko≈õno≈õƒá ujemna - typowa dla region√≥w z wysokim zaanga≈ºowaniem politycznym\nfrekwencja_ujemna &lt;- c(\n  rbeta(200, shape1 = 5, shape2 = 2) * 30 + 50,  # Wiƒôkszo≈õƒá wynik√≥w przesuniƒôtych w prawo\n  runif(50, min = 40, max = 50)  # Ma≈Ça grupa z ni≈ºszƒÖ frekwencjƒÖ\n)\n\n# 3. Rozk≈Çad symetryczny - typowy dla region√≥w z r√≥wnomiernym zaanga≈ºowaniem\nfrekwencja_symetryczna &lt;- rnorm(250, mean = 65, sd = 8)\n\n# Tworzymy ramkƒô danych\ndf_frekwencja &lt;- rbind(\n  data.frame(frekwencja = frekwencja_dodatnia, region = \"Region A: Sko≈õno≈õƒá dodatnia\"),\n  data.frame(frekwencja = frekwencja_ujemna, region = \"Region B: Sko≈õno≈õƒá ujemna\"),\n  data.frame(frekwencja = frekwencja_symetryczna, region = \"Region C: Rozk≈Çad symetryczny\")\n)\n\n# Obliczamy sko≈õno≈õƒá dla ka≈ºdego regionu\nskosnosci_regionow &lt;- df_frekwencja %&gt;%\n  group_by(region) %&gt;%\n  summarise(skosnosc = round(skewness(frekwencja), 2))\n\n# Histogram frekwencji wed≈Çug region√≥w\np3 &lt;- ggplot(df_frekwencja, aes(x = frekwencja)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"darkblue\", alpha = 0.7) +\n  facet_wrap(~region, ncol = 1) +\n  geom_vline(data = df_frekwencja %&gt;% group_by(region) %&gt;% summarise(mean = mean(frekwencja)),\n            aes(xintercept = mean), color = \"red\", linetype = \"dashed\") +\n  geom_vline(data = df_frekwencja %&gt;% group_by(region) %&gt;% summarise(median = median(frekwencja)),\n            aes(xintercept = median), color = \"darkgreen\", linetype = \"dashed\") +\n  geom_text(data = skosnosci_regionow,\n           aes(x = 25, y = 20, label = paste(\"SK =\", skosnosc)),\n           size = 3.5) +\n  labs(\n    title = \"Frekwencja wyborcza w r√≥≈ºnych regionach\",\n    subtitle = \"Pokazuje trzy rodzaje sko≈õno≈õci\",\n    x = \"Frekwencja wyborcza (%)\",\n    y = \"Liczba obwod√≥w\"\n  ) +\n  theme_minimal()\n\n# Wykres pude≈Çkowy\np4 &lt;- ggplot(df_frekwencja, aes(x = region, y = frekwencja, fill = region)) +\n  geom_boxplot() +\n  labs(\n    title = \"Por√≥wnanie rozk≈Çad√≥w frekwencji w regionach\",\n    x = \"Region\",\n    y = \"Frekwencja wyborcza (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Wy≈õwietlenie wykres√≥w\ngrid.arrange(p3, p4, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nPrzewodnik Interpretacji\n\nSko≈õno≈õƒá Dodatnia (&gt; 0): Rozk≈Çad ma d≈Çu≈ºszy ogon prawy - wiƒôkszo≈õƒá warto≈õci jest skupiona po lewej stronie\nSko≈õno≈õƒá Ujemna (&lt; 0): Rozk≈Çad ma d≈Çu≈ºszy ogon lewy - wiƒôkszo≈õƒá warto≈õci jest skupiona po prawej stronie\nSko≈õno≈õƒá Zero: Rozk≈Çad w przybli≈ºeniu symetryczny - warto≈õci roz≈Ço≈ºone r√≥wnomiernie wok√≥≈Ç ≈õredniej\n\n\n\n\nKurtoza\n\nDefinicja\nKurtoza mierzy ‚Äúogoniasto≈õƒá‚Äù rozk≈Çadu, wskazujƒÖc na obecno≈õƒá warto≈õci ekstremalnych w por√≥wnaniu z rozk≈Çadem normalnym.\n\n\nWyra≈ºenie Matematyczne\nK = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\n\n\nUproszczony Przyk≈Çad Numeryczny\n\n# Trzy przyk≈Çadowe zestawy danych z r√≥≈ºnymi poziomami kurtozy\n# 1. Rozk≈Çad leptokurtyczny (wysoka kurtoza, \"ciƒô≈ºkie ogony\")\ndane_leptokurtyczne &lt;- c(\n  rnorm(80, mean = 50, sd = 5),  # Wiƒôkszo≈õƒá danych skupiona wok√≥≈Ç ≈õredniej\n  c(20, 25, 30, 70, 75, 80)      # Kilka warto≈õci ekstremalnych\n)\n\n# 2. Rozk≈Çad platykurtyczny (niska kurtoza, \"p≈Çaski\")\ndane_platykurtyczne &lt;- c(\n  runif(50, min = 30, max = 70)  # R√≥wnomierny rozk≈Çad warto≈õci\n)\n\n# 3. Rozk≈Çad mezokurtyczny (normalna kurtoza)\ndane_mezokurtyczne &lt;- rnorm(50, mean = 50, sd = 10)\n\n# Obliczenie kurtozy\nkurtoza_lepto &lt;- kurtosis(dane_leptokurtyczne)\nkurtoza_platy &lt;- kurtosis(dane_platykurtyczne)\nkurtoza_mezo &lt;- kurtosis(dane_mezokurtyczne)\n\n# Zestawienie wynik√≥w\ndane_kurtozy &lt;- data.frame(\n  \"Typ rozk≈Çadu\" = c(\"Leptokurtyczny\", \"Platykurtyczny\", \"Mezokurtyczny\"),\n  \"Warto≈õƒá kurtozy\" = round(c(kurtoza_lepto, kurtoza_platy, kurtoza_mezo), 3),\n  \"Interpretacja\" = c(\n    \"Wiele warto≈õci blisko ≈õredniej, ale te≈º wiƒôcej warto≈õci ekstremalnych\",\n    \"Warto≈õci roz≈Ço≈ºone bardziej r√≥wnomiernie - p≈Çaski rozk≈Çad\",\n    \"Podobny do rozk≈Çadu normalnego\"\n  )\n)\n\n# Wy≈õwietlenie tabeli\ndane_kurtozy\n\n    Typ.rozk≈Çadu Warto≈õƒá.kurtozy\n1 Leptokurtyczny            7.39\n2 Platykurtyczny            1.85\n3  Mezokurtyczny            2.25\n                                                          Interpretacja\n1 Wiele warto≈õci blisko ≈õredniej, ale te≈º wiƒôcej warto≈õci ekstremalnych\n2             Warto≈õci roz≈Ço≈ºone bardziej r√≥wnomiernie - p≈Çaski rozk≈Çad\n3                                        Podobny do rozk≈Çadu normalnego\n\n\n\n\nWizualizacje Poziom√≥w Kurtozy\n\n# Tworzymy ramkƒô danych dla wszystkich zestaw√≥w\ndf_kurtoza &lt;- rbind(\n  data.frame(wartosc = dane_leptokurtyczne, typ = \"Leptokurtyczny (K &gt; 3)\", \n             kurtoza = round(kurtoza_lepto, 2)),\n  data.frame(wartosc = dane_platykurtyczne, typ = \"Platykurtyczny (K &lt; 3)\", \n             kurtoza = round(kurtoza_platy, 2)),\n  data.frame(wartosc = dane_mezokurtyczne, typ = \"Mezokurtyczny (K ‚âà 3)\", \n             kurtoza = round(kurtoza_mezo, 2))\n)\n\n# Histogramy dla trzech typ√≥w kurtozy\np5 &lt;- ggplot(df_kurtoza, aes(x = wartosc)) +\n  geom_histogram(bins = 15, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~typ, scales = \"free_y\") +\n  geom_text(data = unique(df_kurtoza[, c(\"typ\", \"kurtoza\")]),\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtoza)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Histogramy pokazujƒÖce r√≥≈ºne poziomy kurtozy\",\n    x = \"Warto≈õƒá\",\n    y = \"Czƒôsto≈õƒá\"\n  ) +\n  theme_minimal()\n\n# Wykresy pude≈Çkowe\np6 &lt;- ggplot(df_kurtoza, aes(x = typ, y = wartosc, fill = typ)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"lightgreen\", \"lightsalmon\", \"skyblue\")) +\n  labs(\n    title = \"Wykresy pude≈Çkowe dla r√≥≈ºnych poziom√≥w kurtozy\",\n    x = \"Typ rozk≈Çadu\",\n    y = \"Warto≈õƒá\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Wy≈õwietlenie wykres√≥w\ngrid.arrange(p5, p6, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nPrzyk≈Çad: Analiza G≈Çosowa≈Ñ Parlamentarnych\n\n# Generujemy trzy zestawy danych odzwierciedlajƒÖce r√≥≈ºne poziomy kurtozy\nset.seed(456)\n\n# 1. Rozk≈Çad leptokurtyczny - typowy dla g≈Çosowa≈Ñ z silnƒÖ dyscyplinƒÖ partyjnƒÖ\nglosowania_lepto &lt;- c(\n  rnorm(150, mean = 75, sd = 3),  # Wiƒôkszo≈õƒá g≈Çosowa≈Ñ z wysokƒÖ zgodno≈õciƒÖ\n  c(20, 25, 30, 35, 40, 95, 96, 97, 98, 99)  # Kilka g≈Çosowa≈Ñ odstajƒÖcych\n)\n\n# 2. Rozk≈Çad platykurtyczny - typowy dla g≈Çosowa≈Ñ kontrowersyjnych\nglosowania_platy &lt;- c(\n  runif(80, min = 40, max = 60),  # G≈Çosowania z umiarkowanƒÖ zgodno≈õciƒÖ\n  runif(80, min = 60, max = 80)   # G≈Çosowania z wy≈ºszƒÖ zgodno≈õciƒÖ\n)\n\n# 3. Rozk≈Çad mezokurtyczny - typowy dla normalnych g≈Çosowa≈Ñ\nglosowania_mezo &lt;- rnorm(160, mean = 65, sd = 10)\n\n# Tworzymy ramkƒô danych\ndf_glosowania &lt;- rbind(\n  data.frame(zgodnosc = glosowania_lepto, typ_ustawy = \"Ustawy A: Leptokurtyczne\"),\n  data.frame(zgodnosc = glosowania_platy, typ_ustawy = \"Ustawy B: Platykurtyczne\"),\n  data.frame(zgodnosc = glosowania_mezo, typ_ustawy = \"Ustawy C: Mezokurtyczne\")\n)\n\n# Obliczamy kurtozƒô dla ka≈ºdego typu ustaw\nkurtozy_ustaw &lt;- df_glosowania %&gt;%\n  group_by(typ_ustawy) %&gt;%\n  summarise(kurtoza = round(kurtosis(zgodnosc), 2))\n\n# Histogram zgodno≈õci g≈Çosowa≈Ñ\np7 &lt;- ggplot(df_glosowania, aes(x = zgodnosc)) +\n  geom_histogram(bins = 20, fill = \"lightgreen\", color = \"darkgreen\", alpha = 0.7) +\n  facet_wrap(~typ_ustawy, ncol = 1) +\n  geom_text(data = kurtozy_ustaw,\n           aes(x = Inf, y = Inf, label = paste(\"K =\", kurtoza)),\n           hjust = 1.1, vjust = 1.5, size = 3.5) +\n  labs(\n    title = \"Zgodno≈õƒá g≈Çosowa≈Ñ dla r√≥≈ºnych typ√≥w ustaw\",\n    subtitle = \"Pokazuje trzy poziomy kurtozy\",\n    x = \"Wska≈∫nik zgodno≈õci g≈Çosowa≈Ñ (%)\",\n    y = \"Liczba g≈Çosowa≈Ñ\"\n  ) +\n  theme_minimal()\n\n# Wykres pude≈Çkowy\np8 &lt;- ggplot(df_glosowania, aes(x = typ_ustawy, y = zgodnosc, fill = typ_ustawy)) +\n  geom_boxplot() +\n  labs(\n    title = \"Por√≥wnanie rozk≈Çad√≥w zgodno≈õci g≈Çosowa≈Ñ\",\n    x = \"Typ ustawy\",\n    y = \"Wska≈∫nik zgodno≈õci g≈Çosowa≈Ñ (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Wy≈õwietlenie wykres√≥w\ngrid.arrange(p7, p8, ncol = 2, widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\nPrzewodnik Interpretacji\n\nLeptokurtyczny (K &gt; 3): ‚ÄúWysmuk≈Çy‚Äù rozk≈Çad z ciƒô≈ºkimi ogonami - wiƒôcej warto≈õci skrajnych ni≈º w rozk≈Çadzie normalnym\nPlatykurtyczny (K &lt; 3): ‚ÄúP≈Çaski‚Äù rozk≈Çad - mniej warto≈õci skrajnych ni≈º w rozk≈Çadzie normalnym\nMezokurtyczny (K ‚âà 3): Rozk≈Çad podobny do normalnego pod wzglƒôdem warto≈õci ekstremalnych",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ƒáwiczenie-1.-por√≥wnanie-wynagrodze≈Ñ",
    "href": "rozdzial5.html#ƒáwiczenie-1.-por√≥wnanie-wynagrodze≈Ñ",
    "title": "6¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.10 ƒÜwiczenie 1. Por√≥wnanie wynagrodze≈Ñ",
    "text": "6.10 ƒÜwiczenie 1. Por√≥wnanie wynagrodze≈Ñ\n\nDane\nMamy dane o wynagrodzeniach (w tysiƒÖcach euro) z dw√≥ch ma≈Çych firm europejskich:\n\n\n\nIndex\nFirma X\nFirma Y\n\n\n\n\n1\n2\n3\n\n\n2\n2\n3\n\n\n3\n2\n4\n\n\n4\n3\n4\n\n\n5\n3\n4\n\n\n6\n3\n4\n\n\n7\n3\n4\n\n\n8\n3\n4\n\n\n9\n3\n5\n\n\n10\n4\n5\n\n\n11\n4\n5\n\n\n12\n4\n5\n\n\n13\n4\n5\n\n\n14\n4\n5\n\n\n15\n5\n6\n\n\n16\n5\n6\n\n\n17\n5\n6\n\n\n18\n5\n7\n\n\n19\n20\n7\n\n\n20\n35\n8\n\n\n\n\n\nMiary tendencji centralnej\n\n≈örednia arytmetyczna\n≈örednia arytmetyczna to suma wszystkich warto≈õci podzielona przez ich liczbƒô.\nWz√≥r: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\nMo≈ºna te≈º zapisaƒá ten wz√≥r w postaci:\n\\bar{x} = \\frac{\\sum_{i=1}^{k} x_i f_i}{n}\ngdzie f_i to czƒôsto≈õƒá bezwzglƒôdna (liczba wystƒÖpie≈Ñ, waga bezwzglƒôdna) i-tej warto≈õci, a k to liczba r√≥≈ºnych warto≈õci cechy (liczba warto≈õci wyr√≥≈ºnionych).\nZ u≈ºyciem czƒôsto≈õci wzglƒôdnych:\n\\bar{x} = \\sum_{i=1}^{k} x_i p_i\ngdzie p_i to czƒôsto≈õƒá wzglƒôdna (frakcja, waga znormalizowana) i-tej warto≈õci, a k to liczba r√≥≈ºnych warto≈õci cechy (liczba warto≈õci wyr√≥≈ºnionych).\n\nObliczenia rƒôczne dla Firmy X\n\n\n\nWarto≈õƒá (x_i)\nCzƒôsto≈õƒá (f_i)\nx_i \\cdot f_i\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nSuma\nn = 20\nSuma = 119\n\n\n\n\\bar{x} = \\frac{119}{20} = 5,95\n\n\nObliczenia rƒôczne dla Firmy Y\n\n\n\nWarto≈õƒá (x_i)\nCzƒôsto≈õƒá (f_i)\nx_i \\cdot f_i\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nSuma\nn = 20\nSuma = 100\n\n\n\n\\bar{y} = \\frac{100}{20} = 5\n\n\nWeryfikacja w R\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\nMediana\nMediana to warto≈õƒá ≈õrodkowa w uporzƒÖdkowanym zbiorze danych.\n\nObliczenia rƒôczne dla Firmy X\nUporzƒÖdkowane dane: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\nn = 20 (parzyste), wiƒôc bierzemy ≈õredniƒÖ z 10. i 11. warto≈õci:\nMediana = \\frac{4 + 4}{2} = 4\n\n\nObliczenia rƒôczne dla Firmy Y\nUporzƒÖdkowane dane: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\nn = 20 (parzyste), wiƒôc bierzemy ≈õredniƒÖ z 10. i 11. warto≈õci:\nMediana = \\frac{5 + 5}{2} = 5\n\n\nWeryfikacja w R\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\nDominanta (moda)\nDominanta to najczƒô≈õciej wystƒôpujƒÖca warto≈õƒá w zbiorze danych.\nDla Firmy X dominanta wynosi 3 (wystƒôpuje 6 razy). Dla Firmy Y sƒÖ dwie dominanty: 4 i 5 (obie wystƒôpujƒÖ 6 razy).\n\n# Funkcja do obliczania dominanty\nznajdz_dominante &lt;- function(x) {\n  unikalne_x &lt;- unique(x)\n  unikalne_x[which.max(tabulate(match(x, unikalne_x)))]\n}\n\nznajdz_dominante(X)\n\n[1] 3\n\nznajdz_dominante(Y)\n\n[1] 4\n\n\n\n\n\nMiary rozproszenia\n\nWariancja\nWariancja mierzy ≈õrednie kwadratowe odchylenie od ≈õredniej.\nWz√≥r: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\nPoprawka Bessela jest stosowana przy obliczaniu wariancji z pr√≥by, aby uzyskaƒá nieobciƒÖ≈ºony estymator wariancji populacji. W standardowym wzorze na wariancjƒô z pr√≥by dzielimy przez (n-1) zamiast przez n.\nModyfikacje wzoru dla danych pogrupowanych (szereg czƒôsto≈õci):\nMo≈ºna te≈º zapisaƒá ten wz√≥r w postaci:\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{k} f_i(x_i - \\bar{x})^2\ngdzie f_i to czƒôsto≈õƒá bezwzglƒôdna (liczba wystƒÖpie≈Ñ) i-tej warto≈õci.\nGdy w obliczeniach stosujemy czƒôsto≈õci wzglƒôdne p = f_i/n, gdzie:\n\nf_i to czƒôsto≈õƒá (liczba wystƒÖpie≈Ñ)\nn to ca≈Çkowita liczebno≈õƒá pr√≥by\n\nWz√≥r na wariancjƒô z uwzglƒôdnieniem poprawki Bessela przyjmuje postaƒá:\ns^2 = \\frac{n}{n-1} \\sum_{i=1}^{k} p_i(x_i - \\bar{x})^2\ngdzie:\n\ns^2 to wariancja z pr√≥by\nn to liczebno≈õƒá pr√≥by\np_i = f_i/n to czƒôsto≈õƒá wzglƒôdna i-tej warto≈õci\nx_i to i-ta warto≈õƒá cechy\n\\bar{x} to ≈õrednia arytmetyczna\nk to liczba r√≥≈ºnych warto≈õci cechy\n\nKluczowe jest to, ≈ºe przy stosowaniu czƒôsto≈õci wzglƒôdnych mno≈ºymy ca≈Çe wyra≈ºenie przez czynnik \\frac{n}{n-1}, kt√≥ry wprowadza poprawkƒô Bessela.\n\nObliczenia rƒôczne dla Firmy X\n\n\n\n\n\n\n\n\n\n\nx_i\nf_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\nf_i(x_i - \\bar{x})^2\n\n\n\n\n2\n3\n-3,95\n15,6025\n46,8075\n\n\n3\n6\n-2,95\n8,7025\n52,215\n\n\n4\n5\n-1,95\n3,8025\n19,0125\n\n\n5\n4\n-0,95\n0,9025\n3,61\n\n\n20\n1\n14,05\n197,4025\n197,4025\n\n\n35\n1\n29,05\n843,9025\n843,9025\n\n\nSuma\n20\n\n\n1162,95\n\n\n\ns^2 = \\frac{1162,95}{19} = 61,21\n\n\nObliczenia rƒôczne dla Firmy Y\n\n\n\n\n\n\n\n\n\n\ny_i\nf_i\ny_i - \\bar{x}\n(y_i - \\bar{y})^2\nf_i(y_i - \\bar{y})^2\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nSuma\n20\n\n\n34\n\n\n\ns^2 = \\frac{34}{19} = 1,79\n\n\nWeryfikacja w R\n\nvar(X)\n\n[1] 61.2\n\nvar(Y)\n\n[1] 1.79\n\n\n\n\n\nOdchylenie standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji.\nWz√≥r: s = \\sqrt{s^2}\n\nDla Firmy X: s = \\sqrt{61,21} = 7,82\nDla Firmy Y: s = \\sqrt{1,79} = 1,34\n\n\nWeryfikacja w R\n\nsd(X)\n\n[1] 7.82\n\nsd(Y)\n\n[1] 1.34\n\n\n\n\n\n\nKwartyle\nKwartyle dzielƒÖ zbi√≥r danych na cztery r√≥wne czƒô≈õci.\n\nObliczenia rƒôczne dla Firmy X\nUporzƒÖdkowane dane: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 3\nQ2 (50. percentyl, mediana): 4\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 5\n\n\n\nObliczenia rƒôczne dla Firmy Y\nUporzƒÖdkowane dane: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 4\nQ2 (50. percentyl, mediana): 5\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 6\n\n\n\nWeryfikacja w R\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\nIQR\n\nIQR_x = 5 - 3 = 2\nIQR_y = 6 - 4 = 2\n\n\n\n\nWykres pude≈Çkowy Tukeya\nWykres pude≈Çkowy Tukeya wizualnie przedstawia rozk≈Çad danych na podstawie kwartyli. U≈ºyjemy biblioteki ggplot2 do stworzenia wykresu.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Przygotowanie danych\ndane &lt;- data.frame(\n  Firma = rep(c(\"X\", \"Y\"), each = 20),\n  Wynagrodzenie = c(X, Y)\n)\n\n# Tworzenie wykresu pude≈Çkowego\nggplot(dane, aes(x = Firma, y = Wynagrodzenie, fill = Firma)) +\n  geom_boxplot() +\n  labs(title = \"Rozk≈Çad wynagrodze≈Ñ w firmach X i Y\",\n       x = \"Firma\",\n       y = \"Wynagrodzenie (tysiƒÖce euro)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n# Tworzenie wykresu pude≈Çkowego\nggplot(dane, aes(x = Firma, y = Wynagrodzenie, fill = Firma)) +\n  geom_boxplot(outliers = F) +\n  labs(title = \"Rozk≈Çad wynagrodze≈Ñ w firmach X i Y\",\n       x = \"Firma\",\n       y = \"Wynagrodzenie (tysiƒÖce euro)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\nInterpretacja wykresu pude≈Çkowego\n\nPude≈Çko reprezentuje rozstƒôp miƒôdzykwartylowy (IQR) od Q1 do Q3.\nLinia wewnƒÖtrz pude≈Çka to mediana (Q2).\nWƒÖsy rozciƒÖgajƒÖ siƒô do najmniejszych i najwiƒôkszych warto≈õci w granicach 1,5 * IQR.\nPunkty poza wƒÖsami sƒÖ uznawane za warto≈õci odstajƒÖce.\n\n\n\n\nPor√≥wnanie wynik√≥w\n\n\n\nMiara\nFirma X\nFirma Y\n\n\n\n\n≈örednia\n5,95\n5,00\n\n\nMediana\n4\n5\n\n\nDominanta\n3\n4 i 5\n\n\nWariancja\n61,21\n1,79\n\n\nOdchylenie standard.\n7,82\n1,34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\nKluczowe obserwacje:\n\nTendencja centralna: Firma X ma wy≈ºszƒÖ ≈õredniƒÖ, ale ni≈ºszƒÖ medianƒô ni≈º Firma Y, co wskazuje na prawostronnie sko≈õny rozk≈Çad dla Firmy X.\n\nRozproszenie: Firma X wykazuje znacznie wy≈ºszƒÖ wariancjƒô i odchylenie standardowe, sugerujƒÖc wiƒôksze dysproporcje w wynagrodzeniach.\nKszta≈Çt rozk≈Çadu: Wynagrodzenia w Firmie Y sƒÖ bardziej skupione, podczas gdy Firma X ma warto≈õci ekstremalne (potencjalne warto≈õci odstajƒÖce), kt√≥re znaczƒÖco wp≈ÇywajƒÖ na jej ≈õredniƒÖ i wariancjƒô.\nKwartyle: Rozstƒôp miƒôdzykwartylowy (Q3 - Q1) Firmy Y jest nieznacznie wiƒôkszy, ale jej og√≥lny zakres jest znacznie mniejszy ni≈º Firmy X.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ƒáwiczenie-2.-por√≥wnanie-zmienno≈õci-wielko≈õci-okrƒôg√≥w-wyborczych-miƒôdzy-krajami",
    "href": "rozdzial5.html#ƒáwiczenie-2.-por√≥wnanie-zmienno≈õci-wielko≈õci-okrƒôg√≥w-wyborczych-miƒôdzy-krajami",
    "title": "6¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.11 ƒÜwiczenie 2. Por√≥wnanie Zmienno≈õci Wielko≈õci Okrƒôg√≥w Wyborczych Miƒôdzy Krajami",
    "text": "6.11 ƒÜwiczenie 2. Por√≥wnanie Zmienno≈õci Wielko≈õci Okrƒôg√≥w Wyborczych Miƒôdzy Krajami\n\nDane\nMamy dane o wielko≈õci okrƒôg√≥w wyborczych z dw√≥ch kraj√≥w:\n\nx &lt;- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)  # Kraj wysoka zmienno≈õƒá\ny &lt;- c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13)  # Kraj niska zmienno≈õƒá\n\nkable(data.frame(\n  \"Kraj X (Wysoka zm.)\" = x,\n  \"Kraj Y (Niska zm.)\" = y\n))\n\n\n\n\nKraj.X..Wysoka.zm..\nKraj.Y..Niska.zm..\n\n\n\n\n1\n8\n\n\n3\n9\n\n\n5\n9\n\n\n7\n10\n\n\n9\n10\n\n\n11\n11\n\n\n13\n11\n\n\n15\n12\n\n\n17\n12\n\n\n19\n13\n\n\n\n\n\n\n\nMiary Tendencji Centralnej\n\n≈örednia Arytmetyczna\nWz√≥r: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\nObliczenia dla Kraju X\n\n\n\nElement\nWarto≈õƒá\n\n\n\n\n1\n1\n\n\n2\n3\n\n\n3\n5\n\n\n4\n7\n\n\n5\n9\n\n\n6\n11\n\n\n7\n13\n\n\n8\n15\n\n\n9\n17\n\n\n10\n19\n\n\nSuma\n100\n\n\n\n\\bar{x} = \\frac{100}{10} = 10\n\nmean_x &lt;- mean(x)\nc(\"Rƒôcznie\" = 10, \"R\" = mean_x)\n\nRƒôcznie       R \n     10      10 \n\n\n\n\nObliczenia dla Kraju Y\n\n\n\nElement\nWarto≈õƒá\n\n\n\n\n1\n8\n\n\n2\n9\n\n\n3\n9\n\n\n4\n10\n\n\n5\n10\n\n\n6\n11\n\n\n7\n11\n\n\n8\n12\n\n\n9\n12\n\n\n10\n13\n\n\nSuma\n105\n\n\n\n\\bar{y} = \\frac{105}{10} = 10,5\n\nmean_y &lt;- mean(y)\nc(\"Rƒôcznie\" = 10.5, \"R\" = mean_y)\n\nRƒôcznie       R \n   10.5    10.5 \n\n\n\n\n\nMediana\nMediana to warto≈õƒá ≈õrodkowa w uporzƒÖdkowanym zbiorze danych.\n\nObliczenia dla Kraju X\nUporzƒÖdkowane dane: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nDla n = 10 (parzysta liczba obserwacji): Pozycje ≈õrodkowe: 5 i 6 Warto≈õci ≈õrodkowe: 9 i 11\nMediana = \\frac{9 + 11}{2} = 10\n\nmedian_x &lt;- median(x)\nc(\"Rƒôcznie\" = 10, \"R\" = median_x)\n\nRƒôcznie       R \n     10      10 \n\n\n\n\nObliczenia dla Kraju Y\nUporzƒÖdkowane dane: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nDla n = 10 (parzysta liczba obserwacji): Pozycje ≈õrodkowe: 5 i 6 Warto≈õci ≈õrodkowe: 10 i 11\nMediana = \\frac{10 + 11}{2} = 10,5\n\nmedian_y &lt;- median(y)\nc(\"Rƒôcznie\" = 10.5, \"R\" = median_y)\n\nRƒôcznie       R \n   10.5    10.5 \n\n\n\n\n\nDominanta\n\nObliczenia dla Kraju X\n\n\n\nWarto≈õƒá\nCzƒôsto≈õƒá\n\n\n\n\n1\n1\n\n\n3\n1\n\n\n5\n1\n\n\n7\n1\n\n\n9\n1\n\n\n11\n1\n\n\n13\n1\n\n\n15\n1\n\n\n17\n1\n\n\n19\n1\n\n\n\nWniosek: Brak dominanty (wszystkie warto≈õci wystƒôpujƒÖ jednokrotnie)\n\n\nObliczenia dla Kraju Y\n\n\n\nWarto≈õƒá\nCzƒôsto≈õƒá\n\n\n\n\n8\n1\n\n\n9\n2\n\n\n10\n2\n\n\n11\n2\n\n\n12\n2\n\n\n13\n1\n\n\n\nWniosek: Cztery dominanty: 9, 10, 11, 12 (ka≈ºda wystƒôpuje dwukrotnie)\n\n# Tabele czƒôsto≈õci\ntable_x &lt;- table(x)\ntable_y &lt;- table(y)\n\nlist(\n  \"Kraj X\" = table_x,\n  \"Kraj Y\" = table_y\n)\n\n$`Kraj X`\nx\n 1  3  5  7  9 11 13 15 17 19 \n 1  1  1  1  1  1  1  1  1  1 \n\n$`Kraj Y`\ny\n 8  9 10 11 12 13 \n 1  2  2  2  2  1 \n\n\n\n\n\nWariancja\nWariancja mierzy ≈õrednie kwadratowe odchylenie od ≈õredniej.\nWz√≥r: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\nObliczenia dla Kraju X\n\n\n\nx_i\n(x_i - \\bar{x})\n(x_i - \\bar{x})^2\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSuma\n\n330\n\n\n\ns^2_X = \\frac{330}{9} = 36,67\n\nvar_x &lt;- var(x)\nc(\"Rƒôcznie\" = 36.67, \"R\" = var_x)\n\nRƒôcznie       R \n  36.67   36.67 \n\n\n\n\nObliczenia dla Kraju Y\n\n\n\nx_i\n(y_i - \\bar{y})\n(y_i - \\bar{y})^2\n\n\n\n\n8\n-2,5\n6,25\n\n\n9\n-1,5\n2,25\n\n\n9\n-1,5\n2,25\n\n\n10\n-0,5\n0,25\n\n\n10\n-0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n12\n1,5\n2,25\n\n\n12\n1,5\n2,25\n\n\n13\n2,5\n6,25\n\n\nSuma\n\n22,5\n\n\n\ns^2_Y = \\frac{22,5}{9} = 2,5\n\nvar_y &lt;- var(y)\nc(\"Rƒôcznie\" = 2.5, \"R\" = var_y)\n\nRƒôcznie       R \n    2.5     2.5 \n\n\n\n\n\nOdchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji. Jest miarƒÖ zmienno≈õci wyra≈ºonƒÖ w tych samych jednostkach co dane.\nWz√≥r: s = \\sqrt{s^2}\n\nObliczenia dla Kraju X\nWykorzystujemy wcze≈õniej obliczonƒÖ wariancjƒô: s^2_X = 36,67\nObliczamy pierwiastek: s_X = \\sqrt{36,67} \\approx 6,06\n\n\n\nKrok\nObliczenie\nWynik\n\n\n\n\n1. Wariancja\ns^2_X\n36,67\n\n\n2. Pierwiastek\n\\sqrt{36,67}\n6,06\n\n\n\n\nsd_x &lt;- sd(x)\nc(\"Rƒôcznie\" = 6.06, \"R\" = sd_x)\n\nRƒôcznie       R \n  6.060   6.055 \n\n\n\n\nObliczenia dla Kraju Y\nWykorzystujemy wcze≈õniej obliczonƒÖ wariancjƒô: s^2_Y = 2,5\nObliczamy pierwiastek: s_Y = \\sqrt{2,5} \\approx 1,58\n\n\n\nKrok\nObliczenie\nWynik\n\n\n\n\n1. Wariancja\ns^2_Y\n2,5\n\n\n2. Pierwiastek\n\\sqrt{2,5}\n1,58\n\n\n\n\nsd_y &lt;- sd(y)\nc(\"Rƒôcznie\" = 1.58, \"R\" = sd_y)\n\nRƒôcznie       R \n  1.580   1.581 \n\n\nInterpretacja:\n\nKraj X: Przeciƒôtne odchylenie wielko≈õci okrƒôgu od ≈õredniej wynosi oko≈Ço 6 mandat√≥w\nKraj Y: Przeciƒôtne odchylenie wielko≈õci okrƒôgu od ≈õredniej wynosi oko≈Ço 1,6 mandatu\n\n\n\n\n\nWsp√≥≈Çczynnik Zmienno≈õci (CV)\nWsp√≥≈Çczynnik zmienno≈õci to stosunek odchylenia standardowego do ≈õredniej, wyra≈ºony w procentach.\nWz√≥r: CV = \\frac{s}{\\bar{x}} \\times 100\\%\n\nObliczenia dla Kraju X\nCV_X = \\frac{6,06}{10} \\times 100\\% = 60,6\\%\n\n\n\nSk≈Çadowa\nWarto≈õƒá\n\n\n\n\nOdchylenie standardowe (s)\n6,06\n\n\n≈örednia (\\bar{x})\n10\n\n\nCV\n60,6%\n\n\n\n\ncv_x &lt;- sd(x) / mean(x) * 100\nc(\"Rƒôcznie\" = 60.6, \"R\" = cv_x)\n\nRƒôcznie       R \n  60.60   60.55 \n\n\n\n\nObliczenia dla Kraju Y\nCV_Y = \\frac{1,58}{10,5} \\times 100\\% = 15,0\\%\n\n\n\nSk≈Çadowa\nWarto≈õƒá\n\n\n\n\nOdchylenie standardowe (s)\n1,58\n\n\n≈örednia (\\bar{x})\n10,5\n\n\nCV\n15,0%\n\n\n\n\ncv_y &lt;- sd(y) / mean(y) * 100\nc(\"Rƒôcznie\" = 15.0, \"R\" = cv_y)\n\nRƒôcznie       R \n  15.00   15.06 \n\n\n\n\n\nKwartyle i Rozstƒôp Miƒôdzykwartylowy (IQR)\n\nMetody obliczania kwartyli\nIstniejƒÖ r√≥≈ºne metody obliczania kwartyli. W naszych obliczeniach rƒôcznych zastosujemy metodƒô wy≈ÇƒÖczajƒÖcƒÖ medianƒô:\n\nDzielimy szereg na dwie czƒô≈õci wzglƒôdem mediany\nMediana nie jest uwzglƒôdniana w obliczeniach kwartyli\nDla ka≈ºdej czƒô≈õci obliczamy jej medianƒô - bƒôdzie to odpowiednio Q1 i Q3\n\n\n\nObliczenia dla Kraju X\nUporzƒÖdkowane dane: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMediana = 10 (nie uwzglƒôdniamy w obliczeniach kwartyli)\nDolna po≈Çowa: 1, 3, 5, 7, 9 Q1 = mediana dolnej po≈Çowy = 5\nG√≥rna po≈Çowa: 11, 13, 15, 17, 19 Q3 = mediana g√≥rnej po≈Çowy = 15\nIQR = Q3 - Q1 = 15 - 5 = 10\n\n\nObliczenia dla Kraju Y\nUporzƒÖdkowane dane: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMediana = 10.5 (nie uwzglƒôdniamy w obliczeniach kwartyli)\nDolna po≈Çowa: 8, 9, 9, 10, 10 Q1 = mediana dolnej po≈Çowy = 9\nG√≥rna po≈Çowa: 11, 11, 12, 12, 13 Q3 = mediana g√≥rnej po≈Çowy = 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n# Por√≥wnanie r√≥≈ºnych metod obliczania kwartyli w R\nmethods_comparison &lt;- data.frame(\n  Metoda = c(\"Rƒôcznie (bez mediany)\", \n             \"R type=1\", \"R type=2\", \"R type=7 (domy≈õlna)\"),\n  \"Q1 Kraj X\" = c(5, \n                  quantile(x, 0.25, type=1),\n                  quantile(x, 0.25, type=2),\n                  quantile(x, 0.25, type=7)),\n  \"Q3 Kraj X\" = c(15,\n                  quantile(x, 0.75, type=1),\n                  quantile(x, 0.75, type=2),\n                  quantile(x, 0.75, type=7)),\n  \"Q1 Kraj Y\" = c(9,\n                  quantile(y, 0.25, type=1),\n                  quantile(y, 0.25, type=2),\n                  quantile(y, 0.25, type=7)),\n  \"Q3 Kraj Y\" = c(12,\n                  quantile(y, 0.75, type=1),\n                  quantile(y, 0.75, type=2),\n                  quantile(y, 0.75, type=7))\n)\n\nkable(methods_comparison, digits = 2,\n      caption = \"Por√≥wnanie r√≥≈ºnych metod obliczania kwartyli\")\n\n\nPor√≥wnanie r√≥≈ºnych metod obliczania kwartyli\n\n\nMetoda\nQ1.Kraj.X\nQ3.Kraj.X\nQ1.Kraj.Y\nQ3.Kraj.Y\n\n\n\n\nRƒôcznie (bez mediany)\n5.0\n15.0\n9.00\n12.00\n\n\nR type=1\n5.0\n15.0\n9.00\n12.00\n\n\nR type=2\n5.0\n15.0\n9.00\n12.00\n\n\nR type=7 (domy≈õlna)\n5.5\n14.5\n9.25\n11.75\n\n\n\n\n\n\n\nWyja≈õnienie r√≥≈ºnic w metodach obliczania kwartyli\n\nMetoda rƒôczna (bez mediany):\n\nDzieli dane na dwie czƒô≈õci\nNie uwzglƒôdnia mediany\nZnajduje medianƒô ka≈ºdej czƒô≈õci\n\nR type=1:\n\nMetoda pierwsza w R\nU≈ºywa pozycji ca≈Çkowitych\nNie interpoluje\n\nR type=2:\n\nMetoda druga w R\nU≈ºywa pozycji ca≈Çkowitych\nInterpoluje gdy pozycja nie jest ca≈Çkowita\n\nR type=7 (domy≈õlna):\n\nMetoda domy≈õlna w R\nU≈ºywa quantile()[5] z SAS\nInterpoluje wed≈Çug metody opisanej przez Hyndmana i Fana\n\n\n\n\n\nPor√≥wnanie Wynik√≥w\n\nsummary_df &lt;- data.frame(\n  Miara = c(\"≈örednia\", \"Mediana\", \"Dominanta\", \"Rozstƒôp\", \"Wariancja\", \n            \"Odch. Stand.\", \"Q1\", \"Q3\", \"IQR\", \"CV (%)\"),\n  \"Kraj X\" = c(10, 10, \"brak\", 18, 36.67, 6.06, 5, 15, 10, 60.6),\n  \"Kraj Y\" = c(10.5, 10.5, \"9,10,11,12\", 5, 2.5, 1.58, 9, 12, 3, 15.0)\n)\n\nkable(summary_df, \n      caption = \"Zestawienie wszystkich miar statystycznych\",\n      align = c('l', 'r', 'r'))\n\n\nZestawienie wszystkich miar statystycznych\n\n\nMiara\nKraj.X\nKraj.Y\n\n\n\n\n≈örednia\n10\n10.5\n\n\nMediana\n10\n10.5\n\n\nDominanta\nbrak\n9,10,11,12\n\n\nRozstƒôp\n18\n5\n\n\nWariancja\n36.67\n2.5\n\n\nOdch. Stand.\n6.06\n1.58\n\n\nQ1\n5\n9\n\n\nQ3\n15\n12\n\n\nIQR\n10\n3\n\n\nCV (%)\n60.6\n15\n\n\n\n\n\n\n\nPor√≥wnanie za pomocƒÖ Wykresu Pude≈Çkowego\n\ndf_long &lt;- data.frame(\n  kraj = rep(c(\"X\", \"Y\"), each = 10),\n  wielkosc = c(x, y)\n)\n\n# Wykres podstawowy\np &lt;- ggplot(df_long, aes(x = kraj, y = wielkosc, fill = kraj)) +\n  geom_boxplot(outlier.shape = NA) +  # Wy≈ÇƒÖczamy domy≈õlne punkty odstajƒÖce\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Dodajemy punkty z przezroczysto≈õciƒÖ\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\")) +\n  labs(\n    title = \"Por√≥wnanie Zmienno≈õci Wielko≈õci Okrƒôg√≥w Wyborczych\",\n    subtitle = paste(\"CV: Kraj X =\", round(cv_x, 1), \"%, Kraj Y =\", round(cv_y, 1), \"%\"),\n    x = \"Kraj\",\n    y = \"Wielko≈õƒá Okrƒôgu\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Dodajemy adnotacje z kwartylami\np + annotate(\n  \"text\", \n  x = c(1, 1, 1, 2, 2, 2), \n  y = c(max(x)+1, mean(x), min(x)-1, max(y)+1, mean(y), min(y)-1),\n  label = c(\n    paste(\"Q3 =\", quantile(x, 0.75, type=1)),\n    paste(\"M =\", median(x)),\n    paste(\"Q1 =\", quantile(x, 0.25, type=1)),\n    paste(\"Q3 =\", quantile(y, 0.75, type=1)),\n    paste(\"M =\", median(y)),\n    paste(\"Q1 =\", quantile(y, 0.25, type=1))\n  ),\n  size = 3\n)\n\n\n\n\n\n\n\n\n\n\nUwagi Metodologiczne\n\nObliczenia kwartyli:\n\nZastosowana metoda wy≈ÇƒÖczajƒÖca medianƒô mo≈ºe dawaƒá inne wyniki ni≈º domy≈õlne funkcje R\nR√≥≈ºnice w metodach obliczeniowych nie wp≈ÇywajƒÖ na og√≥lne wnioski\nWarto zawsze zaznaczyƒá stosowanƒÖ metodƒô w raportach\n\nWizualizacja:\n\nWykres pude≈Çkowy skutecznie pokazuje r√≥≈ºnice w rozk≈Çadach\nDodatkowe punkty pokazujƒÖ rzeczywiste warto≈õci\nAdnotacje u≈ÇatwiajƒÖ interpretacjƒô\n\n\n\n\nPodsumowanie\n\nPor√≥wnanie Miar Statystycznych\n\n\n\nMiara\nKraj X\nKraj Y\nR√≥≈ºnica wzglƒôdna\n\n\n\n\n≈örednia\n10,0\n10,5\nPodobna\n\n\nMediana\n10,0\n10,5\nPodobna\n\n\nDominanta\nBrak\nWielokrotna (9,10,11,12)\n-\n\n\nRozstƒôp\n18\n5\n3,6√ó wiƒôkszy w X\n\n\nWariancja\n36,67\n2,5\n14,7√ó wiƒôksza w X\n\n\nIQR\n10\n3\n3,3√ó wiƒôkszy w X\n\n\nCV\n60,6%\n15,0%\n4,0√ó wiƒôkszy w X\n\n\n\n\n\nCharakterystyka Rozk≈Çad√≥w\nKraj X:\n\nRozk≈Çad r√≥wnomierny\nBrak dominujƒÖcej wielko≈õci okrƒôgu (brak dominanty)\nSzeroki zakres: od 1 do 19 mandat√≥w\nWysoka zmienno≈õƒá (CV = 60,6%)\nR√≥wnomierne roz≈Ço≈ºenie warto≈õci w zakresie\n\nKraj Y:\n\nRozk≈Çad skupiony\nWiele typowych wielko≈õci (cztery dominanty)\nWƒÖski zakres: od 8 do 13 mandat√≥w\nNiska zmienno≈õƒá (CV = 15,0%)\nWarto≈õci skoncentrowane wok√≥≈Ç ≈õredniej\n\n\n\nInterpretacja Wykresu Pude≈Çkowego\nWizualizacja w formie wykresu pude≈Çkowego pokazuje:\nElementy Struktury:\n\nPude≈Çko: Pokazuje rozstƒôp miƒôdzykwartylowy (IQR)\nDolna krawƒôd≈∫: Pierwszy kwartyl (Q1)\nG√≥rna krawƒôd≈∫: Trzeci kwartyl (Q3)\nLinia wewnƒôtrzna: Mediana (Q2)\nWƒÖsy: RozciƒÖgajƒÖ siƒô do ¬±1,5 IQR - Punkty: Pojedyncze wielko≈õci okrƒôg√≥w\n\nG≈Ç√≥wne Wnioski Wizualne:\n\nRozmiar Pude≈Çka:\n\n\nKraj X: Du≈ºe pude≈Çko wskazuje na szeroki rozrzut ≈õrodkowych 50%\nKraj Y: Ma≈Çe pude≈Çko pokazuje skupienie warto≈õci ≈õrodkowych\n\n\nD≈Çugo≈õƒá WƒÖs√≥w:\n\nKraj X: D≈Çugie wƒÖsy wskazujƒÖ na szeroki rozk≈Çad ca≈Çkowity\nKraj Y: Kr√≥tkie wƒÖsy pokazujƒÖ ograniczony rozrzut\n\nRozk≈Çad Punkt√≥w:\n\nKraj X: Punkty szeroko rozproszone\nKraj Y: Punkty gƒôsto skupione\n\n\n\n\nKluczowe Obserwacje\n\nTendencja Centralna:\n\nPodobne ≈õrednie wielko≈õci okrƒôg√≥w\nR√≥≈ºne wzorce rozk≈Çadu\nOdmienne podej≈õcia do standaryzacji\n\nMiary Zmienno≈õci:\n\nWszystkie miary pokazujƒÖ 3-15 razy wiƒôkszƒÖ zmienno≈õƒá w Kraju X\nSp√≥jny wzorzec w r√≥≈ºnych miarach statystycznych\nSystematyczna r√≥≈ºnica w projekcie okrƒôg√≥w\n\nProjekt Systemu:\n\nKraj X: Elastyczne, zr√≥≈ºnicowane podej≈õcie\nKraj Y: Ustandaryzowane, jednolite podej≈õcie\nR√≥≈ºne filozoficzne podej≈õcia do reprezentacji\n\nImplikacje Reprezentatywno≈õci:\n\nKraj X: Zmienna proporcja wyborc√≥w do przedstawicieli\nKraj Y: Bardziej sp√≥jne poziomy reprezentacji\nR√≥≈ºne podej≈õcia do reprezentacji demokratycznej\n\n\nAnaliza ta pokazuje fundamentalne r√≥≈ºnice w projektowaniu system√≥w wyborczych miƒôdzy dwoma krajami, gdzie Kraj X przyjmuje bardziej zr√≥≈ºnicowane podej≈õcie, a Kraj Y utrzymuje wiƒôkszƒÖ jednolito≈õƒá w wielko≈õci okrƒôg√≥w wyborczych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#cwiczenie-3.-wykresy-pude≈Çkowe-na-przyk≈Çadzie-danych-o-d≈Çugo≈õci-≈ºycia",
    "href": "rozdzial5.html#cwiczenie-3.-wykresy-pude≈Çkowe-na-przyk≈Çadzie-danych-o-d≈Çugo≈õci-≈ºycia",
    "title": "6¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.12 Cwiczenie 3. Wykresy Pude≈Çkowe na Przyk≈Çadzie Danych o D≈Çugo≈õci ≈ªycia",
    "text": "6.12 Cwiczenie 3. Wykresy Pude≈Çkowe na Przyk≈Çadzie Danych o D≈Çugo≈õci ≈ªycia\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# Przygotowanie danych\ndata_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007)\n\nWykres pude≈Çkowy (ang. box-and-whisker plot) przedstawia piƒôƒá kluczowych statystyk opisowych danych:\n\nMediana: ≈örodkowa linia w pude≈Çku (50. percentyl)\nPierwszy kwartyl (Q1): Dolna krawƒôd≈∫ pude≈Çka (25. percentyl)\nTrzeci kwartyl (Q3): G√≥rna krawƒôd≈∫ pude≈Çka (75. percentyl)\nRozstƒôp miƒôdzykwartylowy (IQR): Wysoko≈õƒá pude≈Çka (Q3 - Q1)\nWƒÖsy: RozciƒÖgajƒÖ siƒô do najbardziej skrajnych warto≈õci niebƒôdƒÖcych obserwacjami odstajƒÖcymi (metoda Tukeya: 1.5 √ó IQR)\nObserwacje odstajƒÖce: Pojedyncze punkty poza wƒÖsami\n\n\nWizualizacja D≈Çugo≈õci ≈ªycia\n\nggplot(data_2007, aes(x = reorder(continent, lifeExp, FUN = median), y = lifeExp)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 24, \n               outlier.fill = \"red\", outlier.alpha = 0.6, outlier.size = 4) +\n  geom_jitter(width = 0.2, alpha = 0.4, color = \"darkblue\") +\n  labs(title = \"D≈Çugo≈õƒá ≈ªycia wed≈Çug Kontynent√≥w (2007)\",\n       subtitle = \"Pojedyncze punkty pokazujƒÖ surowe dane; czerwone punkty oznaczajƒÖ warto≈õci odstajƒÖce\",\n       x = \"Kontynent\",\n       y = \"D≈Çugo≈õƒá ≈ºycia (w latach)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_y_continuous(breaks = seq(40, 85, by = 5))\n\n\n\n\n\n\n\n\n\n\nAnaliza Danych\n\n\nMediana i Rozk≈Çad\nOdpowiedz Prawda lub Fa≈Çsz:\n\n50% kraj√≥w afryka≈Ñskich ma d≈Çugo≈õƒá ≈ºycia poni≈ºej 52 lat\nMediana d≈Çugo≈õci ≈ºycia w Europie wynosi oko≈Ço 78 lat\nPonad 75% kraj√≥w Oceanii ma d≈Çugo≈õƒá ≈ºycia powy≈ºej 75 lat\n25% kraj√≥w azjatyckich ma d≈Çugo≈õƒá ≈ºycia poni≈ºej 68 lat\n≈örodkowe 50% d≈Çugo≈õci ≈ºycia w Europie mie≈õci siƒô miƒôdzy 76 a 80 lat\n\n\n\nRozrzut i Zmienno≈õƒá\nOdpowiedz Prawda lub Fa≈Çsz:\n\nAzja wykazuje najwiƒôkszy rozrzut (IQR) w d≈Çugo≈õci ≈ºycia\nEuropa ma najmniejszy IQR w≈õr√≥d wszystkich kontynent√≥w\nZmienno≈õƒá d≈Çugo≈õci ≈ºycia w Afryce jest wiƒôksza ni≈º w obu Amerykach\nOceania wykazuje najmniejszƒÖ zmienno≈õƒá w d≈Çugo≈õci ≈ºycia\nWƒÖsy dla Azji rozciƒÖgajƒÖ siƒô w przybli≈ºeniu od 58 do 82 lat (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych)\n\n\n\nWarto≈õci OdstajƒÖce i Ekstrema\nOdpowiedz Prawda lub Fa≈Çsz:\n\nAfryka ma dwa kraje z wyjƒÖtkowo niskƒÖ d≈Çugo≈õciƒÖ ≈ºycia\nW rozk≈Çadzie dla Oceanii nie ma warto≈õci odstajƒÖcych\nAzja ma kilka niskich warto≈õci odstajƒÖcych (poni≈ºej 55 lat)\n\n\n\nZmiany w Czasie\n\ntime_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  mutate(year = factor(year))\n\nggplot(time_comparison, aes(x = continent, y = lifeExp, fill = year)) +\n  geom_boxplot(alpha = 0.7, position = \"dodge\", outlier.shape = 21,\n               outlier.alpha = 0.6) +\n  labs(title = \"D≈Çugo≈õƒá ≈ªycia: 1957 vs 2007\",\n       subtitle = \"Por√≥wnanie zmian rozk≈Çadu na przestrzeni 50 lat\",\n       x = \"Kontynent\",\n       y = \"D≈Çugo≈õƒá ≈ºycia (w latach)\",\n       fill = \"Rok\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 14)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(breaks = seq(30, 85, by = 5))\n\n\n\n\n\n\n\n\n\n\nPytania dotyczƒÖce Zmian w Czasie\nOdpowiedz Prawda lub Fa≈Çsz:\n\nMediana d≈Çugo≈õci ≈ºycia wzros≈Ça na wszystkich kontynentach miƒôdzy 1957 a 2007 rokiem\nZmienno≈õƒá d≈Çugo≈õci ≈ºycia (IQR) zmniejszy≈Ça siƒô na wiƒôkszo≈õci kontynent√≥w w czasie\nAfryka wykaza≈Ça najmniejszƒÖ poprawƒô mediany d≈Çugo≈õci ≈ºycia\nRozrzut d≈Çugo≈õci ≈ºycia w Azji znaczƒÖco siƒô zmniejszy≈Ç od 1957 do 2007 roku\nOceania utrzyma≈Ça najwy≈ºszƒÖ medianƒô d≈Çugo≈õci ≈ºycia w obu okresach\n\n\n\nPodsumowanie Statystyczne\n\n# Obliczenie statystyk opisowych\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    mediana = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    min = min(lifeExp),\n    max = max(lifeExp),\n    liczba_odstajƒÖcych = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Statystyki Opisowe wed≈Çug Kontynentu i Roku\")\n\n\nStatystyki Opisowe wed≈Çug Kontynentu i Roku\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontinent\nyear\nmediana\nq1\nq3\niqr\nmin\nmax\nliczba_odstajƒÖcych\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n31.6\n58.1\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n39.6\n76.4\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n40.7\n70.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n60.9\n80.7\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n30.3\n67.8\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n43.8\n82.6\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n48.1\n73.5\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n71.8\n81.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n70.3\n70.3\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n80.2\n81.2\n0\n\n\n\n\n\n\n\nNajwa≈ºniejsze Wnioski\n\nCentrum Rozk≈Çadu:\n\nMediana pokazuje typowƒÖ d≈Çugo≈õƒá ≈ºycia\nZmiany mediany odzwierciedlajƒÖ og√≥lnƒÖ poprawƒô\n\nRozrzut i Zmienno≈õƒá:\n\nIQR (wysoko≈õƒá pude≈Çka) wskazuje na rozproszenie danych\nSzersze pude≈Çka sugerujƒÖ wiƒôksze nier√≥wno≈õci w d≈Çugo≈õci ≈ºycia\n\nWarto≈õci OdstajƒÖce i Ekstrema:\n\nWarto≈õci odstajƒÖce czƒôsto reprezentujƒÖ kraje o wyjƒÖtkowej sytuacji\n\nPor√≥wnanie w Czasie:\n\nPokazuje zar√≥wno bezwzglƒôdnƒÖ poprawƒô, jak i zmiany w wariancji\nUwydatnia utrzymujƒÖce siƒô r√≥≈ºnice regionalne\nUjawnia r√≥≈ºne tempo postƒôpu na poszczeg√≥lnych kontynentach\n\n\n\n\nStatystyki opisowe (podsumowanie)\n\n# Calculate summary statistics\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    median = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    min = min(lifeExp),\n    max = max(lifeExp),\n    n_outliers = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Summary Statistics by Continent and Year\")\n\n\nSummary Statistics by Continent and Year\n\n\ncontinent\nyear\nmedian\nq1\nq3\niqr\nmin\nmax\nn_outliers\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n31.6\n58.1\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n39.6\n76.4\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n40.7\n70.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n60.9\n80.7\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n30.3\n67.8\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n43.8\n82.6\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n48.1\n73.5\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n71.8\n81.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n70.3\n70.3\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n80.2\n81.2\n0",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#appendix-tabele-podsumowujƒÖce-typy-danych-i-odpowiednie-miary-statystyczne",
    "href": "rozdzial5.html#appendix-tabele-podsumowujƒÖce-typy-danych-i-odpowiednie-miary-statystyczne",
    "title": "6¬† Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "6.13 Appendix: Tabele PodsumowujƒÖce Typy Danych i Odpowiednie Miary Statystyczne",
    "text": "6.13 Appendix: Tabele PodsumowujƒÖce Typy Danych i Odpowiednie Miary Statystyczne\n\nZalety i Wady R√≥≈ºnych Miar Statystycznych\n\nMiary Tendencji Centralnej\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\n≈örednia\n- Wykorzystuje wszystkie punkty danych- Pozwala na dalsze obliczenia statystyczne- Idealna dla danych o rozk≈Çadzie normalnym\n- Wra≈ºliwa na warto≈õci odstajƒÖce- Nieodpowiednia dla rozk≈Çad√≥w sko≈õnych- Bez znaczenia dla danych nominalnych\nInterwa≈Çowe, Ilorazowe, niekt√≥re Dyskretne, CiƒÖg≈Çe\n\n\nMediana\n- Niewra≈ºliwa na warto≈õci odstajƒÖce- Dobra dla rozk≈Çad√≥w sko≈õnych- Mo≈ºe byƒá stosowana do danych porzƒÖdkowych\n- Ignoruje rzeczywiste warto≈õci wiƒôkszo≈õci punkt√≥w danych- Mniej u≈ºyteczna do dalszych analiz statystycznych\nPorzƒÖdkowe, Interwa≈Çowe, Ilorazowe, Dyskretne, CiƒÖg≈Çe\n\n\nModa\n- Mo≈ºe byƒá stosowana do ka≈ºdego typu danych- Dobra do znajdowania najczƒôstszej kategorii\n- Mo≈ºe nie byƒá unikalna (rozk≈Çady multimodalne)- Nieprzydatna do wielu typ√≥w analiz- Ignoruje wielko≈õƒá r√≥≈ºnic miƒôdzy warto≈õciami\nWszystkie typy\n\n\n\n\n\nMiary Zmienno≈õci\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nZakres\n- Prosty do obliczenia i zrozumienia- Daje szybki obraz rozproszenia danych\n- Bardzo wra≈ºliwy na warto≈õci odstajƒÖce- Ignoruje wszystkie dane miƒôdzy ekstremami- Nieprzydatny do dalszych analiz statystycznych\nPorzƒÖdkowe, Interwa≈Çowe, Ilorazowe, Dyskretne, CiƒÖg≈Çe\n\n\nRozstƒôp miƒôdzykwartylowy (IQR)\n- Niewra≈ºliwy na warto≈õci odstajƒÖce- Dobry dla rozk≈Çad√≥w sko≈õnych\n- Ignoruje 50% danych- Mniej intuicyjny ni≈º zakres\nPorzƒÖdkowe, Interwa≈Çowe, Ilorazowe, Dyskretne, CiƒÖg≈Çe\n\n\nWariancja\n- Wykorzystuje wszystkie punkty danych- Podstawa wielu procedur statystycznych\n- Wra≈ºliwa na warto≈õci odstajƒÖce- Jednostki sƒÖ podniesione do kwadratu (mniej intuicyjne)\nInterwa≈Çowe, Ilorazowe, niekt√≥re Dyskretne, CiƒÖg≈Çe\n\n\nOdchylenie standardowe\n- Wykorzystuje wszystkie punkty danych- Te same jednostki co oryginalne dane- Szeroko stosowane i zrozumia≈Çe\n- Wra≈ºliwe na warto≈õci odstajƒÖce- Zak≈Çada w przybli≈ºeniu rozk≈Çad normalny dla interpretacji\nInterwa≈Çowe, Ilorazowe, niekt√≥re Dyskretne, CiƒÖg≈Çe\n\n\nWsp√≥≈Çczynnik zmienno≈õci\n- Pozwala na por√≥wnanie miƒôdzy zbiorami danych o r√≥≈ºnych jednostkach lub ≈õrednich\n- Mo≈ºe byƒá mylƒÖcy, gdy ≈õrednie sƒÖ bliskie zeru- Bez znaczenia dla danych z warto≈õciami ujemnymi\nIlorazowe, niekt√≥re Interwa≈Çowe\n\n\n\n\n\nMiary Korelacji/Asocjacji\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nr Pearsona\n- Mierzy zale≈ºno≈õƒá liniowƒÖ- Szeroko stosowany i zrozumia≈Çy\n- Zak≈Çada rozk≈Çad normalny- Wra≈ºliwy na warto≈õci odstajƒÖce- Uchwytuje tylko zale≈ºno≈õci liniowe\nInterwa≈Çowe, Ilorazowe, CiƒÖg≈Çe\n\n\nRho Spearmana\n- Mo≈ºe byƒá stosowany do danych porzƒÖdkowych- Uchwytuje zale≈ºno≈õci monotoniczne- Mniej wra≈ºliwy na warto≈õci odstajƒÖce\n- Traci informacje przez konwersjƒô na rangi- Mo≈ºe pominƒÖƒá niekt√≥re typy zale≈ºno≈õci\nPorzƒÖdkowe, Interwa≈Çowe, Ilorazowe\n\n\nTau Kendalla\n- Mo≈ºe byƒá stosowany do danych porzƒÖdkowych- Bardziej odporny ni≈º Spearman dla ma≈Çych pr√≥bek- Ma ≈ÇadnƒÖ interpretacjƒô (prawdopodobie≈Ñstwo zgodno≈õci)\n- Traci informacje, biorƒÖc pod uwagƒô tylko porzƒÖdek- Bardziej intensywny obliczeniowo\nPorzƒÖdkowe, Interwa≈Çowe, Ilorazowe\n\n\nChi-kwadrat\n- Mo≈ºe byƒá stosowany do danych nominalnych- Testuje niezale≈ºno≈õƒá zmiennych kategorycznych\n- Wymaga du≈ºych rozmiar√≥w pr√≥bek- Wra≈ºliwy na rozmiar pr√≥bki- Nie mierzy si≈Çy asocjacji\nNominalne, PorzƒÖdkowe\n\n\nV Cram√©ra\n- Mo≈ºe byƒá stosowany do danych nominalnych- Dostarcza miarƒô si≈Çy asocjacji- Znormalizowany do zakresu [0,1]\n- Interpretacja mo≈ºe byƒá subiektywna- Mo≈ºe przeszacowaƒá asocjacjƒô w ma≈Çych pr√≥bkach\nNominalne, PorzƒÖdkowe\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nMedian\nMediana\n-\n‚úì\n‚úì\n‚úì\n\n\nArithmetic Mean\n≈örednia arytmetyczna\n-\n-\n‚úì*\n‚úì\n\n\nGeometric Mean\n≈örednia geometryczna\n-\n-\n-\n‚úì\n\n\nHarmonic Mean\n≈örednia harmoniczna\n-\n-\n-\n‚úì\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstƒôp\n-\n‚úì\n‚úì\n‚úì\n\n\nInterquartile Range\nRozstƒôp miƒôdzykwartylowy\n-\n‚úì\n‚úì\n‚úì\n\n\nMean Absolute Deviation\n≈örednie odchylenie bezwzglƒôdne\n-\n-\n‚úì\n‚úì\n\n\nVariance\nWariancja\n-\n-\n‚úì*\n‚úì\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n‚úì*\n‚úì\n\n\nCoefficient of Variation\nWsp√≥≈Çczynnik zmienno≈õci\n-\n-\n-\n‚úì\n\n\nAssociation / Wsp√≥≈Çzale≈ºno≈õƒá:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n‚úì\n‚úì\n‚úì\n\n\nKendall‚Äôs Tau\nTau Kendalla\n-\n‚úì\n‚úì\n‚úì\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n‚úì*\n‚úì\n\n\nCovariance\nKowariancja\n-\n-\n‚úì*\n‚úì\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\nNotes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporzƒÖdkowania\nOrdinal: Ordered categories / Kategorie uporzƒÖdkowane\nInterval: Equal intervals, arbitrary zero / R√≥wne interwa≈Çy, umowne zero\nRatio: Equal intervals, absolute zero / R√≥wne interwa≈Çy, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ‚úì* are commonly used for interval data despite theoretical issues / Niekt√≥re miary oznaczone ‚úì* sƒÖ powszechnie stosowane dla danych przedzia≈Çowych pomimo problem√≥w teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wyb√≥r miary powinien uwzglƒôdniaƒá zar√≥wno poprawno≈õƒá teoretycznƒÖ jak i u≈ºyteczno≈õƒá praktycznƒÖ\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalajƒÖ na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "7¬† Data Visualization: with examples in R",
    "section": "",
    "text": "7.1 Introduction to Data Types and Visualization\nThis chapter explores fundamental types of data visualizations: bar plots, histograms, and box plots, in particular.\nBefore diving into specific visualization techniques, it‚Äôs crucial to understand the different types of data you might encounter and how they influence your choice of visualization method. We‚Äôll explore these concepts with practical examples using the ggplot2 library in R.\nFirst, let‚Äôs load the necessary libraries:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#bar-plots",
    "href": "chapter6.html#bar-plots",
    "title": "7¬† Data Visualization: with examples in R",
    "section": "7.2 Bar Plots",
    "text": "7.2 Bar Plots\nBar plots are excellent for displaying categorical data or summarizing continuous data by groups.\n\nUnderstanding Bar Plots\nA bar plot represents data using rectangular bars with heights proportional to the values they represent. They are used to compare different categories or groups.\nKey components of a bar plot: 1. X-axis: Represents categories 2. Y-axis: Represents values (can be counts, percentages, or any numerical value) 3. Bars: Rectangle for each category, height corresponds to its value\n\nExample Data\nLet‚Äôs use a simple dataset of fruit sales:\n\nfruits &lt;- c(\"Apple\", \"Banana\", \"Orange\", \"Grape\")\nsales &lt;- c(120, 85, 70, 100)\n\n# Create a data frame\ndf &lt;- data.frame(fruit = fruits, sales = sales)\n\n\n\n\nHand-Drawn Bar Plot\nTo create a bar plot by hand:\n\nDraw a horizontal line (x-axis) and a vertical line (y-axis) perpendicular to each other.\nLabel the x-axis with your categories (fruits), evenly spaced.\nLabel the y-axis with a suitable scale for your values (sales, 0 to 120 in increments of 20).\nFor each category, draw a rectangle (bar) whose height corresponds to its value on the y-axis scale.\nColor or shade each bar if desired.\nAdd a title and labels for both axes.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen drawing by hand, use graph paper for more precise measurements and straighter lines. Choose a scale that allows all your data to fit while maximizing the use of space.\n\n\n\n\nBar Plot in Base R\n\n# Create bar plot\nbarplot(sales, names.arg = fruits, \n        main = \"Fruit Sales\",\n        xlab = \"Fruit Types\", ylab = \"Sales\")\n\n\n\n\n\n\n\n\n\n\nBar Plot with ggplot2\n\n# Create bar plot with ggplot2\nggplot(df, aes(x = fruit, y = sales)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Fruit Sales\",\n       x = \"Fruit Types\", y = \"Sales\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpreting Bar Plots\nWhen interpreting a bar plot, consider the following:\n\nRelative Heights: Compare the heights of the bars to understand which categories have higher or lower values.\nOrdering: Sometimes, bars are ordered by height to make comparisons easier.\nPatterns: Look for any patterns or trends across categories.\nOutliers: Identify any bars that are much taller or shorter than the others.\n\n\nExample Interpretation\nFor our fruit sales data:\n\nApples have the highest sales (120), followed by Grapes (100).\nOranges have the lowest sales (70).\nThere‚Äôs a considerable difference between the highest (Apples) and lowest (Oranges) sales.\nBananas and Grapes have similar sales figures, in the middle range.\n\nThis information could be useful for inventory management or marketing strategies in a fruit shop.\n\n\n\n\n\n\nNote\n\n\n\nBar plots are great for comparing categories, but they don‚Äôt show the distribution within each category. For that, you might need other plot types like box plots.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#histograms",
    "href": "chapter6.html#histograms",
    "title": "7¬† Data Visualization: with examples in R",
    "section": "7.3 Histograms",
    "text": "7.3 Histograms\nHistograms visualize the distribution of a continuous variable by dividing it into intervals (bins) and showing the frequency or density of data points in each bin.\n\nUnderstanding Histograms\nKey components of a histogram: 1. X-axis: Represents the variable‚Äôs values, divided into bins 2. Y-axis: Represents frequency, relative frequency, or density 3. Bars: Rectangle for each bin, height corresponds to the y-axis measure\nThere are three main types of histograms:\n\nFrequency Histogram: The y-axis shows the count of data points in each bin.\nRelative Frequency Histogram: The y-axis shows the proportion of data points in each bin (frequency divided by total number of data points).\nDensity Histogram: The y-axis shows the density, which is the relative frequency divided by the bin width. The total area of all bars sums to 1.\n\n\nExample Data\nLet‚Äôs use a dataset of 50 student exam scores (out of 100):\n\nset.seed(123)  # for reproducibility\nscores &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\nHand-Drawn Histogram\nTo create a frequency histogram by hand:\n\nFind the range of your data.\nChoose a number of bins (let‚Äôs use 7 bins).\nCreate a frequency table.\nDraw x and y axes.\nLabel x-axis with bin ranges and y-axis with frequency.\nDraw a rectangle for each bin, with height corresponding to its frequency.\nAdd a title and labels for both axes.\n\nFor a relative frequency histogram, divide each frequency by the total number of data points before drawing the bars.\nFor a density histogram, divide the relative frequency by the bin width before drawing the bars.\n\n\n\n\n\n\nTip\n\n\n\nThe number of bins can affect the interpretation. Too few bins may obscure important features, while too many may introduce noise. A common rule of thumb is to use the square root of the number of data points as the number of bins.\n\n\n\n\nHistograms in Base R\n\n# Frequency Histogram\nhist(scores, breaks = 7, \n     main = \"Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Relative Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Relative Frequency\")\n\n\n\n\n\n\n\n# Density Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Density Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Density\")\nlines(density(scores), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\nHistograms with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(score = scores)\n\n# Frequency Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nggplot(df, aes(x = score, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Relative Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Relative Frequency\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Density Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Density Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Density\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nInterpreting Histograms\nWhen interpreting a histogram, consider:\n\nCentral Tendency: Where is the peak of the distribution?\nSpread: How wide is the distribution?\nShape: Is it symmetric, skewed, or multi-modal?\nOutliers: Are there any unusual values far from the main distribution?",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#box-plots-and-tukey-box-plots",
    "href": "chapter6.html#box-plots-and-tukey-box-plots",
    "title": "7¬† Data Visualization: with examples in R",
    "section": "7.4 Box Plots and Tukey Box Plots",
    "text": "7.4 Box Plots and Tukey Box Plots\nBox plots, also known as box-and-whisker plots, provide a concise summary of a distribution. We‚Äôll focus on the Tukey-style box plot, named after the statistician John Tukey who popularized this type of plot.\n\nUnderstanding Box Plots\nA box plot represents five key statistics:\n\nMinimum value (excluding outliers)\nFirst quartile (Q1)\nMedian\nThird quartile (Q3)\nMaximum value (excluding outliers)\n\nAdditionally, box plots show:\n\nWhiskers: Lines extending from the box to the minimum and maximum values (excluding outliers)\nOutliers: Individual points beyond the whiskers\n\n\nCalculating Quartiles and Outliers\nTo create a box plot, follow these steps:\n\nOrder your data from smallest to largest.\nFind the median (middle value if odd number of data points, average of two middle values if even).\nFind Q1 (median of lower half of data) and Q3 (median of upper half of data).\nCalculate the Interquartile Range (IQR) = Q3 - Q1\nDetermine outliers using Tukey‚Äôs rule:\n\nLower outliers: &lt; Q1 - 1.5 * IQR\nUpper outliers: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe factor 1.5 in Tukey‚Äôs outlier rule is based on the properties of the normal distribution. For normally distributed data, this rule identifies about 0.7% of the data as potential outliers.\n\n\n\n\nExample Data\nLet‚Äôs use a small dataset to illustrate:\n\ndata &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\nHand-Drawn Tukey Box Plot\nTo create a Tukey box plot by hand:\n\nDraw a vertical line representing the range from minimum to maximum (2 to 15 in our example, excluding the outlier).\nDraw a box from Q1 to Q3.\nDraw a horizontal line through the box at the median.\nDraw whiskers from the box to the minimum and maximum values (excluding outliers).\nRepresent the outlier (50) as an individual point beyond the whisker.\nAdd a scale to the vertical axis and label it.\n\n\n\nBox Plot in Base R\n\n# Create box plot\nboxplot(data, main = \"Box Plot of Sample Data\",\n        ylab = \"Values\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\nTukey Box Plot with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(value = data)\n\n# Create Tukey box plot with ggplot2\nggplot(df, aes(x = \"\", y = value)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Tukey Box Plot of Sample Data\",\n       x = \"\", y = \"Values\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\nInterpreting Box Plots\nWhen interpreting a box plot, consider the following:\n\nCentral Tendency: The median shows the center of the distribution.\nSpread: The box (IQR) represents the middle 50% of the data.\nSkewness: If the median line is closer to one end of the box, the distribution is skewed.\nOutliers: Points beyond the whiskers are potential outliers.\nComparisons: When comparing multiple box plots, look at relative positions of medians, box sizes, and presence of outliers.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#conclusion",
    "href": "chapter6.html#conclusion",
    "title": "7¬† Data Visualization: with examples in R",
    "section": "7.5 Conclusion",
    "text": "7.5 Conclusion\nIn this chapter, we explored three fundamental types of data visualizations: bar plots, histograms, and box plots. We demonstrated how to create these plots by hand, using R‚Äôs base plotting system, and using the ggplot2 library.\nEach type of plot serves a different purpose: - Bar plots are excellent for comparing categories. - Histograms show the distribution of a continuous variable. - Box plots provide a concise summary of a distribution, highlighting central tendency, spread, and outliers.\nRemember, the choice of visualization depends on your data type and the insights you want to convey. Always consider your audience and the story you want to tell with your data when selecting and designing your visualizations.\nPractice creating these plots by hand to deepen your understanding of their construction and interpretation. Then, leverage the power of R and ggplot2 to quickly create and customize these visualizations for larger datasets and more complex analyses.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html",
    "href": "rozdzial6.html",
    "title": "8¬† Wizualizacja Danych: z przyk≈Çadami w R",
    "section": "",
    "text": "8.1 Wprowadzenie do Typ√≥w Danych i Wizualizacji\nW tym rozdziale poznamy podstawowe typy wizualizacji danych: wykresy s≈Çupkowe, histogramy i wykresy pude≈Çkowe. Om√≥wimy ich tworzenie zar√≥wno rƒôcznie, jak i przy u≈ºyciu R.\nPrzed zag≈Çƒôbieniem siƒô w konkretne techniki wizualizacji, wa≈ºne jest zrozumienie r√≥≈ºnych typ√≥w danych i ich wp≈Çywu na wyb√≥r metody wizualizacji. Przeanalizujemy te koncepcje na praktycznych przyk≈Çadach z u≈ºyciem biblioteki ggplot2 w R.\nNajpierw za≈Çadujmy niezbƒôdne biblioteki:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Wizualizacja Danych: z przyk≈Çadami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-s≈Çupkowe",
    "href": "rozdzial6.html#wykresy-s≈Çupkowe",
    "title": "8¬† Wizualizacja Danych: z przyk≈Çadami w R",
    "section": "8.2 Wykresy S≈Çupkowe",
    "text": "8.2 Wykresy S≈Çupkowe\nWykresy s≈Çupkowe doskonale nadajƒÖ siƒô do prezentacji danych kategorycznych lub podsumowania danych ciƒÖg≈Çych w grupach.\n\nZrozumienie Wykres√≥w S≈Çupkowych\nWykres s≈Çupkowy przedstawia dane za pomocƒÖ prostokƒÖtnych s≈Çupk√≥w, kt√≥rych wysoko≈õƒá jest proporcjonalna do reprezentowanych przez nie warto≈õci. S≈Çu≈ºƒÖ do por√≥wnywania r√≥≈ºnych kategorii lub grup.\nG≈Ç√≥wne elementy wykresu s≈Çupkowego: 1. O≈õ X: Reprezentuje kategorie 2. O≈õ Y: Reprezentuje warto≈õci (mogƒÖ to byƒá liczebno≈õci, procenty lub dowolne warto≈õci numeryczne) 3. S≈Çupki: ProstokƒÖt dla ka≈ºdej kategorii, wysoko≈õƒá odpowiada jej warto≈õci\n\nPrzyk≈Çadowe Dane\nU≈ºyjmy prostego zestawu danych dotyczƒÖcego sprzeda≈ºy owoc√≥w:\n\nowoce &lt;- c(\"Jab≈Çko\", \"Banan\", \"Pomara≈Ñcza\", \"Winogrono\")\nsprzedaz &lt;- c(120, 85, 70, 100)\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(owoc = owoce, sprzedaz = sprzedaz)\n\n\n\n\nRƒôcznie Rysowany Wykres S≈Çupkowy\nAby stworzyƒá wykres s≈Çupkowy rƒôcznie:\n\nNarysuj liniƒô poziomƒÖ (o≈õ X) i pionowƒÖ (o≈õ Y) prostopad≈Çe do siebie.\nOznacz o≈õ X swoimi kategoriami (owocami), r√≥wnomiernie rozmieszczonymi.\nOznacz o≈õ Y odpowiedniƒÖ skalƒÖ dla Twoich warto≈õci (sprzeda≈º, od 0 do 120 z przyrostami co 20).\nDla ka≈ºdej kategorii narysuj prostokƒÖt (s≈Çupek), kt√≥rego wysoko≈õƒá odpowiada jej warto≈õci na skali osi Y.\nJe≈õli chcesz, pokoloruj lub zacienuj ka≈ºdy s≈Çupek.\nDodaj tytu≈Ç i etykiety dla obu osi.\n\n\n\n\n\n\n\nTip\n\n\n\nPrzy rysowaniu rƒôcznym u≈ºyj papieru milimetrowego dla dok≈Çadniejszych pomiar√≥w i prostszych linii. Wybierz skalƒô, kt√≥ra pozwoli zmie≈õciƒá wszystkie dane, maksymalnie wykorzystujƒÖc dostƒôpnƒÖ przestrze≈Ñ.\n\n\n\n\nWykres S≈Çupkowy w Podstawowym R\n\n# Tworzenie wykresu s≈Çupkowego\nbarplot(sprzedaz, names.arg = owoce, \n        main = \"Sprzeda≈º Owoc√≥w\",\n        xlab = \"Rodzaje Owoc√≥w\", ylab = \"Sprzeda≈º\")\n\n\n\n\n\n\n\n\n\n\nWykres S≈Çupkowy z ggplot2\n\n# Tworzenie wykresu s≈Çupkowego z ggplot2\nggplot(df, aes(x = owoc, y = sprzedaz)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Sprzeda≈º Owoc√≥w\",\n       x = \"Rodzaje Owoc√≥w\", y = \"Sprzeda≈º\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpretacja Wykres√≥w S≈Çupkowych\nPodczas interpretacji wykresu s≈Çupkowego zwr√≥ƒá uwagƒô na:\n\nWzglƒôdne Wysoko≈õci: Por√≥wnaj wysoko≈õci s≈Çupk√≥w, aby zrozumieƒá, kt√≥re kategorie majƒÖ wy≈ºsze lub ni≈ºsze warto≈õci.\nKolejno≈õƒá: Czasami s≈Çupki sƒÖ uporzƒÖdkowane wed≈Çug wysoko≈õci, aby u≈Çatwiƒá por√≥wnania.\nWzorce: Poszukaj wzorc√≥w lub trend√≥w miƒôdzy kategoriami.\nWarto≈õci OdstajƒÖce: Zidentyfikuj s≈Çupki, kt√≥re sƒÖ znacznie wy≈ºsze lub ni≈ºsze od pozosta≈Çych.\n\n\nPrzyk≈Çadowa Interpretacja\nDla naszych danych o sprzeda≈ºy owoc√≥w:\n\nJab≈Çka majƒÖ najwy≈ºszƒÖ sprzeda≈º (120), nastƒôpnie Winogrona (100).\nPomara≈Ñcze majƒÖ najni≈ºszƒÖ sprzeda≈º (70).\nIstnieje znaczna r√≥≈ºnica miƒôdzy najwy≈ºszƒÖ (Jab≈Çka) a najni≈ºszƒÖ (Pomara≈Ñcze) sprzeda≈ºƒÖ.\nBanany i Winogrona majƒÖ podobne warto≈õci sprzeda≈ºy, w ≈õrednim zakresie.\n\nTa informacja mo≈ºe byƒá przydatna dla zarzƒÖdzania zapasami lub strategii marketingowych w sklepie owocowym.\n\n\n\n\n\n\nNote\n\n\n\nWykresy s≈Çupkowe sƒÖ ≈õwietne do por√≥wnywania kategorii, ale nie pokazujƒÖ rozk≈Çadu wewnƒÖtrz ka≈ºdej kategorii. Do tego mogƒÖ byƒá potrzebne inne typy wykres√≥w, jak wykresy pude≈Çkowe.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Wizualizacja Danych: z przyk≈Çadami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#histogramy",
    "href": "rozdzial6.html#histogramy",
    "title": "8¬† Wizualizacja Danych: z przyk≈Çadami w R",
    "section": "8.3 Histogramy",
    "text": "8.3 Histogramy\nHistogramy wizualizujƒÖ rozk≈Çad zmiennej ciƒÖg≈Çej poprzez podzielenie jej na przedzia≈Çy (bins) i pokazanie czƒôsto≈õci lub gƒôsto≈õci punkt√≥w danych w ka≈ºdym przedziale.\n\nZrozumienie Histogram√≥w\nG≈Ç√≥wne elementy histogramu: 1. O≈õ X: Reprezentuje warto≈õci zmiennej, podzielone na przedzia≈Çy 2. O≈õ Y: Reprezentuje czƒôsto≈õƒá, wzglƒôdnƒÖ czƒôsto≈õƒá lub gƒôsto≈õƒá 3. S≈Çupki: ProstokƒÖt dla ka≈ºdego przedzia≈Çu, wysoko≈õƒá odpowiada mierze na osi Y\nIstniejƒÖ trzy g≈Ç√≥wne typy histogram√≥w:\n\nHistogram Czƒôsto≈õci: O≈õ Y pokazuje liczbƒô punkt√≥w danych w ka≈ºdym przedziale.\nHistogram Czƒôsto≈õci Wzglƒôdnej: O≈õ Y pokazuje proporcjƒô punkt√≥w danych w ka≈ºdym przedziale (czƒôsto≈õƒá podzielona przez ca≈ÇkowitƒÖ liczbƒô punkt√≥w danych).\nHistogram Gƒôsto≈õci: O≈õ Y pokazuje gƒôsto≈õƒá, kt√≥ra jest czƒôsto≈õciƒÖ wzglƒôdnƒÖ podzielonƒÖ przez szeroko≈õƒá przedzia≈Çu. Ca≈Çkowita powierzchnia wszystkich s≈Çupk√≥w sumuje siƒô do 1.\n\n\nPrzyk≈Çadowe Dane\nU≈ºyjmy zbioru 50 wynik√≥w egzamin√≥w student√≥w (na 100 punkt√≥w):\n\nset.seed(123)  # dla powtarzalno≈õci\nwyniki &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\nRƒôcznie Rysowany Histogram\nAby stworzyƒá histogram czƒôsto≈õci rƒôcznie:\n\nZnajd≈∫ zakres danych.\nWybierz liczbƒô przedzia≈Ç√≥w (u≈ºyjmy 7 przedzia≈Ç√≥w).\nUtw√≥rz tabelƒô czƒôsto≈õci.\nNarysuj osie X i Y.\nOznacz o≈õ X zakresami przedzia≈Ç√≥w, a o≈õ Y czƒôsto≈õciƒÖ.\nNarysuj prostokƒÖt dla ka≈ºdego przedzia≈Çu, z wysoko≈õciƒÖ odpowiadajƒÖcƒÖ jego czƒôsto≈õci.\nDodaj tytu≈Ç i etykiety dla obu osi.\n\nDla histogramu czƒôsto≈õci wzglƒôdnej, podziel ka≈ºdƒÖ czƒôsto≈õƒá przez ca≈ÇkowitƒÖ liczbƒô punkt√≥w danych przed narysowaniem s≈Çupk√≥w.\nDla histogramu gƒôsto≈õci, podziel czƒôsto≈õƒá wzglƒôdnƒÖ przez szeroko≈õƒá przedzia≈Çu przed narysowaniem s≈Çupk√≥w.\n\n\n\n\n\n\nTip\n\n\n\nLiczba przedzia≈Ç√≥w mo≈ºe wp≈ÇynƒÖƒá na interpretacjƒô. Zbyt ma≈Ço przedzia≈Ç√≥w mo≈ºe ukryƒá wa≈ºne cechy, podczas gdy zbyt wiele mo≈ºe wprowadziƒá szum. PowszechnƒÖ regu≈ÇƒÖ jest u≈ºycie pierwiastka kwadratowego z liczby punkt√≥w danych jako liczby przedzia≈Ç√≥w.\n\n\n\n\nHistogramy w Podstawowym R\n\n# Histogram Czƒôsto≈õci\nhist(wyniki, breaks = 7, \n     main = \"Histogram Czƒôsto≈õci Wynik√≥w Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Czƒôsto≈õƒá\")\n\n\n\n\n\n\n\n# Histogram Czƒôsto≈õci Wzglƒôdnej\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Czƒôsto≈õci Wzglƒôdnej Wynik√≥w Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Czƒôsto≈õƒá Wzglƒôdna\")\n\n\n\n\n\n\n\n# Histogram Gƒôsto≈õci\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Gƒôsto≈õci Wynik√≥w Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Gƒôsto≈õƒá\")\nlines(density(wyniki), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\nHistogramy z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wynik = wyniki)\n\n# Histogram Czƒôsto≈õci\nggplot(df, aes(x = wynik)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Czƒôsto≈õci Wynik√≥w Egzaminu\",\n       x = \"Wyniki\", y = \"Czƒôsto≈õƒá\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram Czƒôsto≈õci Wzglƒôdnej\nggplot(df, aes(x = wynik, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Czƒôsto≈õci Wzglƒôdnej Wynik√≥w Egzaminu\",\n       x = \"Wyniki\", y = \"Czƒôsto≈õƒá Wzglƒôdna\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Histogram Gƒôsto≈õci\nggplot(df, aes(x = wynik)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Histogram Gƒôsto≈õci Wynik√≥w Egzaminu\",\n       x = \"Wyniki\", y = \"Gƒôsto≈õƒá\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nInterpretacja Histogram√≥w\nPodczas interpretacji histogramu zwr√≥ƒá uwagƒô na:\n\nTendencjƒô CentralnƒÖ: Gdzie znajduje siƒô szczyt rozk≈Çadu?\nRozrzut: Jak szeroki jest rozk≈Çad?\nKszta≈Çt: Czy jest symetryczny, sko≈õny, czy wielomodalny?\nWarto≈õci OdstajƒÖce: Czy sƒÖ nietypowe warto≈õci daleko od g≈Ç√≥wnego rozk≈Çadu?",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Wizualizacja Danych: z przyk≈Çadami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-pude≈Çkowe-i-wykresy-pude≈Çkowe-tukeya",
    "href": "rozdzial6.html#wykresy-pude≈Çkowe-i-wykresy-pude≈Çkowe-tukeya",
    "title": "8¬† Wizualizacja Danych: z przyk≈Çadami w R",
    "section": "8.4 Wykresy Pude≈Çkowe i Wykresy Pude≈Çkowe Tukeya",
    "text": "8.4 Wykresy Pude≈Çkowe i Wykresy Pude≈Çkowe Tukeya\nWykresy pude≈Çkowe, znane r√≥wnie≈º jako wykresy skrzynkowe, dostarczajƒÖ zwiƒôz≈Çego podsumowania rozk≈Çadu. Skupimy siƒô na wykresie pude≈Çkowym w stylu Tukeya, nazwanym na cze≈õƒá statystyka Johna Tukeya, kt√≥ry spopularyzowa≈Ç ten typ wykresu.\n\nZrozumienie Wykres√≥w Pude≈Çkowych\nWykres pude≈Çkowy przedstawia piƒôƒá kluczowych statystyk:\n\nWarto≈õƒá minimalna (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych)\nPierwszy kwartyl (Q1)\nMediana\nTrzeci kwartyl (Q3)\nWarto≈õƒá maksymalna (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych)\n\nDodatkowo wykresy pude≈Çkowe pokazujƒÖ:\n\nWƒÖsy: Linie rozciƒÖgajƒÖce siƒô od pude≈Çka do warto≈õci minimalnej i maksymalnej (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych)\nWarto≈õci odstajƒÖce: Indywidualne punkty poza wƒÖsami\n\n\nObliczanie Kwartyli i Warto≈õci OdstajƒÖcych\nAby stworzyƒá wykres pude≈Çkowy, postƒôpuj zgodnie z tymi krokami:\n\nUporzƒÖdkuj dane od najmniejszej do najwiƒôkszej warto≈õci.\nZnajd≈∫ medianƒô (≈õrodkowa warto≈õƒá dla nieparzystej liczby punkt√≥w danych, ≈õrednia z dw√≥ch ≈õrodkowych warto≈õci dla parzystej).\nZnajd≈∫ Q1 (mediana dolnej po≈Çowy danych) i Q3 (mediana g√≥rnej po≈Çowy danych).\nOblicz Rozstƒôp Miƒôdzykwartylowy (IQR) = Q3 - Q1\nOkre≈õl warto≈õci odstajƒÖce u≈ºywajƒÖc regu≈Çy Tukeya:\n\nDolne warto≈õci odstajƒÖce: &lt; Q1 - 1.5 * IQR\nG√≥rne warto≈õci odstajƒÖce: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nWsp√≥≈Çczynnik 1.5 w regule Tukeya dla warto≈õci odstajƒÖcych opiera siƒô na w≈Ça≈õciwo≈õciach rozk≈Çadu normalnego. Dla danych o rozk≈Çadzie normalnym, ta regu≈Ça identyfikuje oko≈Ço 0.7% danych jako potencjalne warto≈õci odstajƒÖce.\n\n\n\n\nPrzyk≈Çadowe Dane\nU≈ºyjmy ma≈Çego zbioru danych do ilustracji:\n\ndane &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\nRƒôcznie Rysowany Wykres Pude≈Çkowy Tukeya\nAby stworzyƒá wykres pude≈Çkowy Tukeya rƒôcznie:\n\nNarysuj liniƒô pionowƒÖ reprezentujƒÖcƒÖ zakres od minimum do maksimum (2 do 15 w naszym przyk≈Çadzie, z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcej).\nNarysuj pude≈Çko od Q1 do Q3.\nNarysuj poziomƒÖ liniƒô przez pude≈Çko na poziomie mediany.\nNarysuj wƒÖsy od pude≈Çka do warto≈õci minimalnej i maksymalnej (z wy≈ÇƒÖczeniem warto≈õci odstajƒÖcych).\nPrzedstaw warto≈õƒá odstajƒÖcƒÖ (50) jako indywidualny punkt poza wƒÖsem.\nDodaj skalƒô do osi pionowej i oznacz jƒÖ.\n\n\n\nWykres Pude≈Çkowy w Podstawowym R\n\n# Tworzenie wykresu pude≈Çkowego\nboxplot(dane, main = \"Wykres Pude≈Çkowy Przyk≈Çadowych Danych\",\n        ylab = \"Warto≈õci\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\nWykres Pude≈Çkowy Tukeya z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wartosc = dane)\n\n# Tworzenie wykresu pude≈Çkowego Tukeya z ggplot2\nggplot(df, aes(x = \"\", y = wartosc)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Wykres Pude≈Çkowy Tukeya Przyk≈Çadowych Danych\",\n       x = \"\", y = \"Warto≈õci\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\nInterpretacja Wykres√≥w Pude≈Çkowych\nPodczas interpretacji wykresu pude≈Çkowego zwr√≥ƒá uwagƒô na nastƒôpujƒÖce elementy:\n\nTendencja Centralna: Mediana pokazuje ≈õrodek rozk≈Çadu.\nRozrzut: Pude≈Çko (IQR) reprezentuje ≈õrodkowe 50% danych.\nSko≈õno≈õƒá: Je≈õli linia mediany jest bli≈ºej jednego ko≈Ñca pude≈Çka, rozk≈Çad jest sko≈õny.\nWarto≈õci OdstajƒÖce: Punkty poza wƒÖsami sƒÖ potencjalnymi warto≈õciami odstajƒÖcymi.\nPor√≥wnania: Przy por√≥wnywaniu wielu wykres√≥w pude≈Çkowych, zwr√≥ƒá uwagƒô na wzglƒôdne po≈Ço≈ºenie median, rozmiary pude≈Çek i obecno≈õƒá warto≈õci odstajƒÖcych.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Wizualizacja Danych: z przyk≈Çadami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "href": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "title": "8¬† Wizualizacja Danych: z przyk≈Çadami w R",
    "section": "8.5 Zaawansowane Techniki Wizualizacji",
    "text": "8.5 Zaawansowane Techniki Wizualizacji\nOpr√≥cz podstawowych typ√≥w wykres√≥w, warto poznaƒá kilka bardziej zaawansowanych technik wizualizacji, kt√≥re mogƒÖ byƒá przydatne w analizie danych.\n\nWykresy Skrzypcowe\nWykresy skrzypcowe ≈ÇƒÖczƒÖ cechy wykres√≥w pude≈Çkowych i wykres√≥w gƒôsto≈õci, dajƒÖc bardziej kompletny obraz rozk≈Çadu danych.\n\n# Tworzenie wykresu skrzypcowego\nggplot(df, aes(x = \"\", y = wartosc)) +\n  geom_violin(fill = \"lightblue\") +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Wykres Skrzypcowy Przyk≈Çadowych Danych\",\n       x = \"\", y = \"Warto≈õci\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWykresy Rozrzutu z Marginesami\n≈ÅƒÖczenie wykres√≥w rozrzutu z histogramami na marginesach mo≈ºe dostarczyƒá wiƒôcej informacji o rozk≈Çadzie danych w dw√≥ch wymiarach.\n\n# Generowanie danych do wykresu rozrzutu\nset.seed(123)\ndf_scatter &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100)\n)\n\n# Tworzenie wykresu rozrzutu z marginesami\nlibrary(ggExtra)\np &lt;- ggplot(df_scatter, aes(x = x, y = y)) +\n  geom_point() +\n  theme_minimal()\nggMarginal(p, type = \"histogram\", fill = \"lightblue\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Wizualizacja Danych: z przyk≈Çadami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wnioski",
    "href": "rozdzial6.html#wnioski",
    "title": "8¬† Wizualizacja Danych: z przyk≈Çadami w R",
    "section": "8.6 Wnioski",
    "text": "8.6 Wnioski\nW tym rozdziale poznali≈õmy trzy podstawowe typy wizualizacji danych: wykresy s≈Çupkowe, histogramy i wykresy pude≈Çkowe. Pokazali≈õmy, jak tworzyƒá te wykresy rƒôcznie, u≈ºywajƒÖc podstawowego systemu wykres√≥w R oraz biblioteki ggplot2.\nKa≈ºdy typ wykresu s≈Çu≈ºy innemu celowi: - Wykresy s≈Çupkowe doskonale nadajƒÖ siƒô do por√≥wnywania kategorii. - Histogramy pokazujƒÖ rozk≈Çad zmiennej ciƒÖg≈Çej. - Wykresy pude≈Çkowe dostarczajƒÖ zwiƒôz≈Çego podsumowania rozk≈Çadu, podkre≈õlajƒÖc tendencjƒô centralnƒÖ, rozrzut i warto≈õci odstajƒÖce.\nPamiƒôtaj, ≈ºe wyb√≥r wizualizacji zale≈ºy od typu danych i wniosk√≥w, kt√≥re chcesz przekazaƒá. Zawsze bierz pod uwagƒô swojƒÖ docelowƒÖ grupƒô odbiorc√≥w i historiƒô, kt√≥rƒÖ chcesz opowiedzieƒá za pomocƒÖ swoich danych, wybierajƒÖc i projektujƒÖc wizualizacje.\nƒÜwicz tworzenie tych wykres√≥w rƒôcznie, aby pog≈Çƒôbiƒá zrozumienie ich konstrukcji i interpretacji. Nastƒôpnie wykorzystaj moc R i ggplot2, aby szybko tworzyƒá i dostosowywaƒá te wizualizacje dla wiƒôkszych zbior√≥w danych i bardziej z≈Ço≈ºonych analiz.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Wizualizacja Danych: z przyk≈Çadami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#ƒáwiczenia-praktyczne",
    "href": "rozdzial6.html#ƒáwiczenia-praktyczne",
    "title": "8¬† Wizualizacja Danych: z przyk≈Çadami w R",
    "section": "8.7 ƒÜwiczenia Praktyczne",
    "text": "8.7 ƒÜwiczenia Praktyczne\n\nZbierz dane o popularno≈õci r√≥≈ºnych gatunk√≥w muzycznych w≈õr√≥d Twoich znajomych. Stw√≥rz wykres s≈Çupkowy przedstawiajƒÖcy te dane.\nZmierz czas reakcji 30 os√≥b na bodziec d≈∫wiƒôkowy (w milisekundach). Utw√≥rz histogram tych danych.\nZbierz dane o wzro≈õcie 50 os√≥b w Twojej spo≈Çeczno≈õci. Stw√≥rz wykres pude≈Çkowy dla tych danych, osobno dla mƒô≈ºczyzn i kobiet.\nZnajd≈∫ zestaw danych online (np. na Kaggle) i stw√≥rz trzy r√≥≈ºne wizualizacje dla tych danych. Opisz, jakie wnioski mo≈ºna wyciƒÖgnƒÖƒá z ka≈ºdej wizualizacji.\nStw√≥rz wykres skrzypcowy dla danych o cenach dom√≥w w r√≥≈ºnych dzielnicach miasta. Por√≥wnaj go z wykresem pude≈Çkowym tych samych danych. Jakie dodatkowe informacje dostarcza wykres skrzypcowy?\n\nPamiƒôtaj, ≈ºe praktyka jest kluczem do opanowania sztuki wizualizacji danych. Eksperymentuj z r√≥≈ºnymi typami wykres√≥w i parametrami, aby znale≈∫ƒá najlepszy spos√≥b przedstawienia swoich danych.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Wizualizacja Danych: z przyk≈Çadami w R</span>"
    ]
  },
  {
    "objectID": "correg_en.html",
    "href": "correg_en.html",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "",
    "text": "9.1 Introduction\nThe distinction between correlation and causation represents a fundamental challenge in statistical analysis. While correlation measures the statistical association between variables, causation implies a direct influence of one variable on another.\nStatistical relationships form the backbone of data-driven decision making across disciplines‚Äîfrom economics and public health to psychology and environmental science. Understanding when a relationship indicates mere association versus genuine causality is crucial for valid inference and effective policy recommendations.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#covariance",
    "href": "correg_en.html#covariance",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.2 Covariance",
    "text": "9.2 Covariance\nCovariance measures how two variables vary together, indicating both the direction and magnitude of their linear relationship.\nFormula: \\text{cov}(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\nWhere:\n\nx_i and y_i are individual data points\n\\bar{x} and \\bar{y} are the means of variables X and Y\nn is the number of observations\nWe divide by (n-1) for sample covariance (Bessel‚Äôs correction)\n\n\nStep-by-Step Manual Calculation Process\nExample 1: Student Study Hours vs.¬†Test Scores\nData:\n\nStudy Hours (X): 2, 4, 6, 8, 10\nTest Scores (Y): 65, 70, 80, 85, 95\n\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate means\n\\bar{x} = \\frac{2+4+6+8+10}{5} = 6 hours\n\n\n\n\n\\bar{y} = \\frac{65+70+80+85+95}{5} = 79 points\n\n\n2\nCalculate deviations\n(x_i - \\bar{x}): -4, -2, 0, 2, 4\n\n\n\n\n(y_i - \\bar{y}): -14, -9, 1, 6, 16\n\n\n3\nCalculate products\n(x_i - \\bar{x})(y_i - \\bar{y}):\n\n\n\n\n(-4)(-14) = 56\n\n\n\n\n(-2)(-9) = 18\n\n\n\n\n(0)(1) = 0\n\n\n\n\n(2)(6) = 12\n\n\n\n\n(4)(16) = 64\n\n\n4\nSum the products\n\\sum = 56 + 18 + 0 + 12 + 64 = 150\n\n\n5\nDivide by (n-1)\n\\text{cov}(X,Y) = \\frac{150}{5-1} = \\frac{150}{4} = 37.5\n\n\n\nR Verification:\n\n# Define the data\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Calculate covariance\ncov_manual &lt;- cov(study_hours, test_scores)\nprint(paste(\"Covariance:\", round(cov_manual, 2)))\n\n[1] \"Covariance: 37.5\"\n\n# Verify step by step\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\nproducts &lt;- x_dev * y_dev\ncov_calculated &lt;- sum(products) / (length(study_hours) - 1)\n\n# Display calculation steps\ndata.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_deviation = x_dev,\n  Y_deviation = y_dev,\n  Product = products\n)\n\n   X  Y X_deviation Y_deviation Product\n1  2 65          -4         -14      56\n2  4 70          -2          -9      18\n3  6 80           0           1       0\n4  8 85           2           6      12\n5 10 95           4          16      64\n\n\nInterpretation: The positive covariance (37.5) indicates that study hours and test scores tend to increase together.\n\n\nPractice Problem with Solution\nCalculate covariance manually for:\n\nTemperature (¬∞F): 32, 50, 68, 86, 95\nIce Cream Sales ($): 100, 200, 400, 600, 800\n\nSolution:\n\n\n\nStep\nCalculation\n\n\n\n\n1. Means\n\\bar{x} = \\frac{32+50+68+86+95}{5} = 66.2¬∞F\n\n\n\n\\bar{y} = \\frac{100+200+400+600+800}{5} = 420\n\n\n2. Deviations\nX: -34.2, -16.2, 1.8, 19.8, 28.8\n\n\n\nY: -320, -220, -20, 180, 380\n\n\n3. Products\n10944, 3564, -36, 3564, 10944\n\n\n4. Sum\n28980\n\n\n5. Covariance\n\\frac{28980}{4} = 7245\n\n\n\n\n# Verify practice problem\ntemp &lt;- c(32, 50, 68, 86, 95)\nsales &lt;- c(100, 200, 400, 600, 800)\nprint(paste(\"Covariance for practice problem:\", round(cov(temp, sales), 2)))\n\n[1] \"Covariance for practice problem: 7245\"",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#correlation-coefficient",
    "href": "correg_en.html#correlation-coefficient",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.3 Correlation Coefficient",
    "text": "9.3 Correlation Coefficient\nThe correlation coefficient standardizes covariance to eliminate scale dependency, producing values between -1 and +1.\n\nInterpretation Guidelines\n\n\n\n\n\n\n\n\n\nCorrelation Value\nStrength\nInterpretation\nExample\n\n\n\n\n¬±0.90 to ¬±1.00\nVery Strong\nAlmost perfect relationship\nHeight of parents and children\n\n\n¬±0.70 to ¬±0.89\nStrong\nHighly related variables\nStudy time and grades\n\n\n¬±0.50 to ¬±0.69\nModerate\nModerately related\nExercise and weight loss\n\n\n¬±0.30 to ¬±0.49\nWeak\nWeakly related\nShoe size and reading ability\n\n\n¬±0.00 to ¬±0.29\nVery Weak/None\nLittle to no relationship\nBirth month and intelligence\n\n\n\n\n\nTypes of Correlations Visualization\n\n# Generate sample data with different correlation patterns\nn &lt;- 100\n\n# Positive linear correlation\nset.seed(123)\nyears_education &lt;- rnorm(n, 14, 3)\nannual_income &lt;- 15000 + 3500 * years_education + rnorm(n, 0, 10000)\npos_cor &lt;- round(cor(years_education, annual_income), 3)\n\n# Negative linear correlation\nscreen_time &lt;- runif(n, 1, 8)\nsleep_hours &lt;- 9 - 0.5 * screen_time + rnorm(n, 0, 0.5)\nneg_cor &lt;- round(cor(screen_time, sleep_hours), 3)\n\n# No correlation\nx_random &lt;- rnorm(n, 50, 10)\ny_random &lt;- rnorm(n, 50, 10)\nno_cor &lt;- round(cor(x_random, y_random), 3)\n\n# Non-linear correlation (quadratic)\nhours_studied &lt;- runif(n, 0, 12)\ntest_score &lt;- -2 * (hours_studied - 6)^2 + 90 + rnorm(n, 0, 5)\nnonlin_cor &lt;- round(cor(hours_studied, test_score), 3)\n\n# Create data frames with correlation values\npositive_data &lt;- data.frame(\n  x = years_education, \n  y = annual_income,\n  label = paste0(\"Positive Linear (r = \", pos_cor, \")\")\n)\n\nnegative_data &lt;- data.frame(\n  x = screen_time, \n  y = sleep_hours,\n  label = paste0(\"Negative Linear (r = \", neg_cor, \")\")\n)\n\nno_corr_data &lt;- data.frame(\n  x = x_random, \n  y = y_random,\n  label = paste0(\"No Correlation (r = \", no_cor, \")\")\n)\n\nnonlinear_data &lt;- data.frame(\n  x = hours_studied, \n  y = test_score,\n  label = paste0(\"Non-linear (Pearson r = \", nonlin_cor, \")\")\n)\n\n# Combine data\nall_data &lt;- rbind(positive_data, negative_data, no_corr_data, nonlinear_data)\n\n# Create faceted plot\nggplot(all_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE, alpha = 0.3) +\n  facet_wrap(~ label, scales = \"free\", ncol = 2) +\n  labs(\n    title = \"Different Types of Correlations\",\n    subtitle = \"Linear regression line shown in red with confidence band\",\n    x = \"\", \n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, color = \"gray50\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#pearson-correlation",
    "href": "correg_en.html#pearson-correlation",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.4 Pearson Correlation",
    "text": "9.4 Pearson Correlation\nFormula: r = \\frac{\\text{cov}(X,Y)}{s_X s_Y} = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}}\n\nComplete Manual Calculation Example\nUsing our study hours example:\n\nStudy Hours (X): 2, 4, 6, 8, 10\nTest Scores (Y): 65, 70, 80, 85, 95\n\nDetailed Calculation Steps:\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate covariance\nFrom above: \\text{cov}(X,Y) = 37.5\n\n\n2\nCalculate deviations squared\n\n\n\n\nFor X\n(x_i - \\bar{x})^2: 16, 4, 0, 4, 16\n\n\n\n\nSum = 40\n\n\n\nFor Y\n(y_i - \\bar{y})^2: 196, 81, 1, 36, 256\n\n\n\n\nSum = 570\n\n\n3\nCalculate standard deviations\n\n\n\n\ns_X\ns_X = \\sqrt{\\frac{40}{4}} = \\sqrt{10} = 3.162\n\n\n\ns_Y\ns_Y = \\sqrt{\\frac{570}{4}} = \\sqrt{142.5} = 11.937\n\n\n4\nCalculate correlation\nr = \\frac{37.5}{3.162 \\times 11.937}\n\n\n\n\nr = \\frac{37.5}{37.73} = 0.994\n\n\n\n\n# Manual calculation verification\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Calculate Pearson correlation\nr_value &lt;- cor(study_hours, test_scores, method = \"pearson\")\nprint(paste(\"Pearson correlation coefficient:\", round(r_value, 3)))\n\n[1] \"Pearson correlation coefficient: 0.993\"\n\n# Detailed calculation\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\n\n# Show calculation table\ncalc_table &lt;- data.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_dev = x_dev,\n  Y_dev = y_dev,\n  X_dev_sq = x_dev^2,\n  Y_dev_sq = y_dev^2,\n  Product = x_dev * y_dev\n)\nprint(calc_table)\n\n   X  Y X_dev Y_dev X_dev_sq Y_dev_sq Product\n1  2 65    -4   -14       16      196      56\n2  4 70    -2    -9        4       81      18\n3  6 80     0     1        0        1       0\n4  8 85     2     6        4       36      12\n5 10 95     4    16       16      256      64\n\n# Summary statistics\ncat(\"\\nSummary:\")\n\n\nSummary:\n\ncat(\"\\nSum of (X-mean)¬≤:\", sum(x_dev^2))\n\n\nSum of (X-mean)¬≤: 40\n\ncat(\"\\nSum of (Y-mean)¬≤:\", sum(y_dev^2))\n\n\nSum of (Y-mean)¬≤: 570\n\ncat(\"\\nSum of products:\", sum(x_dev * y_dev))\n\n\nSum of products: 150\n\ncat(\"\\nStandard deviation of X:\", sd(study_hours))\n\n\nStandard deviation of X: 3.162278\n\ncat(\"\\nStandard deviation of Y:\", sd(test_scores))\n\n\nStandard deviation of Y: 11.93734\n\n# Calculate confidence interval and p-value\ncor_test &lt;- cor.test(study_hours, test_scores, method = \"pearson\")\nprint(cor_test)\n\n\n    Pearson's product-moment correlation\n\ndata:  study_hours and test_scores\nt = 15, df = 3, p-value = 0.0006431\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8994446 0.9995859\nsample estimates:\n      cor \n0.9933993 \n\n\nInterpretation: r = 0.994 indicates an almost perfect positive linear relationship between study hours and test scores. The p-value &lt; 0.05 suggests this relationship is statistically significant.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#spearman-rank-correlation",
    "href": "correg_en.html#spearman-rank-correlation",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.5 Spearman Rank Correlation",
    "text": "9.5 Spearman Rank Correlation\nSpearman correlation measures monotonic relationships using ranks instead of raw values.\nFormula: \\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2 - 1)}\nWhere d_i is the difference between ranks for observation i.\n\nComplete Manual Example\nData: Math and English Scores\n\n\n\nStudent\nMath Score\nEnglish Score\n\n\n\n\nA\n78\n82\n\n\nB\n85\n90\n\n\nC\n88\n88\n\n\nD\n92\n94\n\n\nE\n95\n96\n\n\n\nRanking and Calculation:\n\n\n\n\n\n\n\n\n\n\n\n\nStudent\nMath Score\nMath Rank\nEnglish Score\nEnglish Rank\nd = (Math Rank - English Rank)\nd¬≤\n\n\n\n\nA\n78\n1\n82\n1\n0\n0\n\n\nB\n85\n2\n90\n3\n-1\n1\n\n\nC\n88\n3\n88\n2\n1\n1\n\n\nD\n92\n4\n94\n4\n0\n0\n\n\nE\n95\n5\n96\n5\n0\n0\n\n\n\n\n\n\n\nSum:\n2\n\n\n\nCalculation: \\rho = 1 - \\frac{6 \\times 2}{5(25-1)} = 1 - \\frac{12}{120} = 1 - 0.1 = 0.9\n\n# Data\nmath_scores &lt;- c(78, 85, 88, 92, 95)\nenglish_scores &lt;- c(82, 90, 88, 94, 96)\n\n# Show ranks\nrank_table &lt;- data.frame(\n  Student = LETTERS[1:5],\n  Math_Score = math_scores,\n  Math_Rank = rank(math_scores),\n  English_Score = english_scores,\n  English_Rank = rank(english_scores)\n)\n\nrank_table$d &lt;- rank_table$Math_Rank - rank_table$English_Rank\nrank_table$d_squared &lt;- rank_table$d^2\n\nprint(rank_table)\n\n  Student Math_Score Math_Rank English_Score English_Rank  d d_squared\n1       A         78         1            82            1  0         0\n2       B         85         2            90            3 -1         1\n3       C         88         3            88            2  1         1\n4       D         92         4            94            4  0         0\n5       E         95         5            96            5  0         0\n\ncat(\"\\nSum of d¬≤:\", sum(rank_table$d_squared))\n\n\nSum of d¬≤: 2\n\n# Calculate Spearman correlation\nrho &lt;- cor(math_scores, english_scores, method = \"spearman\")\ncat(\"\\nSpearman correlation (R):\", round(rho, 3))\n\n\nSpearman correlation (R): 0.9\n\n# Manual calculation\nn &lt;- length(math_scores)\nrho_manual &lt;- 1 - (6 * sum(rank_table$d_squared)) / (n * (n^2 - 1))\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.9",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#cross-tabulation-and-categorical-data",
    "href": "correg_en.html#cross-tabulation-and-categorical-data",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.6 Cross-tabulation and Categorical Data",
    "text": "9.6 Cross-tabulation and Categorical Data\nCross-tabulation shows relationships between categorical variables.\n\n# Create more realistic sample data\nset.seed(123)\nn_total &lt;- 120\n\n# Create education and employment data with realistic relationship\neducation &lt;- factor(\n  c(rep(\"High School\", 40), \n    rep(\"College\", 50), \n    rep(\"Graduate\", 30)),\n  levels = c(\"High School\", \"College\", \"Graduate\")\n)\n\n# Employment status with education-related probabilities\nemployment &lt;- factor(\n  c(# High School: 75% employed\n    sample(c(rep(\"Employed\", 30), rep(\"Unemployed\", 10)), 40),\n    # College: 90% employed  \n    sample(c(rep(\"Employed\", 45), rep(\"Unemployed\", 5)), 50),\n    # Graduate: 95% employed\n    sample(c(rep(\"Employed\", 28), rep(\"Unemployed\", 2)), 30)),\n  levels = c(\"Employed\", \"Unemployed\")\n)\n\n# Create contingency table\ncontingency_table &lt;- table(education, employment)\nprint(\"Contingency Table:\")\n\n[1] \"Contingency Table:\"\n\nprint(contingency_table)\n\n             employment\neducation     Employed Unemployed\n  High School       30         10\n  College           45          5\n  Graduate          28          2\n\n# Calculate row percentages\nprint(\"\\nRow Percentages (% within each education level):\")\n\n[1] \"\\nRow Percentages (% within each education level):\"\n\nprint(round(prop.table(contingency_table, 1) * 100, 1))\n\n             employment\neducation     Employed Unemployed\n  High School     75.0       25.0\n  College         90.0       10.0\n  Graduate        93.3        6.7\n\n# Chi-square test for independence\nchi_test &lt;- chisq.test(contingency_table)\n\nWarning in chisq.test(contingency_table): Chi-squared approximation may be\nincorrect\n\nprint(chi_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 5.9623, df = 2, p-value = 0.05073",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#practical-exercises-with-solutions",
    "href": "correg_en.html#practical-exercises-with-solutions",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.7 Practical Exercises with Solutions",
    "text": "9.7 Practical Exercises with Solutions\n\nExercise 1: Calculate Pearson Correlation Manually\nData:\n\nHeight (inches): 66, 68, 70, 72, 74\nWeight (pounds): 140, 155, 170, 185, 200\n\nSolution:\n\n# Data\nheight &lt;- c(66, 68, 70, 72, 74)\nweight &lt;- c(140, 155, 170, 185, 200)\n\n# Step 1: Calculate means\nh_mean &lt;- mean(height)  # 70\nw_mean &lt;- mean(weight)  # 170\n\n# Step 2: Calculate deviations and products\ncalculation_df &lt;- data.frame(\n  Height = height,\n  Weight = weight,\n  H_dev = height - h_mean,\n  W_dev = weight - w_mean,\n  H_dev_sq = (height - h_mean)^2,\n  W_dev_sq = (weight - w_mean)^2,\n  Product = (height - h_mean) * (weight - w_mean)\n)\n\nprint(calculation_df)\n\n  Height Weight H_dev W_dev H_dev_sq W_dev_sq Product\n1     66    140    -4   -30       16      900     120\n2     68    155    -2   -15        4      225      30\n3     70    170     0     0        0        0       0\n4     72    185     2    15        4      225      30\n5     74    200     4    30       16      900     120\n\n# Step 3: Calculate correlation\ncov_hw &lt;- sum(calculation_df$Product) / (length(height) - 1)\nsd_h &lt;- sqrt(sum(calculation_df$H_dev_sq) / (length(height) - 1))\nsd_w &lt;- sqrt(sum(calculation_df$W_dev_sq) / (length(weight) - 1))\nr_manual &lt;- cov_hw / (sd_h * sd_w)\n\ncat(\"\\nManual calculation:\")\n\n\nManual calculation:\n\ncat(\"\\nCovariance:\", round(cov_hw, 2))\n\n\nCovariance: 75\n\ncat(\"\\nSD Height:\", round(sd_h, 2))\n\n\nSD Height: 3.16\n\ncat(\"\\nSD Weight:\", round(sd_w, 2))\n\n\nSD Weight: 23.72\n\ncat(\"\\nCorrelation:\", round(r_manual, 3))\n\n\nCorrelation: 1\n\n# Verify with R\ncat(\"\\n\\nR calculation:\", round(cor(height, weight), 3))\n\n\n\nR calculation: 1\n\n\n\n\nExercise 2: Calculate Spearman Correlation Manually\nData:\n\nStudent rankings in Math: 1, 3, 2, 5, 4\nStudent rankings in Science: 2, 4, 1, 5, 3\n\nSolution:\n\n# Rankings (already ranked)\nmath_rank &lt;- c(1, 3, 2, 5, 4)\nscience_rank &lt;- c(2, 4, 1, 5, 3)\n\n# Calculate differences\nd &lt;- math_rank - science_rank\nd_squared &lt;- d^2\n\n# Create table\nrank_df &lt;- data.frame(\n  Student = 1:5,\n  Math_Rank = math_rank,\n  Science_Rank = science_rank,\n  d = d,\n  d_squared = d_squared\n)\n\nprint(rank_df)\n\n  Student Math_Rank Science_Rank  d d_squared\n1       1         1            2 -1         1\n2       2         3            4 -1         1\n3       3         2            1  1         1\n4       4         5            5  0         0\n5       5         4            3  1         1\n\n# Calculate Spearman correlation\nn &lt;- length(math_rank)\nsum_d_sq &lt;- sum(d_squared)\nrho_manual &lt;- 1 - (6 * sum_d_sq) / (n * (n^2 - 1))\n\ncat(\"\\nSum of d¬≤:\", sum_d_sq)\n\n\nSum of d¬≤: 4\n\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.8\n\ncat(\"\\nSpearman correlation (R):\", round(cor(math_rank, science_rank, method = \"spearman\"), 3))\n\n\nSpearman correlation (R): 0.8\n\n\n\n\nExercise 3: Interpretation Practice\nInterpret these correlation values:\n\nr = 0.85 between hours of practice and performance score\n\nAnswer: Strong positive relationship. As practice hours increase, performance scores tend to increase substantially.\n\nr = -0.72 between outside temperature and heating costs\n\nAnswer: Strong negative relationship. As temperature increases, heating costs decrease substantially.\n\nr = 0.12 between shoe size and intelligence\n\nAnswer: Very weak/no meaningful relationship. Shoe size and intelligence are essentially unrelated.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#important-points-to-remember",
    "href": "correg_en.html#important-points-to-remember",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.8 Important Points to Remember",
    "text": "9.8 Important Points to Remember\n\nCorrelation measures relationship strength: Values range from -1 to +1\nCorrelation ‚â† Causation: High correlation doesn‚Äôt prove one variable causes another\nChoose the right method:\n\nPearson: For linear relationships in continuous data\nSpearman: For monotonic relationships or ranked data\n\nCheck assumptions:\n\nPearson assumes linear relationship and normal distribution\nSpearman only assumes monotonic relationship\n\nWatch for outliers: Extreme values can greatly affect Pearson correlation\nVisualize your data: Always plot before calculating correlation",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-decision-tree-for-correlation-analysis",
    "href": "correg_en.html#summary-decision-tree-for-correlation-analysis",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.9 Summary: Decision Tree for Correlation Analysis",
    "text": "9.9 Summary: Decision Tree for Correlation Analysis\n\n\n\nCHOOSING THE RIGHT CORRELATION METHOD:\n\nIs your data numerical?\n‚îú‚îÄ YES ‚Üí Is the relationship linear?\n‚îÇ   ‚îú‚îÄ YES ‚Üí Use PEARSON correlation\n‚îÇ   ‚îî‚îÄ NO ‚Üí Is it monotonic?\n‚îÇ       ‚îú‚îÄ YES ‚Üí Use SPEARMAN correlation\n‚îÇ       ‚îî‚îÄ NO ‚Üí Consider non-linear methods\n‚îî‚îÄ NO ‚Üí Is it ordinal (ranked)?\n    ‚îú‚îÄ YES ‚Üí Use SPEARMAN correlation\n    ‚îî‚îÄ NO ‚Üí Use CROSS-TABULATION for categorical data\n\n\n\nQuick Reference Card\n\n\n\n\n\n\n\n\n\nMeasure\nUse When\nFormula\nRange\n\n\n\n\nCovariance\nInitial exploration of relationship\n\\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n-‚àû to +‚àû\n\n\nPearson r\nLinear relationships, continuous data\n\\frac{\\text{cov}(X,Y)}{s_X s_Y}\n-1 to +1\n\n\nSpearman œÅ\nMonotonic relationships, ranked data\n1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n-1 to +1\n\n\nCross-tabs\nCategorical variables\nFrequency counts\nN/A",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#understanding-ordinary-least-squares-ols-a-quick-start-guide",
    "href": "correg_en.html#understanding-ordinary-least-squares-ols-a-quick-start-guide",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.10 Understanding Ordinary Least Squares (OLS): A Quick-start Guide",
    "text": "9.10 Understanding Ordinary Least Squares (OLS): A Quick-start Guide\n\n\n\n\n\n\nUnderstanding Ordinary Least Squares (OLS): A Quick-start Guide\n\n\n\n\nIntroduction: What is Regression Analysis?\nRegression analysis helps us understand and measure relationships between things we can observe. It provides mathematical tools to identify patterns in data that help us make predictions.\nConsider these research questions:\n\nHow does study time affect test scores?\nHow does experience affect salary?\nHow does advertising spending influence sales?\n\nRegression gives us systematic methods to answer these questions with real data.\n\n\nThe Starting Point: A Simple Example\nLet‚Äôs begin with something concrete. You‚Äôve collected data from 20 students in your class:\n\n\n\nStudent\nStudy Hours\nExam Score\n\n\n\n\nAlex\n2\n68\n\n\nBeth\n4\n74\n\n\nCarlos\n6\n85\n\n\nDiana\n8\n91\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\nWhen you plot this data, you get a scatter plot with dots all over. Your goal: find the straight line that best describes the relationship between study hours and exam scores.\nBut what does ‚Äúbest‚Äù mean? That‚Äôs what we‚Äôll discover.\n\n\nWhy Real Data Doesn‚Äôt Fall on a Perfect Line\nBefore diving into the math, let‚Äôs understand why data points don‚Äôt line up perfectly.\n\nDeterministic vs.¬†Stochastic Models\nDeterministic Models describe relationships with no uncertainty. Think of physics equations: \\text{Distance} = \\text{Speed} √ó \\text{Time}\nIf you drive at exactly 60 mph for exactly 2 hours, you‚Äôll always travel exactly 120 miles. No variation, no exceptions.\nStochastic Models acknowledge that real-world data contains natural variation. The fundamental structure is: Y = f(X) + \\epsilon\nWhere:\n\nY is what we‚Äôre trying to predict (exam scores)\nf(X) is the systematic pattern (how study hours typically affect scores)\n\\epsilon (epsilon) represents all the random stuff we can‚Äôt measure\n\nIn our example, two students might study for 5 hours but get different scores because:\n\nOne slept better the night before\nOne is naturally better at test-taking\nOne had a noisy roommate during the exam\nPure chance in which questions were asked\n\nThis randomness is natural and expected - that‚Äôs what \\epsilon captures.\n\n\n\nThe Simple Linear Regression Model\nWe express the relationship between study hours and exam scores as: Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\nLet‚Äôs decode this:\n\nY_i = exam score for student i\nX_i = study hours for student i\n\\beta_0 = the intercept (baseline score with zero study hours)\n\\beta_1 = the slope (points gained per study hour)\n\\epsilon_i = everything else affecting student i‚Äôs score\n\nKey insight: We never know the true values of \\beta_0 and \\beta_1. Instead, we use our data to estimate them, calling our estimates \\hat{\\beta}_0 and \\hat{\\beta}_1 (the ‚Äúhats‚Äù mean ‚Äúestimated‚Äù).\n\n\nUnderstanding Residuals: How Wrong Are Our Predictions?\nOnce we draw a line through our data, we can make predictions. For each student:\n\nActual score (y_i): What they really got\nPredicted score (\\hat{y}_i): What our line says they should have gotten\nResidual (e_i): The difference = Actual - Predicted\n\nVisual Example:\nDiana: Studied 8 hours, scored 91\nOur line predicts: 88 points\nResidual: 91 - 88 = +3 points (we underestimated)\n\nEric: Studied 5 hours, scored 70\nOur line predicts: 79 points  \nResidual: 70 - 79 = -9 points (we overestimated)\n\n\nThe Key Insight: Why Square the Residuals?\nHere‚Äôs a puzzle. Consider these residuals from four students:\n\nStudent A: +5 points\nStudent B: -5 points\nStudent C: +3 points\nStudent D: -3 points\n\nIf we just add them: (+5) + (-5) + (+3) + (-3) = 0\nThis suggests perfect predictions, but every prediction was wrong! The positive and negative errors canceled out.\nThe solution: Square each residual before adding:\n\nStudent A: (+5)^2 = 25\nStudent B: (-5)^2 = 25\nStudent C: (+3)^2 = 9\nStudent D: (-3)^2 = 9\nTotal squared error: 68\n\nWhy squaring works:\n\nNo more cancellation: All squared values are positive\nBigger errors matter more: A 10-point error counts 4√ó as much as a 5-point error\nMathematical convenience: Squared functions are smooth and differentiable\n\n\n\nThe Ordinary Least Squares Method\nOLS finds the line that minimizes the Sum of Squared Errors (SSE):\n\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\nExpanding this: \\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1x_i))^2\nIn plain English: ‚ÄúFind the intercept and slope that make the total squared prediction error as small as possible.‚Äù\n\n\nThe Mathematical Solution (Formal Derivation)\nTo minimize SSE, we use calculus. Taking partial derivatives and setting them to zero:\n\\frac{\\partial SSE}{\\partial \\beta_0} = -2\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i) = 0\n\\frac{\\partial SSE}{\\partial \\beta_1} = -2\\sum_{i=1}^n x_i(y_i - \\beta_0 - \\beta_1x_i) = 0\nSolving this system of equations yields:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\nWhere \\bar{x} and \\bar{y} are the sample means.\nWhat this tells us:\n\nThe slope depends on how X and Y vary together (covariance) relative to how much X varies alone (variance)\nThe line always passes through the center point (\\bar{x}, \\bar{y})\n\n\n\nMaking Sense of Variation: How Good Is Our Line?\nTo evaluate our model‚Äôs performance, we break down the variation in exam scores:\n\nTotal Sum of Squares (SST)\n‚ÄúHow much do exam scores vary overall?‚Äù SST = \\sum_{i=1}^n(y_i - \\bar{y})^2\nThis measures how spread out the scores are from the class average.\n\n\nRegression Sum of Squares (SSR)\n‚ÄúHow much variation does our line explain?‚Äù SSR = \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2\nThis measures how much better our predictions are than just guessing the average for everyone.\n\n\nError Sum of Squares (SSE)\n‚ÄúHow much variation is left unexplained?‚Äù SSE = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\nThis is the variation our model couldn‚Äôt capture (the squared residuals).\n\n\nThe Fundamental Equation\nSST = SSR + SSE \\text{Total Variation} = \\text{Explained} + \\text{Unexplained}\n\n\n\nR-Squared: The Model Report Card\nThe coefficient of determination (R¬≤) tells us what percentage of variation our model explains:\nR^2 = \\frac{\\text{Explained Variation}}{\\text{Total Variation}} = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nHow to interpret R¬≤:\n\nR¬≤ = 0.75: ‚ÄúStudy hours explain 75% of the variation in exam scores‚Äù\nR¬≤ = 0.30: ‚ÄúOur model captures 30% of what makes scores different‚Äù\nR¬≤ = 1.00: Perfect prediction (never happens with real data)\nR¬≤ = 0.00: Our line is no better than guessing the average\n\nImportant reality check: In social sciences, R¬≤ = 0.30 might be excellent. In engineering, R¬≤ = 0.95 might be the minimum acceptable. Context matters.\n\n\nInterpreting Your Results\nWhen you run OLS and get \\hat{\\beta}_0 = 60 and \\hat{\\beta}_1 = 4:\nThe Slope (\\hat{\\beta}_1 = 4):\n\n‚ÄúEach additional hour of study is associated with 4 more points on the exam‚Äù\nThis is an average effect across all students\nIt‚Äôs not a guarantee for any individual student\n\nThe Intercept (\\hat{\\beta}_0 = 60):\n\n‚ÄúA student who studies 0 hours is predicted to score 60‚Äù\nOften this is just a mathematical anchor point\nMay not make practical sense (who studies 0 hours?)\n\nThe Prediction Equation: \\text{Predicted Score} = 60 + 4 \\times \\text{Study Hours}\nSo a student studying 5 hours: Predicted score = 60 + 4(5) = 80 points\n\n\nEffect Size and Practical Significance\nStatistical significance tells us whether an effect exists. Practical significance tells us whether it matters. Understanding both is crucial for proper interpretation.\n\nCalculating and Interpreting Raw Effect Sizes\nThe raw (unstandardized) effect size is simply your slope coefficient \\hat{\\beta}_1.\nExample: If \\hat{\\beta}_1 = 4 points per hour:\n\nThis is the raw effect size\nInterpretation: ‚ÄúOne hour of additional study yields 4 exam points‚Äù\n\nTo assess practical significance, consider:\n\nScale of the outcome: 4 points on a 100-point exam (4%) vs.¬†4 points on a 500-point exam (0.8%)\nCost of the intervention: Is one hour of study time worth 4 points?\nContext-specific thresholds: Does 4 points change a letter grade?\n\n\n\nCalculating Standardized Effect Sizes\nStandardized effects allow comparison across different scales and studies.\nFormula for standardized coefficient (beta weight): \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y}\nWhere:\n\ns_X = standard deviation of X (study hours)\ns_Y = standard deviation of Y (exam scores)\n\nStep-by-step calculation:\n\nCalculate the standard deviation of X: s_X = \\sqrt{\\frac{\\sum(x_i - \\bar{x})^2}{n-1}}\nCalculate the standard deviation of Y: s_Y = \\sqrt{\\frac{\\sum(y_i - \\bar{y})^2}{n-1}}\nMultiply: \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y}\n\nExample calculation:\n\nSuppose s_X = 2.5 hours and s_Y = 12 points\nWith \\hat{\\beta}_1 = 4: \\beta_{std} = 4 \\times \\frac{2.5}{12} = 0.83\nInterpretation: ‚ÄúA one standard deviation increase in study hours (2.5 hours) is associated with 0.83 standard deviations increase in exam score‚Äù\n\n\n\nCohen‚Äôs Guidelines for Effect Sizes\nFor standardized regression coefficients:\n\nSmall effect: |Œ≤| ‚âà 0.10 (explains ~1% of variance)\nMedium effect: |Œ≤| ‚âà 0.30 (explains ~9% of variance)\nLarge effect: |Œ≤| ‚âà 0.50 (explains ~25% of variance)\n\nFor R¬≤ (proportion of variance explained):\n\nSmall effect: R¬≤ ‚âà 0.02\nMedium effect: R¬≤ ‚âà 0.13\nLarge effect: R¬≤ ‚âà 0.26\n\nImportant: These are general benchmarks. Field-specific standards often differ:\n\nPsychology/Education: R¬≤ = 0.10 might be meaningful\nPhysics/Engineering: R¬≤ &lt; 0.90 might be unacceptable\nEconomics: R¬≤ = 0.30 might be excellent\n\n\n\nCalculating Confidence Intervals for Effect Sizes\nTo quantify uncertainty in your effect size:\nFor the raw coefficient: CI = \\hat{\\beta}_1 \\pm t_{critical} \\times SE(\\hat{\\beta}_1)\nWhere:\n\nt_{critical} = critical value from t-distribution (usually ‚âà 2 for 95% CI)\nSE(\\hat{\\beta}_1) = standard error of the slope\n\nPractical interpretation: If 95% CI = [3.2, 4.8], we can say: ‚ÄúWe‚Äôre 95% confident that each study hour adds between 3.2 and 4.8 exam points.‚Äù\n\n\nMaking Decisions About Practical Significance\nTo determine if an effect is practically significant, consider:\n\nMinimum meaningful difference: What‚Äôs the smallest effect that would matter?\n\nIn education: Often 0.25 standard deviations\nIn medicine: Determined by clinical relevance\nIn business: Based on cost-benefit analysis\n\nNumber needed to treat (NNT) analog: How much X must change for meaningful Y change?\n\nIf passing requires 10 more points and \\hat{\\beta}_1 = 4\nStudents need 2.5 more study hours to pass\n\nCost-effectiveness ratio: \\text{Efficiency} = \\frac{\\text{Effect Size}}{\\text{Cost of Intervention}}\n\nExample practical significance assessment:\n\nEffect: 4 points per study hour\nPassing threshold: 70 points\nCurrent average: 68 points\nConclusion: 30 minutes of extra study could change fail to pass\nDecision: Practically significant for borderline students\n\n\n\n\nUnderstanding Uncertainty: Nothing Is Perfect\nYour estimates come from a sample, not the entire population. This creates uncertainty.\n\nWhy We Have Uncertainty\n\nYou studied 20 students, not all students ever\nYour sample might be slightly unusual by chance\nMeasurement isn‚Äôt perfect (did students report hours accurately?)\n\n\n\nConfidence Intervals: Being Honest About Uncertainty\nInstead of saying ‚Äúthe effect is exactly 4 points per hour,‚Äù we say:\n\n‚ÄúWe estimate 4 points per hour‚Äù\n‚ÄúWe‚Äôre 95% confident the true effect is between 3.2 and 4.8 points‚Äù\n\nThis range (3.2 to 4.8) is called a 95% confidence interval.\nWhat it means: If we repeated this study many times with different samples, 95% of the intervals we calculate would contain the true effect.\nWhat it doesn‚Äôt mean: There‚Äôs a 95% chance the true value is in this specific interval (it either is or isn‚Äôt).\n\n\nTesting If There‚Äôs Really a Relationship\nThe big question: ‚ÄúIs there actually a relationship, or did we just get lucky with our sample?‚Äù\nWe test this by asking: ‚ÄúIf study hours truly had zero effect on scores, how likely would we be to see a pattern this strong just by chance?‚Äù\nThe process (simplified):\n\nAssume there‚Äôs no relationship (the ‚Äúnull hypothesis‚Äù)\nCalculate how unlikely our data would be if that were true\nIf it‚Äôs very unlikely (typically less than 5% chance), we conclude there probably is a relationship\n\nP-values in plain English:\n\np = 0.03: ‚ÄúIf study hours didn‚Äôt matter at all, there‚Äôs only a 3% chance we‚Äôd see a pattern this strong by luck‚Äù\np = 0.40: ‚ÄúThis pattern could easily happen by chance even if there‚Äôs no real relationship‚Äù\n\nRule of thumb: p &lt; 0.05 ‚Üí ‚Äústatistically significant‚Äù (probably a real relationship)\n\n\n\nWhen Things Go Wrong: Model Diagnostics\n\nQuick Visual Checks\n\nPlot your data first: Does it look roughly linear?\nPlot residuals vs.¬†predicted values: Should look like a random cloud\nLook for outliers: Any points way off from the others?\n\n\n\nWarning Signs Your Model Might Be Misleading\nPattern in residuals: If residuals show a curve or trend, you‚Äôre missing something\nIncreasing spread: If residuals get more spread out as predictions increase, standard errors might be wrong\nInfluential outliers: One or two weird points can drag your whole line off\nMissing variables: If you forgot something important (like prior knowledge), your estimates might be biased\n\n\n\nKey Assumptions: When OLS Works Well\n\nLinearity: The true relationship is approximately straight\n\nCheck: Look at your scatter plot\n\nIndependence: Each observation is separate\n\nCheck: Make sure students didn‚Äôt work together or copy\n\nConstant variance: The spread of residuals is similar everywhere\n\nCheck: Residual plot shouldn‚Äôt fan out\n\nNo perfect multicollinearity: (For multiple regression) Predictors aren‚Äôt perfectly related\n\nCheck: Make sure you didn‚Äôt include the same variable twice\n\nRandom sampling: Your data represents the population you care about\n\nCheck: Did you sample fairly?\n\n\n\n\nSummary: Your OLS Toolkit\nWhat OLS Does:\n\nFinds the straight line that minimizes squared prediction errors\nEstimates how much Y changes when X changes by one unit\nTells you how much variation your model explains (R¬≤)\nQuantifies uncertainty in your estimates\n\nYour Step-by-Step Process:\n\nPlot your data - does a line make sense?\nRun OLS to get \\hat{\\beta}_0 and \\hat{\\beta}_1\nCheck R¬≤ - how much variation do you explain?\nCalculate effect sizes (raw and standardized)\nAssess practical significance using context-specific criteria\nLook at confidence intervals - how uncertain are you?\nCheck residuals - any obvious problems?\nMake decisions based on both statistical and practical significance\n\n\n\nKluczowe interpretacje / Key interpretations\nDomy≈õlny model: regresja OLS Y=\\beta_0+\\beta_1 X+\\varepsilon (lub wieloraka: Y=\\beta_0+\\beta_1 X_1+\\cdots+\\beta_p X_p+\\varepsilon).\n\nNachylenie / Slope (\\beta_1) PL: Przy wzro≈õcie X o 1 jednostkƒô (ceteris paribus), przeciƒôtna warto≈õƒá Y zmienia siƒô o \\beta_1 jednostek. ENG: When X increases by 1 unit (ceteris paribus), the expected value of Y changes by \\beta_1 units.\nStandaryzowane nachylenie / Standardized slope \\big(\\beta_{1}^{(\\mathrm{std})}\\big) Definicja:\n\n\\beta_{1}^{(\\mathrm{std})} \\;=\\; \\beta_1 \\cdot \\frac{s_X}{s_Y},\n\ngdzie s_X i s_Y to odchylenia standardowe X i Y. PL: Przy wzro≈õcie X o 1 odchylenie standardowe (SD), przeciƒôtna warto≈õƒá Y zmienia siƒô o \\beta_{1}^{(\\mathrm{std})} odchyle≈Ñ standardowych Y. ENG: For a 1 standard deviation (SD) increase in X, the expected value of Y changes by \\beta_{1}^{(\\mathrm{std})} SDs of Y. Uwaga/Note: W regresji prostej \\beta_{1}^{(\\mathrm{std})} = r (Pearson). / In simple regression, \\beta_{1}^{(\\mathrm{std})} = r (Pearson).\nWsp√≥≈Çczynnik determinacji / R^2 Definicja:\n\nR^2 \\;=\\; 1 - \\frac{SS_{\\mathrm{res}}}{SS_{\\mathrm{tot}}}.\n\nPL: Model wyja≈õnia 100\\times R^2% zmienno≈õci Y wzglƒôdem modelu tylko z wyrazem wolnym (in-sample). ENG: The model explains 100\\times R^2% of the variance in Y relative to the intercept-only model (in-sample). W wielu zmiennych rozwa≈º: \\text{adjusted } R^2. / With multiple predictors consider: adjusted R^2.\nWarto≈õƒá p / P-value Formalnie/Formally:\n\np \\;=\\; \\Pr\\!\\big(\\,|T|\\ge |t_{\\mathrm{obs}}| \\mid H_0\\,\\big),\n\ngdzie T ma rozk≈Çad t przy H_0. PL: Zak≈ÇadajƒÖc prawdziwo≈õƒá H_0 i spe≈Çnione za≈Ço≈ºenia modelu, prawdopodobie≈Ñstwo uzyskania co najmniej tak ekstremalnej statystyki jak obserwowana wynosi p. ENG: Assuming H_0 and the model assumptions hold, p is the probability of observing a test statistic at least as extreme as the one obtained.\nPrzedzia≈Ç ufno≈õci / Confidence interval (np. dla \\beta_1) Konstrukcja/Construction:\n\n\\hat{\\beta}_1 \\;\\pm\\; t_{1-\\alpha/2,\\ \\mathrm{df}} \\cdot \\mathrm{SE}\\!\\left(\\hat{\\beta}_1\\right).\n\nPL (≈õci≈õle): W d≈Çugiej serii powt√≥rze≈Ñ 95% tak skonstruowanych przedzia≈Ç√≥w zawiera prawdziwƒÖ warto≈õƒá \\beta_1; dla naszych danych oszacowanie mie≈õci siƒô w [\\text{lower},\\ \\text{upper}]. ENG (strict): Over many repetitions, 95% of such intervals would contain the true \\beta_1; for our data, the estimate lies within [\\text{lower},\\ \\text{upper}]. PL (skr√≥t dydaktyczny): ‚ÄûJeste≈õmy 95% pewni, ≈ºe \\beta_1 le≈ºy w [\\text{lower},\\ \\text{upper}].‚Äù ENG (teaching shorthand): ‚ÄúWe are 95% confident that \\beta_1 lies in [\\text{lower},\\ \\text{upper}].‚Äù\n\n\nNajczƒôstsze nieporozumienia / Common pitfalls\n\nPL: p nie jest prawdopodobie≈Ñstwem, ≈ºe H_0 jest prawdziwa. ENG: p is not the probability that H_0 is true.\nPL: 95% CI nie zawiera 95% obserwacji (od tego jest przedzia≈Ç predykcji). ENG: A 95% CI does not contain 95% of observations (that‚Äôs a prediction interval).\nPL/ENG: Wysokie R^2 ‚â† przyczynowo≈õƒá / High R^2 ‚â† causality. Zawsze sprawdzaj/Always check diagnozy reszt, skalƒô efektu, i dopasowanie poza pr√≥bƒÖ.\n\nCritical Reminders:\n\nAssociation does not imply causation\nStatistical significance does not guarantee practical importance\nEvery model is wrong, but some are useful\nAlways visualize your data and residuals\nConsider both effect size and uncertainty when making decisions\n\nOLS provides a principled, mathematical approach to finding patterns in real-world data. While it cannot provide perfect predictions, it offers the best linear approximation possible along with honest assessments of that approximation‚Äôs quality and uncertainty.\n\n\n\n9.11 Complete Manual OLS Calculation: A Step-by-Step Example\nA professor wants to understand the relationship between hours spent studying and exam scores. She collects data from 6 students:\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nOur goal: Find the best-fitting line \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X using OLS.\n\n\n9.12 Step 1: Calculate the Means\nFirst, we need the mean of X and Y.\nFor X (study hours): \\bar{X} = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5\nFor Y (exam scores): \\bar{Y} = \\frac{65 + 70 + 75 + 85 + 88 + 95}{6} = \\frac{478}{6} = 79.67\n\n\n9.13 Step 2: Calculate Deviations from Means\nFor each observation, calculate (X_i - \\bar{X}) and (Y_i - \\bar{Y}):\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n1 - 3.5 = -2.5\n65 - 79.67 = -14.67\n\n\nB\n2\n70\n2 - 3.5 = -1.5\n70 - 79.67 = -9.67\n\n\nC\n3\n75\n3 - 3.5 = -0.5\n75 - 79.67 = -4.67\n\n\nD\n4\n85\n4 - 3.5 = 0.5\n85 - 79.67 = 5.33\n\n\nE\n5\n88\n5 - 3.5 = 1.5\n88 - 79.67 = 8.33\n\n\nF\n6\n95\n6 - 3.5 = 2.5\n95 - 79.67 = 15.33\n\n\n\n\n\n9.14 Step 3: Calculate Products and Squares\nNow calculate (X_i - \\bar{X})(Y_i - \\bar{Y}) and (X_i - \\bar{X})^2:\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n(-2.5)(-14.67) = 36.68\n(-2.5)¬≤ = 6.25\n\n\nB\n(-1.5)(-9.67) = 14.51\n(-1.5)¬≤ = 2.25\n\n\nC\n(-0.5)(-4.67) = 2.34\n(-0.5)¬≤ = 0.25\n\n\nD\n(0.5)(5.33) = 2.67\n(0.5)¬≤ = 0.25\n\n\nE\n(1.5)(8.33) = 12.50\n(1.5)¬≤ = 2.25\n\n\nF\n(2.5)(15.33) = 38.33\n(2.5)¬≤ = 6.25\n\n\nSum\n107.03\n17.50\n\n\n\n\n\n9.15 Step 4: Calculate the Slope (\\hat{\\beta}_1)\nUsing the OLS formula: \\hat{\\beta}_1 = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} = \\frac{107.03}{17.50} = 6.12\nInterpretation: Each additional hour of study is associated with a 6.12-point increase in exam score.\n\n\n9.16 Step 5: Calculate the Intercept (\\hat{\\beta}_0)\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X} = 79.67 - (6.12 \\times 3.5) = 79.67 - 21.42 = 58.25\nInterpretation: A student who studies 0 hours is predicted to score 58.25 points.\n\n\n9.17 Step 6: Write the Regression Equation\n\\hat{Y} = 58.25 + 6.12X\n\n\n9.18 Step 7: Calculate Predicted Values and Residuals\nUsing our equation to predict each student‚Äôs score:\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nResidual e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n58.25 + 6.12(1) = 64.37\n65 - 64.37 = 0.63\n\n\nB\n2\n70\n58.25 + 6.12(2) = 70.49\n70 - 70.49 = -0.49\n\n\nC\n3\n75\n58.25 + 6.12(3) = 76.61\n75 - 76.61 = -1.61\n\n\nD\n4\n85\n58.25 + 6.12(4) = 82.73\n85 - 82.73 = 2.27\n\n\nE\n5\n88\n58.25 + 6.12(5) = 88.85\n88 - 88.85 = -0.85\n\n\nF\n6\n95\n58.25 + 6.12(6) = 94.97\n95 - 94.97 = 0.03\n\n\n\nCheck: Sum of residuals = 0.63 - 0.49 - 1.61 + 2.27 - 0.85 + 0.03 ‚âà 0 ‚úì\n\n\n9.19 Step 8: Calculate Sum of Squares\n\nTotal Sum of Squares (SST)\nHow much total variation exists in exam scores?\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n(65 - 79.67)¬≤ = (-14.67)¬≤ = 215.21\n\n\nB\n70\n(70 - 79.67)¬≤ = (-9.67)¬≤ = 93.51\n\n\nC\n75\n(75 - 79.67)¬≤ = (-4.67)¬≤ = 21.81\n\n\nD\n85\n(85 - 79.67)¬≤ = (5.33)¬≤ = 28.41\n\n\nE\n88\n(88 - 79.67)¬≤ = (8.33)¬≤ = 69.39\n\n\nF\n95\n(95 - 79.67)¬≤ = (15.33)¬≤ = 235.01\n\n\nSum\n\nSST = 663.34\n\n\n\n\n\nRegression Sum of Squares (SSR)\nHow much variation does our model explain?\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n(64.37 - 79.67)¬≤ = (-15.30)¬≤ = 234.09\n\n\nB\n70.49\n(70.49 - 79.67)¬≤ = (-9.18)¬≤ = 84.27\n\n\nC\n76.61\n(76.61 - 79.67)¬≤ = (-3.06)¬≤ = 9.36\n\n\nD\n82.73\n(82.73 - 79.67)¬≤ = (3.06)¬≤ = 9.36\n\n\nE\n88.85\n(88.85 - 79.67)¬≤ = (9.18)¬≤ = 84.27\n\n\nF\n94.97\n(94.97 - 79.67)¬≤ = (15.30)¬≤ = 234.09\n\n\nSum\n\nSSR = 655.44\n\n\n\n\n\nError Sum of Squares (SSE)\nHow much variation is unexplained?\n\n\n\nStudent\nResidual e_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n-0.49\n0.24\n\n\nC\n-1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n-0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSum\n\nSSE = 9.10\n\n\n\nVerification: SST = SSR + SSE 663.34 ‚âà 655.44 + 9.10 = 664.54 ‚úì (small rounding difference)\n\n\n\n9.20 Step 9: Calculate R-Squared\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternative formula: R^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 1 - 0.014 = 0.986\n(Small difference due to rounding)\nInterpretation: Study hours explain 98.8% of the variation in exam scores. This is an extremely strong relationship.\n\n\n9.21 Step 10: Calculate Effect Sizes\n\nRaw Effect Size\nThe raw effect size is simply the slope: 6.12 points per hour\n\n\nStandardized Effect Size\nFirst, calculate standard deviations:\nFor X: s_X = \\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{n-1}} = \\sqrt{\\frac{17.50}{5}} = \\sqrt{3.50} = 1.87\nFor Y: s_Y = \\sqrt{\\frac{\\sum(Y_i - \\bar{Y})^2}{n-1}} = \\sqrt{\\frac{663.34}{5}} = \\sqrt{132.67} = 11.52\nStandardized coefficient: \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y} = 6.12 \\times \\frac{1.87}{11.52} = 6.12 \\times 0.162 = 0.99\nInterpretation: A one standard deviation increase in study hours (1.87 hours) is associated with a 0.99 standard deviation increase in exam score.\nAccording to Cohen‚Äôs guidelines:\n\nSmall effect: |Œ≤| = 0.10\nMedium effect: |Œ≤| = 0.30\nLarge effect: |Œ≤| = 0.50\n\nOur standardized effect of 0.99 is nearly twice Cohen‚Äôs ‚Äúlarge effect‚Äù threshold.\n\n\n\n9.22 Step 11: Practical Significance Assessment\n\nContext Analysis\n\nScale consideration:\n\nEffect: 6.12 points per hour\nExam scale: 0-100 points\nPercentage impact: 6.12% per hour\n\nPractical thresholds:\n\nLetter grade difference: Often 10 points\nTime to improve one letter grade: 10/6.12 = 1.63 hours\nConclusion: Less than 2 hours of extra study could change a letter grade\n\nCost-benefit analysis:\n\nBenefit: 6.12 points per hour\nCost: 1 hour of time\nDecision: Highly favorable return on investment\n\n\n\n\n\n9.23 Summary of Results\nRegression equation: \\hat{Y} = 58.25 + 6.12X\nKey statistics:\n\nSlope (\\hat{\\beta}_1): 6.12 points/hour\nIntercept (\\hat{\\beta}_0): 58.25 points\nR¬≤: 0.988 (98.8% of variance explained)\nStandardized effect: 0.99 (very large effect)\n\nPractical interpretation:\n\nEach hour of study adds about 6 points to exam score\nThe model fits extremely well (R¬≤ near 1)\nThe effect is both statistically and practically significant\nStudents can meaningfully improve grades with modest increases in study time\n\n\n\n9.24 Verification Check\nTo verify our calculations, let‚Äôs check that the regression line passes through (\\bar{X}, \\bar{Y}):\n\\hat{Y} = 58.25 + 6.12(3.5) = 58.25 + 21.42 = 79.67 = \\bar{Y} ‚úì\nThe calculation confirms our regression line passes through the point of means, as it should.\n\n\n9.25 R Code to Verify Manual Calculations\nBelow is R code that checks all our manual calculations. You can run this code to confirm every step.\n\n# Step 1: Create the data\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)  # X variable\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95)  # Y variable\nn &lt;- length(study_hours)  # Sample size\n\n# Create a data frame for easier handling\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Step 2: Calculate means\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Step 3: Calculate deviations from means\ndata$x_dev &lt;- data$X - x_bar  # X deviations\ndata$y_dev &lt;- data$Y - y_bar  # Y deviations\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Step 4: Calculate products and squares for OLS formula\ndata$xy_product &lt;- data$x_dev * data$y_dev  # (Xi - XÃÑ)(Yi - »≤)\ndata$x_dev_sq &lt;- data$x_dev^2  # (Xi - XÃÑ)¬≤\n\n# Sum of products and squares\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - XÃÑ)(Yi - »≤):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - XÃÑ)(Yi - »≤): 107 \n\ncat(\"Sum of (Xi - XÃÑ)¬≤:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - XÃÑ)¬≤: 17.5 \n\n# Step 5: Calculate slope (beta_1) manually\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (Œ≤‚ÇÅ) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (Œ≤‚ÇÅ) calculated manually: 6.11 \n\n# Step 6: Calculate intercept (beta_0) manually\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (Œ≤‚ÇÄ) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (Œ≤‚ÇÄ) calculated manually: 58.27 \n\n# Step 7: Compare with R's lm() function\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Step 8: Calculate predicted values and residuals\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X  # Predicted values\ndata$residual &lt;- data$Y - data$Y_hat  # Residuals\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ‚âà 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ‚âà 0): 0 \n\n# Step 9: Calculate Sum of Squares\n# Total Sum of Squares (SST)\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\n# Regression Sum of Squares (SSR)\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\n# Error Sum of Squares (SSE)\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\n# Verify that SST = SSR + SSE\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Step 10: Calculate R-squared\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R¬≤ (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR¬≤ (Method 1: SSR/SST): 0.9863 \n\ncat(\"R¬≤ (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR¬≤ (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R¬≤ (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR¬≤ (from lm function): 0.9863 \n\n# Step 11: Calculate Effect Sizes\n# Raw effect size (just the slope)\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\n# Standard deviations for standardized effect\nsd_x &lt;- sd(data$X)  # Standard deviation of X\nsd_y &lt;- sd(data$Y)  # Standard deviation of Y\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\n# Standardized effect size\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Correlation coefficient (should equal sqrt(R¬≤) for simple regression)\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R¬≤:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R¬≤: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n# Step 12: Create visualization\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\n# Plot the data and regression line\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5)\n\n# Add the regression line\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Add the mean point\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Add vertical lines for residuals\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\n# Add legend\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# Add the equation to the plot\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R¬≤ = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\n\n\n\n# Final summary\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )\n\n\n\nRunning the Code\nTo run this R code:\n\nCopy the entire code block above\nPaste it into RStudio or any R console\nExecute the code\nCompare the output with our manual calculations\n\nThe code will:\n\nRecreate all our manual calculations step by step\nVerify results using R‚Äôs built-in lm() function\nGenerate a visualization of the data with the regression line\nDisplay all intermediate calculations with clear labels\n\n\n\nExpected Output Highlights\nWhen you run this code, you should see:\n\nSlope: 6.12 (matching our manual calculation)\nIntercept: 58.25 (matching our manual calculation)\nR¬≤: 0.988 (matching our manual calculation)\nStandardized effect: 0.99 (matching our manual calculation)\nA plot showing the data points, regression line, and residuals\n\nThis verification confirms that our pen-and-paper calculations were correct!",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#complete-manual-ols-calculation-a-step-by-step-example",
    "href": "correg_en.html#complete-manual-ols-calculation-a-step-by-step-example",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.11 Complete Manual OLS Calculation: A Step-by-Step Example",
    "text": "9.11 Complete Manual OLS Calculation: A Step-by-Step Example\nA professor wants to understand the relationship between hours spent studying and exam scores. She collects data from 6 students:\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nOur goal: Find the best-fitting line \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X using OLS.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-1-calculate-the-means",
    "href": "correg_en.html#step-1-calculate-the-means",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.12 Step 1: Calculate the Means",
    "text": "9.12 Step 1: Calculate the Means\nFirst, we need the mean of X and Y.\nFor X (study hours): \\bar{X} = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5\nFor Y (exam scores): \\bar{Y} = \\frac{65 + 70 + 75 + 85 + 88 + 95}{6} = \\frac{478}{6} = 79.67",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-2-calculate-deviations-from-means",
    "href": "correg_en.html#step-2-calculate-deviations-from-means",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.13 Step 2: Calculate Deviations from Means",
    "text": "9.13 Step 2: Calculate Deviations from Means\nFor each observation, calculate (X_i - \\bar{X}) and (Y_i - \\bar{Y}):\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n1 - 3.5 = -2.5\n65 - 79.67 = -14.67\n\n\nB\n2\n70\n2 - 3.5 = -1.5\n70 - 79.67 = -9.67\n\n\nC\n3\n75\n3 - 3.5 = -0.5\n75 - 79.67 = -4.67\n\n\nD\n4\n85\n4 - 3.5 = 0.5\n85 - 79.67 = 5.33\n\n\nE\n5\n88\n5 - 3.5 = 1.5\n88 - 79.67 = 8.33\n\n\nF\n6\n95\n6 - 3.5 = 2.5\n95 - 79.67 = 15.33",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-3-calculate-products-and-squares",
    "href": "correg_en.html#step-3-calculate-products-and-squares",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.14 Step 3: Calculate Products and Squares",
    "text": "9.14 Step 3: Calculate Products and Squares\nNow calculate (X_i - \\bar{X})(Y_i - \\bar{Y}) and (X_i - \\bar{X})^2:\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n(-2.5)(-14.67) = 36.68\n(-2.5)¬≤ = 6.25\n\n\nB\n(-1.5)(-9.67) = 14.51\n(-1.5)¬≤ = 2.25\n\n\nC\n(-0.5)(-4.67) = 2.34\n(-0.5)¬≤ = 0.25\n\n\nD\n(0.5)(5.33) = 2.67\n(0.5)¬≤ = 0.25\n\n\nE\n(1.5)(8.33) = 12.50\n(1.5)¬≤ = 2.25\n\n\nF\n(2.5)(15.33) = 38.33\n(2.5)¬≤ = 6.25\n\n\nSum\n107.03\n17.50",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-4-calculate-the-slope-hatbeta_1",
    "href": "correg_en.html#step-4-calculate-the-slope-hatbeta_1",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.15 Step 4: Calculate the Slope (\\hat{\\beta}_1)",
    "text": "9.15 Step 4: Calculate the Slope (\\hat{\\beta}_1)\nUsing the OLS formula: \\hat{\\beta}_1 = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} = \\frac{107.03}{17.50} = 6.12\nInterpretation: Each additional hour of study is associated with a 6.12-point increase in exam score.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-5-calculate-the-intercept-hatbeta_0",
    "href": "correg_en.html#step-5-calculate-the-intercept-hatbeta_0",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.16 Step 5: Calculate the Intercept (\\hat{\\beta}_0)",
    "text": "9.16 Step 5: Calculate the Intercept (\\hat{\\beta}_0)\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X} = 79.67 - (6.12 \\times 3.5) = 79.67 - 21.42 = 58.25\nInterpretation: A student who studies 0 hours is predicted to score 58.25 points.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-6-write-the-regression-equation",
    "href": "correg_en.html#step-6-write-the-regression-equation",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.17 Step 6: Write the Regression Equation",
    "text": "9.17 Step 6: Write the Regression Equation\n\\hat{Y} = 58.25 + 6.12X",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-7-calculate-predicted-values-and-residuals",
    "href": "correg_en.html#step-7-calculate-predicted-values-and-residuals",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.18 Step 7: Calculate Predicted Values and Residuals",
    "text": "9.18 Step 7: Calculate Predicted Values and Residuals\nUsing our equation to predict each student‚Äôs score:\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nResidual e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n58.25 + 6.12(1) = 64.37\n65 - 64.37 = 0.63\n\n\nB\n2\n70\n58.25 + 6.12(2) = 70.49\n70 - 70.49 = -0.49\n\n\nC\n3\n75\n58.25 + 6.12(3) = 76.61\n75 - 76.61 = -1.61\n\n\nD\n4\n85\n58.25 + 6.12(4) = 82.73\n85 - 82.73 = 2.27\n\n\nE\n5\n88\n58.25 + 6.12(5) = 88.85\n88 - 88.85 = -0.85\n\n\nF\n6\n95\n58.25 + 6.12(6) = 94.97\n95 - 94.97 = 0.03\n\n\n\nCheck: Sum of residuals = 0.63 - 0.49 - 1.61 + 2.27 - 0.85 + 0.03 ‚âà 0 ‚úì",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-8-calculate-sum-of-squares",
    "href": "correg_en.html#step-8-calculate-sum-of-squares",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.19 Step 8: Calculate Sum of Squares",
    "text": "9.19 Step 8: Calculate Sum of Squares\n\nTotal Sum of Squares (SST)\nHow much total variation exists in exam scores?\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n(65 - 79.67)¬≤ = (-14.67)¬≤ = 215.21\n\n\nB\n70\n(70 - 79.67)¬≤ = (-9.67)¬≤ = 93.51\n\n\nC\n75\n(75 - 79.67)¬≤ = (-4.67)¬≤ = 21.81\n\n\nD\n85\n(85 - 79.67)¬≤ = (5.33)¬≤ = 28.41\n\n\nE\n88\n(88 - 79.67)¬≤ = (8.33)¬≤ = 69.39\n\n\nF\n95\n(95 - 79.67)¬≤ = (15.33)¬≤ = 235.01\n\n\nSum\n\nSST = 663.34\n\n\n\n\n\nRegression Sum of Squares (SSR)\nHow much variation does our model explain?\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n(64.37 - 79.67)¬≤ = (-15.30)¬≤ = 234.09\n\n\nB\n70.49\n(70.49 - 79.67)¬≤ = (-9.18)¬≤ = 84.27\n\n\nC\n76.61\n(76.61 - 79.67)¬≤ = (-3.06)¬≤ = 9.36\n\n\nD\n82.73\n(82.73 - 79.67)¬≤ = (3.06)¬≤ = 9.36\n\n\nE\n88.85\n(88.85 - 79.67)¬≤ = (9.18)¬≤ = 84.27\n\n\nF\n94.97\n(94.97 - 79.67)¬≤ = (15.30)¬≤ = 234.09\n\n\nSum\n\nSSR = 655.44\n\n\n\n\n\nError Sum of Squares (SSE)\nHow much variation is unexplained?\n\n\n\nStudent\nResidual e_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n-0.49\n0.24\n\n\nC\n-1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n-0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSum\n\nSSE = 9.10\n\n\n\nVerification: SST = SSR + SSE 663.34 ‚âà 655.44 + 9.10 = 664.54 ‚úì (small rounding difference)",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-9-calculate-r-squared",
    "href": "correg_en.html#step-9-calculate-r-squared",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.20 Step 9: Calculate R-Squared",
    "text": "9.20 Step 9: Calculate R-Squared\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternative formula: R^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 1 - 0.014 = 0.986\n(Small difference due to rounding)\nInterpretation: Study hours explain 98.8% of the variation in exam scores. This is an extremely strong relationship.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-10-calculate-effect-sizes",
    "href": "correg_en.html#step-10-calculate-effect-sizes",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.21 Step 10: Calculate Effect Sizes",
    "text": "9.21 Step 10: Calculate Effect Sizes\n\nRaw Effect Size\nThe raw effect size is simply the slope: 6.12 points per hour\n\n\nStandardized Effect Size\nFirst, calculate standard deviations:\nFor X: s_X = \\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{n-1}} = \\sqrt{\\frac{17.50}{5}} = \\sqrt{3.50} = 1.87\nFor Y: s_Y = \\sqrt{\\frac{\\sum(Y_i - \\bar{Y})^2}{n-1}} = \\sqrt{\\frac{663.34}{5}} = \\sqrt{132.67} = 11.52\nStandardized coefficient: \\beta_{std} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y} = 6.12 \\times \\frac{1.87}{11.52} = 6.12 \\times 0.162 = 0.99\nInterpretation: A one standard deviation increase in study hours (1.87 hours) is associated with a 0.99 standard deviation increase in exam score.\nAccording to Cohen‚Äôs guidelines:\n\nSmall effect: |Œ≤| = 0.10\nMedium effect: |Œ≤| = 0.30\nLarge effect: |Œ≤| = 0.50\n\nOur standardized effect of 0.99 is nearly twice Cohen‚Äôs ‚Äúlarge effect‚Äù threshold.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#step-11-practical-significance-assessment",
    "href": "correg_en.html#step-11-practical-significance-assessment",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.22 Step 11: Practical Significance Assessment",
    "text": "9.22 Step 11: Practical Significance Assessment\n\nContext Analysis\n\nScale consideration:\n\nEffect: 6.12 points per hour\nExam scale: 0-100 points\nPercentage impact: 6.12% per hour\n\nPractical thresholds:\n\nLetter grade difference: Often 10 points\nTime to improve one letter grade: 10/6.12 = 1.63 hours\nConclusion: Less than 2 hours of extra study could change a letter grade\n\nCost-benefit analysis:\n\nBenefit: 6.12 points per hour\nCost: 1 hour of time\nDecision: Highly favorable return on investment",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-of-results",
    "href": "correg_en.html#summary-of-results",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.23 Summary of Results",
    "text": "9.23 Summary of Results\nRegression equation: \\hat{Y} = 58.25 + 6.12X\nKey statistics:\n\nSlope (\\hat{\\beta}_1): 6.12 points/hour\nIntercept (\\hat{\\beta}_0): 58.25 points\nR¬≤: 0.988 (98.8% of variance explained)\nStandardized effect: 0.99 (very large effect)\n\nPractical interpretation:\n\nEach hour of study adds about 6 points to exam score\nThe model fits extremely well (R¬≤ near 1)\nThe effect is both statistically and practically significant\nStudents can meaningfully improve grades with modest increases in study time",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#verification-check",
    "href": "correg_en.html#verification-check",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.24 Verification Check",
    "text": "9.24 Verification Check\nTo verify our calculations, let‚Äôs check that the regression line passes through (\\bar{X}, \\bar{Y}):\n\\hat{Y} = 58.25 + 6.12(3.5) = 58.25 + 21.42 = 79.67 = \\bar{Y} ‚úì\nThe calculation confirms our regression line passes through the point of means, as it should.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#r-code-to-verify-manual-calculations",
    "href": "correg_en.html#r-code-to-verify-manual-calculations",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.25 R Code to Verify Manual Calculations",
    "text": "9.25 R Code to Verify Manual Calculations\nBelow is R code that checks all our manual calculations. You can run this code to confirm every step.\n\n# Step 1: Create the data\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)  # X variable\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95)  # Y variable\nn &lt;- length(study_hours)  # Sample size\n\n# Create a data frame for easier handling\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Step 2: Calculate means\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Step 3: Calculate deviations from means\ndata$x_dev &lt;- data$X - x_bar  # X deviations\ndata$y_dev &lt;- data$Y - y_bar  # Y deviations\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Step 4: Calculate products and squares for OLS formula\ndata$xy_product &lt;- data$x_dev * data$y_dev  # (Xi - XÃÑ)(Yi - »≤)\ndata$x_dev_sq &lt;- data$x_dev^2  # (Xi - XÃÑ)¬≤\n\n# Sum of products and squares\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - XÃÑ)(Yi - »≤):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - XÃÑ)(Yi - »≤): 107 \n\ncat(\"Sum of (Xi - XÃÑ)¬≤:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - XÃÑ)¬≤: 17.5 \n\n# Step 5: Calculate slope (beta_1) manually\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (Œ≤‚ÇÅ) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (Œ≤‚ÇÅ) calculated manually: 6.11 \n\n# Step 6: Calculate intercept (beta_0) manually\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (Œ≤‚ÇÄ) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (Œ≤‚ÇÄ) calculated manually: 58.27 \n\n# Step 7: Compare with R's lm() function\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Step 8: Calculate predicted values and residuals\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X  # Predicted values\ndata$residual &lt;- data$Y - data$Y_hat  # Residuals\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ‚âà 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ‚âà 0): 0 \n\n# Step 9: Calculate Sum of Squares\n# Total Sum of Squares (SST)\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\n# Regression Sum of Squares (SSR)\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\n# Error Sum of Squares (SSE)\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\n# Verify that SST = SSR + SSE\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Step 10: Calculate R-squared\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R¬≤ (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR¬≤ (Method 1: SSR/SST): 0.9863 \n\ncat(\"R¬≤ (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR¬≤ (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R¬≤ (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR¬≤ (from lm function): 0.9863 \n\n# Step 11: Calculate Effect Sizes\n# Raw effect size (just the slope)\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\n# Standard deviations for standardized effect\nsd_x &lt;- sd(data$X)  # Standard deviation of X\nsd_y &lt;- sd(data$Y)  # Standard deviation of Y\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\n# Standardized effect size\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Correlation coefficient (should equal sqrt(R¬≤) for simple regression)\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R¬≤:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R¬≤: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n# Step 12: Create visualization\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\n# Plot the data and regression line\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5)\n\n# Add the regression line\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Add the mean point\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Add vertical lines for residuals\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\n# Add legend\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# Add the equation to the plot\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R¬≤ = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\n\n\n\n# Final summary\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )\n\n\n\nRunning the Code\nTo run this R code:\n\nCopy the entire code block above\nPaste it into RStudio or any R console\nExecute the code\nCompare the output with our manual calculations\n\nThe code will:\n\nRecreate all our manual calculations step by step\nVerify results using R‚Äôs built-in lm() function\nGenerate a visualization of the data with the regression line\nDisplay all intermediate calculations with clear labels\n\n\n\nExpected Output Highlights\nWhen you run this code, you should see:\n\nSlope: 6.12 (matching our manual calculation)\nIntercept: 58.25 (matching our manual calculation)\nR¬≤: 0.988 (matching our manual calculation)\nStandardized effect: 0.99 (matching our manual calculation)\nA plot showing the data points, regression line, and residuals\n\nThis verification confirms that our pen-and-paper calculations were correct!",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-linear-regression-model",
    "href": "correg_en.html#the-linear-regression-model",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.26 The Linear Regression Model",
    "text": "9.26 The Linear Regression Model\nRegression analysis provides a statistical framework for modeling relationships between a dependent variable and one or more independent variables. This methodology enables researchers to quantify relationships, test hypotheses, and make predictions based on observed data.\n\nSimple Linear Regression\nThe simple linear regression model expresses the relationship between a dependent variable and a single independent variable:\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\nWhere: - Y_i represents the dependent variable for observation i - X_i represents the independent variable for observation i - \\beta_0 is the intercept parameter - \\beta_1 is the slope parameter - \\varepsilon_i is the error term for observation i\n\n\nMultiple Linear Regression\nThe multiple linear regression model extends this framework to incorporate k independent variables:\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + ... + \\beta_k X_{ki} + \\varepsilon_i\nThis formulation allows for the simultaneous analysis of multiple predictors and their respective contributions to the dependent variable.\n\n\nOrdinary Least Squares Estimation\n\nDefining the Optimization Criterion\nThe estimation of regression parameters requires a criterion for determining the ‚Äúbest‚Äù fit. Consider three potential approaches for defining the optimal line through a set of data points:\n\n\nApproach 1: Minimizing the Sum of Residuals\n\\min \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i) = \\min \\sum_{i=1}^{n} e_i\nThis approach is fundamentally flawed. For any line passing through the data, we can always find another line where positive and negative residuals sum to zero. In fact, infinitely many lines satisfy \\sum e_i = 0. This criterion fails to uniquely identify an optimal solution. Moreover, a horizontal line through the mean of Y would achieve zero sum of residuals while ignoring the relationship with X entirely.\n\n\nApproach 2: Minimizing the Sum of Absolute Residuals\n\\min \\sum_{i=1}^{n} |Y_i - \\hat{Y}_i| = \\min \\sum_{i=1}^{n} |e_i|\nThis criterion, known as Least Absolute Deviations (LAD), addresses the cancellation problem by taking absolute values. It produces estimates that are more robust to outliers than OLS. However, this approach presents significant challenges:\n\nThe absolute value function is not differentiable at zero, complicating analytical solutions\nMultiple solutions may exist (the objective function may have multiple minima)\nNo closed-form solution exists; iterative numerical methods are required\nStatistical inference is more complex, lacking the elegant properties of OLS estimators\n\n\n\nApproach 3: Minimizing the Sum of Squared Residuals\n\\min \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\min \\sum_{i=1}^{n} e_i^2\nThe Ordinary Least Squares (OLS) approach minimizes the sum of squared residuals. This criterion offers several advantages:\n\nPrevents cancellation of positive and negative errors\nProvides a unique solution (except in cases of perfect multicollinearity)\nYields closed-form analytical solutions through differentiation\nProduces estimators with optimal statistical properties under classical assumptions\nFacilitates straightforward statistical inference\n\n\n\n\nVisualizing the Sum of Squared Errors\nThe OLS method can be understood geometrically through the following conceptual framework:\n\nEach error appears as a vertical line from the data point to the regression line\nEach of these vertical lines represents a residual (e_i)\nWe square each residual, which can be visualized as creating a square area\nThe sum of all these squared areas is what OLS minimizes\n\n\n# Visualization of squared errors as geometric areas\nlibrary(ggplot2)\n\n# Generate sample data with clear pattern\nset.seed(42)\nn &lt;- 8  # Small number for clarity\nx &lt;- seq(2, 16, length.out = n)\ny &lt;- 10 + 1.5*x + rnorm(n, 0, 3)\ndf &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, df)\ndf$fitted &lt;- fitted(model)\ndf$residual &lt;- residuals(model)\n\n# Create visualization with actual squares\np &lt;- ggplot(df, aes(x = x, y = y)) +\n  # Add regression line first (bottom layer)\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\", linewidth = 1.2) +\n  \n  # Add squares for each residual\n  # For positive residuals\n  geom_rect(data = subset(df, residual &gt; 0),\n            aes(xmin = x - abs(residual)/2, \n                xmax = x + abs(residual)/2, \n                ymin = fitted, \n                ymax = fitted + abs(residual)),\n            fill = \"red\", alpha = 0.25, color = \"red\", linewidth = 0.5) +\n  \n  # For negative residuals  \n  geom_rect(data = subset(df, residual &lt; 0),\n            aes(xmin = x - abs(residual)/2, \n                xmax = x + abs(residual)/2, \n                ymin = fitted - abs(residual), \n                ymax = fitted),\n            fill = \"red\", alpha = 0.25, color = \"red\", linewidth = 0.5) +\n  \n  # Add residual lines\n  geom_segment(aes(xend = x, yend = fitted), \n               color = \"red\", linewidth = 0.8, linetype = \"solid\") +\n  \n  # Add data points\n  geom_point(size = 3.5, color = \"black\") +\n  \n  # Add text annotations for selected squared values\n  geom_text(data = subset(df, abs(residual) &gt; 2),\n            aes(x = x, \n                y = fitted + sign(residual) * abs(residual)/2,\n                label = paste0(\"e¬≤=\", round(residual^2, 1))),\n            size = 3, color = \"darkred\", fontface = \"italic\") +\n  \n  # Styling\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(face = \"italic\")\n  ) +\n  coord_equal() +  # Ensures squares appear as squares\n  labs(\n    title = \"Geometric Visualization of Sum of Squared Errors\",\n    subtitle = paste(\"SSE =\", round(sum(df$residual^2), 1), \n                     \"- Red squares represent squared residuals\"),\n    x = \"Independent Variable (X)\",\n    y = \"Dependent Variable (Y)\"\n  ) +\n  # Add SSE annotation\n  annotate(\"rect\", \n           xmin = max(df$x) - 3, xmax = max(df$x) - 0.5,\n           ymin = min(df$y) - 2, ymax = min(df$y),\n           fill = \"lightyellow\", alpha = 0.8, color = \"gray40\") +\n  annotate(\"text\", \n           x = max(df$x) - 1.75, y = min(df$y) - 1,\n           label = paste(\"Œ£e¬≤ =\", round(sum(df$residual^2), 1)),\n           size = 4, fontface = \"bold\", color = \"darkred\")\n\nprint(p)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nMathematical Derivation of OLS Estimators\nFor simple linear regression, the OLS estimators are obtained by minimizing the sum of squared residuals:\nSSE = \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2\nTaking partial derivatives with respect to \\beta_0 and \\beta_1 and setting them equal to zero yields the normal equations. Solving this system produces:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} = \\frac{Cov(X,Y)}{Var(X)}\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\n\n\nProperties of OLS Estimators\nThe OLS procedure guarantees several important properties:\n\nZero sum of residuals: \\sum_{i=1}^{n} e_i = 0\nOrthogonality of residuals and predictors: \\sum_{i=1}^{n} X_i e_i = 0\nThe fitted regression line passes through the point (\\bar{X}, \\bar{Y})\nZero covariance between fitted values and residuals: \\sum_{i=1}^{n} \\hat{Y}_i e_i = 0\n\n\n\nClassical Linear Model Assumptions\n\nCore Assumptions\nFor OLS estimators to possess desirable statistical properties, the following assumptions must hold:\n\n\nAssumption 1: Linearity in Parameters\nThe relationship between the dependent and independent variables is linear in the parameters: Y_i = \\beta_0 + \\beta_1 X_{1i} + ... + \\beta_k X_{ki} + \\varepsilon_i\n\n\nAssumption 2: Strict Exogeneity\nThe error term has zero conditional expectation given all values of the independent variables: E[\\varepsilon_i | X] = 0\nThis assumption implies that the independent variables contain no information about the mean of the error term. It is stronger than contemporaneous exogeneity and rules out feedback from past errors to current regressors. This assumption is critical for unbiased estimation and is often violated in time series contexts with lagged dependent variables or in the presence of omitted variables.\nThis assumption is particularly important for our discussion of spurious correlations. Violations of the exogeneity assumption lead to endogeneity problems, which we will discuss later.\n\n\nAssumption 3: No Perfect Multicollinearity\nIn multiple regression, no independent variable can be expressed as a perfect linear combination of other independent variables. The matrix X'X must be invertible.\n\n\nAssumption 4: Homoscedasticity\nThe variance of the error term is constant across all observations: Var(\\varepsilon_i | X) = \\sigma^2\nThis assumption ensures that the precision of the regression does not vary systematically with the level of the independent variables.\n\n\nAssumption 5: No Autocorrelation\nThe error terms are uncorrelated with each other: Cov(\\varepsilon_i, \\varepsilon_j | X) = 0 \\text{ for } i \\neq j\n\n\nAssumption 6: Normality of Errors (for inference)\nThe error terms follow a normal distribution: \\varepsilon_i \\sim N(0, \\sigma^2)\nThis assumption is not required for the unbiasedness or consistency of OLS estimators but is necessary for exact finite-sample inference.\n\n\n\nGauss-Markov Theorem\nUnder Assumptions 1-5, the OLS estimators are BLUE (Best Linear Unbiased Estimators):\n\nBest: Minimum variance among the class of linear unbiased estimators\nLinear: The estimators are linear functions of the dependent variable\nUnbiased: E[\\hat{\\beta}] = \\beta\n\n\n\nVisualization of OLS Methodology\n\nGeometric Interpretation\n\n# Comprehensive visualization of OLS regression\nlibrary(ggplot2)\n\n# Generate sample data\nset.seed(42)\nn &lt;- 50\nx &lt;- runif(n, 0, 100)\nepsilon &lt;- rnorm(n, 0, 15)\ny &lt;- 20 + 0.8*x + epsilon\n\n# Create data frame\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, data = data)\ndata$fitted &lt;- fitted(model)\ndata$residuals &lt;- residuals(model)\n\n# Create comprehensive plot\nggplot(data, aes(x = x, y = y)) +\n  # Add confidence interval\n  geom_smooth(method = \"lm\", se = TRUE, alpha = 0.15, fill = \"blue\") +\n  # Add regression line\n  geom_line(aes(y = fitted), color = \"blue\", linewidth = 1.2) +\n  # Add residual segments\n  geom_segment(aes(xend = x, yend = fitted), \n               color = \"red\", alpha = 0.5, linewidth = 0.7) +\n  # Add observed points\n  geom_point(size = 2.5, alpha = 0.8) +\n  # Add fitted values\n  geom_point(aes(y = fitted), color = \"blue\", size = 1.5, alpha = 0.6) +\n  # Annotations\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(size = 11),\n    plot.title = element_text(size = 12, face = \"bold\")\n  ) +\n  labs(\n    title = \"Ordinary Least Squares Regression\",\n    subtitle = sprintf(\"Estimated equation: Y = %.2f + %.3f X  (R¬≤ = %.3f, RSE = %.2f)\",\n                      coef(model)[1], coef(model)[2], \n                      summary(model)$r.squared, \n                      summary(model)$sigma),\n    x = \"Independent Variable (X)\",\n    y = \"Dependent Variable (Y)\"\n  ) +\n  annotate(\"text\", x = min(x) + 5, y = max(y) - 5,\n           label = sprintf(\"SSE = %.1f\", sum(residuals(model)^2)),\n           hjust = 0, size = 3.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nVisualization of Squared Residuals\n\n# Demonstrate why squaring is necessary\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Generate example with clear pattern\nset.seed(123)\nn &lt;- 20\nx &lt;- seq(1, 10, length.out = n)\ny &lt;- 2 + 1.5*x + rnorm(n, 0, 2)\ndf &lt;- data.frame(x = x, y = y)\n\n# Fit model\nmodel &lt;- lm(y ~ x, df)\ndf$fitted &lt;- fitted(model)\ndf$residual &lt;- residuals(model)\n\n# Plot 1: Raw residuals\np1 &lt;- ggplot(df, aes(x = x)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(aes(y = 0, yend = residual, xend = x), \n               color = ifelse(df$residual &gt; 0, \"darkgreen\", \"darkred\"),\n               linewidth = 1) +\n  geom_point(aes(y = residual), size = 3,\n             color = ifelse(df$residual &gt; 0, \"darkgreen\", \"darkred\")) +\n  theme_minimal() +\n  labs(title = \"Residuals (ei)\",\n       subtitle = sprintf(\"Sum = %.2f (not meaningful)\", sum(df$residual)),\n       x = \"X\", y = \"Residual\") +\n  ylim(c(-6, 6))\n\n# Plot 2: Squared residuals\np2 &lt;- ggplot(df, aes(x = x)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_segment(aes(y = 0, yend = residual^2, xend = x), \n               color = \"darkred\", linewidth = 1) +\n  geom_point(aes(y = residual^2), size = 3, color = \"darkred\") +\n  theme_minimal() +\n  labs(title = \"Squared Residuals (ei¬≤)\",\n       subtitle = sprintf(\"Sum = %.2f (minimized by OLS)\", sum(df$residual^2)),\n       x = \"X\", y = \"Squared Residual\") +\n  ylim(c(0, 36))\n\n# Combine plots\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\nDiagnostic Analysis\n\nResidual Diagnostics\nAssessment of model assumptions requires careful examination of residual patterns:\n\n# Generate diagnostic plots\npar(mfrow = c(2, 2))\n\n# Residuals vs Fitted Values\nplot(model, which = 1)\n# Tests linearity and homoscedasticity assumptions\n\n# Normal Q-Q Plot\nplot(model, which = 2)\n# Tests normality assumption\n\n# Scale-Location Plot\nplot(model, which = 3)\n# Tests homoscedasticity assumption\n\n# Cook's Distance\nplot(model, which = 4)\n\n\n\n\n\n\n\n# Identifies influential observations\n\n\n\nTesting Assumptions Formally\n\n# Formal statistical tests\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(car)\n\n# Test for heteroscedasticity\n# Breusch-Pagan test\nbptest(model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model\nBP = 0.37876, df = 1, p-value = 0.5383\n\n# Test for autocorrelation\n# Durbin-Watson test\ndwtest(model)\n\n\n    Durbin-Watson test\n\ndata:  model\nDW = 2.1781, p-value = 0.5599\nalternative hypothesis: true autocorrelation is greater than 0\n\n# Test for normality\n# Shapiro-Wilk test on residuals\nshapiro.test(residuals(model))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(model)\nW = 0.98221, p-value = 0.9593\n\n# Test for linearity\n# Rainbow test\nraintest(model)\n\n\n    Rainbow test\n\ndata:  model\nRain = 1.2139, df1 = 10, df2 = 8, p-value = 0.3995\n\n\n\n\n\nExtensions and Alternatives\n\nWhen OLS Assumptions Fail\nWhen classical assumptions are violated, alternative approaches may be necessary:\n\nHeteroscedasticity: Weighted Least Squares (WLS) or robust standard errors\nAutocorrelation: Generalized Least Squares (GLS) or Newey-West standard errors\nNon-normality: Bootstrap inference or robust regression methods\nMulticollinearity: Ridge regression or LASSO\nEndogeneity: Instrumental Variables (IV) or Two-Stage Least Squares (2SLS)\n\n\n\nRobust Regression Methods\nWhen outliers are present, robust alternatives to OLS include:\n\nM-estimators (Huber regression)\nLeast Trimmed Squares (LTS)\nMM-estimators\n\n\n\n\nPractical Implementation\n\nComplete Analysis Example\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(car)\n\n# Generate dataset\nset.seed(2024)\nn &lt;- 200\ndata &lt;- data.frame(\n  x1 = rnorm(n, 50, 10),\n  x2 = rnorm(n, 30, 5),\n  x3 = rbinom(n, 1, 0.5)\n)\ndata$y &lt;- 10 + 2*data$x1 + 3*data$x2 + 15*data$x3 + rnorm(n, 0, 10)\n\n# Fit multiple regression model\nfull_model &lt;- lm(y ~ x1 + x2 + x3, data = data)\n\n# Model summary\nsummary(full_model)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25.6507  -6.9674  -0.7472   6.3670  30.4959 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 13.59158    5.97565   2.274               0.024 *  \nx1           2.05837    0.06876  29.934 &lt;0.0000000000000002 ***\nx2           2.76233    0.15259  18.103 &lt;0.0000000000000002 ***\nx3          14.80771    1.42654  10.380 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.897 on 196 degrees of freedom\nMultiple R-squared:  0.8744,    Adjusted R-squared:  0.8725 \nF-statistic: 454.9 on 3 and 196 DF,  p-value: &lt; 0.00000000000000022\n\n# ANOVA table\nanova(full_model)\n\nAnalysis of Variance Table\n\nResponse: y\n           Df Sum Sq Mean Sq F value                Pr(&gt;F)    \nx1          1  82189   82189  839.04 &lt; 0.00000000000000022 ***\nx2          1  40936   40936  417.90 &lt; 0.00000000000000022 ***\nx3          1  10555   10555  107.75 &lt; 0.00000000000000022 ***\nResiduals 196  19200      98                                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Confidence intervals for parameters\nconfint(full_model, level = 0.95)\n\n                2.5 %    97.5 %\n(Intercept)  1.806741 25.376410\nx1           1.922758  2.193977\nx2           2.461401  3.063262\nx3          11.994367 17.621053\n\n# Variance Inflation Factors (multicollinearity check)\nvif(full_model)\n\n      x1       x2       x3 \n1.009487 1.044192 1.038735 \n\n# Model diagnostics\npar(mfrow = c(2, 2))\nplot(full_model)\n\n\n\n\n\n\n\n# Tidy output\ntidy(full_model, conf.int = TRUE)\n\n# A tibble: 4 √ó 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    13.6     5.98        2.27 2.40e- 2     1.81     25.4 \n2 x1              2.06    0.0688     29.9  4.90e-75     1.92      2.19\n3 x2              2.76    0.153      18.1  1.06e-43     2.46      3.06\n4 x3             14.8     1.43       10.4  2.14e-20    12.0      17.6 \n\nglance(full_model)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.874         0.872  9.90      455. 5.21e-88     3  -740. 1490. 1507.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\nConclusion\nOrdinary Least Squares regression remains a fundamental tool in statistical analysis. The method‚Äôs mathematical elegance, combined with its optimal properties under the classical assumptions, explains its widespread application. However, practitioners must carefully verify assumptions and consider alternatives when these conditions are not met. Understanding both the theoretical foundations and practical limitations of OLS is essential for proper statistical inference and prediction.\n\n\nVisualizing OLS Through Different Regression Lines\nA simple but effective way to visualize the concept of ‚Äúbest fit‚Äù is to compare multiple lines and their resulting SSE values:\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Create sample data\nset.seed(123)\nx &lt;- 1:20\ny &lt;- 2 + 3*x + rnorm(20, 0, 5)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, data = data)\ncoef &lt;- coefficients(model)\n\n# Define different lines: optimal and sub-optimal with clearer differences\nlines &lt;- data.frame(\n  label = c(\"Best Fit (OLS)\", \"Line A\", \"Line B\", \"Line C\"),\n  intercept = c(coef[1], coef[1] - 8, coef[1] + 8, coef[1] - 4),\n  slope = c(coef[2], coef[2] - 1.2, coef[2] + 0.8, coef[2] - 0.7)\n)\n\n# Calculate SSE for each line\nlines$sse &lt;- sapply(1:nrow(lines), function(i) {\n  predicted &lt;- lines$intercept[i] + lines$slope[i] * x\n  sum((y - predicted)^2)\n})\n\n# Add percentage increase over optimal SSE\nlines$pct_increase &lt;- round((lines$sse / lines$sse[1] - 1) * 100, 1)\nlines$pct_text &lt;- ifelse(lines$label == \"Best Fit (OLS)\", \n                         \"Optimal\", \n                         paste0(\"+\", lines$pct_increase, \"%\"))\n\n# Assign distinct colors for better visibility\nline_colors &lt;- c(\"Best Fit (OLS)\" = \"blue\", \n                \"Line A\" = \"red\", \n                \"Line B\" = \"darkgreen\", \n                \"Line C\" = \"purple\")\n\n# Create data for mini residual plots\nmini_data &lt;- data.frame()\nfor(i in 1:nrow(lines)) {\n  line_data &lt;- data.frame(\n    x = x,\n    y = y,\n    predicted = lines$intercept[i] + lines$slope[i] * x,\n    residuals = y - (lines$intercept[i] + lines$slope[i] * x),\n    line = lines$label[i]\n  )\n  mini_data &lt;- rbind(mini_data, line_data)\n}\n\n# Create main comparison plot with improved visibility\np1 &lt;- ggplot(data, aes(x = x, y = y)) +\n  # Add background grid for reference\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_line(color = \"gray90\"),\n    panel.grid.major = element_line(color = \"gray85\"),\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 13),\n    axis.title = element_text(size = 13, face = \"bold\"),\n    axis.text = element_text(size = 12)\n  ) +\n  # Add data points\n  geom_point(size = 2.5, alpha = 0.8) +\n  # Add lines with improved visibility\n  geom_abline(data = lines, \n              aes(intercept = intercept, slope = slope, \n                  color = label, linetype = label == \"Best Fit (OLS)\"),\n              size = 1.2) +\n  # Use custom colors\n  scale_color_manual(values = line_colors) +\n  scale_linetype_manual(values = c(\"TRUE\" = \"solid\", \"FALSE\" = \"dashed\"), guide = \"none\") +\n  # Better legends\n  labs(title = \"Comparing Different Regression Lines\",\n       subtitle = \"The OLS line minimizes the sum of squared errors\",\n       x = \"X\", y = \"Y\",\n       color = \"Regression Line\") +\n  guides(color = guide_legend(override.aes = list(size = 2)))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n# Create mini residual plots with improved visibility\np_mini &lt;- list()\n\nfor(i in 1:nrow(lines)) {\n  line_data &lt;- subset(mini_data, line == lines$label[i])\n  \n  p_mini[[i]] &lt;- ggplot(line_data, aes(x = x, y = residuals)) +\n    # Add reference line\n    geom_hline(yintercept = 0, linetype = \"dashed\", size = 0.8, color = \"gray50\") +\n    # Add residual points with line color\n    geom_point(color = line_colors[lines$label[i]], size = 2.5) +\n    # Add squares to represent squared errors\n    geom_rect(aes(xmin = x - 0.3, xmax = x + 0.3,\n                  ymin = 0, ymax = residuals),\n              fill = line_colors[lines$label[i]], alpha = 0.2) +\n    # Improved titles\n    labs(title = lines$label[i],\n         subtitle = paste(\"SSE =\", round(lines$sse[i], 1), \n                          ifelse(i == 1, \" (Optimal)\", \n                                 paste0(\" (+\", lines$pct_increase[i], \"%)\"))),\n         x = NULL, y = NULL) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(size = 12, face = \"bold\", color = line_colors[lines$label[i]]),\n      plot.subtitle = element_text(size = 10),\n      panel.grid.minor = element_blank()\n    )\n}\n\n# Create SSE comparison table with better visibility\nsse_df &lt;- data.frame(\n  x = rep(1, nrow(lines)),\n  y = nrow(lines):1,\n  label = paste0(lines$label, \": SSE = \", round(lines$sse, 1), \" (\", lines$pct_text, \")\"),\n  color = line_colors[lines$label]\n)\n\nsse_table &lt;- ggplot(sse_df, aes(x = x, y = y, label = label, color = color)) +\n  geom_text(hjust = 0, size = 5, fontface = \"bold\") +\n  scale_color_identity() +\n  theme_void() +\n  xlim(1, 10) +\n  ylim(0.5, nrow(lines) + 0.5) +\n  labs(title = \"Sum of Squared Errors (SSE) Comparison\") +\n  theme(plot.title = element_text(hjust = 0, face = \"bold\", size = 14))\n\n# Arrange the plots with better spacing\ngrid.arrange(\n  p1, \n  arrangeGrob(p_mini[[1]], p_mini[[2]], p_mini[[3]], p_mini[[4]], \n              ncol = 2, padding = unit(1, \"cm\")),\n  sse_table, \n  ncol = 1, \n  heights = c(4, 3, 1)\n)\n\n\n\n\nComparing different regression lines\n\n\n\n\n\n\nKey Learning Points\n\nThe Sum of Squared Errors (SSE) is what Ordinary Least Squares (OLS) regression minimizes\nEach residual contributes its squared value to the total SSE\nThe OLS line has a lower SSE than any other possible line\nLarge residuals contribute disproportionately to the SSE due to the squaring operation\nThis is why outliers can have such a strong influence on regression lines\n\n\nStep-by-Step SSE Minimization\nTo illustrate the process of finding the minimum SSE, we can create a sequence that passes through the optimal point, showing how the SSE first decreases to a minimum and then increases again:\n\n# Create sample data\nset.seed(123)\nx &lt;- 1:20\ny &lt;- 2 + 3*x + rnorm(20, 0, 5)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit regression model\nmodel &lt;- lm(y ~ x, data = data)\ncoef &lt;- coefficients(model)\n\n# Create a sequence of steps that passes through the optimal OLS line\nsteps &lt;- 9  # Use odd number to have a middle point at the optimum\nstep_seq &lt;- data.frame(\n  step = 1:steps,\n  intercept = seq(coef[1] - 8, coef[1] + 8, length.out = steps),\n  slope = seq(coef[2] - 1.5, coef[2] + 1.5, length.out = steps)\n)\n\n# Mark the middle step (optimal OLS solution)\noptimal_step &lt;- ceiling(steps/2)\n\n# Calculate SSE for each step\nstep_seq$sse &lt;- sapply(1:nrow(step_seq), function(i) {\n  predicted &lt;- step_seq$intercept[i] + step_seq$slope[i] * x\n  sum((y - predicted)^2)\n})\n\n# Create a \"journey through the SSE valley\" plot\np2 &lt;- ggplot(data, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_abline(data = step_seq, \n              aes(intercept = intercept, slope = slope, \n                  color = sse, group = step),\n              size = 1) +\n  # Highlight the optimal line\n  geom_abline(intercept = step_seq$intercept[optimal_step], \n              slope = step_seq$slope[optimal_step],\n              color = \"green\", size = 1.5) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  labs(title = \"Journey Through the SSE Valley\",\n       subtitle = \"The green line represents the OLS solution with minimum SSE\",\n       color = \"SSE Value\") +\n  theme_minimal()\n\n# Create an SSE valley plot\np3 &lt;- ggplot(step_seq, aes(x = step, y = sse)) +\n  geom_line(size = 1) +\n  geom_point(size = 3, aes(color = sse)) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  # Highlight the optimal point\n  geom_point(data = step_seq[optimal_step, ], aes(x = step, y = sse), \n             size = 5, color = \"green\") +\n  # Add annotation\n  annotate(\"text\", x = optimal_step, y = step_seq$sse[optimal_step] * 1.1, \n           label = \"Minimum SSE\", color = \"darkgreen\", fontface = \"bold\") +\n  labs(title = \"The SSE Valley: Decreasing Then Increasing\",\n       subtitle = \"The SSE reaches its minimum at the OLS solution\",\n       x = \"Step\",\n       y = \"Sum of Squared Errors\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Display both plots\ngrid.arrange(p2, p3, ncol = 1, heights = c(3, 2))\n\n\n\n\nSSE minimization visualization\n\n\n\n\nIn R, the lm() function fits linear regression models:\n\nmodel &lt;- lm(y ~ x, data = data_frame)\n\n\n\n\nModel Interpretation: A Beginner‚Äôs Guide\nLet‚Äôs create a simple dataset to understand regression output better. Imagine we‚Äôre studying how years of education affect annual income:\n\n# Create a simple dataset - this is our Data Generating Process (DGP)\nset.seed(123) # For reproducibility\neducation_years &lt;- 10:20  # Education from 10 to 20 years\nn &lt;- length(education_years)\n\n# True parameters in our model - using more realistic values for Poland\ntrue_intercept &lt;- 3000   # Base monthly income with no education (in PLN)\ntrue_slope &lt;- 250        # Each year of education increases monthly income by 250 PLN\n\n# Generate monthly incomes with some random noise\nincome &lt;- true_intercept + true_slope * education_years + rnorm(n, mean=0, sd=300)\n\n# Create our dataset\neducation_income &lt;- data.frame(\n  education = education_years,\n  income = income\n)\n\n# Let's visualize our data\nlibrary(ggplot2)\nggplot(education_income, aes(x = education, y = income)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  scale_y_continuous(limits = c(5000, 8500), \n                     breaks = seq(5000, 8500, by = 500),\n                     labels = scales::comma) +\n  scale_x_continuous(breaks = 10:20) +\n  labs(\n    title = \"Relationship between Education and Income in Poland\",\n    x = \"Years of Education\",\n    y = \"Monthly Income (PLN)\",\n    subtitle = \"Red line shows the estimated linear relationship\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\")\n  ) +\n  annotate(\"text\", x = 11, y = 8000, \n           label = \"Each point represents\\none person's data\", \n           hjust = 0, size = 4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nFitting the Model\nNow let‚Äôs fit a linear regression model to this data:\n\n# Fit a simple regression model\nedu_income_model &lt;- lm(income ~ education, data = education_income)\n\n# Display the results\nmodel_summary &lt;- summary(edu_income_model)\nmodel_summary\n\n\nCall:\nlm(formula = income ~ education, data = education_income)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-427.72 -206.04  -38.12  207.32  460.78 \n\nCoefficients:\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)   3095.3      447.6   6.915 0.0000695 ***\neducation      247.2       29.2   8.467 0.0000140 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 306.3 on 9 degrees of freedom\nMultiple R-squared:  0.8885,    Adjusted R-squared:  0.8761 \nF-statistic: 71.69 on 1 and 9 DF,  p-value: 0.00001403\n\n\n\n\nUnderstanding the Regression Output Step by Step\nLet‚Äôs break down what each part of this output means in simple terms:\n\n1. The Formula\nAt the top, you see income ~ education, which means we‚Äôre predicting income based on education.\n\n\n2. Residuals\nThese show how far our predictions are from the actual values. Ideally, they should be centered around zero.\n\n\n3. Coefficients Table\n\n\n\nCoefficient Estimates\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n3095.27\n447.63\n6.91\n0\n\n\neducation\n247.23\n29.20\n8.47\n0\n\n\n\n\n\nIntercept (\\beta_0):\n\nValue: Approximately 3095\nInterpretation: This is the predicted monthly income for someone with 0 years of education\nNote: Sometimes the intercept isn‚Äôt meaningful in real-world terms, especially if x=0 is outside your data range\n\nEducation (\\beta_1):\n\nValue: Approximately 247\nInterpretation: For each additional year of education, we expect monthly income to increase by this amount in PLN\nThis is our main coefficient of interest!\n\nStandard Error:\n\nMeasures how precise our estimates are\nSmaller standard errors mean more precise estimates\nThink of it as ‚Äúgive or take how much‚Äù for our coefficients\n\nt value:\n\nThis is the coefficient divided by its standard error\nIt tells us how many standard errors away from zero our coefficient is\nLarger absolute t values (above 2) suggest the effect is statistically significant\n\np-value:\n\nThe probability of seeing our result (or something more extreme) if there was actually no relationship\nTypically, p &lt; 0.05 is considered statistically significant\nFor education, p = 0.000014, which is significant!\n\n\n\n4. Model Fit Statistics\n\n\n\nModel Fit Statistics\n\n\nStatistic\nValue\n\n\n\n\nR-squared\n0.888\n\n\nAdjusted R-squared\n0.876\n\n\nF-statistic\n71.686\n\n\np-value\n0.000\n\n\n\n\n\nR-squared:\n\nValue: 0.888\nInterpretation: 89% of the variation in income is explained by education\nHigher is better, but be cautious of very high values (could indicate overfitting)\n\nF-statistic:\n\nTests whether the model as a whole is statistically significant\nA high F-statistic with a low p-value indicates a significant model\n\n\n\n\nVisualizing the Model Results\nLet‚Äôs visualize what our model actually tells us:\n\n# Predicted values\neducation_income$predicted &lt;- predict(edu_income_model)\neducation_income$residuals &lt;- residuals(edu_income_model)\n\n# Create a more informative plot\nggplot(education_income, aes(x = education, y = income)) +\n  # Actual data points\n  geom_point(size = 3, color = \"blue\") +\n  \n  # Regression line\n  geom_line(aes(y = predicted), color = \"red\", size = 1.2) +\n  \n  # Residual lines\n  geom_segment(aes(xend = education, yend = predicted), \n               color = \"darkgray\", linetype = \"dashed\") +\n  \n  # Set proper scales\n  scale_y_continuous(limits = c(5000, 8500), \n                     breaks = seq(5000, 8500, by = 500),\n                     labels = scales::comma) +\n  scale_x_continuous(breaks = 10:20) +\n  \n  # Annotations\n  annotate(\"text\", x = 19, y = 7850, \n           label = paste(\"Slope =\", round(coef(edu_income_model)[2]), \"PLN per year\"),\n           color = \"red\", hjust = 1, fontface = \"bold\") +\n  \n  annotate(\"text\", x = 10.5, y = 5500, \n           label = paste(\"Intercept =\", round(coef(edu_income_model)[1]), \"PLN\"),\n           color = \"red\", hjust = 0, fontface = \"bold\") +\n  \n  annotate(\"text\", x = 14, y = 8200, \n           label = paste(\"R¬≤ =\", round(model_summary$r.squared, 2)),\n           color = \"black\", fontface = \"bold\") +\n  \n  # Labels\n  labs(\n    title = \"Interpreting the Education-Income Regression Model\",\n    subtitle = \"Red line shows predicted income for each education level\",\n    x = \"Years of Education\",\n    y = \"Monthly Income (PLN)\",\n    caption = \"Gray dashed lines represent residuals (prediction errors)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\nReal-World Interpretation\n\nA person with 16 years of education (college graduate) would be predicted to earn about: \\hat{Y} = 3095 + 247 \\times 16 = 7051 \\text{ PLN monthly}\nThe model suggests that each additional year of education is associated with a 247 PLN increase in monthly income.\nOur model explains approximately 89% of the variation in income in our sample.\nThe relationship is statistically significant (p &lt; 0.001), meaning it‚Äôs very unlikely to observe this relationship if education truly had no effect on income.\n\n\n\nImportant Cautions for Beginners\n\nCorrelation ‚â† Causation: Our model shows association, not necessarily causation\nOmitted Variables: Other factors might influence both education and income\nExtrapolation: Be careful predicting outside the range of your data\nLinear Relationship: We‚Äôve assumed the relationship is linear, which may not always be true",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#regression-analysis-and-ordinary-least-squares",
    "href": "correg_en.html#regression-analysis-and-ordinary-least-squares",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.27 Regression Analysis and Ordinary Least Squares (*)",
    "text": "9.27 Regression Analysis and Ordinary Least Squares (*)\n\nFoundations of Regression Analysis\nRegression analysis constitutes a fundamental statistical methodology for examining relationships between variables. At its core, regression provides a systematic framework for understanding how changes in one or more independent variables influence a dependent variable.\nThe primary objectives of regression analysis include: - Quantifying relationships between variables - Making predictions based on observed patterns - Testing hypotheses about variable associations - Understanding the proportion of variation explained by predictors\n\n\nDeterministic versus Stochastic Models\nStatistical modeling encompasses two fundamental approaches:\nDeterministic models assume precise, invariant relationships between variables. Given specific inputs, these models yield identical outputs without variation. Consider the physics equation:\n\\text{Distance} = \\text{Speed} \\times \\text{Time}\nThis relationship exhibits no randomness; identical inputs always produce identical outputs.\nStochastic models, in contrast, acknowledge inherent variability in real-world phenomena. Regression analysis employs stochastic modeling through the fundamental equation:\nY = f(X) + \\epsilon\nWhere: - Y represents the outcome variable - f(X) captures the systematic relationship between predictors and outcome - \\epsilon represents random variation inherent in the data\nThis formulation recognizes that real-world relationships contain both systematic patterns and random variation.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#simple-linear-regression-model",
    "href": "correg_en.html#simple-linear-regression-model",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.28 Simple Linear Regression Model",
    "text": "9.28 Simple Linear Regression Model\n\nModel Specification\nSimple linear regression models the relationship between a single predictor variable and an outcome variable through a linear equation:\nY_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\nThe model components represent: - Y_i: The dependent variable for observation i - X_i: The independent variable for observation i - \\beta_0: The population intercept parameter - \\beta_1: The population slope parameter - \\epsilon_i: The random error term for observation i\n\n\nInterpretation of Parameters\nThe parameters possess specific interpretations:\n\nIntercept (\\beta_0): The expected value of Y when X = 0. This represents the baseline level of the outcome variable.\nSlope (\\beta_1): The expected change in Y for a one-unit increase in X. This quantifies the strength and direction of the linear relationship.\nError term (\\epsilon_i): Captures all factors affecting Y not explained by X, including measurement error, omitted variables, and inherent randomness.\n\n\n\nEstimation versus True Parameters\nThe distinction between population parameters and sample estimates proves crucial:\n\nPopulation parameters (\\beta_0, \\beta_1) represent true, unknown values\nSample estimates (\\hat{\\beta}_0, \\hat{\\beta}_1) represent our best approximations based on available data\nThe hat notation (^) consistently denotes estimated values\n\nThe fitted regression equation becomes:\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1X_i",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-ordinary-least-squares-method-1",
    "href": "correg_en.html#the-ordinary-least-squares-method-1",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.29 The Ordinary Least Squares Method",
    "text": "9.29 The Ordinary Least Squares Method\n\nThe Fundamental Challenge\nGiven a dataset with observations (X_i, Y_i), we need a systematic method to determine the ‚Äúbest‚Äù values for \\hat{\\beta}_0 and \\hat{\\beta}_1. The challenge lies in defining what constitutes ‚Äúbest‚Äù and developing a practical method to find these values.\nConsider that for any given line through the data, each observation will have a prediction error or residual:\ne_i = Y_i - \\hat{Y}_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1X_i)\nThese residuals represent how far our predictions deviate from actual values. A good fitting line should make these residuals as small as possible overall.\n\n\nWhy Minimize the Sum of Squared Residuals?\nThe Ordinary Least Squares method determines optimal parameter estimates by minimizing the sum of squared residuals. This choice requires justification, as we could conceivably minimize other quantities. The rationale for squaring residuals includes:\nMathematical tractability: Squaring creates a smooth, differentiable function that yields closed-form solutions through calculus. The derivatives of squared terms lead to linear equations that can be solved analytically.\nEqual treatment of positive and negative errors: Simply summing raw residuals would allow positive and negative errors to cancel, potentially yielding a sum of zero even when predictions are poor. Squaring ensures all deviations contribute positively to the total error measure.\nPenalization of large errors: Squaring gives progressively greater weight to larger errors. An error of 4 units contributes 16 to the sum, while an error of 2 units contributes only 4. This property encourages finding a line that avoids extreme prediction errors.\nStatistical optimality: Under certain assumptions (including normally distributed errors), OLS estimators possess desirable statistical properties, including being the Best Linear Unbiased Estimators (BLUE) according to the Gauss-Markov theorem.\nConnection to variance: The sum of squared deviations directly relates to variance, a fundamental measure of spread in statistics. Minimizing squared residuals thus minimizes the variance of prediction errors.\n\n\nThe OLS Optimization Problem\nThe OLS method formally seeks values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that minimize:\n\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1X_i))^2\nThis optimization problem can be solved using calculus by: 1. Taking partial derivatives with respect to \\hat{\\beta}_0 and \\hat{\\beta}_1 2. Setting these derivatives equal to zero 3. Solving the resulting system of equations\n\n\nDerivation of OLS Estimators\nThe minimization yields closed-form solutions:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n(X_i - \\bar{X})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\nThese formulas reveal that: - The slope estimate depends on the covariance between variables relative to the predictor‚Äôs variance - The intercept ensures the regression line passes through the point of means (\\bar{X}, \\bar{Y})\n\n\nProperties of OLS Estimators\nOLS estimators possess several desirable properties:\n\nUnbiasedness: Under appropriate conditions, E[\\hat{\\beta}_j] = \\beta_j\nEfficiency: OLS provides minimum variance among linear unbiased estimators\nConsistency: As sample size increases, estimates converge to true values\nThe regression line passes through the centroid: The point (\\bar{X}, \\bar{Y}) always lies on the fitted line\n\n\n\nExtension to Multiple Regression\nWhile this guide focuses on simple linear regression with one predictor, the OLS framework extends naturally to multiple regression with several predictors:\nY_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + ... + \\beta_kX_{ki} + \\epsilon_i\nThe same principle applies: we minimize the sum of squared residuals, though the mathematics involves matrix algebra rather than simple formulas. The fundamental logic‚Äîfinding parameter values that minimize prediction errors‚Äîremains unchanged.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#understanding-variance-decomposition",
    "href": "correg_en.html#understanding-variance-decomposition",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.30 Understanding Variance Decomposition",
    "text": "9.30 Understanding Variance Decomposition\n\nThe Baseline Model Concept\nBefore introducing predictors, consider the simplest possible model: predicting every observation using the overall mean \\bar{Y}. This baseline model represents our best prediction in the absence of additional information.\nThe baseline model‚Äôs predictions: \\hat{Y}_i^{\\text{baseline}} = \\bar{Y} \\text{ for all } i\nThis model serves as a reference point for evaluating improvement gained through incorporating predictors. The baseline model essentially asks: ‚ÄúIf we knew nothing about the relationship between X and Y, what would be our best constant prediction?‚Äù\n\n\nComponents of Total Variation\nThe total variation in the outcome variable decomposes into three fundamental components:\n\nTotal Sum of Squares (SST)\nSST quantifies the total variation in the outcome variable relative to its mean:\n\\text{SST} = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\nInterpretation: SST represents the total variance that requires explanation. It measures the prediction error when using only the mean as our model‚Äîessentially the variance explained by the baseline (zero) model. This is the starting point: the total amount of variation we hope to explain by introducing predictors.\n\n\nRegression Sum of Squares (SSR)\nSSR measures the variation explained by the regression model:\n\\text{SSR} = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\nInterpretation: SSR quantifies the improvement in prediction achieved by incorporating the predictor variable. It represents the reduction in prediction error relative to the baseline model‚Äîthe portion of total variation that our regression line successfully captures.\n\n\nError Sum of Squares (SSE)\nSSE captures the unexplained variation remaining after regression:\n\\text{SSE} = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\nInterpretation: SSE represents the residual variation that the model cannot explain, reflecting the inherent randomness and effects of omitted variables. This is the variation that remains even after using our best-fitting line.\n\n\n\nThe Fundamental Decomposition Identity\nThese components relate through the fundamental equation:\n\\text{SST} = \\text{SSR} + \\text{SSE}\nThis identity demonstrates that: - Total variation equals the sum of explained and unexplained components - The regression model partitions total variation into systematic and random parts - Model improvement can be assessed by comparing SSR to SST\n\n\nConceptual Framework for Variance Decomposition\n# Demonstration of Variance Decomposition\nlibrary(ggplot2)\n\n# Generate sample data\nset.seed(42)\nn &lt;- 50\nx &lt;- runif(n, 1, 10)\ny &lt;- 3 + 2*x + rnorm(n, 0, 2)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit model\nmodel &lt;- lm(y ~ x, data = data)\ny_mean &lt;- mean(y)\ny_pred &lt;- predict(model)\n\n# Calculate components\nSST &lt;- sum((y - y_mean)^2)\nSSR &lt;- sum((y_pred - y_mean)^2)\nSSE &lt;- sum((y - y_pred)^2)\n\n# Display decomposition\ncat(\"Variance Decomposition\\n\")\ncat(\"======================\\n\")\ncat(\"Total SS (SST):\", round(SST, 2), \n    \"- Total variation from mean\\n\")\ncat(\"Regression SS (SSR):\", round(SSR, 2), \n    \"- Variation explained by model\\n\")\ncat(\"Error SS (SSE):\", round(SSE, 2), \n    \"- Unexplained variation\\n\")\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\ncat(round(SST, 2), \"=\", round(SSR, 2), \"+\", \n    round(SSE, 2), \"\\n\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-coefficient-of-determination-r¬≤",
    "href": "correg_en.html#the-coefficient-of-determination-r¬≤",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.31 The Coefficient of Determination (R¬≤)",
    "text": "9.31 The Coefficient of Determination (R¬≤)\n\nDefinition and Calculation\nThe coefficient of determination, denoted R¬≤, quantifies the proportion of total variation explained by the regression model:\nR^2 = \\frac{\\text{SSR}}{\\text{SST}} = \\frac{\\text{Explained Variation}}{\\text{Total Variation}}\nAlternatively expressed as:\nR^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}} = 1 - \\frac{\\text{Unexplained Variation}}{\\text{Total Variation}}\nR¬≤ directly answers the question: ‚ÄúWhat proportion of the total variation in Y (relative to the baseline mean model) does our regression model explain?‚Äù\n\n\nInterpretation Guidelines\nR¬≤ values range from 0 to 1, with specific interpretations:\n\nR¬≤ = 0: The model explains no variation beyond the baseline mean model. The regression line provides no improvement over simply using \\bar{Y}.\nR¬≤ = 0.25: The model explains 25% of total variation. Three-quarters of the variation remains unexplained.\nR¬≤ = 0.75: The model explains 75% of total variation. This represents substantial explanatory power.\nR¬≤ = 1.00: The model explains all variation (perfect fit). All data points fall exactly on the regression line.\n\n\n\nContextual Considerations\nThe interpretation of R¬≤ requires careful consideration of context:\nField-specific standards: Acceptable R¬≤ values vary dramatically across disciplines - Physical sciences often expect R¬≤ &gt; 0.90 due to controlled conditions - Social sciences may consider R¬≤ = 0.30 meaningful given human complexity - Biological systems typically show intermediate values due to natural variation\nSample size effects: Small samples can artificially inflate R¬≤, leading to overly optimistic assessments of model fit.\nModel complexity: In multiple regression, additional predictors mechanically increase R¬≤, even if they lack true explanatory power.\nPractical significance: Statistical fit should align with substantive importance. A model with R¬≤ = 0.95 may be less useful than one with R¬≤ = 0.60 if the latter addresses more relevant questions.\n\n\nAdjusted R¬≤ for Multiple Regression\nWhen extending to multiple regression, adjusted R¬≤ accounts for the number of predictors:\nR^2_{\\text{adj}} = 1 - \\frac{\\text{SSE}/(n-p)}{\\text{SST}/(n-1)}\nWhere: - n = sample size - p = number of parameters (including intercept)\nAdjusted R¬≤ penalizes model complexity, providing a more conservative measure when comparing models with different numbers of predictors.\n\n\nMeasures of Fit\n\nR-squared (R^2): R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nRoot Mean Square Error (RMSE): RMSE = \\sqrt{\\frac{SSE}{n}}\nMean Absolute Error (MAE): MAE = \\frac{1}{n}\\sum_{i=1}^n |Y_i - \\hat{Y}_i|",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#practical-implementation-1",
    "href": "correg_en.html#practical-implementation-1",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.32 Practical Implementation",
    "text": "9.32 Practical Implementation\n\nComplete Regression Analysis Example\n\n# Comprehensive regression analysis\nlibrary(ggplot2)\n\n# Generate educational data\nset.seed(123)\nn &lt;- 100\nstudy_hours &lt;- runif(n, 0, 10)\nexam_scores &lt;- 50 + 4*study_hours + rnorm(n, 0, 5)\neducation_data &lt;- data.frame(\n  study_hours = study_hours,\n  exam_scores = exam_scores\n)\n\n# Fit regression model\nmodel &lt;- lm(exam_scores ~ study_hours, data = education_data)\n\n# Extract key statistics\ny_mean &lt;- mean(exam_scores)\ny_pred &lt;- predict(model)\n\n# Variance decomposition\nSST &lt;- sum((exam_scores - y_mean)^2)\nSSR &lt;- sum((y_pred - y_mean)^2)\nSSE &lt;- sum((exam_scores - y_pred)^2)\nr_squared &lt;- SSR/SST\n\n# Display results\ncat(\"OLS Regression Results\\n\")\n\nOLS Regression Results\n\ncat(\"======================\\n\")\n\n======================\n\ncat(\"\\nParameter Estimates:\\n\")\n\n\nParameter Estimates:\n\ncat(\"Intercept (Œ≤‚ÇÄ):\", round(coef(model)[1], 2), \"\\n\")\n\nIntercept (Œ≤‚ÇÄ): 49.96 \n\ncat(\"Slope (Œ≤‚ÇÅ):\", round(coef(model)[2], 2), \"\\n\")\n\nSlope (Œ≤‚ÇÅ): 3.96 \n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Expected score with 0 study hours:\", \n    round(coef(model)[1], 2), \"\\n\")\n\n- Expected score with 0 study hours: 49.96 \n\ncat(\"- Score increase per study hour:\", \n    round(coef(model)[2], 2), \"\\n\")\n\n- Score increase per study hour: 3.96 \n\ncat(\"\\nVariance Decomposition:\\n\")\n\n\nVariance Decomposition:\n\ncat(\"Total variation (SST):\", round(SST, 2), \"\\n\")\n\nTotal variation (SST): 14880.14 \n\ncat(\"Explained by model (SSR):\", round(SSR, 2), \"\\n\")\n\nExplained by model (SSR): 12578.3 \n\ncat(\"Unexplained (SSE):\", round(SSE, 2), \"\\n\")\n\nUnexplained (SSE): 2301.84 \n\ncat(\"\\nModel Performance:\\n\")\n\n\nModel Performance:\n\ncat(\"R-squared:\", round(r_squared, 4), \"\\n\")\n\nR-squared: 0.8453 \n\ncat(\"Interpretation: The model explains\", \n    round(r_squared * 100, 1), \n    \"% of the variation in exam scores\\n\")\n\nInterpretation: The model explains 84.5 % of the variation in exam scores\n\ncat(\"beyond what the mean alone could explain.\\n\")\n\nbeyond what the mean alone could explain.\n\n\n\n\nVisualization of Key Concepts\n\n# Create comprehensive visualization\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Plot 1: Baseline model (mean only)\np1 &lt;- ggplot(education_data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = y_mean, color = \"blue\", size = 1) +\n  geom_segment(aes(xend = study_hours, yend = y_mean), \n               alpha = 0.3, color = \"red\") +\n  labs(title = \"Baseline Model: Predicting with Mean\",\n       subtitle = paste(\"Total variation (SST) =\", round(SST, 1)),\n       x = \"Study Hours\", y = \"Exam Scores\") +\n  theme_minimal() +\n  annotate(\"text\", x = 8, y = y_mean + 1, \n           label = \"Mean of Y\", color = \"blue\")\n\n# Plot 2: Regression model\np2 &lt;- ggplot(education_data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_segment(aes(xend = study_hours, yend = y_pred), \n               alpha = 0.3, color = \"red\") +\n  labs(title = \"Regression Model: Improved Predictions\",\n       subtitle = paste(\"Unexplained variation (SSE) =\", \n                       round(SSE, 1), \n                       \"| R¬≤ =\", round(r_squared, 3)),\n       x = \"Study Hours\", y = \"Exam Scores\") +\n  theme_minimal() +\n  annotate(\"text\", x = 8, y = max(y_pred) + 1, \n           label = \"Regression Line\", color = \"blue\")\n\n# Plot 3: Variance components\nvariance_data &lt;- data.frame(\n  Component = c(\"Total (SST)\", \"Explained (SSR)\", \"Unexplained (SSE)\"),\n  Value = c(SST, SSR, SSE),\n  Type = c(\"Total\", \"Explained\", \"Unexplained\")\n)\n\np3 &lt;- ggplot(variance_data, aes(x = Component, y = Value, fill = Type)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"Total\" = \"gray50\", \n                               \"Explained\" = \"green4\", \n                               \"Unexplained\" = \"red3\")) +\n  labs(title = \"Variance Decomposition\",\n       y = \"Sum of Squares\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Combine plots\ngrid.arrange(p1, p2, p3, layout_matrix = rbind(c(1, 2), c(3, 3)))\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-and-key-insights",
    "href": "correg_en.html#summary-and-key-insights",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.33 Summary and Key Insights",
    "text": "9.33 Summary and Key Insights\n\nCore Concepts Review\nRegression analysis models relationships between variables using stochastic frameworks that acknowledge inherent variation. The simple linear regression model expresses this relationship as Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i.\nOrdinary Least Squares provides optimal parameter estimates by minimizing the sum of squared residuals. This choice of minimizing squared errors stems from mathematical tractability, equal treatment of positive and negative errors, appropriate penalization of large errors, and desirable statistical properties.\nVariance decomposition partitions total variation into: - Total variation from the baseline mean model (SST) - Variation explained by the regression model (SSR)\n- Unexplained residual variation (SSE)\nThe fundamental identity SST = SSR + SSE shows how regression improves upon the baseline model.\nR¬≤ quantifies model performance as the proportion of total variation explained, providing a standardized measure of how much better the regression model performs compared to simply using the mean.\n\n\nCritical Considerations\nThe baseline model (predicting with the mean) serves as the fundamental reference point. All regression improvement is measured relative to this simple model. SST represents the total variance requiring explanation when we start with no predictors.\nParameter estimates (\\hat{\\beta}_0, \\hat{\\beta}_1) represent sample-based approximations of unknown population values (\\beta_0, \\beta_1). The distinction between population parameters and sample estimates remains crucial for proper inference.\nModel assessment requires considering both statistical fit (R¬≤) and practical significance. A model with modest R¬≤ may still provide valuable insights, while high R¬≤ does not guarantee causation or practical utility.\n\n\nExtensions and Applications\nWhile this guide focuses on simple linear regression, the framework extends naturally to: - Multiple regression with several predictors - Polynomial regression for nonlinear relationships - Interaction terms to capture conditional effects - Categorical predictors through appropriate coding\nThe fundamental principles‚Äîminimizing prediction errors, decomposing variation, and assessing model fit‚Äîremain consistent across these extensions. The mathematical complexity increases, but the conceptual foundation established here continues to apply.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#key-assumptions-of-linear-regression",
    "href": "correg_en.html#key-assumptions-of-linear-regression",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.34 Key Assumptions of Linear Regression",
    "text": "9.34 Key Assumptions of Linear Regression\n\nStrict Exogeneity: The Fundamental Assumption\nThe most crucial assumption in regression is strict exogeneity:\nE[\\varepsilon|X] = 0\nThis means:\n\nThe error term has zero mean conditional on X\nX contains no information about the average error\nThere are no systematic patterns in how our predictions are wrong\n\nLet‚Äôs visualize when this assumption holds and when it doesn‚Äôt:\n\n# Generate data\nset.seed(789)\nx &lt;- seq(1, 10, by = 0.2)\n\n# Case 1: Exogenous errors\ny_exog &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\n\n# Case 2: Non-exogenous errors (error variance increases with x)\ny_nonexog &lt;- 2 + 3*x + 0.5*x*rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_exog &lt;- data.frame(\n  x = x,\n  y = y_exog,\n  type = \"Exogenous Errors\\n(Assumption Satisfied)\"\n)\n\ndata_nonexog &lt;- data.frame(\n  x = x,\n  y = y_nonexog,\n  type = \"Non-Exogenous Errors\\n(Assumption Violated)\"\n)\n\ndata_combined &lt;- rbind(data_exog, data_nonexog)\n\n# Create plots with residuals\nplot_residuals &lt;- function(data, title) {\n  model &lt;- lm(y ~ x, data = data)\n  data$predicted &lt;- predict(model)\n  data$residuals &lt;- residuals(model)\n  \n  p1 &lt;- ggplot(data, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    theme_minimal() +\n    labs(title = title)\n  \n  p2 &lt;- ggplot(data, aes(x = x, y = residuals)) +\n    geom_point() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n    theme_minimal() +\n    labs(y = \"Residuals\")\n  \n  list(p1, p2)\n}\n\n# Generate plots\nplots_exog &lt;- plot_residuals(data_exog, \"Exogenous Errors\")\nplots_nonexog &lt;- plot_residuals(data_nonexog, \"Non-Exogenous Errors\")\n\n# Arrange plots\ngridExtra::grid.arrange(\n  plots_exog[[1]], plots_exog[[2]],\n  plots_nonexog[[1]], plots_nonexog[[2]],\n  ncol = 2\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure¬†9.1: Exogeneity vs.¬†Non-Exogeneity Examples\n\n\n\n\n\n\n\nLinearity: The Form Assumption\nThe relationship between X and Y should be linear in parameters:\nE[Y|X] = \\beta_0 + \\beta_1X\nNote that this doesn‚Äôt mean X and Y must have a straight-line relationship - we can transform variables. Let‚Äôs see different types of relationships:\n\n# Generate data\nset.seed(101)\nx &lt;- seq(1, 10, by = 0.1)\n\n# Different relationships\ndata_relationships &lt;- data.frame(\n  x = rep(x, 3),\n  y = c(\n    # Linear\n    2 + 3*x + rnorm(length(x), 0, 2),\n    # Quadratic\n    2 + 0.5*x^2 + rnorm(length(x), 0, 2),\n    # Exponential\n    exp(0.3*x) + rnorm(length(x), 0, 2)\n  ),\n  type = rep(c(\"Linear\", \"Quadratic\", \"Exponential\"), each = length(x))\n)\n\n# Plot\nggplot(data_relationships, aes(x = x, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  facet_wrap(~type, scales = \"free_y\") +\n  theme_minimal() +\n  labs(subtitle = \"Red: linear fit, Blue: true relationship\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure¬†9.2: Linear and Nonlinear Relationships\n\n\n\n\n\n\n\nUnderstanding Violations and Solutions\nWhen linearity is violated:\n\nTransform variables:\n\nLog transformation: for exponential relationships\nSquare root: for moderate nonlinearity\nPower transformations: for more complex relationships\n\n\n\n# Generate exponential data\nset.seed(102)\nx &lt;- seq(1, 10, by = 0.2)\ny &lt;- exp(0.3*x) + rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_trans &lt;- data.frame(\n  x = x,\n  y = y,\n  log_y = log(y)\n)\n\nWarning in log(y): NaNs produced\n\n# Original scale plot\np1 &lt;- ggplot(data_trans, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Original Scale\")\n\n# Log scale plot\np2 &lt;- ggplot(data_trans, aes(x = x, y = log_y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Log-Transformed Y\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFigure¬†9.3: Effect of Variable Transformations",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#spurious-correlation-causes-and-examples",
    "href": "correg_en.html#spurious-correlation-causes-and-examples",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.35 Spurious Correlation: Causes and Examples",
    "text": "9.35 Spurious Correlation: Causes and Examples\nSpurious correlation occurs when variables appear related but the relationship is not causal. These misleading correlations arise from several sources:\n\nRandom coincidence (chance)\nConfounding variables (hidden third factors)\nSelection biases\nImproper statistical analysis\nReverse causality\nEndogeneity problems (including simultaneity)\n\n\nRandom Coincidence (Chance)\nWith sufficient data mining or small sample sizes, seemingly meaningful correlations can emerge purely by chance. This is especially problematic when researchers conduct multiple analyses without appropriate corrections for multiple comparisons, a practice known as ‚Äúp-hacking.‚Äù\n\n# Create a realistic example of spurious correlation based on actual country data\n# Using country data on chocolate consumption and Nobel prize winners\n# This example is inspired by a published correlation (Messerli, 2012)\nset.seed(123)\ncountries &lt;- c(\"Switzerland\", \"Sweden\", \"Denmark\", \"Belgium\", \"Austria\", \n               \"Norway\", \"Germany\", \"Netherlands\", \"United Kingdom\", \"Finland\", \n               \"France\", \"Italy\", \"Spain\", \"Poland\", \"Greece\", \"Portugal\")\n\n# Create realistic data: Chocolate consumption correlates with GDP per capita\n# Higher GDP countries tend to consume more chocolate and have better research funding\ngdp_per_capita &lt;- c(87097, 58977, 67218, 51096, 53879, 89154, 51860, 57534, \n                    46510, 53982, 43659, 35551, 30416, 17841, 20192, 24567)\n\n# Normalize GDP values to make them more manageable\ngdp_normalized &lt;- (gdp_per_capita - min(gdp_per_capita)) / \n                 (max(gdp_per_capita) - min(gdp_per_capita))\n\n# More realistic chocolate consumption - loosely based on real consumption patterns\n# plus some randomness, but influenced by GDP\nchocolate_consumption &lt;- 4 + 8 * gdp_normalized + rnorm(16, 0, 0.8)\n\n# Nobel prizes - also influenced by GDP (research funding) with noise\n# The relationship is non-linear, but will show up as correlated\nnobel_prizes &lt;- 2 + 12 * gdp_normalized^1.2 + rnorm(16, 0, 1.5)\n\n# Create dataframe\ncountry_data &lt;- data.frame(\n  country = countries,\n  chocolate = round(chocolate_consumption, 1),\n  nobel = round(nobel_prizes, 1),\n  gdp = gdp_per_capita\n)\n\n# Fit regression model - chocolate vs nobel without controlling for GDP\nchocolate_nobel_model &lt;- lm(nobel ~ chocolate, data = country_data)\n\n# Better model that reveals the confounding\nfull_model &lt;- lm(nobel ~ chocolate + gdp, data = country_data)\n\n# Plot the apparent relationship\nggplot(country_data, aes(x = chocolate, y = nobel)) +\n  geom_point(color = \"darkblue\", size = 3, alpha = 0.7) +\n  geom_text(aes(label = country), hjust = -0.2, vjust = 0, size = 3) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  labs(\n    title = \"Apparent Correlation: Chocolate Consumption vs. Nobel Prizes\",\n    subtitle = \"Demonstrates how confounding variables create spurious correlations\",\n    x = \"Chocolate Consumption (kg per capita)\",\n    y = \"Nobel Prizes per 10M Population\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Show regression results\nsummary(chocolate_nobel_model)\n\n\nCall:\nlm(formula = nobel ~ chocolate, data = country_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9080 -1.4228  0.0294  0.5962  3.2977 \n\nCoefficients:\n            Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept)  -4.0518     1.3633  -2.972     0.0101 *  \nchocolate     1.3322     0.1682   7.921 0.00000154 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.626 on 14 degrees of freedom\nMultiple R-squared:  0.8176,    Adjusted R-squared:  0.8045 \nF-statistic: 62.75 on 1 and 14 DF,  p-value: 0.000001536\n\n# Demonstrate multiple testing problem\np_values &lt;- numeric(100)\nfor(i in 1:100) {\n  # Generate two completely random variables with n=20\n  x &lt;- rnorm(20)\n  y &lt;- rnorm(20)\n  # Test for correlation and store p-value\n  p_values[i] &lt;- cor.test(x, y)$p.value\n}\n\n# How many \"significant\" results at alpha = 0.05?\nsum(p_values &lt; 0.05)\n\n[1] 3\n\n# Visualize the multiple testing phenomenon\nhist(p_values, breaks = 20, main = \"P-values from 100 Tests of Random Data\",\n     xlab = \"P-value\", col = \"lightblue\", border = \"white\")\nabline(v = 0.05, col = \"red\", lwd = 2, lty = 2)\ntext(0.15, 20, paste(\"Approximately\", sum(p_values &lt; 0.05),\n                     \"tests are 'significant'\\nby random chance alone!\"), \n     col = \"darkred\")\n\n\n\n\n\n\n\n\nThis example demonstrates how seemingly compelling correlations can emerge between unrelated variables due to confounding factors and chance. The correlation between chocolate consumption and Nobel prizes appears significant (p &lt; 0.05) when analyzed directly, even though it‚Äôs explained by a third variable - national wealth (GDP per capita).\nWealthier countries typically consume more chocolate and simultaneously invest more in education and research, leading to more Nobel prizes. Without controlling for this confounding factor, we would mistakenly conclude a direct relationship between chocolate and Nobel prizes.\nThe multiple testing demonstration further illustrates why spurious correlations appear so frequently in research. When conducting 100 statistical tests on completely random data, we expect approximately 5 ‚Äúsignificant‚Äù results at Œ± = 0.05 purely by chance. In real research settings where hundreds of variables might be analyzed, the probability of finding false positive correlations increases dramatically.\nThis example underscores three critical points:\n\nSmall sample sizes (16 countries) are particularly vulnerable to chance correlations\nConfounding variables can create strong apparent associations between unrelated factors\nMultiple testing without appropriate corrections virtually guarantees finding ‚Äúsignificant‚Äù but meaningless patterns\n\nSuch findings explain why replication is essential in research and why most initial ‚Äúdiscoveries‚Äù fail to hold up in subsequent studies.\n\n\nConfounding Variables (Hidden Third Factors)\nConfounding occurs when an external variable influences both the predictor and outcome variables, creating an apparent relationship that may disappear when the confounder is accounted for.\n\n# Create sample data\nn &lt;- 200\nability &lt;- rnorm(n, 100, 15)                       # Natural ability \neducation &lt;- 10 + 0.05 * ability + rnorm(n, 0, 2)  # Education affected by ability\nincome &lt;- 10000 + 2000 * education + 100 * ability + rnorm(n, 0, 5000)  # Income affected by both\n\nomitted_var_data &lt;- data.frame(\n  ability = ability,\n  education = education,\n  income = income\n)\n\n# Model without accounting for ability\nmodel_naive &lt;- lm(income ~ education, data = omitted_var_data)\n\n# Model accounting for ability\nmodel_full &lt;- lm(income ~ education + ability, data = omitted_var_data)\n\n# Show results\nsummary(model_naive)\n\n\nCall:\nlm(formula = income ~ education, data = omitted_var_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14422.9  -3362.1    142.7   3647.7  14229.6 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  18982.0     2410.5   7.875    0.000000000000221 ***\neducation     2050.9      158.7  12.926 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5066 on 198 degrees of freedom\nMultiple R-squared:  0.4576,    Adjusted R-squared:  0.4549 \nF-statistic: 167.1 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_full)\n\n\nCall:\nlm(formula = income ~ education + ability, data = omitted_var_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12739.9  -3388.7    -41.1   3572.1  14976.8 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 13203.84    3018.85   4.374            0.0000198 ***\neducation    1871.43     166.03  11.272 &lt; 0.0000000000000002 ***\nability        85.60      27.87   3.071              0.00243 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4961 on 197 degrees of freedom\nMultiple R-squared:  0.4824,    Adjusted R-squared:  0.4772 \nF-statistic: 91.81 on 2 and 197 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Create visualization with ability shown through color\nggplot(omitted_var_data, aes(x = education, y = income, color = ability)) +\n  geom_point(alpha = 0.8) +\n  scale_color_viridis_c(name = \"Ability Score\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) + \n  labs(\n    title = \"Income vs. Education, Colored by Ability\",\n    subtitle = \"Visualizing the confounding variable\",\n    x = \"Years of Education\",\n    y = \"Annual Income (PLN)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis example illustrates omitted variable bias: without accounting for ability, the estimated effect of education on income is exaggerated (2,423 PLN per year vs.¬†1,962 PLN per year). The confounding occurs because ability influences both education and income, creating a spurious component in the observed correlation.\n\nClassic Example: Ice Cream and Drownings\nA classic example of confounding involves the correlation between ice cream sales and drowning incidents, both influenced by temperature:\n\n# Create sample data\nn &lt;- 100\ntemperature &lt;- runif(n, 5, 35)  # Temperature in Celsius\n\n# Both ice cream sales and drownings are influenced by temperature\nice_cream_sales &lt;- 100 + 10 * temperature + rnorm(n, 0, 20)\ndrownings &lt;- 1 + 0.3 * temperature + rnorm(n, 0, 1)\n\nconfounding_data &lt;- data.frame(\n  temperature = temperature,\n  ice_cream_sales = ice_cream_sales,\n  drownings = drownings\n)\n\n# Model without controlling for temperature\nmodel_naive &lt;- lm(drownings ~ ice_cream_sales, data = confounding_data)\n\n# Model controlling for temperature\nmodel_full &lt;- lm(drownings ~ ice_cream_sales + temperature, data = confounding_data)\n\n# Show results\nsummary(model_naive)\n\n\nCall:\nlm(formula = drownings ~ ice_cream_sales, data = confounding_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8163 -0.7597  0.0118  0.7846  2.5797 \n\nCoefficients:\n                 Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)     -1.503063   0.370590  -4.056              0.0001 ***\nice_cream_sales  0.028074   0.001205  23.305 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.088 on 98 degrees of freedom\nMultiple R-squared:  0.8471,    Adjusted R-squared:  0.8456 \nF-statistic: 543.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_full)\n\n\nCall:\nlm(formula = drownings ~ ice_cream_sales + temperature, data = confounding_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.85074 -0.61169  0.01186  0.60556  2.01776 \n\nCoefficients:\n                 Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)      1.243785   0.530123   2.346         0.021 *  \nice_cream_sales -0.002262   0.004839  -0.467         0.641    \ntemperature      0.317442   0.049515   6.411 0.00000000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9169 on 97 degrees of freedom\nMultiple R-squared:  0.8926,    Adjusted R-squared:  0.8904 \nF-statistic: 403.2 on 2 and 97 DF,  p-value: &lt; 0.00000000000000022\n\n# Create visualization\nggplot(confounding_data, aes(x = ice_cream_sales, y = drownings, color = temperature)) +\n  geom_point(alpha = 0.8) +\n  scale_color_viridis_c(name = \"Temperature (¬∞C)\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"The Ice Cream and Drownings Correlation\",\n    subtitle = \"Temperature as a confounding variable\",\n    x = \"Ice Cream Sales\",\n    y = \"Drowning Incidents\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe naive model shows a statistically significant relationship between ice cream sales and drownings. However, once temperature is included in the model, the coefficient for ice cream sales decreases substantially and becomes statistically insignificant. This demonstrates how failing to account for confounding variables can lead to spurious correlations.\n\n\n\nReverse Causality\nReverse causality occurs when the assumed direction of causation is incorrect. Consider this example of anxiety and relaxation techniques:\n\n# Create sample data\nn &lt;- 200\nanxiety_level &lt;- runif(n, 1, 10)  # Anxiety level (1-10)\n\n# People with higher anxiety tend to use more relaxation techniques\nrelaxation_techniques &lt;- 1 + 0.7 * anxiety_level + rnorm(n, 0, 1)\n\nreverse_data &lt;- data.frame(\n  anxiety = anxiety_level,\n  relaxation = relaxation_techniques\n)\n\n# Fit models in both directions\nmodel_incorrect &lt;- lm(anxiety ~ relaxation, data = reverse_data)\nmodel_correct &lt;- lm(relaxation ~ anxiety, data = reverse_data)\n\n# Show regression results\nsummary(model_incorrect)\n\n\nCall:\nlm(formula = anxiety ~ relaxation, data = reverse_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9651 -0.7285 -0.0923  0.7247  3.7996 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) -0.09482    0.21973  -0.432               0.667    \nrelaxation   1.15419    0.04105  28.114 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.182 on 198 degrees of freedom\nMultiple R-squared:  0.7997,    Adjusted R-squared:  0.7987 \nF-statistic: 790.4 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_correct)\n\n\nCall:\nlm(formula = relaxation ~ anxiety, data = reverse_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.15178 -0.51571 -0.00222  0.55513  2.04334 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  1.05726    0.15286   6.917      0.0000000000624 ***\nanxiety      0.69284    0.02464  28.114 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9161 on 198 degrees of freedom\nMultiple R-squared:  0.7997,    Adjusted R-squared:  0.7987 \nF-statistic: 790.4 on 1 and 198 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(reverse_data, aes(x = relaxation, y = anxiety)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Anxiety and Relaxation Techniques\",\n    subtitle = \"Example of reverse causality\",\n    x = \"Use of Relaxation Techniques (frequency/week)\",\n    y = \"Anxiety Level (1-10 scale)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nBoth regression models show statistically significant relationships, but they imply different causal mechanisms. The incorrect model suggests that relaxation techniques increase anxiety, while the correct model reflects the true data generating process: anxiety drives the use of relaxation techniques.\n\n\nCollider Bias (Selection Bias)\nCollider bias occurs when conditioning on a variable that is affected by both the independent and dependent variables of interest, creating an artificial relationship between variables that are actually independent.\n\n# Create sample data\nn &lt;- 1000\n\n# Generate two independent variables (no relationship between them)\nintelligence &lt;- rnorm(n, 100, 15)  # IQ score\nfamily_wealth &lt;- rnorm(n, 50, 15)  # Wealth score (independent from intelligence)\n  \n# True data-generating process: admission depends on both intelligence and wealth\nadmission_score &lt;- 0.4 * intelligence + 0.4 * family_wealth + rnorm(n, 0, 10)\nadmitted &lt;- admission_score &gt; median(admission_score)  # Binary admission variable\n\n# Create full dataset\nfull_data &lt;- data.frame(\n  intelligence = intelligence,\n  wealth = family_wealth,\n  admission_score = admission_score,\n  admitted = admitted\n)\n\n# Regression in full population (true model)\nfull_model &lt;- lm(intelligence ~ wealth, data = full_data)\nsummary(full_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth, data = full_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-49.608 -10.115   0.119  10.832  55.581 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 101.42330    1.73139   58.58 &lt;0.0000000000000002 ***\nwealth       -0.02701    0.03334   -0.81               0.418    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.41 on 998 degrees of freedom\nMultiple R-squared:  0.0006569, Adjusted R-squared:  -0.0003444 \nF-statistic: 0.656 on 1 and 998 DF,  p-value: 0.4182\n\n# Get just the admitted students\nadmitted_only &lt;- full_data[full_data$admitted, ]\n\n# Regression in admitted students (conditioning on the collider)\nadmitted_model &lt;- lm(intelligence ~ wealth, data = admitted_only)\nsummary(admitted_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth, data = admitted_only)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.511  -9.064   0.721   8.965  48.267 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 115.4750     2.6165  44.133 &lt; 0.0000000000000002 ***\nwealth       -0.1704     0.0462  -3.689              0.00025 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.91 on 498 degrees of freedom\nMultiple R-squared:  0.0266,    Adjusted R-squared:  0.02464 \nF-statistic: 13.61 on 1 and 498 DF,  p-value: 0.0002501\n\n# Additional analysis - regression with the collider as a control variable\n# This demonstrates how controlling for a collider introduces bias\ncollider_control_model &lt;- lm(intelligence ~ wealth + admitted, data = full_data)\nsummary(collider_control_model)\n\n\nCall:\nlm(formula = intelligence ~ wealth + admitted, data = full_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.729  -8.871   0.700   8.974  48.044 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  102.90069    1.56858  65.601 &lt; 0.0000000000000002 ***\nwealth        -0.19813    0.03224  -6.145        0.00000000116 ***\nadmittedTRUE  14.09944    0.94256  14.959 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.93 on 997 degrees of freedom\nMultiple R-squared:  0.1838,    Adjusted R-squared:  0.1822 \nF-statistic: 112.3 on 2 and 997 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Plot for full population\np1 &lt;- ggplot(full_data, aes(x = wealth, y = intelligence)) +\n  geom_point(alpha = 0.3, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Full Population\",\n    subtitle = paste(\"Correlation:\", round(cor(full_data$intelligence, full_data$wealth), 3)),\n    x = \"Family Wealth Score\",\n    y = \"Intelligence Score\"\n  ) +\n  theme_minimal()\n\n# Plot for admitted students\np2 &lt;- ggplot(admitted_only, aes(x = wealth, y = intelligence)) +\n  geom_point(alpha = 0.3, color = \"darkgreen\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Admitted Students Only\",\n    subtitle = paste(\"Correlation:\", round(cor(admitted_only$intelligence, admitted_only$wealth), 3)),\n    x = \"Family Wealth Score\",\n    y = \"Intelligence Score\"\n  ) +\n  theme_minimal()\n\n# Display plots side by side\nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis example demonstrates collider bias in three ways:\n\nIn the full population, intelligence and wealth have no relationship (coefficient near zero, p-value = 0.87)\nAmong admitted students (conditioning on the collider), a significant negative relationship appears (coefficient = -0.39, p-value &lt; 0.001)\nWhen controlling for admission status in a regression, a spurious relationship is introduced (coefficient = -0.16, p-value &lt; 0.001)\n\nThe collider bias creates relationships between variables that are truly independent. This can be represented in a directed acyclic graph (DAG):\n\\text{Intelligence} \\rightarrow \\text{Admission} \\leftarrow \\text{Wealth}\nWhen we condition on admission (the collider), we create a spurious association between intelligence and wealth.\n\n\nImproper Analysis\nInappropriate statistical methods can produce spurious correlations. Common issues include using linear models for non-linear relationships, ignoring data clustering, or mishandling time series data.\n\n# Generate data with a true non-linear relationship\nn &lt;- 100\nx &lt;- seq(-3, 3, length.out = n)\ny &lt;- x^2 + rnorm(n, 0, 1)  # Quadratic relationship\n\nimproper_data &lt;- data.frame(x = x, y = y)\n\n# Fit incorrect linear model\nwrong_model &lt;- lm(y ~ x, data = improper_data)\n\n# Fit correct quadratic model\ncorrect_model &lt;- lm(y ~ x + I(x^2), data = improper_data)\n\n# Show results\nsummary(wrong_model)\n\n\nCall:\nlm(formula = y ~ x, data = improper_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2176 -2.1477 -0.6468  2.4365  7.3457 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  3.14689    0.28951  10.870 &lt;0.0000000000000002 ***\nx            0.08123    0.16548   0.491               0.625    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.895 on 98 degrees of freedom\nMultiple R-squared:  0.002453,  Adjusted R-squared:  -0.007726 \nF-statistic: 0.2409 on 1 and 98 DF,  p-value: 0.6246\n\nsummary(correct_model)\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = improper_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81022 -0.65587  0.01935  0.61168  2.68894 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  0.12407    0.14498   0.856               0.394    \nx            0.08123    0.05524   1.470               0.145    \nI(x^2)       0.98766    0.03531  27.972 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9664 on 97 degrees of freedom\nMultiple R-squared:   0.89, Adjusted R-squared:  0.8877 \nF-statistic: 392.3 on 2 and 97 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(improper_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), color = \"green\", se = FALSE) +\n  labs(\n    title = \"Improper Analysis Example\",\n    subtitle = \"Linear model (red) vs. Quadratic model (green)\",\n    x = \"Variable X\",\n    y = \"Variable Y\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe linear model incorrectly suggests no relationship between x and y (coefficient near zero, p-value = 0.847), while the quadratic model reveals the true relationship (R^2 = 0.90). This demonstrates how model misspecification can create spurious non-correlations, masking real relationships that exist in different forms.\n\n\nEndogeneity and Its Sources\nEndogeneity occurs when an explanatory variable is correlated with the error term in a regression model. This violates the exogeneity assumption of OLS regression and leads to biased estimates. There are several sources of endogeneity:\n\nOmitted Variable Bias\nAs shown in the education-income example, when important variables are omitted from the model, their effects are absorbed into the error term, which becomes correlated with included variables.\n\n\nMeasurement Error\nWhen variables are measured with error, the observed values differ from true values, creating correlation between the error term and the predictors.\n\n\nSimultaneity (Bidirectional Causality)\nWhen the dependent variable also affects the independent variable, creating a feedback loop. Let‚Äôs demonstrate this:\n\n# Create sample data with mutual influence\nn &lt;- 100\n\n# Initialize variables\neconomic_growth &lt;- rnorm(n, 2, 1)\nemployment_rate &lt;- rnorm(n, 60, 5)\n\n# Create mutual influence through iterations\nfor(i in 1:3) {\n  economic_growth &lt;- 2 + 0.05 * employment_rate + rnorm(n, 0, 0.5)\n  employment_rate &lt;- 50 + 5 * economic_growth + rnorm(n, 0, 2)\n}\n\nsimultaneity_data &lt;- data.frame(\n  growth = economic_growth,\n  employment = employment_rate\n)\n\n# Model estimating effect of growth on employment\nmodel_growth_on_emp &lt;- lm(employment ~ growth, data = simultaneity_data)\n\n# Model estimating effect of employment on growth\nmodel_emp_on_growth &lt;- lm(growth ~ employment, data = simultaneity_data)\n\n# Show results\nsummary(model_growth_on_emp)\n\n\nCall:\nlm(formula = employment ~ growth, data = simultaneity_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.603 -1.500 -0.099  1.387  5.673 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  49.9665     2.0717   24.12 &lt;0.0000000000000002 ***\ngrowth        5.0151     0.3528   14.22 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.045 on 98 degrees of freedom\nMultiple R-squared:  0.6734,    Adjusted R-squared:  0.6701 \nF-statistic: 202.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\nsummary(model_emp_on_growth)\n\n\nCall:\nlm(formula = growth ~ employment, data = simultaneity_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11417 -0.20626 -0.02185  0.22646  0.72941 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) -4.801257   0.749557  -6.405        0.00000000523 ***\nemployment   0.134283   0.009446  14.216 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3346 on 98 degrees of freedom\nMultiple R-squared:  0.6734,    Adjusted R-squared:  0.6701 \nF-statistic: 202.1 on 1 and 98 DF,  p-value: &lt; 0.00000000000000022\n\n# Visualize\nggplot(simultaneity_data, aes(x = growth, y = employment)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Simultaneity Between Economic Growth and Employment\",\n    x = \"Economic Growth (%)\",\n    y = \"Employment Rate (%)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe true data generating process is a system of simultaneous equations:\n\\text{Growth}_i = \\alpha_0 + \\alpha_1 \\text{Employment}_i + u_i \\text{Employment}_i = \\beta_0 + \\beta_1 \\text{Growth}_i + v_i\nStandard OLS regression cannot consistently estimate either equation because each explanatory variable is correlated with the error term in its respective equation.\n\n\nSelection Bias\nWhen the sample is not randomly selected from the population, the selection process can introduce correlation between the error term and the predictors. The collider bias example demonstrates a form of selection bias.\nThe consequences of endogeneity include: - Biased coefficient estimates - Incorrect standard errors - Invalid hypothesis tests - Misleading causal interpretations\nAddressing endogeneity requires specialized methods such as instrumental variables, system estimation, panel data methods, or experimental designs.\n\n\n\n\n\n\nUnderstanding Endogeneity in Regression\n\n\n\nEndogeneity is a critical concept in statistical analysis that occurs when an explanatory variable in a regression model is correlated with the error term. This creates challenges for accurately understanding cause-and-effect relationships in research. Let‚Äôs examine the three main types of endogeneity and how they affect research outcomes.\n\nOmitted Variable Bias (OVB)\nOmitted Variable Bias occurs when an important variable that affects both the dependent and independent variables is left out of the analysis. This omission leads to incorrect conclusions about the relationship between the variables we‚Äôre studying.\nConsider a study examining the relationship between education and income:\nExample: Education and Income The observed relationship shows that more education correlates with higher income. However, an individual‚Äôs inherent abilities affect both their educational attainment and their earning potential. Without accounting for ability, we may overestimate education‚Äôs direct effect on income.\nThe statistical representation shows why this matters:\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2z_i + \\epsilon_i (Complete model)\ny_i = \\beta_0 + \\beta_1x_i + u_i (Incomplete model)\nWhen we omit an important variable, our estimates of the remaining relationships become biased and unreliable.\n\n\nSimultaneity\nSimultaneity occurs when two variables simultaneously influence each other, making it difficult to determine the direction of causation. This creates a feedback loop that complicates statistical analysis.\nCommon Examples of Simultaneity:\nAcademic Performance and Study Habits represent a clear case of simultaneity. Academic performance influences how much time students dedicate to studying, while study time affects academic performance. This two-way relationship makes it challenging to measure the isolated effect of either variable.\nMarket Dynamics provide another example. Prices influence demand, while demand influences prices. This concurrent relationship requires special analytical approaches to understand the true relationships.\n\n\nMeasurement Error\nMeasurement error occurs when we cannot accurately measure our variables of interest. This imprecision can significantly impact our analysis and conclusions.\nCommon Sources of Measurement Error:\nSelf-Reported Data presents a significant challenge. When participants report their own behaviors or characteristics, such as study time, the reported values often differ from actual values. This discrepancy affects our ability to measure true relationships.\nTechnical Limitations also contribute to measurement error through imprecise measuring tools, inconsistent measurement conditions, and recording or data entry errors.\n\n\nAddressing Endogeneity in Research\n\nIdentification Strategies\n\n# Example of controlling for omitted variables\nmodel_simple &lt;- lm(income ~ education, data = df)\nmodel_full &lt;- lm(income ~ education + ability + experience + region, data = df)\n\n# Compare coefficients\nsummary(model_simple)\nsummary(model_full)\n\n\nInclude Additional Variables: Collect data on potentially important omitted variables and include relevant control variables in your analysis. For example, including measures of ability when studying education‚Äôs effect on income.\nUse Panel Data: Collect data across multiple time periods to control for unobserved fixed characteristics and analyze changes over time.\nInstrumental Variables: Find variables that affect your independent variable but not your dependent variable to isolate the relationship of interest.\n\n\n\nImproving Measurement\n\nMultiple Measurements: Take several measurements of key variables, use averaging to reduce random error, and compare different measurement methods.\nBetter Data Collection: Use validated measurement instruments, implement quality control procedures, and document potential sources of error.\n\n\n\n\nBest Practices for Researchers\nResearch Design fundamentally shapes your ability to address endogeneity. Plan for potential endogeneity issues before collecting data, include measures for potentially important control variables, and consider using multiple measurement approaches.\nAnalysis should include testing for endogeneity when possible, using appropriate statistical methods for your specific situation, and documenting assumptions and limitations.\nReporting must clearly describe potential endogeneity concerns, explain how you addressed these issues, and discuss implications for your conclusions.\n\n\n\n\n\n\n\n\n\nFormal Derivation of OLS Estimators: A Complete Mathematical Treatment\n\n\n\n\nObjective and Setup\nWe seek to find the values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that minimize the sum of squared residuals:\nSSE = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nThis is an unconstrained optimization problem where we treat SSE as a function of two variables: SSE(\\hat{\\beta}_0, \\hat{\\beta}_1).\n\n\nMathematical Prerequisites\nChain Rule for Composite Functions: For f(g(x)), the derivative is: \\frac{d}{dx}[f(g(x))] = f'(g(x)) \\cdot g'(x)\nIn our context:\n\nOuter function: f(u) = u^2 with derivative f'(u) = 2u\nInner function: u = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\n\nFirst-Order Conditions: At a minimum, both partial derivatives equal zero: \\frac{\\partial SSE}{\\partial \\hat{\\beta}_0} = 0 \\quad \\text{and} \\quad \\frac{\\partial SSE}{\\partial \\hat{\\beta}_1} = 0\n\n\nDerivation of \\hat{\\beta}_0\nStep 1: Take the partial derivative with respect to \\hat{\\beta}_0:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_0} = \\frac{\\partial}{\\partial \\hat{\\beta}_0} \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2\nStep 2: Apply the chain rule to each term: = \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot \\frac{\\partial}{\\partial \\hat{\\beta}_0}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\n= \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot (-1)\n= -2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\nStep 3: Set equal to zero and solve: -2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\nDividing by -2 and expanding: \\sum_{i=1}^n y_i - n\\hat{\\beta}_0 - \\hat{\\beta}_1\\sum_{i=1}^n x_i = 0\nStep 4: Isolate \\hat{\\beta}_0: n\\hat{\\beta}_0 = \\sum_{i=1}^n y_i - \\hat{\\beta}_1\\sum_{i=1}^n x_i\n\\boxed{\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}}\nInterpretation: The intercept adjusts to ensure the regression line passes through the point of means (\\bar{x}, \\bar{y}).\n\n\nDerivation of \\hat{\\beta}_1\nStep 1: Take the partial derivative with respect to \\hat{\\beta}_1:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_1} = \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) \\cdot (-x_i)\n= -2\\sum_{i=1}^n x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)\nStep 2: Substitute the expression for \\hat{\\beta}_0: = -2\\sum_{i=1}^n x_i(y_i - (\\bar{y} - \\hat{\\beta}_1\\bar{x}) - \\hat{\\beta}_1x_i)\n= -2\\sum_{i=1}^n x_i((y_i - \\bar{y}) - \\hat{\\beta}_1(x_i - \\bar{x}))\nStep 3: Set equal to zero and expand: \\sum_{i=1}^n x_i(y_i - \\bar{y}) - \\hat{\\beta}_1\\sum_{i=1}^n x_i(x_i - \\bar{x}) = 0\nStep 4: Use the algebraic identity \\sum_{i=1}^n x_i(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}):\nThis identity holds because: \\sum_{i=1}^n x_i(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x} + \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) + \\bar{x}\\sum_{i=1}^n(y_i - \\bar{y}) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) + 0\nSimilarly, \\sum_{i=1}^n x_i(x_i - \\bar{x}) = \\sum_{i=1}^n (x_i - \\bar{x})^2.\nStep 5: Solve for \\hat{\\beta}_1: \\hat{\\beta}_1\\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\n\\boxed{\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}}\n\n\nVerification of Minimum (Second-Order Conditions)\nTo confirm we have found a minimum (not a maximum or saddle point), we examine the Hessian matrix of second partial derivatives:\nSecond partial derivatives:\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_0^2} = 2n &gt; 0\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_1^2} = 2\\sum_{i=1}^n x_i^2 &gt; 0\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_0 \\partial \\hat{\\beta}_1} = 2\\sum_{i=1}^n x_i\nHessian matrix: \\mathbf{H} = 2\\begin{bmatrix} n & \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i & \\sum_{i=1}^n x_i^2 \\end{bmatrix}\nPositive definiteness check:\n\nFirst leading principal minor: 2n &gt; 0 ‚úì\nSecond leading principal minor (determinant): \\det(\\mathbf{H}) = 4\\left(n\\sum_{i=1}^n x_i^2 - \\left(\\sum_{i=1}^n x_i\\right)^2\\right) = 4n\\sum_{i=1}^n(x_i - \\bar{x})^2 &gt; 0 ‚úì\n\nSince the Hessian is positive definite, we have confirmed a minimum.\n\n\nGeometric Interpretation\n\n# Visualizing the optimization surface\nlibrary(tidyverse)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Generate sample data\nset.seed(42)\nx &lt;- runif(20, 1, 8)\ny &lt;- 2 + 3*x + rnorm(20, 0, 1)\n\n# Create grid of beta values\nbeta0_seq &lt;- seq(0, 4, length.out = 50)\nbeta1_seq &lt;- seq(2, 4, length.out = 50)\ngrid &lt;- expand.grid(beta0 = beta0_seq, beta1 = beta1_seq)\n\n# Calculate SSE for each combination\ngrid$SSE &lt;- apply(grid, 1, function(params) {\n  sum((y - (params[1] + params[2]*x))^2)\n})\n\n# Create contour plot\nggplot(grid, aes(x = beta0, y = beta1, z = SSE)) +\n  geom_contour_filled(aes(fill = after_stat(level))) +\n  geom_point(x = coef(lm(y ~ x))[1], \n             y = coef(lm(y ~ x))[2], \n             color = \"red\", size = 3) +\n  labs(title = \"SSE Surface in Parameter Space\",\n       subtitle = \"Red point shows the OLS minimum\",\n       x = expression(hat(beta)[0]),\n       y = expression(hat(beta)[1])) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.1-understanding-correlation-measures-a-self-study-tutorial-using-stress-level-and-cognitive-performance-data",
    "href": "correg_en.html#appendix-a.1-understanding-correlation-measures-a-self-study-tutorial-using-stress-level-and-cognitive-performance-data",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.36 Appendix A.1: Understanding Correlation Measures: A Self-Study Tutorial (Using Stress Level and Cognitive Performance Data)",
    "text": "9.36 Appendix A.1: Understanding Correlation Measures: A Self-Study Tutorial (Using Stress Level and Cognitive Performance Data)\n\nDataset Overview\n\ndata &lt;- data.frame(\n  anxiety_level = c(8, 5, 11, 14, 7, 10),\n  cognitive_performance = c(85, 90, 62, 55, 80, 65)\n)\n\n\n\n1. Covariance Calculation\n\nStep 1: Calculate Means\n\n\n\n\n\n\n\n\nVariable\nCalculation\nResult\n\n\n\n\nMean Anxiety (\\bar{x})\n(8 + 5 + 11 + 14 + 7 + 10) √∑ 6\n9.17\n\n\nMean Cognitive (\\bar{y})\n(85 + 90 + 62 + 55 + 80 + 65) √∑ 6\n72.83\n\n\n\n\n\nStep 2: Calculate Deviations and Products\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\n(x_i - \\bar{x})\n(y_i - \\bar{y})\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n1\n8\n85\n-1.17\n12.17\n-14.24\n\n\n2\n5\n90\n-4.17\n17.17\n-71.60\n\n\n3\n11\n62\n1.83\n-10.83\n-19.82\n\n\n4\n14\n55\n4.83\n-17.83\n-86.12\n\n\n5\n7\n80\n-2.17\n7.17\n-15.56\n\n\n6\n10\n65\n0.83\n-7.83\n-6.50\n\n\nSum\n55\n437\n0.00\n0.00\n-213.84\n\n\n\n\n\nStep 3: Calculate Covariance\n \\text{Cov}(X,Y) = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n-1} = \\frac{-213.84}{5} = -42.77 \n\n\n\n2. Pearson Correlation Coefficient\n\nStep 1: Calculate Squared Deviations\n\n\n\n\n\n\n\n\n\n\ni\n(x_i - \\bar{x})\n(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n-1.17\n12.17\n1.37\n148.11\n\n\n2\n-4.17\n17.17\n17.39\n294.81\n\n\n3\n1.83\n-10.83\n3.35\n117.29\n\n\n4\n4.83\n-17.83\n23.33\n317.91\n\n\n5\n-2.17\n7.17\n4.71\n51.41\n\n\n6\n0.83\n-7.83\n0.69\n61.31\n\n\nSum\n0.00\n0.00\n50.84\n990.84\n\n\n\n\n\nStep 2: Calculate Standard Deviations\n\n\n\n\n\n\n\n\n\nMeasure\nFormula\nCalculation\nResult\n\n\n\n\ns_x\n\\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n-1}}\n\\sqrt{\\frac{50.84}{5}}\n3.19\n\n\ns_y\n\\sqrt{\\frac{\\sum (y_i - \\bar{y})^2}{n-1}}\n\\sqrt{\\frac{990.84}{5}}\n14.08\n\n\n\n\n\nStep 3: Calculate Pearson Correlation\n r = \\frac{\\text{Cov}(X,Y)}{s_x s_y} = \\frac{-42.77}{3.19 \\times 14.08} = -0.95 \n\n\n\n3. Spearman Rank Correlation\n\nStep 1: Assign Ranks\n\n\n\ni\nx_i\ny_i\nRank x_i\nRank y_i\nd_i\nd_i^2\n\n\n\n\n1\n8\n85\n3\n2\n1\n1\n\n\n2\n5\n90\n1\n1\n0\n0\n\n\n3\n11\n62\n5\n5\n0\n0\n\n\n4\n14\n55\n6\n6\n0\n0\n\n\n5\n7\n80\n2\n3\n-1\n1\n\n\n6\n10\n65\n4\n4\n0\n0\n\n\nSum\n\n\n\n\n\n2\n\n\n\n\n\nStep 2: Calculate Spearman Correlation\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} = 1 - \\frac{6(2)}{6(36-1)} = 1 - \\frac{12}{210} = 0.94 \n\n\n\nVerification using R\n\n# Calculate correlations using R\ncor(data$anxiety_level, data$cognitive_performance, method = \"pearson\")\n\n[1] -0.9527979\n\ncor(data$anxiety_level, data$cognitive_performance, method = \"spearman\")\n\n[1] -0.9428571\n\n\n\n\nInterpretation\n\nThe strong negative Pearson correlation (r = -0.95) indicates a very strong negative linear relationship between anxiety level and cognitive performance.\nThe strong positive Spearman correlation (œÅ = 0.94) shows that the relationship is also strongly monotonic.\nThe difference between Pearson and Spearman correlations suggests that while there is a strong relationship, it might not be perfectly linear.\n\n\n\nExercise\n\nVerify each calculation step in the tables above.\nTry calculating these measures with a modified dataset:\n\nAdd one outlier and observe how it affects both correlation coefficients\nChange one pair of values and recalculate",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.2-calculating-covariance-pearson-spearman-correlation-and-ols---worked-example",
    "href": "correg_en.html#appendix-a.2-calculating-covariance-pearson-spearman-correlation-and-ols---worked-example",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.37 Appendix A.2: Calculating Covariance, Pearson, Spearman Correlation, and OLS - worked example",
    "text": "9.37 Appendix A.2: Calculating Covariance, Pearson, Spearman Correlation, and OLS - worked example\nA political science student is investigating the relationship between district magnitude (DM) and Gallagher‚Äôs disproportionality index (GH) in parliamentary elections across 10 randomly selected democracies.\nData on electoral district magnitudes (\\text{DM}) and Gallagher index:\n\n\n\n\\text{DM} (X)\nGallagher (Y)\n\n\n\n\n2\n18.2\n\n\n3\n16.7\n\n\n4\n15.8\n\n\n5\n15.3\n\n\n6\n15.0\n\n\n7\n14.8\n\n\n8\n14.7\n\n\n9\n14.6\n\n\n10\n14.55\n\n\n11\n14.52\n\n\n\n\nStep 1: Calculate Basic Statistics\nCalculation of means:\nFor \\text{DM} (X): \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\nDetailed calculation:\n2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 65 \\bar{x} = \\frac{65}{10} = 6.5\nFor Gallagher index (Y): \\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\nDetailed calculation:\n18.2 + 16.7 + 15.8 + 15.3 + 15.0 + 14.8 + 14.7 + 14.6 + 14.55 + 14.52 = 154.17 \\bar{y} = \\frac{154.17}{10} = 15.417\n\n\nStep 2: Detailed Covariance Calculations\nComplete working table showing all calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18.2\n-4.5\n2.783\n-12.5235\n20.25\n7.7451\n\n\n2\n3\n16.7\n-3.5\n1.283\n-4.4905\n12.25\n1.6461\n\n\n3\n4\n15.8\n-2.5\n0.383\n-0.9575\n6.25\n0.1467\n\n\n4\n5\n15.3\n-1.5\n-0.117\n0.1755\n2.25\n0.0137\n\n\n5\n6\n15.0\n-0.5\n-0.417\n0.2085\n0.25\n0.1739\n\n\n6\n7\n14.8\n0.5\n-0.617\n-0.3085\n0.25\n0.3807\n\n\n7\n8\n14.7\n1.5\n-0.717\n-1.0755\n2.25\n0.5141\n\n\n8\n9\n14.6\n2.5\n-0.817\n-2.0425\n6.25\n0.6675\n\n\n9\n10\n14.55\n3.5\n-0.867\n-3.0345\n12.25\n0.7517\n\n\n10\n11\n14.52\n4.5\n-0.897\n-4.0365\n20.25\n0.8047\n\n\nSum\n65\n154.17\n0\n0\n-28.085\n82.5\n12.8442\n\n\n\nCovariance calculation: \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\text{Cov}(X,Y) = \\frac{-28.085}{9} = -3.120556\n\n\nStep 3: Standard Deviation Calculations\nFor \\text{DM} (X): \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}}\n\\sigma_x = \\sqrt{\\frac{82.5}{9}} = \\sqrt{9.1667} = 3.026582\nFor Gallagher (Y): \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}}\n\\sigma_y = \\sqrt{\\frac{12.8442}{9}} = \\sqrt{1.4271} = 1.194612\n\n\nStep 4: Pearson Correlation Calculation\nr = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\nr = \\frac{-3.120556}{3.026582 \\times 1.194612} = \\frac{-3.120556}{3.615752} = -0.863044\n\n\nStep 5: Spearman Rank Correlation Calculation\nComplete ranking table with all calculations:\n\n\n\ni\nX_i\nY_i\nRank X_i\nRank Y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n18.2\n1\n10\n-9\n81\n\n\n2\n3\n16.7\n2\n9\n-7\n49\n\n\n3\n4\n15.8\n3\n8\n-5\n25\n\n\n4\n5\n15.3\n4\n7\n-3\n9\n\n\n5\n6\n15.0\n5\n6\n-1\n1\n\n\n6\n7\n14.8\n6\n5\n1\n1\n\n\n7\n8\n14.7\n7\n4\n3\n9\n\n\n8\n9\n14.6\n8\n3\n5\n25\n\n\n9\n10\n14.55\n9\n2\n7\n49\n\n\n10\n11\n14.52\n10\n1\n9\n81\n\n\nSum\n\n\n\n\n\n330\n\n\n\nSpearman correlation calculation: \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n\\rho = 1 - \\frac{6 \\times 330}{10(100 - 1)} = 1 - \\frac{1980}{990} = 1 - 2 = -1\n\n\nStep 6: R Verification\n\n# Create vectors\nDM &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nGH &lt;- c(18.2, 16.7, 15.8, 15.3, 15.0, 14.8, 14.7, 14.6, 14.55, 14.52)\n\n# Calculate covariance\ncov(DM, GH)\n\n[1] -3.120556\n\n# Calculate correlations\ncor(DM, GH, method = \"pearson\")\n\n[1] -0.8627742\n\ncor(DM, GH, method = \"spearman\")\n\n[1] -1\n\n\n\n\nStep 7: Basic Visualization\n\nlibrary(ggplot2)\n\n# Create data frame\ndata &lt;- data.frame(DM = DM, GH = GH)\n\n# Create scatter plot\nggplot(data, aes(x = DM, y = GH)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"District Magnitude vs Gallagher Index\",\n    x = \"District Magnitude (DM)\",\n    y = \"Gallagher Index (GH)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nOLS Estimation and Goodness-of-Fit Measures\n\n\nStep 1: Calculate OLS Estimates\nUsing previously calculated values:\n\n\\sum(X_i - \\bar{X})(Y_i - \\bar{Y}) = -28.085\n\\sum(X_i - \\bar{X})^2 = 82.5\n\\bar{X} = 6.5\n\\bar{Y} = 15.417\n\nCalculate slope (\\hat{\\beta_1}): \\hat{\\beta_1} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\hat{\\beta_1} = -28,085 √∑ 82,5 = -0,3404\nCalculate intercept (\\hat{\\beta_0}): \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 15,417 - (-0,3404 √ó 6,5)\n   = 15,417 + 2,2126\n   = 17,6296\nTherefore, the OLS regression equation is: \\hat{Y} = 17.6296 - 0.3404X\n\n\nStep 2: Calculate Fitted Values and Residuals\nComplete table showing all calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i\ne_i = Y_i - \\hat{Y}_i\ne_i^2\n(Y_i - \\bar{Y})^2\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18.2\n16.9488\n1.2512\n1.5655\n7.7451\n2.3404\n\n\n2\n3\n16.7\n16.6084\n0.0916\n0.0084\n1.6461\n1.4241\n\n\n3\n4\n15.8\n16.2680\n-0.4680\n0.2190\n0.1467\n0.7225\n\n\n4\n5\n15.3\n15.9276\n-0.6276\n0.3939\n0.0137\n0.2601\n\n\n5\n6\n15.0\n15.5872\n-0.5872\n0.3448\n0.1739\n0.0289\n\n\n6\n7\n14.8\n15.2468\n-0.4468\n0.1996\n0.3807\n0.0290\n\n\n7\n8\n14.7\n14.9064\n-0.2064\n0.0426\n0.5141\n0.2610\n\n\n8\n9\n14.6\n14.5660\n0.0340\n0.0012\n0.6675\n0.7241\n\n\n9\n10\n14.55\n14.2256\n0.3244\n0.1052\n0.7517\n1.4184\n\n\n10\n11\n14.52\n13.8852\n0.6348\n0.4030\n0.8047\n2.3439\n\n\nSum\n65\n154.17\n154.17\n0\n3.2832\n12.8442\n9.5524\n\n\n\nCalculations for fitted values:\nFor X = 2:\n≈∂ = 17.6296 + (-0.3404 √ó 2) = 16.9488\n\nFor X = 3:\n≈∂ = 17.6296 + (-0.3404 √ó 3) = 16.6084\n\n[... continue for all values]\n\n\nStep 3: Calculate Goodness-of-Fit Measures\nSum of Squared Errors (SSE): SSE = \\sum e_i^2\nSSE = 3.2832\nSum of Squared Total (SST): SST = \\sum(Y_i - \\bar{Y})^2\nSST = 12.8442\nSum of Squared Regression (SSR): SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\nSSR = 9.5524\nVerify decomposition: SST = SSR + SSE\n12.8442 = 9.5524 + 3.2832 (within rounding error)\nR-squared calculation: R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nR¬≤ = 9.5524 √∑ 12.8442\n   = 0.7438\n\n\nStep 4: R Verification\n\n# Fit linear model\nmodel &lt;- lm(GH ~ DM, data = data)\n\n# View summary statistics\nsummary(model)\n\n\nCall:\nlm(formula = GH ~ DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62764 -0.46274 -0.08615  0.26624  1.25109 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept) 17.62976    0.50121  35.174 0.000000000467 ***\nDM          -0.34042    0.07053  -4.827        0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6406 on 8 degrees of freedom\nMultiple R-squared:  0.7444,    Adjusted R-squared:  0.7124 \nF-statistic:  23.3 on 1 and 8 DF,  p-value: 0.00131\n\n# Calculate R-squared manually\nSST &lt;- sum((GH - mean(GH))^2)\nSSE &lt;- sum(residuals(model)^2)\nSSR &lt;- SST - SSE\nR2_manual &lt;- SSR/SST\nR2_manual\n\n[1] 0.7443793\n\n\n\n\nStep 5: Residual Analysis\n\n# Create residual plots\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\nStep 6: Predicted vs Actual Values Plot\n\n# Create predicted vs actual plot\nggplot(data.frame(\n  Actual = GH,\n  Predicted = fitted(model)\n), aes(x = Predicted, y = Actual)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Predicted vs Actual Values\",\n    x = \"Predicted Gallagher Index\",\n    y = \"Actual Gallagher Index\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nLog-Transformed Models\n\n\nStep 1: Data Transformation\nFirst, calculate natural logarithms of variables:\n\n\n\ni\nX_i\nY_i\n\\ln(X_i)\n\\ln(Y_i)\n\n\n\n\n1\n2\n18.2\n0.6931\n2.9014\n\n\n2\n3\n16.7\n1.0986\n2.8154\n\n\n3\n4\n15.8\n1.3863\n2.7600\n\n\n4\n5\n15.3\n1.6094\n2.7278\n\n\n5\n6\n15.0\n1.7918\n2.7081\n\n\n6\n7\n14.8\n1.9459\n2.6946\n\n\n7\n8\n14.7\n2.0794\n2.6878\n\n\n8\n9\n14.6\n2.1972\n2.6810\n\n\n9\n10\n14.55\n2.3026\n2.6777\n\n\n10\n11\n14.52\n2.3979\n2.6757\n\n\n\n\n\nStep 2: Compare Different Model Specifications\nWe estimate three alternative specifications:\n\nLog-linear model: \\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nLinear-log model: Y_i = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\nLog-log model: \\ln(Y_i) = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\n\n\n# Create transformed variables\ndata$log_DM &lt;- log(data$DM)\ndata$log_GH &lt;- log(data$GH)\n\n# Fit models\nmodel_linear &lt;- lm(GH ~ DM, data = data)\nmodel_loglinear &lt;- lm(log_GH ~ DM, data = data)\nmodel_linearlog &lt;- lm(GH ~ log_DM, data = data)\nmodel_loglog &lt;- lm(log_GH ~ log_DM, data = data)\n\n# Compare R-squared values\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Linear\", \"Log-linear\", \"Linear-log\", \"Log-log\"),\n  R_squared = c(\n    summary(model_linear)$r.squared,\n    summary(model_loglinear)$r.squared,\n    summary(model_linearlog)$r.squared,\n    summary(model_loglog)$r.squared\n  )\n)\n\n# Display comparison\nmodels_comparison\n\n       Model R_squared\n1     Linear 0.7443793\n2 Log-linear 0.7670346\n3 Linear-log 0.9141560\n4    Log-log 0.9288088\n\n\n\n\nStep 3: Visual Comparison\n\n# Create plots for each model\np1 &lt;- ggplot(data, aes(x = DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Linear Model\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Log-linear Model\") +\n  theme_minimal()\n\np3 &lt;- ggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Linear-log Model\") +\n  theme_minimal()\n\np4 &lt;- ggplot(data, aes(x = log_DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Log-log Model\") +\n  theme_minimal()\n\n# Arrange plots in a grid\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Residual Analysis for Best Model\nBased on R-squared values, analyze residuals for the best-fitting model:\n\n# Residual plots for best model\npar(mfrow = c(2, 2))\nplot(model_linearlog)\n\n\n\n\n\n\n\n\n\n\nStep 5: Interpretation of Best Model\nThe linear-log model coefficients:\n\nsummary(model_linearlog)\n\n\nCall:\nlm(formula = GH ~ log_DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40702 -0.30207 -0.04907  0.22905  0.60549 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)  19.0223     0.4079   46.64 0.0000000000494 ***\nlog_DM       -2.0599     0.2232   -9.23 0.0000153880425 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3712 on 8 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9034 \nF-statistic: 85.19 on 1 and 8 DF,  p-value: 0.00001539\n\n\nInterpretation: - \\hat{\\beta_0} represents the expected Gallagher Index when ln(DM) = 0 (i.e., when DM = 1) - \\hat{\\beta_1} represents the change in Gallagher Index associated with a one-unit increase in ln(DM)\n\n\nStep 6: Model Predictions\n\n# Create prediction plot for best model\nggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Linear-log Model: Gallagher Index vs ln(District Magnitude)\",\n    x = \"ln(District Magnitude)\",\n    y = \"Gallagher Index\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nStep 7: Elasticity Analysis\nFor the log-log model, coefficients represent elasticities directly. Calculate average elasticity for the linear-log model:\n\n# Calculate elasticity at means\nmean_DM &lt;- mean(data$DM)\nmean_GH &lt;- mean(data$GH)\nbeta1 &lt;- coef(model_linearlog)[2]\nelasticity &lt;- beta1 * (1/mean_GH)\nelasticity\n\n    log_DM \n-0.1336136 \n\n\nThis represents the percentage change in the Gallagher Index for a 1% change in District Magnitude.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.3-understanding-pearson-spearman-and-kendall",
    "href": "correg_en.html#appendix-a.3-understanding-pearson-spearman-and-kendall",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.38 Appendix A.3: Understanding Pearson, Spearman, and Kendall",
    "text": "9.38 Appendix A.3: Understanding Pearson, Spearman, and Kendall\n\nDataset\n\ndata &lt;- data.frame(\n  x = c(2, 4, 5, 3, 8),\n  y = c(3, 5, 4, 4, 7)\n)\n\n\n\nPearson Correlation\n r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}} \n\nStep-by-Step Calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n2\n3\n-2.4\n-1.6\n3.84\n5.76\n2.56\n\n\n2\n4\n5\n-0.4\n0.4\n-0.16\n0.16\n0.16\n\n\n3\n5\n4\n0.6\n-0.6\n-0.36\n0.36\n0.36\n\n\n4\n3\n4\n-1.4\n-0.6\n0.84\n1.96\n0.36\n\n\n5\n8\n7\n3.6\n2.4\n8.64\n12.96\n5.76\n\n\nSum\n22\n23\n0\n0\n12.8\n21.2\n9.2\n\n\n\n\\bar{x} = 4.4 \\bar{y} = 4.6\n r = \\frac{12.8}{\\sqrt{21.2 \\times 9.2}} = \\frac{12.8}{\\sqrt{195.04}} = \\frac{12.8}{13.97} = 0.92 \n\n\n\nSpearman Correlation\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} \n\nStep-by-Step Calculations:\n\n\n\ni\nx_i\ny_i\nRank x_i\nRank y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n3\n1\n1\n0\n0\n\n\n2\n4\n5\n3\n5\n-2\n4\n\n\n3\n5\n4\n4\n2.5\n1.5\n2.25\n\n\n4\n3\n4\n2\n2.5\n-0.5\n0.25\n\n\n5\n8\n7\n5\n4\n1\n1\n\n\nSum\n\n\n\n\n\n7.5\n\n\n\n \\rho = 1 - \\frac{6(7.5)}{5(25-1)} = 1 - \\frac{45}{120} = 0.82 \n\n\n\nKendall‚Äôs Tau\n \\tau = \\frac{\\text{number of concordant pairs} - \\text{number of discordant pairs}}{\\frac{1}{2}n(n-1)} \n\nStep-by-Step Calculations:\n\n\n\nPair (i,j)\nx_i,x_j\ny_i,y_j\nx_j-x_i\ny_j-y_i\nResult\n\n\n\n\n(1,2)\n2,4\n3,5\n+2\n+2\nC\n\n\n(1,3)\n2,5\n3,4\n+3\n+1\nC\n\n\n(1,4)\n2,3\n3,4\n+1\n+1\nC\n\n\n(1,5)\n2,8\n3,7\n+6\n+4\nC\n\n\n(2,3)\n4,5\n5,4\n+1\n-1\nD\n\n\n(2,4)\n4,3\n5,4\n-1\n-1\nC\n\n\n(2,5)\n4,8\n5,7\n+4\n+2\nC\n\n\n(3,4)\n5,3\n4,4\n-2\n0\nD\n\n\n(3,5)\n5,8\n4,7\n+3\n+3\nC\n\n\n(4,5)\n3,8\n4,7\n+5\n+3\nC\n\n\n\nNumber of concordant pairs = 8 Number of discordant pairs = 2  \\tau = \\frac{8-2}{10} = 0.74 \n\n\n\nVerification in R\n\ncat(\"Pearson:\", round(cor(data$x, data$y, method=\"pearson\"), 2), \"\\n\")\n\nPearson: 0.92 \n\ncat(\"Spearman:\", round(cor(data$x, data$y, method=\"spearman\"), 2), \"\\n\")\n\nSpearman: 0.82 \n\ncat(\"Kendall:\", round(cor(data$x, data$y, method=\"kendall\"), 2), \"\\n\")\n\nKendall: 0.74 \n\n\n\n\nInterpretation of Results\n\nPearson Correlation (r = 0.92)\n\nStrong positive linear correlation\nIndicates a very strong linear relationship between variables\n\nSpearman Correlation (œÅ = 0.82)\n\nAlso strong positive correlation\nSlightly lower than Pearson‚Äôs, suggesting some deviations from monotonicity\n\nKendall‚Äôs Tau (œÑ = 0.74)\n\nLowest of the three values, but still indicates strong association\nMore robust to outliers\n\n\n\n\nComparison of Measures\n\nDifferences in Values:\n\nPearson (0.92) - highest value, strong linearity\nSpearman (0.82) - considers only ranking\nKendall (0.74) - most conservative measure\n\nPractical Application:\n\nAll measures confirm strong positive association\nDifferences between measures indicate slight deviations from perfect linearity\nKendall provides the most conservative estimate of relationship strength\n\n\n\n\nExercises\n\nChange y[3] from 4 to 6 and recalculate all three correlations\nAdd an outlier (x=10, y=2) and recalculate correlations\nCompare which measure is most sensitive to changes in the data\n\n\n\nKey Points to Remember\n\nPearson Correlation:\n\nMeasures linear relationship\nMost sensitive to outliers\nRequires interval or ratio data\n\nSpearman Correlation:\n\nMeasures monotonic relationship\nLess sensitive to outliers\nWorks with ordinal data\n\nKendall‚Äôs Tau:\n\nMeasures ordinal association\nMost robust to outliers\nBest for small samples and tied ranks",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-b-bias-in-ols-estimation-with-endogenous-regressors",
    "href": "correg_en.html#appendix-b-bias-in-ols-estimation-with-endogenous-regressors",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.39 Appendix B: Bias in OLS Estimation with Endogenous Regressors",
    "text": "9.39 Appendix B: Bias in OLS Estimation with Endogenous Regressors\nIn this tutorial, we will explore the bias in Ordinary Least Squares (OLS) estimation when the error term is correlated with the explanatory variable, a situation known as endogeneity. We will first derive the bias mathematically and then illustrate it using a simulated dataset in R.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#theoretical-derivation",
    "href": "correg_en.html#theoretical-derivation",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.40 Theoretical Derivation",
    "text": "9.40 Theoretical Derivation\nConsider a data generating process (DGP) where the true relationship between x and y is:\n y = 2x + e \nHowever, there is an endogeneity problem because the error term e is correlated with x in the following way:\n e = 1x + u \nwhere u is an independent error term.\nIf we estimate the simple linear model y = \\hat{\\beta_0} + \\hat{\\beta_1}x + \\varepsilon using OLS, the OLS estimator of \\hat{\\beta_1} will be biased due to the endogeneity issue.\nTo understand the bias, let‚Äôs derive the expected value of the OLS estimator \\hat{\\beta}_1:\n\\begin{align*}\nE[\\hat{\\beta}_1] &= E[(X'X)^{-1}X'y] \\\\\n                 &= E[(X'X)^{-1}X'(2x + 1x + u)] \\\\\n                 &= E[(X'X)^{-1}X'(3x + u)] \\\\\n                 &= 3 + E[(X'X)^{-1}X'u]\n\\end{align*}\nIf the error term u is uncorrelated with x, then E[(X'X)^{-1}X'u] = 0, and the OLS estimator would be unbiased: E[\\hat{\\beta}_1] = 3. However, in this case, the original error term e is correlated with x, so u is also likely to be correlated with x.\nAssuming E[(X'X)^{-1}X'u] \\neq 0, the OLS estimator will be biased:\n\\begin{align*}\n\\text{Bias}(\\hat{\\beta}_1) &= E[\\hat{\\beta}_1] - \\beta_{1,\\text{true}} \\\\\n                           &= 3 + E[(X'X)^{-1}X'u] - 2 \\\\\n                           &= 1 + E[(X'X)^{-1}X'u]\n\\end{align*}\nThe direction and magnitude of the bias will depend on the correlation between x and u. If x and u are positively correlated, the bias will be positive, and the OLS estimator will overestimate the true coefficient. Conversely, if x and u are negatively correlated, the bias will be negative, and the OLS estimator will underestimate the true coefficient.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#simulation-in-r",
    "href": "correg_en.html#simulation-in-r",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.41 Simulation in R",
    "text": "9.41 Simulation in R\nLet‚Äôs create a simple dataset with 10 observations where x is in the interval 1:10, and generate y values based on the given DGP: y = 2x + e, where e = 1x + u, and u is a random error term.\n\nset.seed(123)  # for reproducibility\nx &lt;- 1:10\nu &lt;- rnorm(10, mean = 0, sd = 1)\ne &lt;- 1*x + u\n# e &lt;- 1*x\ny &lt;- 2*x + e\n\n# Generate the data frame\ndata &lt;- data.frame(x = x, y = y)\n\n# Estimate the OLS model\nmodel &lt;- lm(y ~ x, data = data)\n\n# Print the model summary\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1348 -0.5624 -0.1393  0.3854  1.6814 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)   0.5255     0.6673   0.787         0.454    \nx             2.9180     0.1075  27.134 0.00000000367 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9768 on 8 degrees of freedom\nMultiple R-squared:  0.9893,    Adjusted R-squared:  0.9879 \nF-statistic: 736.3 on 1 and 8 DF,  p-value: 0.000000003666\n\n\nIn this example, the true relationship is y = 2x + e, where e = 1x + u. However, when we estimate the OLS model, we get:\n \\hat{y} = 0.18376 + 3.05874x \nThe estimated coefficient for x is 3.05874, which is biased upward from the true value of 2. This bias is due to the correlation between the error term e and the explanatory variable x.\nTo visualize the bias using ggplot2, we can plot the true relationship (y = 2x) and the estimated OLS relationship:\n\nlibrary(ggplot2)\n\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 2, color = \"blue\", linewidth = 1, linetype = \"dashed\") +\n  geom_abline(intercept = coef(model)[1], slope = coef(model)[2], color = \"red\", linewidth = 1) +\n  labs(title = \"True vs. Estimated Relationship\", x = \"x\", y = \"y\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  scale_color_manual(name = \"Lines\", values = c(\"blue\", \"red\"), \n                     labels = c(\"True\", \"OLS\"))\n\n\n\n\n\n\n\n\nThe plot will show that the estimated OLS line (red) is steeper than the true relationship line (blue), illustrating the upward bias in the estimated coefficient.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#conclusion-1",
    "href": "correg_en.html#conclusion-1",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.42 Conclusion",
    "text": "9.42 Conclusion\nIn summary, when the error term is correlated with the explanatory variable (endogeneity), the OLS estimator will be biased. The direction and magnitude of the bias depend on the nature of the correlation between the error term and the explanatory variable. This tutorial demonstrated the bias both mathematically and through a simulated example in R, using ggplot2 for visualization.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-c.-ols---worked-examples",
    "href": "correg_en.html#appendix-c.-ols---worked-examples",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.43 Appendix C. OLS - Worked Examples",
    "text": "9.43 Appendix C. OLS - Worked Examples",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#i.-voter-participation-and-economic-prosperity",
    "href": "correg_en.html#i.-voter-participation-and-economic-prosperity",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.44 I. Voter Participation and Economic Prosperity",
    "text": "9.44 I. Voter Participation and Economic Prosperity\nAnalysis of the relationship between economic prosperity and voter turnout in Amsterdam districts based on data from the 2022 local elections.\n\nData\nThe sample includes five representative districts:\n\n\n\nDistrict\nIncome (‚Ç¨000s)\nTurnout (%)\n\n\n\n\nNoord\n50\n60\n\n\nZuid\n45\n56\n\n\nCentrum\n56\n70\n\n\nWest\n40\n50\n\n\nOost\n60\n75\n\n\n\n\n# Load libraries\nlibrary(tidyverse)\n\n# Create dataset\ndata &lt;- data.frame(\n  district = c(\"Noord\", \"Zuid\", \"Centrum\", \"West\", \"Oost\"),\n  income = c(50, 45, 56, 40, 60),\n  turnout = c(60, 56, 70, 50, 75)\n)\n\n# View data\ndata\n\n  district income turnout\n1    Noord     50      60\n2     Zuid     45      56\n3  Centrum     56      70\n4     West     40      50\n5     Oost     60      75\n\n\n\n\nPart 1: Descriptive Statistics\n\n# Statistics for income\ncat(\"INCOME (‚Ç¨000s):\\n\")\n\nINCOME (‚Ç¨000s):\n\ncat(\"Mean:\", mean(data$income), \"\\n\")\n\nMean: 50.2 \n\ncat(\"Median:\", median(data$income), \"\\n\")\n\nMedian: 50 \n\ncat(\"Standard deviation:\", round(sd(data$income), 2), \"\\n\")\n\nStandard deviation: 8.07 \n\ncat(\"Range:\", min(data$income), \"-\", max(data$income), \"\\n\\n\")\n\nRange: 40 - 60 \n\n# Statistics for turnout\ncat(\"TURNOUT (%):\\n\")\n\nTURNOUT (%):\n\ncat(\"Mean:\", mean(data$turnout), \"\\n\")\n\nMean: 62.2 \n\ncat(\"Median:\", median(data$turnout), \"\\n\")\n\nMedian: 60 \n\ncat(\"Standard deviation:\", round(sd(data$turnout), 2), \"\\n\")\n\nStandard deviation: 10.21 \n\ncat(\"Range:\", min(data$turnout), \"-\", max(data$turnout))\n\nRange: 50 - 75\n\n\n\n\nPart 2: Correlation Analysis\n\n# Pearson correlation test\ncorrelation &lt;- cor.test(data$income, data$turnout)\n\ncat(\"Correlation coefficient (r):\", round(correlation$estimate, 3), \"\\n\")\n\nCorrelation coefficient (r): 0.994 \n\ncat(\"P-value:\", round(correlation$p.value, 3), \"\\n\")\n\nP-value: 0.001 \n\n# Interpretation\nif (correlation$p.value &lt; 0.05) {\n  cat(\"Result is statistically significant (p &lt; 0.05)\")\n} else {\n  cat(\"Result is not statistically significant (p ‚â• 0.05)\")\n}\n\nResult is statistically significant (p &lt; 0.05)\n\n\n\n\nPart 3: Linear Regression Model\n\n# Fit the model\nmodel &lt;- lm(turnout ~ income, data = data)\n\n# Basic model information\nsummary(model)\n\n\nCall:\nlm(formula = turnout ~ income, data = data)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \nincome       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n# R-squared - how well the model explains the data\ncat(\"\\nThe model explains\", round(summary(model)$r.squared * 100, 1), \"% of the variance in turnout\")\n\n\nThe model explains 98.9 % of the variance in turnout\n\n\n\n\nPart 4: Visualization\n\n# Scatter plot with regression line\nggplot(data, aes(x = income, y = turnout)) +\n  geom_point(size = 4, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\", alpha = 0.3) +\n  geom_text(aes(label = district), vjust = -1, size = 4) +\n  labs(\n    title = \"Income vs Voter Turnout\",\n    subtitle = paste(\"r =\", round(correlation$estimate, 3)),\n    x = \"Average Income (‚Ç¨000s)\",\n    y = \"Voter Turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nPart 5: Interpreting Results\n\n# Extract coefficients\ncoefficients &lt;- coef(model)\nintercept &lt;- round(coefficients[1], 1)\nslope &lt;- round(coefficients[2], 2)\n\ncat(\"REGRESSION EQUATION:\\n\")\n\nREGRESSION EQUATION:\n\ncat(\"Turnout =\", intercept, \"+\", slope, \"√ó Income\\n\\n\")\n\nTurnout = -0.9 + 1.26 √ó Income\n\ncat(\"INTERPRETATION:\\n\")\n\nINTERPRETATION:\n\ncat(\"‚Ä¢ An increase of ‚Ç¨1000 in income increases turnout by\", slope, \"percentage points\\n\")\n\n‚Ä¢ An increase of ‚Ç¨1000 in income increases turnout by 1.26 percentage points\n\ncat(\"‚Ä¢ The correlation is\", ifelse(correlation$estimate &gt; 0, \"positive\", \"negative\"), \n    \"and\", ifelse(abs(correlation$estimate) &gt; 0.7, \"strong\", \n               ifelse(abs(correlation$estimate) &gt; 0.5, \"moderate\", \"weak\")))\n\n‚Ä¢ The correlation is positive and strong\n\n\n\n\nConclusions\nKey findings: - There is a strong positive correlation (r = 0.994) between income and voter turnout - The model explains 98.9% of the variance in the data - Districts with higher incomes have higher voter turnout\nPractical application: Results suggest that efforts to increase voter turnout should particularly focus on lower-income districts.\nLimitations and Caveats:\n‚ö†Ô∏è Critical limitations:\n\nVery small sample (n=5) severely limits generalizability\nLow statistical power - risk of Type II errors\nLack of control for confounding variables (age, education, population density)\nPossible spurious correlation - additional control variables needed\n\nRecommendations for future research:\n\nExpand sample to all Amsterdam districts\nInclude demographic and socioeconomic variables\nAnalyze longitudinal data from multiple election cycles",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#ii.-descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "href": "correg_en.html#ii.-descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.45 II. Descriptive Statistics and OLS Example - Income and Voter Turnout",
    "text": "9.45 II. Descriptive Statistics and OLS Example - Income and Voter Turnout\nBackground\nIn preparation for the 2024 municipal elections, the Amsterdam Electoral Commission conducted research on voter participation patterns across different city neighborhoods. A key question emerged:\nDoes economic prosperity of a neighborhood correlate with civic engagement, specifically voter turnout?\nData Collection\nSample: 5 representative neighborhoods in Amsterdam\nTime Period: Data from the 2022 municipal elections\nVariables:\n\nIncome: Average annual household income per capita (thousands ‚Ç¨)\nTurnout: Percentage of registered voters who voted in the election\n\n\nInitial R Output for Reference\n\n# Data\nincome &lt;- c(50, 45, 56, 40, 60)  # thousands ‚Ç¨\nturnout &lt;- c(60, 56, 70, 50, 75) # %\n\n# Full model check\nmodel &lt;- lm(turnout ~ income)\nsummary(model)\n\n\nCall:\nlm(formula = turnout ~ income)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \nincome       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n\n\n\nDispersion Measures\nMeans:\n\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n} = \\frac{50 + 45 + 56 + 40 + 60}{5} = \\frac{251}{5} = 50.2\n\\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n} = \\frac{60 + 56 + 70 + 50 + 75}{5} = \\frac{311}{5} = 62.2\n\n# Verification\nmean(income)  # 50.2\n\n[1] 50.2\n\nmean(turnout) # 62.2\n\n[1] 62.2\n\n\nVariances:\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X: (-0.2, -5.2, 5.8, -10.2, 9.8)\ns^2_X = \\frac{0.04 + 27.04 + 33.64 + 104.04 + 96.04}{4} = \\frac{260.8}{4} = 65.2\nDeviations for Y: (-2.2, -6.2, 7.8, -12.2, 12.8)\ns^2_Y = \\frac{4.84 + 38.44 + 60.84 + 148.84 + 163.84}{4} = \\frac{416.8}{4} = 104.2\n\n# Verification\nvar(income)  # 65.2\n\n[1] 65.2\n\nvar(turnout) # 104.2\n\n[1] 104.2\n\n\n\n\nCovariance and Correlation\nCovariance:\ns_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-0.2 \\times -2.2) = 0.44 (-5.2 \\times -6.2) = 32.24 (5.8 \\times 7.8) = 45.24 (-10.2 \\times -12.2) = 124.44 (9.8 \\times 12.8) = 125.44\ns_{XY} = \\frac{327.8}{4} = 81.95\n\n# Verification\ncov(income, turnout) # 81.95\n\n[1] 81.95\n\n\nCorrelation:\nr_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{81.95}{\\sqrt{65.2}\\sqrt{104.2}} = 0.994\n\n# Verification\ncor(income, turnout) # 0.994\n\n[1] 0.9942402\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{81.95}{65.2} = 1.2571429\nIntercept:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\nStep by step:\n\n1.2571429 \\times 50.2 = 63.1085714\n\\hat{\\beta_0} = 62.2 - 63.1085714 = -0.9085714\n\n\n# Verification\ncoef(model)  # Exact coefficients from R\n\n(Intercept)      income \n -0.8964724   1.2569018 \n\n\n\n\nDetailed Decomposition of Variance and R-squared\nStep 1: Calculate predicted values (\\hat{Y}):\n \\hat{Y} = -0.9085714 + 1.2571429X\nThe predicted values \\hat{Y} for each X value:\nFor X = 50:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (50)\n \\hat{Y} = -0.9085714 + 62.857145 \\hat{Y} = 61.9485736\nFor X = 45:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (45)\n \\hat{Y} = -0.9085714 + 56.5714305 \\hat{Y} = 55.6535591\nFor X = 56:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (56)\n \\hat{Y} = -0.9085714 + 70.4200024 \\hat{Y} = 69.5114310\nFor X = 40:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (40)\n \\hat{Y} = -0.9085714 + 50.2657160 \\hat{Y} = 49.3571446\nFor X = 60:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (60)\n \\hat{Y} = -0.9085714 + 75.4285740 \\hat{Y} = 74.5200026\n\n# Verification of predicted values\ny_hat &lt;- -0.9085714 + 1.2571429 * income\ndata.frame(\n  X = income,\n  Y = turnout,\n  Y_hat = y_hat,\n  row.names = 1:5\n)\n\n   X  Y    Y_hat\n1 50 60 61.94857\n2 45 56 55.66286\n3 56 70 69.49143\n4 40 50 49.37714\n5 60 75 74.52000\n\n\nStep 2: Calculate SST (Total Sum of Squares)\nSST = \\sum(Y_i - \\bar{Y})^2 \\text{ where } \\bar{Y} = 62.2\n(60 - 62.2)^2 = (-2.2)^2 = 4.84 (56 - 62.2)^2 = (-6.2)^2 = 38.44 (70 - 62.2)^2 = (7.8)^2 = 60.84 (50 - 62.2)^2 = (-12.2)^2 = 148.84 (75 - 62.2)^2 = (12.8)^2 = 163.84\nSST = 4.84 + 38.44 + 60.84 + 148.84 + 163.84 = 416.8\nStep 3: Calculate SSR (Regression Sum of Squares)\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\n(61.9485736 - 62.2)^2 = (-0.2514264)^2 = 0.0632151 (55.6535591 - 62.2)^2 = (-6.5464409)^2 = 42.8558689 (69.5114310 - 62.2)^2 = (7.3114310)^2 = 53.4570178 (49.3571446 - 62.2)^2 = (-12.8428554)^2 = 164.9389370 (74.5200026 - 62.2)^2 = (12.3200026)^2 = 151.7824640\nSSR = 413.0975028\nStep 4: Calculate SSE (Error Sum of Squares)\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n(60 - 61.9485736)^2 = (-1.9485736)^2 = 3.7969384 (56 - 55.6535591)^2 = (0.3464409)^2 = 0.1200212 (70 - 69.5114310)^2 = (0.4885690)^2 = 0.2387198 (50 - 49.3571446)^2 = (0.6428554)^2 = 0.4132631 (75 - 74.5200026)^2 = (0.4799974)^2 = 0.2303975\nSSE = 4.7024972\nStep 5: Verify decomposition\nSST = SSR + SSE 416.8 = 413.0975028 + 4.7024972\nStep 6: Calculate R-squared\nR^2 = \\frac{SSR}{SST} = \\frac{413.0975028}{416.8} = 0.9916\n\n# Verification\nsummary(model)$r.squared  # Should match our calculation\n\n[1] 0.9885135\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\ndf &lt;- data.frame(income = income, turnout = turnout)\n\nggplot(df, aes(x = income, y = turnout)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Voter Turnout vs Income per Capita\",\n    x = \"Income per Capita (thousands ‚Ç¨)\",\n    y = \"Voter Turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    text = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\nInterpretation\nThe analysis shows:\n\nA very strong positive correlation (r = 0.994) between income and voter turnout\nThe regression equation \\hat{Y} = -0.9085714 + 1.2571429X indicates that:\n\nFor each ‚Ç¨1,000 increase in income, turnout increases by about 1.26 percentage points\nThe intercept (-0.9086) has little practical meaning as income is never zero\n\nThe R-squared of 0.9916 indicates that 99.16% of the variance in turnout is explained by income",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#iii.-anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "href": "correg_en.html#iii.-anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.46 III. Anxiety Levels and Cognitive Performance: A Laboratory Study",
    "text": "9.46 III. Anxiety Levels and Cognitive Performance: A Laboratory Study\n\nData and Context\nIn a psychology experiment, researchers measured the relationship between anxiety levels (measured by galvanic skin response, GSR) and cognitive performance (score on a working memory task).\n\n# Data\nanxiety &lt;- c(2.1, 3.4, 4.2, 5.1, 5.8, 6.4, 7.2, 8.0)  # GSR readings\nperformance &lt;- c(92, 88, 84, 78, 74, 70, 65, 62)      # Working memory scores\n\n# Initial model check\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8993 -0.6660  0.2162  0.6106  1.5262 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 105.3248     1.3189   79.86 0.00000000026 ***\nanxiety      -5.4407     0.2359  -23.06 0.00000043549 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 6 degrees of freedom\nMultiple R-squared:  0.9888,    Adjusted R-squared:  0.987 \nF-statistic: 531.9 on 1 and 6 DF,  p-value: 0.0000004355\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{2.1 + 3.4 + 4.2 + 5.1 + 5.8 + 6.4 + 7.2 + 8.0}{8} = \\frac{42.2}{8} = 5.275\n\\bar{Y} = \\frac{92 + 88 + 84 + 78 + 74 + 70 + 65 + 62}{8} = \\frac{613}{8} = 76.625\n\n# Verification\nmean(anxiety)\n\n[1] 5.275\n\nmean(performance)\n\n[1] 76.625\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(2.1 - 5.275) = -3.175\n(3.4 - 5.275) = -1.875\n(4.2 - 5.275) = -1.075\n(5.1 - 5.275) = -0.175\n(5.8 - 5.275) = 0.525\n(6.4 - 5.275) = 1.125\n(7.2 - 5.275) = 1.925\n(8.0 - 5.275) = 2.725\n\nSquared deviations:\n10.08063 + 3.51563 + 1.15563 + 0.03063 + 0.27563 + 1.26563 + 3.70563 +\n7.42563 = 27.45500\ns^2_X = \\frac{27.45500}{7} = 3.922143\nSimilarly for Y: Deviations:\n15.375, 11.375, 7.375, 1.375, -2.625, -6.625, -11.625, -14.625\ns^2_Y = \\frac{236.875 + 129.391 + 54.391 + 1.891 + 6.891 + 43.891 + 135.141 + 213.891}{7} = \\frac{822.362}{7} = 117.4803\n\n# Verification\nvar(anxiety)\n\n[1] 3.922143\n\nvar(performance)\n\n[1] 117.4107\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-3.175 √ó 15.375) = -48.815625\n(-1.875 √ó 11.375) = -21.328125\n(-1.075 √ó 7.375) = -7.928125\n(-0.175 √ó 1.375) = -0.240625\n(0.525 √ó -2.625) = -1.378125\n(1.125 √ó -6.625) = -7.453125\n(1.925 √ó -11.625) = -22.378125\n(2.725 √ó -14.625) = -39.853125\nSum = -149.375\ns_{XY} = \\frac{-149.375}{7} = -21.33929\n\n# Verification\ncov(anxiety, performance)\n\n[1] -21.33929\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-21.33929}{\\sqrt{3.922143}\\sqrt{117.4803}} = -0.9932\n\n# Verification\ncor(anxiety, performance)\n\n[1] -0.9944073\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-21.33929}{3.922143} = -5.4407\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-5.4407 √ó 5.275 = -28.6997\n\\hat{\\beta_0} = 76.625 - (-28.6997) = 105.3247\n\n\n# Verification\ncoef(model)\n\n(Intercept)     anxiety \n 105.324804   -5.440721 \n\n\n\n\n4. R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}): \\hat{Y} = 105.3247 - 5.4407X\n\n# Predicted values\ny_hat &lt;- 105.3247 - 5.4407 * anxiety\ndata.frame(\n  Anxiety = anxiety,\n  Performance = performance,\n  Predicted = y_hat,\n  row.names = 1:8\n)\n\n  Anxiety Performance Predicted\n1     2.1          92  93.89923\n2     3.4          88  86.82632\n3     4.2          84  82.47376\n4     5.1          78  77.57713\n5     5.8          74  73.76864\n6     6.4          70  70.50422\n7     7.2          65  66.15166\n8     8.0          62  61.79910\n\n\nStep 2: Sum of Squares\nSST = \\sum(Y_i - \\bar{Y})^2 = 822.362\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 816.3094\nSSE = \\sum(Y_i - \\hat{Y}_i)^2 = 6.0526\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{816.3094}{822.362} = 0.9926\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.9888459\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\n\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Cognitive Performance vs. Anxiety Levels\",\n    x = \"Anxiety (GSR)\",\n    y = \"Performance Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.993) between anxiety and cognitive performance\nFor each unit increase in GSR (anxiety), performance decreases by approximately 5.44 points\nThe model explains 99.26% of the variance in performance scores\nThe relationship appears to be strongly linear, suggesting a reliable anxiety-performance relationship\nThe high intercept (105.32) represents the theoretical maximum performance at zero anxiety\n\n\n\nStudy Limitations\n\nSmall sample size (n=8)\nPossible other confounding variables\nLimited range of anxiety levels\nCross-sectional rather than longitudinal data",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#iv.-anxiety-vs.-performance-correlation-and-regression-analysis",
    "href": "correg_en.html#iv.-anxiety-vs.-performance-correlation-and-regression-analysis",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.47 IV. Anxiety vs.¬†Performance: Correlation and Regression Analysis",
    "text": "9.47 IV. Anxiety vs.¬†Performance: Correlation and Regression Analysis\nIn this tutorial, we‚Äôll explore the relationship between test anxiety levels and exam performance among university students. Research suggests that while a small amount of anxiety can be motivating, excessive anxiety typically impairs performance through reduced concentration, working memory interference, and physical symptoms. We‚Äôll analyze data from 8 students to understand this relationship mathematically.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#data-presentation",
    "href": "correg_en.html#data-presentation",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.48 Data Presentation",
    "text": "9.48 Data Presentation\n\nThe Dataset\nWe collected data from 8 students, measuring: - X: Test anxiety score (1-10 scale, where 1 = very low, 10 = very high) - Y: Exam performance (percentage score)\n\n# Our data\nanxiety &lt;- c(2.5, 3.2, 4.1, 4.8, 5.6, 6.3, 7.0, 7.9)  # Anxiety scores\nperformance &lt;- c(80, 85, 78, 82, 77, 74, 68, 72)      # Exam scores (%)\n\n# Create a data frame for easy viewing\ndata &lt;- data.frame(\n  Student = 1:8,\n  Anxiety = anxiety,\n  Performance = performance\n)\nprint(data)\n\n  Student Anxiety Performance\n1       1     2.5          80\n2       2     3.2          85\n3       3     4.1          78\n4       4     4.8          82\n5       5     5.6          77\n6       6     6.3          74\n7       7     7.0          68\n8       8     7.9          72\n\n\n\n\nInitial Visualization\nLet‚Äôs first visualize our data to get an intuitive sense of the relationship:\n\nlibrary(ggplot2)\n\n# Create scatterplot\nggplot(data, aes(x = Anxiety, y = Performance)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  labs(\n    title = \"Test Anxiety vs. Exam Performance\",\n    x = \"Test Anxiety Score\",\n    y = \"Exam Performance (%)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(2, 8, 1)) +\n  scale_y_continuous(breaks = seq(65, 90, 5))",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-statistics",
    "href": "correg_en.html#summary-statistics",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.49 Summary Statistics",
    "text": "9.49 Summary Statistics\n\nCalculating the Means\nThe mean is the average value, calculated by summing all observations and dividing by the number of observations.\nMean of Anxiety Scores (X): \\bar{X} = \\frac{\\sum_{i=1}^{n} X_i}{n} = \\frac{2.5 + 3.2 + 4.1 + 4.8 + 5.6 + 6.3 + 7.0 + 7.9}{8}\n\\bar{X} = \\frac{41.4}{8} = 5.175\nMean of Performance Scores (Y): \\bar{Y} = \\frac{\\sum_{i=1}^{n} Y_i}{n} = \\frac{80 + 85 + 78 + 82 + 77 + 74 + 68 + 72}{8}\n\\bar{Y} = \\frac{616}{8} = 77\n\n# Verify our calculations\nmean_x &lt;- mean(anxiety)\nmean_y &lt;- mean(performance)\ncat(\"Mean Anxiety:\", mean_x, \"\\n\")\n\nMean Anxiety: 5.175 \n\ncat(\"Mean Performance:\", mean_y, \"\\n\")\n\nMean Performance: 77 \n\n\n\n\nCalculating Variance and Standard Deviation\nVariance measures how spread out the data is from the mean. We use the sample variance formula (dividing by n-1).\nVariance of X:\nFirst, calculate deviations from the mean (X_i - \\bar{X}):\n\n\n\nStudent\nX_i\nX_i - \\bar{X}\n(X_i - \\bar{X})^2\n\n\n\n\n1\n2.5\n2.5 - 5.175 = -2.675\n(-2.675)^2 = 7.1556\n\n\n2\n3.2\n3.2 - 5.175 = -1.975\n(-1.975)^2 = 3.9006\n\n\n3\n4.1\n4.1 - 5.175 = -1.075\n(-1.075)^2 = 1.1556\n\n\n4\n4.8\n4.8 - 5.175 = -0.375\n(-0.375)^2 = 0.1406\n\n\n5\n5.6\n5.6 - 5.175 = 0.425\n(0.425)^2 = 0.1806\n\n\n6\n6.3\n6.3 - 5.175 = 1.125\n(1.125)^2 = 1.2656\n\n\n7\n7.0\n7.0 - 5.175 = 1.825\n(1.825)^2 = 3.3306\n\n\n8\n7.9\n7.9 - 5.175 = 2.725\n(2.725)^2 = 7.4256\n\n\nSum\n\n\n24.555\n\n\n\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1} = \\frac{24.555}{7} = 3.5079\ns_X = \\sqrt{3.5079} = 1.8730\nVariance of Y:\n\n\n\nStudent\nY_i\nY_i - \\bar{Y}\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n80\n80 - 77 = 3\n(3)^2 = 9\n\n\n2\n85\n85 - 77 = 8\n(8)^2 = 64\n\n\n3\n78\n78 - 77 = 1\n(1)^2 = 1\n\n\n4\n82\n82 - 77 = 5\n(5)^2 = 25\n\n\n5\n77\n77 - 77 = 0\n(0)^2 = 0\n\n\n6\n74\n74 - 77 = -3\n(-3)^2 = 9\n\n\n7\n68\n68 - 77 = -9\n(-9)^2 = 81\n\n\n8\n72\n72 - 77 = -5\n(-5)^2 = 25\n\n\nSum\n\n\n214\n\n\n\ns^2_Y = \\frac{\\sum(Y_i - \\bar{Y})^2}{n-1} = \\frac{214}{7} = 30.5714\ns_Y = \\sqrt{30.5714} = 5.5291\n\n# Verify variance and standard deviation\ncat(\"Variance of Anxiety:\", var(anxiety), \"\\n\")\n\nVariance of Anxiety: 3.507857 \n\ncat(\"SD of Anxiety:\", sd(anxiety), \"\\n\")\n\nSD of Anxiety: 1.872927 \n\ncat(\"Variance of Performance:\", var(performance), \"\\n\")\n\nVariance of Performance: 30.57143 \n\ncat(\"SD of Performance:\", sd(performance), \"\\n\")\n\nSD of Performance: 5.529144",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#covariance-and-correlation-2",
    "href": "correg_en.html#covariance-and-correlation-2",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.50 Covariance and Correlation",
    "text": "9.50 Covariance and Correlation\n\nCalculating Covariance\nCovariance measures how two variables vary together. A negative covariance indicates that as one variable increases, the other tends to decrease.\ns_{XY} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nLet‚Äôs calculate the products for each observation:\n\n\n\n\n\n\n\n\n\nStudent\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n\n\n\n\n1\n-2.675\n3\n(-2.675) \\times (3) = -8.025\n\n\n2\n-1.975\n8\n(-1.975) \\times (8) = -15.800\n\n\n3\n-1.075\n1\n(-1.075) \\times (1) = -1.075\n\n\n4\n-0.375\n5\n(-0.375) \\times (5) = -1.875\n\n\n5\n0.425\n0\n(0.425) \\times (0) = 0\n\n\n6\n1.125\n-3\n(1.125) \\times (-3) = -3.375\n\n\n7\n1.825\n-9\n(1.825) \\times (-9) = -16.425\n\n\n8\n2.725\n-5\n(2.725) \\times (-5) = -13.625\n\n\nSum\n\n\n-60.2\n\n\n\ns_{XY} = \\frac{-60.2}{7} = -8.6\n\n\nCalculating Pearson Correlation Coefficient\nThe correlation coefficient standardizes the covariance to a scale from -1 to +1:\nr = \\frac{s_{XY}}{s_X \\cdot s_Y} = \\frac{-8.6}{1.8730 \\times 5.5291} = \\frac{-8.6}{10.3560} = -0.831\nThis gives us a correlation of -0.831, indicating a strong negative relationship between anxiety and performance.\n\n# Verify correlation\nactual_cor &lt;- cor(anxiety, performance)\ncat(\"Pearson correlation coefficient:\", actual_cor, \"\\n\")\n\nPearson correlation coefficient: -0.8304618",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#simple-ols-regression",
    "href": "correg_en.html#simple-ols-regression",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.51 Simple OLS Regression",
    "text": "9.51 Simple OLS Regression\n\nThe Problem of Modeling Relationships\nWhen we observe a relationship between two variables, we want to find a mathematical model that:\n\nDescribes the relationship\nAllows us to make predictions\nQuantifies the strength of the relationship\n\nThe simplest model is a straight line: Y = \\beta_0 + \\beta_1 X + \\epsilon\nWhere:\n\nY is the outcome variable (performance)\nX is the predictor variable (anxiety)\n\\beta_0 is the intercept (performance when anxiety = 0)\n\\beta_1 is the slope (change in performance per unit change in anxiety)\n\\epsilon is the error term (unexplained variation)\n\n\n\nThe Idea of Sum of Squared Errors (SSE)\n\nWhy Do We Need a Criterion?\nImagine trying to draw a line through our data points. There are infinitely many lines we could draw! Some would go through the middle of the points, others might be too high or too low, too steep or too flat. We need a systematic way to determine which line is ‚Äúbest.‚Äù\n\n# Visualize multiple possible lines\nlibrary(ggplot2)\n\n# Create different possible lines\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  # Bad line 1: Too steep\n  geom_abline(intercept = 100, slope = -5, color = \"gray\", linetype = \"dashed\", alpha = 0.5) +\n  # Bad line 2: Too flat\n  geom_abline(intercept = 78, slope = -0.5, color = \"gray\", linetype = \"dashed\", alpha = 0.5) +\n  # Bad line 3: Wrong direction\n  geom_abline(intercept = 65, slope = 2, color = \"gray\", linetype = \"dashed\", alpha = 0.5) +\n  # Good line (we'll calculate this properly)\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Which Line Best Fits Our Data?\",\n    subtitle = \"Gray dashed lines show poor fits, red line shows the optimal fit\",\n    x = \"Test Anxiety Score\",\n    y = \"Exam Performance (%)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWhat Are Errors (Residuals)?\nFor any line we draw, each data point will have an error or residual - the vertical distance from the point to the line. This represents how ‚Äúwrong‚Äù our prediction is for that point.\n\nPositive error: The actual value is above the predicted value (we underestimated)\nNegative error: The actual value is below the predicted value (we overestimated)\n\n\n# Visualize errors for the regression line\nmodel &lt;- lm(performance ~ anxiety)\npredicted &lt;- predict(model)\n\nggplot(data.frame(anxiety, performance, predicted), aes(x = anxiety, y = performance)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_segment(aes(xend = anxiety, yend = predicted), \n               color = \"gray50\", linetype = \"dashed\", alpha = 0.7) +\n  # Add labels for a few errors\n  geom_text(aes(x = 3.2, y = 86, label = \"Error = +4.9\"), size = 3, color = \"gray40\") +\n  geom_text(aes(x = 7.0, y = 69, label = \"Error = -1.4\"), size = 3, color = \"gray40\") +\n  labs(\n    title = \"Visualizing Errors (Residuals) in Regression\",\n    subtitle = \"Dashed lines show the vertical distance from each point to the regression line\",\n    x = \"Test Anxiety Score\",\n    y = \"Exam Performance (%)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWhy Square the Errors?\nSimply adding up the errors won‚Äôt work because positive and negative errors cancel out. We could use absolute values, but squaring has several advantages:\n\nMathematical convenience: Squared functions are differentiable, making it easier to find the minimum using calculus\nPenalizes large errors more: A few large errors are worse than many small errors\n\nError of 4: squared = 16\nTwo errors of 2: squared = 4 + 4 = 8\nFour errors of 1: squared = 1 + 1 + 1 + 1 = 4\n\nCreates a smooth, bowl-shaped function: This guarantees a unique minimum\n\n\n\nThe SSE Formula\nThe Sum of Squared Errors is: SSE = \\sum_{i=1}^{n}(Y_i - \\hat{Y_i})^2 = \\sum_{i=1}^{n}(Y_i - (\\beta_0 + \\beta_1 X_i))^2\n\n\\min_{\\beta}\\ \\sum_{i=1}^n\\bigl(Y_i-\\hat{Y}_i(\\beta)\\bigr)^2,\n\\quad\\text{where } \\hat{Y}_i(\\beta)=\\beta_0+\\beta_1X_i.\n\nThis formula:\n\nTakes each actual value (Y_i)\nSubtracts the predicted value (\\hat{Y}_i(\\beta)=\\beta_0+\\beta_1X_i)\nSquares the difference\nAdds them all up\n\n\n\nFinding the Best Line\nThe ‚Äúbest‚Äù line is the one that minimizes SSE. Using calculus (taking derivatives with respect to \\beta_0 and \\beta_1 and setting them to zero), we get the OLS formulas.\n\n\n\nOLS Estimators\nThe Ordinary Least Squares (OLS) method finds the values of \\beta_0 and \\beta_1 that minimize SSE:\nSlope estimator: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\nIntercept estimator: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\n\nCalculating the OLS Parameters\nUsing our calculated values:\n\ns_{XY} = -8.6\ns^2_X = 3.5079\n\\bar{X} = 5.175\n\\bar{Y} = 77\n\nStep 1: Calculate the slope \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-8.6}{3.5079} = -2.451\nStep 2: Calculate the intercept \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} = 77 - (-2.451 \\times 5.175) = 77 + 12.684 = 89.684\n\n# Verify with R\nmodel &lt;- lm(performance ~ anxiety)\ncoef(model)\n\n(Intercept)     anxiety \n  89.687233   -2.451639 \n\n\nOur regression equation is: \\hat{Y} = 89.684 - 2.451X\nThis means:\n\nWhen anxiety = 0, predicted performance = 89.68%\nFor each 1-point increase in anxiety, performance decreases by 2.45%",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#plotting-the-model",
    "href": "correg_en.html#plotting-the-model",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.52 Plotting the Model",
    "text": "9.52 Plotting the Model\n\n# Create comprehensive plot\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1) +\n  geom_text(aes(label = paste0(\"(\", round(anxiety, 1), \", \", performance, \")\")),\n            vjust = -1, size = 3) +\n  annotate(\"text\", x = 3, y = 70, \n           label = paste0(\"≈∑ = \", round(coef(model)[1], 2), \" - \", \n                         abs(round(coef(model)[2], 2)), \"x\"),\n           size = 5, color = \"red\") +\n  labs(\n    title = \"Regression Line: Performance vs. Anxiety\",\n    subtitle = \"Higher anxiety is associated with lower exam performance\",\n    x = \"Test Anxiety Score\",\n    y = \"Exam Performance (%)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(2, 8, 1)) +\n  scale_y_continuous(breaks = seq(65, 90, 5))",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#model-evaluation",
    "href": "correg_en.html#model-evaluation",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.53 Model Evaluation",
    "text": "9.53 Model Evaluation\n\nVariance Decomposition\nThe total variation in Y can be decomposed into two parts:\nSST = SSE + SSR\nWhere:\n\nSST (Total Sum of Squares): Total variation in Y\nSSE (Sum of Squared Errors): Unexplained variation\n\nSSR (Sum of Squares Regression): Explained variation\n\n\n\n\n\n\n\nTip\n\n\n\nImagine you‚Äôre trying to understand why people have different salaries at a company. Some people make $40,000, others make $80,000, and some make $120,000. There‚Äôs variation in salaries - they‚Äôre not all the same.\n\nThe Total Variation (SST)\nThis is simply asking: ‚ÄúHow spread out are all the salaries from the average salary?‚Äù\nIf the average salary is $70,000, then SST measures how far each person‚Äôs salary differs from $70,000, squares those differences (to make them all positive), and adds them up. It‚Äôs the total amount of variation we‚Äôre trying to understand.\n\n\nThe Explained Variation (SSR)\nNow suppose we build a model that predicts salary based on years of experience. Our model might say: - 2 years experience ‚Üí predicts $50,000 - 5 years experience ‚Üí predicts $70,000\n- 10 years experience ‚Üí predicts $100,000\nSSR measures how much these predictions vary from the average. It‚Äôs the variation that our model successfully ‚Äúexplains‚Äù through the relationship with experience. It‚Äôs like saying ‚Äúthis much of the salary differences between people is because they have different amounts of experience.‚Äù\n\n\nThe Unexplained Variation (SSE)\nThis is what‚Äôs left over - the part our model can‚Äôt explain.\nMaybe two people both have 5 years of experience, but one makes $65,000 and another makes $75,000. Our model predicted $70,000 for both. These differences from our predictions (the errors) represent variation due to other factors we haven‚Äôt captured - maybe education, performance, negotiation skills, or just random luck.\n\n\n9.54 The Key Insight\nThe beautiful thing is that these three always relate as: Total Variation = Explained Variation + Unexplained Variation\nIt‚Äôs like having a pie chart of ‚Äúwhy salaries differ‚Äù: - One slice is ‚Äúdifferences explained by experience‚Äù (SSR) - The other slice is ‚Äúdifferences due to other stuff‚Äù (SSE) - Together they make the whole pie (SST)\n\n\n9.55 Why This Matters\nThis decomposition lets us calculate R-squared (R¬≤), which is simply: R^2 = \\frac{SSR}{SST} = \\frac{\\text{Explained Variation}}{\\text{Total Variation}}\nIf R¬≤ = 0.70, it means our model explains 70% of why the Y values differ from each other. The remaining 30% is due to factors we haven‚Äôt captured or random noise.\nThink of it like solving a mystery: SST is the total mystery to solve, SSR is how much of the mystery you‚Äôve solved, and SSE is what remains unsolved!\n\n\n\nLet‚Äôs calculate each:\nStep 1: Calculate predicted values\nUsing \\hat{Y} = 89.684 - 2.451X:\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i = 89.684 - 2.451X_i\n\n\n\n\n1\n2.5\n80\n89.684 - 2.451(2.5) = 83.556\n\n\n2\n3.2\n85\n89.684 - 2.451(3.2) = 81.841\n\n\n3\n4.1\n78\n89.684 - 2.451(4.1) = 79.635\n\n\n4\n4.8\n82\n89.684 - 2.451(4.8) = 77.919\n\n\n5\n5.6\n77\n89.684 - 2.451(5.6) = 75.968\n\n\n6\n6.3\n74\n89.684 - 2.451(6.3) = 74.253\n\n\n7\n7.0\n68\n89.684 - 2.451(7.0) = 72.527\n\n\n8\n7.9\n72\n89.684 - 2.451(7.9) = 70.321\n\n\n\nStep 2: Calculate sum of squares\nSST (Total variation): SST = \\sum(Y_i - \\bar{Y})^2\nFrom our earlier variance calculation: SST = (n-1) \\times s^2_Y = 7 \\times 30.5714 = 214\nSSE (Unexplained variation): SSE = \\sum(Y_i - \\hat{Y}_i)^2\n\n\n\n\n\n\n\n\n\n\ni\nY_i\n\\hat{Y}_i\nY_i - \\hat{Y}_i\n(Y_i - \\hat{Y}_i)^2\n\n\n\n\n1\n80\n83.556\n-3.556\n12.645\n\n\n2\n85\n81.841\n3.159\n9.979\n\n\n3\n78\n79.635\n-1.635\n2.673\n\n\n4\n82\n77.919\n4.081\n16.655\n\n\n5\n77\n75.968\n1.032\n1.065\n\n\n6\n74\n74.253\n-0.253\n0.064\n\n\n7\n68\n72.527\n-4.527\n20.494\n\n\n8\n72\n70.321\n1.679\n2.819\n\n\nSum\n\n\n\n66.394\n\n\n\nSSR (Explained variation): SSR = SST - SSE = 214 - 66.394 = 147.606\n\n# Verify calculations\nanova_table &lt;- anova(model)\ncat(\"SSR (Regression):\", anova_table$`Sum Sq`[1], \"\\n\")\n\nSSR (Regression): 147.5887 \n\ncat(\"SSE (Residual):\", anova_table$`Sum Sq`[2], \"\\n\")\n\nSSE (Residual): 66.41132 \n\ncat(\"SST (Total):\", sum(anova_table$`Sum Sq`), \"\\n\")\n\nSST (Total): 214 \n\n\n\n\nR-squared (Coefficient of Determination)\nR-squared tells us what proportion of the total variation in Y is explained by our model:\nR^2 = \\frac{SSR}{SST} = \\frac{147.606}{214} = 0.690\nThis means our model explains 69.0% of the variation in exam performance.\nAlternative formula using correlation: R^2 = r^2 = (-0.831)^2 = 0.691\n\n# Verify R-squared\ncat(\"R-squared:\", summary(model)$r.squared, \"\\n\")\n\nR-squared: 0.6896667 \n\ncat(\"Correlation squared:\", cor(anxiety, performance)^2, \"\\n\")\n\nCorrelation squared: 0.6896667",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-key-insight",
    "href": "correg_en.html#the-key-insight",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.54 The Key Insight",
    "text": "9.54 The Key Insight\nThe beautiful thing is that these three always relate as: Total Variation = Explained Variation + Unexplained Variation\nIt‚Äôs like having a pie chart of ‚Äúwhy salaries differ‚Äù: - One slice is ‚Äúdifferences explained by experience‚Äù (SSR) - The other slice is ‚Äúdifferences due to other stuff‚Äù (SSE) - Together they make the whole pie (SST)",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#why-this-matters",
    "href": "correg_en.html#why-this-matters",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.55 Why This Matters",
    "text": "9.55 Why This Matters\nThis decomposition lets us calculate R-squared (R¬≤), which is simply: R^2 = \\frac{SSR}{SST} = \\frac{\\text{Explained Variation}}{\\text{Total Variation}}\nIf R¬≤ = 0.70, it means our model explains 70% of why the Y values differ from each other. The remaining 30% is due to factors we haven‚Äôt captured or random noise.\nThink of it like solving a mystery: SST is the total mystery to solve, SSR is how much of the mystery you‚Äôve solved, and SSE is what remains unsolved!",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#effect-size",
    "href": "correg_en.html#effect-size",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.56 Effect Size",
    "text": "9.56 Effect Size\nThe slope coefficient \\hat{\\beta_1} = -2.451 is our effect size. It tells us:\n\nMagnitude: Each 1-point increase in anxiety is associated with a 2.45% decrease in performance\nPractical significance: A student moving from low anxiety (3) to high anxiety (7) would see an expected performance decrease of 2.451 \\times 4 = 9.80\\%\n\nStandardized effect size (correlation coefficient): The correlation r = -0.831 indicates a strong negative relationship.\nCohen‚Äôs guidelines for interpreting correlation:\n\nSmall effect: |r| = 0.10\nMedium effect: |r| = 0.30\nLarge effect: |r| = 0.50\n\nOur |r| = 0.831 represents a large effect size.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#confidence-intervals-and-statistical-significance",
    "href": "correg_en.html#confidence-intervals-and-statistical-significance",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.57 Confidence Intervals and Statistical Significance",
    "text": "9.57 Confidence Intervals and Statistical Significance\n\nStandard Error of the Regression\nFirst, we need the standard error of the residuals:\ns_e = \\sqrt{\\frac{SSE}{n-2}} = \\sqrt{\\frac{66.394}{6}} = \\sqrt{11.066} = 3.327\n\n\nStandard Error of the Slope\nThe standard error of \\hat{\\beta_1} is:\nSE(\\hat{\\beta_1}) = \\frac{s_e}{\\sqrt{\\sum(X_i - \\bar{X})^2}} = \\frac{3.327}{\\sqrt{24.555}} = \\frac{3.327}{4.955} = 0.671\n\n\n95% Confidence Interval for the Slope\nIn plain English: A confidence interval gives us a range of plausible values for our true slope. If we repeated this study many times, 95% of the intervals we calculate would contain the true slope value.\nThe formula uses a critical value (approximately 2.45 for 6 degrees of freedom):\nCI = \\hat{\\beta_1} \\pm (critical\\_value \\times SE(\\hat{\\beta_1})) CI = -2.451 \\pm (2.45 \\times 0.671) CI = -2.451 \\pm 1.644 CI = [-4.095, -0.807]\nInterpretation: We are 95% confident that the true change in performance per unit change in anxiety is between -4.10% and -0.81%.\n\n\nStatistical Significance\nTo test if the relationship is statistically significant (i.e., not due to chance), we calculate a t-statistic:\nt = \\frac{\\hat{\\beta_1}}{SE(\\hat{\\beta_1})} = \\frac{-2.451}{0.671} = -3.653\nIn plain English: This t-value tells us how many standard errors our slope is away from zero. An absolute value of 3.65 is quite large (typically, values beyond ¬±2.45 are considered significant for our sample size), providing strong evidence of a real negative relationship between anxiety and performance.\n\n# Verify calculations with R\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.526 -2.116  0.400  2.050  4.081 \n\nCoefficients:\n            Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)  89.6872     3.6682  24.450 0.000000308 ***\nanxiety      -2.4516     0.6714  -3.652      0.0107 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.327 on 6 degrees of freedom\nMultiple R-squared:  0.6897,    Adjusted R-squared:  0.6379 \nF-statistic: 13.33 on 1 and 6 DF,  p-value: 0.01069\n\nconfint(model)\n\n                2.5 %     97.5 %\n(Intercept) 80.711582 98.6628838\nanxiety     -4.094474 -0.8088048",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#summary-and-interpretation",
    "href": "correg_en.html#summary-and-interpretation",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.58 Summary and Interpretation",
    "text": "9.58 Summary and Interpretation\n\nWhat We‚Äôve Learned\n\nThere is a negative relationship between test anxiety and exam performance (r = -0.831)\nThe relationship is moderately strong - anxiety explains 69.0% of the variation in performance\nThe effect is substantial - each 1-point increase in anxiety is associated with about a 2.5% decrease in performance\nThe relationship is statistically significant - very unlikely to be due to chance\n\n\n\nPractical Implications\nOur analysis suggests that high test anxiety impairs performance, possibly through:\n\nCognitive interference (worrying thoughts compete for working memory)\nPhysical symptoms (sweating, rapid heartbeat) that distract from the task\nNegative self-talk reducing confidence and motivation\nTest-taking behaviors (rushing, second-guessing answers)\n\n\n\nRecommendations Based on Findings\nGiven the strong negative relationship, interventions might include:\n\nTeaching anxiety management techniques (deep breathing, progressive muscle relaxation)\nCognitive restructuring to address catastrophic thinking\nStudy skills training to increase preparation confidence\nPractice tests to reduce fear of the unknown\n\n\n\nLimitations of Our Analysis\n\nSmall sample size (n = 8) limits generalizability\nLinear assumption - the relationship might be curved (optimal anxiety might exist)\nOther variables not considered (preparation time, ability, sleep, etc.)\nCausation vs.¬†correlation - we cannot prove anxiety causes poor performance\nSelf-reported anxiety - subjective measures may not reflect physiological arousal\n\n\n\nComplete R Code\n\n# Complete analysis code\nanxiety &lt;- c(2.5, 3.2, 4.1, 4.8, 5.6, 6.3, 7.0, 7.9)\nperformance &lt;- c(80, 85, 78, 82, 77, 74, 68, 72)\n\n# Descriptive statistics\nmean(anxiety); sd(anxiety)\nmean(performance); sd(performance)\n\n# Correlation\ncor(anxiety, performance)\n\n# Regression\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\nconfint(model)\n\n# Visualization\nlibrary(ggplot2)\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_minimal()\n\n# Diagnostics\nplot(model)\n\n# ANOVA table\nanova(model)",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#v.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "href": "correg_en.html#v.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.59 V. District Magnitude and Electoral Disproportionality: A Comparative Analysis",
    "text": "9.59 V. District Magnitude and Electoral Disproportionality: A Comparative Analysis\n\nData Generating Process\nLet‚Äôs set up a DGP where:\n\\begin{aligned}\n& Y_{\\text{Gallagher}} = 12 - 0.8X_{\\text{DM}} + \\varepsilon \\\\\n& \\varepsilon \\sim \\mathcal{N}(0, 1) \\\\\n& X_{\\text{DM}} \\in \\{3, 5, 7, 10, 12, 15\\}\n\\end{aligned}\n\n# DGP\nmagnitude &lt;- c(3, 5, 7, 10, 12, 15)\nepsilon &lt;- rnorm(6, mean = 0, sd = 1)\ngallagher &lt;- 12 - 0.8 * magnitude + epsilon\n\n# Round (sampled from the DGP) Gallagher indices to one decimal place\ngallagher &lt;- round(c(9.0, 7.8, 9.2, 4.1, 2.5, 1.7), 1)\n\n# Show data\ndata.frame(\n  District_Magnitude = magnitude,\n  Gallagher_Index = gallagher\n)\n\n  District_Magnitude Gallagher_Index\n1                  3             9.0\n2                  5             7.8\n3                  7             9.2\n4                 10             4.1\n5                 12             2.5\n6                 15             1.7\n\n# Initial model check\nmodel &lt;- lm(gallagher ~ magnitude)\nsummary(model)\n\n\nCall:\nlm(formula = gallagher ~ magnitude)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.6516 -0.4628  2.3260 -0.6908 -0.9020  0.3813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7349     1.3034   9.003 0.000843 ***\nmagnitude    -0.6944     0.1359  -5.110 0.006934 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.368 on 4 degrees of freedom\nMultiple R-squared:  0.8672,    Adjusted R-squared:  0.834 \nF-statistic: 26.11 on 1 and 4 DF,  p-value: 0.006934\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{3 + 5 + 7 + 10 + 12 + 15}{6} = \\frac{52}{6} = 8.6667\n\\bar{Y} = \\frac{9.0 + 7.8 + 9.2 + 4.1 + 2.5 + 1.7}{6} = \\frac{34.3}{6} = 5.7167\n\n# Verification\nmean(magnitude)\n\n[1] 8.666667\n\nmean(gallagher)\n\n[1] 5.716667\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(3 - 8.6667) = -5.6667\n(5 - 8.6667) = -3.6667\n(7 - 8.6667) = -1.6667\n(10 - 8.6667) = 1.3333\n(12 - 8.6667) = 3.3333\n(15 - 8.6667) = 6.3333\n\nSquared deviations:\n\n32.1115 + 13.4445 + 2.7779 + 1.7777 + 11.1109 + 40.1107 = 101.3332\n\ns^2_X = \\frac{101.3332}{5} = 20.2666\nFor Y: Deviations: 3.2833, 2.0833, 3.4833, -1.6167, -3.2167, -4.0167\ns^2_Y = \\frac{56.3483}{5} = 11.2697\n\n# Verification\nvar(magnitude)\n\n[1] 20.26667\n\nvar(gallagher)\n\n[1] 11.26967\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n\n(-5.6667 √ó 3.2833) = -18.6057\n(-3.6667 √ó 2.0833) = -7.6387\n(-1.6667 √ó 3.4833) = -5.8056\n(1.3333 √ó -1.6167) = -2.1556\n(3.3333 √ó -3.2167) = -10.7223\n(6.3333 √ó -4.0167) = -25.4391\n\nSum = -70.3670\ns_{XY} = \\frac{-70.3670}{5} = -14.0734\n\n# Verification\ncov(magnitude, gallagher)\n\n[1] -14.07333\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-14.0734}{\\sqrt{20.2666}\\sqrt{11.2697}} = -0.9279\n\n# Verification\ncor(magnitude, gallagher)\n\n[1] -0.9312157\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-14.0734}{20.2666} = -0.6944\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-0.6944 √ó 8.6667 = -6.0181\n\\hat{\\beta_0} = 5.7167 - (-6.0181) = 11.7348\n\n\n# Verification\ncoef(model)\n\n(Intercept)   magnitude \n 11.7348684  -0.6944079 \n\n\n\n\nR-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = 11.7348 - 0.6944X\n\n# Predicted values\ny_hat &lt;- 11.7348 - 0.6944 * magnitude\ndata.frame(\n  Magnitude = magnitude,\n  Gallagher = gallagher,\n  Predicted = y_hat,\n  row.names = 1:6\n)\n\n  Magnitude Gallagher Predicted\n1         3       9.0    9.6516\n2         5       7.8    8.2628\n3         7       9.2    6.8740\n4        10       4.1    4.7908\n5        12       2.5    3.4020\n6        15       1.7    1.3188\n\n\nStep 2: Sum of Squares SST = \\sum(Y_i - \\bar{Y})^2 = 56.3483 SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 48.5271 SSE = \\sum(Y_i - \\hat{Y}_i)^2 = 7.8212\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{48.5271}{56.3483} = 0.8612\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.8671626\n\n\n\n\nVisualization - True vs.¬†Estimated Parameters\n\nTrue DGP: Y = 12 - 0.8X + Œµ\nEstimated Model: Y = 11.7348 - 0.6944X\n\n\nlibrary(ggplot2)\n\n# Create data frame with original data\ndf &lt;- data.frame(\n  magnitude = magnitude,\n  gallagher = gallagher\n)\n\n# Create sequence for smooth lines\nx_seq &lt;- seq(min(magnitude), max(magnitude), length.out = 100)\n\n# Calculate predicted values for both lines\ntrue_dgp &lt;- 12 - 0.8 * x_seq\nestimated &lt;- 11.7348 - 0.6944 * x_seq\n\n# Combine into a data frame for plotting\nlines_df &lt;- data.frame(\n  magnitude = rep(x_seq, 2),\n  value = c(true_dgp, estimated),\n  Model = rep(c(\"True DGP\", \"Estimated\"), each = length(x_seq))\n)\n\n# Create plot\nggplot() +\n  geom_line(data = lines_df, \n            aes(x = magnitude, y = value, color = Model, linetype = Model),\n            size = 1) +\n  geom_point(data = df, \n             aes(x = magnitude, y = gallagher),\n             color = \"black\", \n             size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  labs(\n    title = \"True DGP vs. Estimated Regression Line\",\n    subtitle = \"Black points show observed data with random noise\",\n    x = \"District Magnitude\",\n    y = \"Gallagher Index\",\n    caption = \"True DGP: Y = 12 - 0.8X + Œµ\\nEstimated: Y = 11.73 - 0.69X\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.caption = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\n\nObservations about Model Fit\n\nSlope Comparison\n\nTrue slope: -0.8\nEstimated slope: -0.69\nThe estimated slope is reasonably close to the true parameter\n\nIntercept Comparison\n\nTrue intercept: 12\nEstimated intercept: 11.73\nThe estimated intercept very closely approximates the true value\n\nVisual Patterns\n\nThe lines are nearly parallel, showing good slope recovery\nPoints scatter around both lines due to the random error term (Œµ)\nThe small sample size (n=6) leads to some imprecision in estimation\nThe estimated line (blue) provides a good approximation of the true DGP (red dashed)\n\nImpact of Random Error\n\nThe scatter of points around the true DGP line reflects the N(0,1) error term\nThis noise leads to the slight differences in estimated parameters\nWith a larger sample, we would expect even closer convergence to true parameters\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.93) between district magnitude and electoral disproportionality\nFor each unit increase in district magnitude, the Gallagher index decreases by approximately 0.69 points\nThe model explains 86.12% of the variance in disproportionality\nThe relationship appears strongly linear with moderate scatter\nThe intercept (11.73) represents the expected disproportionality in a hypothetical single-member district system\n\n\n\nStudy Context\n\nData represents simulated observations from a DGP with moderate noise\nSample shows how increasing district magnitude tends to reduce disproportionality\nRandom component reflects other institutional and political factors affecting disproportionality\n\n\n\nLimitations\n\nSmall sample size (n=6)\nSimulated rather than real-world data\nAssumes linear relationship\nDoes not account for other institutional features",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-d-ols-regression-in-matrix-form---manual-calculations",
    "href": "correg_en.html#appendix-d-ols-regression-in-matrix-form---manual-calculations",
    "title": "9¬† Introduction to Correlation and Regression Analysis",
    "section": "9.60 Appendix D: OLS Regression in Matrix Form - Manual Calculations (*)",
    "text": "9.60 Appendix D: OLS Regression in Matrix Form - Manual Calculations (*)\n\nA.1 Essential Linear Algebra Concepts\nBefore diving into OLS regression, we need to understand some fundamental linear algebra concepts.\n\nA.1.1 Matrix Transpose\nThe transpose of a matrix \\mathbf{A}, denoted \\mathbf{A}^T, is obtained by swapping rows and columns:\n\n\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\implies \\mathbf{A}^T = \\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 4 & 6 \\end{bmatrix}\n\nProperties:\n\n(A^T)^T = A\n(AB)^T = B^T A^T\n(A + B)^T = A^T + B^T\n\n\n\nA.1.2 Matrix Multiplication\nFor matrices \\mathbf{A} (of size m \\times n) and \\mathbf{B} (of size n \\times p), the product \\mathbf{AB} is an m \\times p matrix where element (i,j) is:\n\n(\\mathbf{AB})_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}\n\nExample:\n\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 1(5)+2(7) & 1(6)+2(8) \\\\ 3(5)+4(7) & 3(6)+4(8) \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n\n\n\nA.1.3 Identity Matrix\nThe identity matrix \\mathbf{I}_n is an n \\times n matrix with 1‚Äôs on the diagonal and 0‚Äôs elsewhere:\n\n\\mathbf{I}_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\nProperty: \\mathbf{AI} = \\mathbf{IA} = \\mathbf{A}\n\n\nA.1.4 Determinant\nThe determinant is a scalar value that characterizes certain properties of a square matrix. It indicates whether a matrix is invertible.\nFor a 2 \\times 2 matrix:\n\n\\det\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = ad - bc\n\nExample:\n\n\\det\\begin{bmatrix} 3 & 8 \\\\ 4 & 6 \\end{bmatrix} = 3(6) - 8(4) = 18 - 32 = -14\n\nFor a 3 \\times 3 matrix (cofactor expansion along first row):\n\n\\det\\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} = a\\begin{vmatrix} e & f \\\\ h & i \\end{vmatrix} - b\\begin{vmatrix} d & f \\\\ g & i \\end{vmatrix} + c\\begin{vmatrix} d & e \\\\ g & h \\end{vmatrix}\n\nProperties:\n\nIf \\det(\\mathbf{A}) = 0, the matrix is singular (not invertible)\nIf \\det(\\mathbf{A}) \\neq 0, the matrix is non-singular (invertible)\n\\det(\\mathbf{AB}) = \\det(\\mathbf{A})\\det(\\mathbf{B})\n\\det(\\mathbf{A}^T) = \\det(\\mathbf{A})\n\n\n\nA.1.5 Matrix Inverse\nThe inverse of matrix \\mathbf{A}, denoted \\mathbf{A}^{-1}, satisfies:\n\n\\mathbf{AA}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\n\nFor a 2 \\times 2 matrix:\n\n\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad-bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\nFor larger matrices, the inverse can be computed as:\n\n\\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})}\\text{adj}(\\mathbf{A})\n\nwhere \\text{adj}(\\mathbf{A}) is the adjugate (transpose of the cofactor matrix).\nProperties:\n\n(A^{-1})^{-1} = A\n(AB)^{-1} = B^{-1}A^{-1}\n(A^T)^{-1} = (A^{-1})^T\n\n\n\nA.1.6 Matrix Rank\nThe rank of a matrix is the maximum number of linearly independent rows (or columns). A matrix \\mathbf{X} of size n \\times p has:\n\nFull column rank if \\text{rank}(\\mathbf{X}) = p (all columns are independent)\nRank deficiency if \\text{rank}(\\mathbf{X}) &lt; p (multicollinearity exists)\n\nFull column rank is required for \\mathbf{X}^T\\mathbf{X} to be invertible in regression.\n\n\nA.1.7 Quadratic Forms\nA quadratic form is an expression of the type:\n\n\\mathbf{x}^T\\mathbf{A}\\mathbf{x} = \\sum_{i=1}^n\\sum_{j=1}^n a_{ij}x_ix_j\n\nFor a vector \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} and matrix \\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}:\n\n\\mathbf{x}^T\\mathbf{A}\\mathbf{x} = a_{11}x_1^2 + (a_{12}+a_{21})x_1x_2 + a_{22}x_2^2\n\nThis is crucial for understanding the OLS objective function.\n\n\n\nA.2 OLS Minimization Problem and Derivation\n\nA.2.1 The Objective Function\nIn matrix notation, the linear regression model is:\n\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\nwhere:\n\n\\mathbf{y} is an n \\times 1 vector of observed responses\n\\mathbf{X} is an n \\times (p+1) design matrix of predictors (including intercept)\n\\boldsymbol{\\beta} is a (p+1) \\times 1 vector of regression coefficients\n\\boldsymbol{\\epsilon} is an n \\times 1 vector of errors\n\nThe sum of squared residuals (SSR) is:\n\nS(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2\n\nIn matrix form, this becomes:\n\nS(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n\n\n\nA.2.2 Expanding the Objective Function\nLet‚Äôs expand the expression:\n\nS(\\boldsymbol{\\beta}) = (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n\n\n= \\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\nNote that \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} is a scalar, so it equals its transpose:\n\n\\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} = (\\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta})^T = \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y}\n\nTherefore:\n\nS(\\boldsymbol{\\beta}) = \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\n\n\nA.2.3 Taking the Derivative\nTo minimize S(\\boldsymbol{\\beta}), we take the derivative with respect to \\boldsymbol{\\beta} and set it equal to zero.\nMatrix calculus rules needed:\n\n\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}(\\mathbf{a}^T\\boldsymbol{\\beta}) = \\mathbf{a}\n\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}(\\boldsymbol{\\beta}^T\\mathbf{A}\\boldsymbol{\\beta}) = 2\\mathbf{A}\\boldsymbol{\\beta} (when \\mathbf{A} is symmetric)\n\nApplying these rules:\n\n\\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\n\n\nA.2.4 Setting the Derivative to Zero (Normal Equations)\nSet \\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = \\mathbf{0}:\n\n-2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0}\n\nSimplify:\n\n\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}\n\nThese are called the normal equations.\n\n\nA.2.5 Solving for \\hat{\\boldsymbol{\\beta}}\nAssuming \\mathbf{X}^T\\mathbf{X} is invertible (full rank assumption), multiply both sides by (\\mathbf{X}^T\\mathbf{X})^{-1}:\n\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n\nThis is the OLS estimator.\n\n\nA.2.6 Verifying This is a Minimum\nTo confirm this is a minimum (not maximum or saddle point), we check the second derivative:\n\n\\frac{\\partial^2 S}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T} = 2\\mathbf{X}^T\\mathbf{X}\n\nThis is the Hessian matrix. Since \\mathbf{X}^T\\mathbf{X} is positive definite (when \\mathbf{X} has full column rank), the Hessian is positive definite, confirming we have a minimum.\n\n\n\nA.3 Case 1: Simple Linear Regression (One Predictor)\n\nA.3.1 Setting Up the Problem\nConsider the following dataset with n = 5 observations:\n\n\n\nObservation\nx\ny\n\n\n\n\n1\n1\n2\n\n\n2\n2\n4\n\n\n3\n3\n5\n\n\n4\n4\n4\n\n\n5\n5\n5\n\n\n\nOur model is: y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\n\nA.3.2 Constructing the Design Matrix\nThe design matrix \\mathbf{X} includes a column of ones for the intercept:\n\n\\mathbf{X} = \\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 4 \\\\\n1 & 5\n\\end{bmatrix}, \\quad\n\\mathbf{y} = \\begin{bmatrix}\n2 \\\\\n4 \\\\\n5 \\\\\n4 \\\\\n5\n\\end{bmatrix}\n\n\n\nA.3.3 Step 1: Calculate \\mathbf{X}^T\\mathbf{X}\n\n\\mathbf{X}^T = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5\n\\end{bmatrix}\n\n\n\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 4 \\\\\n1 & 5\n\\end{bmatrix}\n\nElement-by-element calculation:\n\n(1,1) element: 1(1) + 1(1) + 1(1) + 1(1) + 1(1) = 5\n(1,2) element: 1(1) + 1(2) + 1(3) + 1(4) + 1(5) = 15\n(2,1) element: 1(1) + 2(1) + 3(1) + 4(1) + 5(1) = 15\n(2,2) element: 1(1) + 2(2) + 3(3) + 4(4) + 5(5) = 1 + 4 + 9 + 16 + 25 = 55\n\n\n\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n5 & 15 \\\\\n15 & 55\n\\end{bmatrix}\n\n\n\nA.3.4 Step 2: Calculate (\\mathbf{X}^T\\mathbf{X})^{-1}\nFor a 2 \\times 2 matrix \\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}, the inverse is:\n\n\\mathbf{A}^{-1} = \\frac{1}{ad - bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\nCalculate the determinant:\n\n\\det(\\mathbf{X}^T\\mathbf{X}) = 5(55) - 15(15) = 275 - 225 = 50\n\nCalculate the inverse:\n\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\frac{1}{50}\\begin{bmatrix}\n55 & -15 \\\\\n-15 & 5\n\\end{bmatrix} = \\begin{bmatrix}\n1.1 & -0.3 \\\\\n-0.3 & 0.1\n\\end{bmatrix}\n\n\n\nA.3.5 Step 3: Calculate \\mathbf{X}^T\\mathbf{y}\n\n\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\n2 \\\\\n4 \\\\\n5 \\\\\n4 \\\\\n5\n\\end{bmatrix}\n\nElement-by-element calculation:\n\nFirst element: 1(2) + 1(4) + 1(5) + 1(4) + 1(5) = 20\nSecond element: 1(2) + 2(4) + 3(5) + 4(4) + 5(5) = 2 + 8 + 15 + 16 + 25 = 66\n\n\n\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n20 \\\\\n66\n\\end{bmatrix}\n\n\n\nA.3.6 Step 4: Calculate \\hat{\\boldsymbol{\\beta}}\n\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n1.1 & -0.3 \\\\\n-0.3 & 0.1\n\\end{bmatrix}\n\\begin{bmatrix}\n20 \\\\\n66\n\\end{bmatrix}\n\nElement-by-element calculation:\n\n\\hat{\\beta}_0: 1.1(20) + (-0.3)(66) = 22 - 19.8 = 2.2\n\\hat{\\beta}_1: (-0.3)(20) + 0.1(66) = -6 + 6.6 = 0.6\n\n\n\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix}\n2.2 \\\\\n0.6\n\\end{bmatrix}\n\nResult: The fitted regression line is \\hat{y} = 2.2 + 0.6x\n\n\n\nA.4 Case 2: Multiple Linear Regression (Two Predictors)\n\nA.4.1 Setting Up the Problem\nConsider a dataset with n = 5 observations and two predictors:\n\n\n\nObservation\nx_1\nx_2\ny\n\n\n\n\n1\n1\n3\n3\n\n\n2\n2\n2\n5\n\n\n3\n3\n5\n7\n\n\n4\n4\n4\n9\n\n\n5\n5\n7\n10\n\n\n\nOur model is: y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i\n\n\nA.4.2 Constructing the Design Matrix\n\n\\mathbf{X} = \\begin{bmatrix}\n1 & 1 & 3 \\\\\n1 & 2 & 2 \\\\\n1 & 3 & 5 \\\\\n1 & 4 & 4 \\\\\n1 & 5 & 7\n\\end{bmatrix}, \\quad\n\\mathbf{y} = \\begin{bmatrix}\n3 \\\\\n5 \\\\\n7 \\\\\n9 \\\\\n10\n\\end{bmatrix}\n\n\n\nA.4.3 Step 1: Calculate \\mathbf{X}^T\\mathbf{X}\n\n\\mathbf{X}^T = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5 \\\\\n3 & 2 & 5 & 4 & 7\n\\end{bmatrix}\n\n\n\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5 \\\\\n3 & 2 & 5 & 4 & 7\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 1 & 3 \\\\\n1 & 2 & 2 \\\\\n1 & 3 & 5 \\\\\n1 & 4 & 4 \\\\\n1 & 5 & 7\n\\end{bmatrix}\n\nElement-by-element calculation:\nRow 1:\n\n(1,1): 1 + 1 + 1 + 1 + 1 = 5\n(1,2): 1 + 2 + 3 + 4 + 5 = 15\n(1,3): 3 + 2 + 5 + 4 + 7 = 21\n\nRow 2:\n\n(2,1): 1 + 2 + 3 + 4 + 5 = 15\n(2,2): 1 + 4 + 9 + 16 + 25 = 55\n(2,3): 3 + 4 + 15 + 16 + 35 = 73\n\nRow 3:\n\n(3,1): 3 + 2 + 5 + 4 + 7 = 21\n(3,2): 3 + 4 + 15 + 16 + 35 = 73\n(3,3): 9 + 4 + 25 + 16 + 49 = 103\n\n\n\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n5 & 15 & 21 \\\\\n15 & 55 & 73 \\\\\n21 & 73 & 103\n\\end{bmatrix}\n\n\n\nA.4.4 Step 2: Calculate (\\mathbf{X}^T\\mathbf{X})^{-1}\nFor a 3 \\times 3 matrix, we use: \\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})}\\text{adj}(\\mathbf{A})\nStep 2a: Calculate the determinant using cofactor expansion along the first row:\n\n\\det(\\mathbf{X}^T\\mathbf{X}) = 5\\begin{vmatrix}55 & 73 \\\\ 73 & 103\\end{vmatrix} - 15\\begin{vmatrix}15 & 73 \\\\ 21 & 103\\end{vmatrix} + 21\\begin{vmatrix}15 & 55 \\\\ 21 & 73\\end{vmatrix}\n\nCalculate each 2 \\times 2 determinant:\n\n\\begin{vmatrix}55 & 73 \\\\ 73 & 103\\end{vmatrix} = 55(103) - 73(73) = 5665 - 5329 = 336\n\\begin{vmatrix}15 & 73 \\\\ 21 & 103\\end{vmatrix} = 15(103) - 73(21) = 1545 - 1533 = 12\n\\begin{vmatrix}15 & 55 \\\\ 21 & 73\\end{vmatrix} = 15(73) - 55(21) = 1095 - 1155 = -60\n\n\n\\det(\\mathbf{X}^T\\mathbf{X}) = 5(336) - 15(12) + 21(-60) = 1680 - 180 - 1260 = 240\n\nStep 2b: Calculate the cofactor matrix\nThe cofactor C_{ij} is (-1)^{i+j} times the determinant of the matrix obtained by deleting row i and column j.\nC_{11} = (+1)\\begin{vmatrix}55 & 73 \\\\ 73 & 103\\end{vmatrix} = 336\nC_{12} = (-1)\\begin{vmatrix}15 & 73 \\\\ 21 & 103\\end{vmatrix} = -12\nC_{13} = (+1)\\begin{vmatrix}15 & 55 \\\\ 21 & 73\\end{vmatrix} = -60\nC_{21} = (-1)\\begin{vmatrix}15 & 21 \\\\ 73 & 103\\end{vmatrix} = -(15 \\times 103 - 21 \\times 73) = -(1545 - 1533) = -12\nC_{22} = (+1)\\begin{vmatrix}5 & 21 \\\\ 21 & 103\\end{vmatrix} = 5(103) - 21(21) = 515 - 441 = 74\nC_{23} = (-1)\\begin{vmatrix}5 & 15 \\\\ 21 & 73\\end{vmatrix} = -(5 \\times 73 - 15 \\times 21) = -(365 - 315) = -50\nC_{31} = (+1)\\begin{vmatrix}15 & 21 \\\\ 55 & 73\\end{vmatrix} = 15(73) - 21(55) = 1095 - 1155 = -60\nC_{32} = (-1)\\begin{vmatrix}5 & 21 \\\\ 15 & 73\\end{vmatrix} = -(5 \\times 73 - 21 \\times 15) = -(365 - 315) = -50\nC_{33} = (+1)\\begin{vmatrix}5 & 15 \\\\ 15 & 55\\end{vmatrix} = 5(55) - 15(15) = 275 - 225 = 50\nCofactor matrix:\n\n\\mathbf{C} = \\begin{bmatrix}\n336 & -12 & -60 \\\\\n-12 & 74 & -50 \\\\\n-60 & -50 & 50\n\\end{bmatrix}\n\nStep 2c: Calculate the adjugate (transpose of cofactor matrix):\n\n\\text{adj}(\\mathbf{X}^T\\mathbf{X}) = \\mathbf{C}^T = \\begin{bmatrix}\n336 & -12 & -60 \\\\\n-12 & 74 & -50 \\\\\n-60 & -50 & 50\n\\end{bmatrix}\n\nStep 2d: Calculate the inverse:\n\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\frac{1}{240}\\begin{bmatrix}\n336 & -12 & -60 \\\\\n-12 & 74 & -50 \\\\\n-60 & -50 & 50\n\\end{bmatrix} = \\begin{bmatrix}\n1.4 & -0.05 & -0.25 \\\\\n-0.05 & 0.308\\overline{3} & -0.208\\overline{3} \\\\\n-0.25 & -0.208\\overline{3} & 0.208\\overline{3}\n\\end{bmatrix}\n\nFor simplicity in calculations, we‚Äôll use:\n\n(\\mathbf{X}^T\\mathbf{X})^{-1} \\approx \\begin{bmatrix}\n1.400 & -0.050 & -0.250 \\\\\n-0.050 & 0.308 & -0.208 \\\\\n-0.250 & -0.208 & 0.208\n\\end{bmatrix}\n\n\n\nA.4.5 Step 3: Calculate \\mathbf{X}^T\\mathbf{y}\n\n\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5 \\\\\n3 & 2 & 5 & 4 & 7\n\\end{bmatrix}\n\\begin{bmatrix}\n3 \\\\\n5 \\\\\n7 \\\\\n9 \\\\\n10\n\\end{bmatrix}\n\nElement-by-element calculation:\n\nFirst: 3 + 5 + 7 + 9 + 10 = 34\nSecond: 3 + 10 + 21 + 36 + 50 = 120\nThird: 9 + 10 + 35 + 36 + 70 = 160\n\n\n\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}\n34 \\\\\n120 \\\\\n160\n\\end{bmatrix}\n\n\n\nA.4.6 Step 4: Calculate \\hat{\\boldsymbol{\\beta}}\n\n\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix}\n1.400 & -0.050 & -0.250 \\\\\n-0.050 & 0.308 & -0.208 \\\\\n-0.250 & -0.208 & 0.208\n\\end{bmatrix}\n\\begin{bmatrix}\n34 \\\\\n120 \\\\\n160\n\\end{bmatrix}\n\nElement-by-element calculation:\n\n\\hat{\\beta}_0 = 1.400(34) - 0.050(120) - 0.250(160) = 47.6 - 6.0 - 40.0 = 1.6\n\\hat{\\beta}_1 = -0.050(34) + 0.308(120) - 0.208(160) = -1.7 + 37.0 - 33.3 = 2.0\n\\hat{\\beta}_2 = -0.250(34) - 0.208(120) + 0.208(160) = -8.5 - 25.0 + 33.3 = -0.2\n\n\n\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix}\n1.6 \\\\\n2.0 \\\\\n-0.2\n\\end{bmatrix}\n\nResult: The fitted regression is \\hat{y} = 1.6 + 2.0x_1 - 0.2x_2\n(Note: With exact fractions, the result would be slightly different, but this illustrates the manual calculation process.)\n\n\n\nA.5 The Gauss-Markov Theorem\n\nA.5.1 Statement of the Theorem\nUnder the classical linear regression assumptions, the OLS estimator \\hat{\\boldsymbol{\\beta}} is the Best Linear Unbiased Estimator (BLUE) of \\boldsymbol{\\beta}.\n‚ÄúBest‚Äù means it has the minimum variance among all linear unbiased estimators.\n\n\nA.5.2 Required Assumptions\n\nLinearity: \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\nStrict Exogeneity: E[\\boldsymbol{\\epsilon}|\\mathbf{X}] = \\mathbf{0}\nNo Perfect Multicollinearity: \\mathbf{X} has full column rank, i.e., \\text{rank}(\\mathbf{X}) = p+1\nSpherical Errors: \\text{Var}(\\boldsymbol{\\epsilon}|\\mathbf{X}) = \\sigma^2\\mathbf{I}_n\n\nHomoscedasticity: \\text{Var}(\\epsilon_i) = \\sigma^2 for all i\nNo autocorrelation: \\text{Cov}(\\epsilon_i, \\epsilon_j) = 0 for i \\neq j\n\n\n\n\nA.5.3 Proof that OLS is Unbiased\nStarting with: \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\nSubstitute \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}:\n\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon})\n\n\n= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}\n\n\n= \\mathbf{I}\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}\n\n\n= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}\n\nTaking expectations:\n\nE[\\hat{\\boldsymbol{\\beta}}|\\mathbf{X}] = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T E[\\boldsymbol{\\epsilon}|\\mathbf{X}]\n\nSince E[\\boldsymbol{\\epsilon}|\\mathbf{X}] = \\mathbf{0}:\n\nE[\\hat{\\boldsymbol{\\beta}}|\\mathbf{X}] = \\boldsymbol{\\beta}\n\nTherefore, OLS is unbiased.\n\n\nA.5.4 Variance of the OLS Estimator\nFrom \\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}:\n\n\\text{Var}(\\hat{\\boldsymbol{\\beta}}|\\mathbf{X}) = \\text{Var}[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}|\\mathbf{X}]\n\nSince (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T is non-random conditional on \\mathbf{X}:\n\n= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\text{Var}(\\boldsymbol{\\epsilon}|\\mathbf{X}) \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\nWith spherical errors \\text{Var}(\\boldsymbol{\\epsilon}|\\mathbf{X}) = \\sigma^2\\mathbf{I}:\n\n= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T (\\sigma^2\\mathbf{I}) \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\n\n= \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\n\n= \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\n\nThis is the variance-covariance matrix of \\hat{\\boldsymbol{\\beta}}.\n\n\nA.5.5 Example: Variance Calculation for Simple Regression\nFrom Case 1, we had:\n\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\begin{bmatrix}\n1.1 & -0.3 \\\\\n-0.3 & 0.1\n\\end{bmatrix}\n\nWe estimate \\sigma^2 using the residual sum of squares:\n\n\\hat{\\sigma}^2 = \\frac{\\text{RSS}}{n - (p + 1)} = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n - 2}\n\nFor our example, the fitted values are: \\hat{y}_i = 2.2 + 0.6x_i\n\n\n\n\n\n\n\n\n\n\ni\ny_i\n\\hat{y}_i\ny_i - \\hat{y}_i\n(y_i - \\hat{y}_i)^2\n\n\n\n\n1\n2\n2.8\n-0.8\n0.64\n\n\n2\n4\n3.4\n0.6\n0.36\n\n\n3\n5\n4.0\n1.0\n1.00\n\n\n4\n4\n4.6\n-0.6\n0.36\n\n\n5\n5\n5.2\n-0.2\n0.04\n\n\n\n\n\\text{RSS} = 0.64 + 0.36 + 1.00 + 0.36 + 0.04 = 2.4\n\n\n\\hat{\\sigma}^2 = \\frac{2.4}{5 - 2} = \\frac{2.4}{3} = 0.8\n\nThe variance-covariance matrix of \\hat{\\boldsymbol{\\beta}} is:\n\n\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = 0.8 \\begin{bmatrix}\n1.1 & -0.3 \\\\\n-0.3 & 0.1\n\\end{bmatrix} = \\begin{bmatrix}\n0.88 & -0.24 \\\\\n-0.24 & 0.08\n\\end{bmatrix}\n\nStandard errors:\n\n\\text{SE}(\\hat{\\beta}_0) = \\sqrt{0.88} = 0.938\n\\text{SE}(\\hat{\\beta}_1) = \\sqrt{0.08} = 0.283\n\nThese standard errors are used for hypothesis testing and confidence intervals.\n\n\nA.5.6 Why OLS is ‚ÄúBest‚Äù (Sketch of Proof)\nThe Gauss-Markov theorem states that among all linear unbiased estimators, OLS has the minimum variance.\nConsider any other linear unbiased estimator:\n\n\\tilde{\\boldsymbol{\\beta}} = \\mathbf{C}\\mathbf{y}\n\nwhere \\mathbf{C} is some matrix. For unbiasedness:\n\nE[\\tilde{\\boldsymbol{\\beta}}] = \\mathbf{C}E[\\mathbf{y}] = \\mathbf{C}\\mathbf{X}\\boldsymbol{\\beta} = \\boldsymbol{\\beta}\n\nThis requires \\mathbf{C}\\mathbf{X} = \\mathbf{I}.\nWe can write \\mathbf{C} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D} for some matrix \\mathbf{D} where \\mathbf{D}\\mathbf{X} = \\mathbf{0}.\nThe variance of \\tilde{\\boldsymbol{\\beta}} is:\n\n\\text{Var}(\\tilde{\\boldsymbol{\\beta}}) = \\sigma^2\\mathbf{C}\\mathbf{C}^T\n\n\n= \\sigma^2[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D}][(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D}]^T\n\n\n= \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} + \\sigma^2\\mathbf{D}\\mathbf{D}^T\n\nSince \\mathbf{D}\\mathbf{D}^T is positive semi-definite:\n\n\\text{Var}(\\tilde{\\boldsymbol{\\beta}}) - \\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2\\mathbf{D}\\mathbf{D}^T \\geq \\mathbf{0}\n\nTherefore, OLS has the smallest variance among all linear unbiased estimators.\n\n\n\nA.6 Key Takeaways\n\nMatrix notation provides an elegant and scalable framework for regression that extends naturally from simple to multiple regression.\nThe OLS minimization derives from setting \\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = \\mathbf{0}, yielding the normal equations \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}.\nThe OLS solution \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} requires that \\mathbf{X}^T\\mathbf{X} be invertible (no perfect multicollinearity).\nUnder the Gauss-Markov assumptions, OLS is BLUE: Best (minimum variance), Linear, Unbiased Estimator.\nThe variance of OLS estimates is \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}, which can be used for inference.\nKey linear algebra tools include matrix transpose, multiplication, determinant, inverse, and rank‚Äîall essential for understanding OLS.\nAssumption violations (e.g., heteroscedasticity, autocorrelation) don‚Äôt make OLS biased, but they invalidate the ‚Äúbest‚Äù property and standard inference procedures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_pl.html",
    "href": "correg_pl.html",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "",
    "text": "10.1 Wprowadzenie\nR√≥≈ºnica miƒôdzy korelacjƒÖ (correlation) a przyczynowo≈õciƒÖ/kausalno≈õciƒÖ (causation) to jedno z podstawowych wyzwa≈Ñ w analizie statystycznej. Korelacja mierzy statystyczny zwiƒÖzek miƒôdzy zmiennymi, natomiast przyczynowo≈õƒá oznacza bezpo≈õredni wp≈Çyw jednej zmiennej na drugƒÖ.\nZale≈ºno≈õci statystyczne stanowiƒÖ fundament podejmowania decyzji opartych na danych w wielu dyscyplinach ‚Äî od ekonomii i zdrowia publicznego po psychologiƒô i nauki o ≈õrodowisku. Zrozumienie, kiedy zwiƒÖzek wskazuje jedynie na asocjacjƒô (association), a kiedy na prawdziwƒÖ kausalno≈õƒá (genuine causality), jest kluczowe dla poprawnych wniosk√≥w i skutecznych rekomendacji politycznych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kowariancja-covariance",
    "href": "correg_pl.html#kowariancja-covariance",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.2 Kowariancja (Covariance)",
    "text": "10.2 Kowariancja (Covariance)\nKowariancja (covariance) mierzy, w jaki spos√≥b dwie zmienne wsp√≥≈ÇzmieniajƒÖ siƒô, wskazujƒÖc zar√≥wno kierunek, jak i si≈Çƒô ich liniowego zwiƒÖzku.\nWz√≥r (z pr√≥by):\n\n\\operatorname{cov}(X,Y)\n= \\frac{\\sum_{i=1}^{n} (x_i - \\bar x)(y_i - \\bar y)}{n - 1}.\n\nGdzie:\n\nx_i i y_i to poszczeg√≥lne obserwacje,\n\\bar{x} i \\bar{y} to ≈õrednie odpowiednio zmiennych X i Y,\nn to liczba obserwacji,\ndzielimy przez (n-1), poniewa≈º liczymy kowariancjƒô z pr√≥by (tzw. poprawka Bessela; Bessel‚Äôs correction).\n\n\nObliczenia rƒôczne krok po kroku (Step-by-Step Manual Calculation Process)\nPrzyk≈Çad 1: Godziny nauki a wyniki testu\nDane:\n\nGodziny nauki (X): 2, 4, 6, 8, 10\nWyniki testu (Y): 65, 70, 80, 85, 95\n\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nOblicz ≈õrednie\n\\bar{x}=\\frac{2+4+6+8+10}{5}=6 godz.\n\n\n\n\n\\bar{y}=\\frac{65+70+80+85+95}{5}=79 pkt\n\n\n2\nOdchylenia od ≈õrednich\n(x_i-\\bar{x}): -4, -2, 0, 2, 4\n\n\n\n\n(y_i-\\bar{y}): -14, -9, 1, 6, 16\n\n\n3\nIloczyny odchyle≈Ñ\n(x_i-\\bar{x})(y_i-\\bar{y}): 56, 18, 0, 12, 64\n\n\n4\nSuma iloczyn√≥w\n\\sum = 56+18+0+12+64=150\n\n\n5\nPodziel przez (n-1)\n\\operatorname{cov}(X,Y)=\\frac{150}{5-1}=\\frac{150}{4}=37.5\n\n\n\nWeryfikacja w R (R Verification):\n\n# Definicja danych\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Kowariancja\ncov_manual &lt;- cov(study_hours, test_scores)\nprint(paste(\"Covariance:\", round(cov_manual, 2)))\n\n[1] \"Covariance: 37.5\"\n\n# Weryfikacja krok po kroku\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\nproducts &lt;- x_dev * y_dev\ncov_calculated &lt;- sum(products) / (length(study_hours) - 1)\n\n# Tabela oblicze≈Ñ\ndata.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_deviation = x_dev,\n  Y_deviation = y_dev,\n  Product = products\n)\n\n   X  Y X_deviation Y_deviation Product\n1  2 65          -4         -14      56\n2  4 70          -2          -9      18\n3  6 80           0           1       0\n4  8 85           2           6      12\n5 10 95           4          16      64\n\n\nInterpretacja: Dodatnia kowariancja (37.5) wskazuje, ≈ºe wraz ze wzrostem liczby godzin nauki rosnƒÖ tak≈ºe wyniki testu ‚Äî zmienne majƒÖ tendencjƒô do wsp√≥lnego wzrostu.\n\n\nZadanie ƒáwiczeniowe z rozwiƒÖzaniem (Practice Problem with Solution)\nPolicz rƒôcznie kowariancjƒô dla:\n\nTemperatura (¬∞F): 32, 50, 68, 86, 95\nSprzeda≈º lod√≥w ($): 100, 200, 400, 600, 800\n\nRozwiƒÖzanie:\n\n\n\n\n\n\n\nKrok\nObliczenie\n\n\n\n\n1. ≈örednie\n\\bar{x}=\\frac{32+50+68+86+95}{5}=66.2^{\\circ}\\mathrm{F}\n\n\n\n\\bar{y}=\\frac{100+200+400+600+800}{5}=420\n\n\n2. Odchylenia\nX: -34.2, -16.2, 1.8, 19.8, 28.8\n\n\n\nY: -320, -220, -20, 180, 380\n\n\n3. Iloczyny\n10944, 3564, -36, 3564, 10944\n\n\n4. Suma\n28980\n\n\n5. Kowariancja\n\\frac{28980}{4}=7245\n\n\n\n\n# Weryfikacja zadania\ntemp &lt;- c(32, 50, 68, 86, 95)\nsales &lt;- c(100, 200, 400, 600, 800)\nprint(paste(\"Covariance for practice problem:\", round(cov(temp, sales), 2)))\n\n[1] \"Covariance for practice problem: 7245\"",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#wsp√≥≈Çczynnik-korelacji-correlation-coefficient",
    "href": "correg_pl.html#wsp√≥≈Çczynnik-korelacji-correlation-coefficient",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.3 Wsp√≥≈Çczynnik korelacji (Correlation Coefficient)",
    "text": "10.3 Wsp√≥≈Çczynnik korelacji (Correlation Coefficient)\nWsp√≥≈Çczynnik korelacji (correlation coefficient) standaryzuje kowariancjƒô, usuwajƒÖc zale≈ºno≈õƒá od skali i przyjmujƒÖc warto≈õci od -1 do +1.\n\nWskaz√≥wki interpretacyjne (Interpretation Guidelines)\n\n\n\n\n\n\n\n\n\nWarto≈õƒá korelacji\nSi≈Ça\nInterpretacja\nPrzyk≈Çad\n\n\n\n\n¬±0.90 do ¬±1.00\nBardzo silna\nNiemal doskona≈Çy zwiƒÖzek\nWzrost rodzic√≥w i dzieci\n\n\n¬±0.70 do ¬±0.89\nSilna\nZmienne silnie powiƒÖzane\nCzas nauki i oceny\n\n\n¬±0.50 do ¬±0.69\nUmiarkowana\nUmiarkowany zwiƒÖzek\nƒÜwiczenia a spadek masy\n\n\n¬±0.30 do ¬±0.49\nS≈Çaba\nS≈Çaby zwiƒÖzek\nRozmiar buta a umiejƒôtno≈õƒá czytania\n\n\n¬±0.00 do ¬±0.29\nBardzo s≈Çaba/brak\nZnikomy lub brak zwiƒÖzku\nMiesiƒÖc urodzenia a inteligencja\n\n\n\n\n\nWizualizacja typ√≥w zale≈ºno≈õci korelacyjnych (Types of Correlations Visualization)\n\n# Generowanie przyk≈Çadowych danych dla r√≥≈ºnych wzorc√≥w korelacji\nn &lt;- 100\n\n# Dodatnia zale≈ºno≈õƒá liniowa\nset.seed(123)\nyears_education &lt;- rnorm(n, 14, 3)\nannual_income &lt;- 15000 + 3500 * years_education + rnorm(n, 0, 10000)\npos_cor &lt;- round(cor(years_education, annual_income), 3)\n\n# Ujemna zale≈ºno≈õƒá liniowa\nscreen_time &lt;- runif(n, 1, 8)\nsleep_hours &lt;- 9 - 0.5 * screen_time + rnorm(n, 0, 0.5)\nneg_cor &lt;- round(cor(screen_time, sleep_hours), 3)\n\n# Brak korelacji\nx_random &lt;- rnorm(n, 50, 10)\ny_random &lt;- rnorm(n, 50, 10)\nno_cor &lt;- round(cor(x_random, y_random), 3)\n\n# Nieliniowa zale≈ºno≈õƒá (kwadratowa)\nhours_studied &lt;- runif(n, 0, 12)\ntest_score &lt;- -2 * (hours_studied - 6)^2 + 90 + rnorm(n, 0, 5)\nnonlin_cor &lt;- round(cor(hours_studied, test_score), 3)\n\n# Ramki danych z warto≈õciami korelacji\npositive_data &lt;- data.frame(\n  x = years_education, \n  y = annual_income,\n  label = paste0(\"Dodatnia liniowa (r = \", pos_cor, \")\")\n)\n\nnegative_data &lt;- data.frame(\n  x = screen_time, \n  y = sleep_hours,\n  label = paste0(\"Ujemna liniowa (r = \", neg_cor, \")\")\n)\n\nno_corr_data &lt;- data.frame(\n  x = x_random, \n  y = y_random,\n  label = paste0(\"Brak korelacji (r = \", no_cor, \")\")\n)\n\nnonlinear_data &lt;- data.frame(\n  x = hours_studied, \n  y = test_score,\n  label = paste0(\"Nieliniowa (Pearson r = \", nonlin_cor, \")\")\n)\n\n# Po≈ÇƒÖczenie danych\nall_data &lt;- rbind(positive_data, negative_data, no_corr_data, nonlinear_data)\n\n# Wykres fasetowy\nggplot(all_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"darkblue\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE, alpha = 0.3) +\n  facet_wrap(~ label, scales = \"free\", ncol = 2) +\n  labs(\n    title = \"R√≥≈ºne typy korelacji\",\n    subtitle = \"Linia regresji liniowej (na czerwono) z pasmem ufno≈õci\",\n    x = \"\", \n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, color = \"gray50\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#korelacja-pearsona-pearson-correlation",
    "href": "correg_pl.html#korelacja-pearsona-pearson-correlation",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.4 Korelacja Pearsona (Pearson Correlation)",
    "text": "10.4 Korelacja Pearsona (Pearson Correlation)\nWz√≥r:\n\nr\n= \\frac{\\operatorname{cov}(X,Y)}{s_X\\, s_Y}\n= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}\n{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2}\\,\\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}}.\n\n\nPe≈Çny przyk≈Çad oblicze≈Ñ rƒôcznych (Complete Manual Calculation Example)\nNa danych o godzinach nauki:\n\nGodziny nauki (X): 2, 4, 6, 8, 10\nWyniki testu (Y): 65, 70, 80, 85, 95\n\nSzczeg√≥≈Çowe kroki:\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nKowariancja\nZ powy≈ºej: \\operatorname{cov}(X,Y) = 37.5\n\n\n2\nKwadraty odchyle≈Ñ\n\n\n\n\nDla X\n(x_i-\\bar{x})^2: 16, 4, 0, 4, 16\n\n\n\n\nSuma = 40\n\n\n\nDla Y\n(y_i-\\bar{y})^2: 196, 81, 1, 36, 256\n\n\n\n\nSuma = 570\n\n\n3\nOdchylenia standardowe (standard deviations)\n\n\n\n\ns_X\ns_X=\\sqrt{\\frac{40}{4}}=\\sqrt{10}=3.162\n\n\n\ns_Y\ns_Y=\\sqrt{\\frac{570}{4}}=\\sqrt{142.5}=11.937\n\n\n4\nKorelacja\nr=\\frac{37.5}{3.162 \\times 11.937}\n\n\n\n\nr=\\frac{37.5}{37.73}\\approx 0.994\n\n\n\n\n# Weryfikacja oblicze≈Ñ\nstudy_hours &lt;- c(2, 4, 6, 8, 10)\ntest_scores &lt;- c(65, 70, 80, 85, 95)\n\n# Wsp√≥≈Çczynnik korelacji Pearsona\nr_value &lt;- cor(study_hours, test_scores, method = \"pearson\")\nprint(paste(\"Pearson correlation coefficient:\", round(r_value, 3)))\n\n[1] \"Pearson correlation coefficient: 0.993\"\n\n# Obliczenia szczeg√≥≈Çowe\nx_mean &lt;- mean(study_hours)\ny_mean &lt;- mean(test_scores)\nx_dev &lt;- study_hours - x_mean\ny_dev &lt;- test_scores - y_mean\n\n# Tabela oblicze≈Ñ\ncalc_table &lt;- data.frame(\n  X = study_hours,\n  Y = test_scores,\n  X_dev = x_dev,\n  Y_dev = y_dev,\n  X_dev_sq = x_dev^2,\n  Y_dev_sq = y_dev^2,\n  Product = x_dev * y_dev\n)\nprint(calc_table)\n\n   X  Y X_dev Y_dev X_dev_sq Y_dev_sq Product\n1  2 65    -4   -14       16      196      56\n2  4 70    -2    -9        4       81      18\n3  6 80     0     1        0        1       0\n4  8 85     2     6        4       36      12\n5 10 95     4    16       16      256      64\n\n# Statystyki podsumowujƒÖce\ncat(\"\\nSummary:\")\n\n\nSummary:\n\ncat(\"\\nSum of (X-mean)^2:\", sum(x_dev^2))\n\n\nSum of (X-mean)^2: 40\n\ncat(\"\\nSum of (Y-mean)^2:\", sum(y_dev^2))\n\n\nSum of (Y-mean)^2: 570\n\ncat(\"\\nSum of products:\", sum(x_dev * y_dev))\n\n\nSum of products: 150\n\ncat(\"\\nStandard deviation of X:\", sd(study_hours))\n\n\nStandard deviation of X: 3.162278\n\ncat(\"\\nStandard deviation of Y:\", sd(test_scores))\n\n\nStandard deviation of Y: 11.93734\n\n# Przedzia≈Ç ufno≈õci i p-value\ncor_test &lt;- cor.test(study_hours, test_scores, method = \"pearson\")\nprint(cor_test)\n\n\n    Pearson's product-moment correlation\n\ndata:  study_hours and test_scores\nt = 15, df = 3, p-value = 0.0006431\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8994446 0.9995859\nsample estimates:\n      cor \n0.9933993 \n\n\nInterpretacja: r \\approx 0.994 wskazuje na niemal doskona≈Çy dodatni liniowy zwiƒÖzek miƒôdzy godzinami nauki a wynikiem testu. Warto≈õƒá p &lt; 0.05 sugeruje statystycznƒÖ istotno≈õƒá tej zale≈ºno≈õci.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#korelacja-rang-spearmana-spearman-rank-correlation",
    "href": "correg_pl.html#korelacja-rang-spearmana-spearman-rank-correlation",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.5 Korelacja rang Spearmana (Spearman Rank Correlation)",
    "text": "10.5 Korelacja rang Spearmana (Spearman Rank Correlation)\nKorelacja Spearmana mierzy monotoniczne zale≈ºno≈õci, u≈ºywajƒÖc rang zamiast surowych warto≈õci.\nWz√≥r:\n\n\\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2 - 1)},\n\ngdzie d_i to r√≥≈ºnica rang dla obserwacji i.\n\nPe≈Çny przyk≈Çad oblicze≈Ñ rƒôcznych (Complete Manual Example)\nDane: Wyniki z matematyki i angielskiego\n\n\n\nUcze≈Ñ\nMatematyka\nAngielski\n\n\n\n\nA\n78\n82\n\n\nB\n85\n90\n\n\nC\n88\n88\n\n\nD\n92\n94\n\n\nE\n95\n96\n\n\n\nRangowanie i obliczenia:\n\n\n\n\n\n\n\n\n\n\n\n\nUcze≈Ñ\nWynik z mat.\nRanga mat.\nWynik z ang.\nRanga ang.\nd = \\text{ranga mat.} - \\text{ranga ang.}\nd^2\n\n\n\n\nA\n78\n1\n82\n1\n0\n0\n\n\nB\n85\n2\n90\n3\n-1\n1\n\n\nC\n88\n3\n88\n2\n1\n1\n\n\nD\n92\n4\n94\n4\n0\n0\n\n\nE\n95\n5\n96\n5\n0\n0\n\n\n\n\n\n\n\nSuma:\n2\n\n\n\nObliczenie:\n\n\\rho = 1 - \\frac{6 \\times 2}{5(25-1)} = 1 - \\frac{12}{120} = 0.9.\n\n\n# Dane\nmath_scores &lt;- c(78, 85, 88, 92, 95)\nenglish_scores &lt;- c(82, 90, 88, 94, 96)\n\n# Rangi\nrank_table &lt;- data.frame(\n  Student = LETTERS[1:5],\n  Math_Score = math_scores,\n  Math_Rank = rank(math_scores),\n  English_Score = english_scores,\n  English_Rank = rank(english_scores)\n)\n\nrank_table$d &lt;- rank_table$Math_Rank - rank_table$English_Rank\nrank_table$d_squared &lt;- rank_table$d^2\n\nprint(rank_table)\n\n  Student Math_Score Math_Rank English_Score English_Rank  d d_squared\n1       A         78         1            82            1  0         0\n2       B         85         2            90            3 -1         1\n3       C         88         3            88            2  1         1\n4       D         92         4            94            4  0         0\n5       E         95         5            96            5  0         0\n\ncat(\"\\nSum of d^2:\", sum(rank_table$d_squared))\n\n\nSum of d^2: 2\n\n# Korelacja Spearmana\nrho &lt;- cor(math_scores, english_scores, method = \"spearman\")\ncat(\"\\nSpearman correlation (R):\", round(rho, 3))\n\n\nSpearman correlation (R): 0.9\n\n# Obliczenie rƒôczne\nn &lt;- length(math_scores)\nrho_manual &lt;- 1 - (6 * sum(rank_table$d_squared)) / (n * (n^2 - 1))\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.9",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#tabele-krzy≈ºowe-cross-tabulation-i-dane-kategoryczne",
    "href": "correg_pl.html#tabele-krzy≈ºowe-cross-tabulation-i-dane-kategoryczne",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.6 Tabele krzy≈ºowe (Cross-tabulation) i dane kategoryczne",
    "text": "10.6 Tabele krzy≈ºowe (Cross-tabulation) i dane kategoryczne\nTabela krzy≈ºowa (cross-tabulation, contingency table) pokazuje zale≈ºno≈õci miƒôdzy zmiennymi kategorycznymi.\n\n# Bardziej realistyczne dane przyk≈Çadowe\nset.seed(123)\nn_total &lt;- 120\n\n# Poziom edukacji a zatrudnienie\neducation &lt;- factor(\n  c(rep(\"High School\", 40), \n    rep(\"College\", 50), \n    rep(\"Graduate\", 30)),\n  levels = c(\"High School\", \"College\", \"Graduate\")\n)\n\n# Status zatrudnienia z prawdopodobie≈Ñstwami zale≈ºnymi od edukacji\nemployment &lt;- factor(\n  c(\n    # High School: 75% employed\n    sample(c(rep(\"Employed\", 30), rep(\"Unemployed\", 10)), 40),\n    # College: 90% employed  \n    sample(c(rep(\"Employed\", 45), rep(\"Unemployed\", 5)), 50),\n    # Graduate: 95% employed\n    sample(c(rep(\"Employed\", 28), rep(\"Unemployed\", 2)), 30)\n  ),\n  levels = c(\"Employed\", \"Unemployed\")\n)\n\n# Tabela kontyngencji\ncontingency_table &lt;- table(education, employment)\nprint(\"Contingency Table:\")\n\n[1] \"Contingency Table:\"\n\nprint(contingency_table)\n\n             employment\neducation     Employed Unemployed\n  High School       30         10\n  College           45          5\n  Graduate          28          2\n\n# Procenty w wierszach\nprint(\"\\nRow Percentages (% within each education level):\")\n\n[1] \"\\nRow Percentages (% within each education level):\"\n\nprint(round(prop.table(contingency_table, 1) * 100, 1))\n\n             employment\neducation     Employed Unemployed\n  High School     75.0       25.0\n  College         90.0       10.0\n  Graduate        93.3        6.7\n\n# Test niezale≈ºno≈õci chi-kwadrat (Chi-square test)\nchi_test &lt;- chisq.test(contingency_table)\n\nWarning in chisq.test(contingency_table): Chi-squared approximation may be\nincorrect\n\nprint(chi_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 5.9623, df = 2, p-value = 0.05073",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#ƒáwiczenia-praktyczne-z-rozwiƒÖzaniami-practical-exercises-with-solutions",
    "href": "correg_pl.html#ƒáwiczenia-praktyczne-z-rozwiƒÖzaniami-practical-exercises-with-solutions",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.7 ƒÜwiczenia praktyczne z rozwiƒÖzaniami (Practical Exercises with Solutions)",
    "text": "10.7 ƒÜwiczenia praktyczne z rozwiƒÖzaniami (Practical Exercises with Solutions)\n\nƒÜwiczenie 1: Rƒôczne obliczenie korelacji Pearsona (Calculate Pearson Correlation Manually)\nDane:\n\nWzrost (cale): 66, 68, 70, 72, 74\nWaga (funty): 140, 155, 170, 185, 200\n\nRozwiƒÖzanie:\n\n# Dane\nheight &lt;- c(66, 68, 70, 72, 74)\nweight &lt;- c(140, 155, 170, 185, 200)\n\n# Krok 1: ≈örednie\nh_mean &lt;- mean(height)  # 70\nw_mean &lt;- mean(weight)  # 170\n\n# Krok 2: Odchylenia i iloczyny\ncalculation_df &lt;- data.frame(\n  Height = height,\n  Weight = weight,\n  H_dev = height - h_mean,\n  W_dev = weight - w_mean,\n  H_dev_sq = (height - h_mean)^2,\n  W_dev_sq = (weight - w_mean)^2,\n  Product = (height - h_mean) * (weight - w_mean)\n)\n\nprint(calculation_df)\n\n  Height Weight H_dev W_dev H_dev_sq W_dev_sq Product\n1     66    140    -4   -30       16      900     120\n2     68    155    -2   -15        4      225      30\n3     70    170     0     0        0        0       0\n4     72    185     2    15        4      225      30\n5     74    200     4    30       16      900     120\n\n# Krok 3: Korelacja\ncov_hw &lt;- sum(calculation_df$Product) / (length(height) - 1)\nsd_h &lt;- sqrt(sum(calculation_df$H_dev_sq) / (length(height) - 1))\nsd_w &lt;- sqrt(sum(calculation_df$W_dev_sq) / (length(weight) - 1))\nr_manual &lt;- cov_hw / (sd_h * sd_w)\n\ncat(\"\\nManual calculation:\")\n\n\nManual calculation:\n\ncat(\"\\nCovariance:\", round(cov_hw, 2))\n\n\nCovariance: 75\n\ncat(\"\\nSD Height:\", round(sd_h, 2))\n\n\nSD Height: 3.16\n\ncat(\"\\nSD Weight:\", round(sd_w, 2))\n\n\nSD Weight: 23.72\n\ncat(\"\\nCorrelation:\", round(r_manual, 3))\n\n\nCorrelation: 1\n\n# Weryfikacja w R\ncat(\"\\n\\nR calculation:\", round(cor(height, weight), 3))\n\n\n\nR calculation: 1\n\n\n\n\nƒÜwiczenie 2: Rƒôczne obliczenie korelacji Spearmana (Calculate Spearman Correlation Manually)\nDane:\n\nRangi uczni√≥w z matematyki: 1, 3, 2, 5, 4\nRangi uczni√≥w z nauk ≈õcis≈Çych (science): 2, 4, 1, 5, 3\n\nRozwiƒÖzanie:\n\n# Rangi (ju≈º zrankowane)\nmath_rank &lt;- c(1, 3, 2, 5, 4)\nscience_rank &lt;- c(2, 4, 1, 5, 3)\n\n# R√≥≈ºnice\nd &lt;- math_rank - science_rank\nd_squared &lt;- d^2\n\n# Tabela\nrank_df &lt;- data.frame(\n  Student = 1:5,\n  Math_Rank = math_rank,\n  Science_Rank = science_rank,\n  d = d,\n  d_squared = d_squared\n)\n\nprint(rank_df)\n\n  Student Math_Rank Science_Rank  d d_squared\n1       1         1            2 -1         1\n2       2         3            4 -1         1\n3       3         2            1  1         1\n4       4         5            5  0         0\n5       5         4            3  1         1\n\n# Korelacja Spearmana\nn &lt;- length(math_rank)\nsum_d_sq &lt;- sum(d_squared)\nrho_manual &lt;- 1 - (6 * sum_d_sq) / (n * (n^2 - 1))\n\ncat(\"\\nSum of d^2:\", sum_d_sq)\n\n\nSum of d^2: 4\n\ncat(\"\\nSpearman correlation (manual):\", round(rho_manual, 3))\n\n\nSpearman correlation (manual): 0.8\n\ncat(\"\\nSpearman correlation (R):\", round(cor(math_rank, science_rank, method = \"spearman\"), 3))\n\n\nSpearman correlation (R): 0.8\n\n\n\n\nƒÜwiczenie 3: Interpretacja wynik√≥w (Interpretation Practice)\nZinterpretuj nastƒôpujƒÖce warto≈õci korelacji:\n\nr = 0.85 miƒôdzy godzinami treningu a wynikiem sprawdzianu sprawno≈õci\nOdpowied≈∫: Silny dodatni zwiƒÖzek. Wraz ze wzrostem liczby godzin treningu wyniki istotnie rosnƒÖ.\nr = -0.72 miƒôdzy temperaturƒÖ na zewnƒÖtrz a kosztami ogrzewania\nOdpowied≈∫: Silny ujemny zwiƒÖzek. Wraz ze wzrostem temperatury koszty ogrzewania wyra≈∫nie malejƒÖ.\nr = 0.12 miƒôdzy rozmiarem buta a inteligencjƒÖ\nOdpowied≈∫: Bardzo s≈Çaby/brak istotnego zwiƒÖzku. Zmienne sƒÖ praktycznie niezale≈ºne.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#najwa≈ºniejsze-rzeczy-do-zapamiƒôtania-important-points-to-remember",
    "href": "correg_pl.html#najwa≈ºniejsze-rzeczy-do-zapamiƒôtania-important-points-to-remember",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.8 Najwa≈ºniejsze rzeczy do zapamiƒôtania (Important Points to Remember)",
    "text": "10.8 Najwa≈ºniejsze rzeczy do zapamiƒôtania (Important Points to Remember)\n\nKorelacja mierzy si≈Çƒô zwiƒÖzku: Warto≈õci od -1 do +1.\nKorelacja ‚â† przyczynowo≈õƒá (Correlation ‚â† Causation): Wysoka korelacja nie dowodzi wp≈Çywu jednej zmiennej na drugƒÖ.\nDobierz w≈Ça≈õciwƒÖ metodƒô:\n\nPearson: ZwiƒÖzki liniowe dla danych ciƒÖg≈Çych.\nSpearman: ZwiƒÖzki monotoniczne lub dane rangowe.\n\nSprawd≈∫ za≈Ço≈ºenia:\n\nPearson: liniowo≈õƒá i (w praktyce) rozk≈Çad zbli≈ºony do normalnego.\nSpearman: wymagana jedynie monotoniczno≈õƒá.\n\nUwaga na obserwacje odstajƒÖce (outliers): MogƒÖ silnie wp≈Çywaƒá na korelacjƒô Pearsona.\nZawsze wizualizuj dane: Wykresy pomagajƒÖ oceniƒá kszta≈Çt zale≈ºno≈õci.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#podsumowanie-drzewko-decyzyjne-do-analizy-korelacji-summary-decision-tree-for-correlation-analysis",
    "href": "correg_pl.html#podsumowanie-drzewko-decyzyjne-do-analizy-korelacji-summary-decision-tree-for-correlation-analysis",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.9 Podsumowanie: drzewko decyzyjne do analizy korelacji (Summary: Decision Tree for Correlation Analysis)",
    "text": "10.9 Podsumowanie: drzewko decyzyjne do analizy korelacji (Summary: Decision Tree for Correlation Analysis)\n\n\n\nWYB√ìR W≈ÅA≈öCIWEJ MIARY KORELACJI:\n\nCzy dane sƒÖ liczbowe (numeryczne)?\n‚îú‚îÄ TAK ‚Üí Czy zwiƒÖzek jest liniowy?\n‚îÇ   ‚îú‚îÄ TAK ‚Üí U≈ºyj korelacji PEARSONA\n‚îÇ   ‚îî‚îÄ NIE ‚Üí Czy zwiƒÖzek jest monotoniczny?\n‚îÇ       ‚îú‚îÄ TAK ‚Üí U≈ºyj korelacji SPEARMANA\n‚îÇ       ‚îî‚îÄ NIE ‚Üí Rozwa≈º metody nieliniowe\n‚îî‚îÄ NIE ‚Üí Czy dane sƒÖ porzƒÖdkowe (rangi)?\n    ‚îú‚îÄ TAK ‚Üí U≈ºyj korelacji SPEARMANA\n    ‚îî‚îÄ NIE ‚Üí U≈ºyj TABEL KRZY≈ªOWYCH dla danych kategorycznych\n\n\n\n≈öciƒÖga (Quick Reference Card)\n\n\n\n\n\n\n\n\n\nMiara\nKiedy u≈ºywaƒá (Use When)\nWz√≥r (Formula)\nZakres (Range)\n\n\n\n\nKowariancja (Covariance)\nWstƒôpne badanie zwiƒÖzku\n\\displaystyle \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n-\\infty do +\\infty\n\n\nPearson r\nZwiƒÖzki liniowe, dane ciƒÖg≈Çe\n\\displaystyle \\frac{\\operatorname{cov}(X,Y)}{s_X s_Y}\n-1 do +1\n\n\nSpearman \\rho\nZwiƒÖzki monotoniczne, rangi\n\\displaystyle 1-\\frac{6\\sum d_i^2}{n(n^2-1)}\n-1 do +1\n\n\nTabele krzy≈ºowe (Cross-tabs)\nZmienne kategoryczne\nZliczenia czƒôsto≈õci\nn/d",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#analiza-regresji-ols-ordinary-least-squares-przewodnik-na-start-a-quick-start-guide",
    "href": "correg_pl.html#analiza-regresji-ols-ordinary-least-squares-przewodnik-na-start-a-quick-start-guide",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.10 Analiza regresji OLS (Ordinary Least Squares): przewodnik na start (A Quick-start Guide)",
    "text": "10.10 Analiza regresji OLS (Ordinary Least Squares): przewodnik na start (A Quick-start Guide)\n\n\n\n\n\n\nAnaliza regresji OLS: przewodnik na start\n\n\n\n\nWprowadzenie: czym jest analiza regresji?\nAnaliza regresji (regression analysis) pomaga zrozumieƒá i mierzyƒá zale≈ºno≈õci miƒôdzy obserwowalnymi wielko≈õciami. To zestaw narzƒôdzi matematycznych do identyfikowania wzorc√≥w w danych, kt√≥re umo≈ºliwiajƒÖ prognozowanie (prediction).\nRozwa≈º pytania badawcze:\n\nJak czas nauki wp≈Çywa na wynik testu?\nJak do≈õwiadczenie wp≈Çywa na wynagrodzenie?\nJak wydatki na reklamƒô oddzia≈ÇujƒÖ na sprzeda≈º?\n\nRegresja dostarcza systematycznych metod, by na te pytania odpowiadaƒá na podstawie realnych danych.\n\n\nPunkt wyj≈õcia: prosty przyk≈Çad\nZacznijmy od konkretu. Zebrano dane o 20 studentach z Twojej klasy:\n\n\n\nStudent\nStudy Hours\nExam Score\n\n\n\n\nAlex\n2\n68\n\n\nBeth\n4\n74\n\n\nCarlos\n6\n85\n\n\nDiana\n8\n91\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\nPo narysowaniu wykresu punktowego (scatter plot) chcesz znale≈∫ƒá prostƒÖ, kt√≥ra najlepiej opisuje zwiƒÖzek miƒôdzy godzinami nauki a wynikiem.\nAle co znaczy ‚Äûnajlepiej‚Äù? W≈Ça≈õnie to odkryjemy.\n\n\nDlaczego prawdziwe dane nie uk≈ÇadajƒÖ siƒô w idealnƒÖ liniƒô\nZanim przejdziemy do rachunk√≥w, zrozummy, dlaczego punkty zwykle nie le≈ºƒÖ na jednej prostej.\n\nModele deterministyczne vs.¬†stochastyczne\nModele deterministyczne (deterministic models) opisujƒÖ zwiƒÖzki bez niepewno≈õci. Przyk≈Çad z fizyki:\n\\text{Distance} = \\text{Speed} \\times \\text{Time}\nJedziesz dok≈Çadnie 60 mph przez 2 godziny ‚Üí zawsze 120 mil. Zero odchyle≈Ñ.\nModele stochastyczne (stochastic models) uznajƒÖ, ≈ºe w danych naturalnie wystƒôpuje losowo≈õƒá. Og√≥lna postaƒá to:\nY = f(X) + \\epsilon\nGdzie:\n\nY ‚Äî wielko≈õƒá, kt√≥rƒÖ prognozujemy (np. wynik testu),\nf(X) ‚Äî wzorzec systematyczny (jak godziny nauki typowo wp≈ÇywajƒÖ na wyniki),\n\\epsilon ‚Äî ‚Äûreszta‚Äù/szum: wszystko, czego nie mierzymy.\n\nW naszym przyk≈Çadzie dwoje student√≥w mo≈ºe uczyƒá siƒô po 5 godzin, a jednak dostaƒá r√≥≈ºne oceny, bo:\n\njedno lepiej spa≈Ço,\njedno ma talent do test√≥w,\njedno mia≈Ço ha≈Ças na sali,\npytania trafi≈Çy bardziej/mniej pod ich przygotowanie.\n\nTa losowo≈õƒá jest naturalna ‚Äî tym zajmuje siƒô \\epsilon.\n\n\n\nProsty model regresji liniowej\nZale≈ºno≈õƒá miƒôdzy godzinami nauki a wynikiem zapisujemy jako:\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nRozszyfrujmy:\n\nY_i ‚Äî wynik testu studenta i,\nX_i ‚Äî godziny nauki studenta i,\n\\beta_0 ‚Äî wyraz wolny (intercept, ‚Äûpoziom bazowy‚Äù przy 0 godzin),\n\\beta_1 ‚Äî nachylenie (slope, przyrost punkt√≥w na godzinƒô),\n\\epsilon_i ‚Äî ‚Äûwszystko inne‚Äù wp≈ÇywajƒÖce na wynik i.\n\nWa≈ºne: Prawdziwych warto≈õci \\beta_0, \\beta_1 nie znamy. Szacujemy je z danych i oznaczamy ‚Äûz daszkiem‚Äù: \\hat{\\beta}_0, \\hat{\\beta}_1.\n\n\nReszty: jak bardzo mylimy siƒô w przewidywaniach?\nPo dopasowaniu prostej mo≈ºemy przewidzieƒá wyniki. Dla ka≈ºdej obserwacji:\n\nWarto≈õƒá rzeczywista (y_i): faktyczny wynik,\nWarto≈õƒá przewidziana (\\hat{y}_i): co ‚Äûm√≥wi‚Äù nasza prosta,\nReszta (e_i): r√≥≈ºnica = Rzeczywista ‚àí Przewidziana.\n\nPrzyk≈Çad:\nDiana: 8 h nauki, wynik 91\nLinia przewiduje: 88\nReszta: 91 ‚àí 88 = +3 (zani≈ºyli≈õmy)\n\nEric: 5 h nauki, wynik 70\nLinia przewiduje: 79\nReszta: 70 ‚àí 79 = ‚àí9 (zawy≈ºyli≈õmy)\n\n\nKluczowy pomys≈Ç: dlaczego kwadratujemy reszty?\nZa≈Ç√≥≈ºmy reszty czterech student√≥w:\n\nA: +5\nB: ‚àí5\nC: +3\nD: ‚àí3\n\nSuma: (+5) + (-5) + (+3) + (-3) = 0.\nTo nie znaczy, ≈ºe przewidywania sƒÖ idealne ‚Äî b≈Çƒôdy siƒô znoszƒÖ.\nRozwiƒÖzanie: sumujemy kwadraty reszt:\n\n(+5)^2 = 25\n(-5)^2 = 25\n(+3)^2 = 9\n(-3)^2 = 9\nSuma kwadrat√≥w b≈Çƒôd√≥w = 68\n\nDlaczego to dzia≈Ça:\n\nBrak znoszenia znak√≥w, bo kwadraty sƒÖ dodatnie,\nDu≈ºe b≈Çƒôdy wa≈ºƒÖ mocniej (10 punkt√≥w to 4√ó wiƒôcej ni≈º 5 punkt√≥w),\nWygoda matematyczna: funkcje kwadratowe sƒÖ g≈Çadkie i r√≥≈ºniczkowalne.\n\n\n\nMetoda zwyk≈Çych najmniejszych kwadrat√≥w (OLS)\nOLS wybiera takƒÖ prostƒÖ, kt√≥ra minimalizuje sumƒô kwadrat√≥w reszt (SSE ‚Äî Sum of Squared Errors):\n\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\nCzyli:\n\\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n \\big(y_i - (\\beta_0 + \\beta_1 x_i)\\big)^2\n‚ÄûPo ludzku‚Äù: Znajd≈∫ takie \\beta_0 i \\beta_1, by ≈ÇƒÖczny b≈ÇƒÖd (w kwadracie) przewidywa≈Ñ by≈Ç jak najmniejszy.\n\n\nRozwiƒÖzanie matematyczne\nMinimalizujemy SSE rachunkiem r√≥≈ºniczkowym. Warunki pierwszego rzƒôdu:\n\\frac{\\partial SSE}{\\partial \\beta_0} = -2\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i) = 0\n\\frac{\\partial SSE}{\\partial \\beta_1} = -2\\sum_{i=1}^n x_i(y_i - \\beta_0 - \\beta_1x_i) = 0\nRozwiƒÖzanie uk≈Çadu:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\nWnioski:\n\nNachylenie zale≈ºy od tego, jak wsp√≥≈ÇzmieniajƒÖ siƒô X i Y (kowariancja) wzglƒôdem zmienno≈õci samego X (wariancja),\nLinia przechodzi przez punkt ≈õrednich (\\bar{x}, \\bar{y}).\n\n\n\nSkƒÖd wiemy, ≈ºe linia jest ‚Äûdobra‚Äù? Rozk≈Çad zmienno≈õci\nRozbijamy ca≈ÇkowitƒÖ zmienno≈õƒá wynik√≥w:\nCa≈Çkowita suma kwadrat√≥w (SST ‚Äî Total Sum of Squares)\n‚ÄûJak bardzo og√≥lnie r√≥≈ºniƒÖ siƒô wyniki?‚Äù\nSST = \\sum_{i=1}^n (y_i - \\bar{y})^2\nSuma kwadrat√≥w regresji (SSR ‚Äî Regression Sum of Squares)\n‚ÄûIle zmienno≈õci wyja≈õnia nasza linia?‚Äù\nSSR = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\nSuma kwadrat√≥w b≈Çƒôd√≥w (SSE ‚Äî Error Sum of Squares)\n‚ÄûIle nie wyja≈õniamy?‚Äù\nSSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\nTo≈ºsamo≈õƒá wariancyjna:\nSST = SSR + SSE \\quad\\Rightarrow\\quad \\text{Ca≈Çkowita} = \\text{Wyja≈õniona} + \\text{Niewyja≈õniona}\n\n\nR^2: ocena dopasowania\nWsp√≥≈Çczynnik determinacji (R^2) m√≥wi, jaki odsetek zmienno≈õci wyja≈õnia model:\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nInterpretacja:\n\nR^2 = 0.75: ‚ÄûGodziny nauki wyja≈õniajƒÖ 75% zr√≥≈ºnicowania wynik√≥w‚Äù,\nR^2 = 0.30: ‚ÄûModel wyja≈õnia 30% tego, czym r√≥≈ºniƒÖ siƒô wyniki‚Äù,\nR^2 = 1.00: perfekcyjne dopasowanie (w realu prawie nigdy),\nR^2 = 0.00: nie lepiej ni≈º zgadywanie ≈õredniej.\n\nUwaga kontekstowa: w naukach spo≈Çecznych 0.30 bywa ≈õwietne; w in≈ºynierii oczekuje siƒô bardzo wysokich R^2.\n\n\nInterpretacja wynik√≥w\nZa≈Ç√≥≈ºmy, ≈ºe oszacowali≈õmy \\hat{\\beta}_0 = 60 i \\hat{\\beta}_1 = 4.\nNachylenie (\\hat{\\beta}_1 = 4):\n\n‚ÄûKa≈ºda dodatkowa godzina nauki wiƒÖ≈ºe siƒô ≈õrednio z +4 punktami‚Äù,\nTo efekt przeciƒôtny, nie obietnica dla konkretnej osoby.\n\nWyraz wolny (\\hat{\\beta}_0 = 60):\n\n‚ÄûPrzy 0 godzinach nauki przewidujemy 60 punkt√≥w‚Äù,\nCzƒôsto to tylko kotwica matematyczna ‚Äî mo≈ºe nie mieƒá sensu praktycznego.\n\nR√≥wnanie predykcji:\n\\widehat{\\text{Wynik}} = 60 + 4 \\times \\text{Godziny nauki}\n5 godzin ‚Üí 60 + 4 \\cdot 5 = 80 punkt√≥w.\n\n\nWielko≈õƒá efektu i istotno≈õƒá praktyczna\nIstotno≈õƒá statystyczna m√≥wi, czy efekt istnieje; istotno≈õƒá praktyczna ‚Äî czy ma znaczenie. Potrzebujemy obu.\n\nSurowa wielko≈õƒá efektu\nTo po prostu nachylenie \\hat{\\beta}_1.\nPrzyk≈Çad: \\hat{\\beta}_1 = 4 pkt/godz.\nCzy to ‚Äûdu≈ºo‚Äù? Zale≈ºy od:\n\nSkali wyniku (4/100 = 4% vs 4/500 = 0,8%),\nKosztu interwencji (czy 1h nauki warta 4 pkt?),\nProg√≥w decyzyjnych (czy 4 pkt zmienia ocenƒô?).\n\n\n\nStandaryzowana wielko≈õƒá efektu\nU≈Çatwia por√≥wnania miƒôdzy badaniami:\n\\beta_{\\text{std}} = \\hat{\\beta}_1 \\times \\frac{s_X}{s_Y}\ngdzie s_X i s_Y to odchylenia standardowe X i Y.\nPrzyk≈Çad: je≈õli s_X = 2.5 h, s_Y = 12 pkt i \\hat{\\beta}_1 = 4:\n\\beta_{\\text{std}} = 4 \\cdot \\frac{2.5}{12} = 0.83\n‚ÄûPrzy wzro≈õcie X o 1 SD, Y ro≈õnie o 0.83 SD‚Äù.\n\n\nWskaz√≥wki Cohena\nDla standaryzowanych wsp√≥≈Çczynnik√≥w:\n\nma≈Çy: |\\beta| \\approx 0.10 (~1% wariancji),\n≈õredni: |\\beta| \\approx 0.30 (~9%),\ndu≈ºy: |\\beta| \\approx 0.50 (~25%).\n\nDla R^2:\n\nma≈Çy: R^2 \\approx 0.02,\n≈õredni: R^2 \\approx 0.13,\ndu≈ºy: R^2 \\approx 0.26.\n\nUwaga: to og√≥lne progi ‚Äî normy r√≥≈ºniƒÖ siƒô miƒôdzy dziedzinami.\n\n\nPrzedzia≈Çy ufno≈õci dla wielko≈õci efektu\nDla surowego wsp√≥≈Çczynnika:\nCI = \\hat{\\beta}_1 \\pm t_{\\text{critical}} \\cdot SE(\\hat{\\beta}_1)\nJe≈õli 95% CI = [3.2, 4.8], m√≥wimy: ‚ÄûZ 95% pewno≈õciƒÖ prawdziwy efekt mie≈õci siƒô miƒôdzy 3.2 a 4.8 pkt/godz.‚Äù\n\n\nOcena istotno≈õci praktycznej\nWe≈∫ pod uwagƒô:\n\nMinimalnie istotnƒÖ r√≥≈ºnicƒô (MID),\nIle trzeba zmieniƒá X, by osiƒÖgnƒÖƒá sensownƒÖ zmianƒô Y,\nKoszt-efektywno≈õƒá:\n\n\\text{Efektywno≈õƒá} = \\frac{\\text{Wielko≈õƒá efektu}}{\\text{Koszt interwencji}}\nPrzyk≈Çad:\nEfekt: 4 pkt/godz.\nPr√≥g zaliczenia: 70; ≈õrednia: 68 ‚Üí 30 min nauki mo≈ºe zmieniƒá niezal na zal ‚Üí istotne praktycznie.\n\n\n\nNiepewno≈õƒá\nSzacunki pochodzƒÖ z pr√≥by, nie z ca≈Çej populacji ‚Üí niepewno≈õƒá.\n\nSkƒÖd niepewno≈õƒá?\n\nMasz 20 student√≥w, nie wszystkich,\nPr√≥ba mo≈ºe byƒá nietypowa,\nPomiary nie sƒÖ doskona≈Çe (raportowanie godzin nauki).\n\n\n\nPrzedzia≈Çy ufno≈õci\nZamiast ‚Äûefekt to dok≈Çadnie 4‚Äù, m√≥wimy:\n\n‚ÄûSzacujemy 4‚Äù,\n‚Äû95% CI: [3.2, 4.8]‚Äú.\n\nZnaczenie: w wielu powt√≥rzeniach 95% takich przedzia≈Ç√≥w zawiera prawdziwƒÖ warto≈õƒá.\n\n\nTestowanie istnienia zale≈ºno≈õci\nPytamy: ‚ÄûGdyby prawdziwie nie by≈Ço zwiƒÖzku, jak ma≈Ço prawdopodobny by≈Çby obserwowany wzorzec?‚Äù\n\np = 0.03: przy braku efektu tylko 3% szans na tak silny wzorzec ‚Äûz przypadku‚Äù,\np = 0.40: wzorzec ‚Äûm√≥g≈Çby siƒô ≈Çatwo zdarzyƒá‚Äù bez efektu.\n\nRegu≈Ça kciuka: p &lt; 0.05 ‚Üí ‚Äûstatystycznie istotne‚Äù.\n\n\n\nGdy co≈õ idzie ≈∫le: diagnostyka modelu\nSzybkie wizualizacje:\n\nWykres punktowy: czy zwiƒÖzek jest mniej wiƒôcej liniowy?\nReszty vs.¬†przewidywania: powinien byƒá losowy ob≈Çok,\nOutliery: punkty bardzo odleg≈Çe?\n\nSygna≈Çy ostrzegawcze:\n\nWzorzec w resztach ‚Üí brak liniowo≈õci lub zmienna pominiƒôta,\nRozszerzajƒÖcy siƒô wachlarz reszt ‚Üí heteroskedastyczno≈õƒá,\nWp≈Çywowe obserwacje (influential) ciƒÖgnƒÖ prostƒÖ,\nPominiƒôte zmienne ‚Üí obciƒÖ≈ºenia (bias).\n\n\n\nZa≈Ço≈ºenia: kiedy OLS dzia≈Ça dobrze\n\nLiniowo≈õƒá (linearity) ‚Äî zale≈ºno≈õƒá w przybli≈ºeniu prosta,\nNiezale≈ºno≈õƒá (independence) ‚Äî obserwacje od siebie niezale≈ºne,\nSta≈Ça wariancja (homoskedastyczno≈õƒá) ‚Äî rozrzut reszt podobny w ca≈Çym zakresie,\nBrak doskona≈Çej wsp√≥≈Çliniowo≈õci (no perfect multicollinearity) ‚Äî w regresji wielorakiej predyktory nie sƒÖ liniowo zale≈ºne ‚Äûna 100%‚Äú,\nLosowy dob√≥r pr√≥by (random sampling) ‚Äî dane reprezentatywne.\n\n\n\nPodsumowanie\nCo robi OLS:\n\nDopasowuje prostƒÖ minimalizujƒÖcƒÖ SSE,\nSzacuje, o ile ≈õrednio zmienia siƒô Y przy zmianie X o 1,\nPodaje R^2 ‚Äî ile zmienno≈õci wyja≈õniamy,\nKwantyfikuje niepewno≈õƒá (SE, CI, p-values).\n\nKroki praktyczne:\n\nWykres danych ‚Äî czy linia ma sens?\nUruchom OLS ‚Üí \\hat{\\beta}_0, \\hat{\\beta}_1,\nSprawd≈∫ R^2,\nOblicz wielko≈õci efektu (surowƒÖ i standaryzowanƒÖ),\nOce≈Ñ istotno≈õƒá praktycznƒÖ,\nSprawd≈∫ przedzia≈Çy ufno≈õci,\nObejrzyj reszty,\nDecyduj, ≈ÇƒÖczƒÖc istotno≈õƒá statystycznƒÖ i praktycznƒÖ.\n\n\n\nKluczowe interpretacje / Key interpretations\nDomy≈õlny model: regresja OLS Y=\\beta_0+\\beta_1 X+\\varepsilon (lub wieloraka: Y=\\beta_0+\\beta_1 X_1+\\cdots+\\beta_p X_p+\\varepsilon).\n\nNachylenie / Slope (\\beta_1) PL: Przy wzro≈õcie X o 1 jednostkƒô (ceteris paribus), przeciƒôtna warto≈õƒá Y zmienia siƒô o \\beta_1 jednostek. ENG: When X increases by 1 unit (ceteris paribus), the expected value of Y changes by \\beta_1 units.\nStandaryzowane nachylenie / Standardized slope \\big(\\beta_{1}^{(\\mathrm{std})}\\big) Definicja:\n\n\\beta_{1}^{(\\mathrm{std})} \\;=\\; \\beta_1 \\cdot \\frac{s_X}{s_Y},\n\ngdzie s_X i s_Y to odchylenia standardowe X i Y. PL: Przy wzro≈õcie X o 1 odchylenie standardowe (SD), przeciƒôtna warto≈õƒá Y zmienia siƒô o \\beta_{1}^{(\\mathrm{std})} odchyle≈Ñ standardowych Y. ENG: For a 1 standard deviation (SD) increase in X, the expected value of Y changes by \\beta_{1}^{(\\mathrm{std})} SDs of Y. Uwaga/Note: W regresji prostej \\beta_{1}^{(\\mathrm{std})} = r (Pearson). / In simple regression, \\beta_{1}^{(\\mathrm{std})} = r (Pearson).\nWsp√≥≈Çczynnik determinacji / R^2 Definicja:\n\nR^2 \\;=\\; 1 - \\frac{SS_{\\mathrm{res}}}{SS_{\\mathrm{tot}}}.\n\nPL: Model wyja≈õnia 100\\times R^2% zmienno≈õci Y wzglƒôdem modelu tylko z wyrazem wolnym (in-sample). ENG: The model explains 100\\times R^2% of the variance in Y relative to the intercept-only model (in-sample). W wielu zmiennych rozwa≈º: \\text{adjusted } R^2. / With multiple predictors consider: adjusted R^2.\nWarto≈õƒá p / P-value Formalnie/Formally:\n\np \\;=\\; \\Pr\\!\\big(\\,|T|\\ge |t_{\\mathrm{obs}}| \\mid H_0\\,\\big),\n\ngdzie T ma rozk≈Çad t przy H_0. PL: Zak≈ÇadajƒÖc prawdziwo≈õƒá H_0 i spe≈Çnione za≈Ço≈ºenia modelu, prawdopodobie≈Ñstwo uzyskania co najmniej tak ekstremalnej statystyki jak obserwowana wynosi p. ENG: Assuming H_0 and the model assumptions hold, p is the probability of observing a test statistic at least as extreme as the one obtained.\nPrzedzia≈Ç ufno≈õci / Confidence interval (np. dla \\beta_1) Konstrukcja/Construction:\n\n\\hat{\\beta}_1 \\;\\pm\\; t_{1-\\alpha/2,\\ \\mathrm{df}} \\cdot \\mathrm{SE}\\!\\left(\\hat{\\beta}_1\\right).\n\nPL (≈õci≈õle): W d≈Çugiej serii powt√≥rze≈Ñ 95% tak skonstruowanych przedzia≈Ç√≥w zawiera prawdziwƒÖ warto≈õƒá \\beta_1; dla naszych danych oszacowanie mie≈õci siƒô w [\\text{lower},\\ \\text{upper}]. ENG (strict): Over many repetitions, 95% of such intervals would contain the true \\beta_1; for our data, the estimate lies within [\\text{lower},\\ \\text{upper}]. PL (skr√≥t dydaktyczny): ‚ÄûJeste≈õmy 95% pewni, ≈ºe \\beta_1 le≈ºy w [\\text{lower},\\ \\text{upper}].‚Äù ENG (teaching shorthand): ‚ÄúWe are 95% confident that \\beta_1 lies in [\\text{lower},\\ \\text{upper}].‚Äù\n\n\nNajczƒôstsze nieporozumienia / Common pitfalls\n\nPL: p nie jest prawdopodobie≈Ñstwem, ≈ºe H_0 jest prawdziwa. ENG: p is not the probability that H_0 is true.\nPL: 95% CI nie zawiera 95% obserwacji (od tego jest przedzia≈Ç predykcji). ENG: A 95% CI does not contain 95% of observations (that‚Äôs a prediction interval).\nPL/ENG: Wysokie R^2 ‚â† przyczynowo≈õƒá / High R^2 ‚â† causality. Zawsze sprawdzaj/Always check diagnozy reszt, skalƒô efektu, i dopasowanie poza pr√≥bƒÖ.\n\nPamiƒôtaj:\n\nAsocjacja ‚â† przyczynowo≈õƒá,\nIstotno≈õƒá statystyczna ‚â† istotno≈õƒá praktyczna,\n‚ÄûKa≈ºdy model jest b≈Çƒôdny, niekt√≥re sƒÖ u≈ºyteczne‚Äù,\nZawsze wizualizuj dane i reszty,\nDecyzje opieraj na wielko≈õci efektu i niepewno≈õci.\n\nOLS dostarcza uporzƒÖdkowany, matematyczny spos√≥b znajdowania wzorc√≥w w danych. Nie daje doskona≈Çych prognoz, ale zapewnia najlepszƒÖ liniowƒÖ aproksymacjƒô wraz z uczciwƒÖ ocenƒÖ jej jako≈õci i niepewno≈õci.\n\n\n\n10.11 Rƒôczne obliczenia OLS krok po kroku\nBadaczka chce zbadaƒá zale≈ºno≈õƒá miƒôdzy godzinami nauki a wynikiem testu (6 student√≥w):\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nCelem jest wyznaczyƒá \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X metodƒÖ OLS.\n\nKrok 1: ≈örednie\nDla X:\n\\bar{X} = \\frac{1+2+3+4+5+6}{6} = \\frac{21}{6} = 3.5\nDla Y:\n\\bar{Y} = \\frac{65+70+75+85+88+95}{6} = \\frac{478}{6} = 79.67\n\n\nKrok 2: Odchylenia od ≈õrednich\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n‚àí2.5\n‚àí14.67\n\n\nB\n2\n70\n‚àí1.5\n‚àí9.67\n\n\nC\n3\n75\n‚àí0.5\n‚àí4.67\n\n\nD\n4\n85\n0.5\n5.33\n\n\nE\n5\n88\n1.5\n8.33\n\n\nF\n6\n95\n2.5\n15.33\n\n\n\n\n\nKrok 3: Iloczyny i kwadraty\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n36.68\n6.25\n\n\nB\n14.51\n2.25\n\n\nC\n2.34\n0.25\n\n\nD\n2.67\n0.25\n\n\nE\n12.50\n2.25\n\n\nF\n38.33\n6.25\n\n\nSuma\n107.03\n17.50\n\n\n\n\n\nKrok 4: Nachylenie \\hat{\\beta}_1\n\\hat{\\beta}_1 = \\frac{107.03}{17.50} = 6.12\nInterpretacja: +6.12 punktu za ka≈ºdƒÖ dodatkowƒÖ godzinƒô nauki.\n\n\nKrok 5: Wyraz wolny \\hat{\\beta}_0\n\\hat{\\beta}_0 = 79.67 - 6.12 \\cdot 3.5 = 58.25\nInterpretacja: przy 0 godzinach przewidujemy 58.25.\n\n\nKrok 6: R√≥wnanie regresji\n\\hat{Y} = 58.25 + 6.12 X\n\n\nKrok 7: Predykcje i reszty\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nReszta e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n64.37\n0.63\n\n\nB\n2\n70\n70.49\n‚àí0.49\n\n\nC\n3\n75\n76.61\n‚àí1.61\n\n\nD\n4\n85\n82.73\n2.27\n\n\nE\n5\n88\n88.85\n‚àí0.85\n\n\nF\n6\n95\n94.97\n0.03\n\n\n\nKontrola: suma reszt ‚âà 0 ‚úì\n\n\nKrok 8: Sumy kwadrat√≥w\nSST ‚Äî ca≈Çkowita:\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n215.21\n\n\nB\n70\n93.51\n\n\nC\n75\n21.81\n\n\nD\n85\n28.41\n\n\nE\n88\n69.39\n\n\nF\n95\n235.01\n\n\nSuma\n\nSST = 663.34\n\n\n\nSSR ‚Äî wyja≈õniona:\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n234.09\n\n\nB\n70.49\n84.27\n\n\nC\n76.61\n9.36\n\n\nD\n82.73\n9.36\n\n\nE\n88.85\n84.27\n\n\nF\n94.97\n234.09\n\n\nSuma\n\nSSR = 655.44\n\n\n\nSSE ‚Äî b≈ÇƒÖd:\n\n\n\nStudent\ne_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n‚àí0.49\n0.24\n\n\nC\n‚àí1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n‚àí0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSuma\n\nSSE = 9.10\n\n\n\nWeryfikacja: SST \\approx SSR + SSE\n663.34 \\approx 655.44 + 9.10 = 664.54 (drobne r√≥≈ºnice zaokrƒÖgle≈Ñ).\n\n\nKrok 9: R^2\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternatywnie:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 0.986\nInterpretacja: ~98.8% zmienno≈õci wynik√≥w wyja≈õniajƒÖ godziny nauki ‚Äî bardzo silny zwiƒÖzek.\n\n\nKrok 10: Wielko≈õci efektu\nSurowa: 6.12 pkt/godz.\nStandaryzowana:\n\ns_X = \\sqrt{17.50/5} = 1.87,\ns_Y = \\sqrt{663.34/5} = 11.52,\n\\beta_{\\text{std}} = 6.12 \\cdot (1.87/11.52) = 0.99 ‚Üí bardzo du≈ºy efekt (wg Cohena).\n\n\n\nKrok 11: Istotno≈õƒá praktyczna\n\nSkala: 6.12% na 100-punktowej skali / godz.,\nProgi: zmiana oceny (10 pkt) ‚âà 1.63 h,\nKoszt-efekt: korzystny ‚Äî sensowna inwestycja czasu.\n\n\n\nPodsumowanie wynik√≥w\n\nR√≥wnanie: \\hat{Y} = 58.25 + 6.12 X\nNachylenie: 6.12 pkt/godz.\nWyraz wolny: 58.25 pkt\nR^2: 0.988\n\\beta_{\\text{std}}: 0.99\n\nW praktyce: ka≈ºda godzina nauki to ‚âà +6 pkt; dopasowanie znakomite; efekt istotny statystycznie i praktycznie.\n\n\nKontrola wyniku\nSprawd≈∫, ≈ºe linia przechodzi przez (\\bar{X}, \\bar{Y}):\n58.25 + 6.12 \\cdot 3.5 = 79.67 = \\bar{Y} ‚úì\n\n\n\n10.12 Kod R do weryfikacji oblicze≈Ñ\n\n# Krok 1: Dane\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)      # X\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95) # Y\nn &lt;- length(study_hours)\n\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Krok 2: ≈örednie\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Krok 3: Odchylenia od ≈õrednich\ndata$x_dev &lt;- data$X - x_bar\ndata$y_dev &lt;- data$Y - y_bar\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Krok 4: Iloczyny i kwadraty\ndata$xy_product &lt;- data$x_dev * data$y_dev\ndata$x_dev_sq &lt;- data$x_dev^2\n\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - XÃÑ)(Yi - »≤):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - XÃÑ)(Yi - »≤): 107 \n\ncat(\"Sum of (Xi - XÃÑ)¬≤:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - XÃÑ)¬≤: 17.5 \n\n# Krok 5: Nachylenie (beta_1)\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (Œ≤‚ÇÅ) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (Œ≤‚ÇÅ) calculated manually: 6.11 \n\n# Krok 6: Wyraz wolny (beta_0)\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (Œ≤‚ÇÄ) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (Œ≤‚ÇÄ) calculated manually: 58.27 \n\n# Krok 7: Por√≥wnanie z lm()\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Krok 8: Predykcje i reszty\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X\ndata$residual &lt;- data$Y - data$Y_hat\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ‚âà 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ‚âà 0): 0 \n\n# Krok 9: Sumy kwadrat√≥w\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Krok 10: R-kwadrat\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R¬≤ (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR¬≤ (Method 1: SSR/SST): 0.9863 \n\ncat(\"R¬≤ (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR¬≤ (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R¬≤ (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR¬≤ (from lm function): 0.9863 \n\n# Krok 11: Wielko≈õci efektu\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\nsd_x &lt;- sd(data$X)\nsd_y &lt;- sd(data$Y)\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Korelacja (dla regresji prostej |r| = sqrt(R^2))\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R¬≤:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R¬≤: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n\n\n# Krok 12: Wizualizacja\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5,\n     xlim = c(0, 7), ylim = c(60, 100))\n\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Punkt ≈õrednich\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Reszty jako odcinki pionowe\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# R√≥wnanie na wykresie\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R¬≤ = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\nAnaliza regresji OLS - godziny nauki vs.¬†wynik testu\n\n\n\n# Podsumowanie ko≈Ñcowe\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )\n\n\n\n\n10.13 Jak uruchomiƒá kod\n\nSkopiuj ca≈Çy blok kodu,\nWklej do RStudio,\nUruchom chunk po chunk lub ca≈Çy dokument,\nPor√≥wnaj wyniki z obliczeniami rƒôcznymi.\n\nCo zobaczysz:\n\nNachylenie: 6.12,\nWyraz wolny: 58.25,\nR^2: ‚âà 0.988,\nEfekt standaryzowany: ‚âà 0.99,\nWykres z punktami, liniƒÖ regresji i resztami.\n\nTo potwierdza poprawno≈õƒá oblicze≈Ñ manualnych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#rƒôczne-obliczenia-ols-krok-po-kroku",
    "href": "correg_pl.html#rƒôczne-obliczenia-ols-krok-po-kroku",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.11 Rƒôczne obliczenia OLS krok po kroku",
    "text": "10.11 Rƒôczne obliczenia OLS krok po kroku\nBadaczka chce zbadaƒá zale≈ºno≈õƒá miƒôdzy godzinami nauki a wynikiem testu (6 student√≥w):\n\n\n\nStudent\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\nA\n1\n65\n\n\nB\n2\n70\n\n\nC\n3\n75\n\n\nD\n4\n85\n\n\nE\n5\n88\n\n\nF\n6\n95\n\n\n\nCelem jest wyznaczyƒá \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X metodƒÖ OLS.\n\nKrok 1: ≈örednie\nDla X:\n\\bar{X} = \\frac{1+2+3+4+5+6}{6} = \\frac{21}{6} = 3.5\nDla Y:\n\\bar{Y} = \\frac{65+70+75+85+88+95}{6} = \\frac{478}{6} = 79.67\n\n\nKrok 2: Odchylenia od ≈õrednich\n\n\n\nStudent\nX_i\nY_i\nX_i - \\bar{X}\nY_i - \\bar{Y}\n\n\n\n\nA\n1\n65\n‚àí2.5\n‚àí14.67\n\n\nB\n2\n70\n‚àí1.5\n‚àí9.67\n\n\nC\n3\n75\n‚àí0.5\n‚àí4.67\n\n\nD\n4\n85\n0.5\n5.33\n\n\nE\n5\n88\n1.5\n8.33\n\n\nF\n6\n95\n2.5\n15.33\n\n\n\n\n\nKrok 3: Iloczyny i kwadraty\n\n\n\nStudent\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n\n\n\n\nA\n36.68\n6.25\n\n\nB\n14.51\n2.25\n\n\nC\n2.34\n0.25\n\n\nD\n2.67\n0.25\n\n\nE\n12.50\n2.25\n\n\nF\n38.33\n6.25\n\n\nSuma\n107.03\n17.50\n\n\n\n\n\nKrok 4: Nachylenie \\hat{\\beta}_1\n\\hat{\\beta}_1 = \\frac{107.03}{17.50} = 6.12\nInterpretacja: +6.12 punktu za ka≈ºdƒÖ dodatkowƒÖ godzinƒô nauki.\n\n\nKrok 5: Wyraz wolny \\hat{\\beta}_0\n\\hat{\\beta}_0 = 79.67 - 6.12 \\cdot 3.5 = 58.25\nInterpretacja: przy 0 godzinach przewidujemy 58.25.\n\n\nKrok 6: R√≥wnanie regresji\n\\hat{Y} = 58.25 + 6.12 X\n\n\nKrok 7: Predykcje i reszty\n\n\n\n\n\n\n\n\n\n\nStudent\nX_i\nY_i\n\\hat{Y}_i = 58.25 + 6.12X_i\nReszta e_i = Y_i - \\hat{Y}_i\n\n\n\n\nA\n1\n65\n64.37\n0.63\n\n\nB\n2\n70\n70.49\n‚àí0.49\n\n\nC\n3\n75\n76.61\n‚àí1.61\n\n\nD\n4\n85\n82.73\n2.27\n\n\nE\n5\n88\n88.85\n‚àí0.85\n\n\nF\n6\n95\n94.97\n0.03\n\n\n\nKontrola: suma reszt ‚âà 0 ‚úì\n\n\nKrok 8: Sumy kwadrat√≥w\nSST ‚Äî ca≈Çkowita:\n\n\n\nStudent\nY_i\n(Y_i - \\bar{Y})^2\n\n\n\n\nA\n65\n215.21\n\n\nB\n70\n93.51\n\n\nC\n75\n21.81\n\n\nD\n85\n28.41\n\n\nE\n88\n69.39\n\n\nF\n95\n235.01\n\n\nSuma\n\nSST = 663.34\n\n\n\nSSR ‚Äî wyja≈õniona:\n\n\n\nStudent\n\\hat{Y}_i\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\nA\n64.37\n234.09\n\n\nB\n70.49\n84.27\n\n\nC\n76.61\n9.36\n\n\nD\n82.73\n9.36\n\n\nE\n88.85\n84.27\n\n\nF\n94.97\n234.09\n\n\nSuma\n\nSSR = 655.44\n\n\n\nSSE ‚Äî b≈ÇƒÖd:\n\n\n\nStudent\ne_i\ne_i^2\n\n\n\n\nA\n0.63\n0.40\n\n\nB\n‚àí0.49\n0.24\n\n\nC\n‚àí1.61\n2.59\n\n\nD\n2.27\n5.15\n\n\nE\n‚àí0.85\n0.72\n\n\nF\n0.03\n0.00\n\n\nSuma\n\nSSE = 9.10\n\n\n\nWeryfikacja: SST \\approx SSR + SSE\n663.34 \\approx 655.44 + 9.10 = 664.54 (drobne r√≥≈ºnice zaokrƒÖgle≈Ñ).\n\n\nKrok 9: R^2\nR^2 = \\frac{SSR}{SST} = \\frac{655.44}{663.34} = 0.988\nAlternatywnie:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{9.10}{663.34} = 0.986\nInterpretacja: ~98.8% zmienno≈õci wynik√≥w wyja≈õniajƒÖ godziny nauki ‚Äî bardzo silny zwiƒÖzek.\n\n\nKrok 10: Wielko≈õci efektu\nSurowa: 6.12 pkt/godz.\nStandaryzowana:\n\ns_X = \\sqrt{17.50/5} = 1.87,\ns_Y = \\sqrt{663.34/5} = 11.52,\n\\beta_{\\text{std}} = 6.12 \\cdot (1.87/11.52) = 0.99 ‚Üí bardzo du≈ºy efekt (wg Cohena).\n\n\n\nKrok 11: Istotno≈õƒá praktyczna\n\nSkala: 6.12% na 100-punktowej skali / godz.,\nProgi: zmiana oceny (10 pkt) ‚âà 1.63 h,\nKoszt-efekt: korzystny ‚Äî sensowna inwestycja czasu.\n\n\n\nPodsumowanie wynik√≥w\n\nR√≥wnanie: \\hat{Y} = 58.25 + 6.12 X\nNachylenie: 6.12 pkt/godz.\nWyraz wolny: 58.25 pkt\nR^2: 0.988\n\\beta_{\\text{std}}: 0.99\n\nW praktyce: ka≈ºda godzina nauki to ‚âà +6 pkt; dopasowanie znakomite; efekt istotny statystycznie i praktycznie.\n\n\nKontrola wyniku\nSprawd≈∫, ≈ºe linia przechodzi przez (\\bar{X}, \\bar{Y}):\n58.25 + 6.12 \\cdot 3.5 = 79.67 = \\bar{Y} ‚úì",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kod-r-do-weryfikacji-oblicze≈Ñ",
    "href": "correg_pl.html#kod-r-do-weryfikacji-oblicze≈Ñ",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.12 Kod R do weryfikacji oblicze≈Ñ",
    "text": "10.12 Kod R do weryfikacji oblicze≈Ñ\n\n# Krok 1: Dane\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6)      # X\nexam_scores &lt;- c(65, 70, 75, 85, 88, 95) # Y\nn &lt;- length(study_hours)\n\ndata &lt;- data.frame(\n  Student = LETTERS[1:6],\n  X = study_hours,\n  Y = exam_scores\n)\nprint(\"Original Data:\")\n\n[1] \"Original Data:\"\n\nprint(data)\n\n  Student X  Y\n1       A 1 65\n2       B 2 70\n3       C 3 75\n4       D 4 85\n5       E 5 88\n6       F 6 95\n\n# Krok 2: ≈örednie\nx_bar &lt;- mean(study_hours)\ny_bar &lt;- mean(exam_scores)\ncat(\"\\nMeans:\\n\")\n\n\nMeans:\n\ncat(sprintf(\"Mean of X (study hours): %.2f\\n\", x_bar))\n\nMean of X (study hours): 3.50\n\ncat(sprintf(\"Mean of Y (exam scores): %.2f\\n\", y_bar))\n\nMean of Y (exam scores): 79.67\n\n# Krok 3: Odchylenia od ≈õrednich\ndata$x_dev &lt;- data$X - x_bar\ndata$y_dev &lt;- data$Y - y_bar\ncat(\"\\nDeviations from means:\\n\")\n\n\nDeviations from means:\n\nprint(data[, c(\"Student\", \"x_dev\", \"y_dev\")])\n\n  Student x_dev      y_dev\n1       A  -2.5 -14.666667\n2       B  -1.5  -9.666667\n3       C  -0.5  -4.666667\n4       D   0.5   5.333333\n5       E   1.5   8.333333\n6       F   2.5  15.333333\n\n# Krok 4: Iloczyny i kwadraty\ndata$xy_product &lt;- data$x_dev * data$y_dev\ndata$x_dev_sq &lt;- data$x_dev^2\n\nsum_xy_product &lt;- sum(data$xy_product)\nsum_x_dev_sq &lt;- sum(data$x_dev_sq)\ncat(\"\\nSum of (Xi - XÃÑ)(Yi - »≤):\", round(sum_xy_product, 2), \"\\n\")\n\n\nSum of (Xi - XÃÑ)(Yi - »≤): 107 \n\ncat(\"Sum of (Xi - XÃÑ)¬≤:\", sum_x_dev_sq, \"\\n\")\n\nSum of (Xi - XÃÑ)¬≤: 17.5 \n\n# Krok 5: Nachylenie (beta_1)\nbeta_1_manual &lt;- sum_xy_product / sum_x_dev_sq\ncat(\"\\nSlope (Œ≤‚ÇÅ) calculated manually:\", round(beta_1_manual, 2), \"\\n\")\n\n\nSlope (Œ≤‚ÇÅ) calculated manually: 6.11 \n\n# Krok 6: Wyraz wolny (beta_0)\nbeta_0_manual &lt;- y_bar - beta_1_manual * x_bar\ncat(\"Intercept (Œ≤‚ÇÄ) calculated manually:\", round(beta_0_manual, 2), \"\\n\")\n\nIntercept (Œ≤‚ÇÄ) calculated manually: 58.27 \n\n# Krok 7: Por√≥wnanie z lm()\nmodel &lt;- lm(Y ~ X, data = data)\ncat(\"\\n--- Verification with R's lm() function ---\\n\")\n\n\n--- Verification with R's lm() function ---\n\nprint(summary(model)$coefficients[, 1:2])\n\n             Estimate Std. Error\n(Intercept) 58.266667  1.4045278\nX            6.114286  0.3606495\n\n# Krok 8: Predykcje i reszty\ndata$Y_hat &lt;- beta_0_manual + beta_1_manual * data$X\ndata$residual &lt;- data$Y - data$Y_hat\n\ncat(\"\\nPredicted values and residuals:\\n\")\n\n\nPredicted values and residuals:\n\nprint(data[, c(\"Student\", \"X\", \"Y\", \"Y_hat\", \"residual\")])\n\n  Student X  Y    Y_hat    residual\n1       A 1 65 64.38095  0.61904762\n2       B 2 70 70.49524 -0.49523810\n3       C 3 75 76.60952 -1.60952381\n4       D 4 85 82.72381  2.27619048\n5       E 5 88 88.83810 -0.83809524\n6       F 6 95 94.95238  0.04761905\n\ncat(\"Sum of residuals (should be ‚âà 0):\", round(sum(data$residual), 4), \"\\n\")\n\nSum of residuals (should be ‚âà 0): 0 \n\n# Krok 9: Sumy kwadrat√≥w\nSST &lt;- sum((data$Y - y_bar)^2)\ncat(\"\\n--- Sum of Squares Calculations ---\\n\")\n\n\n--- Sum of Squares Calculations ---\n\ncat(\"SST (Total Sum of Squares):\", round(SST, 2), \"\\n\")\n\nSST (Total Sum of Squares): 663.33 \n\nSSR &lt;- sum((data$Y_hat - y_bar)^2)\ncat(\"SSR (Regression Sum of Squares):\", round(SSR, 2), \"\\n\")\n\nSSR (Regression Sum of Squares): 654.23 \n\nSSE &lt;- sum(data$residual^2)\ncat(\"SSE (Error Sum of Squares):\", round(SSE, 2), \"\\n\")\n\nSSE (Error Sum of Squares): 9.1 \n\ncat(\"\\nVerification: SST = SSR + SSE\\n\")\n\n\nVerification: SST = SSR + SSE\n\ncat(sprintf(\"%.2f = %.2f + %.2f\\n\", SST, SSR, SSE))\n\n663.33 = 654.23 + 9.10\n\ncat(\"Difference (due to rounding):\", round(SST - (SSR + SSE), 4), \"\\n\")\n\nDifference (due to rounding): 0 \n\n# Krok 10: R-kwadrat\nR_squared_method1 &lt;- SSR / SST\nR_squared_method2 &lt;- 1 - (SSE / SST)\nR_squared_lm &lt;- summary(model)$r.squared\n\ncat(\"\\n--- R-squared Calculations ---\\n\")\n\n\n--- R-squared Calculations ---\n\ncat(\"R¬≤ (Method 1: SSR/SST):\", round(R_squared_method1, 4), \"\\n\")\n\nR¬≤ (Method 1: SSR/SST): 0.9863 \n\ncat(\"R¬≤ (Method 2: 1 - SSE/SST):\", round(R_squared_method2, 4), \"\\n\")\n\nR¬≤ (Method 2: 1 - SSE/SST): 0.9863 \n\ncat(\"R¬≤ (from lm function):\", round(R_squared_lm, 4), \"\\n\")\n\nR¬≤ (from lm function): 0.9863 \n\n# Krok 11: Wielko≈õci efektu\ncat(\"\\n--- Effect Size Calculations ---\\n\")\n\n\n--- Effect Size Calculations ---\n\ncat(\"Raw effect size (slope):\", round(beta_1_manual, 2), \"points per hour\\n\")\n\nRaw effect size (slope): 6.11 points per hour\n\nsd_x &lt;- sd(data$X)\nsd_y &lt;- sd(data$Y)\ncat(\"\\nStandard deviations:\\n\")\n\n\nStandard deviations:\n\ncat(sprintf(\"SD of X: %.2f\\n\", sd_x))\n\nSD of X: 1.87\n\ncat(sprintf(\"SD of Y: %.2f\\n\", sd_y))\n\nSD of Y: 11.52\n\nbeta_std &lt;- beta_1_manual * (sd_x / sd_y)\ncat(sprintf(\"\\nStandardized effect size: %.2f\\n\", beta_std))\n\n\nStandardized effect size: 0.99\n\n# Korelacja (dla regresji prostej |r| = sqrt(R^2))\ncorrelation &lt;- cor(data$X, data$Y)\ncat(\"\\nPearson correlation coefficient:\", round(correlation, 4), \"\\n\")\n\n\nPearson correlation coefficient: 0.9931 \n\ncat(\"Square root of R¬≤:\", round(sqrt(R_squared_method1), 4), \"\\n\")\n\nSquare root of R¬≤: 0.9931 \n\ncat(\"These should be equal (within rounding error)\\n\")\n\nThese should be equal (within rounding error)\n\n\n\n# Krok 12: Wizualizacja\ncat(\"\\n--- Creating Visualization ---\\n\")\n\n\n--- Creating Visualization ---\n\nplot(data$X, data$Y, \n     main = \"Study Hours vs Exam Scores with OLS Regression Line\",\n     xlab = \"Study Hours\",\n     ylab = \"Exam Score\",\n     pch = 19, col = \"blue\", cex = 1.5,\n     xlim = c(0, 7), ylim = c(60, 100))\n\nabline(a = beta_0_manual, b = beta_1_manual, col = \"red\", lwd = 2)\n\n# Punkt ≈õrednich\npoints(x_bar, y_bar, pch = 15, col = \"green\", cex = 2)\n\n# Reszty jako odcinki pionowe\nfor(i in 1:nrow(data)) {\n  segments(data$X[i], data$Y[i], data$X[i], data$Y_hat[i], \n           col = \"gray\", lty = 2)\n}\n\nlegend(\"bottomright\", \n       legend = c(\"Observed Data\", \"Regression Line\", \"Mean Point\", \"Residuals\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"),\n       pch = c(19, NA, 15, NA),\n       lty = c(NA, 1, NA, 2),\n       lwd = c(NA, 2, NA, 1))\n\n# R√≥wnanie na wykresie\nequation_text &lt;- sprintf(\"Y = %.2f + %.2f*X\", beta_0_manual, beta_1_manual)\nr2_text &lt;- sprintf(\"R¬≤ = %.3f\", R_squared_method1)\ntext(1.5, 92, equation_text, pos = 4)\ntext(1.5, 89, r2_text, pos = 4)\n\n\n\n\nAnaliza regresji OLS - godziny nauki vs.¬†wynik testu\n\n\n\n# Podsumowanie ko≈Ñcowe\ncat(\"\\n========== FINAL SUMMARY ==========\\n\")\n\n\n========== FINAL SUMMARY ==========\n\ncat(\"Regression Equation: Y =\", round(beta_0_manual, 2), \"+\", \n    round(beta_1_manual, 2), \"* X\\n\")\n\nRegression Equation: Y = 58.27 + 6.11 * X\n\ncat(\"R-squared:\", round(R_squared_method1, 4), \n    sprintf(\"(%.1f%% of variance explained)\\n\", R_squared_method1 * 100))\n\nR-squared: 0.9863 (98.6% of variance explained)\n\ncat(\"Standardized Effect Size:\", round(beta_std, 2), \"(Very Large Effect)\\n\")\n\nStandardized Effect Size: 0.99 (Very Large Effect)\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"- Each additional hour of study increases exam score by\", \n    round(beta_1_manual, 2), \"points\\n\")\n\n- Each additional hour of study increases exam score by 6.11 points\n\ncat(\"- Study hours explain\", sprintf(\"%.1f%%\", R_squared_method1 * 100), \n    \"of the variation in exam scores\\n\")\n\n- Study hours explain 98.6% of the variation in exam scores\n\ncat(\"- The relationship is extremely strong (correlation =\", \n    round(correlation, 3), \")\\n\")\n\n- The relationship is extremely strong (correlation = 0.993 )",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#jak-uruchomiƒá-kod",
    "href": "correg_pl.html#jak-uruchomiƒá-kod",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.13 Jak uruchomiƒá kod",
    "text": "10.13 Jak uruchomiƒá kod\n\nSkopiuj ca≈Çy blok kodu,\nWklej do RStudio,\nUruchom chunk po chunk lub ca≈Çy dokument,\nPor√≥wnaj wyniki z obliczeniami rƒôcznymi.\n\nCo zobaczysz:\n\nNachylenie: 6.12,\nWyraz wolny: 58.25,\nR^2: ‚âà 0.988,\nEfekt standaryzowany: ‚âà 0.99,\nWykres z punktami, liniƒÖ regresji i resztami.\n\nTo potwierdza poprawno≈õƒá oblicze≈Ñ manualnych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#appendix-a-obliczanie-kowariancji-korelacji-pearsona-i-spearmana-oraz-modelowanie-ols---przyk≈Çad-wprowadzajƒÖcy",
    "href": "correg_pl.html#appendix-a-obliczanie-kowariancji-korelacji-pearsona-i-spearmana-oraz-modelowanie-ols---przyk≈Çad-wprowadzajƒÖcy",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.14 Appendix A: Obliczanie Kowariancji, Korelacji Pearsona i Spearmana, oraz modelowanie OLS - przyk≈Çad wprowadzajƒÖcy",
    "text": "10.14 Appendix A: Obliczanie Kowariancji, Korelacji Pearsona i Spearmana, oraz modelowanie OLS - przyk≈Çad wprowadzajƒÖcy\nStudentka politologii bada zwiƒÖzek miƒôdzy wielko≈õciƒÖ okrƒôgu wyborczego (DM) a wska≈∫nikiem dysproporcjonalno≈õci Gallaghera (GH) w wyborach parlamentarnych w 10 losowo wybranych demokracjach.\nDane dotyczƒÖce wielko≈õci okrƒôgu wyborczego (\\text{DM}) i indeksu Gallaghera:\n\n\n\n\\text{DM} (X)\nGallagher (Y)\n\n\n\n\n2\n18,2\n\n\n3\n16,7\n\n\n4\n15,8\n\n\n5\n15,3\n\n\n6\n15,0\n\n\n7\n14,8\n\n\n8\n14,7\n\n\n9\n14,6\n\n\n10\n14,55\n\n\n11\n14,52\n\n\n\n\nKrok 1: Obliczanie Podstawowych Statystyk\nObliczanie ≈õrednich:\nDla \\text{DM} (X): \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\nSzczeg√≥≈Çowe obliczenia:\n2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 65 \\bar{x} = \\frac{65}{10} = 6,5\nDla indeksu Gallaghera (Y): \\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\nSzczeg√≥≈Çowe obliczenia:\n18,2 + 16,7 + 15,8 + 15,3 + 15,0 + 14,8 + 14,7 + 14,6 + 14,55 + 14,52 = 154,17 \\bar{y} = \\frac{154,17}{10} = 15,417\n\n\nKrok 2: Szczeg√≥≈Çowe Obliczenia Kowariancji\nPe≈Çna tabela robocza ze wszystkimi obliczeniami:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18,2\n-4,5\n2,783\n-12,5235\n20,25\n7,7451\n\n\n2\n3\n16,7\n-3,5\n1,283\n-4,4905\n12,25\n1,6461\n\n\n3\n4\n15,8\n-2,5\n0,383\n-0,9575\n6,25\n0,1467\n\n\n4\n5\n15,3\n-1,5\n-0,117\n0,1755\n2,25\n0,0137\n\n\n5\n6\n15,0\n-0,5\n-0,417\n0,2085\n0,25\n0,1739\n\n\n6\n7\n14,8\n0,5\n-0,617\n-0,3085\n0,25\n0,3807\n\n\n7\n8\n14,7\n1,5\n-0,717\n-1,0755\n2,25\n0,5141\n\n\n8\n9\n14,6\n2,5\n-0,817\n-2,0425\n6,25\n0,6675\n\n\n9\n10\n14,55\n3,5\n-0,867\n-3,0345\n12,25\n0,7517\n\n\n10\n11\n14,52\n4,5\n-0,897\n-4,0365\n20,25\n0,8047\n\n\nSuma\n65\n154,17\n0\n0\n-28,085\n82,5\n12,8442\n\n\n\nObliczanie kowariancji: \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\text{Cov}(X,Y) = \\frac{-28,085}{9} = -3,120556\n\n\nKrok 3: Obliczanie Odchylenia Standardowego\nDla \\text{DM} (X): \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}}\n\\sigma_x = \\sqrt{\\frac{82,5}{9}} = \\sqrt{9,1667} = 3,026582\nDla Gallaghera (Y): \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}}\n\\sigma_y = \\sqrt{\\frac{12,8442}{9}} = \\sqrt{1,4271} = 1,194612\n\n\nKrok 4: Obliczanie Korelacji Pearsona\nr = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\nr = \\frac{-3,120556}{3,026582 \\times 1,194612} = \\frac{-3,120556}{3,615752} = -0,863044\n\n\nKrok 5: Obliczanie Korelacji Rangowej Spearmana\nPe≈Çna tabela rangowa ze wszystkimi obliczeniami:\n\n\n\ni\nX_i\nY_i\nRanga X_i\nRanga Y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n18,2\n1\n10\n-9\n81\n\n\n2\n3\n16,7\n2\n9\n-7\n49\n\n\n3\n4\n15,8\n3\n8\n-5\n25\n\n\n4\n5\n15,3\n4\n7\n-3\n9\n\n\n5\n6\n15,0\n5\n6\n-1\n1\n\n\n6\n7\n14,8\n6\n5\n1\n1\n\n\n7\n8\n14,7\n7\n4\n3\n9\n\n\n8\n9\n14,6\n8\n3\n5\n25\n\n\n9\n10\n14,55\n9\n2\n7\n49\n\n\n10\n11\n14,52\n10\n1\n9\n81\n\n\nSuma\n\n\n\n\n\n330\n\n\n\nObliczanie korelacji Spearmana: \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n\\rho = 1 - \\frac{6 \\times 330}{10(100 - 1)} = 1 - \\frac{1980}{990} = 1 - 2 = -1\n\n\nKrok 6: Weryfikacja w R\n\n# Tworzenie wektor√≥w\nDM &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nGH &lt;- c(18.2, 16.7, 15.8, 15.3, 15.0, 14.8, 14.7, 14.6, 14.55, 14.52)\n\n# Obliczanie kowariancji\ncov(DM, GH)\n\n[1] -3.120556\n\n# Obliczanie korelacji\ncor(DM, GH, method = \"pearson\")\n\n[1] -0.8627742\n\ncor(DM, GH, method = \"spearman\")\n\n[1] -1\n\n\n\n\nKrok 7: Podstawowa Wizualizacja\n\nlibrary(ggplot2)\n\n# Tworzenie ramki danych\ndata &lt;- data.frame(DM = DM, GH = GH)\n\n# Tworzenie wykresu rozrzutu\nggplot(data, aes(x = DM, y = GH)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Wielko≈õƒá Okrƒôgu vs Indeks Gallaghera\",\n    x = \"Wielko≈õƒá Okrƒôgu (DM)\",\n    y = \"Indeks Gallaghera (GH)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nEstymacja OLS i Miary Dopasowania Modelu\n\n\nKrok 1: Obliczanie Estymator√≥w OLS\nKorzystajƒÖc z wcze≈õniej obliczonych warto≈õci:\n\n\\sum(X_i - \\bar{X})(Y_i - \\bar{Y}) = -28,085\n\\sum(X_i - \\bar{X})^2 = 82,5\n\\bar{X} = 6,5\n\\bar{Y} = 15,417\n\nObliczanie nachylenia (\\hat{\\beta_1}):\n\\hat{\\beta_1} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\hat{\\beta_1} = -28,085 √∑ 82,5 = -0,3404\nObliczanie wyrazu wolnego (\\hat{\\beta_0}): \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 15,417 - (-0,3404 √ó 6,5)\n   = 15,417 + 2,2126\n   = 17,6296\nZatem r√≥wnanie regresji OLS ma postaƒá: \\hat{Y} = 17,6296 - 0,3404X\n\n\nKrok 2: Obliczanie Warto≈õci Dopasowanych i Reszt\nPe≈Çna tabela ze wszystkimi obliczeniami:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i\ne_i = Y_i - \\hat{Y}_i\ne_i^2\n(Y_i - \\bar{Y})^2\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18,2\n16,9488\n1,2512\n1,5655\n7,7451\n2,3404\n\n\n2\n3\n16,7\n16,6084\n0,0916\n0,0084\n1,6461\n1,4241\n\n\n3\n4\n15,8\n16,2680\n-0,4680\n0,2190\n0,1467\n0,7225\n\n\n4\n5\n15,3\n15,9276\n-0,6276\n0,3939\n0,0137\n0,2601\n\n\n5\n6\n15,0\n15,5872\n-0,5872\n0,3448\n0,1739\n0,0289\n\n\n6\n7\n14,8\n15,2468\n-0,4468\n0,1996\n0,3807\n0,0290\n\n\n7\n8\n14,7\n14,9064\n-0,2064\n0,0426\n0,5141\n0,2610\n\n\n8\n9\n14,6\n14,5660\n0,0340\n0,0012\n0,6675\n0,7241\n\n\n9\n10\n14,55\n14,2256\n0,3244\n0,1052\n0,7517\n1,4184\n\n\n10\n11\n14,52\n13,8852\n0,6348\n0,4030\n0,8047\n2,3439\n\n\nSuma\n65\n154,17\n154,17\n0\n3,2832\n12,8442\n9,5524\n\n\n\nObliczenia dla warto≈õci dopasowanych:\nDla X = 2:\n≈∂ = 17,6296 + (-0,3404 √ó 2) = 16,9488\n\nDla X = 3:\n≈∂ = 17,6296 + (-0,3404 √ó 3) = 16,6084\n\n[... kontynuacja dla wszystkich warto≈õci]\n\n\nKrok 3: Obliczanie Miar Dopasowania\nSuma kwadrat√≥w reszt (SSE): SSE = \\sum e_i^2\nSSE = 3,2832\nCa≈Çkowita suma kwadrat√≥w (SST): SST = \\sum(Y_i - \\bar{Y})^2\nSST = 12,8442\nSuma kwadrat√≥w regresji (SSR): SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\nSSR = 9,5524\nWeryfikacja dekompozycji: SST = SSR + SSE\n12,8442 = 9,5524 + 3,2832 (w granicach b≈Çƒôdu zaokrƒÖglenia)\nObliczanie wsp√≥≈Çczynnika determinacji R-kwadrat: R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nR¬≤ = 9,5524 √∑ 12,8442\n   = 0,7438\n\n\nKrok 4: Weryfikacja w R\n\n# Dopasowanie modelu liniowego\nmodel &lt;- lm(GH ~ DM, data = data)\n\n# Podsumowanie statystyk\nsummary(model)\n\n\nCall:\nlm(formula = GH ~ DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62764 -0.46274 -0.08615  0.26624  1.25109 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept) 17.62976    0.50121  35.174 0.000000000467 ***\nDM          -0.34042    0.07053  -4.827        0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6406 on 8 degrees of freedom\nMultiple R-squared:  0.7444,    Adjusted R-squared:  0.7124 \nF-statistic:  23.3 on 1 and 8 DF,  p-value: 0.00131\n\n# Rƒôczne obliczenie R-kwadrat\nSST &lt;- sum((GH - mean(GH))^2)\nSSE &lt;- sum(residuals(model)^2)\nSSR &lt;- SST - SSE\nR2_manual &lt;- SSR/SST\nR2_manual\n\n[1] 0.7443793\n\n\n\n\nKrok 5: Analiza Reszt\n\n# Tworzenie wykres√≥w reszt\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\nKrok 6: Wykres Warto≈õci Przewidywanych vs Rzeczywistych\n\n# Tworzenie wykresu warto≈õci przewidywanych vs rzeczywistych\nggplot(data.frame(\n  Rzeczywiste = GH,\n  Przewidywane = fitted(model)\n), aes(x = Przewidywane, y = Rzeczywiste)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Warto≈õci Przewidywane vs Rzeczywiste\",\n    x = \"Przewidywany Indeks Gallaghera\",\n    y = \"Rzeczywisty Indeks Gallaghera\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nModele z TransformacjƒÖ LogarytmicznƒÖ\n\n\nKrok 1: Transformacja Danych\nNajpierw obliczamy logarytmy naturalne zmiennych:\n\n\n\ni\nX_i\nY_i\n\\ln(X_i)\n\\ln(Y_i)\n\n\n\n\n1\n2\n18,2\n0,6931\n2,9014\n\n\n2\n3\n16,7\n1,0986\n2,8154\n\n\n3\n4\n15,8\n1,3863\n2,7600\n\n\n4\n5\n15,3\n1,6094\n2,7278\n\n\n5\n6\n15,0\n1,7918\n2,7081\n\n\n6\n7\n14,8\n1,9459\n2,6946\n\n\n7\n8\n14,7\n2,0794\n2,6878\n\n\n8\n9\n14,6\n2,1972\n2,6810\n\n\n9\n10\n14,55\n2,3026\n2,6777\n\n\n10\n11\n14,52\n2,3979\n2,6757\n\n\n\n\n\nKrok 2: Por√≥wnanie R√≥≈ºnych Specyfikacji Modelu\nSzacujemy trzy alternatywne specyfikacje:\n\nModel log-liniowy: \\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nModel liniowo-logarytmiczny: Y_i = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\nModel log-log: \\ln(Y_i) = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\n\n\n# Tworzenie zmiennych transformowanych\ndata$log_DM &lt;- log(data$DM)\ndata$log_GH &lt;- log(data$GH)\n\n# Dopasowanie modeli\nmodel_linear &lt;- lm(GH ~ DM, data = data)\nmodel_loglinear &lt;- lm(log_GH ~ DM, data = data)\nmodel_linearlog &lt;- lm(GH ~ log_DM, data = data)\nmodel_loglog &lt;- lm(log_GH ~ log_DM, data = data)\n\n# Por√≥wnanie warto≈õci R-kwadrat\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Liniowy\", \"Log-liniowy\", \"Liniowo-logarytmiczny\", \"Log-log\"),\n  R_kwadrat = c(\n    summary(model_linear)$r.squared,\n    summary(model_loglinear)$r.squared,\n    summary(model_linearlog)$r.squared,\n    summary(model_loglog)$r.squared\n  )\n)\n\n# Wy≈õwietlenie por√≥wnania\nmodels_comparison\n\n                  Model R_kwadrat\n1               Liniowy 0.7443793\n2           Log-liniowy 0.7670346\n3 Liniowo-logarytmiczny 0.9141560\n4               Log-log 0.9288088\n\n\n\n\nKrok 3: Por√≥wnanie Wizualne\n\n# Tworzenie wykres√≥w dla ka≈ºdego modelu\np1 &lt;- ggplot(data, aes(x = DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Liniowy\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Log-liniowy\") +\n  theme_minimal()\n\np3 &lt;- ggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Liniowo-logarytmiczny\") +\n  theme_minimal()\n\np4 &lt;- ggplot(data, aes(x = log_DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Log-log\") +\n  theme_minimal()\n\n# Uk≈Çadanie wykres√≥w w siatkƒô\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nKrok 4: Analiza Reszt dla Najlepszego Modelu\nNa podstawie warto≈õci R-kwadrat, analiza reszt dla najlepiej dopasowanego modelu:\n\n# Wykresy reszt dla najlepszego modelu\npar(mfrow = c(2, 2))\nplot(model_linearlog)\n\n\n\n\n\n\n\n\n\n\nKrok 5: Interpretacja Najlepszego Modelu\nWsp√≥≈Çczynniki modelu liniowo-logarytmicznego:\n\nsummary(model_linearlog)\n\n\nCall:\nlm(formula = GH ~ log_DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40702 -0.30207 -0.04907  0.22905  0.60549 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)  19.0223     0.4079   46.64 0.0000000000494 ***\nlog_DM       -2.0599     0.2232   -9.23 0.0000153880425 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3712 on 8 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9034 \nF-statistic: 85.19 on 1 and 8 DF,  p-value: 0.00001539\n\n\nInterpretacja:\n\n\\hat{\\beta_0} reprezentuje oczekiwany Indeks Gallaghera, gdy ln(DM) = 0 (czyli gdy DM = 1)\n\\hat{\\beta_1} reprezentuje zmianƒô Indeksu Gallaghera zwiƒÖzanƒÖ z jednostkowym wzrostem ln(DM)\n\n\n\nKrok 6: Predykcje Modelu\n\n# Tworzenie wykresu predykcji dla najlepszego modelu\nggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Model Liniowo-logarytmiczny: Indeks Gallaghera vs ln(Wielko≈õƒá Okrƒôgu)\",\n    x = \"ln(Wielko≈õƒá Okrƒôgu)\",\n    y = \"Indeks Gallaghera\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nKrok 7: Analiza Elastyczno≈õci\nDla modelu log-log wsp√≥≈Çczynniki bezpo≈õrednio reprezentujƒÖ elastyczno≈õci. Obliczenie ≈õredniej elastyczno≈õci dla modelu liniowo-logarytmicznego:\n\n# Obliczenie elastyczno≈õci przy warto≈õciach ≈õrednich\nmean_DM &lt;- mean(data$DM)\nmean_GH &lt;- mean(data$GH)\nbeta1 &lt;- coef(model_linearlog)[2]\nelastycznosc &lt;- beta1 * (1/mean_GH)\nelastycznosc\n\n    log_DM \n-0.1336136 \n\n\nWarto≈õƒá ta reprezentuje procentowƒÖ zmianƒô Indeksu Gallaghera przy jednoprocentowej zmianie Wielko≈õci Okrƒôgu.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#appendix-b.-prosty-model-regresji-liniowej-mnkols---przyk≈Çady-r√≥≈ºne",
    "href": "correg_pl.html#appendix-b.-prosty-model-regresji-liniowej-mnkols---przyk≈Çady-r√≥≈ºne",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.15 Appendix B. Prosty Model Regresji Liniowej (MNK/OLS) - przyk≈Çady r√≥≈ºne",
    "text": "10.15 Appendix B. Prosty Model Regresji Liniowej (MNK/OLS) - przyk≈Çady r√≥≈ºne",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przyk≈Çad-1.-analiza-zwiƒÖzku-miƒôdzy-dobrobytem-ekonomicznym-a-frekwencjƒÖ-wyborczƒÖ",
    "href": "correg_pl.html#przyk≈Çad-1.-analiza-zwiƒÖzku-miƒôdzy-dobrobytem-ekonomicznym-a-frekwencjƒÖ-wyborczƒÖ",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.16 Przyk≈Çad 1. Analiza zwiƒÖzku miƒôdzy dobrobytem ekonomicznym a frekwencjƒÖ wyborczƒÖ",
    "text": "10.16 Przyk≈Çad 1. Analiza zwiƒÖzku miƒôdzy dobrobytem ekonomicznym a frekwencjƒÖ wyborczƒÖ\nAnaliza zwiƒÖzku miƒôdzy dobrobytem ekonomicznym a frekwencjƒÖ wyborczƒÖ w dzielnicach Amsterdamu na podstawie danych z wybor√≥w samorzƒÖdowych 2022.\n\nDane\nPr√≥ba obejmuje piƒôƒá reprezentatywnych dzielnic:\n\n\n\nDzielnica\nDoch√≥d (tys. ‚Ç¨)\nFrekwencja (%)\n\n\n\n\nNoord\n50\n60\n\n\nZuid\n45\n56\n\n\nCentrum\n56\n70\n\n\nWest\n40\n50\n\n\nOost\n60\n75\n\n\n\n\n# Wczytanie bibliotek\nlibrary(tidyverse)\n\n# Utworzenie zbioru danych\ndane &lt;- data.frame(\n  dzielnica = c(\"Noord\", \"Zuid\", \"Centrum\", \"West\", \"Oost\"),\n  dochod = c(50, 45, 56, 40, 60),\n  frekwencja = c(60, 56, 70, 50, 75)\n)\n\n# PodglƒÖd danych\ndane\n\n  dzielnica dochod frekwencja\n1     Noord     50         60\n2      Zuid     45         56\n3   Centrum     56         70\n4      West     40         50\n5      Oost     60         75\n\n\n\n\nCzƒô≈õƒá 1: Statystyki opisowe\n\n# Statystyki dla dochodu\ncat(\"DOCH√ìD (tys. ‚Ç¨):\\n\")\n\nDOCH√ìD (tys. ‚Ç¨):\n\ncat(\"≈örednia:\", mean(dane$dochod), \"\\n\")\n\n≈örednia: 50.2 \n\ncat(\"Mediana:\", median(dane$dochod), \"\\n\")\n\nMediana: 50 \n\ncat(\"Odchylenie standardowe:\", round(sd(dane$dochod), 2), \"\\n\")\n\nOdchylenie standardowe: 8.07 \n\ncat(\"Zakres:\", min(dane$dochod), \"-\", max(dane$dochod), \"\\n\\n\")\n\nZakres: 40 - 60 \n\n# Statystyki dla frekwencji\ncat(\"FREKWENCJA (%):\\n\")\n\nFREKWENCJA (%):\n\ncat(\"≈örednia:\", mean(dane$frekwencja), \"\\n\")\n\n≈örednia: 62.2 \n\ncat(\"Mediana:\", median(dane$frekwencja), \"\\n\")\n\nMediana: 60 \n\ncat(\"Odchylenie standardowe:\", round(sd(dane$frekwencja), 2), \"\\n\")\n\nOdchylenie standardowe: 10.21 \n\ncat(\"Zakres:\", min(dane$frekwencja), \"-\", max(dane$frekwencja))\n\nZakres: 50 - 75\n\n\n\n\nCzƒô≈õƒá 2: Analiza korelacji\n\n# Test korelacji Pearsona\nkorelacja &lt;- cor.test(dane$dochod, dane$frekwencja)\n\ncat(\"Wsp√≥≈Çczynnik korelacji (r):\", round(korelacja$estimate, 3), \"\\n\")\n\nWsp√≥≈Çczynnik korelacji (r): 0.994 \n\ncat(\"P-value:\", round(korelacja$p.value, 3), \"\\n\")\n\nP-value: 0.001 \n\n# Interpretacja\nif (korelacja$p.value &lt; 0.05) {\n  cat(\"Wynik jest statystycznie istotny (p &lt; 0.05)\")\n} else {\n  cat(\"Wynik nie jest statystycznie istotny (p ‚â• 0.05)\")\n}\n\nWynik jest statystycznie istotny (p &lt; 0.05)\n\n\n\n\nCzƒô≈õƒá 3: Model regresji liniowej\n\n# Dopasowanie modelu\nmodel &lt;- lm(frekwencja ~ dochod, data = dane)\n\n# Podstawowe informacje o modelu\nsummary(model)\n\n\nCall:\nlm(formula = frekwencja ~ dochod, data = dane)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \ndochod       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n# R-squared - jak dobrze model wyja≈õnia dane\ncat(\"\\nModel wyja≈õnia\", round(summary(model)$r.squared * 100, 1), \"% zmienno≈õci frekwencji\")\n\n\nModel wyja≈õnia 98.9 % zmienno≈õci frekwencji\n\n\n\n\nCzƒô≈õƒá 4: Wizualizacja\n\n# Wykres rozrzutu z liniƒÖ regresji\nggplot(dane, aes(x = dochod, y = frekwencja)) +\n  geom_point(size = 4, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\", alpha = 0.3) +\n  geom_text(aes(label = dzielnica), vjust = -1, size = 4) +\n  labs(\n    title = \"Doch√≥d vs Frekwencja wyborcza\",\n    subtitle = paste(\"r =\", round(korelacja$estimate, 3)),\n    x = \"≈öredni doch√≥d (tys. ‚Ç¨)\",\n    y = \"Frekwencja wyborcza (%)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCzƒô≈õƒá 5: Interpretacja wynik√≥w\n\n# Pobranie wsp√≥≈Çczynnik√≥w\nwspolczynniki &lt;- coef(model)\nprzeciecie &lt;- round(wspolczynniki[1], 1)\nnachylenie &lt;- round(wspolczynniki[2], 2)\n\ncat(\"R√ìWNANIE REGRESJI:\\n\")\n\nR√ìWNANIE REGRESJI:\n\ncat(\"Frekwencja =\", przeciecie, \"+\", nachylenie, \"√ó Doch√≥d\\n\\n\")\n\nFrekwencja = -0.9 + 1.26 √ó Doch√≥d\n\ncat(\"INTERPRETACJA:\\n\")\n\nINTERPRETACJA:\n\ncat(\"‚Ä¢ Wzrost dochodu o 1000‚Ç¨ zwiƒôksza frekwencjƒô o\", nachylenie, \"punkt√≥w procentowych\\n\")\n\n‚Ä¢ Wzrost dochodu o 1000‚Ç¨ zwiƒôksza frekwencjƒô o 1.26 punkt√≥w procentowych\n\ncat(\"‚Ä¢ Korelacja jest\", ifelse(korelacja$estimate &gt; 0, \"dodatnia\", \"ujemna\"), \n    \"i\", ifelse(abs(korelacja$estimate) &gt; 0.7, \"silna\", \n               ifelse(abs(korelacja$estimate) &gt; 0.5, \"umiarkowana\", \"s≈Çaba\")))\n\n‚Ä¢ Korelacja jest dodatnia i silna\n\n\n\n\nWnioski\nG≈Ç√≥wne ustalenia:\n\nIstnieje silna dodatnia korelacja (r = 0.994) miƒôdzy dochodem a frekwencjƒÖ wyborczƒÖ\nModel wyja≈õnia 98.9% zmienno≈õci w danych\nDzielnice z wy≈ºszymi dochodami majƒÖ wy≈ºszƒÖ frekwencjƒô wyborczƒÖ\n\nWa≈ºne ograniczenie:\n‚ö†Ô∏è Ma≈Ça pr√≥ba (n=5) oznacza, ≈ºe wyniki nale≈ºy traktowaƒá ostro≈ºnie i nie mo≈ºna ich generalizowaƒá na ca≈ÇƒÖ populacjƒô bez dodatkowych bada≈Ñ.\nPraktyczne zastosowanie:\nWyniki sugerujƒÖ, ≈ºe dzia≈Çania majƒÖce na celu zwiƒôkszenie frekwencji wyborczej powinny szczeg√≥lnie koncentrowaƒá siƒô na dzielnicach o ni≈ºszych dochodach.\nOgraniczenia i zastrze≈ºenia:\n‚ö†Ô∏è Krytyczne ograniczenia:\n\nBardzo ma≈Ça pr√≥ba (n=5) znacznie ogranicza mo≈ºliwo≈õƒá generalizacji\nNiska moc statystyczna - ryzyko b≈Çƒôd√≥w II rodzaju\nBrak kontroli zmiennych zak≈Ç√≥cajƒÖcych (wiek, wykszta≈Çcenie, gƒôsto≈õƒá zaludnienia)\nMo≈ºliwa korelacja pozorna - potrzebne dodatkowe zmienne kontrolne\n\nRekomendacje dla przysz≈Çych bada≈Ñ:\n\nZwiƒôkszenie pr√≥by do wszystkich dzielnic Amsterdamu\nW≈ÇƒÖczenie zmiennych demograficznych i socjoekonomicznych\nAnaliza danych longitudinalnych z kilku cykli wyborczych",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przyk≈Çad-2.-zwiƒÖzek-miƒôdzy-wielko≈õciƒÖ-okrƒôgu-a-dysproporcjonalno≈õciƒÖ-wyborczƒÖ-1",
    "href": "correg_pl.html#przyk≈Çad-2.-zwiƒÖzek-miƒôdzy-wielko≈õciƒÖ-okrƒôgu-a-dysproporcjonalno≈õciƒÖ-wyborczƒÖ-1",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.17 Przyk≈Çad 2. ZwiƒÖzek Miƒôdzy Wielko≈õciƒÖ Okrƒôgu a Dysproporcjonalno≈õciƒÖ WyborczƒÖ (1)",
    "text": "10.17 Przyk≈Çad 2. ZwiƒÖzek Miƒôdzy Wielko≈õciƒÖ Okrƒôgu a Dysproporcjonalno≈õciƒÖ WyborczƒÖ (1)\nTa analiza bada zwiƒÖzek miƒôdzy wielko≈õciƒÖ okrƒôgu wyborczego (DM) a wska≈∫nikiem dysproporcjonalno≈õci Loosemore-Hanby (LH) w wyborach parlamentarnych w 6 krajach. Indeks Loosemore-Hanby mierzy dysproporcjonalno≈õƒá wyborczƒÖ, gdzie wy≈ºsze warto≈õci wskazujƒÖ na wiƒôkszƒÖ dysproporcjonalno≈õƒá miƒôdzy g≈Çosami a mandatami.\n\nDane\n\n\nWarning: package 'knitr' was built under R version 4.4.3\n\n\n\nWielko≈õƒá Okrƒôgu i Indeks LH wed≈Çug Kraju\n\n\nCountry\nDM\nLH\n\n\n\n\nA\n3\n15.50\n\n\nB\n4\n14.25\n\n\nC\n5\n13.50\n\n\nD\n6\n13.50\n\n\nE\n7\n13.00\n\n\nF\n8\n12.75\n\n\n\n\n\n\n\nKrok 1: Obliczenie wariancji i odchylenia standardowego dla DM i LH\n\nObliczenia dla Wielko≈õci Okrƒôgu (DM)\nNajpierw obliczam ≈õredniƒÖ warto≈õci DM:\n\\bar{x}_{DM} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\frac{3 + 4 + 5 + 6 + 7 + 8}{6} = \\frac{33}{6} = 5.5\nNastƒôpnie obliczam wariancjƒô u≈ºywajƒÖc formu≈Çy z korektƒÖ Bessela:\n\\sigma^2_{DM} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\n\n\nKraj\nDM (x_i)\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\n\n\n\n\nA\n3\n3 - 5.5 = -2.5\n(-2.5)^2 = 6.25\n\n\nB\n4\n4 - 5.5 = -1.5\n(-1.5)^2 = 2.25\n\n\nC\n5\n5 - 5.5 = -0.5\n(-0.5)^2 = 0.25\n\n\nD\n6\n6 - 5.5 = 0.5\n(0.5)^2 = 0.25\n\n\nE\n7\n7 - 5.5 = 1.5\n(1.5)^2 = 2.25\n\n\nF\n8\n8 - 5.5 = 2.5\n(2.5)^2 = 6.25\n\n\nSuma\n\n\n17.5\n\n\n\n\\sigma^2_{DM} = \\frac{17.5}{6-1} = \\frac{17.5}{5} = 3.5\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji:\n\\sigma_{DM} = \\sqrt{\\sigma^2_{DM}} = \\sqrt{3.5} = 1.871\n\n\nObliczenia dla Indeksu LH\nNajpierw obliczam ≈õredniƒÖ warto≈õci LH:\n\\bar{x}_{LH} = \\frac{15.5 + 14.25 + 13.5 + 13.5 + 13 + 12.75}{6} = \\frac{82.5}{6} = 13.75\nNastƒôpnie obliczam wariancjƒô:\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n15.5\n15.5 - 13.75 = 1.75\n(1.75)^2 = 3.0625\n\n\nB\n14.25\n14.25 - 13.75 = 0.5\n(0.5)^2 = 0.25\n\n\nC\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nD\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nE\n13\n13 - 13.75 = -0.75\n(-0.75)^2 = 0.5625\n\n\nF\n12.75\n12.75 - 13.75 = -1\n(-1)^2 = 1\n\n\nSuma\n\n\n5\n\n\n\n\\sigma^2_{LH} = \\frac{5}{6-1} = \\frac{5}{5} = 1\nOdchylenie standardowe wynosi:\n\\sigma_{LH} = \\sqrt{\\sigma^2_{LH}} = \\sqrt{1} = 1\nPodsumowanie Zadania 1:\n\nWariancja DM (z korektƒÖ Bessela): 3.5\nOdchylenie Standardowe DM: 1.871\nWariancja LH (z korektƒÖ Bessela): 1\nOdchylenie Standardowe LH: 1\n\n\n\n\nKrok 2: Obliczenie kowariancji miƒôdzy DM i LH dla tej pr√≥by danych\nKowariancja jest obliczana przy u≈ºyciu formu≈Çy z korektƒÖ Bessela:\nCov(DM, LH) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\nA\n3\n15.5\n-2.5\n1.75\n(-2.5)(1.75) = -4.375\n\n\nB\n4\n14.25\n-1.5\n0.5\n(-1.5)(0.5) = -0.75\n\n\nC\n5\n13.5\n-0.5\n-0.25\n(-0.5)(-0.25) = 0.125\n\n\nD\n6\n13.5\n0.5\n-0.25\n(0.5)(-0.25) = -0.125\n\n\nE\n7\n13\n1.5\n-0.75\n(1.5)(-0.75) = -1.125\n\n\nF\n8\n12.75\n2.5\n-1\n(2.5)(-1) = -2.5\n\n\nSuma\n\n\n\n\n-8.75\n\n\n\nCov(DM, LH) = \\frac{-8.75}{5} = -1.75\nKowariancja miƒôdzy DM i LH: -1.75\nUjemna kowariancja wskazuje na odwrotnƒÖ zale≈ºno≈õƒá: gdy wielko≈õƒá okrƒôgu wzrasta, indeks dysproporcjonalno≈õci LH ma tendencjƒô do spadku.\n\n\nKrok 3: Obliczenie wsp√≥≈Çczynnika korelacji liniowej Pearsona miƒôdzy DM i LH\nWsp√≥≈Çczynnik korelacji Pearsona obliczany jest przy u≈ºyciu formu≈Çy:\nr = \\frac{Cov(DM, LH)}{\\sigma_{DM} \\cdot \\sigma_{LH}}\nMamy ju≈º obliczone:\n\nCov(DM, LH) = -1.75\n\\sigma_{DM} = 1.871\n\\sigma_{LH} = 1\n\nr = \\frac{-1.75}{1.871 \\cdot 1} = \\frac{-1.75}{1.871} = -0.935\nWsp√≥≈Çczynnik korelacji Pearsona: -0.935\n\nInterpretacja:\nWsp√≥≈Çczynnik korelacji -0.935 wskazuje:\n\nKierunek: Znak ujemny pokazuje odwrotnƒÖ zale≈ºno≈õƒá miƒôdzy wielko≈õciƒÖ okrƒôgu a indeksem LH.\nSi≈Ça: Warto≈õƒá bezwzglƒôdna 0.935 wskazuje na bardzo silnƒÖ korelacjƒô (blisko -1).\nInterpretacja praktyczna: Poniewa≈º wy≈ºsze warto≈õci indeksu LH wskazujƒÖ na wiƒôkszƒÖ dysproporcjonalno≈õƒá, ta silna ujemna korelacja sugeruje, ≈ºe gdy wielko≈õƒá okrƒôgu wzrasta, dysproporcjonalno≈õƒá wyborcza ma tendencjƒô do znacznego spadku. Innymi s≈Çowy, systemy wyborcze z wiƒôkszymi okrƒôgami (wiƒôcej przedstawicieli wybieranych z jednego okrƒôgu) zwykle dajƒÖ bardziej proporcjonalne wyniki (ni≈ºsza dysproporcjonalno≈õƒá).\n\nOdkrycie to jest zgodne z teoriƒÖ nauk politycznych, kt√≥ra sugeruje, ≈ºe wiƒôksze okrƒôgi zapewniajƒÖ wiƒôcej mo≈ºliwo≈õci mniejszym partiom, aby uzyskaƒá reprezentacjƒô, co prowadzi do wynik√≥w wyborczych, kt√≥re lepiej odzwierciedlajƒÖ rozk≈Çad g≈Ços√≥w miƒôdzy partiami.\n\n\n\nKrok 4: Skonstruowanie modelu regresji liniowej prostej i obliczenie R-kwadrat\nFormu≈Ça dla regresji liniowej prostej:\nLH = \\beta_0 + \\beta_1 \\cdot DM + \\varepsilon\nGdzie:\n\n\\beta_0 to wyraz wolny (przeciƒôcie)\n\\beta_1 to wsp√≥≈Çczynnik nachylenia dla DM\n\nFormu≈Ça do obliczenia \\beta_1 to:\n\\beta_1 = \\frac{Cov(DM, LH)}{\\sigma^2_{DM}} = \\frac{-1.75}{3.5} = -0.5\nAby obliczyƒá \\beta_0, u≈ºywam:\n\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x} = 13.75 - (-0.5) \\cdot 5.5 = 13.75 + 2.75 = 16.5\nZatem r√≥wnanie regresji to:\nLH = 16.5 - 0.5 \\cdot DM\n\nObliczanie warto≈õci przewidywanych i b≈Çƒôd√≥w\nU≈ºywajƒÖc naszego r√≥wnania regresji, obliczam przewidywane warto≈õci LH:\n\\hat{LH} = 16.5 - 0.5 \\cdot DM\n\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i = 16.5 - 0.5x_i)\nB≈ÇƒÖd (y_i - \\hat{y}_i)\nB≈ÇƒÖd bezwzglƒôdny (|y_i - \\hat{y}_i|)\nB≈ÇƒÖd kwadratowy ([y_i - \\hat{y}_i]^2)\n\n\n\n\nA\n3\n15.5\n16.5 - 0.5(3) = 16.5 - 1.5 = 15\n15.5 - 15 = 0.5\n|0.5| = 0.5\n(0.5)^2 = 0.25\n\n\nB\n4\n14.25\n16.5 - 0.5(4) = 16.5 - 2 = 14.5\n14.25 - 14.5 = -0.25\n|-0.25| = 0.25\n(-0.25)^2 = 0.0625\n\n\nC\n5\n13.5\n16.5 - 0.5(5) = 16.5 - 2.5 = 14\n13.5 - 14 = -0.5\n|-0.5| = 0.5\n(-0.5)^2 = 0.25\n\n\nD\n6\n13.5\n16.5 - 0.5(6) = 16.5 - 3 = 13.5\n13.5 - 13.5 = 0\n|0| = 0\n(0)^2 = 0\n\n\nE\n7\n13\n16.5 - 0.5(7) = 16.5 - 3.5 = 13\n13 - 13 = 0\n|0| = 0\n(0)^2 = 0\n\n\nF\n8\n12.75\n16.5 - 0.5(8) = 16.5 - 4 = 12.5\n12.75 - 12.5 = 0.25\n|0.25| = 0.25\n(0.25)^2 = 0.0625\n\n\nSuma\n\n\n\n\n1.5\n0.625\n\n\n\n\n\nObliczanie R-kwadrat\n\nSST (Ca≈Çkowita suma kwadrat√≥w)\n\nSST = \\sum_{i=1}^{n}(y_i - \\bar{y})^2\nObliczyli≈õmy ju≈º te odchylenia w Kroku 1:\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n15.5\n1.75\n3.0625\n\n\nB\n14.25\n0.5\n0.25\n\n\nC\n13.5\n-0.25\n0.0625\n\n\nD\n13.5\n-0.25\n0.0625\n\n\nE\n13\n-0.75\n0.5625\n\n\nF\n12.75\n-1\n1\n\n\nSuma\n\n\n5\n\n\n\nSST = 5\n\nSSR (Suma kwadrat√≥w regresji)\n\nSSR = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2\n\n\n\n\n\n\n\n\n\nKraj\nPrzewidywane LH (\\hat{y}_i)\n\\hat{y}_i - \\bar{y}\n(\\hat{y}_i - \\bar{y})^2\n\n\n\n\nA\n15\n15 - 13.75 = 1.25\n(1.25)^2 = 1.5625\n\n\nB\n14.5\n14.5 - 13.75 = 0.75\n(0.75)^2 = 0.5625\n\n\nC\n14\n14 - 13.75 = 0.25\n(0.25)^2 = 0.0625\n\n\nD\n13.5\n13.5 - 13.75 = -0.25\n(-0.25)^2 = 0.0625\n\n\nE\n13\n13 - 13.75 = -0.75\n(-0.75)^2 = 0.5625\n\n\nF\n12.5\n12.5 - 13.75 = -1.25\n(-1.25)^2 = 1.5625\n\n\nSuma\n\n\n4.375\n\n\n\nSSR = 4.375\n\nSSE (Suma kwadrat√≥w b≈Çƒôd√≥w)\n\nSSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\nZ tabeli powy≈ºej, suma kwadrat√≥w b≈Çƒôd√≥w wynosi:\nSSE = 0.625\n\nWeryfikacja\n\nMo≈ºemy zweryfikowaƒá nasze obliczenia sprawdzajƒÖc, czy SST = SSR + SSE:\n5 = 4.375 + 0.625 = 5 \\checkmark\n\nObliczanie R-kwadrat\n\nR^2 = \\frac{SSR}{SST} = \\frac{4.375}{5} = 0.875\n\n\nObliczanie RMSE (Root Mean Square Error)\nRMSE jest obliczane przy u≈ºyciu formu≈Çy:\nRMSE = \\sqrt{\\frac{SSE}{n}} = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}}\nU≈ºywajƒÖc naszego obliczonego SSE:\nRMSE = \\sqrt{\\frac{0.625}{6}} = \\sqrt{0.104} \\approx 0.323\n\n\nObliczanie MAE (Mean Absolute Error)\nMAE jest obliczane przy u≈ºyciu formu≈Çy:\nMAE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}\nU≈ºywajƒÖc sum z tabeli:\nMAE = \\frac{1.5}{6} = 0.25\nModel regresji: LH = 16.5 - 0.5 \\cdot DM\nR-kwadrat: 0.875\nRMSE: 0.323\nMAE: 0.25\n\n\nInterpretacja:\n\nR√≥wnanie regresji: Dla ka≈ºdego wzrostu jednostkowego wielko≈õci okrƒôgu, indeks dysproporcjonalno≈õci LH jest oczekiwany spadek o 0.5 jednostki. Wyraz wolny (16.5) reprezentuje oczekiwany indeks LH, gdy wielko≈õƒá okrƒôgu wynosi zero (choƒá nie ma to praktycznego znaczenia, poniewa≈º wielko≈õƒá okrƒôgu nie mo≈ºe wynosiƒá zero).\nR-kwadrat: 0.875 wskazuje, ≈ºe oko≈Ço 87.5% wariancji w dysproporcjonalno≈õci wyborczej (indeks LH) mo≈ºe byƒá wyja≈õnione przez wielko≈õƒá okrƒôgu. Jest to wysoka warto≈õƒá, sugerujƒÖca, ≈ºe wielko≈õƒá okrƒôgu jest rzeczywi≈õcie silnym predyktorem dysproporcjonalno≈õci wyborczej.\nRMSE i MAE: Niskie warto≈õci RMSE (0.323) i MAE (0.25) wskazujƒÖ, ≈ºe model dobrze dopasowuje siƒô do danych, z ma≈Çymi b≈Çƒôdami predykcji.\nImplikacje polityczne: Odkrycia sugerujƒÖ, ≈ºe zwiƒôkszanie wielko≈õci okrƒôgu mog≈Çoby byƒá skutecznƒÖ strategiƒÖ reformy wyborczej dla kraj√≥w dƒÖ≈ºƒÖcych do zmniejszenia dysproporcjonalno≈õci miƒôdzy g≈Çosami a mandatami. Jednak korzy≈õci marginalne wydajƒÖ siƒô zmniejszaƒá wraz ze wzrostem wielko≈õci okrƒôgu, jak widaƒá w wzorcu danych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przyk≈Çad-3.-descriptive-statistics-and-ols-example---income-and-voter-turnout-en",
    "href": "correg_pl.html#przyk≈Çad-3.-descriptive-statistics-and-ols-example---income-and-voter-turnout-en",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.18 Przyk≈Çad 3. Descriptive Statistics and OLS Example - Income and Voter Turnout [EN]",
    "text": "10.18 Przyk≈Çad 3. Descriptive Statistics and OLS Example - Income and Voter Turnout [EN]\nBackground\nIn preparation for the 2024 municipal elections, the Amsterdam Electoral Commission conducted research on voter participation patterns across different city neighborhoods. A key question emerged:\nDoes economic prosperity of a neighborhood correlate with civic engagement, specifically voter turnout?\nData Collection\nSample: 5 representative neighborhoods in Amsterdam\nTime Period: Data from the 2022 municipal elections\nVariables:\n\nIncome: Average annual household income per capita (thousands ‚Ç¨)\nTurnout: Percentage of registered voters who voted in the election\n\n\nInitial R Output for Reference\n\n# Data\nincome &lt;- c(50, 45, 56, 40, 60)  # thousands ‚Ç¨\nturnout &lt;- c(60, 56, 70, 50, 75) # %\n\n# Full model check\nmodel &lt;- lm(turnout ~ income)\nsummary(model)\n\n\nCall:\nlm(formula = turnout ~ income)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \nincome       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n\n\n\nDispersion Measures\nMeans:\n\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n} = \\frac{50 + 45 + 56 + 40 + 60}{5} = \\frac{251}{5} = 50.2\n\\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n} = \\frac{60 + 56 + 70 + 50 + 75}{5} = \\frac{311}{5} = 62.2\n\n# Verification\nmean(income)  # 50.2\n\n[1] 50.2\n\nmean(turnout) # 62.2\n\n[1] 62.2\n\n\nVariances:\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X: (-0.2, -5.2, 5.8, -10.2, 9.8)\ns^2_X = \\frac{0.04 + 27.04 + 33.64 + 104.04 + 96.04}{4} = \\frac{260.8}{4} = 65.2\nDeviations for Y: (-2.2, -6.2, 7.8, -12.2, 12.8)\ns^2_Y = \\frac{4.84 + 38.44 + 60.84 + 148.84 + 163.84}{4} = \\frac{416.8}{4} = 104.2\n\n# Verification\nvar(income)  # 65.2\n\n[1] 65.2\n\nvar(turnout) # 104.2\n\n[1] 104.2\n\n\n\n\nCovariance and Correlation\nCovariance:\ns_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-0.2 \\times -2.2) = 0.44 (-5.2 \\times -6.2) = 32.24 (5.8 \\times 7.8) = 45.24 (-10.2 \\times -12.2) = 124.44 (9.8 \\times 12.8) = 125.44\ns_{XY} = \\frac{327.8}{4} = 81.95\n\n# Verification\ncov(income, turnout) # 81.95\n\n[1] 81.95\n\n\nCorrelation:\nr_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{81.95}{\\sqrt{65.2}\\sqrt{104.2}} = 0.994\n\n# Verification\ncor(income, turnout) # 0.994\n\n[1] 0.9942402\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{81.95}{65.2} = 1.2571429\nIntercept:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\nStep by step:\n\n1.2571429 \\times 50.2 = 63.1085714\n\\hat{\\beta_0} = 62.2 - 63.1085714 = -0.9085714\n\n\n# Verification\ncoef(model)  # Exact coefficients from R\n\n(Intercept)      income \n -0.8964724   1.2569018 \n\n\n\n\nDetailed Decomposition of Variance and R-squared\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = -0.9085714 + 1.2571429X\nThe predicted values \\hat{Y} for each X value:\nFor X = 50:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (50)\n \\hat{Y} = -0.9085714 + 62.857145 \\hat{Y} = 61.9485736\nFor X = 45:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (45)\n \\hat{Y} = -0.9085714 + 56.5714305 \\hat{Y} = 55.6535591\nFor X = 56:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (56)\n \\hat{Y} = -0.9085714 + 70.4200024 \\hat{Y} = 69.5114310\nFor X = 40:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (40)\n \\hat{Y} = -0.9085714 + 50.2657160 \\hat{Y} = 49.3571446\nFor X = 60:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (60)\n \\hat{Y} = -0.9085714 + 75.4285740 \\hat{Y} = 74.5200026\n\n# Verification of predicted values\ny_hat &lt;- -0.9085714 + 1.2571429 * income\ndata.frame(\n  X = income,\n  Y = turnout,\n  Y_hat = y_hat,\n  row.names = 1:5\n)\n\n   X  Y    Y_hat\n1 50 60 61.94857\n2 45 56 55.66286\n3 56 70 69.49143\n4 40 50 49.37714\n5 60 75 74.52000\n\n\nStep 2: Calculate SST (Total Sum of Squares)\nSST = \\sum(Y_i - \\bar{Y})^2 \\text{ where } \\bar{Y} = 62.2\n(60 - 62.2)^2 = (-2.2)^2 = 4.84 (56 - 62.2)^2 = (-6.2)^2 = 38.44 (70 - 62.2)^2 = (7.8)^2 = 60.84 (50 - 62.2)^2 = (-12.2)^2 = 148.84 (75 - 62.2)^2 = (12.8)^2 = 163.84\nSST = 4.84 + 38.44 + 60.84 + 148.84 + 163.84 = 416.8\nStep 3: Calculate SSR (Regression Sum of Squares)\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\n(61.9485736 - 62.2)^2 = (-0.2514264)^2 = 0.0632151 (55.6535591 - 62.2)^2 = (-6.5464409)^2 = 42.8558689 (69.5114310 - 62.2)^2 = (7.3114310)^2 = 53.4570178 (49.3571446 - 62.2)^2 = (-12.8428554)^2 = 164.9389370 (74.5200026 - 62.2)^2 = (12.3200026)^2 = 151.7824640\nSSR = 413.0975028\nStep 4: Calculate SSE (Error Sum of Squares)\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n(60 - 61.9485736)^2 = (-1.9485736)^2 = 3.7969384 (56 - 55.6535591)^2 = (0.3464409)^2 = 0.1200212 (70 - 69.5114310)^2 = (0.4885690)^2 = 0.2387198 (50 - 49.3571446)^2 = (0.6428554)^2 = 0.4132631 (75 - 74.5200026)^2 = (0.4799974)^2 = 0.2303975\nSSE = 4.7024972\nStep 5: Verify decomposition\nSST = SSR + SSE 416.8 = 413.0975028 + 4.7024972\nStep 6: Calculate R-squared\nR^2 = \\frac{SSR}{SST} = \\frac{413.0975028}{416.8} = 0.9916\n\n# Verification\nsummary(model)$r.squared  # Should match our calculation\n\n[1] 0.9885135\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\ndf &lt;- data.frame(income = income, turnout = turnout)\n\nggplot(df, aes(x = income, y = turnout)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Voter Turnout vs Income per Capita\",\n    x = \"Income per Capita (thousands ‚Ç¨)\",\n    y = \"Voter Turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    text = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\nInterpretation\nThe analysis shows:\n\nA very strong positive correlation (r = 0.994) between income and voter turnout\nThe regression equation \\hat{Y} = -0.9085714 + 1.2571429X indicates that:\n\nFor each ‚Ç¨1,000 increase in income, turnout increases by about 1.26 percentage points\nThe intercept (-0.9086) has little practical meaning as income is never zero\n\nThe R-squared of 0.9916 indicates that 99.16% of the variance in turnout is explained by income",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przyk≈Çad-4.-anxiety-levels-and-cognitive-performance-a-laboratory-study-en",
    "href": "correg_pl.html#przyk≈Çad-4.-anxiety-levels-and-cognitive-performance-a-laboratory-study-en",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.19 Przyk≈Çad 4. Anxiety Levels and Cognitive Performance: A Laboratory Study [EN]",
    "text": "10.19 Przyk≈Çad 4. Anxiety Levels and Cognitive Performance: A Laboratory Study [EN]\n\nData and Context\nIn a psychology experiment, researchers measured the relationship between anxiety levels (measured by galvanic skin response, GSR) and cognitive performance (score on a working memory task).\n\n# Data\nanxiety &lt;- c(2.1, 3.4, 4.2, 5.1, 5.8, 6.4, 7.2, 8.0)  # GSR readings\nperformance &lt;- c(92, 88, 84, 78, 74, 70, 65, 62)      # Working memory scores\n\n# Initial model check\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8993 -0.6660  0.2162  0.6106  1.5262 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 105.3248     1.3189   79.86 0.00000000026 ***\nanxiety      -5.4407     0.2359  -23.06 0.00000043549 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 6 degrees of freedom\nMultiple R-squared:  0.9888,    Adjusted R-squared:  0.987 \nF-statistic: 531.9 on 1 and 6 DF,  p-value: 0.0000004355\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{2.1 + 3.4 + 4.2 + 5.1 + 5.8 + 6.4 + 7.2 + 8.0}{8} = \\frac{42.2}{8} = 5.275\n\\bar{Y} = \\frac{92 + 88 + 84 + 78 + 74 + 70 + 65 + 62}{8} = \\frac{613}{8} = 76.625\n\n# Verification\nmean(anxiety)\n\n[1] 5.275\n\nmean(performance)\n\n[1] 76.625\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(2.1 - 5.275) = -3.175\n(3.4 - 5.275) = -1.875\n(4.2 - 5.275) = -1.075\n(5.1 - 5.275) = -0.175\n(5.8 - 5.275) = 0.525\n(6.4 - 5.275) = 1.125\n(7.2 - 5.275) = 1.925\n(8.0 - 5.275) = 2.725\n\nSquared deviations:\n10.08063 + 3.51563 + 1.15563 + 0.03063 + 0.27563 + 1.26563 + 3.70563 +\n7.42563 = 27.45500\ns^2_X = \\frac{27.45500}{7} = 3.922143\nSimilarly for Y: Deviations:\n15.375, 11.375, 7.375, 1.375, -2.625, -6.625, -11.625, -14.625\ns^2_Y = \\frac{236.875 + 129.391 + 54.391 + 1.891 + 6.891 + 43.891 + 135.141 + 213.891}{7} = \\frac{822.362}{7} = 117.4803\n\n# Verification\nvar(anxiety)\n\n[1] 3.922143\n\nvar(performance)\n\n[1] 117.4107\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-3.175 √ó 15.375) = -48.815625\n(-1.875 √ó 11.375) = -21.328125\n(-1.075 √ó 7.375) = -7.928125\n(-0.175 √ó 1.375) = -0.240625\n(0.525 √ó -2.625) = -1.378125\n(1.125 √ó -6.625) = -7.453125\n(1.925 √ó -11.625) = -22.378125\n(2.725 √ó -14.625) = -39.853125\nSum = -149.375\ns_{XY} = \\frac{-149.375}{7} = -21.33929\n\n# Verification\ncov(anxiety, performance)\n\n[1] -21.33929\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-21.33929}{\\sqrt{3.922143}\\sqrt{117.4803}} = -0.9932\n\n# Verification\ncor(anxiety, performance)\n\n[1] -0.9944073\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-21.33929}{3.922143} = -5.4407\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-5.4407 √ó 5.275 = -28.6997\n\\hat{\\beta_0} = 76.625 - (-28.6997) = 105.3247\n\n\n# Verification\ncoef(model)\n\n(Intercept)     anxiety \n 105.324804   -5.440721 \n\n\n\n\n4. R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}): \\hat{Y} = 105.3247 - 5.4407X\n\n# Predicted values\ny_hat &lt;- 105.3247 - 5.4407 * anxiety\ndata.frame(\n  Anxiety = anxiety,\n  Performance = performance,\n  Predicted = y_hat,\n  row.names = 1:8\n)\n\n  Anxiety Performance Predicted\n1     2.1          92  93.89923\n2     3.4          88  86.82632\n3     4.2          84  82.47376\n4     5.1          78  77.57713\n5     5.8          74  73.76864\n6     6.4          70  70.50422\n7     7.2          65  66.15166\n8     8.0          62  61.79910\n\n\nStep 2: Sum of Squares\nSST = \\sum(Y_i - \\bar{Y})^2 = 822.362\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 816.3094\nSSE = \\sum(Y_i - \\hat{Y}_i)^2 = 6.0526\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{816.3094}{822.362} = 0.9926\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.9888459\n\n\n\n\nVisualization\n\nlibrary(ggplot2)\n\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Cognitive Performance vs. Anxiety Levels\",\n    x = \"Anxiety (GSR)\",\n    y = \"Performance Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.993) between anxiety and cognitive performance\nFor each unit increase in GSR (anxiety), performance decreases by approximately 5.44 points\nThe model explains 99.26% of the variance in performance scores\nThe relationship appears to be strongly linear, suggesting a reliable anxiety-performance relationship\nThe high intercept (105.32) represents the theoretical maximum performance at zero anxiety\n\n\n\nStudy Limitations\n\nSmall sample size (n=8)\nPossible other confounding variables\nLimited range of anxiety levels\nCross-sectional rather than longitudinal data",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przyk≈Çad-5.-analiza-zwiƒÖzku-miƒôdzy-wielko≈õciƒÖ-okrƒôgu-a-wska≈∫nikiem-dysproporcjonalno≈õci-wyborczej-2",
    "href": "correg_pl.html#przyk≈Çad-5.-analiza-zwiƒÖzku-miƒôdzy-wielko≈õciƒÖ-okrƒôgu-a-wska≈∫nikiem-dysproporcjonalno≈õci-wyborczej-2",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.20 Przyk≈Çad 5. Analiza zwiƒÖzku miƒôdzy wielko≈õciƒÖ okrƒôgu a wska≈∫nikiem dysproporcjonalno≈õci wyborczej (2)",
    "text": "10.20 Przyk≈Çad 5. Analiza zwiƒÖzku miƒôdzy wielko≈õciƒÖ okrƒôgu a wska≈∫nikiem dysproporcjonalno≈õci wyborczej (2)\nTa analiza bada zwiƒÖzek miƒôdzy wielko≈õciƒÖ okrƒôgu (DM) a wska≈∫nikiem dysproporcjonalno≈õci Loosemore-Hanby (LH) w wyborach parlamentarnych w 6 krajach. Wska≈∫nik Loosemore-Hanby mierzy dysproporcjonalno≈õƒá wyborczƒÖ, przy czym wy≈ºsze warto≈õci wskazujƒÖ na wiƒôkszƒÖ dysproporcjonalno≈õƒá miƒôdzy g≈Çosami a mandatami.\n\nDane\n\n\n\nWielko≈õƒá okrƒôgu i wska≈∫nik LH wed≈Çug kraju\n\n\nKraj\nDM\nLH\n\n\n\n\nA\n4\n12\n\n\nB\n10\n8\n\n\nC\n3\n15\n\n\nD\n8\n10\n\n\nE\n7\n6\n\n\nF\n4\n13\n\n\n\n\n\n\n\nKrok 1: Obliczenie wariancji i odchylenia standardowego dla DM i LH\n\nObliczenia dla wielko≈õci okrƒôgu (DM)\nNajpierw obliczƒô ≈õredniƒÖ warto≈õci DM:\n\\bar{x}_{DM} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\frac{4 + 10 + 3 + 8 + 7 + 4}{6} = \\frac{36}{6} = 6\nNastƒôpnie obliczƒô wariancjƒô z korektƒÖ Bessela, korzystajƒÖc z wzoru:\ns^2_{DM} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\n\n\nKraj\nDM (x_i)\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\n\n\n\n\nA\n4\n4 - 6 = -2\n(-2)^2 = 4\n\n\nB\n10\n10 - 6 = 4\n(4)^2 = 16\n\n\nC\n3\n3 - 6 = -3\n(-3)^2 = 9\n\n\nD\n8\n8 - 6 = 2\n(2)^2 = 4\n\n\nE\n7\n7 - 6 = 1\n(1)^2 = 1\n\n\nF\n4\n4 - 6 = -2\n(-2)^2 = 4\n\n\nSuma\n\n\n38\n\n\n\ns^2_{DM} = \\frac{38}{6-1} = \\frac{38}{5} = 7.6\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji:\ns_{DM} = \\sqrt{s^2_{DM}} = \\sqrt{7.6} = 2.757\n\n\nObliczenia dla wska≈∫nika LH\nNajpierw obliczƒô ≈õredniƒÖ warto≈õci LH:\n\\bar{y}_{LH} = \\frac{12 + 8 + 15 + 10 + 6 + 13}{6} = \\frac{64}{6} = 10.667\nNastƒôpnie obliczƒô wariancjƒô z korektƒÖ Bessela:\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n12\n12 - 10.667 = 1.333\n(1.333)^2 = 1.777\n\n\nB\n8\n8 - 10.667 = -2.667\n(-2.667)^2 = 7.113\n\n\nC\n15\n15 - 10.667 = 4.333\n(4.333)^2 = 18.775\n\n\nD\n10\n10 - 10.667 = -0.667\n(-0.667)^2 = 0.445\n\n\nE\n6\n6 - 10.667 = -4.667\n(-4.667)^2 = 21.781\n\n\nF\n13\n13 - 10.667 = 2.333\n(2.333)^2 = 5.443\n\n\nSuma\n\n\n55.334\n\n\n\ns^2_{LH} = \\frac{55.334}{6-1} = \\frac{55.334}{5} = 11.067\nOdchylenie standardowe to:\ns_{LH} = \\sqrt{s^2_{LH}} = \\sqrt{11.067} = 3.327\nPodsumowanie kroku 1:\n\nWariancja DM (z korektƒÖ Bessela): 7.6\nOdchylenie standardowe DM: 2.757\nWariancja LH (z korektƒÖ Bessela): 11.067\nOdchylenie standardowe LH: 3.327\n\n\n\n\nKrok 2: Obliczenie kowariancji miƒôdzy DM a LH dla tej pr√≥by danych\nKowariancja jest obliczana przy u≈ºyciu wzoru z korektƒÖ Bessela:\nCov(DM, LH) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\nA\n4\n12\n-2\n1.333\n(-2)(1.333) = -2.666\n\n\nB\n10\n8\n4\n-2.667\n(4)(-2.667) = -10.668\n\n\nC\n3\n15\n-3\n4.333\n(-3)(4.333) = -12.999\n\n\nD\n8\n10\n2\n-0.667\n(2)(-0.667) = -1.334\n\n\nE\n7\n6\n1\n-4.667\n(1)(-4.667) = -4.667\n\n\nF\n4\n13\n-2\n2.333\n(-2)(2.333) = -4.666\n\n\nSuma\n\n\n\n\n-37\n\n\n\nCov(DM, LH) = \\frac{-37}{6-1} = \\frac{-37}{5} = -7.4\nKowariancja miƒôdzy DM a LH: -7.4\nUjemna kowariancja wskazuje na odwrotnƒÖ zale≈ºno≈õƒá: wraz ze wzrostem wielko≈õci okrƒôgu wska≈∫nik dysproporcjonalno≈õci LH ma tendencjƒô do spadku.\n\n\nKrok 3: Obliczenie wsp√≥≈Çczynnika korelacji liniowej Pearsona miƒôdzy DM a LH\nWsp√≥≈Çczynnik korelacji Pearsona oblicza siƒô przy u≈ºyciu wzoru:\nr = \\frac{Cov(DM, LH)}{s_{DM} \\cdot s_{LH}}\nMamy ju≈º obliczone:\n\nCov(DM, LH) = -7.4\ns_{DM} = 2.757\ns_{LH} = 3.327\n\nr = \\frac{-7.4}{2.757 \\cdot 3.327} = \\frac{-7.4}{9.172} = -0.807\nWsp√≥≈Çczynnik korelacji Pearsona: -0.807\n\nInterpretacja:\nWsp√≥≈Çczynnik korelacji -0.807 wskazuje:\n\nKierunek: Ujemny znak pokazuje odwrotnƒÖ zale≈ºno≈õƒá miƒôdzy wielko≈õciƒÖ okrƒôgu a wska≈∫nikiem LH.\nSi≈Ça: Warto≈õƒá bezwzglƒôdna 0.807 wskazuje na silnƒÖ korelacjƒô (blisko -1).\nInterpretacja praktyczna: Poniewa≈º wy≈ºsze warto≈õci wska≈∫nika LH wskazujƒÖ na wiƒôkszƒÖ dysproporcjonalno≈õƒá, ta silna ujemna korelacja sugeruje, ≈ºe wraz ze wzrostem wielko≈õci okrƒôgu, dysproporcjonalno≈õƒá wyborcza ma tendencjƒô do znacznego spadku. Innymi s≈Çowy, systemy wyborcze z wiƒôkszymi okrƒôgami wyborczymi (wiƒôcej przedstawicieli wybieranych w okrƒôgu) majƒÖ tendencjƒô do generowania bardziej proporcjonalnych wynik√≥w (mniejsza dysproporcjonalno≈õƒá).\n\nUstalenie to jest zgodne z teoriƒÖ nauk politycznych, kt√≥ra sugeruje, ≈ºe wiƒôksze okrƒôgi wyborcze zapewniajƒÖ mniejszym partiom wiƒôcej mo≈ºliwo≈õci uzyskania reprezentacji, co prowadzi do wynik√≥w wyborczych, kt√≥re lepiej odzwierciedlajƒÖ rozk≈Çad g≈Ços√≥w miƒôdzy partiami.\n\n\n\nKrok 4: Skonstruowanie prostego modelu regresji liniowej i obliczenie R-kwadrat\nU≈ºyjƒô wzoru na prostƒÖ regresjƒô liniowƒÖ:\nLH = \\beta_0 + \\beta_1 \\cdot DM + \\varepsilon\nGdzie:\n\n\\beta_0 to wyraz wolny\n\\beta_1 to wsp√≥≈Çczynnik nachylenia dla DM\n\nWz√≥r na obliczenie \\beta_1 z korektƒÖ Bessela to:\n\\beta_1 = \\frac{Cov(DM, LH)}{s^2_{DM}} = \\frac{-7.4}{7.6} = -0.974\nAby obliczyƒá \\beta_0, u≈ºyjƒô:\n\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x} = 10.667 - (-0.974) \\cdot 6 = 10.667 + 5.844 = 16.511\nZatem r√≥wnanie regresji to:\nLH = 16.511 - 0.974 \\cdot DM\n\nObliczanie R-kwadrat\nAby w≈Ça≈õciwie obliczyƒá R-kwadrat, muszƒô obliczyƒá nastƒôpujƒÖce sumy kwadrat√≥w:\n\nSST (Ca≈Çkowita suma kwadrat√≥w): Mierzy ca≈ÇkowitƒÖ zmienno≈õƒá zmiennej zale≈ºnej (LH)\nSSR (Suma kwadrat√≥w regresji): Mierzy zmienno≈õƒá wyja≈õnionƒÖ przez model regresji\nSSE (Suma kwadrat√≥w b≈Çƒôd√≥w): Mierzy niewyja≈õnionƒÖ zmienno≈õƒá modelu\n\nNajpierw obliczƒô przewidywane warto≈õci LH, u≈ºywajƒÖc naszego r√≥wnania regresji:\n\\hat{LH} = 16.511 - 0.974 \\cdot DM\n\n\n\n\n\n\n\n\n\nKraj\nDM (x_i)\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i = 16.511 - 0.974x_i)\n\n\n\n\nA\n4\n12\n16.511 - 0.974(4) = 16.511 - 3.896 = 12.615\n\n\nB\n10\n8\n16.511 - 0.974(10) = 16.511 - 9.74 = 6.771\n\n\nC\n3\n15\n16.511 - 0.974(3) = 16.511 - 2.922 = 13.589\n\n\nD\n8\n10\n16.511 - 0.974(8) = 16.511 - 7.792 = 8.719\n\n\nE\n7\n6\n16.511 - 0.974(7) = 16.511 - 6.818 = 9.693\n\n\nF\n4\n13\n16.511 - 0.974(4) = 16.511 - 3.896 = 12.615\n\n\n\nTeraz, obliczƒô ka≈ºdƒÖ sumƒô kwadrat√≥w:\n\nSST (Ca≈Çkowita suma kwadrat√≥w)\n\nSST = \\sum_{i=1}^{n}(y_i - \\bar{y})^2\nJu≈º obliczyli≈õmy te odchylenia w Kroku 1:\n\n\n\nKraj\nLH (y_i)\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\n\n\n\n\nA\n12\n1.333\n1.777\n\n\nB\n8\n-2.667\n7.113\n\n\nC\n15\n4.333\n18.775\n\n\nD\n10\n-0.667\n0.445\n\n\nE\n6\n-4.667\n21.781\n\n\nF\n13\n2.333\n5.443\n\n\nSuma\n\n\n55.334\n\n\n\nSST = 55.334\n\nSSR (Suma kwadrat√≥w regresji)\n\nSSR = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2\n\n\n\n\n\n\n\n\n\nKraj\nPrzewidywane LH (\\hat{y}_i)\n\\hat{y}_i - \\bar{y}\n(\\hat{y}_i - \\bar{y})^2\n\n\n\n\nA\n12.615\n12.615 - 10.667 = 1.948\n(1.948)^2 = 3.795\n\n\nB\n6.771\n6.771 - 10.667 = -3.896\n(-3.896)^2 = 15.178\n\n\nC\n13.589\n13.589 - 10.667 = 2.922\n(2.922)^2 = 8.538\n\n\nD\n8.719\n8.719 - 10.667 = -1.948\n(-1.948)^2 = 3.795\n\n\nE\n9.693\n9.693 - 10.667 = -0.974\n(-0.974)^2 = 0.949\n\n\nF\n12.615\n12.615 - 10.667 = 1.948\n(1.948)^2 = 3.795\n\n\nSuma\n\n\n36.05\n\n\n\nSSR = 36.05\n\nSSE (Suma kwadrat√≥w b≈Çƒôd√≥w)\n\nSSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i)\ny_i - \\hat{y}_i\n(y_i - \\hat{y}_i)^2\n\n\n\n\nA\n12\n12.615\n12 - 12.615 = -0.615\n(-0.615)^2 = 0.378\n\n\nB\n8\n6.771\n8 - 6.771 = 1.229\n(1.229)^2 = 1.510\n\n\nC\n15\n13.589\n15 - 13.589 = 1.411\n(1.411)^2 = 1.991\n\n\nD\n10\n8.719\n10 - 8.719 = 1.281\n(1.281)^2 = 1.641\n\n\nE\n6\n9.693\n6 - 9.693 = -3.693\n(-3.693)^2 = 13.638\n\n\nF\n13\n12.615\n13 - 12.615 = 0.385\n(0.385)^2 = 0.148\n\n\nSuma\n\n\n\n19.306\n\n\n\nSSE = 19.306\n\nWeryfikacja\n\nMo≈ºemy zweryfikowaƒá nasze obliczenia, sprawdzajƒÖc czy SST = SSR + SSE:\n55.334 \\approx 36.05 + 19.306 = 55.356\nNiewielka r√≥≈ºnica (0.022) wynika z zaokrƒÖgle≈Ñ w obliczeniach.\n\nObliczanie R-kwadrat\n\nR^2 = \\frac{SSR}{SST} = \\frac{36.05}{55.334} = 0.652\nAlternatywnie, mo≈ºemy te≈º obliczyƒá:\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{19.306}{55.334} = 1 - 0.349 = 0.651\nDrobna r√≥≈ºnica wynika z zaokrƒÖgle≈Ñ.\n\n\nObliczanie RMSE (Pierwiastek ≈õredniego b≈Çƒôdu kwadratowego)\nObliczanie RMSE (Pierwiastek ≈õredniego b≈Çƒôdu kwadratowego)\nRMSE oblicza siƒô przy u≈ºyciu wzoru:\nRMSE = \\sqrt{\\frac{SSE}{n}} = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}}\nKorzystajƒÖc z naszej obliczonej SSE:\nRMSE = \\sqrt{\\frac{19.306}{6}} = \\sqrt{3.218} = 1.794\nKorekta Bessela (dzielenie przez n-1 zamiast n) stosuje siƒô do estymacji wariancji pr√≥by, ale nie jest standardowo stosowana przy obliczaniu RMSE, gdy≈º RMSE jest miarƒÖ b≈Çƒôdu predykcji, a nie estymatorem parametru populacji.\n\n\nObliczanie MAE (≈öredni b≈ÇƒÖd bezwzglƒôdny)\nMAE oblicza siƒô przy u≈ºyciu wzoru:\nMAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\n\n\n\n\n\n\n\n\n\nKraj\nLH (y_i)\nPrzewidywane LH (\\hat{y}_i)\n|y_i - \\hat{y}_i|\n\n\n\n\nA\n12\n12.615\n|12 - 12.615| = 0.615\n\n\nB\n8\n6.771\n|8 - 6.771| = 1.229\n\n\nC\n15\n13.589\n|15 - 13.589| = 1.411\n\n\nD\n10\n8.719\n|10 - 8.719| = 1.281\n\n\nE\n6\n9.693\n|6 - 9.693| = 3.693\n\n\nF\n13\n12.615\n|13 - 12.615| = 0.385\n\n\nSuma\n\n\n8.614\n\n\n\nMAE = \\frac{8.614}{6} = 1.436\nModel regresji: LH = 16.511 - 0.974 \\cdot DM\nR-kwadrat: 0.651\nRMSE: 1.794\nMAE: 1.436\n\n\nInterpretacja:\n\nR√≥wnanie regresji: Dla ka≈ºdego jednostkowego wzrostu wielko≈õci okrƒôgu, wska≈∫nik dysproporcjonalno≈õci LH zmniejsza siƒô o 0.974 jednostki. Wyraz wolny (16.511) reprezentuje oczekiwany wska≈∫nik LH, gdy wielko≈õƒá okrƒôgu wynosi zero (choƒá nie ma to praktycznego znaczenia, poniewa≈º wielko≈õƒá okrƒôgu nie mo≈ºe wynosiƒá zero).\nR-kwadrat: 0.651 wskazuje, ≈ºe oko≈Ço 65.1% wariancji dysproporcjonalno≈õci wyborczej (wska≈∫nik LH) mo≈ºe byƒá wyja≈õnione przez wielko≈õƒá okrƒôgu. Jest to do≈õƒá wysoka warto≈õƒá, sugerujƒÖca, ≈ºe wielko≈õƒá okrƒôgu jest rzeczywi≈õcie silnym predyktorem dysproporcjonalno≈õci wyborczej, choƒá mniejszym ni≈º w poprzednim zestawie danych.\nRMSE: Warto≈õƒá 1.794 informuje nas o przeciƒôtnym b≈Çƒôdzie prognozy modelu. Jest to miara dok≈Çadno≈õci przewidywa≈Ñ modelu wyra≈ºona w jednostkach zmiennej zale≈ºnej (LH).\nMAE: Warto≈õƒá 1.436 informuje nas o przeciƒôtnym bezwzglƒôdnym b≈Çƒôdzie prognozy modelu. W por√≥wnaniu z RMSE, MAE jest mniej czu≈Çy na warto≈õci odstajƒÖce, co potwierdza, ≈ºe niekt√≥re obserwacje (np. dla kraju E) majƒÖ stosunkowo du≈ºy b≈ÇƒÖd predykcji.\nImplikacje polityczne: Wyniki sugerujƒÖ, ≈ºe zwiƒôkszenie wielko≈õci okrƒôgu mog≈Çoby byƒá skutecznƒÖ strategiƒÖ reform wyborczych dla kraj√≥w starajƒÖcych siƒô zmniejszyƒá dysproporcjonalno≈õƒá miƒôdzy g≈Çosami a mandatami. Jednak≈ºe, korzy≈õci marginalne wydajƒÖ siƒô zmniejszaƒá wraz ze wzrostem wielko≈õci okrƒôgu, jak widaƒá we wzorcu danych.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przyk≈Çad-6.-lƒôk-vs.-wyniki-egzaminu-analiza-korelacji-i-regresji",
    "href": "correg_pl.html#przyk≈Çad-6.-lƒôk-vs.-wyniki-egzaminu-analiza-korelacji-i-regresji",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.21 Przyk≈Çad 6. Lƒôk vs.¬†Wyniki Egzaminu: Analiza Korelacji i Regresji",
    "text": "10.21 Przyk≈Çad 6. Lƒôk vs.¬†Wyniki Egzaminu: Analiza Korelacji i Regresji\nW tym tutorialu zbadamy zwiƒÖzek miƒôdzy poziomem lƒôku przed egzaminem a wynikami egzaminacyjnymi w≈õr√≥d student√≥w uniwersytetu. Badania sugerujƒÖ, ≈ºe podczas gdy niewielka ilo≈õƒá lƒôku mo≈ºe byƒá motywujƒÖca, nadmierny lƒôk zazwyczaj pogarsza wyniki poprzez zmniejszonƒÖ koncentracjƒô, zak≈Ç√≥cenia pamiƒôci roboczej i objawy fizyczne. Przeanalizujemy dane od 8 student√≥w, aby zrozumieƒá ten zwiƒÖzek matematycznie.\n\nPrezentacja Danych\n\nZbi√≥r Danych\nZebrali≈õmy dane od 8 student√≥w, mierzƒÖc:\n\nX: Wynik lƒôku przed testem (skala 1-10, gdzie 1 = bardzo niski, 10 = bardzo wysoki)\nY: Wyniki egzaminu (wynik procentowy)\n\n\n# Nasze dane\nlƒôk &lt;- c(2.5, 3.2, 4.1, 4.8, 5.6, 6.3, 7.0, 7.9)      # Wyniki lƒôku\nwyniki &lt;- c(80, 85, 78, 82, 77, 74, 68, 72)            # Wyniki egzaminu (%)\n\n# Stworzenie ramki danych dla ≈Çatwego przeglƒÖdu\ndane &lt;- data.frame(\n  Student = 1:8,\n  Lƒôk = lƒôk,\n  Wyniki = wyniki\n)\nprint(dane)\n\n  Student Lƒôk Wyniki\n1       1 2.5     80\n2       2 3.2     85\n3       3 4.1     78\n4       4 4.8     82\n5       5 5.6     77\n6       6 6.3     74\n7       7 7.0     68\n8       8 7.9     72\n\n\n\n\nWstƒôpna Wizualizacja\nNajpierw zwizualizujmy nasze dane, aby uzyskaƒá intuicyjne zrozumienie zwiƒÖzku:\n\nlibrary(ggplot2)\n\n# Stworzenie wykresu punktowego\nggplot(dane, aes(x = Lƒôk, y = Wyniki)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  labs(\n    title = \"Lƒôk przed Testem vs. Wyniki Egzaminu\",\n    x = \"Wynik Lƒôku przed Testem\",\n    y = \"Wyniki Egzaminu (%)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(2, 8, 1)) +\n  scale_y_continuous(breaks = seq(65, 90, 5))\n\n\n\n\n\n\n\n\n\n\n\nStatystyki Opisowe\n\nObliczanie ≈örednich\n≈örednia to warto≈õƒá przeciƒôtna, obliczana przez zsumowanie wszystkich obserwacji i podzielenie przez liczbƒô obserwacji.\n≈örednia Wynik√≥w Lƒôku (X): \\bar{X} = \\frac{\\sum_{i=1}^{n} X_i}{n} = \\frac{2,5 + 3,2 + 4,1 + 4,8 + 5,6 + 6,3 + 7,0 + 7,9}{8}\n\\bar{X} = \\frac{41,4}{8} = 5,175\n≈örednia Wynik√≥w Egzaminu (Y): \\bar{Y} = \\frac{\\sum_{i=1}^{n} Y_i}{n} = \\frac{80 + 85 + 78 + 82 + 77 + 74 + 68 + 72}{8}\n\\bar{Y} = \\frac{616}{8} = 77\n\n# Weryfikacja naszych oblicze≈Ñ\n≈õrednia_x &lt;- mean(lƒôk)\n≈õrednia_y &lt;- mean(wyniki)\ncat(\"≈öredni Lƒôk:\", ≈õrednia_x, \"\\n\")\n\n≈öredni Lƒôk: 5.175 \n\ncat(\"≈örednie Wyniki:\", ≈õrednia_y, \"\\n\")\n\n≈örednie Wyniki: 77 \n\n\n\n\nObliczanie Wariancji i Odchylenia Standardowego\nWariancja mierzy, jak bardzo rozproszone sƒÖ dane od ≈õredniej. U≈ºywamy wzoru na wariancjƒô pr√≥bkowƒÖ (dzielƒÖc przez n-1).\nWariancja X:\nNajpierw obliczamy odchylenia od ≈õredniej (X_i - \\bar{X}):\n\n\n\nStudent\nX_i\nX_i - \\bar{X}\n(X_i - \\bar{X})^2\n\n\n\n\n1\n2,5\n2,5 - 5,175 = -2,675\n(-2,675)^2 = 7,1556\n\n\n2\n3,2\n3,2 - 5,175 = -1,975\n(-1,975)^2 = 3,9006\n\n\n3\n4,1\n4,1 - 5,175 = -1,075\n(-1,075)^2 = 1,1556\n\n\n4\n4,8\n4,8 - 5,175 = -0,375\n(-0,375)^2 = 0,1406\n\n\n5\n5,6\n5,6 - 5,175 = 0,425\n(0,425)^2 = 0,1806\n\n\n6\n6,3\n6,3 - 5,175 = 1,125\n(1,125)^2 = 1,2656\n\n\n7\n7,0\n7,0 - 5,175 = 1,825\n(1,825)^2 = 3,3306\n\n\n8\n7,9\n7,9 - 5,175 = 2,725\n(2,725)^2 = 7,4256\n\n\nSuma\n\n\n24,555\n\n\n\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1} = \\frac{24,555}{7} = 3,5079\ns_X = \\sqrt{3,5079} = 1,8730\nWariancja Y:\n\n\n\nStudent\nY_i\nY_i - \\bar{Y}\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n80\n80 - 77 = 3\n(3)^2 = 9\n\n\n2\n85\n85 - 77 = 8\n(8)^2 = 64\n\n\n3\n78\n78 - 77 = 1\n(1)^2 = 1\n\n\n4\n82\n82 - 77 = 5\n(5)^2 = 25\n\n\n5\n77\n77 - 77 = 0\n(0)^2 = 0\n\n\n6\n74\n74 - 77 = -3\n(-3)^2 = 9\n\n\n7\n68\n68 - 77 = -9\n(-9)^2 = 81\n\n\n8\n72\n72 - 77 = -5\n(-5)^2 = 25\n\n\nSuma\n\n\n214\n\n\n\ns^2_Y = \\frac{\\sum(Y_i - \\bar{Y})^2}{n-1} = \\frac{214}{7} = 30,5714\ns_Y = \\sqrt{30,5714} = 5,5291\n\n# Weryfikacja wariancji i odchylenia standardowego\ncat(\"Wariancja Lƒôku:\", var(lƒôk), \"\\n\")\n\nWariancja Lƒôku: 3.507857 \n\ncat(\"Odch. Stand. Lƒôku:\", sd(lƒôk), \"\\n\")\n\nOdch. Stand. Lƒôku: 1.872927 \n\ncat(\"Wariancja Wynik√≥w:\", var(wyniki), \"\\n\")\n\nWariancja Wynik√≥w: 30.57143 \n\ncat(\"Odch. Stand. Wynik√≥w:\", sd(wyniki), \"\\n\")\n\nOdch. Stand. Wynik√≥w: 5.529144 \n\n\n\n\n\nKowariancja i Korelacja\n\nObliczanie Kowariancji\nKowariancja mierzy, jak dwie zmienne zmieniajƒÖ siƒô razem. Ujemna kowariancja wskazuje, ≈ºe gdy jedna zmienna wzrasta, druga ma tendencjƒô do spadku.\ns_{XY} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nObliczmy iloczyny dla ka≈ºdej obserwacji:\n\n\n\n\n\n\n\n\n\nStudent\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n\n\n\n\n1\n-2,675\n3\n(-2,675) \\times (3) = -8,025\n\n\n2\n-1,975\n8\n(-1,975) \\times (8) = -15,800\n\n\n3\n-1,075\n1\n(-1,075) \\times (1) = -1,075\n\n\n4\n-0,375\n5\n(-0,375) \\times (5) = -1,875\n\n\n5\n0,425\n0\n(0,425) \\times (0) = 0\n\n\n6\n1,125\n-3\n(1,125) \\times (-3) = -3,375\n\n\n7\n1,825\n-9\n(1,825) \\times (-9) = -16,425\n\n\n8\n2,725\n-5\n(2,725) \\times (-5) = -13,625\n\n\nSuma\n\n\n-60,2\n\n\n\ns_{XY} = \\frac{-60,2}{7} = -8,6\n\n\nObliczanie Wsp√≥≈Çczynnika Korelacji Pearsona\nWsp√≥≈Çczynnik korelacji standaryzuje kowariancjƒô do skali od -1 do +1:\nr = \\frac{s_{XY}}{s_X \\cdot s_Y} = \\frac{-8,6}{1,8730 \\times 5,5291} = \\frac{-8,6}{10,3560} = -0,831\nTo daje nam korelacjƒô -0,831, wskazujƒÖcƒÖ na silny ujemny zwiƒÖzek miƒôdzy lƒôkiem a wynikami.\n\n# Weryfikacja korelacji\nrzeczywista_kor &lt;- cor(lƒôk, wyniki)\ncat(\"Wsp√≥≈Çczynnik korelacji Pearsona:\", rzeczywista_kor, \"\\n\")\n\nWsp√≥≈Çczynnik korelacji Pearsona: -0.8304618 \n\n\n\n\n\nProsta Regresja MNK\n\nProblem Modelowania ZwiƒÖzk√≥w\nKiedy obserwujemy zwiƒÖzek miƒôdzy dwiema zmiennymi, chcemy znale≈∫ƒá model matematyczny, kt√≥ry:\n\nOpisuje zwiƒÖzek\nPozwala nam dokonywaƒá prognoz\nKwantyfikuje si≈Çƒô zwiƒÖzku\n\nNajprostszym modelem jest linia prosta: Y = \\beta_0 + \\beta_1 X + \\epsilon\nGdzie:\n\nY to zmienna wynikowa (wyniki)\nX to zmienna predykcyjna (lƒôk)\n\\beta_0 to punkt przeciƒôcia (wyniki gdy lƒôk = 0)\n\\beta_1 to nachylenie (zmiana wynik√≥w na jednostkƒô zmiany lƒôku)\n\\epsilon to sk≈Çadnik b≈Çƒôdu (niewyja≈õniona zmienno≈õƒá)\n\n\n\nIdea Sumy Kwadrat√≥w B≈Çƒôd√≥w (SSE)\nWyobra≈∫ sobie pr√≥bƒô narysowania linii przez nasze punkty danych. Jest niesko≈Ñczenie wiele linii, kt√≥re mogliby≈õmy narysowaƒá! Niekt√≥re przesz≈Çyby przez ≈õrodek punkt√≥w, inne mog≈Çyby byƒá za wysokie lub za niskie, za strome lub za p≈Çaskie. Potrzebujemy systematycznego sposobu okre≈õlenia, kt√≥ra linia jest ‚Äúnajlepsza‚Äù.\n\nCzym sƒÖ B≈Çƒôdy (Reszty)?\nDla ka≈ºdej linii, kt√≥rƒÖ narysujemy, ka≈ºdy punkt danych bƒôdzie mia≈Ç b≈ÇƒÖd lub resztƒô - pionowƒÖ odleg≈Ço≈õƒá od punktu do linii. To reprezentuje, jak ‚Äúb≈Çƒôdna‚Äù jest nasza prognoza dla tego punktu.\n\nB≈ÇƒÖd dodatni: Rzeczywista warto≈õƒá jest powy≈ºej przewidywanej warto≈õci (niedoszacowali≈õmy)\nB≈ÇƒÖd ujemny: Rzeczywista warto≈õƒá jest poni≈ºej przewidywanej warto≈õci (przeszacowali≈õmy)\n\n\n\nDlaczego Podnosimy B≈Çƒôdy do Kwadratu?\nProste dodawanie b≈Çƒôd√≥w nie zadzia≈Ça, poniewa≈º b≈Çƒôdy dodatnie i ujemne siƒô znoszƒÖ. Mogliby≈õmy u≈ºyƒá warto≈õci bezwzglƒôdnych, ale podnoszenie do kwadratu ma kilka zalet:\n\nWygoda matematyczna: Funkcje kwadratowe sƒÖ r√≥≈ºniczkowalne, co u≈Çatwia znalezienie minimum za pomocƒÖ rachunku r√≥≈ºniczkowego\nPenalizuje wiƒôksze b≈Çƒôdy bardziej: Kilka du≈ºych b≈Çƒôd√≥w jest gorsze ni≈º wiele ma≈Çych b≈Çƒôd√≥w\nTworzy g≈ÇadkƒÖ, miskowatƒÖ funkcjƒô: To gwarantuje unikalne minimum\n\n\n\nWz√≥r SSE\nSuma Kwadrat√≥w B≈Çƒôd√≥w to: SSE = \\sum_{i=1}^{n}(Y_i - \\hat{Y_i})^2 = \\sum_{i=1}^{n}(Y_i - (\\beta_0 + \\beta_1 X_i))^2\n\n\nZnajdowanie Najlepszej Linii\n‚ÄúNajlepsza‚Äù linia to ta, kt√≥ra minimalizuje SSE. U≈ºywajƒÖc rachunku r√≥≈ºniczkowego (biorƒÖc pochodne wzglƒôdem \\beta_0 i \\beta_1 i przyr√≥wnujƒÖc je do zera), otrzymujemy wzory MNK.\n\n\n\nEstymatory MNK\nMetoda Najmniejszych Kwadrat√≥w (MNK) znajduje warto≈õci \\beta_0 i \\beta_1, kt√≥re minimalizujƒÖ SSE:\nEstymator nachylenia: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\nEstymator punktu przeciƒôcia: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\n\nObliczanie Parametr√≥w MNK\nU≈ºywajƒÖc naszych obliczonych warto≈õci:\n\ns_{XY} = -8,6\ns^2_X = 3,5079\n\\bar{X} = 5,175\n\\bar{Y} = 77\n\nKrok 1: Oblicz nachylenie \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-8,6}{3,5079} = -2,451\nKrok 2: Oblicz punkt przeciƒôcia \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} = 77 - (-2,451 \\times 5,175) = 77 + 12,684 = 89,684\n\n# Weryfikacja z R\nmodel &lt;- lm(wyniki ~ lƒôk)\ncoef(model)\n\n(Intercept)         lƒôk \n  89.687233   -2.451639 \n\n\nNasze r√≥wnanie regresji to: \\hat{Y} = 89,684 - 2,451X\nTo oznacza:\n\nGdy lƒôk = 0, przewidywane wyniki = 89,68%\nZa ka≈ºdy wzrost lƒôku o 1 punkt, wyniki spadajƒÖ o 2,45%\n\n\n\n\nWykres Modelu\n\nlibrary(ggplot2)\n\n# Stworzenie kompleksowego wykresu\nggplot(data.frame(lƒôk, wyniki), aes(x = lƒôk, y = wyniki)) +\n  geom_point(size = 4, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1) +\n  geom_text(aes(label = paste0(\"(\", round(lƒôk, 1), \", \", wyniki, \")\")),\n            vjust = -1, size = 3) +\n  annotate(\"text\", x = 3, y = 70, \n           label = paste0(\"≈∑ = \", round(coef(model)[1], 2), \" - \", \n                         abs(round(coef(model)[2], 2)), \"x\"),\n           size = 5, color = \"red\") +\n  labs(\n    title = \"Linia Regresji: Wyniki vs. Lƒôk\",\n    subtitle = \"Wy≈ºszy lƒôk jest zwiƒÖzany z ni≈ºszymi wynikami egzaminu\",\n    x = \"Wynik Lƒôku przed Testem\",\n    y = \"Wyniki Egzaminu (%)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(2, 8, 1)) +\n  scale_y_continuous(breaks = seq(65, 90, 5))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nOcena Modelu\n\nDekompozycja Wariancji\nCa≈Çkowita zmienno≈õƒá w Y mo≈ºe byƒá roz≈Ço≈ºona na dwie czƒô≈õci:\nSST = SSE + SSR\nGdzie:\n\nSST (Sum of Squares Total): Ca≈Çkowita zmienno≈õƒá w Y\nSSE (Sum of Squared Errors): Niewyja≈õniona zmienno≈õƒá\n\nSSR (Sum of Squares Regression): Wyja≈õniona zmienno≈õƒá\n\n\n\n\n\n\n\nTip\n\n\n\nWyobra≈∫ sobie, ≈ºe chcesz zrozumieƒá, dlaczego pensje w firmie siƒô r√≥≈ºniƒÖ. Jedni zarabiajƒÖ 40 000, inni 80 000, a jeszcze inni 120 000. Mamy wiƒôc zmienno≈õƒá wynagrodze≈Ñ ‚Äî nie sƒÖ takie same.\n\nCa≈Çkowita zmienno≈õƒá (SST)\nTo pytanie: ‚ÄûJak bardzo wszystkie pensje sƒÖ rozproszone wok√≥≈Ç ≈õredniej pensji?‚Äù Je≈õli ≈õrednia to 70 000, to SST mierzy, o ile ka≈ºda pensja r√≥≈ºni siƒô od 70 000, podnosi te r√≥≈ºnice do kwadratu (≈ºeby by≈Çy dodatnie) i sumuje. To ca≈Çkowita ilo≈õƒá zmienno≈õci, kt√≥rƒÖ pr√≥bujemy wyja≈õniƒá.\n\n\nZmienno≈õƒá wyja≈õniona (SSR)\nZa≈Ç√≥≈ºmy, ≈ºe budujemy model przewidujƒÖcy pensjƒô na podstawie lat do≈õwiadczenia. Model mo≈ºe m√≥wiƒá:\n\n2 lata do≈õwiadczenia ‚Üí przewiduje 50 000\n5 lat do≈õwiadczenia ‚Üí przewiduje 70 000\n10 lat do≈õwiadczenia ‚Üí przewiduje 100 000\n\nSSR mierzy, jak bardzo te przewidywania r√≥≈ºniƒÖ siƒô od ≈õredniej. To ta czƒô≈õƒá zmienno≈õci, kt√≥rƒÖ model ‚Äûwyja≈õnia‚Äù relacjƒÖ z do≈õwiadczeniem. Innymi s≈Çowy: ‚Äûtƒô czƒô≈õƒá r√≥≈ºnic w pensjach da siƒô przypisaƒá r√≥≈ºnym poziomom do≈õwiadczenia‚Äù.\n\n\nZmienno≈õƒá niewyja≈õniona (SSE)\nTo to, co zostaje ‚Äî czƒô≈õƒá, kt√≥rej model nie t≈Çumaczy. Mo≈ºe byƒá tak, ≈ºe dwie osoby majƒÖ po 5 lat do≈õwiadczenia, ale jedna zarabia 65 000, a druga 75 000. Model obu przewidzia≈Ç 70 000. Te r√≥≈ºnice wzglƒôdem przewidywa≈Ñ (b≈Çƒôdy) reprezentujƒÖ zmienno≈õƒá wynikajƒÖcƒÖ z innych czynnik√≥w ‚Äî np. edukacja, wyniki pracy, umiejƒôtno≈õci negocjacyjne albo po prostu losowo≈õƒá.\n\n\n10.22 Kluczowa intuicja\nPiƒôkne jest to, ≈ºe te trzy wielko≈õci zawsze spe≈ÇniajƒÖ zale≈ºno≈õƒá: Ca≈Çkowita zmienno≈õƒá = Zmienno≈õƒá wyja≈õniona + Zmienno≈õƒá niewyja≈õniona czyli SST = SSR + SSE.\nMo≈ºesz my≈õleƒá o tym jak o ‚Äûwykresie ko≈Çowym powod√≥w, dlaczego pensje siƒô r√≥≈ºniƒÖ‚Äù:\n\nJeden kawa≈Çek to ‚Äûr√≥≈ºnice wyja≈õnione do≈õwiadczeniem‚Äù (SSR),\nDrugi kawa≈Çek to ‚Äûr√≥≈ºnice z innych powod√≥w‚Äù (SSE),\nRazem dajƒÖ ca≈Ço≈õƒá (SST).\n\n\n\n10.23 Dlaczego to wa≈ºne\nTo rozbicie pozwala policzyƒá wsp√≥≈Çczynnik determinacji R^2:\n\nR^2 = \\frac{\\mathrm{SSR}}{\\mathrm{SST}} = \\frac{\\text{Zmienno≈õƒá wyja≈õniona}}{\\text{Ca≈Çkowita zmienno≈õƒá}}.\n\nJe≈õli R^2 = 0{,}70, to model wyja≈õnia 70% tego, dlaczego warto≈õci Y (tu: pensje) r√≥≈ºniƒÖ siƒô miƒôdzy sobƒÖ. Pozosta≈Çe 30% to czynniki nieuwzglƒôdnione lub szum losowy.\nPomy≈õl o tym jak o rozwiƒÖzywaniu zagadki: SST to ca≈Ça zagadka do rozwiƒÖzania, SSR to czƒô≈õƒá ju≈º rozwiƒÖzana, a SSE to to, co wciƒÖ≈º pozostaje do wyja≈õnienia!\n\n\n\nObliczmy ka≈ºdƒÖ:\nKrok 1: Oblicz przewidywane warto≈õci\nU≈ºywajƒÖc \\hat{Y} = 89,684 - 2,451X:\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i = 89,684 - 2,451X_i\n\n\n\n\n1\n2,5\n80\n89,684 - 2,451(2,5) = 83,556\n\n\n2\n3,2\n85\n89,684 - 2,451(3,2) = 81,841\n\n\n3\n4,1\n78\n89,684 - 2,451(4,1) = 79,635\n\n\n4\n4,8\n82\n89,684 - 2,451(4,8) = 77,919\n\n\n5\n5,6\n77\n89,684 - 2,451(5,6) = 75,968\n\n\n6\n6,3\n74\n89,684 - 2,451(6,3) = 74,253\n\n\n7\n7,0\n68\n89,684 - 2,451(7,0) = 72,527\n\n\n8\n7,9\n72\n89,684 - 2,451(7,9) = 70,321\n\n\n\nKrok 2: Oblicz sumy kwadrat√≥w\nSST (Ca≈Çkowita zmienno≈õƒá): SST = \\sum(Y_i - \\bar{Y})^2\nZ naszych wcze≈õniejszych oblicze≈Ñ wariancji: SST = (n-1) \\times s^2_Y = 7 \\times 30,5714 = 214\nSSE (Niewyja≈õniona zmienno≈õƒá): SSE = \\sum(Y_i - \\hat{Y}_i)^2\n\n\n\n\n\n\n\n\n\n\ni\nY_i\n\\hat{Y}_i\nY_i - \\hat{Y}_i\n(Y_i - \\hat{Y}_i)^2\n\n\n\n\n1\n80\n83,556\n-3,556\n12,645\n\n\n2\n85\n81,841\n3,159\n9,979\n\n\n3\n78\n79,635\n-1,635\n2,673\n\n\n4\n82\n77,919\n4,081\n16,655\n\n\n5\n77\n75,968\n1,032\n1,065\n\n\n6\n74\n74,253\n-0,253\n0,064\n\n\n7\n68\n72,527\n-4,527\n20,494\n\n\n8\n72\n70,321\n1,679\n2,819\n\n\nSuma\n\n\n\n66,394\n\n\n\nSSR (Wyja≈õniona zmienno≈õƒá): SSR = SST - SSE = 214 - 66,394 = 147,606\n\n# Weryfikacja oblicze≈Ñ\ntabela_anova &lt;- anova(model)\ncat(\"SSR (Regresja):\", tabela_anova$`Sum Sq`[1], \"\\n\")\n\nSSR (Regresja): 147.5887 \n\ncat(\"SSE (Reszty):\", tabela_anova$`Sum Sq`[2], \"\\n\")\n\nSSE (Reszty): 66.41132 \n\ncat(\"SST (Ca≈Çkowita):\", sum(tabela_anova$`Sum Sq`), \"\\n\")\n\nSST (Ca≈Çkowita): 214 \n\n\n\n\nR-kwadrat (Wsp√≥≈Çczynnik Determinacji)\nR-kwadrat m√≥wi nam, jaka czƒô≈õƒá ca≈Çkowitej zmienno≈õci w Y jest wyja≈õniona przez nasz model:\nR^2 = \\frac{SSR}{SST} = \\frac{147,606}{214} = 0,690\nTo oznacza, ≈ºe nasz model wyja≈õnia 69,0% zmienno≈õci w wynikach egzaminu.\nAlternatywny wz√≥r u≈ºywajƒÖcy korelacji: R^2 = r^2 = (-0,831)^2 = 0,691\n\n# Weryfikacja R-kwadrat\ncat(\"R-kwadrat:\", summary(model)$r.squared, \"\\n\")\n\nR-kwadrat: 0.6896667 \n\ncat(\"Korelacja do kwadratu:\", cor(lƒôk, wyniki)^2, \"\\n\")\n\nKorelacja do kwadratu: 0.6896667 \n\n\n\n\n\nWielko≈õƒá Efektu\nWsp√≥≈Çczynnik nachylenia \\hat{\\beta_1} = -2,451 to nasza wielko≈õƒá efektu. M√≥wi nam:\n\nWielko≈õƒá: Ka≈ºdy wzrost lƒôku o 1 punkt jest zwiƒÖzany z 2,45% spadkiem wynik√≥w\nZnaczenie praktyczne: Student przechodzƒÖcy od niskiego lƒôku (3) do wysokiego lƒôku (7) do≈õwiadczy≈Çby oczekiwanego spadku wynik√≥w o 2,451 \\times 4 = 9,80\\%\n\nStandaryzowana wielko≈õƒá efektu (wsp√≥≈Çczynnik korelacji): Korelacja r = -0,831 wskazuje na silny ujemny zwiƒÖzek.\nWytyczne Cohena dla interpretacji korelacji:\n\nMa≈Çy efekt: |r| = 0,10\n≈öredni efekt: |r| = 0,30\nDu≈ºy efekt: |r| = 0,50\n\nNasze |r| = 0,831 reprezentuje du≈ºƒÖ wielko≈õƒá efektu.\n\n\nPrzedzia≈Çy Ufno≈õci i Istotno≈õƒá Statystyczna\n\nB≈ÇƒÖd Standardowy Regresji\nNajpierw potrzebujemy b≈Çƒôdu standardowego reszt:\ns_e = \\sqrt{\\frac{SSE}{n-2}} = \\sqrt{\\frac{66,394}{6}} = \\sqrt{11,066} = 3,327\n\n\nB≈ÇƒÖd Standardowy Nachylenia\nB≈ÇƒÖd standardowy \\hat{\\beta_1} to:\nBE(\\hat{\\beta_1}) = \\frac{s_e}{\\sqrt{\\sum(X_i - \\bar{X})^2}} = \\frac{3,327}{\\sqrt{24,555}} = \\frac{3,327}{4,955} = 0,671\n\n\n95% Przedzia≈Ç Ufno≈õci dla Nachylenia\nM√≥wiƒÖc pro≈õciej: Przedzia≈Ç ufno≈õci daje nam zakres prawdopodobnych warto≈õci dla naszego prawdziwego nachylenia. Gdyby≈õmy powt√≥rzyli to badanie wiele razy, 95% obliczonych przez nas przedzia≈Ç√≥w zawiera≈Çoby prawdziwƒÖ warto≈õƒá nachylenia.\nWz√≥r u≈ºywa warto≈õci krytycznej (oko≈Ço 2,45 dla 6 stopni swobody):\nPU = \\hat{\\beta_1} \\pm (warto≈õƒá\\_krytyczna \\times BE(\\hat{\\beta_1})) PU = -2,451 \\pm (2,45 \\times 0,671) PU = -2,451 \\pm 1,644 PU = [-4,095, -0,807]\nInterpretacja: Jeste≈õmy w 95% pewni, ≈ºe prawdziwa zmiana wynik√≥w na jednostkƒô zmiany lƒôku mie≈õci siƒô miƒôdzy -4,10% a -0,81%.\n\n\nIstotno≈õƒá Statystyczna\nAby sprawdziƒá, czy zwiƒÖzek jest statystycznie istotny (tj. nie jest przypadkowy), obliczamy statystykƒô t:\nt = \\frac{\\hat{\\beta_1}}{BE(\\hat{\\beta_1})} = \\frac{-2,451}{0,671} = -3,653\nM√≥wiƒÖc pro≈õciej: Ta warto≈õƒá t m√≥wi nam, o ile b≈Çƒôd√≥w standardowych nasze nachylenie odbiega od zera. Warto≈õƒá bezwzglƒôdna 3,65 jest do≈õƒá du≈ºa (zazwyczaj warto≈õci przekraczajƒÖce ¬±2,45 sƒÖ uwa≈ºane za istotne dla naszej wielko≈õci pr√≥by), dostarczajƒÖc silnego dowodu na rzeczywisty ujemny zwiƒÖzek miƒôdzy lƒôkiem a wynikami.\n\n# Weryfikacja oblicze≈Ñ z R\nsummary(model)\n\n\nCall:\nlm(formula = wyniki ~ lƒôk)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.526 -2.116  0.400  2.050  4.081 \n\nCoefficients:\n            Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)  89.6872     3.6682  24.450 0.000000308 ***\nlƒôk          -2.4516     0.6714  -3.652      0.0107 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.327 on 6 degrees of freedom\nMultiple R-squared:  0.6897,    Adjusted R-squared:  0.6379 \nF-statistic: 13.33 on 1 and 6 DF,  p-value: 0.01069\n\nconfint(model)\n\n                2.5 %     97.5 %\n(Intercept) 80.711582 98.6628838\nlƒôk         -4.094474 -0.8088048\n\n\n\n\n\nPodsumowanie i Interpretacja\n\nCzego siƒô Nauczyli≈õmy\n\nIstnieje ujemny zwiƒÖzek miƒôdzy lƒôkiem przed testem a wynikami egzaminu (r = -0,831)\nZwiƒÖzek jest umiarkowanie silny - lƒôk wyja≈õnia 69,0% zmienno≈õci w wynikach\nEfekt jest znaczƒÖcy - ka≈ºdy wzrost lƒôku o 1 punkt jest zwiƒÖzany z oko≈Ço 2,5% spadkiem wynik√≥w\nZwiƒÖzek jest statystycznie istotny - bardzo ma≈Ço prawdopodobne, ≈ºeby by≈Ç przypadkowy\n\n\n\nImplikacje Praktyczne\nNasza analiza sugeruje, ≈ºe wysoki lƒôk przed testem pogarsza wyniki, prawdopodobnie poprzez:\n\nZak≈Ç√≥cenia poznawcze (niepokojƒÖce my≈õli konkurujƒÖ o pamiƒôƒá roboczƒÖ)\nObjawy fizyczne (pocenie siƒô, szybkie bicie serca), kt√≥re odwracajƒÖ uwagƒô od zadania\nNegatywny monolog wewnƒôtrzny zmniejszajƒÖcy pewno≈õƒá siebie i motywacjƒô\nZachowania podczas testu (po≈õpiech, podwa≈ºanie odpowiedzi)\n\n\n\nRekomendacje na Podstawie Ustale≈Ñ\nBiorƒÖc pod uwagƒô silny ujemny zwiƒÖzek, interwencje mog≈Çyby obejmowaƒá:\n\nNauczanie technik zarzƒÖdzania lƒôkiem (g≈Çƒôbokie oddychanie, progresywne rozlu≈∫nianie miƒô≈õni)\nRestrukturyzacjƒô poznawczƒÖ w celu radzenia sobie z katastroficznym my≈õleniem\nTrening umiejƒôtno≈õci nauki w celu zwiƒôkszenia pewno≈õci siebie w przygotowaniu\nTesty pr√≥bne w celu zmniejszenia strachu przed nieznanym\n\n\n\nOgraniczenia Naszej Analizy\n\nMa≈Ça wielko≈õƒá pr√≥by (n = 8) ogranicza mo≈ºliwo≈õƒá uog√≥lnienia\nZa≈Ço≈ºenie liniowo≈õci - zwiƒÖzek mo≈ºe byƒá krzywoliniowy (mo≈ºe istnieƒá optymalny lƒôk)\nInne zmienne nieuwzglƒôdnione (czas przygotowania, zdolno≈õci, sen, itp.)\nPrzyczynowo≈õƒá vs.¬†korelacja - nie mo≈ºemy udowodniƒá, ≈ºe lƒôk powoduje s≈Çabe wyniki\nSamoopisowy lƒôk - subiektywne miary mogƒÖ nie odzwierciedlaƒá pobudzenia fizjologicznego\n\n\n\n\nDodatek: Kompletny Kod R\n\n# Kompletny kod analizy\nlƒôk &lt;- c(2.5, 3.2, 4.1, 4.8, 5.6, 6.3, 7.0, 7.9)\nwyniki &lt;- c(80, 85, 78, 82, 77, 74, 68, 72)\n\n# Statystyki opisowe\nmean(lƒôk); sd(lƒôk)\n\n[1] 5.175\n\n\n[1] 1.872927\n\nmean(wyniki); sd(wyniki)\n\n[1] 77\n\n\n[1] 5.529144\n\n# Korelacja\ncor(lƒôk, wyniki)\n\n[1] -0.8304618\n\n# Regresja\nmodel &lt;- lm(wyniki ~ lƒôk)\nsummary(model)\n\n\nCall:\nlm(formula = wyniki ~ lƒôk)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.526 -2.116  0.400  2.050  4.081 \n\nCoefficients:\n            Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)  89.6872     3.6682  24.450 0.000000308 ***\nlƒôk          -2.4516     0.6714  -3.652      0.0107 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.327 on 6 degrees of freedom\nMultiple R-squared:  0.6897,    Adjusted R-squared:  0.6379 \nF-statistic: 13.33 on 1 and 6 DF,  p-value: 0.01069\n\nconfint(model)\n\n                2.5 %     97.5 %\n(Intercept) 80.711582 98.6628838\nlƒôk         -4.094474 -0.8088048\n\n# Wizualizacja\nlibrary(ggplot2)\nggplot(data.frame(lƒôk, wyniki), aes(x = lƒôk, y = wyniki)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Diagnostyka\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Tabela ANOVA\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: wyniki\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nlƒôk        1 147.589 147.589  13.334 0.01069 *\nResiduals  6  66.411  11.069                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kluczowa-intuicja",
    "href": "correg_pl.html#kluczowa-intuicja",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.22 Kluczowa intuicja",
    "text": "10.22 Kluczowa intuicja\nPiƒôkne jest to, ≈ºe te trzy wielko≈õci zawsze spe≈ÇniajƒÖ zale≈ºno≈õƒá: Ca≈Çkowita zmienno≈õƒá = Zmienno≈õƒá wyja≈õniona + Zmienno≈õƒá niewyja≈õniona czyli SST = SSR + SSE.\nMo≈ºesz my≈õleƒá o tym jak o ‚Äûwykresie ko≈Çowym powod√≥w, dlaczego pensje siƒô r√≥≈ºniƒÖ‚Äù:\n\nJeden kawa≈Çek to ‚Äûr√≥≈ºnice wyja≈õnione do≈õwiadczeniem‚Äù (SSR),\nDrugi kawa≈Çek to ‚Äûr√≥≈ºnice z innych powod√≥w‚Äù (SSE),\nRazem dajƒÖ ca≈Ço≈õƒá (SST).",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#dlaczego-to-wa≈ºne",
    "href": "correg_pl.html#dlaczego-to-wa≈ºne",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.23 Dlaczego to wa≈ºne",
    "text": "10.23 Dlaczego to wa≈ºne\nTo rozbicie pozwala policzyƒá wsp√≥≈Çczynnik determinacji R^2:\n\nR^2 = \\frac{\\mathrm{SSR}}{\\mathrm{SST}} = \\frac{\\text{Zmienno≈õƒá wyja≈õniona}}{\\text{Ca≈Çkowita zmienno≈õƒá}}.\n\nJe≈õli R^2 = 0{,}70, to model wyja≈õnia 70% tego, dlaczego warto≈õci Y (tu: pensje) r√≥≈ºniƒÖ siƒô miƒôdzy sobƒÖ. Pozosta≈Çe 30% to czynniki nieuwzglƒôdnione lub szum losowy.\nPomy≈õl o tym jak o rozwiƒÖzywaniu zagadki: SST to ca≈Ça zagadka do rozwiƒÖzania, SSR to czƒô≈õƒá ju≈º rozwiƒÖzana, a SSE to to, co wciƒÖ≈º pozostaje do wyja≈õnienia!",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#przyk≈Çad-7.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis-en",
    "href": "correg_pl.html#przyk≈Çad-7.-district-magnitude-and-electoral-disproportionality-a-comparative-analysis-en",
    "title": "10¬† Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "10.24 Przyk≈Çad 7. District Magnitude and Electoral Disproportionality: A Comparative Analysis (*) [EN]",
    "text": "10.24 Przyk≈Çad 7. District Magnitude and Electoral Disproportionality: A Comparative Analysis (*) [EN]\n\nData Generating Process\nLet‚Äôs set up a DGP where:\n\\begin{aligned}\n& Y_{\\text{Gallagher}} = 12 - 0.8X_{\\text{DM}} + \\varepsilon \\\\\n& \\varepsilon \\sim \\mathcal{N}(0, 1) \\\\\n& X_{\\text{DM}} \\in \\{3, 5, 7, 10, 12, 15\\}\n\\end{aligned}\n\n# DGP\nmagnitude &lt;- c(3, 5, 7, 10, 12, 15)\nepsilon &lt;- rnorm(6, mean = 0, sd = 1)\ngallagher &lt;- 12 - 0.8 * magnitude + epsilon\n\n# Round (sampled from the DGP) Gallagher indices to one decimal place\ngallagher &lt;- round(c(9.0, 7.8, 9.2, 4.1, 2.5, 1.7), 1)\n\n# Show data\ndata.frame(\n  District_Magnitude = magnitude,\n  Gallagher_Index = gallagher\n)\n\n  District_Magnitude Gallagher_Index\n1                  3             9.0\n2                  5             7.8\n3                  7             9.2\n4                 10             4.1\n5                 12             2.5\n6                 15             1.7\n\n# Initial model check\nmodel &lt;- lm(gallagher ~ magnitude)\nsummary(model)\n\n\nCall:\nlm(formula = gallagher ~ magnitude)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.6516 -0.4628  2.3260 -0.6908 -0.9020  0.3813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7349     1.3034   9.003 0.000843 ***\nmagnitude    -0.6944     0.1359  -5.110 0.006934 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.368 on 4 degrees of freedom\nMultiple R-squared:  0.8672,    Adjusted R-squared:  0.834 \nF-statistic: 26.11 on 1 and 4 DF,  p-value: 0.006934\n\n\n\n\nDescriptive Statistics\nMeans: \\bar{X} = \\frac{3 + 5 + 7 + 10 + 12 + 15}{6} = \\frac{52}{6} = 8.6667\n\\bar{Y} = \\frac{9.0 + 7.8 + 9.2 + 4.1 + 2.5 + 1.7}{6} = \\frac{34.3}{6} = 5.7167\n\n# Verification\nmean(magnitude)\n\n[1] 8.666667\n\nmean(gallagher)\n\n[1] 5.716667\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(3 - 8.6667) = -5.6667\n(5 - 8.6667) = -3.6667\n(7 - 8.6667) = -1.6667\n(10 - 8.6667) = 1.3333\n(12 - 8.6667) = 3.3333\n(15 - 8.6667) = 6.3333\n\nSquared deviations:\n\n32.1115 + 13.4445 + 2.7779 + 1.7777 + 11.1109 + 40.1107 = 101.3332\n\ns^2_X = \\frac{101.3332}{5} = 20.2666\nFor Y: Deviations: 3.2833, 2.0833, 3.4833, -1.6167, -3.2167, -4.0167\ns^2_Y = \\frac{56.3483}{5} = 11.2697\n\n# Verification\nvar(magnitude)\n\n[1] 20.26667\n\nvar(gallagher)\n\n[1] 11.26967\n\n\n\n\nCovariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n\n(-5.6667 √ó 3.2833) = -18.6057\n(-3.6667 √ó 2.0833) = -7.6387\n(-1.6667 √ó 3.4833) = -5.8056\n(1.3333 √ó -1.6167) = -2.1556\n(3.3333 √ó -3.2167) = -10.7223\n(6.3333 √ó -4.0167) = -25.4391\n\nSum = -70.3670\ns_{XY} = \\frac{-70.3670}{5} = -14.0734\n\n# Verification\ncov(magnitude, gallagher)\n\n[1] -14.07333\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-14.0734}{\\sqrt{20.2666}\\sqrt{11.2697}} = -0.9279\n\n# Verification\ncor(magnitude, gallagher)\n\n[1] -0.9312157\n\n\n\n\nOLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-14.0734}{20.2666} = -0.6944\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-0.6944 √ó 8.6667 = -6.0181\n\\hat{\\beta_0} = 5.7167 - (-6.0181) = 11.7348\n\n\n# Verification\ncoef(model)\n\n(Intercept)   magnitude \n 11.7348684  -0.6944079 \n\n\n\n\nR-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = 11.7348 - 0.6944X\n\n# Predicted values\ny_hat &lt;- 11.7348 - 0.6944 * magnitude\ndata.frame(\n  Magnitude = magnitude,\n  Gallagher = gallagher,\n  Predicted = y_hat,\n  row.names = 1:6\n)\n\n  Magnitude Gallagher Predicted\n1         3       9.0    9.6516\n2         5       7.8    8.2628\n3         7       9.2    6.8740\n4        10       4.1    4.7908\n5        12       2.5    3.4020\n6        15       1.7    1.3188\n\n\nStep 2: Sum of Squares SST = \\sum(Y_i - \\bar{Y})^2 = 56.3483 SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 48.5271 SSE = \\sum(Y_i - \\hat{Y}_i)^2 = 7.8212\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{48.5271}{56.3483} = 0.8612\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.8671626\n\n\n\n\nVisualization - True vs.¬†Estimated Parameters\n\nTrue DGP: Y = 12 - 0.8X + Œµ\nEstimated Model: Y = 11.7348 - 0.6944X\n\n\nlibrary(ggplot2)\n\n# Create data frame with original data\ndf &lt;- data.frame(\n  magnitude = magnitude,\n  gallagher = gallagher\n)\n\n# Create sequence for smooth lines\nx_seq &lt;- seq(min(magnitude), max(magnitude), length.out = 100)\n\n# Calculate predicted values for both lines\ntrue_dgp &lt;- 12 - 0.8 * x_seq\nestimated &lt;- 11.7348 - 0.6944 * x_seq\n\n# Combine into a data frame for plotting\nlines_df &lt;- data.frame(\n  magnitude = rep(x_seq, 2),\n  value = c(true_dgp, estimated),\n  Model = rep(c(\"True DGP\", \"Estimated\"), each = length(x_seq))\n)\n\n# Create plot\nggplot() +\n  geom_line(data = lines_df, \n            aes(x = magnitude, y = value, color = Model, linetype = Model),\n            size = 1) +\n  geom_point(data = df, \n             aes(x = magnitude, y = gallagher),\n             color = \"black\", \n             size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  labs(\n    title = \"True DGP vs. Estimated Regression Line\",\n    subtitle = \"Black points show observed data with random noise\",\n    x = \"District Magnitude\",\n    y = \"Gallagher Index\",\n    caption = \"True DGP: Y = 12 - 0.8X + Œµ\\nEstimated: Y = 11.73 - 0.69X\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.caption = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\n\nObservations about Model Fit\n\nSlope Comparison\n\nTrue slope: -0.8\nEstimated slope: -0.69\nThe estimated slope is reasonably close to the true parameter\n\nIntercept Comparison\n\nTrue intercept: 12\nEstimated intercept: 11.73\nThe estimated intercept very closely approximates the true value\n\nVisual Patterns\n\nThe lines are nearly parallel, showing good slope recovery\nPoints scatter around both lines due to the random error term (Œµ)\nThe small sample size (n=6) leads to some imprecision in estimation\nThe estimated line (blue) provides a good approximation of the true DGP (red dashed)\n\nImpact of Random Error\n\nThe scatter of points around the true DGP line reflects the N(0,1) error term\nThis noise leads to the slight differences in estimated parameters\nWith a larger sample, we would expect even closer convergence to true parameters\n\n\n\n\nInterpretation\n\nStrong negative correlation (r = -0.93) between district magnitude and electoral disproportionality\nFor each unit increase in district magnitude, the Gallagher index decreases by approximately 0.69 points\nThe model explains 86.12% of the variance in disproportionality\nThe relationship appears strongly linear with moderate scatter\nThe intercept (11.73) represents the expected disproportionality in a hypothetical single-member district system\n\n\n\nStudy Context\n\nData represents simulated observations from a DGP with moderate noise\nSample shows how increasing district magnitude tends to reduce disproportionality\nRandom component reflects other institutional and political factors affecting disproportionality\n\n\n\nLimitations\n\nSmall sample size (n=6)\nSimulated rather than real-world data\nAssumes linear relationship\nDoes not account for other institutional features",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "inference_en.html",
    "href": "inference_en.html",
    "title": "13¬† Introduction to Statistical Inference: The Logic of Statistical Hypothesis Testing",
    "section": "",
    "text": "(‚Ä¶)",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Introduction to Statistical Inference: The Logic of Statistical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "inference_pl.html",
    "href": "inference_pl.html",
    "title": "14¬† Wprowadzenie do Wnioskowania Statystycznego: Logika Testowania Hipotez Statystycznych",
    "section": "",
    "text": "(‚Ä¶)",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Wprowadzenie do Wnioskowania Statystycznego: Logika Testowania Hipotez Statystycznych</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\n\n\n\nBlair, G., Coppock, A., & Humphreys, M. (2023). Research design in the social sciences: declaration, diagnosis, and redesign. Princeton University Press. https://book.declaredesign.org/\nBryman, A., 2016. Social research methods. Oxford University Press.\nBueno de Mesquita, Ethan and Anthony Fowler. 2021. Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis. Princeton University Press.\nCausality for Machine Learning. https://ff13.fastforwardlabs.com/\nCetinkaya-Rundel, M., Diez, D.M. and Barr, C.D., 2019 (4th ed.). OpenIntro Statistics: an Open-source Textbook: https://www.openintro.org/book/os/\nClaude [Large language model], 2024. https://www.anthropic.com\nConcepts and Computation: An Introduction to Political Methodology. https://pos3713.github.io/notes/\nHannay, K. (2019). Introduction to statistics and data science. http://khannay.com/StatsBook/\nIsmay, C. and Kim, A.Y., 2019. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. https://moderndive.com/index.html\nNavarro, D.J. and Foxcroft, D.R. (2019). Learning statistics with Jamovi: a tutorial for psychology students and other beginners. (Version 0.70). DOI: 10.24384/hgc3-7p15\nRemler, D.K. and Van Ryzin, G.G., 2014. Research methods in practice: Strategies for description and causation. Sage Publications.\nSanchez, G., Marzban, E. (2020) All Models Are Wrong: Concepts of Statistical Learning. https://allmodelsarewrong.github.io\nSchneider, W. J. (2023). Psycheval: A psychological evaluation toolkit. https://github.com/wjschne/psycheval\nTimbers, T., Campbell, T., & Lee, M. (2022). Data science: A first introduction. Chapman and Hall/CRC. https://datasciencebook.ca/",
    "crumbs": [
      "References"
    ]
  }
]